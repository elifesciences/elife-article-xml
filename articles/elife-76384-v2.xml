<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">76384</article-id><article-id pub-id-type="doi">10.7554/eLife.76384</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>Learning cortical representations through perturbed and adversarial dreaming</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-255085"><name><surname>Deperrois</surname><given-names>Nicolas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7178-1818</contrib-id><email>nicolas.deperrois@unibe.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-224157"><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2632-0427</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-224156"><name><surname>Senn</surname><given-names>Walter</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3622-0497</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-220986"><name><surname>Jordan</surname><given-names>Jakob</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3438-5001</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02k7v4d05</institution-id><institution>Department of Physiology, University of Bern</institution></institution-wrap><addr-line><named-content content-type="city">Bern</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/038t36y30</institution-id><institution>Kirchhoff-Institute for Physics, Heidelberg University</institution></institution-wrap><addr-line><named-content content-type="city">Heidelberg</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schapiro</surname><given-names>Anna C</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors shared senior authorship to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>04</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e76384</elocation-id><history><date date-type="received" iso-8601-date="2021-12-14"><day>14</day><month>12</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-03-07"><day>07</day><month>03</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-09-09"><day>09</day><month>09</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.48550/arXiv.2109.04261"/></event></pub-history><permissions><copyright-statement>© 2022, Deperrois et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Deperrois et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-76384-v2.pdf"/><abstract><p>Humans and other animals learn to extract general concepts from sensory experience without extensive teaching. This ability is thought to be facilitated by offline states like sleep where previous experiences are systemically replayed. However, the characteristic creative nature of dreams suggests that learning semantic representations may go beyond merely replaying previous experiences. We support this hypothesis by implementing a cortical architecture inspired by generative adversarial networks (GANs). Learning in our model is organized across three different global brain states mimicking wakefulness, non-rapid eye movement (NREM), and REM sleep, optimizing different, but complementary, objective functions. We train the model on standard datasets of natural images and evaluate the quality of the learned representations. Our results suggest that generating new, virtual sensory inputs via adversarial dreaming during REM sleep is essential for extracting semantic concepts, while replaying episodic memories via perturbed dreaming during NREM sleep improves the robustness of latent representations. The model provides a new computational perspective on sleep states, memory replay, and dreams, and suggests a cortical implementation of GANs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>representation learning</kwd><kwd>sleep</kwd><kwd>GANs</kwd><kwd>cortical networks</kwd><kwd>NREM</kwd><kwd>REM</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>604102</award-id><principal-award-recipient><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name><name><surname>Senn</surname><given-names>Walter</given-names></name><name><surname>Jordan</surname><given-names>Jakob</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>720270</award-id><principal-award-recipient><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name><name><surname>Senn</surname><given-names>Walter</given-names></name><name><surname>Jordan</surname><given-names>Jakob</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>785907</award-id><principal-award-recipient><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name><name><surname>Senn</surname><given-names>Walter</given-names></name><name><surname>Jordan</surname><given-names>Jakob</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000780</institution-id><institution>European Commission</institution></institution-wrap></funding-source><award-id>945539</award-id><principal-award-recipient><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name><name><surname>Senn</surname><given-names>Walter</given-names></name><name><surname>Jordan</surname><given-names>Jakob</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009068</institution-id><institution>Universität Bern</institution></institution-wrap></funding-source><award-id>Interfaculty Research Cooperation 'Decoding Sleep'</award-id><principal-award-recipient><name><surname>Deperrois</surname><given-names>Nicolas</given-names></name><name><surname>Senn</surname><given-names>Walter</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001661</institution-id><institution>Universität Heidelberg</institution></institution-wrap></funding-source><award-id>Manfred Stärk Foundation</award-id><principal-award-recipient><name><surname>Petrovici</surname><given-names>Mihai A</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>Sinergia Grant CRSII5-180316</award-id><principal-award-recipient><name><surname>Senn</surname><given-names>Walter</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A cortical model inspired by cognitive theories and deep learning demonstrates how the combination of wakefulness, non-rapid eye movement (NREM), and REM dreams leads to the emergence of robust and semantically organized neuronal representations in higher cortical areas.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>After just a single night of bad sleep, we are acutely aware of the importance of sleep for orderly body and brain function. In fact, it has become clear that sleep serves multiple crucial physiological functions (<xref ref-type="bibr" rid="bib106">Siegel, 2009</xref>; <xref ref-type="bibr" rid="bib129">Xie et al., 2013</xref>), and growing evidence highlights its impact on cognitive processes (<xref ref-type="bibr" rid="bib122">Walker, 2009</xref>). Yet, a lot remains unknown about the precise contribution of sleep, and in particular dreams, on normal brain function.</p><p>One remarkable cognitive ability of humans and other animals lies in the extraction of general concepts and statistical regularities from sensory experience without extensive teaching (<xref ref-type="bibr" rid="bib11">Bergelson and Swingley, 2012</xref>). Such regularities in the sensorium are reflected on the neuronal level in invariant object-specific representations in high-level areas of the visual cortex (<xref ref-type="bibr" rid="bib40">Grill-Spector et al., 2001</xref>; <xref ref-type="bibr" rid="bib54">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib25">DiCarlo et al., 2012</xref>) on which downstreams areas can operate. These so-called semantic representations are progressively constructed and enriched over an organism’s lifetime (<xref ref-type="bibr" rid="bib114">Tenenbaum et al., 2011</xref>; <xref ref-type="bibr" rid="bib131">Yee et al., 2013</xref>), and their emergence is hypothesized to be facilitated by offline states such as sleep (<xref ref-type="bibr" rid="bib29">Dudai et al., 2015</xref>).</p><p>Previously, several cortical models have been proposed to explain how offline states could contribute to the emergence of high-level, semantic representations. Stochastic hierarchical models that learn to maximize the likelihood of observed data under a generative model such as the Helmholtz machine (<xref ref-type="bibr" rid="bib24">Dayan et al., 1995</xref>) and the closely related Wake–Sleep algorithm (<xref ref-type="bibr" rid="bib47">Hinton et al., 1995</xref>; <xref ref-type="bibr" rid="bib14">Bornschein and Bengio, 2015</xref>) have demonstrated the potential of combining online and offline states to learn semantic representations. However, these models do not leverage offline states to improve their generative model but are explicitly trained to reproduce sensory inputs during wakefulness. In contrast, most dreams during REM sleep exhibit realistic imagery beyond past sensory experience (<xref ref-type="bibr" rid="bib32">Fosse et al., 2003</xref>; <xref ref-type="bibr" rid="bib88">Nir and Tononi, 2010</xref>; <xref ref-type="bibr" rid="bib123">Wamsley, 2014</xref>), suggesting learning principles that go beyond mere reconstructions.</p><p>In parallel, cognitive models inspired by psychological studies of sleep proposed a ‘trace transformation theory’ where semantic knowledge is actively extracted in the cortex from replayed hippocampal episodic memories (<xref ref-type="bibr" rid="bib84">Nadel and Moscovitch, 1997</xref>; <xref ref-type="bibr" rid="bib128">Winocur et al., 2010</xref>; <xref ref-type="bibr" rid="bib68">Lewis and Durrant, 2011</xref>). However, these models lack a mechanistic implementation compatible with cortical structures and only consider the replay of waking activity during sleep.</p><p>Recently, implicit generative models that do not explicitly try to reconstruct observed sensory inputs, and in particular generative adversarial networks (GANs; <xref ref-type="bibr" rid="bib38">Goodfellow et al., 2014</xref>), have been successfully applied in machine learning to generate new but realistic data from random patterns. This ability has been shown to be accompanied by the learning of disentangled and semantically meaningful representations (<xref ref-type="bibr" rid="bib94">Radford et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Donahue et al., 2016</xref>; <xref ref-type="bibr" rid="bib73">Liu et al., 2021</xref>). They thus may provide computational principles for learning cortical semantic representations during offline states by generating previously unobserved sensory content as reported from dream experiences.</p><p>Most dreams experienced during rapid eye movement (REM) sleep only incorporate fragments of previous waking experience, often intermingled with past memories (<xref ref-type="bibr" rid="bib102">Schwartz, 2003</xref>). Surprisingly, such random combinations of memory fragments often result in visual experiences that are perceived as highly structured and realistic by the dreamer. The striking similarity between the inner world of dreams and the external world of wakefulness suggests that the brain actively creates novel experiences by rearranging stored episodic patterns in a meaningful manner (<xref ref-type="bibr" rid="bib88">Nir and Tononi, 2010</xref>). A few hypothetical functions were attributed to this phenomenon, such as enhancing creative problem solving by building novel associations between unrelated memory elements (<xref ref-type="bibr" rid="bib20">Cai et al., 2009</xref>; <xref ref-type="bibr" rid="bib75">Llewellyn, 2016</xref>; <xref ref-type="bibr" rid="bib69">Lewis et al., 2018</xref>), forming internal prospective codes oriented toward future waking experiences (<xref ref-type="bibr" rid="bib74">Llewellyn, 2015</xref>), or refining a generative model by minimizing its complexity and improving generalization (<xref ref-type="bibr" rid="bib51">Hobson et al., 2014</xref>; <xref ref-type="bibr" rid="bib52">Hoel, 2021</xref>). However, these theories do not consider the role of dreams for a more basic function, such as the formation of semantic cortical representations.</p><p>Here, we propose that dreams, and in particular their creative combination of episodic memories, play an essential role in forming semantic representations over the course of development. The formation of representations that abstract away redundant information from sensory input and that can thus be easily used by downstream areas is an important basis for memory semantization. To support this hypothesis, we introduce a new, functional model of cortical representation learning. The central ingredient of our model is a creative generative process via feedback from higher to lower cortical areas that mimics dreaming during REM sleep. This generative process is trained to produce a more realistic virtual sensory experience in an adversarial fashion by trying to fool an internal mechanism distinguishing low-level activities between wakefulness and REM sleep. Intuitively, generating new but realistic sensory experiences, instead of merely reconstructing previous observations, requires the brain to understand the composition of its sensorium. In line with transformation theories, this suggests that cortical representations should carry semantic, decontextualized gist information.</p><p>We implement this model in a cortical architecture with hierarchically organized forward and backward pathways, loosely inspired by GANs. The connectivity of the model is adapted by gradient-based synaptic plasticity, optimizing different, but complementary objective functions depending on the brain’s global state. During wakefulness, the model learns to recognize that low-level activity is externally driven, stores high-level representations in the hippocampus, and tries to predict low-level from high-level activity (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). During NREM sleep, the model learns to reconstruct replayed high-level activity patterns from generated low-level activity, perturbed by virtual occlusions, referred to as perturbed dreaming (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). During REM sleep, the model learns to generate realistic low-level activity patterns from random combinations of several hippocampal memories and spontaneous cortical activity, while simultaneously learning to distinguish these virtual experiences from externally driven waking experiences, referred to as adversarial dreaming (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Together with the wakefulness, the two sleep states, NREM and REM, jointly implement our model of perturbed and adversarial dreaming (PAD).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Cortical representation learning through perturbed and adversarial dreaming (PAD).</title><p>(<bold>a</bold>) During wakefulness (Wake), cortical feedforward pathways learn to recognize that low-level activity is externally driven and feedback pathways learn to reconstruct it from high-level neuronal representations. These high-level representations are stored in the hippocampus. (<bold>b</bold>) During non-rapid eye movement sleep (NREM), feedforward pathways learn to reconstruct high-level activity patterns replayed from the hippocampus affected by low-level perturbations, referred to as perturbed dreaming. (<bold>c</bold>) During rapid eye movement sleep (REM), feedforward and feedback pathways operate in an adversarial fashion, referred to as adversarial dreaming. Feedback pathways generate virtual low-level activity from combinations of multiple hippocampal memories and spontaneous cortical activity. While feedforward pathways learn to recognize low-level activity patterns as internally generated, feedback pathways learn to fool feedforward pathways.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig1-v2.tif"/></fig><p>Over the course of learning, constrained by its architecture and the prior distribution of latent activities, our cortical model trained on natural images develops rich latent representations along with the capacity to generate plausible early sensory activities. We demonstrate that adversarial dreaming during REM sleep is essential for learning representations organized according to object semantics, which are improved and robustified by perturbed dreaming during NREM sleep. Together, our results demonstrate a potential role of dreams and suggest complementary functions of REM and NREM sleep in cortical representation learning.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Complementary objectives for wakefulness, NREM, and REM sleep</title><p>We consider an abstract model of the visual ventral pathway consisting of multiple, hierarchically organized cortical areas, with a feedforward pathway, or encoder, transforming neuronal activities from lower to higher areas (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <inline-formula><mml:math id="inf1"><mml:mi>E</mml:mi></mml:math></inline-formula>). These high-level activities are compressed representations of low-level activities and are called latent representations, here denoted by <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In addition to this feedforward pathway, we similarly model a feedback pathway, or generator, projecting from higher to lower areas (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <inline-formula><mml:math id="inf3"><mml:mi>G</mml:mi></mml:math></inline-formula>). These two pathways are supported by a simple hippocampal module that can store and replay latent representations. Three different global brain states are considered: wakefulness (Wake), non-REM sleep (NREM), and REM sleep (REM). We focus on the functional role of these phases while abstracting away dynamic features such as bursts, spindles, or slow waves (<xref ref-type="bibr" rid="bib67">Léger et al., 2018</xref>), in line with previous approaches based on goal-driven modeling that successfully predict physiological features along the ventral stream (<xref ref-type="bibr" rid="bib130">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib134">Zhuang et al., 2021</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Different objectives during wakefulness, non-rapid eye movement (NREM), and rapid eye movement (REM) sleep govern the organization of feedforward and feedback pathways in perturbed and adversarial dreaming (PAD).</title><p>The variable <inline-formula><mml:math id="inf4"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> corresponds to 32 × 32 image, <inline-formula><mml:math id="inf5"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> is a 256-dimensional vector representing the latent layer (higher sensory cortex). Encoder (<inline-formula><mml:math id="inf6"><mml:mi>E</mml:mi></mml:math></inline-formula>, green) and generator (<inline-formula><mml:math id="inf7"><mml:mi>G</mml:mi></mml:math></inline-formula>, blue) networks project bottom-up and top-down signals between lower and higher sensory areas. An oblique arrow (<inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">↗</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) indicates that learning occurs in a given pathway. (<bold>a</bold>) During Wake, low-level activities <inline-formula><mml:math id="inf9"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> are reconstructed. At the same time, <inline-formula><mml:math id="inf10"><mml:mi>E</mml:mi></mml:math></inline-formula> learns to classify low-level activity as external (red target ‘external!’) with its output discriminator <inline-formula><mml:math id="inf11"><mml:mi>d</mml:mi></mml:math></inline-formula>. The obtained latent representations <inline-formula><mml:math id="inf12"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> are stored in the hippocampus. (<bold>b</bold>) During NREM, the activity <inline-formula><mml:math id="inf13"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> stored during wakefulness is replayed from the hippocampal memory and regenerates visual input from the previous day perturbed by occlusions, modeled by squares of various sizes applied along the generated low-level activity with a certain probability (see Materials and methods). In this phase, <inline-formula><mml:math id="inf14"><mml:mi>E</mml:mi></mml:math></inline-formula> adapts to reproduce the replayed latent activity. (<bold>c</bold>) During REM, convex combinations of multiple random hippocampal memories (<inline-formula><mml:math id="inf15"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mtext>old</mml:mtext></mml:msub></mml:math></inline-formula>) and spontaneous cortical activity (<inline-formula><mml:math id="inf17"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>), here with specific prefactors, generate a virtual activity in lower areas. While the encoder learns to classify this activity as internal (red target ‘internal!’), the generator adversarially learns to generate visual inputs that would be classified as external. The red minus on <inline-formula><mml:math id="inf18"><mml:mi>G</mml:mi></mml:math></inline-formula> indicates the inverted plasticity implementing this adversarial training.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig2-v2.tif"/></fig><p>In our model, the three brain states only differ in their objective function and the presence or absence of external input. Synaptic plasticity performs stochastic gradient descent on state-specific objective functions via error backpropagation (<xref ref-type="bibr" rid="bib66">LeCun et al., 2015</xref>). We assume that efficient credit assignment is realized in the cortex and focus on the functional consequences of our specific architecture. For potential implementations of biophysically plausible backpropagation in cortical circuits, we refer to previous work (e.g., <xref ref-type="bibr" rid="bib125">Whittington and Bogacz, 2019</xref>; <xref ref-type="bibr" rid="bib71">Lillicrap et al., 2020</xref>).</p><p>During Wake (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), sensory inputs evoke activities <inline-formula><mml:math id="inf19"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> in the lower sensory cortex that are transformed via the feedforward pathway <inline-formula><mml:math id="inf20"><mml:mi>E</mml:mi></mml:math></inline-formula> into latent representations <inline-formula><mml:math id="inf21"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> in the higher sensory cortex. The hippocampal module stores these latent representations, mimicking the formation of episodic memories. Simultaneously, the feedback pathway <inline-formula><mml:math id="inf22"><mml:mi>G</mml:mi></mml:math></inline-formula> generates low-level activities <inline-formula><mml:math id="inf23"><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:math></inline-formula> from these representations. Synaptic plasticity adapts the encoding and generative pathways (<inline-formula><mml:math id="inf24"><mml:mi>E</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf25"><mml:mi>G</mml:mi></mml:math></inline-formula>) to minimize the mismatch between externally driven and internally generated activities (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Thus, the network learns to reproduce low-level activity from abstract high-level representations. Simultaneously, <inline-formula><mml:math id="inf26"><mml:mi>E</mml:mi></mml:math></inline-formula> also acts as a ‘discriminator’ with output <inline-formula><mml:math id="inf27"><mml:mi>d</mml:mi></mml:math></inline-formula> that is trained to become active, reflecting that the low-level activity was driven by an external stimuli. The discriminator learning during Wake is essential to drive adversarial learning during REM. Note that computationally the classification of low-level cortical activities into ‘externally driven’ and ‘internally generated’ is not different from classification into, for example, different object categories, even though conceptually they serve different purposes. The dual use of <inline-formula><mml:math id="inf28"><mml:mi>E</mml:mi></mml:math></inline-formula> reflects a view of cortical information processing in which several network functions are preferentially shared among a single network mimicking the ventral visual stream (<xref ref-type="bibr" rid="bib25">DiCarlo et al., 2012</xref>). This approach has been previously successfully employed in machine learning models (<xref ref-type="bibr" rid="bib53">Huang et al., 2018</xref>; <xref ref-type="bibr" rid="bib18">Brock et al., 2017</xref>; <xref ref-type="bibr" rid="bib118">Ulyanov et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Munjal et al., 2020</xref>; <xref ref-type="bibr" rid="bib7">Bang et al., 2020</xref>).</p><p>For the subsequent sleep phases, the system is disconnected from the external environment, and activity in the lower sensory cortex is driven by top-down signals originating from higher areas, as previously suggested (<xref ref-type="bibr" rid="bib88">Nir and Tononi, 2010</xref>; <xref ref-type="bibr" rid="bib4">Aru et al., 2020</xref>). During NREM (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), latent representations <inline-formula><mml:math id="inf29"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> are recalled from the hippocampal module, corresponding to the replay of episodic memories. These representations generate low-level activities that are perturbed by suppressing early sensory neurons, modeling the observed differences between replayed and waking activities (<xref ref-type="bibr" rid="bib56">Ji and Wilson, 2007</xref>). The encoder reconstructs latent representations from these activity patterns, and synaptic plasticity adjusts the feedforward pathway to make the latent representation of the perturbed generated activity similar to the original episodic memory. This process defines perturbed dreaming.</p><p>During REM (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), sleep is characterized by creative dreams generating realistic virtual sensory experiences out of the combination of episodic memories (<xref ref-type="bibr" rid="bib32">Fosse et al., 2003</xref>; <xref ref-type="bibr" rid="bib69">Lewis et al., 2018</xref>). In PAD, multiple random episodic memories from the hippocampal module are linearly combined and projected to the cortex. Reflecting the decreased coupling (<xref ref-type="bibr" rid="bib126">Wierzynski et al., 2009</xref>; <xref ref-type="bibr" rid="bib69">Lewis et al., 2018</xref>) between hippocampus and cortex during REM sleep, these mixed representations are diluted with spontaneous cortical activity, here abstracted as Gaussian noise with zero mean and unit variance. From this new high-level cortical representation, activity in the lower sensory cortex is generated and finally passed through the feedforward pathway. Synaptic plasticity adjusts feedforward connections <inline-formula><mml:math id="inf30"><mml:mi>E</mml:mi></mml:math></inline-formula> to silence the activity of the discriminator output as it should learn to distinguish it from externally evoked sensory activity. Simultaneously, feedback connections are adjusted adversarially to generate activity patterns that appear externally driven and thereby trick the discriminator into believing that the low-level activity was externally driven. This is achieved by inverting the sign of the errors that determine synaptic weight changes in the generative network. This process defines adversarial dreaming.</p><p>The functional differences between our proposed NREM and REM sleep phases are motivated by experimental data describing a reactivation of hippocampal memories during NREM sleep and the occurrence of creative dreams during REM sleep. In particular, hippocampal replay has been reported during NREM sleep within sharp-wave-ripples (<xref ref-type="bibr" rid="bib90">O’Neill et al., 2010</xref>), also observed in the visual cortex (<xref ref-type="bibr" rid="bib56">Ji and Wilson, 2007</xref>), which resembles activity from wakefulness. Our REM sleep phase is built upon cognitive theories of REM dreams (<xref ref-type="bibr" rid="bib74">Llewellyn, 2015</xref>; <xref ref-type="bibr" rid="bib69">Lewis et al., 2018</xref>) postulating that they emerge from random combinations between episodic memory elements, sometimes remote from each other, which appear realistic for the dreamer. This random coactivation could be caused by theta oscillations in the hippocampus during REM sleep (<xref ref-type="bibr" rid="bib19">Buzsáki, 2002</xref>). The addition of cortical noise is motivated by experimental work showing reduced correlations between hippocampal and cortical activity during REM sleep (<xref ref-type="bibr" rid="bib126">Wierzynski et al., 2009</xref>), and the occurrence of ponto-geniculo-occipital (PGO) waves (<xref ref-type="bibr" rid="bib86">Nelson et al., 1983</xref>) in the visual cortex often associated with the generation of novel visual imagery in dreams (<xref ref-type="bibr" rid="bib49">Hobson et al., 2000</xref>; <xref ref-type="bibr" rid="bib51">Hobson et al., 2014</xref>). Furthermore, the cortical contribution in REM dreaming is supported by experimental evidence that dreaming still occurs with hippocampal damage, while reported to be less episodic-like in nature (<xref ref-type="bibr" rid="bib110">Spanò et al., 2020</xref>).</p><p>Within our suggested framework, ‘dreams’ arise as early sensory activity that is internally generated via feedback pathways during offline states, and subsequently processed by feedforward pathways. In particular, this implies that besides REM dreams, NREM dreams exist. However, in contrast to REM dreams, which are significantly different from waking experiences (<xref ref-type="bibr" rid="bib32">Fosse et al., 2003</xref>), our model implies that NREM dreams are more similar to waking experiences since they are driven by single episodic memories, in contrast to REM dreams that are generated from a mixture of episodic memories. Furthermore, the implementation of adversarial dreaming requires an internal representation of whether early sensory activity is externally or internally generated, that is, a distinction whether a sensory experience is real or imagined.</p></sec><sec id="s2-2"><title>Dreams become more realistic over the course of learning</title><p>Dreams in our model arise from both NREM (perturbed dreaming) and REM (adversarial dreaming) phases. In both cases, they are characterized by activity in early sensory areas generated via feedback pathways. To illustrate learning in PAD, we consider these low-level activities during NREM and during REM for a model with little learning experience (‘early training’) and a model that has experienced many wake–sleep cycles (‘late training’; <xref ref-type="fig" rid="fig3">Figure 3</xref>). A single wake–sleep cycle consists of Wake, NREM, and REM phases. As an example, we train our model on a dataset of natural images (CIFAR-10; <xref ref-type="bibr" rid="bib65">Krizhevsky and Hinton, 2009</xref>) and a dataset of images of house numbers (SVHN; <xref ref-type="bibr" rid="bib87">Netzer et al., 2011</xref>). Initially, internally generated low-level activities during sleep do not share significant similarities with sensory-evoked activities from Wake (<xref ref-type="fig" rid="fig3">Figure 3a</xref>); for example, no obvious object shapes are represented (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). After plasticity has organized network connectivity over many wake–sleep cycles (50 training epochs), low-level internally generated activity patterns resemble sensory-evoked activity (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). NREM-generated activities reflect the sensory content of the episodic memory (sensory input from the previous day). REM-generated activities are different from the sensory activities corresponding to the original episodic memories underlying them as they recombine features of sensory activities from the two previous days, but still exhibit a realistic structure. This increase in similarity between externally driven and internally generated low-level activity patterns is also reflected in a decreasing Fréchet inception distance (<xref ref-type="fig" rid="fig3">Figure 3d</xref>), a metric used to quantify the realism of generated images (<xref ref-type="bibr" rid="bib46">Heusel et al., 2018</xref>). The increase of dreams realism, here mostly driven by a combination of reconstruction learning (Wake) and adversarial learning (Wake and REM), correlates with the development of dreams in children, which are initially plain and fail to represent objects, people, but become more realistic and structured over time (<xref ref-type="bibr" rid="bib33">Foulkes, 1999</xref>; <xref ref-type="bibr" rid="bib88">Nir and Tononi, 2010</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Both non-rapid eye movement (NREM) and rapid eye movement (REM) dreams become more realistic over the course of learning.</title><p>(<bold>a</bold>) Examples of sensory inputs observed during wakefulness. Their corresponding latent representations are stored in the hippocampus. (<bold>b, c</bold>) Single episodic memories (latent representations of stimuli) during NREM from the previous day and combinations of episodic memories from the two previous days during REM are recalled from hippocampus and generate early sensory activity via feedback pathways. This activity is shown for early (epoch 1) and late (epoch 50) training stages of the model. (<bold>d</bold>) Discrepancy between externally driven and internally generated early sensory activity as measured by the Fréchet inception distance (FID) (<xref ref-type="bibr" rid="bib46">Heusel et al., 2018</xref>) during NREM and REM for networks trained on CIFAR-10 (top) and SVHN (bottom). Lower distance reflects higher similarity between sensory-evoked and generated activity. Error bars indicate ±1 SEM over four different initial conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig3-v2.tif"/></fig><p>The PAD training paradigm hence leads to internally generated low-level activity patterns that become more difficult to discern from externally driven activities, whether they originate from single episodic memories during NREM or from noisy random combinations thereof during REM. We will next demonstrate that the same learning process leads to the emergence of robust semantic representations.</p></sec><sec id="s2-3"><title>Adversarial dreaming during REM facilitates the emergence of semantic representations</title><p>Semantic knowledge is fundamental for animals to learn quickly, adapt to new environments and communicate, and is hypothesized to be held by so-called semantic representations in the cortex (<xref ref-type="bibr" rid="bib25">DiCarlo et al., 2012</xref>). An example of such semantic representations are neurons from higher visual areas that contain linearly separable information about object category, invariant to other factors of variation, such as background, orientation or pose (<xref ref-type="bibr" rid="bib40">Grill-Spector et al., 2001</xref>; <xref ref-type="bibr" rid="bib54">Hung et al., 2005</xref>; <xref ref-type="bibr" rid="bib77">Majaj et al., 2015</xref>).</p><p>Here, we demonstrate that PAD, due to the specific combination of plasticity mechanisms during Wake, NREM, and REM, develops such semantic representations in higher visual areas. Similarly as in the previous section, we train our model on the CIFAR-10 and SVHN datasets. To quantify the quality of inferred latent representations, we measure how easily downstream neurons can read out object identity from these. For a simple linear readout, its classification accuracy reflects the linear separability of different contents represented in a given dataset. Technically, we train a linear classifier that distinguishes object categories based on their latent representations <inline-formula><mml:math id="inf31"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> after different numbers of wake–sleep cycles (‘epochs’, <xref ref-type="fig" rid="fig4">Figure 4a</xref>) and report its accuracy on data not used during training of the model and classifier (‘test data’). While training the classifier, the connectivity of the network (<inline-formula><mml:math id="inf32"><mml:mi>E</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:mi>G</mml:mi></mml:math></inline-formula>) is fixed.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Adversarial dreaming during rapid eye movement (REM) improves the linear separability of the latent representation.</title><p>(<bold>a</bold>) A linear classifier is trained on the latent representations <inline-formula><mml:math id="inf34"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> inferred from an external input <inline-formula><mml:math id="inf35"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> to predict its associated label (here, the category ‘car’). (<bold>b</bold>) Training phases and pathological conditions: full model (perturbed and adversarial dreaming [PAD], black), no REM phase (pink) and PAD with a REM phase using a single episodic memory only (‘w/o memory mix’, purple). (<bold>c, d</bold>) Classification accuracy obtained on test datasets (<bold>c</bold>: CIFAR-10; <bold>d</bold>: SVHN) after training the linear classifier to convergence on the latent space <inline-formula><mml:math id="inf36"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> for each epoch of the <inline-formula><mml:math id="inf37"><mml:mi>E</mml:mi></mml:math></inline-formula>-<inline-formula><mml:math id="inf38"><mml:mi>G</mml:mi></mml:math></inline-formula>-network learning. Full model (PAD): black line; without REM: pink line; with REM, but without memory mix: purple line. Solid lines represent mean, and shaded areas indicate ±1 SEM over four different initial conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig4-v2.tif"/></fig><p>The latent representation (<inline-formula><mml:math id="inf39"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula>) emerging from the trained network (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, PAD) shows increasing linear separability reaching around 59% test accuracy on CIFAR-10 (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, black line; for details, see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>) and 79% on SVHN (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, black line), comparable to less biologically plausible machine learning models (<xref ref-type="bibr" rid="bib13">Berthelot et al., 2018</xref>). These results show the ability of PAD to discover semantic concepts across wake–sleep cycles in an unsupervised fashion.</p><p>Within our computational framework, we can easily consider sleep pathologies by directly interfering with the sleep phases. To highlight the importance of REM in learning semantic representations, we consider a reduced model in which the REM phase with adversarial dreaming is suppressed and only perturbed dreaming during NREM remains (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, pink cross). Without REM sleep, linear separability increases much slower and even after a large number of epochs remains significantly below the PAD (see also <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3c and d</xref>). This suggests that adversarial dreaming during REM, here modeled by an adversarial game between feedforward and feedback pathways, is essential for the emergence of easily readable, semantic representations in the cortex. From a computational point of view, this result is in line with previous work showing that learning to generate virtual inputs via adversarial learning (GANs variants) forms better representations than simply learning to reproduce external inputs (<xref ref-type="bibr" rid="bib94">Radford et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Donahue et al., 2016</xref>; <xref ref-type="bibr" rid="bib13">Berthelot et al., 2018</xref>).</p><p>Finally, we consider a different pathology in which REM is not driven by randomly combined episodic memories and noise, but by single episodic memories without noise, as during NREM (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, purple cross). Similarly to removing REM, linear separability increases much slower across epochs, leading to worse performance of the readout (<xref ref-type="fig" rid="fig4">Figure 4c and d</xref>, purple lines). For the SVHN dataset, the performance does not reach the level of the PAD even after many wake–sleep cycles (see also <xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3d</xref>). This suggests that combining different, possibly nonrelated episodic memories, together with spontaneous cortical activity, as reported during REM dreaming (<xref ref-type="bibr" rid="bib32">Fosse et al., 2003</xref>), leads to significantly faster representation learning.</p><p>Our results suggest that generating virtual sensory inputs during REM dreaming, via a high-level combination of hippocampal memories and spontaneous cortical activity and subsequent adversarial learning, allows animals to extract semantic concepts from their sensorium. Our model provides hypotheses about the effects of REM deprivation, complementing pharmacological and optogenetic studies reporting impairments in the learning of complex rules and spatial object recognition (<xref ref-type="bibr" rid="bib15">Boyce et al., 2016</xref>). For example, our model predicts that object identity would be less easily decodable from recordings of neuronal activity in the inferior-temporal (IT) cortex in animal models with chronically impaired REM sleep.</p></sec><sec id="s2-4"><title>Perturbed dreaming during NREM improves robustness of semantic representations</title><p>Generalizing beyond previously experienced stimuli is essential for an animal’s survival. This generalization is required due to natural perturbations of sensory inputs, for example, partial occlusions, noise, or varying viewing angles. These alter the stimulation pattern, but in general should not change its latent representation subsequently used to make decisions.</p><p>Here, we model such sensory perturbations by silencing patches of neurons in early sensory areas during the stimulus presentation (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). As before, linear separability is measured via a linear classifier that has been trained on latent representations of unoccluded images and we use stimuli that were not used during training. Adding occlusions hence directly tests the out-of-distribution generalization capabilities of the learned representations. For the model trained with all phases (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, full model), the linear separability of latent representations decreases as occlusion intensity increases, until reaching chance level for fully occluded images (<xref ref-type="fig" rid="fig5">Figure 5c and d</xref>, black line).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Perturbed dreaming during non-rapid eye movement (NREM) improves robustness of latent representations.</title><p>(<bold>a</bold>) A trained linear classifier (<xref ref-type="fig" rid="fig4">Figure 4</xref>) infers class labels from latent representations. The classifier was trained on latent representations of original images, but evaluated on representations of images with varying levels of occlusion. (<bold>b</bold>) Training phases and pathological conditions: full model (perturbed and adversarial dreaming [PAD], black), without NREM phase (w/o NREM, orange). (<bold>c, d</bold>) Classification accuracy obtained on the test dataset (<bold>c</bold>: CIFAR-10; <bold>d</bold>: SVHN) after 50 epochs for different levels of occlusion (0% to 100%). Full model (PAD): black line; w/o NREM: orange line. SEM over four different initial conditions overlap with data points. Note that due to an unbalanced distribution of samples the highest performance of a naive classifier is 18.9% for the SVHN dataset.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig5-v2.tif"/></fig><p>We next consider a sleep pathology in which we suppress perturbed dreaming during the NREM phase while keeping adversarial dreaming during REM (<xref ref-type="fig" rid="fig5">Figure 5b</xref>, orange cross). Without NREM, linear separability of partially occluded images is significantly decreased for identical occlusion levels (<xref ref-type="fig" rid="fig5">Figure 5c and d</xref>; compare black and orange lines). In particular, performance degrades much faster with increasing occlusion levels. Note that despite the additional training objective the full PAD develops equally good or even better latent representations of unoccluded images (0% occlusion intensity) compared to this pathological condition without perturbed dreams.</p><p>Crucially, the perturbed dreams in NREM are generated by replaying single episodic memories. If the latent activity fed to the generator during NREM was of similar origin as during REM, that is, obtained from a convex combination of multiple episodic memories coupled with cortical spontaneous activity, the quality of the latent representations significantly decreases (see also <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>). This suggests that only replaying single memories, as hypothesized to occur during NREM sleep (<xref ref-type="bibr" rid="bib90">O’Neill et al., 2010</xref>), rather than their noisy combination, is beneficial to robustify latent representations against input perturbations.</p><p>This robustification originates from the training objective defined in the NREM phase, forcing feedforward pathways to map perturbed inputs to the latent representation corresponding to their clean, nonoccluded version. This procedure is reminiscent of a regularization technique from machine learning called ‘data augmentation’ (<xref ref-type="bibr" rid="bib105">Shorten and Khoshgoftaar, 2019</xref>), which increases the amount of training data by adding stochastic perturbations to each input sample. However, in contrast to data augmentation methods that directly operate on samples, here the system autonomously generates augmented data in offline states, preventing interference with online cognition and avoiding storage of the original samples. Our ‘dream augmentation’ suggests that NREM hippocampal replay not only maintains or strengthens cortical memories, as traditionally suggested (<xref ref-type="bibr" rid="bib63">Klinzing et al., 2019</xref>), but also improves latent representations when only partial information is available. For example, our model predicts that animals lacking such dream augmentation, potentially due to impaired NREM sleep, fail to react reliably to partially occluded stimuli even though their responses to clean stimuli are accurate.</p></sec><sec id="s2-5"><title>Latent organization in healthy and pathological models</title><p>The results so far demonstrate that PAD, during REM and NREM sleep states, contributes to cortical representation learning by increasing the linear separability of latent representations into object classes. We next investigate how the learned latent space is organized, that is, whether representations of sensory inputs with similar semantic content are grouped together even if their low-level structure may be quite different, for example, due to different viewing angles, variations among an object category, or (partial) occlusions.</p><p>We illustrate the latent organization by projecting the latent variable <inline-formula><mml:math id="inf40"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> using principal component analysis (PCA, <xref ref-type="fig" rid="fig6">Figure 6a</xref>, <xref ref-type="bibr" rid="bib57">Jolliffe and Cadima, 2016</xref>). This method is well-suited for visualizing high-dimensional data in a low-dimensional space while preserving as much of the data’s variation as possible.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Effects of non-rapid eye movement (NREM) and rapid eye movement (REM) sleep on latent representations.</title><p>(<bold>a</bold>) Inputs <inline-formula><mml:math id="inf41"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> are mapped to their corresponding latent representations <inline-formula><mml:math id="inf42"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> via the encoder <inline-formula><mml:math id="inf43"><mml:mi>E</mml:mi></mml:math></inline-formula>. Principal component analysis (PCA; <xref ref-type="bibr" rid="bib57">Jolliffe and Cadima, 2016</xref>) is performed on the latent space to visualize its structure (<bold>b–d</bold>). Clustering distances (<bold>e, f</bold>) are computed directly on latent features <inline-formula><mml:math id="inf44"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula>. (<bold>b–d</bold>) PCA visualization of latent representations projected on the first two principal components. Full circles represent clean images, open circles represent images with 30% occlusion. Each color represents an object category from the SVHN dataset (purple: ‘0’; cyan: ‘1’; yellow: ‘2’; red: ‘3’). (<bold>e</bold>) Ratio between average intra-class and average inter-class distances in latent space for randomly initialized networks (no training, gray), full model (black), model trained without REM sleep (w/o REM, pink), and model trained without NREM sleep (w/o NREM, orange) for unoccluded inputs. (<bold>f</bold>) Ratio between average clean-occluded (30% occlusion) and average inter-class distances in latent space for the full model (black), w/o REM (pink), and w/o NREM (orange). Error bars represent SEM over four different initial conditions.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig6-v2.tif"/></fig><p>For PAD, the obtained PCA projection shows relatively distinct clusters of latent representations according to the semantic category (‘class identity’) of their corresponding images (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). The model thus tends to organize latent representations such that high-level, semantic clusters are discernible. Furthermore, partially occluded objects (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, empty circles) are represented close by their corresponding unoccluded version (<xref ref-type="fig" rid="fig6">Figure 6b</xref>, full circles).</p><p>As shown in the previous sections, removing either REM or NREM has a negative impact on the linear separability of sensory inputs. However, the reasons for these effects are different between REM and NREM. If REM sleep is removed from training, representations of unoccluded images are less organized according to their semantic category, but still match their corresponding occluded versions (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). REM is thus necessary to organize latent representations into semantic clusters, providing an easily readable representation for downstream neurons. In contrast, removing NREM causes representations of occluded inputs to be remote from their unoccluded representations (<xref ref-type="fig" rid="fig6">Figure 6d</xref>).</p><p>We quantify these observations by computing the average distances between latent representations from the same object category (intra-class distance) and between representations of different object category (inter-class distance). Since the absolute distances are difficult to interpret, we focus on their ratio (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). On both datasets, this ratio increases if the REM phase is removed from training (<xref ref-type="fig" rid="fig6">Figure 6e</xref>, compare black and pink bars), reaching levels comparable to the one with the untrained network. Moreover, removing NREM from training also increases this ratio. These observations suggest that both PAD jointly reorganize the latent space such that stimuli with a similar semantic structure are mapped to similar latent representations. In addition, we compute the distance between the latent representations inferred from clean images and their corresponding occluded versions, also divided by the inter-class distance (<xref ref-type="fig" rid="fig6">Figure 6f</xref>). By removing NREM from training, this ratio increases significantly, highlighting the importance of NREM in making latent representations invariant to input perturbations.</p></sec><sec id="s2-6"><title>Cortical implementation of PAD</title><p>We have shown that PAD can learn semantic cortical representations useful for downstream tasks. Here, we hypothesize how the associated mechanisms may be implemented in the cortex.</p><p>First, PAD implies the existence of discriminator neurons that would learn to be differentially active during wakefulness and REM sleep. It also postulates a conductor that orchestrates learning by providing a teaching (‘nudging’) signal to the discriminator neurons during Wake and REM. Experimental evidence suggests that discriminator neurons, differentiating between internally generated end externally driven sensory activity, may reside in the anterior cingulate cortex or the medial prefrontal cortex, but functionally similar neurons may be located across cortex to deliver local learning signals (<xref ref-type="bibr" rid="bib111">Subramaniam et al., 2012</xref>; <xref ref-type="bibr" rid="bib108">Simons et al., 2017</xref>; <xref ref-type="bibr" rid="bib34">Gershman, 2019</xref>; <xref ref-type="bibr" rid="bib10">Benjamin and Kording, 2021</xref>).</p><p>Second, learning in PAD is orchestrated across three different phases: (1) learning stimulus reconstruction during Wake, (2) learning latent variable reconstruction during NREM sleep (’perturbed dreaming’), and (3) learning to generate realistic sensory activity during REM sleep (’adversarial dreaming’). Our model suggests that objective functions and synaptic plasticity are affected by these phases (<xref ref-type="fig" rid="fig7">Figure 7</xref>). Wakefulness is associated with increased activity of modulatory brainstem neurons releasing neuromodulators such as acetylcholine (ACh) and noradrenaline (NA), hypothesized to prioritize the amplification of information from external stimuli (<xref ref-type="bibr" rid="bib1">Adamantidis et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Aru et al., 2020</xref>). In contrast, neuromodulator concentrations during NREM are reduced compared to Wake, while REM is characterized by high ACh and low NA levels (<xref ref-type="bibr" rid="bib50">Hobson, 2009</xref>). We postulate that the state-specific modulation provides a high activity target for the discriminator during Wake, which is decreased during REM and entirely gated off during NREM. Furthermore, we suggest that adversarial learning is implemented by a sign-switched plasticity in the generative network during REM sleep, with respect to Wake. During wakefulness, plasticity in these apical synapses may be enhanced by NA as opposed to NREM (<xref ref-type="bibr" rid="bib1">Adamantidis et al., 2019</xref>; <xref ref-type="bibr" rid="bib4">Aru et al., 2020</xref>). The presence of ACh alone during REM (<xref ref-type="bibr" rid="bib49">Hobson et al., 2000</xref>) may switch the sign of plasticity in apical synapses of (hippocampal) pyramidal neurons (<xref ref-type="bibr" rid="bib81">McKay et al., 2007</xref>). Furthermore, it is known that somato-dendritic synchrony is reduced in REM versus NREM sleep (<xref ref-type="bibr" rid="bib103">Seibt et al., 2017</xref>); this suggests a reduced somato-dendritic backpropagation of action potentials, which, in turn, is known to switch the sign of apical plasticity (<xref ref-type="bibr" rid="bib109">Sjöström and Häusser, 2006</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Model features and physiological counterparts during Wake, non-rapid eye movement (NREM), and rapid eye movement (REM) phases.</title><p>ACh: acetylcholine; NA: noradrenaline. ‘Sign switch’ indicates that identical local errors lead to opposing weight changes between Wake and REM sleep.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig7-v2.tif"/></fig><p>Third, learning in our model requires the computation of reconstruction errors, that is, mismatches between top-down and bottom-up activity. So far, two nonexclusive candidates for computing mismatch signals have been proposed. One suggests a dendritic error representation in layer 5 pyramidal neurons that compare bottom-up with top-down inputs from our encoding (<inline-formula><mml:math id="inf45"><mml:mi>E</mml:mi></mml:math></inline-formula>) and generative (<inline-formula><mml:math id="inf46"><mml:mi>G</mml:mi></mml:math></inline-formula>) pathways (<xref ref-type="bibr" rid="bib41">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="bib99">Sacramento et al., 2018</xref>). The other suggests an explicit mismatch representation by subclasses of layer 2/3 pyramidal neurons (<xref ref-type="bibr" rid="bib60">Keller and Mrsic-Flogel, 2018</xref>).</p><p>Fourth, our computational framework assumes effectively separate feedforward and feedback streams. A functional separation of these streams does not necessarily imply a structural separation at the network level. Indeed, such cross-projections are observed in experimental data (<xref ref-type="bibr" rid="bib36">Gilbert and Li, 2013</xref>) and also used in, for example, the predictive processing framework (<xref ref-type="bibr" rid="bib96">Rao and Ballard, 1999</xref>). In our model, an effective separation of the information flows is required to prevent ‘information shortcuts’ across early sensory cortices that would prevent learning of good representations in higher sensory areas. This suggests that for significant periods of time intra-areal lateral interactions between cortical feedforward and feedback pathways are effectively gated off in most of the areas.</p><p>Fifth, similar to previous work (<xref ref-type="bibr" rid="bib58">Káli and Dayan, 2004</xref>), the hippocampus is not explicitly modeled but rather mimicked by a buffer allowing simple store and retrieve operations. An extension of our model could replace this simple mechanism with attractor networks that have been previously employed to model hippocampal function (<xref ref-type="bibr" rid="bib113">Tang et al., 2010</xref>). The combination of episodic memories underlying REM dreams in our model could either occur in the hippocampus or in the cortex. In either case, we would predict a nearly simultaneous activation of different episodic memories in the hippocampus that results in the generation of creative virtual early cortical activity.</p><p>Finally, beyond the mechanisms discussed above, our model assumes that cortical circuits can efficiently perform credit assignment, similar to the classical error backpropagation algorithm. Most biologically plausible implementations for error backpropagation involve feedback connections to deliver error signals (<xref ref-type="bibr" rid="bib125">Whittington and Bogacz, 2019</xref>; <xref ref-type="bibr" rid="bib98">Richards et al., 2019</xref>; <xref ref-type="bibr" rid="bib71">Lillicrap et al., 2020</xref>), for example, to the apical dendrites of pyramidal neurons (<xref ref-type="bibr" rid="bib99">Sacramento et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Guerguiev et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Haider et al., 2021</xref>). An implementation of our model in such a framework would hence require additional feedforward and feedback connections for each neuron. For example, neurons in the feedforward pathway would not only project to higher cortical areas to transmit signals, but additionally project back to earlier areas to allow these to compute the local errors required for effective learning. Overall, our proposed model could be mechanistically implemented in cortical networks through different classes of pyramidal neurons with a biological version of supervised learning based on a dendritic prediction of somatic activity (<xref ref-type="bibr" rid="bib119">Urbanczik and Senn, 2014</xref>), and a corresponding global modulation of synaptic plasticity by state-specific neuromodulators.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Semantic representations in cortical networks emerge in early life despite most observations lacking an explicit class label, and sleep has been hypothesized to facilitate this process (<xref ref-type="bibr" rid="bib63">Klinzing et al., 2019</xref>). However, the role of dreams in cortical representation learning remains unclear. Here, we proposed that creating virtual sensory experiences by randomly combining episodic memories during REM sleep lies at the heart of cortical representation learning. Based on a functional cortical architecture, we introduced the PAD model and demonstrated that REM sleep can implement an adversarial learning process that, constrained by the network architecture and the choice of latent prior distributions, builds semantically organized latent representations. Additionally, perturbed dreaming based on the episodic memory replay during NREM stabilizes the cortical representations against sensory perturbations. Our computational framework allowed us to investigate the effects of specific sleep-related pathologies on cortical representations. Together, our results demonstrate complementary effects of perturbed dreaming from individual episodes during NREM and adversarial dreaming from mixed episodes during REM. PAD suggests that the generalization abilities exhibited by humans and other animals arise from distinct processes during the two sleep phases: REM dreams organize representations semantically and NREM dreams stabilize these representations against perturbations. Finally, the model suggests how adversarial learning inspired by GANs can potentially be implemented by cortical circuits and associated plasticity mechanisms.</p><sec id="s3-1"><title>Relation to cognitive theories of sleep</title><p>PAD focuses on the functional role of sleep, and in particular dreams. Many dynamic features of brain states during NREM and REM sleep, such as cortical oscillations (<xref ref-type="bibr" rid="bib67">Léger et al., 2018</xref>), are hence ignored here but will potentially become relevant when constructing detailed circuit models of the suggested architectures, for example, for switching between memories (<xref ref-type="bibr" rid="bib64">Korcsak-Gorzo et al., 2021</xref>). Our proposed model of sleep is complementary to theories suggesting that sleep is important for physiological and cognitive maintenance (<xref ref-type="bibr" rid="bib80">McClelland et al., 1995</xref>; <xref ref-type="bibr" rid="bib58">Káli and Dayan, 2004</xref>; <xref ref-type="bibr" rid="bib97">Rennó-Costa et al., 2019</xref>; <xref ref-type="bibr" rid="bib120">van de Ven et al., 2020</xref>). In particular, <xref ref-type="bibr" rid="bib89">Norman et al., 2005</xref> proposed a model where autonomous reactivation of memories (from cortex and hippocampus) coupled with oscillating inhibition during REM sleep helps detect weak parts of memories and selectively strengthen them to overcome catastrophic forgetting. While our REM phase serves different purposes, an interesting commonality is the view of REM as a period where the cortex ‘thinks about what it already knows’ from past and recent memories and reorganizes its representations by replaying them together, as opposed to NREM where only recent memories are replayed and consolidated. Recent work has also suggested that the brain learns using adversarial principles either as a reality monitoring mechanism potentially explaining delusions in some mental disorders (<xref ref-type="bibr" rid="bib34">Gershman, 2019</xref>), in the context of dreams to overcome overfitting and promote generalization (<xref ref-type="bibr" rid="bib52">Hoel, 2021</xref>), or for learning inference in recurrent biological networks (<xref ref-type="bibr" rid="bib10">Benjamin and Kording, 2021</xref>).</p><p>Cognitive theories propose that sleep promotes the abstraction of semantic concepts from episodic memories through a hippocampo-cortical replay of waking experiences, referred to as ‘memory semantization’ (<xref ref-type="bibr" rid="bib84">Nadel and Moscovitch, 1997</xref>; <xref ref-type="bibr" rid="bib68">Lewis and Durrant, 2011</xref>). The learning of organized representations is an important basis for semantization. An extension of our model would consider the influence of different sensory modalities on representation learning (<xref ref-type="bibr" rid="bib43">Guo et al., 2019</xref>), which is known to significantly influence cortical schemas (<xref ref-type="bibr" rid="bib69">Lewis et al., 2018</xref>) and can encourage the formation of computationally powerful representations (<xref ref-type="bibr" rid="bib95">Radford et al., 2021</xref>).</p><p>Finally, sleep has previously been considered as a state where ‘noisy’ connections acquired during wakefulness are selectively forgotten (<xref ref-type="bibr" rid="bib23">Crick and Mitchison, 1983</xref>; <xref ref-type="bibr" rid="bib92">Poe, 2017</xref>), or similarly, as a homeostatic process to desaturate learning and renormalize synaptic strength (synaptic homeostasis hypothesis; <xref ref-type="bibr" rid="bib115">Tononi and Cirelli, 2014</xref>; <xref ref-type="bibr" rid="bib116">Tononi and Cirelli, 2020</xref>). In contrast, our model offers an additional interpretation of plasticity during sleep, where synapses are globally readapted to satisfy different but complementary learning objectives than Wake, either by improving feedforward recognition of perturbed inputs (NREM) or by adversarially tuning top-down generation (REM).</p></sec><sec id="s3-2"><title>Relation to representation learning models</title><p>Recent advances in machine learning, such as self-supervised learning approaches, have provided powerful techniques to extract semantic information from complex datasets (<xref ref-type="bibr" rid="bib73">Liu et al., 2021</xref>). Here, we mainly took inspiration from self-supervised generative models combining autoencoder and adversarial learning approaches (<xref ref-type="bibr" rid="bib94">Radford et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Donahue et al., 2016</xref>; <xref ref-type="bibr" rid="bib30">Dumoulin et al., 2017</xref>; <xref ref-type="bibr" rid="bib13">Berthelot et al., 2018</xref>; <xref ref-type="bibr" rid="bib73">Liu et al., 2021</xref>). It is theoretically not yet fully understood how linearly separable representations are learned from objectives that do not explicitly encourage them, that is, reconstruction and adversarial losses. We hypothesize that the presence of architectural constraints and latent priors, in combination with our objectives, enables their emergence (see also <xref ref-type="bibr" rid="bib2">Alemi et al., 2018</xref>; <xref ref-type="bibr" rid="bib117">Tschannen et al., 2020</xref>). Note that similar generative machine learning models often report a higher linear separability of network representations, but use all convolutional layers as a basis for the readout (<xref ref-type="bibr" rid="bib94">Radford et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Dumoulin et al., 2017</xref>), while we only used low-dimensional features <inline-formula><mml:math id="inf47"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula>. Approaches similar to ours, that is, those that perform classification only on the latent features, report comparable performance to ours (<xref ref-type="bibr" rid="bib13">Berthelot et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Hjelm et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Beckham et al., 2019</xref>).</p><p>Furthermore, in contrast to previous GAN variants, our model removes many optimization tricks such as batch normalization layers (<xref ref-type="bibr" rid="bib55">Ioffe and Szegedy, 2015</xref>), spectral normalization layers (<xref ref-type="bibr" rid="bib82">Miyato et al., 2018</xref>), or optimizing the min-max GAN objective in three steps with different objectives, which are challenging to implement in biological substrates. Despite their absence, our model maintains a high quality of latent representations. As our model is relatively simple, it is amenable to implementations within frameworks approximating backpropagation in the brain (<xref ref-type="bibr" rid="bib125">Whittington and Bogacz, 2019</xref>; <xref ref-type="bibr" rid="bib98">Richards et al., 2019</xref>; <xref ref-type="bibr" rid="bib71">Lillicrap et al., 2020</xref>). However, some components remain challenging for implementations in biological substrates, for example, convolutional layers (but see <xref ref-type="bibr" rid="bib93">Pogodin et al., 2021</xref>) and batched training (but see <xref ref-type="bibr" rid="bib79">Marblestone et al., 2016</xref>).</p></sec><sec id="s3-3"><title>Dream augmentations, mixing strategies, and fine-tuning</title><p>To make representations robust, a computational strategy consists of learning to map different sensory inputs containing the same object to the same latent representation, a procedure reminiscent of data augmentation (<xref ref-type="bibr" rid="bib105">Shorten and Khoshgoftaar, 2019</xref>). As mentioned above, unlike standard data augmentation methods, our NREM phase does not require the storage of raw sensory inputs to create altered inputs necessary for such data augmentation and instead relies on (hippocampal) replay being able to regenerate similar inputs from high-level representations stored during wakefulness. Our results obtained through perturbed dreaming during NREM provide initial evidence that this dream augmentation may robustify cortical representations.</p><p>Furthermore, as discussed above, introducing more specific modifications of the replayed activity, for example, mimicking translations or rotations of objects, coupled with a negative phase where latent representations from different images are pushed apart, may further contribute to the formation of invariant representations. Along this line, recent self-supervised contrastive learning methods (<xref ref-type="bibr" rid="bib35">Gidaris et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Chen et al., 2020</xref>; <xref ref-type="bibr" rid="bib132">Zbontar et al., 2021</xref>) have been shown to enhance the semantic structure of latent representations by using a similarity objective where representations of stimuli under different views are pulled together in a first phase, while, crucially, embedding distances between unrelated images are increased in a second phase.</p><p>In our REM phase, different mixing strategies in the latent layer could be considered. For instance, latent activities could be mixed up by retaining some vector components of a representation and using the rest from a second one (<xref ref-type="bibr" rid="bib8">Beckham et al., 2019</xref>). Moreover, more than two memory representations could have been used. Alternatively, our model could be trained with spontaneous cortical activity only. In our experimental setting, we do not observe significant differences between using a combination of episodic memories with spontaneous activity or only using spontaneous activity (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). However, we hypothesize that for models that learn continuously, a preferential replay of combinations of recent episodic memories encourages the formation of cortical representations that are useful in the present.</p><p>Here, we used a simple linear classifier to measure the quality of latent representations, which is an obvious simplification with regard to cortical processing. Note, however, that also for more complex ‘readouts’ organized latent representations enable more efficient and faster learning (<xref ref-type="bibr" rid="bib107">Silver et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Ha and Schmidhuber, 2018</xref>; <xref ref-type="bibr" rid="bib101">Schrittwieser et al., 2020</xref>). In its current form, PAD assumes that training the linear readout does not lead to weight changes in the encoder network. However, in cortical networks, cognitive or motor tasks leveraging latent representations likely shape the encoder network, which could in our model be reflected in ‘fine-tuning’ the encoder for specific tasks (compare <xref ref-type="bibr" rid="bib73">Liu et al., 2021</xref>).</p><p>Finally, our model does not show significant differences in performance when the order of sleep phases is switched (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>). However, NREM and REM are observed to occur in a specific order throughout the night (<xref ref-type="bibr" rid="bib26">Diekelmann and Born, 2010</xref>), and this order has been hypothesized to be important for memory consolidation (‘sequential hypothesis,’ <xref ref-type="bibr" rid="bib37">Giuditta et al., 1995</xref>). The independence of phases in our model may be due to the relatively small synaptic changes occurring in each phase. We expect the order of sleep phases to influence model performance if these changes become larger either due to longer phases or increased learning rates. The latter may become particularly relevant in continual learning settings where it becomes important to control the emphasis put on recent observations.</p></sec><sec id="s3-4"><title>Signatures of generative learning</title><p>PAD makes several experimentally testable predictions at the neuronal and systems level. We first address generally whether the brain learns via generative models during sleep before discussing specific signatures of adversarial learning.</p><p>First, our NREM phase assumes that hippocampal replay generates perturbed wake-like early sensory activity (see also <xref ref-type="bibr" rid="bib56">Ji and Wilson, 2007</xref>), which is subsequently processed by feedforward pathways. Moreover, our model predicts that over the course of learning sensory-evoked neuronal activity and internally generated activity during sleep become more similar. In particular, we predict that (spatial) activity in both NREM and REM become more similar to Wake; however, patterns observed during REM remain distinctly different due to the creative combination of episodic memories. Future experimental studies could confirm these hypotheses by recording early sensory activity during wakefulness, NREM, and REM sleep at different developmental stages and evaluating commonalities and differences between activity patterns. Previous work has already demonstrated increasing similarity between stimulus-evoked and spontaneous (generated) activity patterns during wakefulness in the ferret visual cortex (<xref ref-type="bibr" rid="bib12">Berkes et al., 2011</xref>; but see <xref ref-type="bibr" rid="bib5">Avitan et al., 2021</xref>).</p><p>On a behavioral level, the improvement of internally generated activity patterns correlates with the development of dreams in children, which are initially unstructured, simple, and plain, and gradually become full-fledged, meaningful, narrative, implicating known characters and reflecting life episodes (<xref ref-type="bibr" rid="bib88">Nir and Tononi, 2010</xref>). In spite of their increase in realism, REM dreams in adulthood are still reported as bizarre (<xref ref-type="bibr" rid="bib127">Williams et al., 1992</xref>). Bizarre dreams, such as ‘flying dogs,’ are typically defined as discontinuities or incongruities of the sensory experience (<xref ref-type="bibr" rid="bib78">Mamelak and Hobson, 1989</xref>) rather than completely structureless experiences. This definition hence focuses on high-level logical structure, not on the low-level sensory content. In contrast, the low FID score, that is, high realism, of REM dreams in our experiments reflects that the low-level structure on which this evaluation metric mainly focuses (e.g., <xref ref-type="bibr" rid="bib17">Brendel and Bethge, 2019</xref>) is similar to actual sensory input. Capturing the ‘logical realism’ of our generated neuronal activities most likely requires a more sophisticated evaluation metric and an extension of the model capable of generating temporal sequences of sensory stimulation. We note, however, that even such surreal dreams as ‘flying dogs’ can be interpreted as altered combinations of episodic memories and thus, in principle, can arise from our model.</p><p>Second, our model suggests that the development of semantic representations is mainly driven by REM sleep. This allows us to make predictions that connect the network with the systems level, in the specific case of acquiring skills from complex and unfamiliar sensory input. For humans, this could be learning a foreign language with unfamiliar phonetics. Initially, cortical representations cannot reflect relevant nuances in these sounds. Phonetic representations develop gradually over experience and are reflected in changes of the sensory-evoked latent activity, specifically in the reallocation of neuronal resources to represent the relevant latent dimensions. We hypothesize that in the case of impaired REM sleep this change of latent representations is significantly reduced, which goes hand in hand with decreased learning speed. Future experimental studies could investigate these effects, for instance, by trying to decode sound identity from high-level cortical areas in patients where REM sleep is impaired over long periods through pharmacological agents such as antidepressants (<xref ref-type="bibr" rid="bib16">Boyce et al., 2017</xref>). An equivalent task in the nonhuman animal domain would be song acquisition in songbirds (<xref ref-type="bibr" rid="bib31">Fiete et al., 2007</xref>). On a neuronal level, one could selectively silence feedback pathways during REM sleep in animal models over many nights, for example, via optogenetic tools. Our model predicts that this silencing would significantly impact the animal’s learning speed, as reported from animals with reduced theta rhythm during REM sleep (<xref ref-type="bibr" rid="bib16">Boyce et al., 2017</xref>).</p></sec><sec id="s3-5"><title>Signatures of adversarial learning</title><p>The experimental predictions discussed above mainly address whether the brain learns via generative models during sleep. Here, we make experimental predictions that would support our hypotheses and contrast them to alternative theories of learning during offline states.</p><sec id="s3-5-1"><title>Existence of an external/internal discriminator</title><p>The discriminator provides our model with the ability to distinguish externally driven from internally generated low-level cortical activity. Due to this unique property, the discriminator may be leveraged to distinguish actual from imagined sensations. According to our model, reduced REM sleep would lead to an impaired discriminator, and could thus result in an inability of subjects to realize that self-generated imagery is not part of the external sensorium. This may result in the formation of delusions, as previously suggested (<xref ref-type="bibr" rid="bib34">Gershman, 2019</xref>). For instance, hallucinations in schizophrenic patients, often mistaken for veridical perceptions (<xref ref-type="bibr" rid="bib124">Waters et al., 2016</xref>), could be partially caused by abnormal REM sleep patterns, related to observed reduced REM latency and density (<xref ref-type="bibr" rid="bib22">Cohrs, 2008</xref>). Based on these observations, we predict in the context of our model a negative correlation between REM sleep quality and delusional perceptions of hallucinations. Systematic differences in REM sleep quality may hence explain why some patients are able to recognize that their hallucinations are self-generated while some others mistake them to be real. Moreover, although locating discriminator neurons may prove nontrivial (but see ‘Cortical implementation of PAD’ for specific suggestions), we predict that once the relevant cells have been identified, perturbing them may lead to detrimental effects on differentiating between external sensory inputs and internally generated percepts.</p><p>The state-specific activity of the discriminator population makes predictions about plasticity on synapses in the feedforward stream during wakefulness and sleep. In our model, the discriminator is trained to distinguish externally from internally generated patterns by opposed targets imposed during Wake and REM. After many wake–sleep cycles, the KL loss as well as the reconstruction loss (see Materials and methods) in our model become small compared to the adversarial loss (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>), which remains nonzero due to a balance between discriminator and generator. The same low-level activity pattern would hence cause opposite weight changes during wakefulness and sleep on feedforward synapses. This could be tested experimentally by actively instantiating similar spatial activity patterns in low-level sensory cortex during wakefulness and REM and comparing the statistics of observed changes in (feedforward) downstream synapses.</p></sec><sec id="s3-5-2"><title>Adversarial training of a generator during sleep</title><p>To drive adversarial learning and maintain a balance between the generator and discriminator, the generative network must be trained in parallel to the discriminative (encoder) network during REM. In contrast, in alternative representation learning models that involve offline states such as the Wake–Sleep algorithm (<xref ref-type="bibr" rid="bib47">Hinton et al., 1995</xref>), generative pathways are not trained to produce realistic dreams during the sleep phase. Rather, they are trained by reconstruction on real input data during the wake phase. This allows an experimental distinction between our model and Wake–Sleep-like models: while our model predicts plasticity in both bottom-up and top-down pathways both during wake and during REM sleep, Wake–Sleep models alternate between training feedback and feedforward connections during online and offline states, respectively.</p><p>Previous work has developed methods to infer plasticity rules from neuronal activity (<xref ref-type="bibr" rid="bib72">Lim et al., 2015</xref>; <xref ref-type="bibr" rid="bib104">Senn and Sacramento, 2015</xref>) or weight changes (<xref ref-type="bibr" rid="bib85">Nayebi et al., 2020</xref>). In the spirit of existing in vivo experiments, we suggest to optogenetically monitor and potentially modulate apical dendritic activities in cortical pyramidal neurons of mice during wakefulness and REM sleep (<xref ref-type="bibr" rid="bib70">Li et al., 2017</xref>; <xref ref-type="bibr" rid="bib121">Voigts and Harnett, 2020</xref>; <xref ref-type="bibr" rid="bib100">Schoenfeld et al., 2022</xref>). From the statistics of the recorded dendritic and neuronal activity, the plasticity rules could be inferred and compared to the state-dependent rules suggested by our model, particularly to the predicted sign switch of plasticity between wakefulness and REM sleep.</p></sec><sec id="s3-5-3"><title>Adversarial learning and creativity</title><p>Adversarial learning, for example, in GANs, enables a form of creativity, reflected in their ability to generate realistic new data or to create semantically meaningful interpolations (<xref ref-type="bibr" rid="bib94">Radford et al., 2015</xref>; <xref ref-type="bibr" rid="bib13">Berthelot et al., 2018</xref>; <xref ref-type="bibr" rid="bib59">Karras et al., 2018</xref>). This creativity might be partly caused by the freedom in generating sensory activity that is not restricted by requiring good reconstructions, but is only guided by the internal/external judgment (<xref ref-type="bibr" rid="bib39">Goodfellow, 2016</xref>). This is less constraining on the generator than direct reconstruction losses used in alternative models such as variational autoencoders (<xref ref-type="bibr" rid="bib61">Kingma and Welling, 2013</xref>) or the Wake–Sleep algorithm (<xref ref-type="bibr" rid="bib47">Hinton et al., 1995</xref>). We thus predict that REM sleep, here implementing adversarial learning, should boost creativity, as previously reported (<xref ref-type="bibr" rid="bib20">Cai et al., 2009</xref>; <xref ref-type="bibr" rid="bib75">Llewellyn, 2016</xref>; <xref ref-type="bibr" rid="bib69">Lewis et al., 2018</xref>). Furthermore, we predict that REM sleep influences a subject’s ability to visualize creative mental images, for instance, associating nonobvious visual patterns with distinct memories. For example, we predict that participants chronically lacking REM sleep would perform worse than control participants at a creative synthesis task (<xref ref-type="bibr" rid="bib91">Palmiero et al., 2015</xref>), consisting of combining different visual components into a new, potentially useful object.</p></sec><sec id="s3-5-4"><title>Adversarial learning and lucid dreaming</title><p>Finally, adversarial dreaming offers a theoretical framework to investigate neuronal correlates of normal versus lucid dreaming (<xref ref-type="bibr" rid="bib28">Dresler et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Baird et al., 2019</xref>). While in normal dreaming the internally generated activity is perceived as externally caused, in lucid dreaming it is perceived as what it is, that is, internally generated. We hypothesize that the ‘neuronal conductor’ that orchestrates adversarial dreaming is also involved in lucid dreaming by providing to the dreamer conscious access to the target ‘internal’ that the conductor imposes during REM sleep. Our cortical implementation suggests that the neuronal conductor could gate the discriminator teaching via the apical activity of cortical pyramidal neurons. The same apical dendrites were also speculated to be involved in conscious perception (<xref ref-type="bibr" rid="bib112">Takahashi et al., 2020</xref>), dreaming (<xref ref-type="bibr" rid="bib4">Aru et al., 2020</xref>), and in representing the state and content of consciousness (<xref ref-type="bibr" rid="bib3">Aru et al., 2019</xref>).</p><p>Our model demonstrates that adversarial learning during wakefulness and sleep can provide significant benefits to extract semantic concepts from sensory experience. By bringing insights from modern artificial intelligence to cognitive theories of sleep function, we suggest that cortical representation learning during dreaming is a creative process, orchestrated by brain-state-regulated adversarial games between separated feedforward and feedback streams. Adversarial dreaming may further be helpful to understand learning beyond the standard student–teacher paradigm. By ‘seeing’ the world from new perspectives every night, dreaming represents an active learning phenomenon, constantly improving our understanding, our creativity, and our awareness.</p></sec></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Network architecture</title><p>The network consists of two separate pathways, mapping from the pixel to the latent space (‘encoder’/‘discriminator’) and from the latent to pixel space (‘generator’). Encoder/discriminator and generator architectures follow a similar structure as the Deep Convolutional Generative Adversarial Networks (DCGANs) model (<xref ref-type="bibr" rid="bib94">Radford et al., 2015</xref>). The encoder <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula> has four convolutional layers (<xref ref-type="bibr" rid="bib66">LeCun et al., 2015</xref>) containing 64, 128, 256, and 256 channels, respectively (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Convolutional neural network (CNN) architecture of encoder/discriminator and generator used in perturbed and adversarial dreaming (PAD).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig8-v2.tif"/></fig><p>Each layer uses a 4 × 4 kernel, a padding of 1 (0 for last layer), and a stride of 2, that is, feature size is halved in each layer. All convolutional layers except the last one are followed by a LeakyReLU nonlinearity (<xref ref-type="bibr" rid="bib76">Maas et al., 2013</xref>). We denote the activity in the last convolutional layer as <inline-formula><mml:math id="inf49"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula>. An additional convolutional layer followed by a sigmoid nonlinearity is added on top of the second-to-last layer of the encoder and maps to a single scalar value <inline-formula><mml:math id="inf50"><mml:mi>d</mml:mi></mml:math></inline-formula>, the internal/external discrimination (with putative teaching signal 0 or 1). We denote the mapping from <inline-formula><mml:math id="inf51"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf52"><mml:mi>d</mml:mi></mml:math></inline-formula> by <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf55"><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> thus share the first three convolutional layers. We jointly denote them by <inline-formula><mml:math id="inf56"><mml:mi>E</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf57"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>,</mml:mo><mml:mi>d</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><p>Mirroring the structure of <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula>, the generator <inline-formula><mml:math id="inf59"><mml:mi>G</mml:mi></mml:math></inline-formula> has four deconvolutional layers containing 256, 128, 64, and 3 channels. They all use a 4 × 4 kernel, a padding of 1 (0 for first deconvolutional layer), and a stride of 2, that is, the feature size is doubled in each layer. The first three deconvolutional layers are followed by a LeakyReLU nonlinearity, and the last one by a tanh nonlinearity.</p><p>As a detailed hippocampus model is outside the scope of this study, we mimic hippocampal storage and retrieval by storing and reading latent representations to and from memory.</p></sec><sec id="s4-2"><title>Datasets</title><p>We use the CIFAR-10 (<xref ref-type="bibr" rid="bib65">Krizhevsky and Hinton, 2009</xref>) and SVHN (<xref ref-type="bibr" rid="bib87">Netzer et al., 2011</xref>) datasets to evaluate our model. They consist of 32 × 32 pixel images with three color channels. We consider their usual split into a training set and a smaller test set.</p></sec><sec id="s4-3"><title>Training procedure</title><p>We train our model by performing stochastic gradient descent with mini-batches on condition-specific objective functions, in the following also referred to as loss functions, using the ADAM optimizer (<inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf61"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.999</mml:mn></mml:mrow></mml:math></inline-formula>; <xref ref-type="bibr" rid="bib62">Kingma and Ba, 2017</xref>) with a learning rate of 0.0002 and mini-batch size of 64. We rely on our model being fully differentiable. The following section describes the loss functions for the respective conditions.</p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="2" valign="bottom">Algorithm 1: Training procedure</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>                            //initialize network parameters<break/><bold>for</bold> <italic>number of training iterations</italic> <bold>do</bold><break/>  Wake<break/>  <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>←</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>                    //random mini-batch from dataset<break/>  <inline-formula><mml:math id="inf64"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mo>←</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>                    //infer latent and discriminative outputs<break/>  <inline-formula><mml:math id="inf65"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>←</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>                        //reconstruct input via generator<break/>  <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>img</mml:mtext></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>b</mml:mi></mml:msubsup><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>-</mml:mo><mml:mmultiscripts><mml:mi mathvariant="bold-italic">x</mml:mi><mml:none/><mml:mo>′</mml:mo><mml:none/><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mmultiscripts></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>                 //compute reconstruction loss<break/>  <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mtext>KL</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo stretchy="false">|</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>                        //compute KL-loss<break/>  <inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>real</mml:mtext></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>b</mml:mi></mml:msubsup><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>           //compute discriminator loss on real samples<break/>  <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>img</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>real</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>      //update encoder/discriminator parameters<break/>  <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>img</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>                    //update generator parameters<break/><break/>  NREM sleep<break/>  <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>←</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>                 //mini-batch of latent vectors from Wake<break/>  <inline-formula><mml:math id="inf72"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>←</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>                        //reconstruct input via generator<break/>  <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">←</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold">Ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>                         //infer perturbed input&gt;<break/>  <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext mathvariant="script">NREM</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">←</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>                //compute reconstruction loss<break/>  <inline-formula><mml:math id="inf75"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>NREM</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><break/><break/>  REM sleep<break/>  <bold>if</bold> <italic>first iteration</italic> <bold>then</bold><break/>    <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>mix</mml:mtext></mml:msub><mml:mo>←</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:math></inline-formula><break/>  else<break/>    <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>mix</mml:mtext></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>old</mml:mtext></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">ϵ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> //convex combination of current and old latent vectors with noise<break/>  end<break/>  <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mo>←</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>mix</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><break/>  <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>REM</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">←</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>                 //compute adversarial loss<break/>  <inline-formula><mml:math id="inf80"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>E</mml:mi></mml:msub></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>REM</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula><break/>  <inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>←</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mo>∇</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>G</mml:mi></mml:msub></mml:msub><mml:mo>⁡</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>REM</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>               //gradient ascent on discriminator loss<break/>  <inline-formula><mml:math id="inf82"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>old</mml:mtext></mml:msub><mml:mo>←</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:math></inline-formula>                     //keep current vectors for next iteration<break/><bold>end</bold></td></tr></tbody></table></table-wrap><sec id="s4-3-1"><title>Loss functions</title><sec id="s4-3-1-1"><title>Wake</title><p>In the Wake condition, we minimize the following objective function, composed of a loss for image encoding, a regularization, and a real/fake (external/internal) discriminator,<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>Wake</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>img</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>real</mml:mtext></mml:msub></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf83"><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:mi>G</mml:mi></mml:math></inline-formula> learn to reconstruct the mini-batch of images <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> similarly to autoencoders (<xref ref-type="bibr" rid="bib9">Bengio et al., 2013</xref>) by minimizing the image reconstruction loss <inline-formula><mml:math id="inf86"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>img</mml:mtext></mml:msub></mml:math></inline-formula> defined by<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>img</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf87"><mml:mi>b</mml:mi></mml:math></inline-formula> denotes the size of the mini-batch. We store the latent vectors <inline-formula><mml:math id="inf88"><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> corresponding to the current mini-batch for usage during the NREM and REM phases.</p><p>We additionally impose a Kullback–Leibler divergence loss on the encoder <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula>. This acts as a regularizer and encourages latent activities to be Gaussian with zero mean and unit variance:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mtext>KL</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:mi>q</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">§#0956;</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">ॣ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is a distribution over the latent variables <inline-formula><mml:math id="inf91"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math></inline-formula>, parameterized by mean µ and standard deviation <inline-formula><mml:math id="inf92"><mml:mi mathvariant="bold-italic">§#0963;</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf93"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the prior distribution over latent variables. <inline-formula><mml:math id="inf94"><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula> is trained to minimize the following loss:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:munderover></mml:mstyle><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mmultiscripts><mml:mi>μ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:none/><mml:mn>2</mml:mn></mml:mmultiscripts><mml:mo>+</mml:mo><mml:mmultiscripts><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:none/><mml:mn>2</mml:mn></mml:mmultiscripts></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mmultiscripts><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:none/><mml:mn>2</mml:mn></mml:mmultiscripts><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <italic>n</italic><sub><italic>z</italic></sub> denotes the dimension of the latent space and where <inline-formula><mml:math id="inf95"><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf96"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> represent the <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>j</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> elements of respectively the empirical mean <inline-formula><mml:math id="inf98"><mml:msup><mml:mi mathvariant="bold-italic">§#0956;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and empirical standard deviation <inline-formula><mml:math id="inf99"><mml:msup><mml:mi mathvariant="bold-italic">§#0963;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> of the set of latent vectors <inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:math></inline-formula>.</p><p>As part of the adversarial game, <inline-formula><mml:math id="inf101"><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> is trained to classify the mini-batch of images as real. This corresponds to minimizing the loss defined as sum across the mini-batch size <inline-formula><mml:math id="inf102"><mml:mi>b</mml:mi></mml:math></inline-formula>,<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>real</mml:mtext></mml:msub><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>GAN</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>b</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that, in principle, <inline-formula><mml:math id="inf103"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>GAN</mml:mtext></mml:msub></mml:math></inline-formula> can be any GAN-specific loss function (<xref ref-type="bibr" rid="bib42">Gui et al., 2020</xref>). Here, we choose the binary cross-entropy loss.</p></sec><sec id="s4-3-1-2"><title>NREM sleep</title><p>Each Wake phase is followed by an NREM phase. During this phase, we make use of the mini-batch of latent vectors <inline-formula><mml:math id="inf104"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> stored during the Wake phase. Starting from a mini-batch of latent vectors, we generate images <inline-formula><mml:math id="inf105"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Each obtained image of <inline-formula><mml:math id="inf106"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is multiplied by a binary occlusion mask <inline-formula><mml:math id="inf107"><mml:mi mathvariant="bold-italic">w</mml:mi></mml:math></inline-formula> of the same dimension. This mask is generated by randomly picking two occlusion parameters, occlusion intensity and square size (for details, see section ‘Image occlusion’). The encoder <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula> learns to reconstruct the latent vectors <inline-formula><mml:math id="inf109"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> by minimizing the following reconstruction loss:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="script">L</mml:mi><mml:mrow><mml:mtext>NREM</mml:mtext></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>⊙</mml:mo><mml:mi mathvariant="bold-italic">ω</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf110"><mml:mo>⊙</mml:mo></mml:math></inline-formula> denotes the element-wise product.</p></sec><sec id="s4-3-1-3"><title>REM sleep</title><p>In REM, each latent vector from the mini-batch considered during Wake is combined with the latent vector from the previous mini-batch, the whole being convex combined with a mini-batch of noise vectors <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf112"><mml:mi>I</mml:mi></mml:math></inline-formula> is the identity matrix, leading to a mini-batch of latent vectors <inline-formula><mml:math id="inf113"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>mix</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>λ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>old</mml:mtext></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">ϵ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf115"><mml:mrow><mml:msup><mml:mi>λ</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf116"><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>old</mml:mtext></mml:msub></mml:math></inline-formula> is the previous mini-batch of latent activities. This batch of latent vectors is passed through <inline-formula><mml:math id="inf117"><mml:mi>G</mml:mi></mml:math></inline-formula> to generate the associated images <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>mix</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In this phase, the loss function encourages <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> to classify <inline-formula><mml:math id="inf120"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>mix</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as fake, while adversarially pushing <inline-formula><mml:math id="inf121"><mml:mi>G</mml:mi></mml:math></inline-formula> to generate images that are less likely to be classified as fake by the minimax objective<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:munder><mml:mi>max</mml:mi><mml:mi>G</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mpadded width="+2.8pt"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>REM</mml:mtext></mml:msub></mml:mpadded></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>REM</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>GAN</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In our model, the adversarial process is simply described by a full backpropagation of error through <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>E</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf123"><mml:mi>G</mml:mi></mml:math></inline-formula> with a sign switch of weight changes in <inline-formula><mml:math id="inf124"><mml:mi>G</mml:mi></mml:math></inline-formula>.</p><p>In summary, each Wake–NREM–REM cycle consists of (1) reconstructing a mini-batch <inline-formula><mml:math id="inf125"><mml:mi mathvariant="bold-italic">x</mml:mi></mml:math></inline-formula> of images during Wake, (2) reconstructing a mini-batch of latent activities <inline-formula><mml:math id="inf126"><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> during NREM with perturbation of <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and (3) replaying <inline-formula><mml:math id="inf128"><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:math></inline-formula> convex combined with <inline-formula><mml:math id="inf129"><mml:msub><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mtext>old</mml:mtext></mml:msub></mml:math></inline-formula> and noise from the <inline-formula><mml:math id="inf130"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> th cycle. In PAD training, all losses are weighted equally and we did not use a schedule for <inline-formula><mml:math id="inf131"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>KL</mml:mtext></mml:msub></mml:math></inline-formula>, as opposed to standard variational autoencoder training (<xref ref-type="bibr" rid="bib61">Kingma and Welling, 2013</xref>). One training epoch is defined by the number of mini-batches necessary to cover the whole dataset. The evolution of losses with training epochs is shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> and <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>. The whole training procedure is summarized in the pseudo-code implemented in the section ‘Algorithm 1:Training procedure’.</p></sec></sec></sec><sec id="s4-4"><title>Image occlusion</title><p>Following previous work (<xref ref-type="bibr" rid="bib133">Zeiler and Fergus, 2013</xref>), gray squares of various sizes are applied along the image with a certain probability (<xref ref-type="fig" rid="fig9">Figure 9</xref>). For each mini-batch, a probability and square size were randomly picked between 0 and 1, and 1–8, respectively. We divide the image into patches of the given size and replace each patch with a constant value (here, 0) according to the defined probability.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Varying size and intensity of occlusions on example images from CIFAR-10.</title><p>Image occlusions vary along two parameters: occlusion intensity, defined by the probability to apply a gray square at a given position, and square size (s).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-fig9-v2.tif"/></fig></sec><sec id="s4-5"><title>Evaluation</title><sec id="s4-5-1"><title>Training of linear readout</title><p>A linear classifier is trained on top of latent features <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf133"><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf134"><mml:mi>N</mml:mi></mml:math></inline-formula> is the number of training dataset images. A latent feature <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>256</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> is projected via a weight matrix <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi>W</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mn>256</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> to the label neurons to obtain the vector <inline-formula><mml:math id="inf137"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>This weight matrix is trained in a supervised fashion by using a multiclass cross-entropy loss. For a feature <inline-formula><mml:math id="inf138"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> labeled with a target class <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, the per-sample classification loss is given by<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>;</mml:mo><mml:mi>W</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>p</mml:mi><mml:mtext>W</mml:mtext></mml:msub></mml:math></inline-formula> is the conditional probability of the classifier defined by the linear projection and the softmax function<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>Y</mml:mi><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mspace width="thickmathspace"/><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mspace width="thickmathspace"/><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The classifier is trained by mini-batch (<inline-formula><mml:math id="inf141"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:mrow></mml:math></inline-formula>) stochastic gradient descent on the loss <inline-formula><mml:math id="inf142"><mml:msup><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mi>C</mml:mi></mml:msup></mml:math></inline-formula> with a learning rate <inline-formula><mml:math id="inf143"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> for 20 epochs using the whole training dataset.</p></sec><sec id="s4-5-2"><title>Linear separability</title><p>Following previous work (<xref ref-type="bibr" rid="bib48">Hjelm et al., 2019</xref>), we define linear separability as the classification accuracy of the trained classifier on inferred latent activities <inline-formula><mml:math id="inf144"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mtext>test</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from a separate test dataset <inline-formula><mml:math id="inf145"><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mtext>test</mml:mtext></mml:msub></mml:math></inline-formula>. Given a latent feature <inline-formula><mml:math id="inf146"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula>, class prediction is made by picking the index of the maximal activity in the vector <inline-formula><mml:math id="inf147"><mml:mi mathvariant="bold-italic">y</mml:mi></mml:math></inline-formula>. We ran several simulations for four different initial parameters of <inline-formula><mml:math id="inf148"><mml:mi>E</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf149"><mml:mi>G</mml:mi></mml:math></inline-formula> and report the average test accuracy and standard error of the mean (SEM) over trials. To evaluate performance on occluded data, we applied random square occlusion masks on each sample from <inline-formula><mml:math id="inf150"><mml:msub><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mtext>test</mml:mtext></mml:msub></mml:math></inline-formula> for a fixed probability of occlusion and square size. We report only results for occlusions of size 4, after observing similar results with other square sizes.</p></sec><sec id="s4-5-3"><title>PCA visualization</title><p>To visualize the 256-dimensional latent representation <inline-formula><mml:math id="inf151"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the trained model, we used the PCA reduction algorithm (<xref ref-type="bibr" rid="bib57">Jolliffe and Cadima, 2016</xref>). We project the latent representations to the first two principal components.</p></sec><sec id="s4-5-4"><title>Latent space organization metrics</title><p>Intra-class distance is computed by randomly picking 1000 pairs of images of the same class, projecting them to the encoder latent space <inline-formula><mml:math id="inf152"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> and computing their Euclidean distance. This process is repeated over the 10 classes in order to obtain the average over 10 classes. Similarly, inter-class distance is computed by randomly picking 10,000 pairs of images of different classes, projecting them to the encoder latent space <inline-formula><mml:math id="inf153"><mml:mi mathvariant="bold-italic">z</mml:mi></mml:math></inline-formula> and computing their Euclidean distance. The ratio of intra- and inter-class distance is obtained by dividing the mean intra-class distance by the mean inter-class distance. Clean-occluded distance is computed by randomly picking 10,000 pairs of nonoccluded/occluded images, projecting them to the encoder latent space, and computing their Euclidean distance. The ratio of clean-occluded and inter-class distance is obtained by dividing the clean-occluded distance by the mean inter-class distance. We performed this analysis for several different trained networks with different initial conditions and report the mean ratios and SEM over trials.</p></sec><sec id="s4-5-5"><title>Fréchet inception distance</title><p>Following <xref ref-type="bibr" rid="bib46">Heusel et al., 2018</xref>, FID is computed by comparing the statistics of generated (NREM or REM) samples to real images from the training dataset projected through an Inception v3 network pre-trained on ImageNet<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mtext>FID</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo>∥</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mtext>real</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mtext>gen</mml:mtext></mml:msub></mml:mrow><mml:mo>∥</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mtext>real</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mtext>gen</mml:mtext></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mtext>real</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mtext>gen</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where µ and <inline-formula><mml:math id="inf154"><mml:mi mathvariant="normal">Σ</mml:mi></mml:math></inline-formula> represent the empirical mean and covariance of the 2048-dimensional activations of the Inception v3 pool3 layer for 10,000 pairs of data samples and generated images. Results represent mean FID and SEM FID over four different trained networks with different initializations.</p></sec><sec id="s4-5-6"><title>Modifications specific to pathological models</title><p>To evaluate the differential effects of each phase, we removed NREM and/or REM phases from training (<xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig6">6</xref>). For instance, for the condition w/o NREM, the network is never trained with NREM.</p><p>A few adjustments were empirically observed to be necessary in order to obtain a fair comparison between each condition. When removing the REM phase during training, we observed a decrease of linear separability after some (<italic>gt</italic><sub>25</sub>) epochs. We suspect that this decrease is a result of overfitting due to unconstrained autoencoding objective of <inline-formula><mml:math id="inf155"><mml:mi>E</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf156"><mml:mi>G</mml:mi></mml:math></inline-formula>. Models trained without REM hence would not provide a good baseline to reveal the effect of adversarial dreaming on linear separability. For models without the REM phase, we hence added a vector of Gaussian noise <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ϵ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>⋅</mml:mo><mml:mi>I</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> to the encoded activities <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of dimension <italic>n</italic><sub><italic>z</italic></sub> before feeding them to the generator. Thus, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> becomes<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>img</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>b</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munderover><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mspace width="thinmathspace"/><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mi>G</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">ϵ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mo fence="false" stretchy="false">‖</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mspace width="thickmathspace"/><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which stabilizes linear separability of latent activities around its maximal value for both CIFAR-10 and SVHN datasets until the end of training.</p><p>Furthermore, we observed that the NREM phase alters linear performance in the absence of REM (w/o REM condition). To overcome this issue, we reduced the effect of NREM by scaling down its loss with a factor of 0.5. This enabled to benefit from NREM (recognition under image occlusion) without altering linear separability on full images.</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Supervision, Validation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Supervision, Validation, Writing – review and editing, Shared senior authorship</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Methodology, Supervision, Validation, Visualization, Writing – original draft, Writing – review and editing, Shared senior authorship</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-76384-transrepform1-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Deep learning benchmark datasets (CIFAR-10 and SVHN) were used for the simulations. We published all code necessary to repeat our experiments in the following repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/NicoZenith/PAD">https://github.com/NicoZenith/PAD</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:443118c5ab54267481814990611f91c9256f7ecb;origin=https://github.com/NicoZenith/PAD;visit=swh:1:snp:acf706a0ce930fcae2fdacad23193ac110a0c6ee;anchor=swh:1:rev:0a2a4ad4da69f2f8c53fd5ee96e895c72aeb9f26">swh:1:rev:0a2a4ad4da69f2f8c53fd5ee96e895c72aeb9f26</ext-link>).</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><data-title>CIFAR-10</data-title><source>Department of Computer Science, University of Toronto</source><pub-id pub-id-type="accession" xlink:href="https://www.cs.toronto.edu/~kriz/cifar.html">~kriz/cifar</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Netzer</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Coates</surname><given-names>A</given-names></name><name><surname>Bissacco</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2011">2011</year><data-title>SVHN</data-title><source>Computer Science Department, Stanford University</source><pub-id pub-id-type="accession" xlink:href="http://ufldl.stanford.edu/housenumbers/">housenumbers</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work has received funding from the European Union 7th Framework Programme under grant agreement 604102 (HBP), the Horizon 2020 Framework Programme under grant agreements 720270, 785907, and 945539 (HBP), the Swiss National Science Foundation (SNSF, Sinergia grant CRSII5-180316), the Interfaculty Research Cooperation (IRC) ‘Decoding Sleep’ of the University of Bern, and the Manfred Stärk Foundation. We thank the IRC collaborators Paolo Favaro for inspiring discussions on related methods in AI and deep learning, and Antoine Adamantidis and Christoph Nissen for helpful discussions on REM/NREM sleep phenomena in mice and humans.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adamantidis</surname><given-names>AR</given-names></name><name><surname>Gutierrez Herrera</surname><given-names>C</given-names></name><name><surname>Gent</surname><given-names>TC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Oscillating circuitries in the sleeping brain</article-title><source>Nature Reviews. Neuroscience</source><volume>20</volume><fpage>746</fpage><lpage>762</lpage><pub-id pub-id-type="doi">10.1038/s41583-019-0223-4</pub-id><pub-id pub-id-type="pmid">31616106</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Alemi</surname><given-names>AA</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Fischer</surname><given-names>I</given-names></name><name><surname>Dillon</surname><given-names>JV</given-names></name><name><surname>Saurous</surname><given-names>RA</given-names></name><name><surname>Murphy</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Fixing a broken elbo</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1711.00464">https://arxiv.org/abs/1711.00464</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aru</surname><given-names>J</given-names></name><name><surname>Suzuki</surname><given-names>M</given-names></name><name><surname>Rutiku</surname><given-names>R</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name><name><surname>Bachmann</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Coupling the State and Contents of Consciousness</article-title><source>Frontiers in Systems Neuroscience</source><volume>13</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.3389/fnsys.2019.00043</pub-id><pub-id pub-id-type="pmid">31543762</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aru</surname><given-names>J</given-names></name><name><surname>Siclari</surname><given-names>F</given-names></name><name><surname>Phillips</surname><given-names>WA</given-names></name><name><surname>Storm</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Apical drive—A cellular mechanism of dreaming</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>119</volume><fpage>440</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2020.09.018</pub-id><pub-id pub-id-type="pmid">33002561</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avitan</surname><given-names>L</given-names></name><name><surname>Pujic</surname><given-names>Z</given-names></name><name><surname>Mölter</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>B</given-names></name><name><surname>Goodhill</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Spontaneous and evoked activity patterns diverge over development</article-title><source>eLife</source><volume>10</volume><elocation-id>e61942</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.61942</pub-id><pub-id pub-id-type="pmid">33871351</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baird</surname><given-names>B</given-names></name><name><surname>Mota-Rolim</surname><given-names>SA</given-names></name><name><surname>Dresler</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The cognitive neuroscience of lucid dreaming</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>100</volume><fpage>305</fpage><lpage>323</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2019.03.008</pub-id><pub-id pub-id-type="pmid">30880167</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Kang</surname><given-names>S</given-names></name><name><surname>Shim</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Discriminator feature-based inference by recycling the discriminator of gans</article-title><source>International Journal of Computer Vision</source><volume>128</volume><fpage>2436</fpage><lpage>2458</lpage><pub-id pub-id-type="doi">10.1007/s11263-020-01311-4</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Beckham</surname><given-names>C</given-names></name><name><surname>Honari</surname><given-names>S</given-names></name><name><surname>Verma</surname><given-names>V</given-names></name><name><surname>Lamb</surname><given-names>AM</given-names></name><name><surname>Ghadiri</surname><given-names>F</given-names></name><name><surname>Hjelm</surname><given-names>RD</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Pal</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>On Adversarial Mixup Resynthesis</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1903.02709">https://arxiv.org/abs/1903.02709</ext-link></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Vincent</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representation Learning: A Review and New Perspectives</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>35</volume><fpage>1798</fpage><lpage>1828</lpage><pub-id pub-id-type="doi">10.1109/TPAMI.2013.50</pub-id><pub-id pub-id-type="pmid">23787338</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>AS</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning to infer in recurrent biological networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.10811">https://arxiv.org/abs/2006.10811</ext-link></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bergelson</surname><given-names>E</given-names></name><name><surname>Swingley</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>At 6-9 months, human infants know the meanings of many common nouns</article-title><source>PNAS</source><volume>109</volume><fpage>3253</fpage><lpage>3258</lpage><pub-id pub-id-type="doi">10.1073/pnas.1113380109</pub-id><pub-id pub-id-type="pmid">22331874</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkes</surname><given-names>P</given-names></name><name><surname>Orbán</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Fiser</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Spontaneous cortical activity reveals hallmarks of an optimal internal model of the environment</article-title><source>Science (New York, N.Y.)</source><volume>331</volume><fpage>83</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1126/science.1195870</pub-id><pub-id pub-id-type="pmid">21212356</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Berthelot</surname><given-names>D</given-names></name><name><surname>Raffel</surname><given-names>C</given-names></name><name><surname>Roy</surname><given-names>A</given-names></name><name><surname>Goodfellow</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.07543">https://arxiv.org/abs/1807.07543</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bornschein</surname><given-names>J</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Reweighted wake-sleep</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.2751">https://arxiv.org/abs/1406.2751</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyce</surname><given-names>R</given-names></name><name><surname>Glasgow</surname><given-names>SD</given-names></name><name><surname>Williams</surname><given-names>S</given-names></name><name><surname>Adamantidis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Causal evidence for the role of rem sleep theta rhythm in contextual memory consolidation</article-title><source>Science (New York, N.Y.)</source><volume>352</volume><fpage>812</fpage><lpage>816</lpage><pub-id pub-id-type="doi">10.1126/science.aad5252</pub-id><pub-id pub-id-type="pmid">27174984</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boyce</surname><given-names>R</given-names></name><name><surname>Williams</surname><given-names>S</given-names></name><name><surname>Adamantidis</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>REM sleep and memory</article-title><source>Current Opinion in Neurobiology</source><volume>44</volume><fpage>167</fpage><lpage>177</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.05.001</pub-id><pub-id pub-id-type="pmid">28544929</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brendel</surname><given-names>W</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Approximating cnns with bag-of-local-features models works surprisingly well on imagenet</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1904.00760">https://arxiv.org/abs/1904.00760</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Brock</surname><given-names>A</given-names></name><name><surname>Lim</surname><given-names>T</given-names></name><name><surname>Ritchie</surname><given-names>JM</given-names></name><name><surname>Weston</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural Photo Editing with Introspective Adversarial Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.07093">https://arxiv.org/abs/1609.07093</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Theta Oscillations in the Hippocampus</article-title><source>Neuron</source><volume>33</volume><fpage>325</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00586-x</pub-id><pub-id pub-id-type="pmid">11832222</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname><given-names>DJ</given-names></name><name><surname>Mednick</surname><given-names>SA</given-names></name><name><surname>Harrison</surname><given-names>EM</given-names></name><name><surname>Kanady</surname><given-names>JC</given-names></name><name><surname>Mednick</surname><given-names>SC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>REM, not incubation, improves creativity by priming associative networks</article-title><source>PNAS</source><volume>106</volume><fpage>10130</fpage><lpage>10134</lpage><pub-id pub-id-type="doi">10.1073/pnas.0900271106</pub-id><pub-id pub-id-type="pmid">19506253</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>T</given-names></name><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Norouzi</surname><given-names>M</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Simple Framework for Contrastive Learning of Visual Representations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohrs</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Sleep Disturbances in Patients with Schizophrenia</article-title><source>CNS Drugs</source><volume>22</volume><fpage>939</fpage><lpage>962</lpage><pub-id pub-id-type="doi">10.2165/00023210-200822110-00004</pub-id><pub-id pub-id-type="pmid">18840034</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crick</surname><given-names>F</given-names></name><name><surname>Mitchison</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The function of dream sleep</article-title><source>Nature</source><volume>304</volume><fpage>111</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1038/304111a0</pub-id><pub-id pub-id-type="pmid">6866101</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Neal</surname><given-names>RM</given-names></name><name><surname>Zemel</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The Helmholtz Machine</article-title><source>Neural Computation</source><volume>7</volume><fpage>889</fpage><lpage>904</lpage><pub-id pub-id-type="doi">10.1162/neco.1995.7.5.889</pub-id><pub-id pub-id-type="pmid">7584891</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How Does the Brain Solve Visual Object Recognition</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Diekelmann</surname><given-names>S</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The memory function of sleep</article-title><source>Nature Reviews. Neuroscience</source><volume>11</volume><fpage>114</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1038/nrn2762</pub-id><pub-id pub-id-type="pmid">20046194</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Krähenbühl</surname><given-names>P</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Adversarial Feature Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1605.09782">https://arxiv.org/abs/1605.09782</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dresler</surname><given-names>M</given-names></name><name><surname>Wehrle</surname><given-names>R</given-names></name><name><surname>Spoormaker</surname><given-names>VI</given-names></name><name><surname>Koch</surname><given-names>SP</given-names></name><name><surname>Holsboer</surname><given-names>F</given-names></name><name><surname>Steiger</surname><given-names>A</given-names></name><name><surname>Obrig</surname><given-names>H</given-names></name><name><surname>Sämann</surname><given-names>PG</given-names></name><name><surname>Czisch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural correlates of dream lucidity obtained from contrasting lucid versus non-lucid REM sleep: A combined EEG/fMRI case study</article-title><source>Sleep</source><volume>35</volume><fpage>1017</fpage><lpage>1020</lpage><pub-id pub-id-type="doi">10.5665/sleep.1974</pub-id><pub-id pub-id-type="pmid">22754049</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dudai</surname><given-names>Y</given-names></name><name><surname>Karni</surname><given-names>A</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The Consolidation and Transformation of Memory</article-title><source>Neuron</source><volume>88</volume><fpage>20</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.004</pub-id><pub-id pub-id-type="pmid">26447570</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dumoulin</surname><given-names>V</given-names></name><name><surname>Belghazi</surname><given-names>I</given-names></name><name><surname>Poole</surname><given-names>B</given-names></name><name><surname>Mastropietro</surname><given-names>O</given-names></name><name><surname>Lamb</surname><given-names>A</given-names></name><name><surname>Arjovsky</surname><given-names>M</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adversarially Learned Inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1606.00704">https://arxiv.org/abs/1606.00704</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Model of birdsong learning based on gradient estimation by dynamic perturbation of neural conductances</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>2038</fpage><lpage>2057</lpage><pub-id pub-id-type="doi">10.1152/jn.01311.2006</pub-id><pub-id pub-id-type="pmid">17652414</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fosse</surname><given-names>MJ</given-names></name><name><surname>Fosse</surname><given-names>R</given-names></name><name><surname>Hobson</surname><given-names>JA</given-names></name><name><surname>Stickgold</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Dreaming and episodic memory: A functional dissociation</article-title><source>Journal of Cognitive Neuroscience</source><volume>15</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1162/089892903321107774</pub-id><pub-id pub-id-type="pmid">12590838</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Foulkes</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1999">1999</year><source>Children’s Dreaming and the Development of Consciousness</source><publisher-name>Harvard University Press</publisher-name><pub-id pub-id-type="doi">10.4159/9780674037168</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Generative Adversarial Brain</article-title><source>Frontiers in Artificial Intelligence</source><volume>2</volume><elocation-id>18</elocation-id><pub-id pub-id-type="doi">10.3389/frai.2019.00018</pub-id><pub-id pub-id-type="pmid">33733107</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gidaris</surname><given-names>S</given-names></name><name><surname>Singh</surname><given-names>P</given-names></name><name><surname>Komodakis</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unsupervised Representation Learning by Predicting Image Rotations</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.07728">https://arxiv.org/abs/1803.07728</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Top-down influences on visual processing</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>350</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nrn3476</pub-id><pub-id pub-id-type="pmid">23595013</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giuditta</surname><given-names>A</given-names></name><name><surname>Ambrosini</surname><given-names>MV</given-names></name><name><surname>Montagnese</surname><given-names>P</given-names></name><name><surname>Mandile</surname><given-names>P</given-names></name><name><surname>Cotugno</surname><given-names>M</given-names></name><name><surname>Grassi Zucconi</surname><given-names>G</given-names></name><name><surname>Vescia</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The sequential hypothesis of the function of sleep</article-title><source>Behavioural Brain Research</source><volume>69</volume><fpage>157</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1016/0166-4328(95)00012-i</pub-id><pub-id pub-id-type="pmid">7546307</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>IJ</given-names></name><name><surname>Pouget-Abadie</surname><given-names>J</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Warde-Farley</surname><given-names>D</given-names></name><name><surname>Ozair</surname><given-names>S</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generative adversarial networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>NIPS 2016 Tutorial: Generative Adversarial Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.00160">https://arxiv.org/abs/1701.00160</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kourtzi</surname><given-names>Z</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The lateral occipital complex and its role in object recognition</article-title><source>Vision Research</source><volume>41</volume><fpage>1409</fpage><lpage>1422</lpage><pub-id pub-id-type="doi">10.1016/s0042-6989(01)00073-6</pub-id><pub-id pub-id-type="pmid">11322983</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards deep learning with segregated dendrites</article-title><source>eLife</source><volume>6</volume><elocation-id>e22901</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22901</pub-id><pub-id pub-id-type="pmid">29205151</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gui</surname><given-names>J</given-names></name><name><surname>Sun</surname><given-names>Z</given-names></name><name><surname>Wen</surname><given-names>Y</given-names></name><name><surname>Tao</surname><given-names>D</given-names></name><name><surname>Ye</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Review on Generative Adversarial Networks: Algorithms, Theory, and Applications</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2001.06937">https://arxiv.org/abs/2001.06937</ext-link></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep multimodal representation learning: A survey</article-title><source>IEEE Access</source><volume>7</volume><fpage>63373</fpage><lpage>63394</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2916887</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ha</surname><given-names>D</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>World models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.10122">https://arxiv.org/abs/1803.10122</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Haider</surname><given-names>P</given-names></name><name><surname>Ellenberger</surname><given-names>B</given-names></name><name><surname>Kriener</surname><given-names>L</given-names></name><name><surname>Jordan</surname><given-names>J</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Petrovici</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Latent Equilibrium: Arbitrarily fast computation with arbitrarily slow neurons</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>17839</fpage><lpage>17851</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Heusel</surname><given-names>M</given-names></name><name><surname>Ramsauer</surname><given-names>H</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.08500">https://arxiv.org/abs/1706.08500</ext-link></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>GE</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name><name><surname>Neal</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The &quot;wake-sleep&quot; algorithm for unsupervised neural networks</article-title><source>Science (New York, N.Y.)</source><volume>268</volume><fpage>1158</fpage><lpage>1161</lpage><pub-id pub-id-type="doi">10.1126/science.7761831</pub-id><pub-id pub-id-type="pmid">7761831</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hjelm</surname><given-names>RD</given-names></name><name><surname>Fedorov</surname><given-names>A</given-names></name><name><surname>Lavoie-Marchildon</surname><given-names>S</given-names></name><name><surname>Grewal</surname><given-names>K</given-names></name><name><surname>Bachman</surname><given-names>P</given-names></name><name><surname>Trischler</surname><given-names>A</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning deep representations by mutual information estimation and maximization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1808.06670">https://arxiv.org/abs/1808.06670</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobson</surname><given-names>JA</given-names></name><name><surname>Pace-Schott</surname><given-names>EF</given-names></name><name><surname>Stickgold</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dreaming and the brain: Toward a cognitive neuroscience of conscious states</article-title><source>The Behavioral and Brain Sciences</source><volume>23</volume><fpage>793</fpage><lpage>842</lpage><pub-id pub-id-type="doi">10.1017/s0140525x00003976</pub-id><pub-id pub-id-type="pmid">11515143</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobson</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>REM sleep and dreaming: towards a theory of protoconsciousness</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>803</fpage><lpage>813</lpage><pub-id pub-id-type="doi">10.1038/nrn2716</pub-id><pub-id pub-id-type="pmid">19794431</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hobson</surname><given-names>JA</given-names></name><name><surname>Hong</surname><given-names>CCH</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Virtual reality and consciousness inference in dreaming</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>1133</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01133</pub-id><pub-id pub-id-type="pmid">25346710</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoel</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The overfitted brain: Dreams evolved to assist generalization</article-title><source>Patterns (New York, N.Y.)</source><volume>2</volume><elocation-id>100244</elocation-id><pub-id pub-id-type="doi">10.1016/j.patter.2021.100244</pub-id><pub-id pub-id-type="pmid">34036289</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Huang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>He</surname><given-names>R</given-names></name><name><surname>Sun</surname><given-names>Z</given-names></name><name><surname>Tan</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Introvae: Introspective variational autoencoders for photographic image synthesis</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.06358">https://arxiv.org/abs/1807.06358</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>CP</given-names></name><name><surname>Kreiman</surname><given-names>G</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Fast Readout of Object Identity from Macaque Inferior Temporal Cortex</article-title><source>Science (New York, N.Y.)</source><volume>310</volume><fpage>863</fpage><lpage>866</lpage><pub-id pub-id-type="doi">10.1126/science.1117593</pub-id><pub-id pub-id-type="pmid">16272124</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ioffe</surname><given-names>S</given-names></name><name><surname>Szegedy</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Batch normalization: Accelerating deep network training by reducing internal covariate shift</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ji</surname><given-names>D</given-names></name><name><surname>Wilson</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Coordinated memory replay in the visual cortex and hippocampus during sleep</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>100</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1038/nn1825</pub-id><pub-id pub-id-type="pmid">17173043</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jolliffe</surname><given-names>IT</given-names></name><name><surname>Cadima</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Principal component analysis: a review and recent developments</article-title><source>Philosophical Transactions. Series A, Mathematical, Physical, and Engineering Sciences</source><volume>374</volume><elocation-id>20150202</elocation-id><pub-id pub-id-type="doi">10.1098/rsta.2015.0202</pub-id><pub-id pub-id-type="pmid">26953178</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Káli</surname><given-names>S</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Off-line replay maintains declarative memories in a model of hippocampal-neocortical interactions</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>286</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1038/nn1202</pub-id><pub-id pub-id-type="pmid">14983183</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Karras</surname><given-names>T</given-names></name><name><surname>Laine</surname><given-names>S</given-names></name><name><surname>Aila</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A Style-Based Generator Architecture for Generative Adversarial Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.04948">https://arxiv.org/abs/1812.04948</ext-link></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keller</surname><given-names>GB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predictive Processing: A Canonical Cortical Computation</article-title><source>Neuron</source><volume>100</volume><fpage>424</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.003</pub-id><pub-id pub-id-type="pmid">30359606</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Auto-encoding variational bayes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1312.6114">https://arxiv.org/abs/1312.6114</ext-link></element-citation></ref><ref id="bib62"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klinzing</surname><given-names>JG</given-names></name><name><surname>Niethard</surname><given-names>N</given-names></name><name><surname>Born</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Mechanisms of systems memory consolidation during sleep</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1598</fpage><lpage>1610</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0467-3</pub-id><pub-id pub-id-type="pmid">31451802</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Korcsak-Gorzo</surname><given-names>A</given-names></name><name><surname>Müller</surname><given-names>MG</given-names></name><name><surname>Baumbach</surname><given-names>A</given-names></name><name><surname>Leng</surname><given-names>L</given-names></name><name><surname>Breitwieser</surname><given-names>OJ</given-names></name><name><surname>van Albada</surname><given-names>SJ</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Meier</surname><given-names>K</given-names></name><name><surname>Legenstein</surname><given-names>R</given-names></name><name><surname>Petrovici</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.11099">https://arxiv.org/abs/2006.11099</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Learning multiple layers of features from tiny images</source><publisher-name>University of Toronto</publisher-name></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep learning</article-title><source>Nature</source><volume>521</volume><fpage>436</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature14539</pub-id><pub-id pub-id-type="pmid">26017442</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Léger</surname><given-names>D</given-names></name><name><surname>Debellemaniere</surname><given-names>E</given-names></name><name><surname>Rabat</surname><given-names>A</given-names></name><name><surname>Bayon</surname><given-names>V</given-names></name><name><surname>Benchenane</surname><given-names>K</given-names></name><name><surname>Chennaoui</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Slow-wave sleep: From the cell to the clinic</article-title><source>Sleep Medicine Reviews</source><volume>41</volume><fpage>113</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1016/j.smrv.2018.01.008</pub-id><pub-id pub-id-type="pmid">29490885</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>PA</given-names></name><name><surname>Durrant</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Overlapping memory replay during sleep builds cognitive schemata</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>343</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2011.06.004</pub-id><pub-id pub-id-type="pmid">21764357</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lewis</surname><given-names>PA</given-names></name><name><surname>Knoblich</surname><given-names>G</given-names></name><name><surname>Poe</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>How Memory Replay in Sleep Boosts Creative Problem-Solving</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>491</fpage><lpage>503</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.03.009</pub-id><pub-id pub-id-type="pmid">29776467</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Ma</surname><given-names>L</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Gan</surname><given-names>W-B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>REM sleep selectively prunes and maintains new synapses in development and learning</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>427</fpage><lpage>437</lpage><pub-id pub-id-type="doi">10.1038/nn.4479</pub-id><pub-id pub-id-type="pmid">28092659</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Santoro</surname><given-names>A</given-names></name><name><surname>Marris</surname><given-names>L</given-names></name><name><surname>Akerman</surname><given-names>CJ</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Backpropagation and the brain</article-title><source>Nature Reviews Neuroscience</source><volume>21</volume><fpage>335</fpage><lpage>346</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0277-3</pub-id><pub-id pub-id-type="pmid">32303713</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>S</given-names></name><name><surname>McKee</surname><given-names>JL</given-names></name><name><surname>Woloszyn</surname><given-names>L</given-names></name><name><surname>Amit</surname><given-names>Y</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Sheinberg</surname><given-names>DL</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inferring learning rules from distributions of firing rates</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1804</fpage><lpage>1810</lpage><pub-id pub-id-type="doi">10.1038/nn.4158</pub-id><pub-id pub-id-type="pmid">26523643</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>F</given-names></name><name><surname>Hou</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Mian</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Tang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Self-supervised Learning: Generative or Contrastive</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.08218">https://arxiv.org/abs/2006.08218</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Llewellyn</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dream to Predict? REM Dreaming as Prospective Coding</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>1961</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01961</pub-id><pub-id pub-id-type="pmid">26779078</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Llewellyn</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Crossing the invisible line: De-differentiation of wake, sleep and dreaming may engender both creative insight and psychopathology</article-title><source>Consciousness and Cognition</source><volume>46</volume><fpage>127</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2016.09.018</pub-id><pub-id pub-id-type="pmid">27718406</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maas</surname><given-names>AL</given-names></name><name><surname>Hannun</surname><given-names>AY</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rectifier nonlinearities improve neural network acoustic models</article-title><conf-name>ICML Workshop on Deep Learning for Audio, Speech and Language Processing</conf-name></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Simple Learned Weighted Sums of Inferior Temporal Neuronal Firing Rates Accurately Predict Human Core Object Recognition Performance</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13402</fpage><lpage>13418</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5181-14.2015</pub-id><pub-id pub-id-type="pmid">26424887</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mamelak</surname><given-names>AN</given-names></name><name><surname>Hobson</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Dream Bizarreness as the Cognitive Correlate of Altered Neuronal Behavior in REM Sleep</article-title><source>Journal of Cognitive Neuroscience</source><volume>1</volume><fpage>201</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1162/jocn.1989.1.3.201</pub-id><pub-id pub-id-type="pmid">23968505</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marblestone</surname><given-names>AH</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Toward an integration of deep learning and neuroscience</article-title><source>Frontiers in Computational Neuroscience</source><volume>10</volume><elocation-id>94</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2016.00094</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname><given-names>JL</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>O’Reilly</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory</article-title><source>Psychological Review</source><volume>102</volume><fpage>419</fpage><lpage>457</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.102.3.419</pub-id><pub-id pub-id-type="pmid">7624455</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McKay</surname><given-names>BE</given-names></name><name><surname>Placzek</surname><given-names>AN</given-names></name><name><surname>Dani</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Regulation of synaptic transmission and plasticity by neuronal nicotinic acetylcholine receptors</article-title><source>Biochemical Pharmacology</source><volume>74</volume><fpage>1120</fpage><lpage>1133</lpage><pub-id pub-id-type="doi">10.1016/j.bcp.2007.07.001</pub-id><pub-id pub-id-type="pmid">17689497</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Miyato</surname><given-names>T</given-names></name><name><surname>Kataoka</surname><given-names>T</given-names></name><name><surname>Koyama</surname><given-names>M</given-names></name><name><surname>Yoshida</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spectral normalization for generative adversarial networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1802.05957">https://arxiv.org/abs/1802.05957</ext-link></element-citation></ref><ref id="bib83"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Munjal</surname><given-names>P</given-names></name><name><surname>Paul</surname><given-names>A</given-names></name><name><surname>Krishnan</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Implicit discriminator in variational autoencoder</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.13062">https://arxiv.org/abs/1909.13062</ext-link></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadel</surname><given-names>L</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Memory consolidation, retrograde amnesia and the hippocampal complex</article-title><source>Current Opinion in Neurobiology</source><volume>7</volume><fpage>217</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(97)80010-4</pub-id><pub-id pub-id-type="pmid">9142752</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Srivastava</surname><given-names>S</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Yamins</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Identifying Learning Rules From Neural Network Observables</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>2639</fpage><lpage>2650</lpage></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>JP</given-names></name><name><surname>McCarley</surname><given-names>RW</given-names></name><name><surname>Hobson</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>REM sleep burst neurons, PGO waves, and eye movement information</article-title><source>Journal of Neurophysiology</source><volume>50</volume><fpage>784</fpage><lpage>797</lpage><pub-id pub-id-type="doi">10.1152/jn.1983.50.4.784</pub-id><pub-id pub-id-type="pmid">6631463</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Netzer</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Coates</surname><given-names>A</given-names></name><name><surname>Bissacco</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reading Digits in Natural Images with Unsupervised Feature Learning</article-title><conf-name>NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011</conf-name></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nir</surname><given-names>Y</given-names></name><name><surname>Tononi</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dreaming and the brain: from phenomenology to neurophysiology</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>88</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2009.12.001</pub-id><pub-id pub-id-type="pmid">20079677</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Newman</surname><given-names>EL</given-names></name><name><surname>Perotte</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Methods for reducing interference in the Complementary Learning Systems model: Oscillating inhibition and autonomous memory rehearsal</article-title><source>Neural Networks</source><volume>18</volume><fpage>1212</fpage><lpage>1228</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2005.08.010</pub-id><pub-id pub-id-type="pmid">16260116</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Neill</surname><given-names>J</given-names></name><name><surname>Pleydell-Bouverie</surname><given-names>B</given-names></name><name><surname>Dupret</surname><given-names>D</given-names></name><name><surname>Csicsvari</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Play it again: reactivation of waking experience and memory</article-title><source>Trends in Neurosciences</source><volume>33</volume><fpage>220</fpage><lpage>229</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2010.01.006</pub-id><pub-id pub-id-type="pmid">20207025</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palmiero</surname><given-names>M</given-names></name><name><surname>Nori</surname><given-names>R</given-names></name><name><surname>Aloisi</surname><given-names>V</given-names></name><name><surname>Ferrara</surname><given-names>M</given-names></name><name><surname>Piccardi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Domain-Specificity of Creativity: A Study on the Relationship Between Visual Creativity and Visual Mental Imagery</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>1870</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.01870</pub-id><pub-id pub-id-type="pmid">26648904</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poe</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Sleep is for forgetting</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>464</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0820-16.2017</pub-id><pub-id pub-id-type="pmid">28100731</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pogodin</surname><given-names>R</given-names></name><name><surname>Mehta</surname><given-names>Y</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Towards Biologically Plausible Convolutional Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2106.13031">https://arxiv.org/abs/2106.13031</ext-link></element-citation></ref><ref id="bib94"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Metz</surname><given-names>L</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.06434">https://arxiv.org/abs/1511.06434</ext-link></element-citation></ref><ref id="bib95"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>JW</given-names></name><name><surname>Hallacy</surname><given-names>C</given-names></name><name><surname>Ramesh</surname><given-names>A</given-names></name><name><surname>Goh</surname><given-names>G</given-names></name><name><surname>Agarwal</surname><given-names>S</given-names></name><name><surname>Sastry</surname><given-names>G</given-names></name><name><surname>Askell</surname><given-names>A</given-names></name><name><surname>Mishkin</surname><given-names>P</given-names></name><name><surname>Clark</surname><given-names>J</given-names></name><name><surname>Krueger</surname><given-names>G</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning transferable visual models from natural language supervision</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.00020">https://arxiv.org/abs/2103.00020</ext-link></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rao</surname><given-names>RPN</given-names></name><name><surname>Ballard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>79</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1038/4580</pub-id><pub-id pub-id-type="pmid">10195184</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rennó-Costa</surname><given-names>C</given-names></name><name><surname>da Silva</surname><given-names>ACC</given-names></name><name><surname>Blanco</surname><given-names>W</given-names></name><name><surname>Ribeiro</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Computational models of memory consolidation and long-term synaptic plasticity during sleep</article-title><source>Neurobiology of Learning and Memory</source><volume>160</volume><fpage>32</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2018.10.003</pub-id><pub-id pub-id-type="pmid">30321652</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Beaudoin</surname><given-names>P</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Christensen</surname><given-names>A</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Costa</surname><given-names>RP</given-names></name><name><surname>de Berker</surname><given-names>A</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Gillon</surname><given-names>CJ</given-names></name><name><surname>Hafner</surname><given-names>D</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Latham</surname><given-names>P</given-names></name><name><surname>Lindsay</surname><given-names>GW</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Roelfsema</surname><given-names>P</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Scellier</surname><given-names>B</given-names></name><name><surname>Schapiro</surname><given-names>AC</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Yamins</surname><given-names>D</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Zylberberg</surname><given-names>J</given-names></name><name><surname>Therien</surname><given-names>D</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A deep learning framework for neuroscience</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1761</fpage><lpage>1770</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0520-2</pub-id><pub-id pub-id-type="pmid">31659335</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Sacramento</surname><given-names>J</given-names></name><name><surname>Ponte Costa</surname><given-names>R</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dendritic cortical microcircuits approximate the backpropagation algorithm</article-title><conf-name>Advances in Neural Information Processing Systems 31 (NeurIPS 2018</conf-name></element-citation></ref><ref id="bib100"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schoenfeld</surname><given-names>G</given-names></name><name><surname>Kollmorgen</surname><given-names>S</given-names></name><name><surname>Lewis</surname><given-names>C</given-names></name><name><surname>Bethge</surname><given-names>P</given-names></name><name><surname>Reuss</surname><given-names>AM</given-names></name><name><surname>Aguzzi</surname><given-names>A</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Dendritic Integration of Sensory and Reward Information Facilitates Learning</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.12.28.474360</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrittwieser</surname><given-names>J</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Hubert</surname><given-names>T</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Sifre</surname><given-names>L</given-names></name><name><surname>Schmitt</surname><given-names>S</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Lockhart</surname><given-names>E</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Graepel</surname><given-names>T</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mastering atari, go, chess and shogi by planning with a learned model</article-title><source>Nature</source><volume>588</volume><fpage>604</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-03051-4</pub-id><pub-id pub-id-type="pmid">33361790</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Are life episodes replayed during dreaming</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>325</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1016/s1364-6613(03)00162-1</pub-id><pub-id pub-id-type="pmid">12907219</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seibt</surname><given-names>J</given-names></name><name><surname>Richard</surname><given-names>CJ</given-names></name><name><surname>Sigl-Glöckner</surname><given-names>J</given-names></name><name><surname>Takahashi</surname><given-names>N</given-names></name><name><surname>Kaplan</surname><given-names>DI</given-names></name><name><surname>Doron</surname><given-names>G</given-names></name><name><surname>de Limoges</surname><given-names>D</given-names></name><name><surname>Bocklisch</surname><given-names>C</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Cortical dendritic activity correlates with spindle-rich oscillations during sleep in rodents</article-title><source>Nature Communications</source><volume>8</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-017-00735-w</pub-id><pub-id pub-id-type="pmid">28947770</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Sacramento</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Backward reasoning the formation rules</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1705</fpage><lpage>1706</lpage><pub-id pub-id-type="doi">10.1038/nn.4172</pub-id><pub-id pub-id-type="pmid">26605880</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shorten</surname><given-names>C</given-names></name><name><surname>Khoshgoftaar</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A survey on image data augmentation for deep learning</article-title><source>Journal of Big Data</source><volume>6</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1186/s40537-019-0197-0</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Sleep viewed as a state of adaptive inactivity</article-title><source>Nature Reviews. Neuroscience</source><volume>10</volume><fpage>747</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1038/nrn2697</pub-id><pub-id pub-id-type="pmid">19654581</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Hasselt</surname><given-names>H</given-names></name><name><surname>Hessel</surname><given-names>M</given-names></name><name><surname>Schaul</surname><given-names>T</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Harley</surname><given-names>T</given-names></name><name><surname>Dulac-Arnold</surname><given-names>G</given-names></name><name><surname>Reichert</surname><given-names>D</given-names></name><name><surname>Rabinowitz</surname><given-names>N</given-names></name><name><surname>Barreto</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The predictron: End-to-end learning and planning</article-title><conf-name>In International Conference on Machine Learning</conf-name><fpage>3191</fpage><lpage>3199</lpage></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simons</surname><given-names>JS</given-names></name><name><surname>Garrison</surname><given-names>JR</given-names></name><name><surname>Johnson</surname><given-names>MK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Brain mechanisms of reality monitoring</article-title><source>Trends in Cognitive Sciences</source><volume>21</volume><fpage>462</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2017.03.012</pub-id><pub-id pub-id-type="pmid">28462815</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Häusser</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A Cooperative Switch Determines the Sign of Synaptic Plasticity in Distal Dendrites of Neocortical Pyramidal Neurons</article-title><source>Neuron</source><volume>51</volume><fpage>227</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.06.017</pub-id><pub-id pub-id-type="pmid">16846857</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spanò</surname><given-names>G</given-names></name><name><surname>Pizzamiglio</surname><given-names>G</given-names></name><name><surname>McCormick</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>IA</given-names></name><name><surname>De Felice</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>TD</given-names></name><name><surname>Edgin</surname><given-names>JO</given-names></name><name><surname>Rosenthal</surname><given-names>CR</given-names></name><name><surname>Maguire</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dreaming with hippocampal damage</article-title><source>eLife</source><volume>9</volume><elocation-id>e56211</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56211</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Subramaniam</surname><given-names>K</given-names></name><name><surname>Luks</surname><given-names>TL</given-names></name><name><surname>Fisher</surname><given-names>M</given-names></name><name><surname>Simpson</surname><given-names>GV</given-names></name><name><surname>Nagarajan</surname><given-names>S</given-names></name><name><surname>Vinogradov</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Computerized Cognitive Training Restores Neural Activity within the Reality Monitoring Network in Schizophrenia</article-title><source>Neuron</source><volume>73</volume><fpage>842</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.12.024</pub-id><pub-id pub-id-type="pmid">22365555</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takahashi</surname><given-names>N</given-names></name><name><surname>Ebner</surname><given-names>C</given-names></name><name><surname>Sigl-Glöckner</surname><given-names>J</given-names></name><name><surname>Moberg</surname><given-names>S</given-names></name><name><surname>Nierwetberg</surname><given-names>S</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Active dendritic currents gate descending cortical outputs in perception</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>1277</fpage><lpage>1285</lpage><pub-id pub-id-type="doi">10.1038/s41593-020-0677-8</pub-id><pub-id pub-id-type="pmid">32747790</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tang</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Yan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Memory Dynamics in Attractor Networks with Saliency Weights</article-title><source>Neural Computation</source><volume>22</volume><fpage>1899</fpage><lpage>1926</lpage><pub-id pub-id-type="doi">10.1162/neco.2010.07-09-1050</pub-id><pub-id pub-id-type="pmid">20235821</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Kemp</surname><given-names>C</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Goodman</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How to Grow a Mind: Statistics, Structure, and Abstraction</article-title><source>Science (New York, N.Y.)</source><volume>331</volume><fpage>1279</fpage><lpage>1285</lpage><pub-id pub-id-type="doi">10.1126/science.1192788</pub-id><pub-id pub-id-type="pmid">21393536</pub-id></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Cirelli</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sleep and the Price of Plasticity: From Synaptic and Cellular Homeostasis to Memory Consolidation and Integration</article-title><source>Neuron</source><volume>81</volume><fpage>12</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.12.025</pub-id><pub-id pub-id-type="pmid">24411729</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tononi</surname><given-names>G</given-names></name><name><surname>Cirelli</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sleep and synaptic down-selection</article-title><source>The European Journal of Neuroscience</source><volume>51</volume><fpage>413</fpage><lpage>421</lpage><pub-id pub-id-type="doi">10.1111/ejn.14335</pub-id><pub-id pub-id-type="pmid">30614089</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tschannen</surname><given-names>M</given-names></name><name><surname>Djolonga</surname><given-names>J</given-names></name><name><surname>Rubenstein</surname><given-names>PK</given-names></name><name><surname>Gelly</surname><given-names>S</given-names></name><name><surname>Lucic</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On mutual information maximization for representation learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1907.13625">https://arxiv.org/abs/1907.13625</ext-link></element-citation></ref><ref id="bib118"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ulyanov</surname><given-names>D</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Lempitsky</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>It Takes (Only) Two: Adversarial Generator-Encoder Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1704.02304">https://arxiv.org/abs/1704.02304</ext-link></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning by the Dendritic Prediction of Somatic Spiking</article-title><source>Neuron</source><volume>81</volume><fpage>521</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.030</pub-id><pub-id pub-id-type="pmid">24507189</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van de Ven</surname><given-names>GM</given-names></name><name><surname>Siegelmann</surname><given-names>HT</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Brain-inspired replay for continual learning with artificial neural networks</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>4069</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17866-2</pub-id><pub-id pub-id-type="pmid">32792531</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voigts</surname><given-names>J</given-names></name><name><surname>Harnett</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Somatic and Dendritic Encoding of Spatial Variables in Retrosplenial Cortex Differs during 2D Navigation</article-title><source>Neuron</source><volume>105</volume><fpage>237</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.10.016</pub-id><pub-id pub-id-type="pmid">31759808</pub-id></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The Role of Sleep in Cognition and Emotion</article-title><source>Annals of the New York Academy of Sciences</source><volume>1156</volume><fpage>168</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1111/j.1749-6632.2009.04416.x</pub-id><pub-id pub-id-type="pmid">19338508</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wamsley</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dreaming and offline memory consolidation</article-title><source>Current Neurology and Neuroscience Reports</source><volume>14</volume><elocation-id>433</elocation-id><pub-id pub-id-type="doi">10.1007/s11910-013-0433-5</pub-id><pub-id pub-id-type="pmid">24477388</pub-id></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waters</surname><given-names>F</given-names></name><name><surname>Blom</surname><given-names>JD</given-names></name><name><surname>Dang-Vu</surname><given-names>TT</given-names></name><name><surname>Cheyne</surname><given-names>AJ</given-names></name><name><surname>Alderson-Day</surname><given-names>B</given-names></name><name><surname>Woodruff</surname><given-names>P</given-names></name><name><surname>Collerton</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What Is the Link Between Hallucinations, Dreams, and Hypnagogic-Hypnopompic Experiences?</article-title><source>Schizophrenia Bulletin</source><volume>42</volume><fpage>1098</fpage><lpage>1109</lpage><pub-id pub-id-type="doi">10.1093/schbul/sbw076</pub-id><pub-id pub-id-type="pmid">27358492</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Theories of Error Back-Propagation in the Brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>235</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.12.005</pub-id><pub-id pub-id-type="pmid">30704969</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wierzynski</surname><given-names>CM</given-names></name><name><surname>Lubenov</surname><given-names>EV</given-names></name><name><surname>Gu</surname><given-names>M</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>State-dependent spike-timing relationships between hippocampal and prefrontal circuits during sleep</article-title><source>Neuron</source><volume>61</volume><fpage>587</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.011</pub-id><pub-id pub-id-type="pmid">19249278</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>J</given-names></name><name><surname>Merritt</surname><given-names>J</given-names></name><name><surname>Rittenhouse</surname><given-names>C</given-names></name><name><surname>Hobson</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Bizarreness in dreams and fantasies: Implications for the activation-synthesis hypothesis</article-title><source>Consciousness and Cognition</source><volume>1</volume><fpage>172</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1016/1053-8100(92)90059-J</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Winocur</surname><given-names>G</given-names></name><name><surname>Moscovitch</surname><given-names>M</given-names></name><name><surname>Bontempi</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Memory formation and long-term retention in humans and animals: Convergence towards a transformation account of hippocampal–neocortical interactions</article-title><source>Neuropsychologia</source><volume>48</volume><fpage>2339</fpage><lpage>2356</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2010.04.016</pub-id><pub-id pub-id-type="pmid">20430044</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>L</given-names></name><name><surname>Kang</surname><given-names>H</given-names></name><name><surname>Xu</surname><given-names>Q</given-names></name><name><surname>Chen</surname><given-names>MJ</given-names></name><name><surname>Liao</surname><given-names>Y</given-names></name><name><surname>Thiyagarajan</surname><given-names>M</given-names></name><name><surname>O’Donnell</surname><given-names>J</given-names></name><name><surname>Christensen</surname><given-names>DJ</given-names></name><name><surname>Nicholson</surname><given-names>C</given-names></name><name><surname>Iliff</surname><given-names>JJ</given-names></name><name><surname>Takano</surname><given-names>T</given-names></name><name><surname>Deane</surname><given-names>R</given-names></name><name><surname>Nedergaard</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sleep drives metabolite clearance from the adult brain</article-title><source>Science (New York, N.Y.)</source><volume>342</volume><fpage>373</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1126/science.1241224</pub-id><pub-id pub-id-type="pmid">24136970</pub-id></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yee</surname><given-names>E</given-names></name><name><surname>Chrysikou</surname><given-names>EG</given-names></name><name><surname>Thompson-Schill</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Semantic Memory</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib132"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zbontar</surname><given-names>J</given-names></name><name><surname>Jing</surname><given-names>L</given-names></name><name><surname>Misra</surname><given-names>I</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Deny</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Barlow twins: Self-supervised learning via redundancy reduction</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2103.03230">https://arxiv.org/abs/2103.03230</ext-link></element-citation></ref><ref id="bib133"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zeiler</surname><given-names>MD</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Visualizing and Understanding Convolutional Networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1311.2901">https://arxiv.org/abs/1311.2901</ext-link></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>C</given-names></name><name><surname>Yan</surname><given-names>S</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised neural network models of the ventral visual stream</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2014196118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2014196118</pub-id><pub-id pub-id-type="pmid">33431673</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Training losses for full and pathological models</title><p>In the following, we report the measured losses over training for the various different pathological conditions. <inline-formula><mml:math id="inf159"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>img</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf160"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>KL</mml:mtext></mml:msub></mml:math></inline-formula> are optimized for each condition and systematically decrease with learning, while <inline-formula><mml:math id="inf161"><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>NREM</mml:mtext></mml:msub></mml:math></inline-formula> is significantly reduced in models with NREM (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>, <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Its initial increase in the models with REM is explained by its competitive optimization with the GAN losses. Generator loss <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>fake</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mtext>REM</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and discriminator loss <inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>real</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mtext>fake</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> are only optimized in models with REM, showing a progressive decrease of the discriminator loss in parallel with an increase of the generator loss, reflecting adversarial learning between the two streams.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Training losses for the full and pathological models with the CIFAR-10 dataset.</title><p>Evolution of training losses used to optimize <inline-formula><mml:math id="inf164"><mml:mi>E</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:mi>G</mml:mi></mml:math></inline-formula> networks (see Materials and methods) over training epochs for the full and pathological models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-app1-fig1-v2.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Training losses for the full and pathological models with the SVHN dataset.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-app1-fig2-v2.tif"/></fig></sec><sec id="s9" sec-type="appendix"><title>Linear classification performance</title><p>We report the mean and SEM of the final linear classification performance (epoch 50) on latent representations of the PAD and pathological models in <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Final classification performance for the full model and all pathological conditions for unoccluded images.</title><p>Mean and standard error of the mean (SEM) over four different initial condition of linear separability of latent representations at the end of training (epoch 50) for perturbed and adversarial dreaming (PAD) and its pathological variants.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Dataset</th><th align="left" valign="bottom">PAD</th><th align="left" valign="bottom">W/o memory mix</th><th align="left" valign="bottom">W/o REM</th><th align="left" valign="bottom">W/o NREM</th><th align="left" valign="bottom">Wake only</th></tr></thead><tbody><tr><td align="left" valign="bottom">CIFAR-10</td><td align="char" char="plusmn" valign="bottom">58.25 ± 0.70</td><td align="char" char="plusmn" valign="bottom">53.87 ± 0.85</td><td align="char" char="plusmn" valign="bottom">46.00 ± 0.43</td><td align="char" char="plusmn" valign="bottom">58.00 ± 0.34</td><td align="char" char="plusmn" valign="bottom">42.25 ± 0.54</td></tr><tr><td align="left" valign="bottom">SVHN</td><td align="char" char="plusmn" valign="bottom">78.92 ± 0.40</td><td align="char" char="plusmn" valign="bottom">60.87 ± 5.07</td><td align="char" char="plusmn" valign="bottom">42.30 ± 1.51</td><td align="char" char="plusmn" valign="bottom">73.25 ± 0.22</td><td align="char" char="plusmn" valign="bottom">41.93 ± 0.65</td></tr></tbody></table><table-wrap-foot><fn><p>REM: rapid eye movement; NREM: non-rapid eye movement.</p></fn></table-wrap-foot></table-wrap><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Linear classification performance for the full model and all pathological conditions.</title><p>For details, see <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-app1-fig3-v2.tif"/></fig><p>We also report the linear classification performance for the full and pathological models over 100 epochs. Linear separability for the ‘w/o REM’ (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3c and d</xref>, pink curves) and ‘w/o memory mix’ (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3d</xref>, purple curve) conditions does not reach levels of the full model (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3c and d</xref>, black curves) even after many training epochs. Furthermore, without NREM (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3c and d</xref>, ‘w/o NREM’ and ‘Wake only,’ orange and gray curves), linear separability tends to decrease after many training epochs, suggesting that NREM helps to stabilize performance with training by preventing overfitting.</p></sec><sec id="s10" sec-type="appendix"><title>Comparison of performance with REM driven by convex combination or noise</title><p>We report the linear classifier performance for PAD using different latent inputs to the generator. In the main text, we use a convex combination of mixed memories (being a convex combination of two different replayed latent vectors) and noise sampled from a Gaussian unit distribution (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>, black). We here show the results when only random Gaussian noise is used (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>, green) and when only a convex combination of memories is used (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>, red). These different mixing strategies do not show a big difference in linear separability over training epochs.</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Linear classification performance for different mixing strategies during rapid eye movement (REM).</title><p>Linear separability of latent representations with training epochs for perturbed and adversarial dreaming (PAD) trained with different REM phases: one driven by a convex combination of mixed memories and noise (black), one by pure noise (green), and one by mixed memories only (red). For details, see <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-app1-fig4-v2.tif"/></fig></sec><sec id="s11" sec-type="appendix"><title>The order of sleep phases has no influence on the performance of the linear classifier</title><p>To investigate the role of the order of NREM and REM sleep phases, we consider a variation in which their order is reversed with respect to the model described in the main text. The performance of the linear classifier is not influenced by this change (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>).</p><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>Linear classification performance for different order of sleep phases.</title><p>Linear separability of latent representations with training epochs for perturbed and adversarial dreaming (PAD) trained when non-rapid eye movement (NREM) precedes rapid eye movement (REM) phase (Wake–NREM–REM, black) or when REM precedes NREM (Wake–REM–NREM, brown).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-app1-fig5-v2.tif"/></fig></sec><sec id="s12" sec-type="appendix"><title>Replaying multiple episodic memories during NREM sleep</title><p>While in the main text we considered NREM to use only a single episodic memory, here we report the results for a model in which also NREM uses multiple (here: two) episodic memories. In the full model (<xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>, black curves, same data as in <xref ref-type="fig" rid="fig5">Figure 5c and d</xref>), NREM uses a single stored latent representation. Here, we additionally consider an additional model in which these representations are obtained from a convex combination of mixed memories and spontaneous cortical activity. The better performance of a single replay suggests that replay from single episodic memories as postulated to occur during NREM sleep is more efficient to robustify latent representations against input perturbations.</p><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Importance of replaying single hippocampal memories during non-rapid eye movement (NREM).</title><p>Linear separability of latent representations at the end of learning with occlusion intensity for a model trained with all phases.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-76384-app1-fig6-v2.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76384.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Schapiro</surname><given-names>Anna C</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>This paper presents a generative adversarial network-inspired model of how learning during wakefulness, non-rapid eye movement (NREM), and REM sleep work together to facilitate the emergence of object category representations. The model is impressive in its ability to shape representations based on internally generated activity that does not directly recapitulate prior experience, and has properties that correspond to replay and dreams in NREM and REM sleep. The model makes predictions that can be tested in sleep experiments in humans and animals.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76384.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schapiro</surname><given-names>Anna C</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Richards</surname><given-names>Blake A</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McGill University</institution></institution-wrap><country>Canada</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Benjamin</surname><given-names>Ari</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting the paper &quot;Memory semantization through perturbed and adversarial dreaming&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by a Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Blake A Richards (Reviewer #2); Ari Benjamin (Reviewer #3).</p><p>After consultation with the reviewers, we have decided on a rejection of the current manuscript with the possibility of resubmission of a substantially revised new manuscript. All reviewers felt that the model was interesting and impressive. The main issues that led us to this decision were a use of semanticization that seemed too different from its use in the cognitive neuroscience literature, making it unclear how to align the model and its predictions with the empirical literature, and lack of theory or analysis to make it clear why the model works. We felt that a version of the paper that addressed these points would essentially be a new paper, as it involves restructuring the framing and discussion, a new title, and substantial new analysis/theory. This comment from Reviewer 3 is the crux of the theory issue: &quot;If the objective of GANs and VAEs are themselves insufficient for representation learning, absent architecture, why would their combination here avoid that problem? &quot; If you decide to resubmit to <italic>eLife</italic>, the paper will likely go back to the same editors and reviewers.</p><p><italic>Reviewer #1:</italic></p><p>This paper presents a model inspired by generative adversarial networks that shows how different forms of nonveridical generative replay can lead to meaningful changes in representations. The authors simulated three cycling states––wakeful learning, NREM sleep, and REM sleep––as optimizing different learning objectives. The model consists of a feedforward pathway that learns to map low–level sensory inputs to high–level latent representations and to discriminate between external stimuli and internally simulated activity, and a feedback pathway that learns to reconstruct low–level patterns from high–level representations. During wakefulness, the model learns to reconstruct the sensory inputs it receives, while storing their associated latent vectors into a memory buffer representing the hippocampus. During NREM, low–level patterns are generated from individual memory traces in the hippocampus. The feedforward pathway processes occluded versions of these patterns and learns to reconstruct the original memory traces. In REM, patterns are generated based on random combinations of hippocampal memories. The feedforward pathway learns to avoid labeling these patterns as external inputs and the feedback pathway tries to trick the feedforward pathway into recognizing them as external stimuli. After training with several cycles of these three learning phases on naturalistic images, the model develops more linearly separable latent representations for different categories, which the authors interpret as evidence for semanticization.</p><p>There are several aspects of this model that are very interesting and impressive. It is able to reorganize internal memory representations in a meaningful way based only on internally generated activity. The demonstrations that occlusions in NREM work best on single episodes and that REM works best when using combined episodes have an intriguing correspondence to known properties of replay and dreams in these sleep stages. As detailed below, there are other aspects of the model that seem less consistent with known properties of sleep, and it is unclear whether the performance metrics demonstrate &quot;semanticization&quot; as the term is typically used in the literature.</p><p>1. One aspect of the model that stands out as being in tension with neural data is that cortical activity during REM is known to be less driven by the hippocampus than during NREM, due to fluctuations in acetylcholine. In the model, the hippocampus is the driver of replay throughout the model in both states.</p><p>2. The model predicts that internally generated patterns should become more similar to actual inputs over time, but REM dreams appear to be persistently bizarre, and in rodents there seems to be a decrease in veridical NREM replay over time after a new experience.</p><p>3. The use of linear separability as an index of semanticization seems in tension with the literature on semantics in a few ways. Areas of visual cortex that are responsive to certain categories of objects are not typically thought to be processing the semantic structure of those objects, in the way that higher level areas (e.g. anterior temporal lobes) do. Semanticization has the connotation of stripping episodes of their details and source information and representing the structure across experiences within the relevant domain, often in a modality–independent way. It is not clear that a simple separation between visual categories captures these senses of the word.</p><p>4. It is unclear when the predictions of the model apply to a night of sleep vs. several nights vs. hundreds or thousands (a developmental timescale). For example, the authors propose depriving subjects of REM sleep and testing the ability to form false memories. Putting aside the difficulty of REM deprivation, it is unclear how many nights of deprivation would be required to test the predictions of the model, especially because REM does not seem to be beneficial during the first few learning epochs (Figure 4).</p><p>Recommendations for the authors</p><p>1. McClelland, McNaughton, and O'Reilly 1995 is cited as a standard consolidation theory, but it belongs in the transformation theory category. The hippocampal memories in that model do not tend to retain their episodic character when consolidated in neocortex.</p><p>2. Norman, Newman, and Perotte 2005, Neural Networks, had a neural network model of REM that deserves discussion here.</p><p>3. The authors might consider simulations demonstrating to what extent alternation between sleep stages is needed and simulations demonstrating whether the order of replay matters – does the model behave differently if REM precedes NREM?</p><p>4. My understanding is that for analyses in Figure 5 the test dataset consists of occluded versions of training images. Does linear separability increase for occluded versions of images that are not presented during training?</p><p>5. Memories are linearly combined to guide reconstruction during REM. It could be useful to demonstrate that this does better than random activity patterns (patterns not based on memories).</p><p><italic>Reviewer #2:</italic></p><p>In this paper, Deperrois et al. develop a neural network model of unsupervised learning that uses three distinct training phases, corresponding to wakefulness, NREM sleep, and REM sleep. These phases are respectively, used to train for reconstructing inputs (and recognizing them as real), representing perturbed sensory inputs similar to non–perturbed sensory inputs, and recognizing internally generated inputs created from mixing stored memories. They show that this model can learn decent semantic concepts that are robust to perturbations, and they use ablation studies to examine the contribution of each phase to these abilities.</p><p>Overall, I really enjoyed this paper and I think it is fantastic. Its major strengths are its originality, its clarity, and its well–designed ablation studies. The authors have developed a model unlike any other in this area, and they have given the reader sufficient data to understand its design and how it works. I believe this paper will be important for researchers in the area of memory consolidation to consider. Moreover, the model makes interesting empirical predictions that can and should be tested.</p><p>The weaknesses of the paper are as follows:</p><p>1) It is odd that eliminating the NREM phase didn't have much of an impact on the accuracy for non–occluded images (Figure 5). My guess: classification on non–occluded images would drop more with the removal of NREM if the authors had used more perturbations than just occlusion, e.g. like those used in SimCLR. Though these additional experiments do not need to be included for the paper to be publishable, per se, I do think they should be considered by the authors (or other researchers) for future studies. This is particularly so because, as it stands, the results suggest that the NREM phase is merely helping the system to be better at recognizing occluded images, which is a wee bit trivial/obvious given that the NREM phase is literally training on occluded images. All that being said, Figure 6e seems to suggest that NREM does help with separating inter–class distances. So, I am left a little confused as to what the actual result is on this matter. The authors only discuss these issues briefly on lines 393–397, and this really could be expanded.</p><p>2) I do not see any reason to run z through the linear classifier weights before performing t–SNE. Moreover, I am concerned that this ends up just being equivalent to an alternative means of visualizing classification accuracy. First, t–SNE should be able to identify these clusters from z itself, and there is essentially no logic provided as to why it wouldn't be able to do this–after all, this is what t–SNE was designed to do. Second, the linear projection of z with the classifier weight will necessarily correspond to a projection of the z vectors that increases the separation between classes. So, really, what we're visualizing here is how well that linear projection separates the classes. But that is already measured by classification accuracy. As such, I don't see what this analysis does beyond the existing data on classification accuracy. I think the authors should have performed t–SNE on the z vectors directly. If the authors are determined not to do this, they should provide much better logic explaining why this is not an appropriate analysis. To reiterate: t–SNE is designed for this purpose and has been used like this in many other publications!</p><p>3) In the discussion on potential mechanisms for implementing the credit assignment proposed here, the authors only mention one potential solution when there are literally dozens of papers on biologically realistic credit assignment in recent years. Lillicrap et al. (2020) and Whittington and Bogacz (2019) both provide reviews of these papers. Plus, Payeur et al. (2021) provide an exhaustive table in their supplementary material listing the different solutions on offer and their properties. The authors should note that there are a multitude of potential solutions, not just one, and reference at least some of these.</p><p>Recommendations for the authors</p><p>1) It is probably worth noting/mentioning that most people report having dreams with completely novel/surreal elements that can be wholly different from their past experiences (e.g. flying), suggesting that not all dreams are a result of rearranging fragments from stored episodic memories. The authors should discuss this and recognize it as a potential limitation of the model.</p><p>2) The perturbed dreaming phase is highly reminiscent of existing self–supervised models from machine learning (e.g. SimCLR, BarlowTwins, etc.), since it is essentially training the feedforward network to match perturbed/transformed versions of the same images to the same latent state as each other. For sake of providing the reader with more intuition about what is happening in the model, the authors should expand the discussion of these links.</p><p>3) A few typos to fix:</p><p>– Line 30: organisms –&gt; organism's</p><p>– Line 47: sleep state –&gt; sleep states</p><p>– Line 341: Our NREM phase does not require to store raw sensory inputs… –&gt; Our NREM phase does not require the storage of raw sensory inputs…</p><p>4) Figure 6e and f are confusing and need to be improved. First, it is unclear what the two different bars for each training regime represent. Second, the y–axes don't make it clear that this is the ratio of intra–to–inter class distances, and the legend has to be referred to for that, which is not helpful for clarity.</p><p>5) To be completely candid with the authors, Figure 7b is very confusing and not terribly helpful for the reader. I understand that this is a sketch of the authors' current thinking on how their PAD model could relate to cortical circuits, but making concrete sense of exactly what is being proposed is nigh impossible. I think the authors should consider removing this panel and simply noting in the text that there are potential biological mechanisms to make the PAD model feasible. As it stands, Figure 7b takes a strong, clear paper and ends it on a very confusing note…</p><p>6) In equation 1, are all three losses really weighted equally over all of training? I'm surprised that the KLD term isn't given a schedule. This is common with VAE models and can help with training.</p><p>7) In section 4.4.4 and 4.4.5 the numbers use a single quote to denote the thousands decimal, but that's a mistake: it should be a comma, e.g. 10,000 not 10'000.</p><p>8) Figure 10 and section 6.1: L_latent is never defined. What is it? Is that what equation 12 was supposed to define (which would make sense, given that equation 2 already defined L_img). Also, why does it increase during training? Similarly, L_fake is never defined.</p><p><italic>Reviewer #3:</italic></p><p>The proposal that the brain learns adversarially during sleep stages is fascinating. The authors propose that not only does feedback to the earliest areas form a generative model of those areas, but that also feedforward activity carries the dual interpretation of a discriminator. (This proposal aligns with that of Gershman (2019) https://www.frontiersin.org/articles/10.3389/frai.2019.00018/, which should be cited here). If it could be shown that this is indeed what the brain does the impact would be tremendous. However, the evidence presented in the manuscript does not yet make a strong case.</p><p>The paper focuses primarily on modeling semantization, and this is defined as the degree to which object categories can be linearly decoded from the top layer of an encoding/decoding neural network. It is worth noting that other communities might call this a model of 'unsupervised representation learning' rather than semantization during memory consolidation. But is linear decodability of object categories an equivalent concept to the semantization of episodic memory? This seems to miss much about memory consolidation.</p><p>The focus on decodability is also problematic in part because it's not clear what about the model leads to it. In the ML community, it is known that the objectives of generative modeling and autoencoding are by themselves insufficient to provide &quot;good&quot; representations measured by linear decodability of supervised labels. (For arguments why, see https://arxiv.org/pdf/1711.00464.pdf and https://arxiv.org/abs/1907.13625 for autoencoders and https://www.inference.vc/maximum–likelihood–for–representation–learning–2/ for GANs). If such a system empirically learns untangled representations of categories, it is because the network architecture or prior distribution over latents is constraining in some way. The authors claim that &quot;generating new, virtual sensory inputs via adversarial dreaming during REM sleep is essential for extracting semantic concepts&quot; (line 14–15, also 221–222). If the objective of GANs and VAEs are themselves insufficient for representation learning, absent architecture, why would their combination here avoid that problem? For example, is the DCGAN–like architecture crucial? This is possible, but only one architecture was tested. (It is also concerning that the linear decodability of representations in DCGANs can be much higher than reported here; some component of the model is deteriorating, rather than giving, this quality. See Radford et al. (2014)). What about the REM stage in particular is necessary – for example, does it work when randomly sampling from the prior over Z or just convex combinations? Overall, from the computational perspective, I don't think it is yet supported that this objective function necessarily leads to learning untangled, semantic representations from which labels are linearly decodable.</p><p>Linear decoding aside, is this a good model of neural physiology and plasticity? It's a promising direction, and I like the discussion of NREM and REM. However, for a model this radical to be convincing I think much more attention should be paid to what this would look like biologically. Some specific questions stand out:</p><p>– I find it concerning that the generative model is only over the low–level inputs, e.g. V1 (or do the authors believe it would be primary thalamus?). In the predictive processing literature, it is generally assumed that *at every layer* feedback forms an effective generative model of that layer. In the hierarchical model here, there is no relation between the intermediate activations in the feedforward path to those in the feedback path. This prevents the integration of top–down information in intermediate sensory areas and makes the model unrealistic.</p><p>– What neurobiological system do the authors propose implements the output discriminator? If there are no obvious candidates, what would it look like, and what sorts of experiments could identify it?</p><p>– What consequences would the re–use of the feedforward model as a discriminator have for sensory physiology? This is a rather radical change to traditional models of forward sensory processing.</p><p>– The proposed experiments would test if sleep stages are involved in learning, but wouldn't implicate any adversarial learning. For example, the proposal to interrupt REM sleep would not dissociate this proposal from any other in which REM sleep is involved in sensory learning.</p><p>– I think an article modeling consolidation should be situated in hippocampal modeling. Yet here the hippocampus is modeled simply as a RAM memory bank, and the bulk of modeling decisions are about cortical perceptual streams. If the proposal is that this is what the hippocampus effectively does, it would be nice to have a mechanistic discussion as to how the hippocampus might linearly interpolate between two memory traces during the NREM stage. In general, what would this predict a hippocampal physiologist would see?</p><p>– Many related algorithms are dismissed in lines 380–381. I'm not sure what optimization tricks have been removed. Perhaps the authors could explain what was removed and why this makes PAD biologically plausible. In my opinion many of these are comparable.</p><p>I love the originality of this work. Yet to be taken seriously I think it needs to be much more firmly rooted in experimental findings and predictions. A review/perspective format with demonstrative simulations could be more appropriate.</p><p>In my opinion the focus on semantization/ linear decodability is a cherry on top of the main proposal, which is the adversarial framework for sleep stages. Given my reservations about the decodability aspects I think it may be a stronger paper if the framing shifts to focus on sleep physiology and unsupervised learning.</p><p>Miscellaneous comments.</p><p>– Is it spelled semantization or semanticization? The latter appears to be in more common use.</p><p>– I found the tSNE plots not particularly useful. tSNE is nonlinear so it is not a measure of linear category untangling. Please say more about what exactly this measure means, and report the perplexity parameter and how it was chosen.</p><p>– The authors should be aware of recent failures to replicate the Berkes (2011) result: https://elifesciences.org/articles/61942</p><p>Finally, some citations that I think could be mentioned:</p><p>Previous proposals that the brain may learn adversarially:</p><p>– Gershman, Samuel J. &quot;The generative adversarial brain.&quot; Frontiers in Artificial Intelligence 2 (2019): 18.</p><p>– https://arxiv.org/abs/2006.10811 (full disclosure, a work of my own)</p><p>Work in the ML community in which the encoder is also a discriminator:</p><p>– Brock, Andrew, Lim, Theodore, Ritchie, James M, and Weston, Nick. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.</p><p>– Ulyanov, Dmitry, Vedaldi, Andrea, and Lempitsky, Victor. It takes (only) two: Adversarial generatorencoder networks. In Thirty–Second AAAI Conference on Artificial Intelligence, 2018.</p><p>– Huang, Huaibo, He, Ran, Sun, Zhenan, Tan, Tieniu, et al. Introvae: Introspective variational autoencoders for photographic image synthesis. In Advances in neural information processing systems, pp. 52–63, 2018.</p><p>– Munjal, Prateek, Paul, Akanksha, and Krishnan, Narayanan C. Implicit discriminator in variational autoencoder. arXiv preprint arXiv:1909.13062, 2019.</p><p>– Bang, Duhyeon, Kang, Seoungyoon, and Shim, Hyunjung. Discriminator feature–based inference by recycling the discriminator of gans. International Journal of Computer Vision, pp. 1–23, 2020.</p><p>[Editors’ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Learning cortical representations through perturbed and adversarial dreaming&quot; for further consideration by eLife. Your revised article has been evaluated by the same three reviewers.</p><p>The manuscript has been improved and the reviewers were overall positive, but they have one significant remaining request and one minor request for a caveat:</p><p>The significant request is to expand on how exactly the theory can be tested in experiments, with particular emphasis on diagnostic experiments that would allow us to rule out plausible alternatives. From Reviewer 3: &quot;It takes a lot of thought to imagine how this particular hypothesis would surface in data and I don't think it should be left to the reader. More specifically, the paper still has no experimental predictions that could separate this idea from other similar possibilities involving generative models. The authors agreed in the response that they pose few predictions for the adversarial component, and instead only for &quot;whether REM sleep is involved in cortical representation learning using a generative model.&quot; The anterior prefrontal cortex is now briefly mentioned as the top of the discriminator, but surely there would be a great deal of evidence in connectivity, power over plasticity, lesion studies, etc., that could confirm this. Re: &quot;we interpret the reported novelty of REM dreams as strong existing evidence that this learning is based on adversarial principles rather than driven by reconstructions,&quot; (531-533): Here I disagree (though yes, not reconstructions). It is a strong hint that it is driven by an offline stage involving generative processes. This need not be adversarial. The Wake-Sleep algorithm, for example, also has a Sleep phase in which the hierarchical generative model generates samples via ancestral sampling from the top-level prior distribution. (Perhaps there is a misunderstanding regarding WS: the introduction currently dismisses the WS algorithm with the sentence, &quot;these models explicitly try to reconstruct observed sensory inputs, while most dreams observed during REM sleep rarely reproduce past sensory experiences&quot;, lines 40-41. WS does try to reconstruct inputs during wake, but during sleep it 'fantasizes' randomly like a GAN.) Thus I still feel the paper does not offer tests by which we could know if the model were true.&quot;</p><p>The minor request is to acknowledge in the intro and the new discussion paragraph that the generative algorithm likely requires certain architectures &amp; priors to deliver semantic representations.</p><p>Typo: &quot;Moreover, removing NREM from training also increases [this] ratio.&quot; .</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.76384.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: the authors resubmitted a revised version of the paper for consideration. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>This paper presents a model inspired by generative adversarial networks that shows how different forms of nonveridical generative replay can lead to meaningful changes in representations. The authors simulated three cycling states––wakeful learning, NREM sleep, and REM sleep––as optimizing different learning objectives. The model consists of a feedforward pathway that learns to map low–level sensory inputs to high–level latent representations and to discriminate between external stimuli and internally simulated activity, and a feedback pathway that learns to reconstruct low–level patterns from high–level representations. During wakefulness, the model learns to reconstruct the sensory inputs it receives, while storing their associated latent vectors into a memory buffer representing the hippocampus. During NREM, low–level patterns are generated from individual memory traces in the hippocampus. The feedforward pathway processes occluded versions of these patterns and learns to reconstruct the original memory traces. In REM, patterns are generated based on random combinations of hippocampal memories. The feedforward pathway learns to avoid labeling these patterns as external inputs and the feedback pathway tries to trick the feedforward pathway into recognizing them as external stimuli. After training with several cycles of these three learning phases on naturalistic images, the model develops more linearly separable latent representations for different categories, which the authors interpret as evidence for semanticization.</p></disp-quote><p>We appreciate that the reviewer fully understood the mechanisms of our model and its aim. In the following, we reply point-by-point to the reviewer’s feedback.</p><disp-quote content-type="editor-comment"><p>There are several aspects of this model that are very interesting and impressive. It is able to reorganize internal memory representations in a meaningful way based only on internally generated activity. The demonstrations that occlusions in NREM work best on single episodes and that REM works best when using combined episodes have an intriguing correspondence to known properties of replay and dreams in these sleep stages. As detailed below, there are other aspects of the model that seem less consistent with known properties of sleep, and it is unclear whether the performance metrics demonstrate &quot;semanticization&quot; as the term is typically used in the literature.</p></disp-quote><p>We thank the reviewer for appreciating the link of computational properties of our model to known properties of sleep and dreaming. We agree that the model does not cover all aspects of semantization as typically used in the literature but that it rather suggests mechanisms for one aspect of semantization, the learning of useful cortical representations. We thus changed the framing of our manuscript by focusing on representation learning rather than semantization.</p><disp-quote content-type="editor-comment"><p>1. One aspect of the model that stands out as being in tension with neural data is that cortical activity during REM is known to be less driven by the hippocampus than during NREM, due to fluctuations in acetylcholine. In the model, the hippocampus is the driver of replay throughout the model in both states.</p></disp-quote><p>We agree with the reviewer that hippocampal influence is reduced during REM sleep (e.g., Lewis et al., 2018). Nevertheless, dreams in human patients with hippocampal damage were reported to be less frequent and less episodic-like in nature (Spano` et al., 2020), showing that the hippocampus is at least partially involved in dreaming. We have adapted our model to reflect this combination of reduced hippocampal drive and increased cortical drive during REM by using a combination of episodic memories from hippocampus with spontaneous cortical activity. The conclusions drawn from our model are not influenced by this change. This is expected, since the main purpose of interpolating memories as in our original model is to cover points in latent space that were not experienced by the generator during wakefulness. Hence, adding noise as in the revised model has only small influences on learning.</p><disp-quote content-type="editor-comment"><p>2. The model predicts that internally generated patterns should become more similar to actual inputs over time, but REM dreams appear to be persistently bizarre, and in rodents there seems to be a decrease in veridical NREM replay over time after a new experience.</p></disp-quote><p>Indeed, the model predicts that generated patterns become more similar to inputs over time. However, an important distinction is which aspects of the patterns become similar over time. We use the Fr´echet inception distance (FID) that leverages representations in the deep layers of a pre-trained neural network to measure similarity between dreams and external inputs. In particular, neural networks are known to focus on low-level properties of the input. Our model hence predicts that low-level properties of dreams become more structured, just as reported in the literature (Nir and Tononi, 2010). Capturing the “logical realism” of our generated neuronal activities most likely requires a more sophisticated evaluation metric and an extension of the model capable of generating temporal sequences of sensory stimulation. We discuss this point in the revised manuscript (section 3, lines 505-515).</p><disp-quote content-type="editor-comment"><p>3. The use of linear separability as an index of semanticization seems in tension with the literature on semantics in a few ways. Areas of visual cortex that are responsive to certain categories of objects are not typically thought to be processing the semantic structure of those objects, in the way that higher level areas (e.g. anterior temporal lobes) do. Semanticization has the connotation of stripping episodes of their details and source information and representing the structure across experiences within the relevant domain, often in a modality–independent way. It is not clear that a simple separation between visual categories captures these senses of the word.</p></disp-quote><p>We agree that our model addresses only one aspect of semantization, namely cortical representation learning. Please note that representation learning does capture some essential aspects of semantization as it leads to cortical activity which reflects semantic content of stimuli which are stripped of (some of) their episodic details. We revised the manuscript accordingly, especially the introduction, the discussion and the interpretation of our results. Furthermore, we agree that the extension to multiple modalities is interesting, and we expect our model to also be able to extract supra-modal information by introducing multiple sensory pathways, e.g., auditory in addition to visual. In the revised manuscript, we discuss this in section 3, lines 443-449.</p><disp-quote content-type="editor-comment"><p>4. It is unclear when the predictions of the model apply to a night of sleep vs. several nights vs. hundreds or thousands (a developmental timescale). For example, the authors propose depriving subjects of REM sleep and testing the ability to form false memories. Putting aside the difficulty of REM deprivation, it is unclear how many nights of deprivation would be required to test the predictions of the model, especially because REM does not seem to be beneficial during the first few learning epochs (Figure 4).</p></disp-quote><p>Indeed, our model aims at capturing the effects of REM over many nights, i.e., development. In the revised manuscript, we emphasize this point more strongly to reduce confusion in section 1, lines 67. We also adapted the experimental predictions (section 3, lines 517-525) to reflect this.</p><disp-quote content-type="editor-comment"><p>5. McClelland, McNaughton, and O'Reilly 1995 is cited as a standard consolidation theory, but it belongs in the transformation theory category. The hippocampal memories in that model do not tend to retain their episodic character when consolidated in neocortex.</p></disp-quote><p>We thank the reviewer for pointing this out. Due to the shift of the focus of the manuscript, we however do not discuss the differences between consolidation and transformation theories in detail any more.</p><disp-quote content-type="editor-comment"><p>6. Norman, Newman, and Perotte 2005, Neural Networks, had a neural network model of REM that deserves discussion here.</p></disp-quote><p>We thank the reviewer for the reference. Norman et al.’s model of REM and inhibitory oscillatory inhibition presents another potential role of REM in learning representations. We discussed this model in section 3, lines 410-417.</p><disp-quote content-type="editor-comment"><p>7. The authors might consider simulations demonstrating to what extent alternation between sleep stages is needed and simulations demonstrating whether the order of replay matters – does the model behave differently if REM precedes NREM?</p></disp-quote><p>This is an interesting point. We have run additional experiments to investigate the role of the order of sleep phases and found that it has negligible effect on our results. The independence of phases in our model may be due to the statistically similarity of training samples across training. In the revised manuscript we report these results in section 6.4. of the revised manuscript and discuss them in section 3 (lines 483-489).</p><disp-quote content-type="editor-comment"><p>8. My understanding is that for analyses in Figure 5 the test dataset consists of occluded versions of training images. Does linear separability increase for occluded versions of images that are not presented during training?</p></disp-quote><p>Unfortunately, we did not make this clear in the original manuscript, thanks for pointing this out. We did present occluded versions of images from the test dataset, not shown during training. In the revised manuscript, we clarify this point (section 2.5, lines 263-264).</p><disp-quote content-type="editor-comment"><p>9. Memories are linearly combined to guide reconstruction during REM. It could be useful to demonstrate that this does better than random activity patterns (patterns not based on memories).</p></disp-quote><p>We thank the reviewer for this suggestion. This variant would be similar to the original formulation of GANs (Goodfellow et al., 2014). Similar to our approach, other work in machine learning used convex combinations of encoded vectors to train autoencoders and produced good representations (Beckham et al., 2019; Berthelot et al., 2018). In the revised manuscript, we use a convex combination of mixed memories and random activity patterns mimicking spontaneous activity in cortex. While in the main manuscript we choose equal contributions of episodic memories and spontaneous background activity for simplicity, we include additional experiments in the supplements in which we investigate the extremes, i.e., only combinations of episodic memories or only noise. These results show no significant difference between these versions. We hypothesize that this invariance arises in our model due the statistical similarity of training samples across all training epochs. We speculate, however, that for models which learn continuously a preferential replay of combinations of episodic memories encourages the formation of representations which are useful in the more recent context. Loosely speaking, using episodic memories biases the model to focus on what is relevant now rather than what was relevant at some point. We have included a discussion of this point in the revised manuscript in section 3, lines 470-475.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>In this paper, Deperrois et al. develop a neural network model of unsupervised learning that uses three distinct training phases, corresponding to wakefulness, NREM sleep, and REM sleep. These phases are respectively, used to train for reconstructing inputs (and recognizing them as real), representing perturbed sensory inputs similar to non–perturbed sensory inputs, and recognizing internally generated inputs created from mixing stored memories. They show that this model can learn decent semantic concepts that are robust to perturbations, and they use ablation studies to examine the contribution of each phase to these abilities.</p><p>Overall, I really enjoyed this paper and I think it is fantastic. Its major strengths are its originality, its clarity, and its well–designed ablation studies. The authors have developed a model unlike any other in this area, and they have given the reader sufficient data to understand its design and how it works. I believe this paper will be important for researchers in the area of memory consolidation to consider. Moreover, the model makes interesting empirical predictions that can and should be tested.</p></disp-quote><p>We thank the reviewer for this positive feedback and for appreciating the novelty of our model. In the following, we reply point-by-point to the reviewer’s feedback.</p><disp-quote content-type="editor-comment"><p>The weaknesses of the paper are as follows:</p><p>1) It is odd that eliminating the NREM phase didn't have much of an impact on the accuracy for non–occluded images (Figure 5). My guess: classification on non–occluded images would drop more with the removal of NREM if the authors had used more perturbations than just occlusion, e.g. like those used in SimCLR. Though these additional experiments do not need to be included for the paper to be publishable, per se, I do think they should be considered by the authors (or other researchers) for future studies. This is particularly so because, as it stands, the results suggest that the NREM phase is merely helping the system to be better at recognizing occluded images, which is a wee bit trivial/obvious given that the NREM phase is literally training on occluded images. All that being said, Figure 6e seems to suggest that NREM does help with separating inter–class distances. So, I am left a little confused as to what the actual result is on this matter. The authors only discuss these issues briefly on lines 393–397, and this really could be expanded.</p></disp-quote><p>We agree with the reviewer that our NREM phase is reminiscent of one phase of contrastive learning in which representations of different views of the same image are pulled together (Chen et al., 2020; Liu et al., 2021; Zbontar et al., 2021). However, our training is missing the negative phase in which representations of different images are pushed apart. We thus believe that the NREM phase in our model should rather be interpreted as a form of data augmentation mainly useful for robustifying against occlusions (as the reviewer correctly points out). We agree that including such a negative phase to perform a form of contrastive learning is a very promising future research direction. In the revised manuscript we discuss the link between our model and contrastive learning in section 3 (lines 459-466).</p><disp-quote content-type="editor-comment"><p>2) I do not see any reason to run z through the linear classifier weights before performing t–SNE. Moreover, I am concerned that this ends up just being equivalent to an alternative means of visualizing classification accuracy. First, t–SNE should be able to identify these clusters from z itself, and there is essentially no logic provided as to why it wouldn't be able to do this–after all, this is what t–SNE was designed to do. Second, the linear projection of z with the classifier weight will necessarily correspond to a projection of the z vectors that increases the separation between classes. So, really, what we're visualizing here is how well that linear projection separates the classes. But that is already measured by classification accuracy. As such, I don't see what this analysis does beyond the existing data on classification accuracy. I think the authors should have performed t–SNE on the z vectors directly. If the authors are determined not to do this, they should provide much better logic explaining why this is not an appropriate analysis. To reiterate: t–SNE is designed for this purpose and has been used like this in many other publications!</p></disp-quote><p>We agree with the reviewer that in principle t-SNE does not require any additional mapping as it should be able to identify these clusters autonomously. We initially used the linear classifier projection as we observed that clustered were easier discovered by t-SNE. We agree however that this analysis does not provide any significant new insights. To remove interpretation difficulties easily arising from t-SNE projections (Wattenberg et al., 2016) and make this analysis more accessible, we have replaced it with Principal Component Analysis (Jolliffe and Cadima, 2016) directly on the latent space to visualize its organization. The results obtained from PCA are very similar to those of tSNE.</p><disp-quote content-type="editor-comment"><p>3) In the discussion on potential mechanisms for implementing the credit assignment proposed here, the authors only mention one potential solution when there are literally dozens of papers on biologically realistic credit assignment in recent years. Lillicrap et al. (2020) and Whittington and Bogacz (2019) both provide reviews of these papers. Plus, Payeur et al. (2021) provide an exhaustive table in their supplementary material listing the different solutions on offer and their properties. The authors should note that there are a multitude of potential solutions, not just one, and reference at least some of these.</p></disp-quote><p>We agree that modern neuroscience is in the fortunate position to have a multitude of suggestions for efficient bioplausible credit assignment. For simplicity and fairness, we now cite the reviews the reviewer mentioned (Lillicrap et al., 2020; Whittington and Bogacz, 2019) and the perspective paper from Richards et al. (2019) in section 3, lines 439.</p><disp-quote content-type="editor-comment"><p>1) It is probably worth noting/mentioning that most people report having dreams with completely novel/surreal elements that can be wholly different from their past experiences (e.g. flying), suggesting that not all dreams are a result of rearranging fragments from stored episodic memories. The authors should discuss this and recognize it as a potential limitation of the model.</p></disp-quote><p>We agree with the reviewer that the typical reports contain experiences which are very different from past experiences. In the revised manuscript, we consider not just combinations of episodic memories but in addition the influence of spontaneous cortical memory during REM, in part to reflect the reduced correlation between hippocampal and cortical activity during this sleep stage (Wierzynski et al., 2009). We hypothesize that such a combination of episodic memories with ongoing cortical activity may indeed support surreal experiences. Nevertheless, we believe that also the described surreal experience of flying can be interpreted purely as the combination of episodic memories. We also would like to point out that the decrease in FID score reported on our manuscript does <italic>not</italic> mean that the semantic content of dreams, as measured by reports of bizarreness (Mamelak and Hobson, 1989), becomes more plausible over development. Capturing the ”logical realism” of our generated neuronal activities most likely requires a more sophisticated evaluation metric and an extension of the model capable of generating temporal sequences of sensory stimulation. In the revised manuscript, we discuss these points in detail in section 3, lines 505-515.</p><disp-quote content-type="editor-comment"><p>2) The perturbed dreaming phase is highly reminiscent of existing self–supervised models from machine learning (e.g. SimCLR, BarlowTwins, etc.), since it is essentially training the feedforward network to match perturbed/transformed versions of the same images to the same latent state as each other. For sake of providing the reader with more intuition about what is happening in the model, the authors should expand the discussion of these links.</p></disp-quote><p>As discussed above, our current model only implements one phase of these self-supervised models. Nevertheless we agree that this link is important and in the revised manuscript we discuss the link with contrastive learning methods in (section 3 lines 459-466).</p><disp-quote content-type="editor-comment"><p>3) A few typos to fix:</p><p>– Line 30: organisms –&gt; organism's</p><p>– Line 47: sleep state –&gt; sleep states</p><p>– Line 341: Our NREM phase does not require to store raw sensory inputs… –&gt; Our NREM phase does not require the storage of raw sensory inputs…</p></disp-quote><p>We thank the reviewer to point out these typos which are fixed in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>4) Figure 6e and f are confusing and need to be improved. First, it is unclear what the two different bars for each training regime represent. Second, the y–axes don't make it clear that this is the ratio of intra–to–inter class distances, and the legend has to be referred to for that, which is not helpful for clarity.</p></disp-quote><p>We changed Figure 6e and f to make them more accessible, changing the y-axis into ‘ratio of intra/inter distances’ and added the dataset names to the first bars.</p><disp-quote content-type="editor-comment"><p>5) To be completely candid with the authors, Figure 7b is very confusing and not terribly helpful for the reader. I understand that this is a sketch of the authors' current thinking on how their PAD model could relate to cortical circuits, but making concrete sense of exactly what is being proposed is nigh impossible. I think the authors should consider removing this panel and simply noting in the text that there are potential biological mechanisms to make the PAD model feasible. As it stands, Figure 7b takes a strong, clear paper and ends it on a very confusing note…</p></disp-quote><p>We agree that the presentation of Figure 7b was suboptimal and that this content may be more appropriate for a perspective paper. We thus removed Figure 7b from the revised manuscript.</p><disp-quote content-type="editor-comment"><p>6) In equation 1, are all three losses really weighted equally over all of training? I'm surprised that the KLD term isn't given a schedule. This is common with VAE models and can help with training.</p></disp-quote><p>Indeed, all three losses are weighted equally over training. The only exception is during WAKE+NREM training where we reduced the weight of the NREM loss to 0<italic>.</italic>5 (see also Methods), which lead to better performance of the linear readout. Our training procedure did not make use of a schedule for the KL loss.</p><disp-quote content-type="editor-comment"><p>7) In section 4.4.4 and 4.4.5 the numbers use a single quote to denote the thousands decimal, but that's a mistake: it should be a comma, e.g. 10,000 not 10'000.</p></disp-quote><p>We thank the reviewer for pointing this out, we corrected it accordingly.</p><disp-quote content-type="editor-comment"><p>8) Figure 10 and section 6.1: L_latent is never defined. What is it? Is that what equation 12 was supposed to define (which would make sense, given that equation 2 already defined L_img). Also, why does it increase during training? Similarly, L_fake is never defined.</p></disp-quote><p>Indeed, we did not define this properly in the original manuscript, we thank the reviewer for pointing this out and their attention to detail. L<sub>latent</sub> was actually the L<sub>NREM</sub> defined in Equation 6., L<sub>fake</sub> is defined in section 6.1., and is actually equal to L<sub>REM</sub>. We have improved this exposition in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The proposal that the brain learns adversarially during sleep stages is fascinating.</p></disp-quote><p>We appreciate that the reviewer shares the same vision of the brain and their enthusiasm. In the following, we reply point-by-point to the reviewer’s feedback.</p><disp-quote content-type="editor-comment"><p>The authors propose that not only does feedback to the earliest areas form a generative model of those areas, but that also feedforward activity carries the dual interpretation of a discriminator. (This proposal aligns with that of Gershman (2019) https://www.frontiersin.org/articles/10.3389/frai.2019.00018/, which should be cited here). If it could be shown that this is indeed what the brain does the impact would be tremendous. However, the evidence presented in the manuscript does not yet make a strong case.</p></disp-quote><p>We thank the reviewer for the reference, in the revised manuscript we refer to Gershman (2019) in sections 2.6 (lines 334) and 3 (lines 418). However, please note that in contrast to our model Gershman mainly focuses on reality monitoring and does not propose a concrete implementation, only referring to adversarially learned inference (Dumoulin et al., 2017).</p><disp-quote content-type="editor-comment"><p>The paper focuses primarily on modeling semantization, and this is defined as the degree to which object categories can be linearly decoded from the top layer of an encoding/decoding neural network. It is worth noting that other communities might call this a model of 'unsupervised representation learning' rather than semantization during memory consolidation. But is linear decodability of object categories an equivalent concept to the semantization of episodic memory? This seems to miss much about memory consolidation.</p></disp-quote><p>We agree with the reviewer that our model does not capture the various facets of semantization discussed in the cognitive literature on memory consolidation. Rather, as the reviewer correctly points out, we address one component relevant for semantization, namely representation learning. In the revised manuscript, we hence sharpen the focus on cortical representation learning. Nevertheless, we are convinced that understanding representation learning is a key step towards understanding semantization as a whole.</p><disp-quote content-type="editor-comment"><p>The focus on decodability is also problematic in part because it's not clear what about the model leads to it. In the ML community, it is known that the objectives of generative modeling and autoencoding are by themselves insufficient to provide &quot;good&quot; representations measured by linear decodability of supervised labels. (For arguments why, see https://arxiv.org/pdf/1711.00464.pdf and https://arxiv.org/abs/1907.13625 for autoencoders and https://www.inference.vc/maximum–likelihood–for–representation–learning–2/ for GANs). If such a system empirically learns untangled representations of categories, it is because the network architecture or prior distribution over latents is constraining in some way. The authors claim that &quot;generating new, virtual sensory inputs via adversarial dreaming during REM sleep is essential for extracting semantic concepts&quot; (line 14–15, also 221–222). If the objective of GANs and VAEs are themselves insufficient for representation learning, absent architecture, why would their combination here avoid that problem? For example, is the DCGAN–like architecture crucial? This is possible, but only one architecture was tested. (It is also concerning that the linear decodability of representations in DCGANs can be much higher than reported here; some component of the model is deteriorating, rather than giving, this quality. See Radford et al. (2014)). What about the REM stage in particular is necessary – for example, does it work when randomly sampling from the prior over Z or just convex combinations? Overall, from the computational perspective, I don't think it is yet supported that this objective function necessarily leads to learning untangled, semantic representations from which labels are linearly decodable.</p></disp-quote><p>We thank the reviewer for providing these references and we agree that theoretically it is not well understood how disentangled representations are learned from objectives which do not directly encourage them, i.e., reconstruction and adversarial losses. Nevertheless, as the reviewer points out, there exists a plethora of evidence that empirically these systems do learn useful representations. In line with previous arguments, we believe that it is the combination of objectives, architectural constraints, and latent priors which encourages the learning of disentangled representations (Tschannen et al., 2020).</p><p>In the revised manuscript we discuss these important theoretical points. Note however that we are focusing on leveraging empirically successful methods to explain computational and phenomenological properties of sleep and do not aim at an exhaustive theoretical evaluation of different representation learning techniques.</p><p>The linear decodability reported in the DCGANs paper (Radford et al., 2015) has been computed differently from our model case, explaining the much higher linear separability. In the revised manuscript, we discuss this point in section 3, lines 428-433.</p><p>Reflecting the reduced correlation between hippocampus and cortex during REM sleep (Wierzynski et al., 2009), we have modified the construction of latent states during REM in the revised manuscript to contain both a mixture of episodic memories and spontaneous cortical activity. While different mixing strategies could be used, we picked one that matches well with dream phenomenology. Extending the results of the manuscript, we show in the appendix (section 6.3) the performance with REM driven by convex combination or random Gaussian noise and report that there is no big difference. We speculate however that for models which learn continuously, a preferential replay of combinations of episodic memories encourages the formation of representations which are useful in the more recent context. We discuss this in the revised manuscript in section 3, lines 470-475.</p><disp-quote content-type="editor-comment"><p>Linear decoding aside, is this a good model of neural physiology and plasticity? It's a promising direction, and I like the discussion of NREM and REM. However for a model this radical to be convincing I think much more attention should be paid to what this would look like biologically. Some specific questions stand out:</p><p>– I find it concerning that the generative model is only over the low–level inputs, e.g. V1 (or do the authors believe it would be primary thalamus?). In the predictive processing literature, it is generally assumed that *at every layer* feedback forms an effective generative model of that layer. In the hierarchical model here, there is no relation between the intermediate activations in the feedforward path to those in the feedback path. This prevents the integration of top–down information in intermediate sensory areas and makes the model unrealistic.</p></disp-quote><p>We thank the reviewer for bringing this point up. First, we would like to clarify that the generative pathway in our model generates activities across all layers during both NREM and REM sleep. Second, we agree that our implementation contrasts with the traditional view of the visual cortex where all bottom-up and top-down activities are merged at every layer. From a computational perspective of representation learning, such an architecture can be challenging to train, due to information shortcuts, e.g., V1 → V2 → V1, which would prevent information (at least during reconstruction learning) to propagate to higher areas (e.g., Inferior-Temporal cortex) where compressed representations should be learned. Naturally, this issue would also arise in predictive processing models (unless explicitly or implicitly prevented) as these information shortcuts are a property of the underlying graphical model and not of a particular implementation thereof.</p><p>Some strategies exist to achieve training of recurrent networks in the presence of such shortcuts, e.g., abandoning backpropagation training in favor of backpropagation through time (Spoerer et al., 2017), (greedy) layerwise training (Hinton, 2012), or using adversarial principles (Benjamin and Kording, 2021). We hypothesize that local recurrence and merging of the two information streams at intermediate layers are especially beneficial for processing of temporally-extended stimuli. Extending our model with local recurrence and combining it with appropriate training principles is an exciting avenue for future work.</p><p>Note however, that our model only describes the ”functional” connections (which are purely feedforward). In bioplausible implementations of backpropagation (Lillicrap et al., 2020; Whittington and Bogacz, 2019), additional connections would appear which are likely to be found in the biological substrate. In the revised manuscript, we discuss these points in section 2.6., lines 360-366.</p><disp-quote content-type="editor-comment"><p>– What neurobiological system do the authors propose implements the output discriminator? If there are no obvious candidates, what would it look like, and what sorts of experiments could identify it?</p></disp-quote><p>In the revised manuscript, we discuss the potential neurobiological systems to implement the output discriminator in section 2.6., lines 333-334, referring to the initial proposal from Gershman (2019) Indeed, the anterior prefrontal cortex has been shown to play a major role in reality monitoring by distinguishing internally from externally generated information (Simons et al., 2017).</p><disp-quote content-type="editor-comment"><p>– What consequences would the re–use of the feedforward model as a discriminator have for sensory physiology? This is a rather radical change to traditional models of forward sensory processing.</p></disp-quote><p>We agree with the reviewer that this is a break with traditional models of forward sensory processing which are focused on distinguishing the <italic>content</italic> of stimuli (e.g., object identity), whereas in our model the discriminator needs to distinguish the <italic>source</italic> of stimuli (“inside” vs. “outside”). However, it is commonly assumed that cortex performs many functions within sensory processing streams (DiCarlo et al., 2012). The distinction between externally or internally generated, while computationally new, is mechanistically not too different from the distinction of different object types, i.e., ultimately it is a classification based on stimulus properties. We have included a discussion of this in the revised manuscript in section 2.1, lines 125-131.</p><disp-quote content-type="editor-comment"><p>– The proposed experiments would test if sleep stages are involved in learning, but wouldn't implicate any adversarial learning. For example, the proposal to interrupt REM sleep would not dissociate this proposal from any other in which REM sleep is involved in sensory learning.</p></disp-quote><p>We agree with the reviewer that many predictions of our model address mainly the question of whether REM sleep is involved in cortical representation learning using a generative model. While we believe that experimentally testing this prediction would be a significant step forward, we would additionally like to point out that the reported novelty of REM dreams is a strong hint that this learning is not driven by reconstruction, but rather by adversarial principles. Otherwise, the similarity of dreams to waking experience should be higher. In addition, we make specific predictions for plasticity switching sign between wake and sleep phases, which should only be the case for adversarial learning. We discuss these points in section 3, lines 531-533 of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>– I think an article modeling consolidation should be situated in hippocampal modeling. Yet here the hippocampus is modeled simply as a RAM memory bank, and the bulk of modeling decisions are about cortical perceptual streams. If the proposal is that this is what the hippocampus effectively does, it would be nice to have a mechanistic discussion as to how the hippocampus might linearly interpolate between two memory traces during the NREM stage. In general, what would this predict a hippocampal physiologist would see?</p></disp-quote><p>We agree that most sleep models focus on hippocampal replay. Yet semantic knowledge is represented in the neocortex, and sleep contributes to extracting semantic information from hippocampal memories in the cortex (Inostroza and Born, 2013). Hence, we believe that focusing on the neocortex and representation learning brings a missing computational principle to sleep models. Due to our focus, a simple hippocampal storage and replay mechanism is desirable and detailed hippocampal modeling would go beyond the scope of our manuscript.</p><p>Note that the combination of episodic memories does not need to take place in hippocampus, but rather it could occur in cortical neurons to which hippocampal neurons project to. Based on these ideas we would predict that one should observe a temporally close activation of different episodic memories in hippocampus during REM. In the revised manuscript, we discuss a potential hippocampus implementation, and the way memories would be combined during REM in section</p><p>2.6, lines 367-373.</p><disp-quote content-type="editor-comment"><p>– Many related algorithms are dismissed in lines 380–381. I'm not sure what optimization tricks have been removed. Perhaps the authors could explain what was removed and why this makes PAD biologically plausible. In my opinion many of these are comparable.</p></disp-quote><p>We thank the reviewer for pointing this out. We have improved the corresponding paragraph in the revised manuscript and now explicitly mention what was removed. Note however that we do not dismiss them entirely as computationally powerful and potentially implementable, rather we would like to highlight that even in the absence of these mechanisms, our model behaves well.</p><disp-quote content-type="editor-comment"><p>I love the originality of this work. Yet to be taken seriously I think it needs to be much more firmly rooted in experimental findings and predictions. A review/perspective format with demonstrative simulations could be more appropriate.</p></disp-quote><p>We appreciate the reviewers enthusiasm for our model. We however disagree that a review/perspective format would be better suited. From the construction of the model over the relation to brain physiology and cognitive theories to the competitive computational results, our manuscript clearly presents suitable material for an original research article and an advancement of the state of the art.</p><disp-quote content-type="editor-comment"><p>In my opinion the focus on semantization/ linear decodability is a cherry on top of the main proposal, which is the adversarial framework for sleep stages. Given my reservations about the decodability aspects I think it may be a stronger paper if the framing shifts to focus on sleep physiology and unsupervised learning.</p></disp-quote><p>We thank the reviewer for appreciating the novelty of our adversarial approach to sleep and dreams. However, we disagree that decodability, or in general representation learning is the cherry on top. On the contrary we are convinced that transforming sensory input into representations which are useful for a variety of downstream tasks is a core component of cortical information processing. The linear readout here can be seen as an approximation of what a single downstream neuron can extract from the latent representation and serves as a convenient evaluation measure. While in the cortex readouts may be more complex, i.e., consist of multiple processing stages, disentangled representations nevertheless are a convenient basis (e.g., Ha and Schmidhuber, 2018). We agree that a shift of focus towards representation learning is appropriate, in particular because semantization and memory consolidation in general, may include other aspects which are not captured by our current model. We have adapted the revised manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>Miscellaneous comments.</p><p>– Is it spelled semantization or semanticization? The latter appears to be in more common use.</p></disp-quote><p>Both are used, and it seems that memory-related articles tend to use semantization” (Dudai et al., 2015; Gilboa and Marlatte, 2017; Meeter and Murre, 2004; Sweegers and Talamini, 2014).</p><disp-quote content-type="editor-comment"><p>– I found the tSNE plots not particularly useful. tSNE is nonlinear so it is not a measure of linear category untangling. Please say more about what exactly this measure means, and report the perplexity parameter and how it was chosen.</p></disp-quote><p>We agree that tSNE visualization are not easily interpretable. In the revised manuscript, we thus use the PCA algorithm directly on <italic>z</italic> features (Figure 6).</p><disp-quote content-type="editor-comment"><p>– The authors should be aware of recent failures to replicate the Berkes (2011) result: https://elifesciences.org/articles/61942</p></disp-quote><p>We thank the reviewer for providing this reference. We have included it in the revised manuscript in section 3, lines 500-502.</p><disp-quote content-type="editor-comment"><p>Finally, some citations that I think could be mentioned:</p><p>Previous proposals that the brain may learn adversarially:</p><p>– Gershman, Samuel J. &quot;The generative adversarial brain.&quot; Frontiers in Artificial Intelligence 2 (2019): 18.</p><p>– https://arxiv.org/abs/2006.10811 (full disclosure, a work of my own)</p></disp-quote><p>We thank the reviewer for these references. In the revised manuscript, we include these in lines 417-420.</p><disp-quote content-type="editor-comment"><p>Work in the ML community in which the encoder is also a discriminator:</p><p>– Brock, Andrew, Lim, Theodore, Ritchie, James M, and Weston, Nick. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.</p><p>– Ulyanov, Dmitry, Vedaldi, Andrea, and Lempitsky, Victor. It takes (only) two: Adversarial generatorencoder networks. In Thirty–Second AAAI Conference on Artificial Intelligence, 2018.</p><p>– Huang, Huaibo, He, Ran, Sun, Zhenan, Tan, Tieniu, et al. Introvae: Introspective variational autoencoders for photographic image synthesis. In Advances in neural information processing systems, pp. 52–63, 2018.</p><p>– Munjal, Prateek, Paul, Akanksha, and Krishnan, Narayanan C. Implicit discriminator in variational autoencoder. arXiv preprint arXiv:1909.13062, 2019.</p><p>– Bang, Duhyeon, Kang, Seoungyoon, and Shim, Hyunjung. Discriminator feature–based inference by recycling the discriminator of gans. International Journal of Computer Vision, pp. 1–23, 2020.</p></disp-quote><p>We thank the reviewer for these references. In the revised manuscript, these are included (section 2.1, lines 129-131).</p><p>References</p><p>Beckham, C., Honari, S., Verma, V., Lamb, A. M., Ghadiri, F., Hjelm, R. D., Bengio, Y., and Pal, C. (2019). On Adversarial Mixup Resynthesis. page 12.</p><p>Benjamin, A. S. and Kording, K. P. (2021). Learning to infer in recurrent biological networks.</p><p>Berthelot, D., Raffel, C., Roy, A., and Goodfellow, I. (2018). Understanding and Improving Interpolation in Autoencoders via an Adversarial Regularizer. <italic>arXiv:1807.07543 [cs, stat]</italic>. arXiv: 1807.07543.</p><p>Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A Simple Framework for Contrastive Learning of Visual Representations. <italic>arXiv:2002.05709 [cs, stat]</italic>. arXiv: 2002.05709.</p><p>DiCarlo, J. J., Zoccolan, D., and Rust, N. C. (2012). How Does the Brain Solve Visual Object Recognition? <italic>Neuron</italic>, 73(3):415–434.</p><p>Dudai, Y., Karni, A., and Born, J. (2015). The Consolidation and Transformation of Memory. <italic>Neuron</italic>, 88(1):20–32.</p><p>Dumoulin, V., Belghazi, I., Poole, B., Mastropietro, O., Lamb, A., Arjovsky, M., and Courville, A. (2017). Adversarially Learned Inference. <italic>arXiv:1606.00704 [cs, stat]</italic>. arXiv: 1606.00704.</p><p>Gershman, S. J. (2019). The Generative Adversarial Brain. <italic>Frontiers in Artificial Intelligence</italic>, 2.</p><p>Gilboa, A. and Marlatte, H. (2017). Neurobiology of Schemas and Schema-Mediated Memory. <italic>Trends in Cognitive Sciences</italic>, 21(8):618–631.</p><p>Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative adversarial networks.</p><p>Ha, D. and Schmidhuber, J. (2018). World models. <italic>arXiv preprint arXiv:1803.10122</italic>.</p><p>Hinton, G. E. (2012). A Practical Guide to Training Restricted Boltzmann Machines, pages 599–619. Springer Berlin Heidelberg, Berlin, Heidelberg.</p><p>Inostroza, M. and Born, J. (2013). Sleep for Preserving and Transforming Episodic Memory. <italic>Annual Review of Neuroscience</italic>, 36(1):79–102.</p><p>Jolliffe, I. T. and Cadima, J. (2016). Principal component analysis: a review and recent developments. Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences, 374(2065):20150202.</p><p>Lewis, P. A., Knoblich, G., and Poe, G. (2018). How Memory Replay in Sleep Boosts Creative Problem-Solving. <italic>Trends in Cognitive Sciences</italic>, 22(6):491–503.</p><p>Lillicrap, T. P., Santoro, A., Marris, L., Akerman, C. J., and Hinton, G. (2020). Backpropagation and the brain. <italic>Nature Reviews Neuroscience</italic>.</p><p>Liu, X., Zhang, F., Hou, Z., Wang, Z., Mian, L., Zhang, J., and Tang, J. (2021). Self-supervised Learning: Generative or Contrastive. <italic>arXiv:2006.08218 [cs, stat]</italic>. arXiv: 2006.08218.</p><p>Mamelak, A. N. and Hobson, J. A. (1989). Dream Bizarreness as the Cognitive Correlate of Altered Neuronal Behavior in REM Sleep. <italic>Journal of Cognitive Neuroscience</italic>, 1(3):201–222.</p><p>Meeter, M. and Murre, J. M. J. (2004). Consolidation of Long-Term Memory: Evidence and</p><p>Alternatives. <italic>Psychological Bulletin</italic>, 130(6):843–857.</p><p>Nir, Y. and Tononi, G. (2010). Dreaming and the brain: from phenomenology to neurophysiology. <italic>Trends in Cognitive Sciences</italic>, 14(2):88–100.</p><p>Radford, A., Metz, L., and Chintala, S. (2015). Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. <italic>arXiv:1511.06434 [cs]</italic>. arXiv: 1511.06434.</p><p>Richards, B. A., Lillicrap, T. P., Beaudoin, P., Bengio, Y., Bogacz, R., Christensen, A., Clopath,</p><p>C., Costa, R. P., de Berker, A., Ganguli, S., Gillon, C. J., Hafner, D., Kepecs, A., Kriegeskorte, N., Latham, P., Lindsay, G. W., Miller, K. D., Naud, R., Pack, C. C., Poirazi, P., Roelfsema, P., Sacramento, J., Saxe, A., Scellier, B., Schapiro, A. C., Senn, W., Wayne, G., Yamins, D., Zenke, F., Zylberberg, J., Therien, D., and Kording, K. P. (2019). A deep learning framework for neuroscience. <italic>Nature Neuroscience</italic>, 22(11):1761–1770.</p><p>Simons, J. S., Garrison, J. R., and Johnson, M. K. (2017). Brain mechanisms of reality monitoring. <italic>Trends in Cognitive Sciences</italic>, 21(6):462–473.</p><p>Spano`, G., Pizzamiglio, G., McCormick, C., Clark, I. A., De Felice, S., Miller, T. D., Edgin, J. O., Rosenthal, C. R., and Maguire, E. A. (2020). Dreaming with hippocampal damage. <italic>eLife</italic>, 9.</p><p>Spoerer, C. J., McClure, P., and Kriegeskorte, N. (2017). Recurrent Convolutional Neural Networks: A Better Model of Biological Object Recognition. <italic>Frontiers in Psychology</italic>, 8.</p><p>Sweegers, C. C. and Talamini, L. M. (2014). Generalization from episodic memories across time: A route for semantic knowledge acquisition. <italic>Cortex</italic>, 59:49–61.</p><p>Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lucic, M. (2020). On mutual information maximization for representation learning.</p><p>Wattenberg, M., Vi´egas, F., and Johnson, I. (2016). How to use t-sne effectively. <italic>Distill</italic>.</p><p>Whittington, J. C. and Bogacz, R. (2019). Theories of Error Back-Propagation in the Brain. <italic>Trends in Cognitive Sciences</italic>, 23(3):235–250.</p><p>Wierzynski, C. M., Lubenov, E. V., Gu, M., and Siapas, A. G. (2009). State-dependent spike-timing relationships between hippocampal and prefrontal circuits during sleep. <italic>Neuron</italic>, 61(4):587–596.</p><p>Zbontar, J., Jing, L., Misra, I., LeCun, Y., and Deny, S. (2021). Barlow twins: Self-supervised learning via redundancy reduction. <italic>arXiv preprint arXiv:2103.03230</italic>.</p><p>[Editors’ note: what follows is the authors’ response to the second round of review.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved and the reviewers were overall positive, but they have one significant remaining request and one minor request for a caveat:</p><p>The significant request is to expand on how exactly the theory can be tested in experiments, with particular emphasis on diagnostic experiments that would allow us to rule out plausible alternatives. From Reviewer 3: &quot;It takes a lot of thought to imagine how this particular hypothesis would surface in data and I don't think it should be left to the reader. More specifically, the paper still has no experimental predictions that could separate this idea from other similar possibilities involving generative models. The authors agreed in the response that they pose few predictions for the adversarial component, and instead only for &quot;whether REM sleep is involved in cortical representation learning using a generative model.&quot; The anterior prefrontal cortex is now briefly mentioned as the top of the discriminator, but surely there would be a great deal of evidence in connectivity, power over plasticity, lesion studies, etc., that could confirm this. Re: &quot;we interpret the reported novelty of REM dreams as strong existing evidence that this learning is based on adversarial principles rather than driven by reconstructions,&quot; (531-533): Here I disagree (though yes, not reconstructions). It is a strong hint that it is driven by an offline stage involving generative processes. This need not be adversarial. The Wake-Sleep algorithm, for example, also has a Sleep phase in which the hierarchical generative model generates samples via ancestral sampling from the top-level prior distribution. (Perhaps there is a misunderstanding regarding WS: the introduction currently dismisses the WS algorithm with the sentence, &quot;these models explicitly try to reconstruct observed sensory inputs, while most dreams observed during REM sleep rarely reproduce past sensory experiences&quot;, lines 40-41. WS does try to reconstruct inputs during wake, but during sleep it 'fantasizes' randomly like a GAN.) Thus I still feel the paper does not offer tests by which we could know if the model were true.&quot;</p></disp-quote><p>The main request was to provide specific experimental predictions of our model, in particular predictions that distinguish the adversarial dreaming idea from other representation learning frameworks using generative models. Following these requests, we added a new section in the Discussion (”Signatures of adversarial learning in the brain”, lines 557-627). Here, we briefly summarize the main predictions.</p><p>First, we predict the existence of a cortical mechanism to distinguish externally from internally generated activity, in particular:</p><p>– That a specific subset of neurons implement the discriminator function; these neurons are characterized by being in opposite activity regimes during wakefulness and REM sleep.</p><p>– An inverse correlation between the veridicality of hallucinations and REM sleep quality in schizophrenic patients due to impaired discriminator function.</p><p>Second, our model makes predictions about plasticity on feedforward and feedback pathways, in particular:</p><p>– That the same low-level activity pattern triggers opposite plasticity during wakefulness and REM sleep on feedforward synapses due to the opposite target of the discriminator neurons.</p><p>– That plasticity occurs on both bottom-up and top-down pathways both during wakefulness and REM sleep; this is in strong contrast to alternative models involving offline states such as Waake-Sleep.</p><p>– sign-switch of the plasticity rule for synapses in feedback pathways between wakefulness and REM sleep, potentially testable by inferring plasticity rules from experimental observables.</p><disp-quote content-type="editor-comment"><p>The minor request is to acknowledge in the intro and the new discussion paragraph that the generative algorithm likely requires certain architectures &amp; priors to deliver semantic representations.</p><p>Typo: &quot;Moreover, removing NREM from training also increases [this] ratio.&quot;.</p></disp-quote><p>The minor request was to acknowledge in the introduction and the discussion sections that the generative algorithm likely requires certain architectures and priors to deliver semantic representations. We modified the manuscript in lines 93-94 and 403-404 accordingly.</p></body></sub-article></article>