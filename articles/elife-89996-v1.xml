<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">89996</article-id><article-id pub-id-type="doi">10.7554/eLife.89996</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.89996.4</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Asymmetric distribution of color-opponent response types across mouse visual cortex supports superior color vision in the sky</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Franke</surname><given-names>Katrin</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8649-4835</contrib-id><email>kafranke@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Cai</surname><given-names>Chenchen</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0007-3724-5203</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Ponder</surname><given-names>Kayla</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Jiakun</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Sokoloski</surname><given-names>Sacha</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4166-1772</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Berens</surname><given-names>Philipp</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0199-4727</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tolias</surname><given-names>Andreas Savas</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Ophthalmology, Byers Eye Institute, Stanford University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford Bio-X, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Wu Tsai Neurosciences Institute, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Department of Neuroscience &amp; Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Institute for Ophthalmic Research, University of Tübingen</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Graduate Training Center of Neuroscience, International Max Planck Research School, University of Tübingen</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03a1kwz48</institution-id><institution>Hertie Institute for AI in Brain Health, University of Tübingen</institution></institution-wrap><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Electrical Engineering, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00cvxb145</institution-id><institution>University of Washington</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>05</day><month>09</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP89996</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-06-26"><day>26</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-06-05"><day>05</day><month>06</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.06.01.543054"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-09-12"><day>12</day><month>09</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89996.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-04-26"><day>26</day><month>04</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89996.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-08-01"><day>01</day><month>08</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.89996.3"/></event></pub-history><permissions><copyright-statement>© 2023, Franke et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Franke et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-89996-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-89996-figures-v1.pdf"/><abstract><p>Color is an important visual feature that informs behavior, and the retinal basis for color vision has been studied across various vertebrate species. While many studies have investigated how color information is processed in visual brain areas of primate species, we have limited understanding of how it is organized beyond the retina in other species, including most dichromatic mammals. In this study, we systematically characterized how color is represented in the primary visual cortex (V1) of mice. Using large-scale neuronal recordings and a luminance and color noise stimulus, we found that more than a third of neurons in mouse V1 are color-opponent in their receptive field center, while the receptive field surround predominantly captures luminance contrast. Furthermore, we found that color-opponency is especially pronounced in posterior V1 that encodes the sky, matching the statistics of natural scenes experienced by mice. Using unsupervised clustering, we demonstrate that the asymmetry in color representations across cortex can be explained by an uneven distribution of green-On/UV-Off color-opponent response types that are represented in the upper visual field. Finally, a simple model with natural scene-inspired parametric stimuli shows that green-On/UV-Off color-opponent response types may enhance the detection of ‘predatory’-like dark UV-objects in noisy daylight scenes. The results from this study highlight the relevance of color processing in the mouse visual system and contribute to our understanding of how color information is organized in the visual hierarchy across species.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>color vision</kwd><kwd>visual cortex</kwd><kwd>visual ecology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003493</institution-id><institution>Hertie-Stiftung</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Berens</surname><given-names>Philipp</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>DFG CRC 1233</award-id><principal-award-recipient><name><surname>Franke</surname><given-names>Katrin</given-names></name><name><surname>Berens</surname><given-names>Philipp</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>DFG Excellence Cluster 2064</award-id><principal-award-recipient><name><surname>Berens</surname><given-names>Philipp</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>101039115</award-id><principal-award-recipient><name><surname>Berens</surname><given-names>Philipp</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>UF1 NS126566</award-id><principal-award-recipient><name><surname>Tolias</surname><given-names>Andreas Savas</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>RF1 MH126883</award-id><principal-award-recipient><name><surname>Tolias</surname><given-names>Andreas Savas</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Widespread color-opponency in mouse V1 enhances object decoding in the sky, highlighting the evolutionary importance of color processing in non-primate species.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Color is an important property of the visual world informing behavior. The retinal basis for color vision has been studied in many vertebrate species, including zebrafish, mice, and primates (reviewed in <xref ref-type="bibr" rid="bib4">Baden and Osorio, 2019</xref>): Signals from different photoreceptor types which are sensitive to different wavelengths are compared by retinal circuits, thereby creating color-opponent cell types. In primate species, it is well studied how color-opponent signals from the retina are processed in downstream brain areas (<xref ref-type="bibr" rid="bib31">Livingstone and Hubel, 1984</xref>; <xref ref-type="bibr" rid="bib63">Wiesel and Hubel, 1966</xref>; <xref ref-type="bibr" rid="bib18">Gegenfurtner et al., 1996</xref>; <xref ref-type="bibr" rid="bib58">Tanigawa et al., 2010</xref>; <xref ref-type="bibr" rid="bib7">Chatterjee and Callaway, 2003</xref>). In most other species, however, we know relatively little about how color information is processed beyond the retina. Thus, our understanding of color processing along the visual hierarchy across species remains limited, highlighting the need for further research to uncover general rules governing this fundamental aspect of vision.</p><p>Here, we systematically studied how color is represented in the primary visual cortex (V1) of mice. Like most mammals, mice are dichromatic and have two cone photoreceptor types, expressing ultraviolet (UV)- and green-sensitive S- and M-opsin (<xref ref-type="bibr" rid="bib56">Szél et al., 1992</xref>), respectively. In addition, they have one type of rod photoreceptor which is green-sensitive. Importantly, UV- and green-sensitive cone photoreceptors predominantly sample the upper and lower visual field, respectively, through an uneven opsin distribution across the retina (<xref ref-type="bibr" rid="bib56">Szél et al., 1992</xref>; <xref ref-type="bibr" rid="bib3">Baden et al., 2013</xref>). Behavioral studies have demonstrated that mice can discriminate different colors (<xref ref-type="bibr" rid="bib25">Jacobs et al., 2004</xref>), at least in the upper visual field (<xref ref-type="bibr" rid="bib10">Denman et al., 2018</xref>). However, a thorough understanding of the neuronal correlates underlying this behavior is still missing.</p><p>At the level of the mouse retina, a large body of literature has identified mechanisms underlying color-opponent responses, including cone-type selective (<xref ref-type="bibr" rid="bib54">Stabio et al., 2018</xref>; <xref ref-type="bibr" rid="bib38">Nadal-Nicolás et al., 2020</xref>; <xref ref-type="bibr" rid="bib21">Haverkamp et al., 2005</xref>) or cone-type unselective wiring (<xref ref-type="bibr" rid="bib6">Chang et al., 2013</xref>) and rod-cone opponency (<xref ref-type="bibr" rid="bib26">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>). The latter is widespread across many neuron types located in the ventral retina sampling the sky, where rod and cone photoreceptors exhibit the strongest difference in spectral sensitivity, and requires the integration across center and surround components of receptive fields (RFs). In visual areas downstream to the retina, the frequency of color-opponency has remained controversial. Some studies have reported very low numbers of color-opponent neurons in mouse dLGN (<xref ref-type="bibr" rid="bib9">Denman et al., 2017</xref>) and V1 (<xref ref-type="bibr" rid="bib57">Tan et al., 2015</xref>), while two more recent studies identified pronounced cone- and rod-cone-dependent color-opponency (<xref ref-type="bibr" rid="bib37">Mouland et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Rhim and Nauhaus, 2023</xref>).</p><p>In this study, we systematically characterized color and luminance center-surround RF properties of mouse V1 neurons across different light levels using large-scale neuronal recordings and a luminance and color noise stimulus. This revealed that more than a third of neurons in mouse V1 are highly sensitive to color features of the visual input in their RF center, while the RF surround predominantly captures luminance contrast. Color-opponency in the RF center was strongest for photopic light levels largely activating cone photoreceptors and greatly decreased for mesopic light levels, suggesting that the observed color-opponency in V1 is at least partially mediated by the comparison of cone photoreceptor signals. We further showed that color-opponency is especially pronounced in posterior V1 which encodes the sky, in line with previous work in the retina (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>), and matching the statistics of mouse natural scenes (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Abballe and Asari, 2022</xref>). Using unsupervised clustering we demonstrated that the asymmetry in color representations across cortex can be explained by an uneven distribution of green-On/UV-Off color-opponent response types that almost exclusively represented the upper visual field. Finally, by implementing a simple model with natural scene inspired parametric stimuli, we showed that green-On/UV-Off color-opponent response types may enhance the detection of ‘predatory’-like dark UV-objects in noisy daylight scenes.</p><p>The results of our study support the hypothesis that neurons in the visual cortex asymmetrically represent information across the visual field, facilitating specific visual tasks such as the robust detection of aerial predators in noisy natural scenes.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Characterizing color and luminance center-surround RFs of mouse V1 neurons</title><p>To study the neuronal representation of color in mouse V1, we characterized center (i.e. classical) and surround (i.e. extra-classical) RFs of excitatory V1 neurons in awake, head-fixed mice in response to a luminance and color noise stimulus (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The noise stimulus consisted of a center spot (37.5 degrees visual angle in diameter) and a surround annulus (approx. 120×90 degrees visual angle without the center spot) that simultaneously flickered in UV and green based on 5 Hz binary random sequences (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), thereby capturing chromatic, temporal, as well as one spatial dimension of the neurons’ RFs. Neuronal responses to such relatively simple, parametric stimuli are easy to interpret and allow to systematically quantify chromatic RF properties of visual neurons in the mouse (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>) and zebrafish retina (<xref ref-type="bibr" rid="bib68">Zimmermann et al., 2018</xref>), as well as in primate V1 (<xref ref-type="bibr" rid="bib7">Chatterjee and Callaway, 2003</xref>). We presented visual stimuli to awake, head-fixed mice positioned on a treadmill while at the same time recording the population calcium activity within L2/3 of V1 using two-photon imaging (700×700 µm<sup>2</sup> recordings at 15 Hz). Visual stimulation was performed in the photopic light regime that predominantly activates cone photoreceptors. We back-projected visual stimuli on a Teflon screen using a custom projector with UV and green LEDs that allow differential activation of mouse cone photoreceptors (<xref ref-type="bibr" rid="bib13">Franke et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Franke et al., 2022</xref>). Functional recordings were obtained from posterior and anterior V1 (<xref ref-type="fig" rid="fig1">Figure 1c</xref>), encoding the upper and lower visual field (<xref ref-type="bibr" rid="bib50">Schuett et al., 2002</xref>), respectively.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Color noise stimulus identifies center-surround receptive field properties of mouse primary visual cortex (V1) neurons.</title><p>(<bold>a</bold>) Schematic illustrating experimental setup: Awake, head-fixed mice on a treadmill were presented with a center-surround color noise stimulus while recording the population calcium activity in L2/3 neurons of V1 using two-photon imaging. Stimuli were back-projected on a Teflon screen by a DLP-based projector equipped with a ultraviolet (UV) (390 nm) and green (460 nm) LED, allowing to differentially activate mouse cone photoreceptors. (<bold>b</bold>) Schematic drawing illustrating stimulus paradigm: UV and green center spot (UVC /GreenC) and surround annulus (UVS /Green<sub>S</sub>) flickered independently at 5 Hz according to binary random sequences. Top images depict example stimulus frames. See also <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. (<bold>c</bold>) Left side shows a schematic of V1 with a posterior and anterior recording field, and the recorded neurons of the posterior field overlaid on top of the mean projection of the recording. Right side shows the activity of <italic>n</italic>=150 neurons of this recording in response to the stimulus sequence shown in (<bold>b</bold>). (<bold>d</bold>) Event-triggered averages (ETAs) of six example neurons, shown for the four stimulus conditions. Gray: Original ETA. Black: Reconstruction using principal component analysis (PCA). See also <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>. Cells are grouped based on their ETA properties and include luminance-sensitive, color selective, and color-opponent neurons. Black dotted lines indicate time of response.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Verification of stimulus paradigm.</title><p>(<bold>a</bold>) Peak positions of spatial event-triggered averages (ETAs) estimated in response to a sparse noise stimulus (<italic>n</italic>=1434 cells, <italic>n</italic>=5 recording field, <italic>n</italic>=3 mice) relative to area of center spot of color noise stimulus (black). The peak position of the vast majority of cells lies within the center spot area. (<bold>b</bold>) Peak-normalized spatial ETA of four example neurons, capturing the center receptive field (RF) of the neurons. White solid line shows spatial ETA border (contour drawn at level = 0.25) and gray dot corresponds to peak of spatial ETA (see (<bold>a</bold>)). White dotted line indicates area of center spot of color noise stimulus shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Overlap values depict the overlap of the spatial ETA with the center spot area, ranging from 1 (spatial ETA lies within the center spot area) to 0 (spatial ETA outside center spot area). Bottom shows distribution of spatial ETA overlap values (<italic>n</italic>=1434 cells, <italic>n</italic>=5 recording field, <italic>n</italic>=3 mice). For most cells (83%), the spatial ETA exhibited an overlap with the center spot area of the color noise stimulus of more than 0.65. (<bold>c</bold>) Principal component analysis (PCA)-reconstructed ETAs of all neurons above quality threshold (<italic>n</italic>=3331 cells, <italic>n</italic>=6 recording fields, <italic>n</italic>=3 mice). (<bold>d</bold>) Distribution of Pearson correlation coefficients of center and surround ETAs, estimated by correlating center and surround ETAs for the ultraviolet (UV) (blue) and green stimulus condition, respectively. (<bold>e</bold>) Neurons recorded in a posterior and anterior recording field of an example mouse, color-coded based on the cells’ color preference for center (left) and surround ETA (right), quantified as spectral contrast. (<bold>f</bold>) Distribution of center (left) and surround ETA (right) spectral contrast values for posterior (black; <italic>n</italic>=1616 cells) and anterior (gray; <italic>n</italic>=1695 cells) neurons from <italic>n</italic>=3 mice. Spectral contrast significantly differed between posterior and anterior neurons, for both center (p&lt;0.001, two-sided two-sample t-test) and surround (p&lt;0.001, two-sided two-sample t-test). Spectral contrast significantly differed between center and surround, for both posterior (p&lt;0.001, two-sided two-sample t-test) and anterior neurons (p&lt;0.001, two-sided two-sample t-test).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Event-triggered averages (ETAs) and quality control.</title><p>(<bold>a</bold>) Raw calcium trace (black) and trace deconvolved with the calcium sensor’s decay function (red) for one example neuron. (<bold>b</bold>) ETAs estimated based on raw (left) and deconvolved calcium traces (right), respectively, for one example neuron. (<bold>c</bold>) Same as (<bold>c</bold>) but for another example neuron. (<bold>d</bold>) Histogram of quality measure for green center ETA versus ultraviolet (UV) center ETA. The quality measure indicates the ratio of ETA to baseline variance (the higher the better quality). (<bold>e</bold>) Fraction of neurons above quality threshold, for varying quality thresholds. All analyses were performed for neurons with a UV or green center ETA quality &gt;10 (red), corresponding to 63% of neurons. The blue indicates a more conservative threshold (&gt;50), corresponding to only the the best 25% of neurons. (<bold>f</bold>) Density plot of peak amplitudes of center (left) and surround (right) ETAs across all neurons above conservative threshold (blue in (<bold>e</bold>)). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Reconstruction of event-triggered averages (ETAs) using sparse principal component analysis (PCA).</title><p>(<bold>a</bold>) Mean reconstruction error across neurons (s.d. in gray) quantified as mean squared error (mse) between the original ETA and the reconstruction using sparse PCA for varying numbers of principal components (PCs). For further analysis, we used sparse PCA with eight PCs, because (i) adding more PCs only slightly decreased reconstruction error and (ii) PCs ETArted to capture noise. (<bold>b</bold>) PCs obtained from sparse PCA on the ETAs (<bold>c</bold>) used for reconstructions. (<bold>c</bold>) Distribution of reconstruction mse values for sparse PCA with eight PCs. (<bold>d</bold>) Original ETA (gray) and PCA reconstruction (black) for four example neurons with varying mse.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig1-figsupp3-v1.tif"/></fig></fig-group><p>To record from many V1 neurons simultaneously, we used a center stimulus size of 37.5 degrees visual angle in diameter, which is slightly larger than the center RFs we estimated for single V1 neurons (26.2±4.6 degrees visual angle in diameter) using a sparse noise paradigm (<xref ref-type="bibr" rid="bib28">Jones and Palmer, 1987</xref>). The disadvantage of this approach is that the stimulus is only roughly centered on the neurons’ center RFs. To reduce the impact of potential stimulus misalignment on our results, we used the following steps and controls. First, for each recording, we positioned the monitor such that the population RF across all neurons, estimated using a short sparse noise stimulus, lies within the center of the stimulus field of view. Second, we confirmed that this procedure results in good stimulus alignment at the level of individual neurons by using a longer sparse noise stimulus for a subset of experiments (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a, b</xref>). Specifically, we found that for the majority of tested neurons (83%), more than two-thirds of their center RF overlapped with the center spot of the color noise stimulus (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1b</xref>). Finally, we excluded neurons from analysis, which did not show significant center responses (<italic>n</italic>=1937 neurons excluded from <italic>n</italic>=5248; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2d, e</xref>), which may be caused by misalignment of the stimulus. Together, this suggests that the center spot and the surround annulus of the noise stimulus predominantly drive center (i.e. classical RF) and surround (i.e. extra-classical RF), respectively, of the recorded V1 neurons.</p><p>For analysis of the neurons’ stimulus preference, we first deconvolved the calcium traces (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2a</xref>) to account for the slow kinetics of the calcium sensor (e.g. <xref ref-type="bibr" rid="bib40">Pachitariu et al., 2018</xref>) and then used the deconvolved noise responses of each neuron to estimate an ‘event-triggered average’ (ETAs) for the four stimulus conditions - center (C) and surround (S) for both UV and green (Green<sub>C</sub>, UV<sub>C</sub>, Green<sub>S</sub>, UV<sub>C</sub>). Specifically, deconvolved neuronal responses were reverse-correlated with the stimulus trace and the raw ETAs were then transformed into a lower dimensional representation using principal component analysis (PCA; <xref ref-type="fig" rid="fig1">Figure 1d</xref> and <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). In other words, the ETA was obtained by summing the stimulus sequences that elicit an event (i.e. response), weighted by the amplitude of the response. Consequently, the absolute amplitude of the ETA correlated with the calcium amplitude and the ETA amplitudes of different stimulus conditions were comparable. Note that the deconvolution of raw calcium responses changes the kinetics of the ETAs, but not the neurons’ stimulus selectivity (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2b, c</xref>).</p><p>Using this approach, we obtained ETAs of <italic>n</italic>=3331 excitatory V1 neurons (<italic>n</italic>=6 recording fields, <italic>n</italic>=3 mice) with diverse center-surround stimulus preferences (<xref ref-type="fig" rid="fig1">Figure 1d</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1c</xref>). This included neurons sensitive to luminance contrast that did not discriminate between stimulus color (cells 1 and 2 in <xref ref-type="fig" rid="fig1">Figure 1d</xref>) and color selective cells only responding to one color of the stimulus (cells 3 and 4). In addition, some neurons exhibited color-opponency in the center (cells 5 and 6) or surround, meaning that a neuron prefers a stimulus of opposite polarity in the UV and green channel (e.g. UV-On and green-Off). To validate our experimental approach, we confirmed that the noise stimulus recovers well-described RF properties of mouse V1 neurons. First, the majority of neurons showed negatively correlated center and surround ETAs for both the UV and green channels (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1d</xref>), meaning that a neuron preferring an increment of light in the center (‘On’) favors a light decrement in the surround (‘Off’) and vice versa. This finding is consistent with On-Off center-surround antagonism of neurons in early visual areas, and has been described in both the mouse retina (e.g. <xref ref-type="bibr" rid="bib12">Franke et al., 2017</xref>) and mouse thalamus (e.g. <xref ref-type="bibr" rid="bib19">Grubb and Thompson, 2003</xref>). Second, neurons recorded in posterior and anterior V1 preferred UV and green stimuli (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1e, f</xref>), respectively, in line with the distribution of cone opsins across the retina (<xref ref-type="bibr" rid="bib56">Szél et al., 1992</xref>; <xref ref-type="bibr" rid="bib3">Baden et al., 2013</xref>) and previous cortical work (<xref ref-type="bibr" rid="bib47">Rhim et al., 2017</xref>; <xref ref-type="bibr" rid="bib14">Franke et al., 2022</xref>; <xref ref-type="bibr" rid="bib2">Aihara et al., 2017</xref>). This asymmetry in color preference was less pronounced for the surround (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1e, f</xref>), as has been reported for retinal neurons in mice (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>). Taken together, these results show that our experimental paradigm using a parametric luminance and color noise stimulus accurately captures known center-surround RF properties of cortical neurons in mice.</p></sec><sec id="s2-2"><title>Color contrast is represented by the RF center in a large number of mouse V1 neurons</title><p>To systematically study how color is represented by the population of mouse V1 neurons, we mapped each cell’s center and surround ETA into a two-dimensional space depicting neuronal sensitivity for luminance and color contrast (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). For each neuron, we extracted ETA peak amplitudes relative to baseline for all four stimulus conditions, with positive and negative peak amplitudes for On and Off stimulus preference, respectively. In this space, neurons sensitive to luminance contrast responding with the same polarity (i.e. On versus Off) to either color of the stimulus fall along the diagonal (cell 1 in <xref ref-type="fig" rid="fig2">Figure 2a</xref>) and color-opponent neurons scatter along the off-diagonal (cell 2). In addition, the neuronal selectivity for UV and green stimuli is indicated by the relative distance to the <italic>x</italic>- and <italic>y</italic>-axis (cell 3), respectively. We found that most V1 neurons were sensitive to luminance contrast and fell in the upper right or lower left quadrant along the diagonal, both for the center and surround component of V1 RFs (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Nevertheless, a substantial fraction of neurons (33.1%) preferred color-opponent stimuli and scattered along the off-diagonal in the upper left and lower right quadrants, especially for the RF center. We quantified the fraction of variance explained by the luminance versus the color axis across the neuronal population by performing PCA on the center and surround contrast space, respectively (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>). The luminance axis captured the major part of the variance of stimulus sensitivity for the RF surround (82%), while it explained less of the variance for the RF center (67%). As a result, one-third (33%) of the variance within the tested stimulus sensitivity space of the RF center was explained by the color axis. Please note that the percentages of variance explained by color and luminance axis correlate with the number of neurons located in the color (top left and bottom right) and luminance contrast quadrants (top right and bottom left), respectively. Our results were consistent for a more conservative quality threshold, which only considered the best 25% of neurons (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2e, f</xref>). In addition, the above results obtained by pooling data across animals were consistent within all three mice tested (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1a</xref>). Therefore, all the following analyses were based on data pooled across animals.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Strong neuronal representation of color in mouse primary visual cortex.</title><p>(<bold>a</bold>) Top left panel shows schematic drawing illustrating the green and ultraviolet (UV) contrast space used in the other panels and <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref>. Amplitudes below and above zero indicates an Off and On cell, respectively. Achromatic On and Off cells will scatter in the lower left and upper right quadrants along the diagonal (‘luminance contrast‘), while color-opponent cells will fall within the upper left and lower right quadrants along the off-diagonal (‘color contrast‘). Blue and green shading indicates stronger responses to UV and green stimuli, respectively. The three other panels show event-triggered averages (ETAs) of three example neurons (top) with the peak amplitudes (‘contrast‘) of their center (dot) and surround responses (triangle) indicated in the bottom. (<bold>b</bold>) Density plot of peak amplitudes of center (top) and surround (bottom) ETAs across all neurons (<italic>n</italic>=3331 cells, <italic>n</italic>=6 recording fields, <italic>n</italic>=3 mice). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated. For reproducibility across animals, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. The percentages of variance explained by color (off-diagonal) and luminance axis (diagonal) correlate with the number of neurons located in the color (top left and bottom right) and luminance contrast quadrants (top right and bottom left), respectively. Scale bars indicate the number of neurons in the 2D histogram. (<bold>c</bold>) Decoding discriminability of stimulus luminance (top) and stimulus color (bottom) based on center (black) and surround (gray) responses of different numbers of neurons. Decoding was performed using a support vector machine (SVM). Lines indicate the mean of 10-fold cross-validation (shown as dots). For luminance contrast, decoding discriminability was significantly different between center and surround for <italic>n</italic>=50 and <italic>n</italic>=100 neurons (t-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). For color contrast, decoding discriminability was significantly different between center and surround for all numbers of neurons tested, except <italic>n</italic>=1 neuron. Dotted horizontal lines indicate decoding accuracy in % for 60%, 80%, 90%, and 99%, with a change level of 50% corresponding to 0 bits.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Consistency across mice.</title><p>(<bold>a</bold>) Density plots of peak amplitudes of center (top) and surround (bottom) event-triggered averages (ETAs), separately for mouse 1 (<italic>n</italic>=2 recording fields, <italic>n</italic>=1170 cells), mouse 2 (<italic>n</italic>=2 recording fields, <italic>n</italic>=1102 cells), and mouse 3 (<italic>n</italic>=2 recording fields, <italic>n</italic>=1039 cells). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated. (<bold>b</bold>) Density plots of peak amplitudes of center ETAs for neurons encoding the upper visual field (posterior primary visual cortex [V1]; top) and lower visual field (anterior V1; bottom), respectively, separately for mouse 1 (<italic>n</italic>=2 recording fields, <italic>n</italic>=1170 cells), mouse 2 (<italic>n</italic>=2 recording fields, <italic>n</italic>=1102 cells), and mouse 3 (<italic>n</italic>=2 recording fields, <italic>n</italic>=1039 cells). Red lines correspond to axes of PCs obtained from a PCA on the center or surround data, with percentage of variance explained along the polarity and color axis indicated.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Neuronal representation of color in the mouse retina.</title><p>(<bold>a</bold>) Top panel shows schematic of a flat-mounted ex vivo retina, with distribution of all recording fields (<italic>n</italic>=88 fields) from an available dataset (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>) that has recorded the responses of ganglion cell layer (GCL) cells in response to center and surround flicker of ultraviolet (UV) and green LED. The bottom panel shows one example scan field of the GCL with 64×64 pixels, recorded at 7.8 Hz. The cells indicated with numbers are shown in panel (<bold>b</bold>). D: dorsal, T: temporal. (<bold>b</bold>) Event-triggered averages (ETAs) of three example neurons, concatenated across the four stimulus conditions (center and surround for UV and green flicker). Gray: Original ETA. Black: Reconstruction using principal component analysis (PCA). Similar to primary visual cortex (V1), there are luminance-sensitive neurons (cell 1) and color selective neurons (cells 2 and 3). In contrast to V1, color-opponent neurons were rare. (<bold>c</bold>) This panel shows the ETA of neuron 1 in (<bold>b</bold>), with its peak amplitudes of center and surround indicated in the luminance and contrast sensitivity space. (<bold>d</bold>) Density plot of peak amplitudes of center (top) and surround (bottom) ETAs across all retinal ganglion cell (RGC) (<italic>n</italic>=3215 cells, <italic>n</italic>=88 recording fields, <italic>n</italic>=18 mice). Red lines correspond to axes of principal components (PCs) obtained from a PCA on the center or surround data, with percentage of variance explained along the polarity and color axis indicated.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig2-figsupp2-v1.tif"/></fig></fig-group><p>We confirmed and quantified the pronounced color-opponency in mouse V1 we observed based on the neurons’ preferred stimuli (i.e. ETAs) using an independent decoding analysis. For that, we trained a nonlinear support vector machine (SVM) to decode stimulus luminance (On versus Off) or color (UV versus green) based on the recorded and deconvolved neuronal activity. Decoding was performed using 10-fold cross-validation and decoding accuracy (in %) was transformed into mutual information (in bits). The discriminability of luminance contrast rapidly increased with the number of neurons used by the SVM decoder, and saturated close to perfect discriminability (&gt;0.92 bits, i.e. &gt;99% accuracy) when including more than 1000 neurons (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). Interestingly, decoding performance was similar for center and surround stimuli, indicating that V1 responses are as informative about luminance contrast in the center as in the surround. We next trained a decoder to discriminate stimulus color. The decoding performance was lower for stimulus color compared to stimulus luminance (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), consistent with the finding described above that V1 neurons are more sensitive to luminance than color contrast. In addition, discriminability of stimulus color was significantly better for stimuli presented in the RF center compared to stimuli shown in the RF surround, thereby verifying our ETA results. Together, our results demonstrate that for photopic light levels, neurons in mouse visual cortex strongly encode color features of the visual input in their RF center, while the RF surround predominantly captures luminance contrast.</p><p>Next, we tested to what extent the strong representation of color by the center component of V1 RFs was inherited by color-opponency present in the center RF of retinal output neurons (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>). We tested this by using a publicly available dataset of retinal ganglion cell responses to a center and surround color flicker stimulus (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>), similar to the one used here, but with center and surround stimulus sizes adjusted to the smaller RF sizes of retinal neurons. We embedded each cell’s center and surround ETA into the luminance and color contrast space described above (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). We found that at the level of the retinal output, the color axis explained only 12% of the variance in the tested sensitivity space for the RF center (10 degrees visual angle), which is much lower than what we observed for V1 center RFs (37.5 degrees visual angle). The low fraction of center color-opponent retinal ganglion cells is in line with a recent study that characterized RF properties of these neurons using natural movies recorded in the mouse’s natural environment (<xref ref-type="bibr" rid="bib23">Hoefling et al., 2022</xref>). Collectively, these findings suggest that the pronounced center color-opponency in V1 neurons cannot be solely attributed to the color-opponency present in the RF center of retinal ganglion cells. It likely depends on the activation of both the center and surround of retinal neurons, as well as a potential remapping of retinal center and surround RFs in downstream processing stages.</p></sec><sec id="s2-3"><title>The neuronal representation of color in mouse V1 decreases with lower ambient light levels</title><p>Previous studies have reported varying numbers of color-opponent neurons in mouse visual areas, ranging from very few in mouse dLGN (<xref ref-type="bibr" rid="bib9">Denman et al., 2017</xref>) and V1 (<xref ref-type="bibr" rid="bib57">Tan et al., 2015</xref>) to a large number in the thalamus (<xref ref-type="bibr" rid="bib37">Mouland et al., 2021</xref>), visual cortex (<xref ref-type="bibr" rid="bib48">Rhim and Nauhaus, 2023</xref>), and the retina (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>). In part, this discrepancy regarding the role of color for visual processing in mice is likely due to the fact that different studies have used different light levels, resulting in varying activations of rod and cone photoreceptors that are both involved in chromatic processing. To systematically study how ambient light levels affect the neuronal representation of color in mouse visual cortex, we repeated our experiments with the noise stimulus performed in photopic conditions (approx. 15,000 photoisomerizations (P*) per cone and second) in high (approx. 400 P* per cone and second) and low mesopic light conditions (approx. 50 P* per cone and second). The high mesopic light condition is expected to equally activate rod and cone photoreceptors, while the low mesopic condition largely drives rod photoreceptors, with only a small cone contribution. Indeed, decreasing ambient light levels resulted in a reduction of UV sensitivity in posterior V1 neurons (<xref ref-type="fig" rid="fig3">Figure 3a</xref>), indicative for a gradual activation shift from UV-sensitive cones to rods, which are green sensitive. The neuronal representation of color greatly decreased when reducing ambient light levels: Both the fraction of ETA variance explained by the color axis (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) and the decoding discriminability of stimulus color dropped significantly (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). The drop in decoding discriminability was not due to lower signal-to-noise levels for the mesopic light levels, as the decoding of stimulus luminance was not significantly higher for the photopic condition compared to the mesopic conditions (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). Across all light levels tested, the RF surround of V1 neurons was less informative about stimulus color, resulting in 40–60% lower discriminability in the surround compared to the center (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Interestingly, even for the lowest light level, where only a small fraction of ETA variance was explained by stimulus color (<xref ref-type="fig" rid="fig3">Figure 3b</xref>), V1 neurons reliably encoded color information (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). This suggests that weak cone activation in low mesopic light levels is sufficient to extract color features from the visual input. In summary, our results demonstrate that the neuronal representation of color in mouse visual cortex greatly depends on ambient light levels, and is strongest for photopic light levels that predominantly drive cone photoreceptors.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Reduced representation of color contrast in mouse primary visual cortex (V1) for lower ambient light levels.</title><p>(<bold>a</bold>) Distribution of spectral contrast of center event-triggered averages (ETAs) of all neurons recorded in posterior V1, for photopic (top, <italic>n</italic>=1616 cells, <italic>n</italic>=3 recording fields, <italic>n</italic>=3 mice), high mesopic (middle, <italic>n</italic>=1485 cells, <italic>n</italic>=3 recording fields, <italic>n</italic>=3 mice), and low mesopic (bottom, <italic>n</italic>=1295 cells, <italic>n</italic>=3 recording fields, <italic>n</italic>=3 mice) ambient light levels. Black dotted lines indicate mean of distribution. Spectral contrast significantly differed across all combinations of light levels (t-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). The triangle on the right indicates ultraviolet (UV) sensitivity of the neurons, which is decreasing with lower ambient light levels. (<bold>b</bold>) Density plot of peak amplitudes of center (left) and surround (right) ETAs. Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA) on the center or surround data, with percentage of variance explained along the polarity and color axis indicated. Top row shows high mesopic (<italic>n</italic>=3522 cells, <italic>n</italic>=6 recording fields, <italic>n</italic>=3 mice) and bottom row low mesopic (<italic>n</italic>=2705 cells, <italic>n</italic>=6 recording fields, <italic>n</italic>=3 mice) light levels. The percentages of variance explained by color (off-diagonal) and luminance axis (diagonal) correlate with the number of neurons located in the color (top left and bottom right) and luminance contrast quadrants (top right and bottom left), respectively. Scale bars indicate the number of neurons in the 2D histogram. (<bold>c</bold>) Discriminability (in bits) of luminance contrast (On versus Off) for the center across the three light levels tested, obtained from training support vector machine (SVM) decoders based on recorded noise responses of V1 neurons. Right plot shows the discriminability of luminance contrast for <italic>n</italic>=500 neurons for center and surround. Dots show decoding performance of 10 train/test trial splits. For <italic>n</italic>=500 neurons, decoding discriminability of the center was not significantly different across light levels (t-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). The surround discriminability was significantly lower than the center for the photopic condition. Dotted horizontal lines indicate decoding accuracy in % for 60%, 80%, 90%, and 99%, with a change level of 50% corresponding to 0 bits. (<bold>d</bold>) Like (<bold>c</bold>), but showing discriminability of color contrast (green versus UV). Decoding discriminability was significantly different between center and surround for all three light levels. In addition, discriminability for the center was significantly different between photopic and mesopic conditions, but not between the two mesopic conditions (t-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). Dotted horizontal lines indicate decoding accuracy in % for 60%, 80%, 90%, and 99%, with a change level of 50% corresponding to 0 bits.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig3-v1.tif"/></fig></sec><sec id="s2-4"><title>Cortical representation of color changes across the visual field</title><p>Chromatic and achromatic features present in natural scenes systematically vary across the visual field, with notable differences between regions below and above the horizon (<xref ref-type="bibr" rid="bib39">Nilsson et al., 2022</xref>; <xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>). Recently, it has been demonstrated that color contrast in scenes from the mouse’s natural environment is enriched in the upper visual field (<xref ref-type="fig" rid="fig4">Figure 4a</xref>; <xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Abballe and Asari, 2022</xref>). To encode the sensory input efficiently, these scene statistics should ideally be reflected in the neuronal representations, as has been observed at the level of the mouse retina (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>). To study how the representation of color changes across the visual field in mouse V1, we separately analyzed the neurons recorded in posterior and anterior V1, which encode visual information from the upper and lower visual field, respectively. We focused this analysis on the RF center because V1 surround RFs were on average predominantly explained by luminance contrast (<xref ref-type="fig" rid="fig2">Figure 2</xref>). We found that the color axis explained twice as much ETA variance in posterior compared to anterior V1 (<xref ref-type="fig" rid="fig4">Figure 4b</xref>): It captured 39% of the variance in the upper visual field and only 19% of the variance in the lower visual field. This finding was consistent across animals (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1b</xref>). In line with this, the discriminability of stimulus color was significantly higher when using the responses of posterior V1 neurons for decoding (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). Together, this revealed a stronger cortical representation of color in posterior than anterior mouse visual cortex, which might be an adaptation to efficiently encode the enriched color contrast in the upper visual field of mouse natural scenes (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cortical representation of color changes across visual space.</title><p>(<bold>a</bold>) Natural scene captured in the natural environment of mice using a custom-built camera adjusted to the mouse’s spectral sensitivity (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>). Dashed line indicates the horizon and separates the scene into lower and upper visual field. Previous studies <xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Abballe and Asari, 2022</xref> have reported higher color contrast in the upper compared to the lower visual field. (<bold>b</bold>) Density plot of peak amplitudes of center event-triggered averages (ETAs) across neurons recorded in posterior (top) and anterior primary visual cortex (V1) (bottom). Red lines correspond to axes of principal components (PCs) obtained from a principal component analysis (PCA), with percentage of variance explained along the polarity and color axis indicated. For reproducibility across animals, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. The percentages of variance explained by color (off-diagonal) and luminance axis (diagonal) correlate with the number of neurons located in the color (top left and bottom right) and luminance contrast quadrants (top right and bottom left), respectively. Scale bars indicate the number of neurons in the 2D histogram. (<bold>c</bold>) Discriminability (in bits) of color contrast (ultraviolet [UV] or green) for neurons recorded in posterior (blue) and anterior V1 (green), obtained from training support vector machine (SVM) decoders based on recorded noise responses of V1 neurons. The decoding discriminability was significantly different between anterior and posterior neurons (t-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). Dotted horizontal lines indicate decoding accuracy in % for 60%, 80%, 90%, and 99%, with a change level of 50% corresponding to 0 bits.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig4-v1.tif"/></fig></sec><sec id="s2-5"><title>Asymmetric distribution of color response types explains higher color sensitivity in posterior V1</title><p>Next, we investigated the mechanism underlying this asymmetry in color encoding across mouse visual cortex. In the mouse retina, different retinal ganglion cell types are differentially distributed across the retina and, therefore, asymmetrically sample the visual space (reviewed in <xref ref-type="bibr" rid="bib5">Baden et al., 2020</xref>). For example, W3 cells that have been linked to aerial predator detection exhibit the highest density in the ventral retina looking at the sky (<xref ref-type="bibr" rid="bib66">Zhang et al., 2012</xref>). Similarly, we hypothesized that the difference in decoding performance of stimulus color in posterior and anterior V1 might be due to an asymmetric distribution of functional neuron types sensitive to color versus luminance contrast. To test this, we clustered the ETAs of all neurons into ‘functional response types’ and quantified the distribution of the identified response types across cortical position. Specifically, we used the features extracted from the ETAs by PCA (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>) and clustered the feature weights into 17 response types using a Gaussian mixture model (GMM; <xref ref-type="fig" rid="fig5">Figure 5a</xref> and <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). We used 17 components for the GMM because this resulted in the best model on held-out test data (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a</xref>), although the performance was relatively flat for a wide range of components. The mean assignment accuracy of generated ground-truth labels was 89.2% (±6%) and all response types were present in all mice (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b, c</xref>), indicating that the response types are well separated and robust. The response types greatly differed with respect to functional properties, such as color-opponency, response polarity, and surround antagonism (<xref ref-type="fig" rid="fig5">Figure 5a</xref>), and, therefore, covered distinct sub-spaces of the color and luminance sensitivity space (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1d</xref>). Approximately half of the response types were sensitive to luminance contrast (types 1–8) and exhibited different response polarities and surround strengths. The other half consisted of types with a strong selectivity for UV or green center stimuli (types 9–13) and color-opponency in the center (types 14–17).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Asymmetric distribution of color response types explains higher color sensitivity in posterior primary visual cortex (V1).</title><p>(<bold>a</bold>) Clustering result of the Gaussian mixture model with <italic>n</italic>=17 clusters (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref> for details). Model input corresponded to the weights of the principal components used for reconstructing event-triggered averages (ETAs). Left panel shows ETAs of all cells, respectively, sorted by cluster assignment. Right panel shows mean ETA of each cluster (s.d. shading in gray). Clusters are sorted based on broad response categories, which are indicated on the right. (<bold>b</bold>) Left shows distribution of cells assigned to three different clusters (color) in a posterior and anterior recording field of one example animal. Gray dots show cells assigned to other clusters. Distribution index for each cluster is indicated below. Right shows the mean distribution index per cluster, with different marker shapes indicating the indices for individual animals. Zero indicates an even distribution across anterior and posterior V1 and values above and below zero indicate that cells are enriched in anterior and posterior V1, respectively. Dotted horizontal lines at –0.33/0.33 indicate twice as many cells in posterior than anterior cortex and vice versa. (<bold>c</bold>) Histograms of peak amplitudes of center ETAs for clusters that are evenly distributed across V1 (left, distribution index &gt;–0.33 and &lt;0.33), enriched in anterior V1 (middle, distribution index) and enriched in posterior V1 (right). Cluster means and s.d. are indicated in color. Dotted lines correspond to axes of PCs obtained from a principal component analysis (PCA), with percentage of variance explained along the luminance and color axis indicated. The percentages of variance explained by color (off-diagonal) and luminance axis (diagonal) correlate with the number of neurons located in the color (top left and bottom right) and luminance contrast quadrants (top right and bottom left), respectively. Scale bars indicate the number of neurons in the 2D histogram. (<bold>d</bold>) Discriminability (in bits) of stimulus color contrast based on response types enriched in anterior (black) and posterior (gray) V1. Dots show decoding performance across 10 train/test trial splits. Decoding discriminability was significantly different between anterior- and posterior-enriched types for all numbers of neurons tested (t-test for unpaired data, p-value was adjusted for multiple comparisons using Bonferroni correction). Dotted horizontal lines indicate decoding accuracy in % for 60%, 80%, and 90%, with a change level of 50% corresponding to 0 bits. (<bold>e</bold>) Noise images with or without a ‘predator‘-like dark object in the UV channel were convolved with simulated center receptive fields (RFs), depicting the mean amplitudes of the green and UV center ETA per response type (shown for types 4 and 14). The resulting activity maps were summed and thresholded to simulate responses to <italic>n</italic>=1000 noise and object scenes. (<bold>f</bold>) Discriminability (in bits) of the presence of a ‘predator’-like dark object in the UV channel per response type. Error bars show s.d. across 10 train/test trial splits.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Unsupervised clustering of spike-triggered averages.</title><p>(<bold>a</bold>) Log likelihood of Gaussian mixture models (GMMs) with varying numbers of clusters. Model input corresponded to the weights of the principal components used for reconstructing event-triggered averages (ETAs) (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Black solid trace corresponds to the mean across 10 train/test data splits (gray dots). Black dotted trace indicates maximum log likelihood for <italic>n</italic>=17 clusters. (<bold>b</bold>) Box plot shows distribution of assignment accuracy across clusters, obtained from comparing ground-truth labels generated using mean and covariance matrix of each Gaussian (i.e. cluster) with labels predicted by the pre-trained GMM (see also <xref ref-type="bibr" rid="bib61">Tolias et al., 2007</xref>). Right panel shows confusion matrix of true versus predicted labels. Please note that false positives are usually across clusters with similar response properties. (<bold>c</bold>) Number of cells assigned to the different clusters, sorted by animal. d, scatter plot of peak amplitudes of center (top) and surround (bottom) ETAs across all neurons, with mean and s.d. of each cluster from <xref ref-type="fig" rid="fig5">Figure 5a</xref> indicated in color.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-89996-fig5-figsupp1-v1.tif"/></fig></fig-group><p>We next investigated the distribution of individual response types across anterior and posterior V1 by computing a cortical distribution index (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This index was –1 and 1 if all cells of one response type were located in posterior and anterior V1, respectively, and 0 if the respective response type was evenly distributed across cortex. We found that approximately half of the response types were equally distributed in mouse V1 (distribution index from –0.3 to 0.3), including mostly response types sensitive to luminance contrast. Interestingly, response types with green-Off/UV-On color-opponency were also uniformly spread across the anterior-posterior axis of mouse V1, suggesting that a neuronal substrate supporting color vision exists in both the upper and lower visual field. Response types enriched in anterior V1 (distribution index &gt;0.3) fell along the luminance contrast axis but showed a preference for green center stimuli, consistent with the higher green sensitivity of cone photoreceptors sampling the ground (<xref ref-type="bibr" rid="bib3">Baden et al., 2013</xref>). Similarly, as expected from the high density of UV-sensitive cone photoreceptors in the ventral retina (<xref ref-type="bibr" rid="bib3">Baden et al., 2013</xref>), one response type strongly enriched in the posterior cortex (distribution index &lt;0.3) preferred UV in the RF center. To our surprise, response types with a green-On/UV-Off color-opponency were almost exclusively confined to posterior V1. As a result of this, the color axis explained 73% of ETA variance for the response types enriched in posterior cortex, while it explained only 17% for the anterior-enriched response types. We confirmed the higher sensitivity for color versus luminance contrast of posterior response types by showing that their decoding discriminability of color was significantly better than that for anterior response types (<xref ref-type="fig" rid="fig5">Figure 5d</xref>). Together, these results demonstrate that the asymmetry in neuronal color tuning across cortical position we report in mice can be explained by an uneven distribution of color-opponent response types.</p><p>We next speculated about the computational role of the green-On/UV-Off color-opponent response types largely present in posterior V1. As most predators are expected to approach the mouse from above, color-opponency in the upper visual field could well support threat detection. Especially for visual scenes with inhomogeneous illumination (e.g. in the forest), which result in large intensity fluctuations at the photoreceptor array, color-opponent RF structures may result in a more reliable signal (discussed in <xref ref-type="bibr" rid="bib36">Maximov, 2000</xref>; <xref ref-type="bibr" rid="bib29">Kelber et al., 2003</xref>). To test this prediction, we used parametric stimuli inspired by noisy natural scenes, containing only noise, or a dark ellipse of varying size, angle, and position on top of noise (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). The dark object had a higher contrast in the UV than green channel, as it has been shown that objects in the sky (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>), underwater (<xref ref-type="bibr" rid="bib8">Cronin and Bok, 2016</xref>; <xref ref-type="bibr" rid="bib34">Losey et al., 1999</xref>), or in the snow (<xref ref-type="bibr" rid="bib62">Tyler et al., 2014</xref>) are often more visible in the UV than the green wavelength range. For each response type, we first simulated responses to these scenes based on the type’s luminance and color contrast sensitivity of the RF center using a simple linear-nonlinear model (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). We found that all UV-Off types responded to the predator-like object, but the green-On/UV-Off response types did so more selectively, without also responding to the noise scenes. This selectivity arises because driving color-opponent neurons requires the mean intensity of the UV and green channels within the RF to have opposite polarity, a condition less likely to occur in noisy scenes compared to when the mean intensity is lower than the background. We then used the simulated responses to train an SVM decoder to discriminate between object and noise-only scenes. While all Off-center response types sensitive to luminance contrast could decode the dark object, the two best performing types corresponded to the green-On/UV-Off response types enriched in posterior V1 (<xref ref-type="fig" rid="fig5">Figure 5f</xref>). Interestingly, the reason for their good performance was the absence of responses to the noise scenes, rather than strong responses to the object scenes per se (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). Our results suggest that functional neuron types in mouse V1 with distinct color properties unevenly sample different parts of the visual scene, and might thereby serve a distinct role in driving visually guided behavior like predator detection.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Here, we found that a large fraction of neurons in mouse visual cortex encode color features of the visual input in their RF center. Color-opponency was strongest for photopic light levels and especially pronounced in posterior V1 encoding the sky. This asymmetry in color processing across visual space was due to an inhomogeneous distribution of color-opponent response types, with green-On/UV-Off response types predominantly being present in posterior V1. Using a simple model and natural scene inspired parametric stimuli, we showed that this type of color-opponency may enhance the detection of aerial predators in noisy daylight scenes.</p><sec id="s3-1"><title>Neuronal correlates of color vision in mice</title><p>In most species, color vision is mediated by comparing signals from different cone photoreceptor types sensitive to different wavelengths (reviewed in <xref ref-type="bibr" rid="bib4">Baden and Osorio, 2019</xref>). This includes circuits with cone-type selective wiring present in many vertebrate species and circuits with random and cone type-unselective wiring like red-green opponency in primates. Recently, it has been demonstrated that there is extensive rod-cone opponency in mice, comparing signals from UV-sensitive cones in the ventral retina to rod signals (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Joesch and Meister, 2016</xref>; <xref ref-type="bibr" rid="bib48">Rhim and Nauhaus, 2023</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>). In the retina, this mechanism relies on integrating information from cone signals in the RF center with rod signals in the RF surround (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="bib26">Joesch and Meister, 2016</xref>). Interestingly, there is also evidence for rod-cone opponency in monochromatic humans (<xref ref-type="bibr" rid="bib46">Reitner et al., 1991</xref>), suggesting that a neuronal circuit to compare rod and cone signals exists in other mammals as well. At this point, it is still unclear to what extent behavioral color discrimination in mice (<xref ref-type="bibr" rid="bib10">Denman et al., 2018</xref>; <xref ref-type="bibr" rid="bib25">Jacobs et al., 2004</xref>) is driven by rod-cone versus cone-cone comparisons.</p><p>Here, we found that the neuronal representation of color in mouse visual cortex is most prominent for photopic light levels and decreases for mesopic conditions, indicating that color-opponency in mouse V1 is at least partially mediated by the comparison of cone signals and not purely by cone-rod comparisons. Our result is consistent with a recent study reporting pronounced cone-mediated color-opponency in mouse dLGN (<xref ref-type="bibr" rid="bib37">Mouland et al., 2021</xref>). In our experimental paradigm, dissecting the relative contribution of rods and M-cones in color-opponency of mouse V1 neurons was not possible, due to the highly overlapping wavelength sensitivity profiles of mouse M-opsin and Rhodopsin. However, it is very likely that rods contribute to the prominent color-opponent neuronal responses we observed in mouse V1. This is especially true for posterior V1 receiving input from the ventral retina where cone-cone comparisons are challenging due to the co-expression of S-opsin in M-cones (<xref ref-type="bibr" rid="bib56">Szél et al., 1992</xref>; <xref ref-type="bibr" rid="bib3">Baden et al., 2013</xref>). The involvement of rods in generating color-opponent responses is supported by retinal data (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="bib26">Joesch and Meister, 2016</xref>) and a recent study performed in mouse visual cortex showing that color-opponency in posterior V1 is best explained by a model that compares S-opsin with Rhodopsin (<xref ref-type="bibr" rid="bib48">Rhim and Nauhaus, 2023</xref>). Surprisingly, we found that the mouse visual system still extracts color information for relatively low light levels present during early dusk and dawn, potentially by comparing rod to remaining cone signals. Future studies will tell whether color-opponency under dim light, as observed here for low mesopic conditions, require a specific neuronal pathway amplifying the relatively weak cone signals to encode color features present in the environment.</p><p>The results from our study, together with recent findings across the visual hierarchy of mice (<xref ref-type="bibr" rid="bib37">Mouland et al., 2021</xref>; <xref ref-type="bibr" rid="bib48">Rhim and Nauhaus, 2023</xref>; <xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>), demonstrate a pronounced neuronal representation of color in mouse visual brain areas that is mediated by both cone-cone and cone-rod comparisons. While this highlights the relevance of color information in mouse vision, it remains unclear as to how mice use color vision to inform natural behaviors. Two behavioral studies using parametric stimuli and relatively simple behavioral paradigms have shown that mice can discriminate different colors (<xref ref-type="bibr" rid="bib25">Jacobs et al., 2004</xref>; <xref ref-type="bibr" rid="bib10">Denman et al., 2018</xref>), at least in their central and upper visual field (<xref ref-type="bibr" rid="bib10">Denman et al., 2018</xref>). Here, we found that green-Off/UV-On color-opponency was equally distributed across cortex, suggesting that there is a neuronal substrate for color vision in mice across the entire visual field. In contrast, green-On/UV-Off color-opponency was confined to posterior V1, where it might aid the detection of aerial predators present in cluttered and noisy daylight scenes, such as in the forest. Testing this hypothesis and further elucidating the role of color vision in mouse natural behaviors will require combining more unrestrained behavioral paradigms with ecologically relevant stimuli.</p></sec><sec id="s3-2"><title>Limitations of the stimulus and analysis paradigm</title><p>To study color processing in the mouse V1, we employed a parametric center-surround color flicker stimulus, similar to those used in previous studies (<xref ref-type="bibr" rid="bib7">Chatterjee and Callaway, 2003</xref>; <xref ref-type="bibr" rid="bib68">Zimmermann et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>). The advantage of this relatively simple approach is that it allows for clear interpretation of neuronal responses using linear methods, such as the spike-triggered average method (<xref ref-type="bibr" rid="bib51">Schwartz et al., 2006</xref>). However, a limitation of the linear ETA approach employed here is its potential inadequacy in capturing the stimulus selectivity of nonlinear neurons. Our linear analysis provides preliminary insights into color representations in the mouse visual cortex, which future studies could enhance with nonlinear methods to achieve a more comprehensive understanding. Additionally, the ETA method may be less effective for neurons that respond to both On and Off stimuli with similar amplitudes. Although we cannot rule out that our analysis may have been biased against On-Off neurons, the observation that over 62% of neurons exhibited a significant ETA for their RF center suggests that we captured a substantial proportion of V1 neurons.</p><p>For an ETA analysis, the stimulus should ideally be aligned to the center RF of each neuron, which requires detailed RF mapping of individual neurons. As this procedure is relatively time consuming and low throughput, we instead used a center stimulus that was slightly larger than RFs of single neurons, and was centered on the mean RF across a population of V1 neurons. To reduce the impact of potential stimulus misalignment on the single cell level on our results, we used different experimental steps and controls, such as confirming that the RF center of most recorded neurons greatly overlaps with the center noise stimulus. The fact that response types identified using an automated clustering approach were consistent across animals suggests that stimulus alignment did not significantly contribute to the neurons’ visual responses. Nevertheless, we cannot exclude that the stimulus was misaligned for a subset of the recorded neurons used for analysis. Stimulus misalignment might have contributed to single cells not having surround ETAs, due to simultaneous activation of antagonistic center and surround RF components by the surround stimulus.</p></sec><sec id="s3-3"><title>Asymmetric processing of color information across the visual field</title><p>The spatial arrangements of sensory neurons are ordered in a way that encodes particular characteristics of the surrounding environment. One classical example in the visual system is that the density of all retinal output neurons increases and their dendritic arbor size decreases toward retinal locations with higher sampling frequency, such as the fovea in primates and the area centralis in carnivores (discussed in <xref ref-type="bibr" rid="bib41">Peichl, 2005</xref>). More recent research has uncovered how the visual circuits in certain species are customized to suit the statistics of the visual information they receive, including the distribution of spatial, temporal, and spectral information, as well as the specific requirements of their behavior (discussed in <xref ref-type="bibr" rid="bib5">Baden et al., 2020</xref>). For example, a study in zebrafish larvae showed that UV cones in one particular retinal location are specifically tuned for UV-bright objects, thereby supporting prey capture in their upper frontal visual field (<xref ref-type="bibr" rid="bib64">Yoshimatsu et al., 2020</xref>).</p><p>Here, we found that there is a pronounced asymmetry in how color is represented across visual space in mouse V1. A similar asymmetry in color processing was reported at the level of the mouse retina (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>) and dLGN (<xref ref-type="bibr" rid="bib37">Mouland et al., 2021</xref>), and has been linked to an inhomogeneous distribution of color contrast across natural scenes from the mouse’s environment (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>; <xref ref-type="bibr" rid="bib1">Abballe and Asari, 2022</xref>). Specifically, it has been speculated that the higher color contrast present in the upper visual field of natural scenes captured in the natural habitat of mice might have driven superior color-opponency in the ventral retina (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>), thereby supporting color discrimination in the sky (<xref ref-type="bibr" rid="bib10">Denman et al., 2018</xref>). Our results extend these previous studies by demonstrating that the asymmetry across visual cortex can be explained by the asymmetric distribution of response types with distinct color tuning in their RF center, and by linking them to a neuronal computation relevant for the upper visual field, namely the detection of aerial predators.</p><p>At the level of the mouse retina, color-opponency is largely mediated by center-surround interactions (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>; <xref ref-type="bibr" rid="bib26">Joesch and Meister, 2016</xref>), with only a few neurons exhibiting color-opponency in their centers (<xref ref-type="bibr" rid="bib23">Hoefling et al., 2022</xref>). Consistent with this, we found that the pronounced representation of color by the center component of V1 RFs was not solely inherited from the color-opponency present in the RF centers of retinal output neurons. Similarly, a recent study concluded that the extensive and sophisticated color processing in the mouse LGN cannot be fully explained by the proposed retinal opponency mechanisms (<xref ref-type="bibr" rid="bib37">Mouland et al., 2021</xref>). However, we compared properties of the center components of retinal (3–10 degrees visual angle) and V1 RFs (10–25 degrees visual angle), which differ in size. Therefore, the integration of center and surround retinal signals might still contribute to the color-opponency observed in downstream visual areas, such as the mouse V1, as observed in this study. Generally, comparing ex vivo retinal data with in vivo cortical data is challenging, not only due to differences in RF size but also due to varying levels of adaptation.</p></sec><sec id="s3-4"><title>Strategies of color processing across animal species: distributed versus specialized code</title><p>In primates, physiological and anatomical evidence suggest that a small number of distinct retinal cell types transmit color information to downstream visual areas (reviewed in <xref ref-type="bibr" rid="bib60">Thoreson and Dacey, 2019</xref>), where the neuronal representation of color remains partially segregated from the representation of other visual features like form (<xref ref-type="bibr" rid="bib32">Livingstone and Hubel, 1988</xref>; <xref ref-type="bibr" rid="bib65">Zeki, 1978</xref>, but see <xref ref-type="bibr" rid="bib16">Garg et al., 2019</xref>). For example, color-sensitive neurons in primary and secondary visual cortex are enriched in the so-called ‘blob’ (<xref ref-type="bibr" rid="bib24">Hubel and Livingstone, 1987</xref>) and ‘inter-stripe’ regions (<xref ref-type="bibr" rid="bib11">DeYoe and Van Essen, 1985</xref>), respectively. Interestingly, in other vertebrate species, color processing is distributed across many neuron types and cannot easily be separated from the processing of other visual features. In zebrafish, birds, <italic>Drosophila,</italic> and mice, a large number of retinal output types encode information about stimulus color (<xref ref-type="bibr" rid="bib52">Seifert et al., 2023</xref>; <xref ref-type="bibr" rid="bib67">Zhou et al., 2020</xref>; <xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Khani and Gollisch, 2021</xref>), in addition to each type’s preferred feature like direction of motion. In addition, there is evidence for distributed processing of color in visual areas downstream to the retina in zebrafish (<xref ref-type="bibr" rid="bib20">Guggiana Nilo et al., 2021</xref>), mice (<xref ref-type="bibr" rid="bib48">Rhim and Nauhaus, 2023</xref>; <xref ref-type="bibr" rid="bib37">Mouland et al., 2021</xref>), <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib33">Longden et al., 2023</xref>), and tree shrew (<xref ref-type="bibr" rid="bib27">Johnson et al., 2010</xref>). Our results demonstrate a prominent neuronal representation of color in mouse V1, which is distributed across many neurons and multiple response types.</p><p>What might be the benefit of such a distributed code of color processing? It is important to note that chromatic signals may not only be used for color discrimination per se, but instead different spectral channels might facilitate the extraction of specific features from the environment. For example, it has been shown that the UV wavelength range aids the detection of objects like prey, predators, and food (reviewed in <xref ref-type="bibr" rid="bib8">Cronin and Bok, 2016</xref>) by increasing their contrast, as recently shown for leaf surface contrasts in forest environments (<xref ref-type="bibr" rid="bib59">Tedore and Nilsson, 2019</xref>). Indeed, it is hypothesized that different photoreceptor types sensitive to distinct wavelength bands did not evolve to support color discrimination, but instead to reduce lighting noise in the natural environment of early vertebrates (discussed in <xref ref-type="bibr" rid="bib36">Maximov, 2000</xref>; <xref ref-type="bibr" rid="bib29">Kelber et al., 2003</xref>). In line with this idea, our analysis suggests that green-On/UV-Off color-opponency might facilitate the detection of predatory-like dark objects in the UV channel by reducing the neurons’ activation to noise, rather than increasing the neurons’ activation to the object. If chromatic signals are predominantly used to boost contrast of specific aspects of the environment, it might make sense to widely distribute chromatic tuning and color-opponency across visual neurons. Further experiments and analysis will uncover the computational relevance of the pronounced and distributed color representations observed in mice and other vertebrate species.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Neurophysiological experiments</title><p>All procedures were approved by the Institutional Animal Care and Use Committee of Baylor College of Medicine (animal protocol number: AN-4703). Owing to the explanatory nature of our study, we did not use randomization and blinding. No statistical methods were used to predetermine sample size.</p><p>Mice of either sex (<italic>Mus musculus</italic>, <italic>n</italic>=9; 2–5 months of age) expressing GCaMP6s in excitatory neurons via Slc17a7-Cre and Ai162 transgenic lines (stock number 023527 and 031562, respectively; The Jackson Laboratory) were anesthetized and a 4 mm craniotomy was made over the visual cortex of the right hemisphere as described previously (<xref ref-type="bibr" rid="bib45">Reimer et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Froudarakis et al., 2014</xref>). For functional recordings, awake mice were head-mounted above a cylindrical treadmill and calcium imaging was performed using a Ti-Sapphire laser tuned to 920 nm and a two-photon microscope equipped with resonant scanners (Thorlabs) and a ×25 objective (MRD77220, Nikon). Laser power after the objective was kept below 60 mW. The rostro-caudal treadmill movement was measured using a rotary optical encoder with a resolution of 8000 pulses per revolution. We used light diffusing from the laser through the pupil to capture eye movements and pupil size. Images of the pupil were reflected through a hot mirror and captured with a GigE CMOS camera (Genie Nano C1920M; Teledyne Dalsa) at 20 fps at a 1920×1200 pixel resolution. The contour of the pupil for each frame was extracted using DeepLabCut (<xref ref-type="bibr" rid="bib35">Mathis et al., 2018</xref>) and the center and major radius of a fitted ellipse were used as the position and dilation of the pupil.</p><p>For image acquisition, we used ScanImage. To identify V1 boundaries, we used pixelwise responses to drifting bar stimuli of a 2400×2400 µm<sup>2</sup> scan at 200 µm depth from cortical surface (<xref ref-type="bibr" rid="bib17">Garrett et al., 2014</xref>), recorded using a large field of view mesoscope (<xref ref-type="bibr" rid="bib53">Sofroniew et al., 2016</xref>). Functional imaging was performed using 512×512 pixel scans (700×700 µm<sup>2</sup>) recorded at approx. 15 Hz and positioned within L2/3 (depth 200 µm) in posterior or anterior V1. Imaging data were motion-corrected, automatically segmented, and deconvolved using the CNMF algorithm (<xref ref-type="bibr" rid="bib43">Pnevmatikakis et al., 2016</xref>); cells were further selected by a classifier trained to detect somata based on the segmented masks. This resulted in approx. 500–1200 selected soma masks per scan depending on response quality and blood vessel pattern.</p><p>To achieve photopic stimulation of the mouse visual system, we dilated the pupil pharmacologically with atropine eye drops (<xref ref-type="bibr" rid="bib14">Franke et al., 2022</xref>). Specifically, atropine was applied to the left eye of the animal facing the screen for visual stimulation. Functional recordings started after the pupil was dilated. Pharmacological pupil dilation lasted &gt;2 hr, thereby ensuring a constant pupil size during all functional recordings.</p></sec><sec id="s4-2"><title>Visual stimulation</title><p>Visual stimuli were presented to the left eye of the mouse on a 42×26 cm<sup>2</sup> light-transmitting Teflon screen (McMaster-Carr) positioned 12 cm from the animal, covering approx. 120×90 degrees visual angle. Light was back-projected onto the screen by a DLP-based projector (EKB Technologies Ltd; <xref ref-type="bibr" rid="bib13">Franke et al., 2019</xref>) with UV (395 nm) and green (460 nm) LEDs that differentially activated mouse S- and M-opsin. LEDs were synchronized with the microscope’s scan retrace.</p><p>Light intensity (estimated as photoisomerization rate, P* per second per cone) was calibrated using a spectrometer (USB2000+, Ocean Optics) to result in equal activation rates for mouse M- and S-opsin (for details, see <xref ref-type="bibr" rid="bib13">Franke et al., 2019</xref>). In brief, the spectrometer output was divided by the integration time to obtain counts/s and then converted into electrical power (in nW) using the calibration data (in µJ/count) provided by Ocean Optics. To obtain the estimated photoisomerization rate per photoreceptor type, we first converted electrical power into energy flux (in eV/s) and then calculated the photon flux (in photons/s) using the photon energy (in eV). The photon flux density (in photons/s/µm<sup>2</sup>) was then computed and converted into photoisomerization rate using the effective activation of mouse cone photoreceptors by the LEDs and the light collection area of cone outer segments. In addition, we considered both the wavelength-specific transmission of the mouse optical apparatus (<xref ref-type="bibr" rid="bib22">Henriksson et al., 2010</xref>) and the ratio between pupil size and retinal area (<xref ref-type="bibr" rid="bib49">Schmucker and Schaeffel, 2004</xref>). Please see the calibration iPython notebook provided online for further details.</p><p>We used three different light levels, ranging from photopic levels primarily activating cone photoreceptors to low mesopic levels that predominantly drive rod photoreceptors. For a mean pupil size across recordings within one light level and a maximal stimulus intensity (255 pixel values), this resulted in 50 P*, 400 P*, and 15,000 P* for low mesopic, high mesopic, and photopic light levels, respectively. Please note that the difference between photopic and high mesopic light levels is higher than between high and low photopic light levels.</p><p>Prior to functional recordings, the screen was positioned such that the population RF across all neurons, estimated using an achromatic sparse noise paradigm, was within the center of the screen. Screen position was fixed and kept constant across recordings of the same neurons. We used Psychtoolbox in MATLAB for stimulus presentation and showed the following light stimuli.</p><sec id="s4-2-1"><title>Center-surround luminance and color noise</title><p>We used a center (diameter: 37.5 degrees visual angle) and surround (full screen except the center) binary noise stimulus of UV and green LED to characterize center and surround chromatic properties of mouse V1 neurons. For that, the intensity of UV and green center and surround spots was determined independently by a binary and balanced 25 min random sequence updated at 5 Hz. A similar stimulus was recently used in recordings of the mouse retina (<xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>). The center size of 37.5 degrees visual angle in diameter is larger than the mean center RF size of mouse V1 neurons (26.2±4.6 degrees visual angle in diameter). This allowed to record from a large neuron population, despite some variability in RF center location. We verified that the center RF of the majority of neurons lies within the center spot of the noise stimulus using a sparse noise stimulus for spatial RF mapping (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1a, b</xref>).</p></sec><sec id="s4-2-2"><title>Sparse noise</title><p>To map the spatial RFs of V1 neurons, we used a sparse noise paradigm. UV and green bright (pixel value 255) and dark (pixel value 0) dots of approx. 12 degrees visual angle were presented on a gray background (pixel value 127) in randomized order. Dots were presented for 8 and 5 positions along the horizontal and vertical axis of the screen, respectively, excluding screen margins. Each presentation lasted 200 ms and each condition (e.g. UV-bright dot at position <italic>x</italic>=1 and <italic>y</italic>=1) was repeated 50 times.</p></sec></sec><sec id="s4-3"><title>Preprocessing of neural responses and behavioral data</title><p>Neuronal calcium responses were deconvolved using constrained non-negative calcium deconvolution (<xref ref-type="bibr" rid="bib43">Pnevmatikakis et al., 2016</xref>) to obtain estimated spike trains. For the decoding paradigm, we subsequently extracted the accumulated activity of each neuron between 50 ms after stimulus onset and offset using a Hamming window. Behavioral traces (treadmill velocity and pupil size) were synchronized to the recorded neuronal response traces, but not used for further processing - i.e., we did not distinguish between arousal states of the animal.</p></sec><sec id="s4-4"><title>RF mapping based on the center-surround color noise stimulus</title><p>We used the responses to the 5 Hz center-surround noise stimulus of UV and green LED to compute temporal ETAs of V1 neurons. Specifically, we upsampled both stimulus and responses to 30 Hz, normalized each upsampled response trace by its sum and then multiplied the stimulus matrix with the response matrix for each neuron. Per cell, this resulted in a temporal ETA for center (C) and surround (S) in response to UV and green flicker, respectively (Green<sub>C</sub>, UV<sub>C</sub>, Green<sub>S</sub>, UV<sub>C)</sub>. For each of the four stimulus conditions, kernel quality was measured by comparing the variance of the ETA with the variance of the baseline, defined as the first 500 ms of the ETA. Only cells with at least 10 times more variance of the kernel compared to baseline for UV or green center ETA were considered for further analysis.</p></sec><sec id="s4-5"><title>Sparse noise spatial RF mapping and overlap index</title><p>We estimated spatial ETAs of V1 neurons in response to the sparse noise stimulus by multiplying the stimulus matrix with the response matrix of each neuron (<xref ref-type="bibr" rid="bib51">Schwartz et al., 2006</xref>). For that, we averaged across On and Off and UV and green stimuli, thereby obtaining a two-dimensional (8×5 pixels) spatial ETA per neuron. To assess ETA quality, we generated response predictions by multiplying the flattened ETA of each neuron with the flattened stimulus frames and compared the predictions to the recorded responses by estimating the linear correlation coefficient. For analysis, we only included cells where correlation &gt;0.25. For these cells, we upsampled and peak-normalized the spatial ETAs (resulting in 40×25 pixels), and then estimated the overlap with the center spot of the noise stimulus using a contour threshold of 0.25. Specifically, we calculated the ratio of pixels &gt;0.25 with respect to the peak of the ETA inside and outside the area of the noise center spot.</p></sec><sec id="s4-6"><title>PCA for ETA reconstruction</title><p>To increase the signal-to-noise ratio of the ETAs, we converted them into lower dimensional representations using sparse PCA (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). Specifically, we concatenated the ETAs of the four stimulus conditions and used the resulting matrix with dimensions <italic>neurons</italic> × <italic>time</italic> to perform sparse PCA using the package <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>S</mml:mi><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>P</mml:mi><mml:mi>C</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> in Python. We used sparse PCA because each principal component (PC) then captured one of the four stimulus conditions (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3b</xref>), thereby making the PCs interpretable. We tested different numbers of PCs (<italic>n</italic>=2 to <italic>n</italic>=12 components) and evaluated the quality of the PCA reconstructions by computing the mean squared error (<italic>mse</italic>) between the original ETA and the one reconstructed based on the PCs. We decided to use eight PCs because (i) reconstruction <italic>mse</italic> dropped only slightly with more PCs (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3a</xref>) and (ii) additional PCs captured variance outside the time window of expected stimulus sensitivity, e.g., after the response time.</p></sec><sec id="s4-7"><title>Spectral contrast</title><p>For estimating the chromatic preference of the recorded neurons, we used spectral contrast (<italic>SC</italic>). It is estimated as Michelson contrast ranging from –1 to 1 for a neuron responding solely to UV and green contrast, respectively. We define <italic>SC</italic> as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi></mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>U</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <italic>r<sub>green</sub></italic> and <italic>r<sub>UV</sub></italic> correspond to the amplitude of UV and green ETA to estimate the neurons’ chromatic preference.</p></sec><sec id="s4-8"><title>Luminance and color contrast sensitivity space</title><p>To represent each neuron in a two-dimensional luminance and color contrast space, we extracted ETA peak amplitudes relative to baseline for all four stimulus conditions, with positive and negative peak amplitudes for On and Off cells, respectively. Peak amplitudes of green and UV ETA were then used as <italic>x</italic> and <italic>y</italic> coordinates, respectively, in the two-dimensional contrast spaces for center and surround. To obtain the fraction of variance explained by the luminance and color axis within the contrast space for center and surround RF components, we performed PCA on the two-dimensional matrix with dimensions <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi><mml:mo>×</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>. The relative weights of the resulting PCs were used as a measure of fraction variance explained. A similar method was recently used to quantify chromatic and achromatic contrasts in mouse natural scenes (<xref ref-type="bibr" rid="bib44">Qiu et al., 2021</xref>).</p></sec><sec id="s4-9"><title>Decoding analysis</title><p>We used an SVM classifier with a radial basis function kernel to estimate decoding accuracy between the neuronal representations of two stimulus classes - either On or Off (stimulus luminance) and UV or green (stimulus color). We used varying numbers of neurons for decoding and built separate decoders for stimulus luminance and stimulus color. Specifically, we split the data into 10 equally sized trial blocks, trained the decoder on 90% of the data, tested its accuracy on the remaining 10% of the data, and computed the mean accuracy across <italic>n</italic>=10 different training/test trial splits. Finally, we converted the decoding accuracy into discriminability, the mutual information between the true class and its estimate using<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">M</mml:mi></mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>c</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mi mathvariant="italic">i</mml:mi><mml:mi mathvariant="italic">j</mml:mi></mml:mrow></mml:msub><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:msub><mml:mi mathvariant="normal">g</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mi mathvariant="italic">i</mml:mi><mml:mi mathvariant="italic">j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mi mathvariant="italic">i</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mi mathvariant="italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where p<italic><sub>ij</sub></italic> is the probability of observing the true class <italic>i</italic> and predicted class <italic>j</italic> and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mi mathvariant="italic">i</mml:mi><mml:mo>:</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">p</mml:mi></mml:mrow><mml:mrow><mml:mo>:</mml:mo><mml:mi mathvariant="italic">j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> denote the respective marginal probabilities.</p></sec><sec id="s4-10"><title>Retinal data</title><p>We used an available dataset from <xref ref-type="bibr" rid="bib55">Szatko et al., 2020</xref>, to test how color is represented in the luminance and color contrast space at the level of the retinal output. This dataset consisted of UV and green center and surround ETAs of <italic>n</italic>=3215 retinal ganglion cells (<italic>n</italic>=88 recording fields, <italic>n</italic>=18 mice), obtained from responses to a center (10 degrees visual angle) and surround (30×30 degrees visual angle without the center) luminance and color noise stimulus. We estimated the ETAs and embedded each neuron in the sensitivity space as described above.</p></sec><sec id="s4-11"><title>Functional clustering using GMM</title><p>For clustering of center and surround ETAs into distinct response types, we used a GMM (<inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>s</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>n</mml:mi><mml:mo>.</mml:mo><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mo>.</mml:mo><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>M</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:math></inline-formula> package). We used the weights of the PCs extracted from the ETAs as input to the GMM (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>). To test how many GMM components (i.e. response types) best explain the data, we built GMMs with varying numbers of components and cross-validated the models’ log likelihood on 10% of left-out test data, using 10 different test/train trial splits (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1a</xref>). We picked the model with <italic>n</italic>=17 components for further analysis because this resulted in the highest log likelihood. However, please note that the models’ performance was relatively ETAble across a wide range of components. To test the assignment accuracy of the final model, we used the mean and covariance matrix of each GMM component to generate data with ground-truth labels and compared those to the GMM-predicted labels (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1b</xref>), as described previously (<xref ref-type="bibr" rid="bib61">Tolias et al., 2007</xref>). Assignment accuracy ranged between 75% and 98%, with a mean ± s.d. of 89% ± 6%. Most response types were evenly distributed across mice and all response types were present in all mice (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1c</xref>), suggesting that clustering was not predominantly driven by inter-experimental variations.</p></sec><sec id="s4-12"><title>Cortical distribution index</title><p>For estimating the distribution of response types across cortical position, we used the cortical distribution index. It was estimated as Michelson contrast ranging from –1 to 1 for a response type solely present in posterior and anterior V1, respectively. We define the distribution index as<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">D</mml:mi></mml:mrow><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <italic>n<sub>anterior</sub></italic> and <italic>n<sub>posterior</sub></italic> correspond to the fraction of neurons in anterior and posterior V1 assigned to a specific response type.</p></sec><sec id="s4-13"><title>Decoding of noise and object scenes</title><p>For decoding noise versus object scenes based on simulated responses, we used natural scene inspired parametric stimuli. Specifically, we generated images with independent Perlin noise <xref ref-type="bibr" rid="bib42">Perlin, 1985</xref> in each color channel using the Perlin-noise package for Python. Then, for the object images, we added a dark ellipse of varying size, position, and angle to the UV color channels. We adjusted the contrast of all images with a dark object to match the contrast of noise images, such that the distribution of image contrasts did not differ between noise and object images. We then simulated responses to 1000 object and noise scenes that were used by an SVM decoder to decode stimulus class (object or noise) as described above. For simulating responses, we modeled each response type to have a square RF with 10 degrees visual angle in diameter, with the luminance and color contrast sensitivity of the response type’s RF center. Then, we created response maps by convolving the simulated RFs with the scenes and summed up all positive values to result in one response value per scene and response type.</p></sec><sec id="s4-14"><title>Statistical analysis</title><p>We used the t-test for two independent samples to test whether the decoding performance of 10 test/train trial splits differ between (i) center and surround, (ii) photopic and mesopic light levels, (iii) anterior and posterior V1, and (iv) anterior and posterior response types. For all these tests, the p-value was adjusted for multiple comparisons using the Bonferroni correction.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration</p></fn><fn fn-type="con" id="con2"><p>Formal analysis, Validation, Visualization, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Data acquisition</p></fn><fn fn-type="con" id="con4"><p>Investigation, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Formal analysis, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Funding acquisition, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Supervision, Funding acquisition, Investigation, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All procedures were approved by the Institutional Animal Care and Use Committee of Baylor College of Medicine (animal protocol number: AN-4703).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-89996-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and code is available at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.rv15dv4hb">https://doi.org/10.5061/dryad.rv15dv4hb</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Data from: Asymmetric distribution of color-opponent response types across mouse visual cortex supports superior color vision in the sky</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.rv15dv4hb</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Thomas Euler and Tom Baden for feedback on the manuscript. This work was supported by the Hertie Foundation (to PB), the German Research Foundation (DFG CRC 1233 'Robust Vision' to PB and KF, DFG Excellence Cluster 2064 'Machine Learning - New Perspectives for Science' to PB), the European Research Council (ERC) under the European Union’s Horizon Europe research and innovation programme ('NextMechMod' grant agreement No. 101039115 to PB), and the National Institutes of Health (UF1 NS126566 and RF1 MH126883 to AST).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abballe</surname><given-names>L</given-names></name><name><surname>Asari</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Natural image statistics for mouse vision</article-title><source>PLOS ONE</source><volume>17</volume><elocation-id>e0262763</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0262763</pub-id><pub-id pub-id-type="pmid">35051230</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aihara</surname><given-names>S</given-names></name><name><surname>Yoshida</surname><given-names>T</given-names></name><name><surname>Hashimoto</surname><given-names>T</given-names></name><name><surname>Ohki</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Color representation is retinotopically biased but locally intermingled in mouse V1</article-title><source>Frontiers in Neural Circuits</source><volume>11</volume><elocation-id>22</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2017.00022</pub-id><pub-id pub-id-type="pmid">28405186</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Wei</surname><given-names>T</given-names></name><name><surname>Zaichuk</surname><given-names>M</given-names></name><name><surname>Wissinger</surname><given-names>B</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A tale of two retinal domains: near-optimal sampling of achromatic contrasts in natural scenes through asymmetric photoreceptor distribution</article-title><source>Neuron</source><volume>80</volume><fpage>1206</fpage><lpage>1217</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.030</pub-id><pub-id pub-id-type="pmid">24314730</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The retinal basis of vertebrate color vision</article-title><source>Annual Review of Vision Science</source><volume>5</volume><fpage>177</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091718-014926</pub-id><pub-id pub-id-type="pmid">31226010</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Understanding the retinal basis of vision across species</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>5</fpage><lpage>20</lpage><pub-id pub-id-type="doi">10.1038/s41583-019-0242-1</pub-id><pub-id pub-id-type="pmid">31780820</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Breuninger</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Chromatic coding from cone-type unselective circuits in the mouse retina</article-title><source>Neuron</source><volume>77</volume><fpage>559</fpage><lpage>571</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.12.012</pub-id><pub-id pub-id-type="pmid">23395380</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chatterjee</surname><given-names>S</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Parallel colour-opponent pathways to primary visual cortex</article-title><source>Nature</source><volume>426</volume><fpage>668</fpage><lpage>671</lpage><pub-id pub-id-type="doi">10.1038/nature02167</pub-id><pub-id pub-id-type="pmid">14668866</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cronin</surname><given-names>TW</given-names></name><name><surname>Bok</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Photoreception and vision in the ultraviolet</article-title><source>The Journal of Experimental Biology</source><volume>219</volume><fpage>2790</fpage><lpage>2801</lpage><pub-id pub-id-type="doi">10.1242/jeb.128769</pub-id><pub-id pub-id-type="pmid">27655820</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Siegle</surname><given-names>JH</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Blanche</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatial organization of chromatic pathways in the mouse dorsal lateral geniculate nucleus</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>1102</fpage><lpage>1116</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1742-16.2016</pub-id><pub-id pub-id-type="pmid">27986926</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denman</surname><given-names>DJ</given-names></name><name><surname>Luviano</surname><given-names>JA</given-names></name><name><surname>Ollerenshaw</surname><given-names>DR</given-names></name><name><surname>Cross</surname><given-names>S</given-names></name><name><surname>Williams</surname><given-names>D</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Olsen</surname><given-names>SR</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mouse color and wavelength-specific luminance contrast sensitivity are non-uniform across visual space</article-title><source>eLife</source><volume>7</volume><elocation-id>e31209</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.31209</pub-id><pub-id pub-id-type="pmid">29319502</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeYoe</surname><given-names>EA</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Segregation of efferent connections and receptive field properties in visual area V2 of the macaque</article-title><source>Nature</source><volume>317</volume><fpage>58</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1038/317058a0</pub-id><pub-id pub-id-type="pmid">2412132</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inhibition decorrelates visual feature representations in the inner retina</article-title><source>Nature</source><volume>542</volume><fpage>439</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1038/nature21394</pub-id><pub-id pub-id-type="pmid">28178238</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Maia Chagas</surname><given-names>A</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Zimmermann</surname><given-names>MJ</given-names></name><name><surname>Bartel</surname><given-names>P</given-names></name><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An arbitrary-spectrum spatial visual stimulator for vision research</article-title><source>eLife</source><volume>8</volume><elocation-id>e48779</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48779</pub-id><pub-id pub-id-type="pmid">31545172</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Willeke</surname><given-names>KF</given-names></name><name><surname>Ponder</surname><given-names>K</given-names></name><name><surname>Galdamez</surname><given-names>M</given-names></name><name><surname>Zhou</surname><given-names>N</given-names></name><name><surname>Muhammad</surname><given-names>T</given-names></name><name><surname>Patel</surname><given-names>S</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>State-dependent pupil dilation rapidly shifts visual feature selectivity</article-title><source>Nature</source><volume>610</volume><fpage>128</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-05270-3</pub-id><pub-id pub-id-type="pmid">36171291</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Cotton</surname><given-names>RJ</given-names></name><name><surname>Sinz</surname><given-names>FH</given-names></name><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Saggau</surname><given-names>P</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Population code in mouse V1 facilitates readout of natural scenes through increased sparseness</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>851</fpage><lpage>857</lpage><pub-id pub-id-type="doi">10.1038/nn.3707</pub-id><pub-id pub-id-type="pmid">24747577</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garg</surname><given-names>AK</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Rashid</surname><given-names>MS</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Color and orientation are jointly coded and spatially organized in primate primary visual cortex</article-title><source>Science</source><volume>364</volume><fpage>1275</fpage><lpage>1279</lpage><pub-id pub-id-type="doi">10.1126/science.aaw5868</pub-id><pub-id pub-id-type="pmid">31249057</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrett</surname><given-names>ME</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Topography and areal organization of mouse visual cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>12587</fpage><lpage>12600</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1124-14.2014</pub-id><pub-id pub-id-type="pmid">25209296</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gegenfurtner</surname><given-names>KR</given-names></name><name><surname>Kiper</surname><given-names>DC</given-names></name><name><surname>Fenstemaker</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Processing of color, form, and motion in macaque area V2</article-title><source>Visual Neuroscience</source><volume>13</volume><fpage>161</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1017/s0952523800007203</pub-id><pub-id pub-id-type="pmid">8730997</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grubb</surname><given-names>MS</given-names></name><name><surname>Thompson</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Quantitative characterization of visual response properties in the mouse dorsal lateral geniculate nucleus</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>3594</fpage><lpage>3607</lpage><pub-id pub-id-type="doi">10.1152/jn.00699.2003</pub-id><pub-id pub-id-type="pmid">12944530</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guggiana Nilo</surname><given-names>DA</given-names></name><name><surname>Riegler</surname><given-names>C</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name><name><surname>Engert</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Distributed chromatic processing at the interface between retina and brain in the larval zebrafish</article-title><source>Current Biology</source><volume>31</volume><fpage>1945</fpage><lpage>1953</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.01.088</pub-id><pub-id pub-id-type="pmid">33636122</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haverkamp</surname><given-names>S</given-names></name><name><surname>Wässle</surname><given-names>H</given-names></name><name><surname>Duebel</surname><given-names>J</given-names></name><name><surname>Kuner</surname><given-names>T</given-names></name><name><surname>Augustine</surname><given-names>GJ</given-names></name><name><surname>Feng</surname><given-names>G</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The primordial, blue-cone color system of the mouse retina</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>5438</fpage><lpage>5445</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1117-05.2005</pub-id><pub-id pub-id-type="pmid">15930394</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henriksson</surname><given-names>JT</given-names></name><name><surname>Bergmanson</surname><given-names>JPG</given-names></name><name><surname>Walsh</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Ultraviolet radiation transmittance of the mouse eye and its individual media components</article-title><source>Experimental Eye Research</source><volume>90</volume><fpage>382</fpage><lpage>387</lpage><pub-id pub-id-type="doi">10.1016/j.exer.2009.11.004</pub-id><pub-id pub-id-type="pmid">19925789</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hoefling</surname><given-names>L</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Behrens</surname><given-names>C</given-names></name><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Klindt</surname><given-names>DA</given-names></name><name><surname>Jessen</surname><given-names>Z</given-names></name><name><surname>Schwartz</surname><given-names>GS</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A chromatic feature detector in the retina signals visual context changes</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.11.30.518492</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Segregation of form, color, and stereopsis in primate area 18</article-title><source>The Journal of Neuroscience</source><volume>7</volume><fpage>3378</fpage><lpage>3415</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.07-11-03378.1987</pub-id><pub-id pub-id-type="pmid">2824714</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobs</surname><given-names>GH</given-names></name><name><surname>Williams</surname><given-names>GA</given-names></name><name><surname>Fenwick</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Influence of cone pigment coexpression on spectral sensitivity and color vision in the mouse</article-title><source>Vision Research</source><volume>44</volume><fpage>1615</fpage><lpage>1622</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.01.016</pub-id><pub-id pub-id-type="pmid">15135998</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joesch</surname><given-names>M</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A neuronal circuit for colour vision based on rod-cone opponency</article-title><source>Nature</source><volume>532</volume><fpage>236</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1038/nature17158</pub-id><pub-id pub-id-type="pmid">27049951</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>EN</given-names></name><name><surname>Van Hooser</surname><given-names>SD</given-names></name><name><surname>Fitzpatrick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The representation of S-cone signals in primary visual cortex</article-title><source>The Journal of Neuroscience</source><volume>30</volume><fpage>10337</fpage><lpage>10350</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1428-10.2010</pub-id><pub-id pub-id-type="pmid">20685977</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>JP</given-names></name><name><surname>Palmer</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>The two-dimensional spatial structure of simple receptive fields in cat striate cortex</article-title><source>Journal of Neurophysiology</source><volume>58</volume><fpage>1187</fpage><lpage>1211</lpage><pub-id pub-id-type="doi">10.1152/jn.1987.58.6.1187</pub-id><pub-id pub-id-type="pmid">3437330</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelber</surname><given-names>A</given-names></name><name><surname>Vorobyev</surname><given-names>M</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Animal colour vision--behavioural tests and physiological concepts</article-title><source>Biological Reviews of the Cambridge Philosophical Society</source><volume>78</volume><fpage>81</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1017/s1464793102005985</pub-id><pub-id pub-id-type="pmid">12620062</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khani</surname><given-names>MH</given-names></name><name><surname>Gollisch</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Linear and nonlinear chromatic integration in the mouse retina</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>1900</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-22042-1</pub-id><pub-id pub-id-type="pmid">33772000</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>MS</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Anatomy and physiology of a color system in the primate visual cortex</article-title><source>The Journal of Neuroscience</source><volume>4</volume><fpage>309</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.04-01-00309.1984</pub-id><pub-id pub-id-type="pmid">6198495</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Livingstone</surname><given-names>M</given-names></name><name><surname>Hubel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Segregation of form, color, movement, and depth: anatomy, physiology, and perception</article-title><source>Science</source><volume>240</volume><fpage>740</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1126/science.3283936</pub-id><pub-id pub-id-type="pmid">3283936</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Longden</surname><given-names>KD</given-names></name><name><surname>Rogers</surname><given-names>EM</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Dionne</surname><given-names>H</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Different spectral sensitivities of ON- and OFF-motion pathways enhance the detection of approaching color objects in <italic>Drosophila</italic></article-title><source>Nature Communications</source><volume>14</volume><elocation-id>7693</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-43566-8</pub-id><pub-id pub-id-type="pmid">38001097</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Losey</surname><given-names>GS</given-names></name><name><surname>Cronin</surname><given-names>TW</given-names></name><name><surname>Goldsmith</surname><given-names>TH</given-names></name><name><surname>Hyde</surname><given-names>D</given-names></name><name><surname>Marshall</surname><given-names>NJ</given-names></name><name><surname>McFarland</surname><given-names>WN</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The UV visual world of fishes: a review</article-title><source>Journal of Fish Biology</source><volume>54</volume><fpage>921</fpage><lpage>943</lpage><pub-id pub-id-type="doi">10.1111/j.1095-8649.1999.tb00848.x</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname><given-names>A</given-names></name><name><surname>Mamidanna</surname><given-names>P</given-names></name><name><surname>Cury</surname><given-names>KM</given-names></name><name><surname>Abe</surname><given-names>T</given-names></name><name><surname>Murthy</surname><given-names>VN</given-names></name><name><surname>Mathis</surname><given-names>MW</given-names></name><name><surname>Bethge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maximov</surname><given-names>VV</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Environmental factors which may have led to the appearance of colour vision</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>355</volume><fpage>1239</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1098/rstb.2000.0675</pub-id><pub-id pub-id-type="pmid">11079406</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mouland</surname><given-names>JW</given-names></name><name><surname>Pienaar</surname><given-names>A</given-names></name><name><surname>Williams</surname><given-names>C</given-names></name><name><surname>Watson</surname><given-names>AJ</given-names></name><name><surname>Lucas</surname><given-names>RJ</given-names></name><name><surname>Brown</surname><given-names>TM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extensive cone-dependent spectral opponency within a discrete zone of the lateral geniculate nucleus supporting mouse color vision</article-title><source>Current Biology</source><volume>31</volume><fpage>3391</fpage><lpage>3400</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.05.024</pub-id><pub-id pub-id-type="pmid">34111401</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nadal-Nicolás</surname><given-names>FM</given-names></name><name><surname>Kunze</surname><given-names>VP</given-names></name><name><surname>Ball</surname><given-names>JM</given-names></name><name><surname>Peng</surname><given-names>BT</given-names></name><name><surname>Krishnan</surname><given-names>A</given-names></name><name><surname>Zhou</surname><given-names>G</given-names></name><name><surname>Dong</surname><given-names>L</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>True S-cones are concentrated in the ventral mouse retina and wired for color detection in the upper visual field</article-title><source>eLife</source><volume>9</volume><elocation-id>e56840</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56840</pub-id><pub-id pub-id-type="pmid">32463363</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nilsson</surname><given-names>DE</given-names></name><name><surname>Smolka</surname><given-names>J</given-names></name><name><surname>Bok</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The vertical light-gradient and its potential impact on animal distribution and behavior</article-title><source>Frontiers in Ecology and Evolution</source><volume>10</volume><elocation-id>e1328</elocation-id><pub-id pub-id-type="doi">10.3389/fevo.2022.951328</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Robustness of spike deconvolution for neuronal calcium imaging</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7976</fpage><lpage>7985</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3339-17.2018</pub-id><pub-id pub-id-type="pmid">30082416</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peichl</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Diversity of mammalian photoreceptor properties: adaptations to habitat and lifestyle?</article-title><source>The Anatomical Record. Part A, Discoveries in Molecular, Cellular, and Evolutionary Biology</source><volume>287</volume><fpage>1001</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1002/ar.a.20262</pub-id><pub-id pub-id-type="pmid">16200646</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perlin</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>An image synthesizer</article-title><source>ACM SIGGRAPH Computer Graphics</source><volume>19</volume><fpage>287</fpage><lpage>296</lpage><pub-id pub-id-type="doi">10.1145/325165.325247</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pnevmatikakis</surname><given-names>EA</given-names></name><name><surname>Soudry</surname><given-names>D</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Merel</surname><given-names>J</given-names></name><name><surname>Pfau</surname><given-names>D</given-names></name><name><surname>Reardon</surname><given-names>T</given-names></name><name><surname>Mu</surname><given-names>Y</given-names></name><name><surname>Lacefield</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>W</given-names></name><name><surname>Ahrens</surname><given-names>M</given-names></name><name><surname>Bruno</surname><given-names>R</given-names></name><name><surname>Jessell</surname><given-names>TM</given-names></name><name><surname>Peterka</surname><given-names>DS</given-names></name><name><surname>Yuste</surname><given-names>R</given-names></name><name><surname>Paninski</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Simultaneous denoising, deconvolution, and demixing of calcium imaging data</article-title><source>Neuron</source><volume>89</volume><fpage>285</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.037</pub-id><pub-id pub-id-type="pmid">26774160</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qiu</surname><given-names>Y</given-names></name><name><surname>Zhao</surname><given-names>Z</given-names></name><name><surname>Klindt</surname><given-names>D</given-names></name><name><surname>Kautzky</surname><given-names>M</given-names></name><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Schaeffel</surname><given-names>F</given-names></name><name><surname>Rifai</surname><given-names>K</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name><name><surname>Busse</surname><given-names>L</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Natural environment statistics in the upper and lower visual field are reflected in mouse retinal specializations</article-title><source>Current Biology</source><volume>31</volume><fpage>3233</fpage><lpage>3247</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.05.017</pub-id><pub-id pub-id-type="pmid">34107304</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reimer</surname><given-names>J</given-names></name><name><surname>Froudarakis</surname><given-names>E</given-names></name><name><surname>Cadwell</surname><given-names>CR</given-names></name><name><surname>Yatsenko</surname><given-names>D</given-names></name><name><surname>Denfield</surname><given-names>GH</given-names></name><name><surname>Tolias</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil fluctuations track fast switching of cortical states during quiet wakefulness</article-title><source>Neuron</source><volume>84</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.09.033</pub-id><pub-id pub-id-type="pmid">25374359</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reitner</surname><given-names>A</given-names></name><name><surname>Sharpe</surname><given-names>LT</given-names></name><name><surname>Zrenner</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Is colour vision possible with only rods and blue-sensitive cones?</article-title><source>Nature</source><volume>352</volume><fpage>798</fpage><lpage>800</lpage><pub-id pub-id-type="doi">10.1038/352798a0</pub-id><pub-id pub-id-type="pmid">1881435</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhim</surname><given-names>I</given-names></name><name><surname>Coello-Reyes</surname><given-names>G</given-names></name><name><surname>Ko</surname><given-names>HK</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Maps of cone opsin input to mouse V1 and higher visual areas</article-title><source>Journal of Neurophysiology</source><volume>117</volume><fpage>1674</fpage><lpage>1682</lpage><pub-id pub-id-type="doi">10.1152/jn.00849.2016</pub-id><pub-id pub-id-type="pmid">28100658</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rhim</surname><given-names>I</given-names></name><name><surname>Nauhaus</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Joint representations of color and form in mouse visual cortex described by random pooling from rods and cones</article-title><source>Journal of Neurophysiology</source><volume>129</volume><fpage>619</fpage><lpage>634</lpage><pub-id pub-id-type="doi">10.1152/jn.00138.2022</pub-id><pub-id pub-id-type="pmid">36696968</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmucker</surname><given-names>C</given-names></name><name><surname>Schaeffel</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A paraxial schematic eye model for the growing C57BL/6 mouse</article-title><source>Vision Research</source><volume>44</volume><fpage>1857</fpage><lpage>1867</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2004.03.011</pub-id><pub-id pub-id-type="pmid">15145680</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuett</surname><given-names>S</given-names></name><name><surname>Bonhoeffer</surname><given-names>T</given-names></name><name><surname>Hübener</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Mapping retinotopic structure in mouse visual cortex with optical imaging</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>6549</fpage><lpage>6559</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-15-06549.2002</pub-id><pub-id pub-id-type="pmid">12151534</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwartz</surname><given-names>O</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spike-triggered neural characterization</article-title><source>Journal of Vision</source><volume>6</volume><fpage>484</fpage><lpage>507</lpage><pub-id pub-id-type="doi">10.1167/6.4.13</pub-id><pub-id pub-id-type="pmid">16889482</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seifert</surname><given-names>M</given-names></name><name><surname>Roberts</surname><given-names>PA</given-names></name><name><surname>Kafetzis</surname><given-names>G</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Birds multiplex spectral and temporal visual information via retinal On- and Off-channels</article-title><source>Nature Communications</source><volume>14</volume><elocation-id>5308</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-023-41032-z</pub-id><pub-id pub-id-type="pmid">37652912</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sofroniew</surname><given-names>NJ</given-names></name><name><surname>Flickinger</surname><given-names>D</given-names></name><name><surname>King</surname><given-names>J</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A large field of view two-photon mesoscope with subcellular resolution for in vivo imaging</article-title><source>eLife</source><volume>5</volume><elocation-id>e14472</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.14472</pub-id><pub-id pub-id-type="pmid">27300105</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stabio</surname><given-names>ME</given-names></name><name><surname>Sabbah</surname><given-names>S</given-names></name><name><surname>Quattrochi</surname><given-names>LE</given-names></name><name><surname>Ilardi</surname><given-names>MC</given-names></name><name><surname>Fogerson</surname><given-names>PM</given-names></name><name><surname>Leyrer</surname><given-names>ML</given-names></name><name><surname>Kim</surname><given-names>MT</given-names></name><name><surname>Kim</surname><given-names>I</given-names></name><name><surname>Schiel</surname><given-names>M</given-names></name><name><surname>Renna</surname><given-names>JM</given-names></name><name><surname>Briggman</surname><given-names>KL</given-names></name><name><surname>Berson</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The M5 cell: a color-opponent intrinsically photosensitive retinal ganglion cell</article-title><source>Neuron</source><volume>97</volume><fpage>150</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.11.030</pub-id><pub-id pub-id-type="pmid">29249284</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szatko</surname><given-names>KP</given-names></name><name><surname>Korympidou</surname><given-names>MM</given-names></name><name><surname>Ran</surname><given-names>Y</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Dalkara</surname><given-names>D</given-names></name><name><surname>Schubert</surname><given-names>T</given-names></name><name><surname>Euler</surname><given-names>T</given-names></name><name><surname>Franke</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural circuits in the mouse retina support color vision in the upper visual field</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3481</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17113-8</pub-id><pub-id pub-id-type="pmid">32661226</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szél</surname><given-names>A</given-names></name><name><surname>Röhlich</surname><given-names>P</given-names></name><name><surname>Caffé</surname><given-names>AR</given-names></name><name><surname>Juliusson</surname><given-names>B</given-names></name><name><surname>Aguirre</surname><given-names>G</given-names></name><name><surname>Van Veen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Unique topographic separation of two spectral classes of cones in the mouse retina</article-title><source>The Journal of Comparative Neurology</source><volume>325</volume><fpage>327</fpage><lpage>342</lpage><pub-id pub-id-type="doi">10.1002/cne.903250302</pub-id><pub-id pub-id-type="pmid">1447405</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>Z</given-names></name><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Chen</surname><given-names>TW</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Ji</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neuronal representation of ultraviolet visual stimuli in mouse primary visual cortex</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>12597</elocation-id><pub-id pub-id-type="doi">10.1038/srep12597</pub-id><pub-id pub-id-type="pmid">26219604</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tanigawa</surname><given-names>H</given-names></name><name><surname>Lu</surname><given-names>HD</given-names></name><name><surname>Roe</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional organization for color and orientation in macaque V4</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1542</fpage><lpage>1548</lpage><pub-id pub-id-type="doi">10.1038/nn.2676</pub-id><pub-id pub-id-type="pmid">21076422</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tedore</surname><given-names>C</given-names></name><name><surname>Nilsson</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Avian UV vision enhances leaf surface contrasts in forest environments</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>238</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-08142-5</pub-id><pub-id pub-id-type="pmid">30670700</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thoreson</surname><given-names>WB</given-names></name><name><surname>Dacey</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Diverse cell types, circuits, and mechanisms for color vision in the vertebrate retina</article-title><source>Physiological Reviews</source><volume>99</volume><fpage>1527</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1152/physrev.00027.2018</pub-id><pub-id pub-id-type="pmid">31140374</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Ecker</surname><given-names>AS</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name><name><surname>Hoenselaar</surname><given-names>A</given-names></name><name><surname>Keliris</surname><given-names>GA</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Recording chronically from the same neurons in awake, behaving primates</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>3780</fpage><lpage>3790</lpage><pub-id pub-id-type="doi">10.1152/jn.00260.2007</pub-id><pub-id pub-id-type="pmid">17942615</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyler</surname><given-names>NJC</given-names></name><name><surname>Jeffery</surname><given-names>G</given-names></name><name><surname>Hogg</surname><given-names>CR</given-names></name><name><surname>Stokkan</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Ultraviolet vision may enhance the ability of reindeer to discriminate plants in snow</article-title><source>ARCTIC</source><volume>67</volume><elocation-id>159</elocation-id><pub-id pub-id-type="doi">10.14430/arctic4381</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiesel</surname><given-names>TN</given-names></name><name><surname>Hubel</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>Spatial and chromatic interactions in the lateral geniculate body of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>29</volume><fpage>1115</fpage><lpage>1156</lpage><pub-id pub-id-type="doi">10.1152/jn.1966.29.6.1115</pub-id><pub-id pub-id-type="pmid">4961644</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshimatsu</surname><given-names>T</given-names></name><name><surname>Schröder</surname><given-names>C</given-names></name><name><surname>Nevala</surname><given-names>NE</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fovea-like photoreceptor specializations underlie single UV cone driven prey-capture behavior in zebrafish</article-title><source>Neuron</source><volume>107</volume><fpage>320</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.04.021</pub-id><pub-id pub-id-type="pmid">32473094</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zeki</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Functional specialisation in the visual cortex of the rhesus monkey</article-title><source>Nature</source><volume>274</volume><fpage>423</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1038/274423a0</pub-id><pub-id pub-id-type="pmid">97565</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>IJ</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title><source>PNAS</source><volume>109</volume><fpage>E2391</fpage><lpage>E2398</lpage><pub-id pub-id-type="doi">10.1073/pnas.1211547109</pub-id><pub-id pub-id-type="pmid">22891316</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>M</given-names></name><name><surname>Bear</surname><given-names>J</given-names></name><name><surname>Roberts</surname><given-names>PA</given-names></name><name><surname>Janiak</surname><given-names>FK</given-names></name><name><surname>Semmelhack</surname><given-names>J</given-names></name><name><surname>Yoshimatsu</surname><given-names>T</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Zebrafish retinal ganglion cells asymmetrically encode spectral and temporal information across visual space</article-title><source>Current Biology</source><volume>30</volume><fpage>2927</fpage><lpage>2942</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.05.055</pub-id><pub-id pub-id-type="pmid">32531283</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zimmermann</surname><given-names>MJY</given-names></name><name><surname>Nevala</surname><given-names>NE</given-names></name><name><surname>Yoshimatsu</surname><given-names>T</given-names></name><name><surname>Osorio</surname><given-names>D</given-names></name><name><surname>Nilsson</surname><given-names>DE</given-names></name><name><surname>Berens</surname><given-names>P</given-names></name><name><surname>Baden</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Zebrafish differentially process color across visual space to match natural scenes</article-title><source>Current Biology</source><volume>28</volume><fpage>2018</fpage><lpage>2032</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.04.075</pub-id><pub-id pub-id-type="pmid">29937350</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89996.4.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Rieke</surname><given-names>Fred</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Washington</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>Franke et al. explore and characterize color response properties of neurons in mouse primary visual cortex (V1), revealing specific color opponent encoding strategies across the visual field. The paper provides evidence for the existence of color opponency in a subset of neurons within V1 and shows that these color opponent neurons are more numerous in the upper visual field. Support for the main conclusions is <bold>convincing</bold> and the dataset that forms the basis of the paper is impressive. The paper will make an <bold>important</bold> contribution to understanding how color is coded in mouse V1.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89996.4.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In this study, Franke et al. explore and characterize color response properties across primary visual cortex, revealing specific color opponent encoding strategies across the visual field. The authors use awake 2P imaging to define the spectral response properties of visual interneurons in layer 2/3. They find that opponent responses are more pronounced at photopic light levels, and that diversity in color opponent responses exists across the visual field, with green ON/ UV OFF responses more strongly represented in the upper visual field. This is argued to be relevant for the detection of certain features that are more salient when using chromatic space, possibly due to noise reduction. In the revised version, Franke et al. have addressed the potential pitfalls in the discussion, which is an important point for the non-expert reader. Thus, this study provides a solid characterization of the color properties of V1 and is a valuable addition to visual neuroscience research.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89996.4.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Franke et al. characterize the representation of color in the primary visual cortex of mice, highlighting how this changes across the visual field. Using calcium imaging in awake, head-fixed mice, they characterize the properties of V1 neurons (layer 2/3) using a large center-surround stimulation where green and ultra-violet colors were presented in random combinations. Clustering of responses revealed a set of functional cell-types based on their preference to different combinations of green and UV in their center and surround. These functional types were demonstrated to have different spatial distributions across V1, including one neuronal type (Green-ON/UV-OFF) that was much more prominent in the posterior V1 (i.e. upper visual field). Modelling work suggests that these neurons likely support the detection of predator-like objects in the sky.</p><p>Strengths:</p><p>The large-scale single-cell resolution imaging used in this work allows the authors to map the responses of individual neurons across large regions of the visual cortex. Combining this large dataset with clustering analysis enabled the authors to group V1 neurons into distinct functional cell types and demonstrate their relative distribution in the upper and lower visual fields. Modelling work demonstrated the different capacity of each functional type to detect objects in the sky, providing insight into the ethological relevance of color opponent neurons in V1.</p><p>Weaknesses:</p><p>It is unfortunate the authors were unable to provide stronger mechanistic insights into how color opponent neurons in V1 are formed.</p><p>Overall, this study will be a valuable resource for researchers studying color vision, cortical processing, and the processing of ethologically relevant information. It provides a useful basis for future work on the origin of color opponency in V1 and its ethological relevance.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89996.4.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This paper improves our understanding of the coding of chromatic signals in mouse visual cortex. Calcium responses of a large collection of cells are measured in response to a simple spot stimulus. These responses are used to estimate chromatic tuning properties - specifically sensitivity to UV and green stimuli presented in a large central spot or a larger still surrounding region. Cells are divided based on their responses to these stimuli into luminance or chromatic sensitive groups.</p><p>The paper has improved substantially in revisions and makes an important contribution to how color is coded in mouse V1. The revisions have nicely clarified a few limitations of the current study, and that serves to emphasize the strengths of the data and clear conclusions that can be drawn from it.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.89996.4.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Franke</surname><given-names>Katrin</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford University</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Cai</surname><given-names>Chenchen</given-names></name><role specific-use="author">Author</role><aff><institution>Institute for Ophthalmic Research, University of Tuebingen, Tuebingen, Germany; Graduate Training Center of Neuroscience, International Max Planck Research School, University of Tuebingen</institution><addr-line><named-content content-type="city">Tuebingen</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Ponder</surname><given-names>Kayla</given-names></name><role specific-use="author">Author</role><aff><institution>Baylor College of Medicine</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Jiakun</given-names></name><role specific-use="author">Author</role><aff><institution>Baylor College of Medicine</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Sokoloski</surname><given-names>Sacha</given-names></name><role specific-use="author">Author</role><aff><institution>Albert Einstein College of Medicine</institution><addr-line><named-content content-type="city">New York City</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Berens</surname><given-names>Philipp</given-names></name><role specific-use="author">Author</role><aff><institution>University of Tüebingen</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff></contrib><contrib contrib-type="author"><name><surname>Tolias</surname><given-names>Andreas Savas</given-names></name><role specific-use="author">Author</role><aff><institution>Stanford Medicine</institution><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife assessment:</bold></p><p>Franke et al. explore and characterize the color response properties in the mouse primary visual cortex, revealing specific color opponent encoding strategies across the visual field. The data is solid; however, the evidence supporting some conclusions is incomplete. In its current form, the paper makes a useful contribution to how color is coded in mouse V1. Significance would be enhanced with some additional analyses and a clearer discussion of the limitations of the data presented.</p></disp-quote><p>We thank the reviewers for appreciating our manuscript. We have rewritten the conclusions of the paper to be more conservative and now more explicitly focus on color processing in mouse V1, rather than comparing V1 to the retina. Additionally, we discuss the limitations of our approach in detail in the Discussion section. Finally, we have addressed all comments from the reviewers below.</p><disp-quote content-type="editor-comment"><p><bold>Referee 1 (Remarks to the Author):</bold></p><p>In this study, Franke et al. explore and characterize color response properties across primary visual cortex, revealing specific color opponent encoding strategies across the visual field. The authors use awake 2P imaging to define the spectral response properties of visual interneurons in layer 2/3. They find that opponent responses are more pronounced at photopic light levels, and that diversity in color opponent responses exists across the visual field, with green ON/ UV OFF responses more strongly represented in the upper visual field. This is argued to be relevant for the detection of certain features that are more salient when using chromatic space, possibly due to noise reduction. In the revised version, Franke et al. have addressed the potential pitfalls in the discussion, which is an important point for the non-expert reader. Thus, this study provides a solid characterization of the color properties of V1 and is a valuable addition to visual neuroscience research.</p><p>My remaining concerns are based more on the interpretation. I’m still not convinced by the statement &quot;This type of color-opponency in the receptive field center of V1 neurons was not present in the receptive field center of retinal ganglion cells and, therefore, is likely computed by integrating center and surround information downstream of the retina.&quot; and I would suggest rewording it in the abstract.</p><p>As discussed previously and now nicely added to the discussion, it is difficult to make a direct comparison given the different stimulus types used to characterize the retina and V1 recordings and the different levels of adaptation in both tissues. I will leave this point to the discussion, which allows for a more nuanced description of the phenomenon. Why do I think this is important? In the introduction, the authors argue that &quot;the discrepancy [of previous studies] may be due to differences in stimulus design or light levels.&quot; However, while different light levels can be tested in V1, this cannot be done properly in the retina with 2P experiments. To address this, one would have to examine color-opponency in RGC terminals in vivo, which is beyond the scope of this study. Addressing these latter points directly in the discussion would, in my opinion, only strengthen the study.</p></disp-quote><p>We thank the reviewer for the feedback. We removed the sentence mentioned by the reviewer from the abstract, as well as from the summary of our results in the Introduction. Additionally, we now phrase the interpretation of the retinal results more conservatively and specifically highlight in the Discussion that comparing ex-vivo retinal to in-vivo cortical data is challenging. With these changes, we believe that the focus of the paper is explicitly defined to be on the neuronal representation of color in mouse visual cortex, rather than on the comparison of retinal and cortical color processing.</p><disp-quote content-type="editor-comment"><p>Minor:</p><p>In the abstract, the second sentence says that we already know the mechanisms in primates.</p><p>Unfortunately, I do not think this is true. First, primates refers to an order with several species, which might have adaptations to their color-processing. Second, I’m aware of several characterizations in &quot;primates&quot; that have led to convincing models (as referenced), but in my opinion, this is far from a true understanding the mechanisms, especially since very little is known about foveal color processing due to the difficulties of these experiments. Similarly in the introduction. &quot;Primates&quot; is indirectly defined as a species. Perhaps some rewording is needed here as well, since we know how different cone distributions can be in rodents (see Peichl’s work).</p></disp-quote><p>Thanks. We have reworded the Abstract and Introduction towards indicating that many studies have been performed in primate species, without suggesting that the mechanisms are described.</p><disp-quote content-type="editor-comment"><p>The legend in Fig. 2 has a &quot;Fig. ???&quot;</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p><bold>Referee 2 (Remarks to the Author):</bold></p><p>Franke et al. characterize the representation of color in the primary visual cortex of mice, highlighting how this changes across the visual field. Using calcium imaging in awake, head-fixed mice, they characterize the properties of V1 neurons (layer 2/3) using a large center-surround stimulation where green and ultra-violet colors were presented in random combinations. Clustering of responses revealed a set of functional cell-types based on their preference to different combinations of green and UV in their center and surround. These functional types were demonstrated to have different spatial distributions across V1, including one neuronal type (Green-ON/UV-OFF) that was much more prominent in the posterior V1 (i.e. upper visual field). Modelling work suggests that these neurons likely support the detection of predator-like objects in the sky.</p><p>Strengths: The large-scale single-cell resolution imaging used in this work allows the authors to map the responses of individual neurons across large regions of the visual cortex. Combining this large dataset with clustering analysis enabled the authors to group V1 neurons into distinct functional cell types and demonstrate their relative distribution in the upper and lower visual fields. Modelling work demonstrated the different capacity of each functional type to detect objects in the sky, providing insight into the ethological relevance of color opponent neurons in V1.</p></disp-quote><p>We thank the reviewer for appreciating our study.</p><disp-quote content-type="editor-comment"><p>Weaknesses: While the study presents convincing evidence about the asymmetric distribution of color-opponent neurons in V1, the paper would greatly benefit from a more in-depth discussion of the caveats related to the conclusions drawn about their origin. This is particularly relevant regarding the conclusion drawn about the contribution of color opponent neurons in the retina. The mismatch between retinal color opponency and V1 color opponency could imply that this feature is not solely inherited from the retina, however, there are other plausible explanations that are not discussed here. Direct evidence for this statement remains weak.</p></disp-quote><p>Thanks for this comment. We removed the retinal findings from the abstract, as well as from the summary of our results in the Introduction. In addition, we now phrase the interpretation of the retinal results more conservatively and specifically highlight in the Discussion that comparing ex-vivo retinal to in-vivo cortical data is challenging. With these changes, we believe that the focus of the paper is explicitly defined to be on the neuronal representation of color in mouse visual cortex, rather than on the comparison of retinal and cortical color processing.</p><disp-quote content-type="editor-comment"><p>In addition, the paper would benefit from adding explicit neuron counts or percentages to the quadrants of each of the density plots in Figures 2-5. The variance explained by the principal components does not capture the percentage of color opponent cells. Additionally, there appear to be some remaining errors in the figure legend and labels that have not been addressed (e.g. ’??’ in Fig 2 legend).</p></disp-quote><p>Thank you for this suggestion. We believe that adding the numbers or percentages to the figure panels would make them too crowded. Instead, we have now mentioned in the Results section and the legends that the percentages of variance explained by the color (off-diagonal) and luminance axis (diagonal) correlate with the number of neurons located in the color (top left and bottom right) and luminance contrast quadrants (top right and bottom left), respectively. Together with the number of neurons in each plot stated in the legends and the scale bar indicating the number of neurons per gray level, we hope this approach provides clarity for the reader to interpret the panels. Additionally, we have fixed the broken reference in the legend of Fig. 2.</p><disp-quote content-type="editor-comment"><p>Overall, this study will be a valuable resource for researchers studying color vision, cortical processing, and the processing of ethologically relevant information. It provides a useful basis for future work on the origin of color opponency in V1 and its ethological relevance.</p><p>General Suggestions:</p><p>- Please add possible caveats of using ETA method to the discussion section. For example, it is unclear to what extent ON/OFF cells are being overlooked by using ETA method.</p></disp-quote><p>We now discuss the limitations of the ETA approach in the Discussion section.</p><disp-quote content-type="editor-comment"><p>- The caveats of using the percentage of variance explained in the retina as evidence against V1 solely inheriting color-opponency from retinal output neurons are not adequately addressed. For example, could the mismatch in explained variance of the color axis between V1 and RGCs be explained by a subset of non-color opponent RGCs projecting elsewhere (not dLGN-V1) or that color opponent cells project to a larger number of neurons in V1 than non-color opponent cells? We suggest adding a paragraph to the discussion to address this issue.</p></disp-quote><p>We have removed these conclusions from the paper, more carefully interpret the retinal results and mention that comparing ex-vivo retina data with in-vivo cortical data is challenging.</p><disp-quote content-type="editor-comment"><p>- Please clarify how the different response types shown in Figure 5e-f lead to differences in noise detection and thereby differences in predator discriminability. For example, why does Gon/UVoff not respond to the noise scene while Goff/UVoff does?</p></disp-quote><p>We added this to the Results section.</p><disp-quote content-type="editor-comment"><p>- Please clarify the relationship between ETA amplitude, neural response probability, and neural response amplitude. For example, do color-opponent cells have equal absolute neural response amplitudes to the different colors?</p></disp-quote><p>Thank you for bringing up this point. The ETA is obtained by summing the stimulus sequences that elicit an event (i.e., response), weighted by the amplitude of the response. Consequently, the absolute amplitude of the ETA correlates with the calcium amplitude. Importantly, the ETA amplitudes of different stimulus conditions are comparable because they were estimated on the same normalized calcium trace. Therefore, comparing the absolute amplitudes of ETAs of color-opponent neurons reveals the response magnitude of the cells to different colors. We have now included this information in the Results section.</p><disp-quote content-type="editor-comment"><p>Abstract: - &quot;more than a third of neurons in mouse V1 are color-opponent in their receptive field center&quot;. It is unclear what data supports this statement. Can you please provide a statement in the manuscript that supports this directly using the number of neurons?</p></disp-quote><p>We added the following sentence to the Results section: Nevertheless, a substantial fraction of neurons (33.1%) preferred color-opponent stimuli and scattered along the off-diagonal in the upper left and lower right quadrants, especially for the RF center.</p><disp-quote content-type="editor-comment"><p>Figure 2: - There is a ?? in the figure legend. Which figure should this refer to? - please provide explicit neuron counts/percentages for each quadrant in b.</p></disp-quote><p>We fixed the figure reference. We believe that adding the numbers or percentages to the figure panels would make them too crowded. Instead, we have now mentioned in the Results section and the legends that the percentages of variance explained by the color (off-diagonal) and luminance axis (diagonal) correlate with the number of neurons located in the color (top left and bottom right) and luminance contrast quadrants (top right and bottom left), respectively. Together with the number of neurons in each plot stated in the legends and the scale bar indicating the number of neurons per gray level, we hope this approach provides clarity for the reader to interpret the panels.</p><disp-quote content-type="editor-comment"><p>Figure 3: - Fig 3: Color scheme makes it very difficult to differentiate the different conditions, especially when printed.</p></disp-quote><p>Thanks we changed the color scheme.</p><disp-quote content-type="editor-comment"><p>- Add explicit neuron counts/percentages for each quadrant in b.</p></disp-quote><p>See above.</p><disp-quote content-type="editor-comment"><p>Figure 4: - Add explicit neuron counts/percentages for each quadrant in b.</p></disp-quote><p>See above.</p><disp-quote content-type="editor-comment"><p>Figure 5: - Add explicit neuron counts/percentages for each quadrant in c.</p></disp-quote><p>See above.</p><disp-quote content-type="editor-comment"><p>Methods: - &quot;we modeled each response type to have a square RF with 10 degrees visual angle in diameter&quot;. There appears to be a mismatch between this statement and Figure 5e where 18 degrees is reported.</p></disp-quote><p>Thanks we fixed that.</p><disp-quote content-type="editor-comment"><p><bold>Referee 3 (Remarks to the Author):</bold></p><p>This paper studies chromatic coding in mouse primary visual cortex. Calcium responses of a large collection of cells are measured in response to a simple spot stimulus. These responses are used to estimate chromatic tuning properties - specifically sensitivity to UV and green stimuli presented in a large central spot or a larger still surrounding region. Cells are divided based on their responses to these stimuli into luminance or chromatic sensitive groups. The results are interesting and many aspects of the experiments and conclusions are well done; several technical concerns, however, limit the support for several main conclusions,</p><p>Limitations of stimulus choice The paper relies on responses to a large (37.5 degree diameter) modulated spot and surround region. This spot is considerably larger than the receptive fields of both V1 cells and retinal ganglion cells (it is twice the area of the average V1 receptive field). As a result, the spot itself is very likely to strongly activate both center and surround mechanisms, and responses of cells are likely to depend on where the receptive fields are located within the spot</p><p>(and, e.g., how much of the true neural surround samples the center spot vs the surround region). Most importantly, the surrounds of most of the recorded cells will be strongly activated by the central spot. This brings into question statements in the paper about selective activation of center and surround (e.g. page 2, right column). This in turn raises questions about several subsequent analyses that rely on selective center and surround activation.</p></disp-quote><p>Thank you for this comment. A similar point was raised by a reviewer in the first round of revision. We agree with the reviewers that it is critical to discuss both the rationale behind our stimulus design and its limitations to facilitate better interpretation by the reader.</p><p>To be able to record from many V1 neurons simultaneously, we used a stimulus size of 37.5 degree visual angle in diameter, which is slightly larger than center RFs of single V1 neurons (between 20 - 30 degrees visual angle depending on the stimulus, see here). The disadvantage of this approach is that the stimulus is only roughly centered on the neurons’ center RFs. To reduce the impact of potential stimulus misalignment on our results, we used the following steps: { For each recording, we positioned the monitor such that the mean RF across all neurons lies within the center of the stimulus field of view.</p><p>We confirmed that this procedure results in good stimulus alignment for the large majority of recorded neurons within individual recording fields by using a sparse noise stimulus (Suppl. Fig. 1a-c). Specifically, we found that for 83% of tested neurons, more than two thirds of their center RF, determined by the sparse noise stimulus, overlapped with the center spot of the color noise stimulus.</p><p>For analysis, we excluded neurons without a significant center STA, which may be caused by misalignment of the stimulus.</p><p>Together, we believe these points strongly suggest that the center spot and the surround annulus of the noise stimulus predominantly drive center (i.e. classical RF) and surround (i.e. extraclassical RF), respectively, of the recorded V1 neurons. This is further supported by the fact that color response types identified using an automated clustering method were robust across mice (Suppl. Fig. 6c), indicating consistent stimulus centering.</p><p>Nevertheless, we cannot exclude the possibility that the stimulus was misaligned for a subset of the recorded neurons used in our analysis. We agree with the reviewer that such misalignment might have caused the center stimulus to partially activate the surround. To further address this issue beyond the controls we have already implemented, one could compare the results of our approach with an approach that centers the stimulus on individual neurons. However, we believe that performing these additional experiments is beyond the scope of the current study.</p><p>To acknowledge the experimental limitations of our study and the concerns brought up by the reviewer, we have added the steps we perform to reduce the effects of stimulus misalignment in the Results section and discuss the problem of stimulus alignment in the Discussion in a separate section. With this, we believe our manuscript explains both the rationale behind our stimulus design as well as important limitations of the approach.</p><disp-quote content-type="editor-comment"><p>Comparison with retina A key conclusion of the paper is that the chromatic tuning in V1 is not inherited from retinal ganglion cells. This conclusion comes from comparing chromatic tuning in a previously-collected data set from retina with the present results. But the retina recordings were made using a considerably smaller spot, and hence it is not clear that the comparison made in the paper is accurate. For example, the stimulus used for the V1 experiments almost certainly strongly stimulates both center and surround of retinal ganglion cells. The text focuses on color opponency in the receptive field centers of retinal ganglion cells, but center-surround opponency seems at least as relevant for such large spots. This issue needs to be described more clearly and earlier in the paper.</p></disp-quote><p>Thanks for this comment. We removed the retinal findings from the abstract, as well as from the summary of our results in the Introduction. In addition, we now phrase the interpretation of the retinal results more conservatively and specifically highlight in the Discussion that comparing ex-vivo retinal to in-vivo cortical data is challenging. With these changes, we believe that the focus of the paper is explicitly defined to be on the neuronal representation of color in mouse visual cortex, rather than on the comparison of retinal and cortical color processing.</p><disp-quote content-type="editor-comment"><p>Limitations associated with ETA analysis One of the reviewers in the previous round of reviews raised the concern that the ETA analysis may not accurately capture responses of cells with nonlinear receptive field properties such as On/Off cells. This possibility and whether it is a concern should be discussed.</p></disp-quote><p>Thanks for this comment. We now discuss the limitation of using an ETA analysis in the</p><p>Discussion section.</p><disp-quote content-type="editor-comment"><p>Discrimination performance poor Discriminability of color or luminance is used as a measure of population coding. The discrimination performance appears to be quite poor - with 500-1000 neurons needed to reliably distinguish light from dark or green from UV. Intuitively I would expect that a single cell would provide such discrimination. Is this intuition wrong? If not, how do we interpret the discrimination analyses?</p></disp-quote><p>Thank you for raising this point. The plots in Fig. 2c (and Figs. 3-5) show discriminability in bits, with the discrimination accuracy in % highlighted by the dotted horizontal lines. For 500 neurons, the discriminability is approx. 0.8 bits, corresponding to 95% accuracy. Even for 50 neurons, the accuracy is significantly above chance level. We now mention in the legends that the dotted lines indicate decoding accuracy in %.</p></body></sub-article></article>