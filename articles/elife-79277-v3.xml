<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79277</article-id><article-id pub-id-type="doi">10.7554/eLife.79277</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Neuroscout, a unified platform for generalizable and reproducible fMRI research</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-135866"><name><surname>de la Vega</surname><given-names>Alejandro</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9062-3778</contrib-id><email>delavega@utexas.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-276105"><name><surname>Rocca</surname><given-names>Roberta</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-249639"><name><surname>Blair</surname><given-names>Ross W</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-249641"><name><surname>Markiewicz</surname><given-names>Christopher J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6533-164X</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-276106"><name><surname>Mentch</surname><given-names>Jeff</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-276107"><name><surname>Kent</surname><given-names>James D</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-276108"><name><surname>Herholz</surname><given-names>Peer</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9840-6257</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-54797"><name><surname>Ghosh</surname><given-names>Satrajit S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5312-6729</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-140955"><name><surname>Poldrack</surname><given-names>Russell A</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-60357"><name><surname>Yarkoni</surname><given-names>Tal</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Department of Psychology, The University of Texas at Austin</institution></institution-wrap><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01aj84f44</institution-id><institution>Interacting Minds Centre, Aarhus University</institution></institution-wrap><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Department of Psychology, Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03vek6s52</institution-id><institution>Program in Speech and Hearing Bioscience and Technology, Harvard University</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ymca674</institution-id><institution>McGovern Institute for Brain Research, Massachusetts Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01pxwe438</institution-id><institution>McConnell Brain Imaging Centre, Montreal Neurological Institute, McGill University</institution></institution-wrap><addr-line><named-content content-type="city">Montreal</named-content></addr-line><country>Canada</country></aff><aff id="aff7"><label>7</label><institution>Department of Otolaryngology, Harvard Medical School</institution><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Hunt</surname><given-names>Laurence Tudor</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Makin</surname><given-names>Tamar R</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/013meh722</institution-id><institution>University of Cambridge</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>30</day><month>08</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e79277</elocation-id><history><date date-type="received" iso-8601-date="2022-04-06"><day>06</day><month>04</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-08-27"><day>27</day><month>08</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-04-08"><day>08</day><month>04</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.04.05.487222"/></event></pub-history><permissions><copyright-statement>© 2022, de la Vega, Rocca et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>de la Vega, Rocca et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79277-v3.pdf"/><abstract><p>Functional magnetic resonance imaging (fMRI) has revolutionized cognitive neuroscience, but methodological barriers limit the generalizability of findings from the lab to the real world. Here, we present Neuroscout, an end-to-end platform for analysis of naturalistic fMRI data designed to facilitate the adoption of robust and generalizable research practices. Neuroscout leverages state-of-the-art machine learning models to automatically annotate stimuli from dozens of fMRI studies using naturalistic stimuli—such as movies and narratives—allowing researchers to easily test neuroscientific hypotheses across multiple ecologically-valid datasets. In addition, Neuroscout builds on a robust ecosystem of open tools and standards to provide an easy-to-use analysis builder and a fully automated execution engine that reduce the burden of reproducible research. Through a series of meta-analytic case studies, we validate the automatic feature extraction approach and demonstrate its potential to support more robust fMRI research. Owing to its ease of use and a high degree of automation, Neuroscout makes it possible to overcome modeling challenges commonly arising in naturalistic analysis and to easily scale analyses within and across datasets, democratizing generalizable fMRI research.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>naturalistic</kwd><kwd>fMRI</kwd><kwd>generalizability</kwd><kwd>neuroinformatics</kwd><kwd>reproducibility</kwd><kwd>open source</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH109682</award-id><principal-award-recipient><name><surname>de la Vega</surname><given-names>Alejandro</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH096906</award-id><principal-award-recipient><name><surname>de la Vega</surname><given-names>Alejandro</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R24MH117179</award-id><principal-award-recipient><name><surname>Herholz</surname><given-names>Peer</given-names></name><name><surname>Ghosh</surname><given-names>Satrajit S</given-names></name><name><surname>Blair</surname><given-names>Ross W</given-names></name><name><surname>Markiewicz</surname><given-names>Christopher J</given-names></name><name><surname>Poldrack</surname><given-names>Russell A</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100010785</institution-id><institution>Canada First Research Excellence Fund</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Herholz</surname><given-names>Peer</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009408</institution-id><institution>Brain Canada Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Herholz</surname><given-names>Peer</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution>Unifying Neuroscience and Artificial Intelligence</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Herholz</surname><given-names>Peer</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A web-based analysis platform for public fMRI data using naturalistic stimuli, leveraging state-of-the-art feature extraction models to enable more generalizable and reproducible findings.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Functional magnetic resonance imaging (fMRI) is a popular tool for investigating how the brain supports real-world cognition and behavior. Vast amounts of resources have been invested in fMRI research, and thousands of fMRI studies mapping cognitive functions to brain anatomy are published every year. Yet, increasingly urgent methodological concerns threaten the reliability of fMRI results and their generalizability from laboratory conditions to the real world.</p><p>A key weakness of current fMRI research concerns its generalizability—that is, whether conclusions drawn from individual studies apply beyond the participant sample and experimental conditions of the original study (<xref ref-type="bibr" rid="bib75">Turner et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Bossier et al., 2020</xref>; <xref ref-type="bibr" rid="bib85">Yarkoni, 2020</xref>; <xref ref-type="bibr" rid="bib72">Szucs and Ioannidis, 2017</xref>). A major concern is the type of stimuli used in the majority of fMRI research. Many studies attempt to isolate cognitive constructs using highly controlled and limited sets of reductive stimuli, such as still images depicting specific classes of objects in isolation, or pure tones. However, such stimuli radically differ in complexity and cognitive demand from real-world contexts, calling into question whether resulting inferences generalize outside the laboratory to more ecological settings (<xref ref-type="bibr" rid="bib61">Nastase et al., 2020</xref>). In addition, predominant statistical analysis approaches generally fail to model stimulus-related variability. As a result, many studies–and especially those relying on small stimulus sets–likely overestimate the strength of their statistical evidence and their generalizability to new but equivalent stimuli (<xref ref-type="bibr" rid="bib79">Westfall et al., 2016</xref>). Finally, since fMRI studies are frequently underpowered due to the cost of data collection, results can fail to replicate on new participant samples (<xref ref-type="bibr" rid="bib18">Button et al., 2013</xref>; <xref ref-type="bibr" rid="bib25">Cremers et al., 2017</xref>).</p><p>Naturalistic paradigms using life-like stimuli have been advocated as a way to increase the generalizability of fMRI studies (<xref ref-type="bibr" rid="bib28">DuPre et al., 2020</xref>; <xref ref-type="bibr" rid="bib43">Hamilton and Huth, 2020</xref>; <xref ref-type="bibr" rid="bib61">Nastase et al., 2020</xref>; <xref ref-type="bibr" rid="bib71">Sonkusare et al., 2019</xref>). Stimuli such as movies and narratives feature rich, multidimensional variation, presenting an opportunity to test hypotheses from highly controlled experiments in more ecological settings. Yet, despite the proliferation of openly available naturalistic datasets, challenges in modeling these data limit their impact. Naturalistic features are difficult to characterize and co-occur with potential confounds in complex and unexpected ways (<xref ref-type="bibr" rid="bib61">Nastase et al., 2020</xref>). This is exacerbated by the laborious task of annotating events at fine temporal resolution, which limits the number of variables that can realistically be defined and modelled. As a result, isolating relationships between specific features of the stimuli and brain activity in naturalistic data is especially challenging, which deters researchers from conducting naturalistic experiments and limiting re-use of existing public datasets.</p><p>A related and more fundamental concern limiting the impact of fMRI research is the low reproducibility of analysis workflows. Incomplete reporting practices in combination with flexible and variable analysis methods (<xref ref-type="bibr" rid="bib20">Carp, 2012</xref>) are a major culprit. For instance, a recent large-scale effort to test identical hypotheses in the same dataset by 70 teams found a high degree of variability in the results, with different teams often reaching different conclusions (<xref ref-type="bibr" rid="bib14">Botvinik-Nezer et al., 2020</xref>). Even re-executing the original analysis from an existing publication is rarely possible, due to insufficient provenance and a reliance on exclusively verbal descriptions of statistical models and analytical workflows (<xref ref-type="bibr" rid="bib36">Ghosh et al., 2017</xref>; <xref ref-type="bibr" rid="bib53">Mackenzie-Graham et al., 2008</xref>).</p><p>The recent proliferation of community-led tools and standards—most notably the Brain Imaging Data Structure (<xref ref-type="bibr" rid="bib38">Gorgolewski et al., 2016</xref>) standard—has galvanized efforts to foster reproducible practices across the data analysis lifecycle. A growing number of data archives, such as OpenNeuro (<xref ref-type="bibr" rid="bib56">Markiewicz et al., 2021c</xref>), now host hundreds of publicly available neuroimaging datasets, including dozens of naturalistic fMRI datasets. The development of standardized quality control and preprocessing pipelines, such as MRIQC (<xref ref-type="bibr" rid="bib29">Esteban et al., 2017</xref>), fmriprep (<xref ref-type="bibr" rid="bib30">Esteban et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Esteban et al., 2022</xref>), and C-PAC (<xref ref-type="bibr" rid="bib24">Craddock et al., 2013</xref>), facilitate their analysis and can be launched on emerging cloud-based platforms, such as <ext-link ext-link-type="uri" xlink:href="https://brainlife.io/about/">https://brainlife.io/about/</ext-link> (<xref ref-type="bibr" rid="bib10">Avesani et al., 2019</xref>). However, fMRI model specification and estimation remains challenging to standardize, and typically results in bespoke modeling pipelines that are not often shared, and can be difficult to re-use. Unfortunately, despite the availability of a rich ecosystem of tools, assembling them into a complete and reproducible workflow remains out of reach for many scientists due to substantial technical challenges.</p><p>In response to these challenges, we developed Neuroscout: a unified platform for generalizable and reproducible analysis of naturalistic fMRI data. Neuroscout improves current research practice in three key ways. First, Neuroscout provides an easy-to-use interface for reproducible analysis of BIDS datasets, seamlessly integrating a diverse ecosystem of community-developed resources into a unified workflow. Second, Neuroscout encourages re-analysis of public naturalistic datasets by providing access to hundreds of predictors extracted through an expandable set of state-of-the-art feature extraction algorithms spanning multiple stimulus modalities. Finally, by using standardized model specifications and automated workflows, Neuroscout enables researchers to easily operationalize hypotheses in a uniform way across multiple (and diverse) datasets, facilitating more generalizable multi-dataset workflows such as meta-analysis.</p><p>In the following, we provide a broad overview of the Neuroscout platform, and validate it by replicating well-established cognitive neuroscience findings using a diverse set of public naturalistic datasets. In addition, we present two case studies—face sensitivity of the fusiform face area and selectivity to word frequency in visual word form area—to show how Neuroscout can be used to conduct original research on public naturalistic data. Through these examples, we demonstrate how Neuroscout’s flexible interface and wide range of predictors make it possible to dynamically refine models and draw robust inference on naturalistic data, while simultaneously democratizing gold standard practices for reproducible research.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Overview of the Neuroscout platform</title><p>At its core, Neuroscout is a platform for reproducible fMRI research, encompassing the complete lifecycle of fMRI analysis from model specification and estimation to the dissemination of results. We focus particular attention on encouraging the re-use of public datasets that use intrinsically high dimensional and generalizable naturalistic stimuli such as movies and audio narratives. The platform is composed of three primary components: a data ingestion and feature extraction server, interactive analysis creation tools, and an automated model fitting workflow. All elements of the platform are seamlessly integrated and can be accessed interactively online (<ext-link ext-link-type="uri" xlink:href="https://neuroscout.org">https://neuroscout.org</ext-link>). Complete and up-to-date documentation of all of the platform’s components, including Getting Started guides to facilitate first time users, is available in the official Neuroscout Documentation (<ext-link ext-link-type="uri" xlink:href="https://neuroscout.org/docs">https://neuroscout.org/docs</ext-link>).</p><sec id="s2-1-1"><title>Preprocessed and harmonized naturalistic fMRI datasets</title><p>The Neuroscout server indexes a curated set of publicly available naturalistic fMRI datasets, and hosts automatically extracted annotations of visual, auditory, and linguistic events from the experimental stimuli. Datasets are harmonized, preprocessed, and ingested into a database using robust BIDS-compliant pipelines, facilitating future expansion.</p></sec><sec id="s2-1-2"><title>Automated annotation of stimuli</title><p>Annotations of stimuli are automatically extracted using <italic>pliers</italic> (<xref ref-type="bibr" rid="bib59">McNamara et al., 2017</xref>), a comprehensive feature extraction framework supporting state-of-the-art algorithms and deep learning models (<xref ref-type="fig" rid="fig1">Figure 1</xref>). Currently available features include hundreds of predictors coding for both low-level (e.g. brightness, loudness) and mid-level (e.g. object recognition indicators) properties of audiovisual stimuli, as well as natural language properties from force aligned speech transcripts (e.g. lexical frequency annotations). The set of available predictors can be easily expanded through community-driven implementation of new <italic>pliers</italic> extractors, as well as publicly shared repositories of deep learning models, such as HuggingFace (<xref ref-type="bibr" rid="bib81">Wolf et al., 2020</xref>) and TensorFlowHub (<xref ref-type="bibr" rid="bib1">Abadi et al., 2015</xref>). We expect that as machine learning models continue to evolve, it will be possible automatically extract higher level features from naturalistic stimuli. All extracted predictors are made publicly available through a well-documented application programming interface (<ext-link ext-link-type="uri" xlink:href="https://neuroscout.org/api">https://neuroscout.org/api</ext-link>). An interactive web tool that makes it possible to further refine extracted features through expert human curation is currently under development.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Example of automated feature extraction on stimuli from the “Merlin” dataset.</title><p>Visual features were extracted from video stimuli at a frequency of 1 Hz. ‘Faces’: we applied a well-validated cascaded convolutional network trained to detect the presence of faces (<xref ref-type="bibr" rid="bib90">Zhang et al., 2016</xref>). ‘Building’: We used Clarifai’s General Image Recognition model to compute the probability of the presence of buildings in each frame. ‘Spoken word frequency’ codes for the lexical frequency of words in the transcript, as determined by the SubtlexUS database (<xref ref-type="bibr" rid="bib16">Brysbaert and New, 2009</xref>). Language features are extracted using speech transcripts with precise word-by-word timing determined through forced alignment.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79277-fig1-v3.tif"/></fig></sec><sec id="s2-1-3"><title>Analysis creation and execution tools</title><p>Neuroscout’s interactive analysis creation tools—available as a web application (<ext-link ext-link-type="uri" xlink:href="https://neuroscout.org/builder">https://neuroscout.org/builder</ext-link>) and python library (pyNS)—enable easy creation of fully reproducible fMRI analyses (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). To build an analysis, users choose a dataset and task to analyze, select among pre-extracted predictors and nuisance confounds to include in the model, and specify statistical contrasts. Raw predictor values can be modified by applying model-specific variable transformations such as scaling, thresholding, orthogonalization, and hemodynamic convolution. Internally all elements of the multi-level statistical model are formally represented using the BIDS Statistical Models specification (<xref ref-type="bibr" rid="bib54">Markiewicz et al., 2021a</xref>), ensuring transparency and reproducibility. At this point, users can inspect the model through quality-control reports and interactive visualizations of the design matrix and predictor covariance matrix, iteratively refining models if necessary. Finalized analyses are locked from further modification, assigned a unique identifier, and packaged into a self-contained bundle.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Overview schematic of analysis creation and model execution.</title><p>(<bold>a</bold>) Interactive analysis creation is made possible through an easy-to-use web application, resulting in a fully specified reproducible analysis bundle. (<bold>b</bold>) Automated model execution is achieved with little-to-no configuration through a containerized model fitting workflow. Results are automatically made available in NeuroVault, a public repository for statistical maps.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79277-fig2-v3.tif"/></fig><p>Analyses can be executed in a single command line using Neuroscout’s automated model execution workflow (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Neuroscout uses container technology (i.e. Docker and Singularity) to minimize software dependencies, facilitate installation, and ensure portability across a wide range of environments (including high performance computers (HPC) and the cloud). At run time, preprocessed imaging data are automatically fetched using DataLad (<xref ref-type="bibr" rid="bib42">Halchenko et al., 2021</xref>), and the analysis is executed using FitLins (<xref ref-type="bibr" rid="bib55">Markiewicz et al., 2021b</xref>), a standardized pipeline for estimating BIDS Stats Models. Once completed, thresholded statistical maps and provenance metadata are submitted to NeuroVault (<xref ref-type="bibr" rid="bib37">Gorgolewski et al., 2015</xref>), a public repository for statistical maps, guaranteeing compliance to FAIR (findable, accessible, interoperable, and reusable) scientific principles (<xref ref-type="bibr" rid="bib80">Wilkinson et al., 2016</xref>). Finally, Neuroscout facilitates sharing and appropriately crediting the dataset and tools used in the analysis by automatically generating a bibliography that can be used in original research reports.</p></sec></sec><sec id="s2-2"><title>Scalable workflows for generalizable inference</title><p>Neuroscout makes it trivial to specify and analyze fMRI data in a way that meets gold standard reproducibility principles. This is per se a crucial contribution to fMRI research, which often fails basic reproducibility standards. However, Neuroscout’s transformative potential is fully realized through the scalability of its workflows. Automated feature extraction and standardized model specification make it easy to operationalize and test equivalent hypotheses across many datasets, spanning larger participant samples and a more diverse range of stimuli.</p><p>The following analyses demonstrate the potential of multi-dataset approaches and their importance for generalizable inference by investigating a set of well-established fMRI findings across all of Neuroscout’s datasets. We focused these analyses on three feature modalities (visual, auditory, and language), ranging from low-level features of the signal (loudness, brightness, presence of speech, and shot change), to mid-level characteristics with well established focal correlates (visual presence of buildings, faces, tools, landscape and text). For each feature and stimulus, we fit a whole-brain univariate GLM with the target feature as the sole predictor, in addition to standard nuisance covariates (see Methods for details). Finally, we combined estimates across twenty studies using random-effects image-based meta-analysis (IBMA), resulting in a consensus statistical map for each feature.</p><p>Even using a simple one-predictor approach, we observed robust meta-analytic activation patterns largely consistent with expectations from the existing literature (<xref ref-type="fig" rid="fig3">Figure 3</xref>), a strong sign of the reliability of automatically extracted predictors. We observed activation in the primary visual cortex for brightness (<xref ref-type="bibr" rid="bib64">Peters et al., 2010</xref>), parahippocampal place area (PPA) activation in response to buildings and landscapes (<xref ref-type="bibr" rid="bib63">Park and Chun, 2009</xref>; <xref ref-type="bibr" rid="bib45">Häusler et al., 2022</xref>), visual word form area (VWFA) activation in response to text (<xref ref-type="bibr" rid="bib22">Chen et al., 2019</xref>), and lateral occipito-temporal cortex (LOTC) and parietal activation in regions associated with action perception and action knowledge (<xref ref-type="bibr" rid="bib68">Schone et al., 2021</xref>; <xref ref-type="bibr" rid="bib77">Valyear et al., 2007</xref>) in response to the presence of tools on screen. For auditory features, we observed primary auditory cortex activation in response to loudness (<xref ref-type="bibr" rid="bib51">Langers et al., 2007</xref>), and superior temporal sulcus and gyrus activity in response to speech (<xref ref-type="bibr" rid="bib70">Sekiyama et al., 2003</xref>). We also observed plausible results for visual shot changes, a feature with fewer direct analogs from the literature, which yielded activations in the frontal eye fields, the precuneus, and parietal regions areas traditionally implicated in attentional orienting and reference frame shifts (<xref ref-type="bibr" rid="bib23">Corbetta et al., 1998</xref>; <xref ref-type="bibr" rid="bib35">Fox et al., 2006</xref>; <xref ref-type="bibr" rid="bib50">Kravitz et al., 2011</xref>; <xref ref-type="bibr" rid="bib65">Rocca et al., 2020</xref>). The only notable exception was a failure to detect fusiform face area (FFA) activity in response to faces (Figure 5), an interesting result that we dissect in the following section.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Meta-analytic statistical maps for GLM models targeting a variety of effects with strong priors from fMRI research.</title><p>Individual GLM models were fit for each effect of interest, and dataset level estimates were combined using image-based meta-analysis. Images were thresholded at Z=3.29 (<italic>P</italic>&lt;0.001) voxel-wise. Abbreviations: V1=primary visual cortex; FEF = frontal eye fields; AG = angular gyrus; PCUN = precuneus; A1=primary auditory cortex; PMC = premotor cortex; IFG = inferior frontal gyrus; STS = superior temporal sulcus; STG = superior temporal gyrus; PPA = parahippocampal place area; VWFA = visual word-form area; IPL = inferior parietal lobule; IPS = inferior parietal sulcus; LOTC = lateral occipito-temporal cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79277-fig3-v3.tif"/></fig><p>Crucially, although study-level results largely exhibited plausible activation patterns, a wide range of idiosyncratic variation was evident across datasets (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For instance, for ‘building’ we observed PPA activity in almost every study. However, we observed a divergent pattern of activity in the anterior temporal lobe (ATL), with some studies indicating a deactivation, others activation, and others no relationship. This dissonance was resolved in the meta-analysis, which indicated no relationship with ‘building’ and the ATL, but confirmed a strong association with the PPA. Similar study-specific variation can be observed with other features. These results highlight the limits of inferences made from single datasets, which could lead to drawing overly general conclusions. In contrast, multi-dataset meta-analytic approaches are intrinsically more robust to stimulus-specific variation, licensing broader generalization.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Comparison of a sample of four single study results with meta-analysis (N=20) for three features: ‘building’ and ‘text’ extracted through Clarifai visual scene detection models, and sound ‘loudness’ (root mean squared of the auditory signal).</title><p>Images were thresholded at Z=3.29 (p&lt;0.001) voxel-wise. Regions with a priori association with each predictor are highlighted: PPA, parahippocampal place area; VWFA, visual word form area; STS, superior temporal sulcus. Datasets: Budapest, Learning Temporal Structure (LTS), 500daysofsummer task from Naturalistic Neuroimaging Database, and Sherlock.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79277-fig4-v3.tif"/></fig></sec><sec id="s2-3"><title>Flexible covariate addition for robust naturalistic analysis</title><p>A notable exception to the successful replications presented in the previous section is the absence of fusiform face area (FFA) activation for faces in naturalistic stimuli (<xref ref-type="fig" rid="fig5">Figure 5</xref>). Given long-standing prior evidence implicating the FFA in face processing (<xref ref-type="bibr" rid="bib48">Kanwisher et al., 1997</xref>), it is highly unlikely that these results are indicative of flaws in the extant literature. A more plausible explanation is that our ‘naive’ single predictor models failed to account for complex scene dynamics present in naturalistic stimuli. Unlike controlled experimental designs, naturalistic stimuli are characterized by systematic co-occurrences between cognitively relevant events. For example, in narrative-driven movies (the most commonly used audio-visual naturalistic stimuli) the presentation of faces often co-occurs with speech—a strong driver of brain activity. Failing to account for this shared variance can confound model estimates and mask true effects attributable to predictors of interest.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Meta-analysis of face perception with iterative addition of covariates.</title><p>Left; Only including binary predictors coding for the presence of faces on screen did not reveal activity in the right fusiform face area (rFFA). Middle; Controlling for speech removed spurious activations and revealed rFFA association with face presentation. Right; Controlling for temporal adaptation to face identity in addition to speech further strengthened the association between rFFA and face presentation. N=17 datasets; images were thresholded at Z=3.29 (p&lt;0.001) voxel-wise.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79277-fig5-v3.tif"/></fig><p>Neuroscout addresses these challenges by pairing access to a wide range of pre-extracted features with a flexible and scalable model specification framework. Researchers can use Neuroscout’s model builder to iteratively build models that control and assess the impact of a wide range of potential confounds without the need for additional data collection or manual feature annotation. Analysis reports provide visualizations of the correlation structure of design matrices, which can inform covariate selection and facilitate interpretation. These affordances for iterative covariate control allow us to readily account for the potential confounding effect of speech, a predictor that co-varies with faces in some datasets but not others (Pearson’s R range: –0.55, 0.57; mean: 0.18). After controlling for speech, we observed an association between face presentation and right FFA activity across 17 datasets (<xref ref-type="fig" rid="fig5">Figure 5</xref>; peak z=5.70). Yet, the strength of this relationship remained weaker than one might expect from traditional face localizer tasks.</p><p>In movies, face perception involves repeated and protracted presentation of a relatively narrow set of individual faces. Given evidence of rapid adaptation of category-selective fMRI response to individual stimuli (<xref ref-type="bibr" rid="bib41">Grill-Spector et al., 1999</xref>), FFA activation in naturalistic stimuli may be attenuated by a failure to distinguish transient processes (e.g. initial encoding) from indiscriminate face exposure. To test the hypothesis that adaptation to specific faces suppresses FFA activity, we further refined our models by controlling for the cumulative time of exposure to face identities (in addition to controlling for speech). Using embeddings from FaceNet, a face recognition convolutional neural network, we clustered individual face presentations into groups representing distinct characters in each movie. We then computed the cumulative presentation of each face identity and included this regressor as a covariate.</p><p>After controlling for face adaptation, we observed stronger effects in the right FFA (<xref ref-type="fig" rid="fig5">Figure 5</xref>; peak z=7.35), highlighting its sensitivity to dynamic characteristics of face presentation which cannot always be captured by traditional designs. Notably, unlike in traditional localizer tasks, we still observe significant activation outside of the FFA, areas whose relation to face perception can be further explored in future analyses using Neuroscout’s rich feature set.</p></sec><sec id="s2-4"><title>Large samples meet diverse stimuli: a linguistic case study</title><p>Our final example illustrates the importance of workflow scalability in the domain of language processing, where the use of naturalistic input has been explicitly identified as not only beneficial but necessary for real-world generalizability (<xref ref-type="bibr" rid="bib43">Hamilton and Huth, 2020</xref>). Owing to their ability to provide more robust insights into real-life language processing, studies using naturalistic input (e.g. long written texts or narratives) are becoming increasingly common in language neuroscience (<xref ref-type="bibr" rid="bib8">Andric and Small, 2015</xref>; <xref ref-type="bibr" rid="bib15">Brennan, 2016</xref>; <xref ref-type="bibr" rid="bib62">Nastase et al., 2021</xref>). Yet, even when naturalistic stimuli are used, individual studies are rarely representative of the many contexts in which language production and comprehension take place in daily life (e.g. dialogues, narratives, written exchanges, etc), which raises concerns on the generalizability of their findings. Additionally, modeling covariates is particularly challenging for linguistic stimuli, due to their complex hierarchical structure. As a consequence, single studies are often at risk of lacking the power required to disentangle the independent contributions of multiple variables.</p><p>A concrete example of this scenario comes from one of the authors’ (TY) previous work (<xref ref-type="bibr" rid="bib82">Yarkoni et al., 2008</xref>). In a naturalistic rapid serial visual presentation (RSVP) reading experiment, <xref ref-type="bibr" rid="bib82">Yarkoni et al., 2008</xref> reported an interesting incidental result: activity in the visual word form area (VWFA)—an area primarily associated with visual feature detection and orthography-phonology mapping (<xref ref-type="bibr" rid="bib27">Dietz et al., 2005</xref>)—was significantly modulated by lexical frequency. Interestingly, these effects were robust to phonological and orthographic covariates, suggesting that VWFA activity may not only be involved in orthographic and phonological reading subprocesses, but also modulated by modality-independent lexical-semantic properties of linguistic input. Yet, as the experiment only involved visual presentation of linguistic stimuli, this hypothesis could not be corroborated empirically. In addition, the authors observed that frequency effects disappeared when controlling for lexical concreteness. As the two variables were highly correlated, the authors speculated that the study may have lacked the power to disentangle their contributions and declared the results inconclusive.</p><p>Neuroscout makes it possible to re-evaluate linguistic hypotheses in ecological stimuli using a wide range of linguistic annotations spanning both phonological/orthographic word properties (e.g. duration and phonological distinctiveness), semantic descriptors (e.g. valence, concreteness, sensorimotor attributes), and higher-level information-theoretic properties of language sequences (e.g. entropy in next-word prediction and word-by-word surprisal). We reimplemented analytic models from <xref ref-type="bibr" rid="bib82">Yarkoni et al., 2008</xref> across all Neuroscout datasets, including regressors for word frequency, concreteness, speech, and control orthographic measures (number of syllables, number of phones, and duration), alongside a standard set of nuisance parameters. As before, we used IBMA to compute meta-analytic estimates for each variable. The resulting maps displayed significant VWFA effects for both frequency and concreteness (<xref ref-type="fig" rid="fig6">Figure 6</xref>), corroborating the hypothesis of its involvement in lexical processing independent of presentation modality, and arguably in the context of language-to-imagery mapping.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Meta-analytic statistical maps for concreteness and frequency controlling for speech, text length, number of syllables and phonemes, and phone-level Levenshtein distance.</title><p>N=33 tasks; images were thresholded at Z=3.29 (p&lt;0.001) voxel-wise. Visual word form area, VWFA.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79277-fig6-v3.tif"/></fig><p>Note that had we only had access to results from the original study, our conclusions might have been substantially different. Using a relatively liberal threshold of p&lt;0.01, only 12 out of 33 tasks showed significant ROI-level association between VWFA and frequency, and only 5 tasks showed an association between VWFA and concreteness. In addition, in only one task was VWFA significantly associated with both frequency and concreteness. These ROI-level results highlight the power of scalability in the context of naturalistic fMRI analysis. By drawing on larger participant samples and more diverse stimuli, meta-analysis overcomes power and stimulus variability limitations that can cause instability in dataset-level results.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Neuroscout seeks to promote the widespread adoption of reproducible and generalizable fMRI research practices, allowing users to easily test a wide range of hypotheses in dozens of open naturalistic datasets using automatically extracted neural predictors. The platform is designed with a strong focus on reproducibility, providing a unified framework for fMRI analysis that reduces the burden of reproducible fMRI analysis and facilitates transparent dissemination of models and statistical results. Owing to its high degree of automation, Neuroscout also facilitates the use of meta-analytic workflows, enabling researchers to test the robustness and generalizability of their models across multiple datasets.</p><p>We have demonstrated how Neuroscout can incentivize more ecologically generalizable fMRI research by addressing common modeling challenges that have traditionally deterred naturalistic research. In particular, as we show in our meta-analyses, automatically extracted predictors can be used to test a wide range of hypotheses on naturalistic datasets without the need for costly manual annotation. Although we primarily focused on replicating established effects for validation, a range of predictors operationalizing less explored cognitive variables are already available in the platform, and, as machine learning algorithms continue to advance, we expect possibilities for interesting additions to Neuroscout’s feature set to keep growing at a fast pace. As a result, we have designed Neuroscout and its underlying feature extraction framework <italic>pliers</italic> to facilitate community-led expansion to novel extractors— made possible by the rapid increase in public repositories of pre-trained deep learning models such as HuggingFace (<xref ref-type="bibr" rid="bib1">Abadi et al., 2015</xref>) and TensorFlow Hub (<xref ref-type="bibr" rid="bib1">Abadi et al., 2015</xref>).</p><p>We have also shown how Neuroscout’s scalability facilitates the use of meta-analytic workflows, which enable more robust and generalizable inference. As we have pointed out in some of our examples, small participant samples and stimulus-specific effects can at times lead to misleading dataset-level results. Automatically extracted predictors are particularly powerful when paired with Neuroscout’s flexible model specification and execution workflow, as their combination makes it easy to operationalize hypotheses in identical ways across multiple diverse dataset and gather more generalizable consensus estimates. While large-N studies are becoming increasingly common in cognitive neuroscience, the importance of relying on large and diverse stimulus sets has been thus far underestimated (<xref ref-type="bibr" rid="bib79">Westfall et al., 2016</xref>), placing Neuroscout in a unique position in the current research landscape. Importantly, although we have primarily focused on demonstrating the advantages of large-scale workflows in the context of meta-analysis, scalability can also be leveraged for other secondary workflows (e.g. machine learning pipelines, multi-verse analyses, or mega-analyses) and along dimensions other than datasets (e.g. model parameters such as transformations and covariates).</p><p>A fundamental goal of Neuroscout is to provide researchers with tools that automatically ensure the adoption of gold-standard research practices throughout the analysis lifecycle. We have paid close attention to ensuring transparency and reproducibility of statistical modeling by adopting a community-developed specification of statistical models, and developing accessible tools to specify, visualize and execute analyses. Neuroscout’s model builder can be readily accessed online, and the execution engine is designed to be portable, ensuring seamless deployment across computational environments. This is a key contribution to cognitive neuroscience, which too often falls short of meeting these basic criteria of sound scientific research.</p><sec id="s3-1"><title>Challenges and future directions</title><p>A major challenge in the analysis of naturalistic stimuli is the high degree of collinearity between features, as the interpretation of individual features is dependent on co-occurring features. In many cases, controlling for confounding variables is critical for the interpretation of the primary feature— as is evident in our investigation of the relationship between FFA and face perception. However, it can also be argued that in dynamic narrative driven media (i.e. films and movies), the so-called confounds themselves encode information of interest that cannot or should not be cleanly regressed out (<xref ref-type="bibr" rid="bib39">Grall and Finn, 2020</xref>).</p><p>Absent a consensus on how to model naturalistic data, we designed Neuroscout to be agnostic to the goals of the user and empower them to construct sensibly designed models through comprehensive model reports. An ongoing goal of the platform—especially as the number of features continues to increase—will be to expand the visualizations and quality control reports to enable users to better understand the predictors and their relationship. For instance, we are developing an interactive visualization of the covariance between all features in Neuroscout that may help users discover relationships between a predictor of interest and potential confounds.</p><p>However, as the number of features continues to grow, a critical future direction for Neuroscout will be to implement statistical models which are optimized to estimate a large number of covarying targets. Of note are regularized encoding models, such as the banded-ridge regression as implemented by the Himalaya package (<xref ref-type="bibr" rid="bib52">Latour et al., 2022</xref>). These models have the additional advantage of implementing feature-space selection and variance partitioning methods, which can deal with the difficult problem of model selection in highly complex feature spaces such as naturalistic stimuli. Such models are particularly useful for modeling high-dimensional embeddings, such as those produced by deep learning models. Many such extractors are already implemented in pliers and we have begun to extract and analyze these data in a prototype workflow that will soon be made widely available.</p><p>Although we have primarily focused on naturalistic datasets—as they intrinsically feature a high degree of reusability and ecological validity—Neuroscout workflows are applicable to any BIDS-compliant dataset due to the flexibility of the BIDS Stats Model specification. Indexing non-naturalistic fMRI datasets will be an important next step, an effort that will be supported by the proliferation of data sharing portals and require the widespread sharing of harmonized preprocessed derivatives that can be automatically ingested. Other important expansions include facilitating analysis execution by directly integrating with cloud-based neuroscience analysis platforms, such as <ext-link ext-link-type="uri" xlink:href="https://brainlife.io/about/">https://brainlife.io/about/</ext-link> (<xref ref-type="bibr" rid="bib10">Avesani et al., 2019</xref>), and facilitating the collection of higher-level stimulus features by integrating with crowdsourcing platforms such as MechanicalTurk or Prolific.</p><p>In addition, as Neuroscout grows to facilitate the re-analysis of a broader set of public datasets, it will be important to reckon with the threat of ‘dataset decay’ which can occur from repeated sequential re-analysis (<xref ref-type="bibr" rid="bib73">Thompson et al., 2020</xref>). By encouraging the central registration of all analysis attempts and the associated results, Neuroscout is designed to minimize undocumented researcher degrees of freedom and link the final published results with all previous attempts. By encouraging the public sharing of all results, we hope to encourage meta-scientists to empirically investigate statistical solutions to the problem of dataset decay, and develop methods to minimize the effect of false positives.</p></sec><sec id="s3-2"><title>Long-term sustainability</title><p>An on-going challenge for scientific software tools—especially those that rely on centralized services—is long-term maintenance, development, and user support. On-going upkeep of core tools and development of new features require a non-trivial amount of developer time. This problem is exacerbated for projects primarily supported by government funding, which generally prefers novel research to the on-going maintenance of existing tools. This is particularly challenging for centralized services, such as the Neuroscout server and web application, which require greater maintenance and coordination for upkeep.</p><p>With this in mind, we have designed many of the core components of Neuroscout with modularity as a guiding principle in order to maximize the longevity and impact of the platform. Although components of the platform are tightly integrated, they are also designed to be independently useful, increasing their general utility, and encouraging broader adoption by the community. For example, our feature extraction library (pliers) is designed for general purpose use on multimodal stimuli, and can be easily expanded to adopt novel extractors. On the analysis execution side, rather than implementing a bespoke analysis workflow, we worked to develop a general specification for statistical models under the BIDS standard (<ext-link ext-link-type="uri" xlink:href="https://bids-standard.github.io/stats-models/">https://bids-standard.github.io/stats-models/</ext-link>) and a compatible execution workflow (FitLins; <ext-link ext-link-type="uri" xlink:href="https://github.com/poldracklab/fitlins">https://github.com/poldracklab/fitlins</ext-link>; <xref ref-type="bibr" rid="bib57">Markiewicz, 2022</xref>). By distributing the technical debt of Neuroscout across various independently used and supported projects, we hope to maximize the robustness and impact of the platform. To ensure the community’s needs are met, users are encouraged to vote on the prioritization of features by voting on issues on Neuroscout’s GitHub repository, and code from new contributors is actively encouraged.</p></sec><sec id="s3-3"><title>User support and feedback</title><p>A comprehensive overview of the platform and guides for getting started can be found in the integrated Neuroscout documentation (<ext-link ext-link-type="uri" xlink:href="https://neuroscout.org/docs">https://neuroscout.org/docs</ext-link>), as well as in each tool’s version-specific automatically generated documentation (hosted by ReadTheDocs, a community-supported documentation platform). We plan to grow the collection of complete tutorials replicating exemplary analyses and host them in the centralized Neuroscout documentation.</p><p>Users can ask questions to developers and the community using the topic ‘neuroscout’ <ext-link ext-link-type="uri" xlink:href="https://neurostars.org/">Neurostars.org</ext-link>— a public forum for neuroscience researchers and neuroinformatics infrastructure maintainers supported by the International Neuroinformatics Coordinating Facility (INCF). In addition, users can provide direct feedback through a form found on all pages in the Neuroscout website, which directly alerts developers to user concerns. A quarterly mailing list is also available to stay up to date with the latest feature developments in the platform. Finally, the Neuroscout developer team frequently participates at major neuroinformatics hackathons (such as Brainhack events and at major neuroimaging conferences), and plans on hosting ongoing Neuroscout-specific hackathons.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Code availability</title><p>All code from our processing pipeline and core Neuroscout infrastructure is available online (<ext-link ext-link-type="uri" xlink:href="https://www.github.com/neuroscout/neuroscout">https://www.github.com/neuroscout/neuroscout</ext-link>; <xref ref-type="bibr" rid="bib3">Alejandro de la, 2022a</xref>), including the Python client library pyNS (<ext-link ext-link-type="uri" xlink:href="https://www.github.com/neuroscout/pyNS">https://www.github.com/neuroscout/pyNS</ext-link>; <xref ref-type="bibr" rid="bib4">Alejandro de la, 2022b</xref>). The Neuroscout-CLI analysis engine is available as a Docker and Singularity container, and the source code is also made available (<ext-link ext-link-type="uri" xlink:href="https://github.com/neuroscout/neuroscout-cli/">https://github.com/neuroscout/neuroscout-cli/</ext-link>; <xref ref-type="bibr" rid="bib5">Alejandro de la, 2022c</xref>). Finally, an online supplement following the analyses showcased in this paper is available as interactive Jupyter Book (<ext-link ext-link-type="uri" xlink:href="https://neuroscout.github.io/neuroscout-paper/">https://neuroscout.github.io/neuroscout-paper/</ext-link>). All are available under a permissive BSD license.</p></sec><sec id="s4-2"><title>Datasets</title><p>The analyses presented in this paper are based on 13 naturalistic fMRI datasets sourced from various open data repositories (see <xref ref-type="table" rid="table1">Table 1</xref>). We focused on BIDS-compliant datasets which included the exact stimuli presented with precise timing information. Datasets were queried and parsed using <italic>pybids</italic> (<ext-link ext-link-type="uri" xlink:href="https://github.com/bids-standard/pybids">https://github.com/bids-standard/pybids</ext-link>; <xref ref-type="bibr" rid="bib84">Yarkoni et al., 2019b</xref>; <xref ref-type="bibr" rid="bib83">Yarkoni et al., 2019a</xref>) and ingested into a SQL database for further subsequent analysis. Several datasets spanned various original studies or distinct simuli (e.g. Narratives, NNDb), resulting in 35 unique ‘tasks’ or ‘studies’ available for analysis. The full list of datasets and their available predictors are available on Neuroscout (<ext-link ext-link-type="uri" xlink:href="https://neuroscout.org/datasets">https://neuroscout.org/datasets</ext-link>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Neuroscout datasets included in the validation analyses.</title><p>Subj is the number of unique subjects. Scan Time is the mean scan time per subject (in minutes). AV = Audio-Visual; AN = Audio Narrative.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Name</th><th align="left" valign="bottom">Subj</th><th align="left" valign="bottom">DOI/URI</th><th align="left" valign="bottom">Scan time</th><th align="left" valign="bottom">Modality</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom">Study Forrest (<xref ref-type="bibr" rid="bib44">Hanke et al., 2014</xref>)</td><td align="char" char="." valign="bottom">13</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds000113.v1.3.0">10.18112/openneuro.ds000113.v1.3.0</ext-link></td><td align="char" char="." valign="bottom">120</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">Slightly abridged German version of the movie: ‘Forrest Gump’</td></tr><tr><td align="left" valign="bottom">Life (<xref ref-type="bibr" rid="bib60">Nastase et al., 2018</xref>)</td><td align="char" char="." valign="bottom">19</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://datasets.datalad.org/?dir=/labs/haxby/life">datasets.datalad.org/?dir=/labs/haxby/life</ext-link></td><td align="char" char="." valign="bottom">62.8</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">Four segments of the Life nature documentary</td></tr><tr><td align="left" valign="bottom">Raiders (<xref ref-type="bibr" rid="bib46">Haxby et al., 2011</xref>)</td><td align="char" char="." valign="bottom">11</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://datasets.datalad.org/?dir=/labs/haxby/raiders">datasets.datalad.org/?dir=/labs/haxby/raiders</ext-link></td><td align="char" char="." valign="bottom">113.3</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">Full movie: ‘Raiders of the Lost Ark’</td></tr><tr><td align="left" valign="bottom">Learning Temporal Structure (LTS) (<xref ref-type="bibr" rid="bib7">Aly et al., 2018</xref>)</td><td align="char" char="." valign="bottom">30</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds001545.v1.1.1">10.18112/openneuro.ds001545.v1.1.1</ext-link></td><td align="char" char="." valign="bottom">20.1</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">Three clips from the movie ‘Grand Budapest Hotel’, presented six times each. Some clips were scrambled.</td></tr><tr><td align="left" valign="bottom">Sherlock (<xref ref-type="bibr" rid="bib21">Chen et al., 2017</xref>)</td><td align="char" char="." valign="bottom">16</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds001132.v1.0.0">10.18112/openneuro.ds001132.v1.0.0</ext-link></td><td align="char" char="." valign="bottom">23.7</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">The first half of the first episode from ‘Sherlock’ TV series.</td></tr><tr><td align="left" valign="bottom">SherlockMerlin (<xref ref-type="bibr" rid="bib88">Zadbood et al., 2017</xref>)</td><td align="char" char="." valign="bottom">18</td><td align="left" valign="bottom">Temporarily unavailable</td><td align="char" char="." valign="bottom">25.1</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">Full episode from ‘Merlin’ TV series. Only used Merlin task to avoid analyzing the Sherlock task twice.</td></tr><tr><td align="left" valign="bottom">Schematic Narrative (<xref ref-type="bibr" rid="bib11">Baldassano et al., 2018</xref>)</td><td align="char" char="." valign="bottom">31</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds001510.v2.0.2">10.18112/openneuro.ds001510.v2.0.2</ext-link></td><td align="char" char="." valign="bottom">50.4</td><td align="left" valign="bottom">AV/AN</td><td align="left" valign="bottom">16 three-minute clips, including audiovisual clips and narration.</td></tr><tr><td align="left" valign="bottom">ParanoiaStory (<xref ref-type="bibr" rid="bib33">Finn et al., 2018</xref>)</td><td align="char" char="." valign="bottom">22</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds001338.v1.0.0">10.18112/openneuro.ds001338.v1.0.0</ext-link></td><td align="char" char="." valign="bottom">21.8</td><td align="left" valign="bottom">AN</td><td align="left" valign="bottom">Audio narrative designed to elicit individual variation in suspicion/paranoia.</td></tr><tr><td align="left" valign="bottom">Budapest (<xref ref-type="bibr" rid="bib78">Visconti et al., 2020</xref>)</td><td align="char" char="." valign="bottom">25</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds003017.v1.0.3">10.18112/openneuro.ds003017.v1.0.3</ext-link></td><td align="char" char="." valign="bottom">50.9</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">The majority of the movie ‘Grand Budapest Hotel’, presented in intact order</td></tr><tr><td align="left" valign="bottom">Naturalistic Neuroimaging Database (NNDb) (<xref ref-type="bibr" rid="bib6">Aliko et al., 2020</xref>)</td><td align="char" char="." valign="bottom">86</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds002837.v2.0.0">10.18112/openneuro.ds002837.v2.0.0</ext-link></td><td align="char" char="." valign="bottom">112.03</td><td align="left" valign="bottom">AV</td><td align="left" valign="bottom">Movie watching of 10 full-length movies</td></tr><tr><td align="left" valign="bottom">Narratives (<xref ref-type="bibr" rid="bib62">Nastase et al., 2021</xref>)</td><td align="char" char="." valign="bottom">328</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds002345.v1.1.4">10.18112/openneuro.ds002345.v1.1.4</ext-link></td><td align="char" char="." valign="bottom">32.5</td><td align="left" valign="bottom">AN</td><td align="left" valign="bottom">Passive listening of 16 audio narratives (two tasks were not analyzed due to preprocessing error)</td></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>fMRI Preprocessing</title><p>Neuroscout datasets are uniformly preprocessed using FMRIPREP (version 1.2.2) (<xref ref-type="bibr" rid="bib31">Esteban et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Esteban et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Esteban et al., 2022</xref>), a robust NiPype-based MRI preprocessing pipeline. The resulting preprocessed data are publicly available for download (<ext-link ext-link-type="uri" xlink:href="https://github.com/neuroscout-datasets">https://github.com/neuroscout-datasets</ext-link>). The following methods description was semi-automatically generated by FMRIPREP.</p><p>Each T1-weighted (T1w) volume is corrected for intensity non-uniformity using N4BiasFieldCorrection v2.1.0 (<xref ref-type="bibr" rid="bib76">Tustison et al., 2010</xref>) and skull-stripped using antsBrainExtraction.sh v2.1.0 (using the OASIS template). Spatial normalization to the ICBM 152 Nonlinear Asymmetrical template version 2009c (<xref ref-type="bibr" rid="bib34">Fonov et al., 2009</xref>) is performed through nonlinear registration with the antsRegistration tool of ANTs v2.1.0 (<xref ref-type="bibr" rid="bib9">Avants et al., 2008</xref>), using brain-extracted versions of both T1w volume and template. Brain tissue segmentation of cerebrospinal fluid (CSF), white matter (WM), and gray matter (GM) were performed on the brain-extracted T1w using fast (<xref ref-type="bibr" rid="bib89">Zhang et al., 2001</xref>) (FSL v5.0.9).</p><p>Functional data are motion-corrected using mcflirt (FSL v5.0.9, <xref ref-type="bibr" rid="bib47">Jenkinson et al., 2002</xref>). The images are subsequently co-registered to the T1w volume using boundary-based registration (<xref ref-type="bibr" rid="bib40">Greve and Fischl, 2009</xref>) with 9 degrees of freedom, using flirt (FSL). Motion correcting transformations, BOLD-to-T1w transformation, and T1w-to-template warp were concatenated and applied in a single step using antsApplyTransforms (ANTs v2.1.0) using Lanczos interpolation.</p><p>Anatomically based physiological noise regressors were created using CompCor (<xref ref-type="bibr" rid="bib12">Behzadi et al., 2007</xref>). A mask to exclude signals with cortical origin is obtained by eroding the brain mask, ensuring it only contains subcortical structures. Six principal components are calculated within the intersection of the subcortical mask and the union of CSF and WM masks calculated in T1w space, after their projection to the native space of each functional run. Many internal operations of FMRIPREP use Nilearn (<xref ref-type="bibr" rid="bib2">Abraham et al., 2014</xref>), principally within the BOLD-processing workflow.</p></sec><sec id="s4-4"><title>Automatically extracted features</title><sec id="s4-4-1"><title>Overview</title><p>Neuroscout leverages state-of-the-art machine learning algorithms to automatically extract hundreds of novel neural predictors from the original experimental stimuli. Automated feature extraction relies on <italic>pliers</italic>, a python library for multimodal feature extraction which provides a standardized interface to a diverse set of machine learning algorithms and APIs (<xref ref-type="bibr" rid="bib59">McNamara et al., 2017</xref>). Feature values are ingested directly with no in place modifications, with the exception of down sampling of highly dense variables to 3 hz to facilitate storage on the server. For all analyses reported in this paper the same set of feature extractors are applied across all datasets (see <xref ref-type="table" rid="table2">Table 2</xref>), except where not possible due to modality mismatch (e.g. visual features in audio narratives), or features intrinsically absent from the stimuli (e.g. faces in the <italic>Life</italic> nature documentary). A description of all features included in this paper is provided below. A complete list of available predictors and features is actualized online at: <ext-link ext-link-type="uri" xlink:href="https://neuroscout.org/predictors">https://neuroscout.org/predictors</ext-link>.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Extractor name, feature name, and description for all Neuroscout features used in the validation analyses.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Extractor</th><th align="left" valign="bottom">Feature</th><th align="left" valign="bottom">Description</th></tr></thead><tbody><tr><td align="left" valign="bottom">Brightness</td><td align="left" valign="bottom">brightness</td><td align="left" valign="bottom">Average luminosity across all pixels in each video frame.</td></tr><tr><td align="left" valign="bottom">Clarifai</td><td align="left" valign="bottom">building, landscape, text, tool</td><td align="left" valign="bottom">Indicators of the probability that an object belonging to each of these categories is present in the video frame.</td></tr><tr><td align="left" valign="bottom">FaceNet</td><td align="left" valign="bottom">any_faces, log_mean_time_cum</td><td align="left" valign="bottom">For each video frame, any_faces indicates the probability that the image displays at least one face. log_mean_time_cum indicates the cumulative time (in seconds) a given face has been on screen up since the beginning of the movie. If multiple faces are present, their cumulative time on screen is averaged.</td></tr><tr><td align="left" valign="bottom">Google Video Intelligence</td><td align="left" valign="bottom">shot_change</td><td align="left" valign="bottom">Binary indicator coding for shot changes.</td></tr><tr><td align="left" valign="bottom">FAVE/Rev</td><td align="left" valign="bottom">speech</td><td align="left" valign="bottom">Binary indicator coding for the presence of speech in the audio signal, inferred from word onsets/offsets information from force-aligned speech transcripts.</td></tr><tr><td align="left" valign="bottom">RMS</td><td align="left" valign="bottom">rms</td><td align="left" valign="bottom">Root mean square (RMS) energy of the audio signal.</td></tr><tr><td align="left" valign="bottom">Lexical norms</td><td align="left" valign="bottom">Log10WF, concreteness, phonlev, numsylls, numphones, duration, text_length</td><td align="left" valign="bottom">Logarithm of SubtlexUS lexical frequency, concreteness rating, phonological Levenshtein distance, number of syllables, number of phones, average auditory duration and number of characters for each word in the speech transcript. These metrics are extracted from lexical databases available through pliers.</td></tr></tbody></table></table-wrap></sec><sec id="s4-4-2"><title>Visual features</title><sec id="s4-4-2-1"><title>Brightness</title><p>We computed brightness (average luminosity) for frame samples of videos by computing the average luminosity for pixels across the entire image. We took the maximum value at each pixel from the RGB channels, computed the mean, and divided by 255.0 (the maximum value in RGB space), resulting in a scalar ranging from 0 to 1. This extractor is available through pliers as <italic>BrightnessExtractor</italic>.</p></sec><sec id="s4-4-2-2"><title>Clarifai object detection</title><p>Clarifai is a computer vision company that specializes in using deep learning networks to annotate images through their API as a service. We used Clarifai’s ‘General’ model, a pre-trained deep convolutional neural network (CNN) for multi-class classification of over 11,000 categories of visual concepts, including objects and themes.</p><p>To reduce the space of possible concepts, we pre-selected four concepts that could plausibly capture psychologically relevant categories (see <xref ref-type="table" rid="table2">Table 2</xref>). Feature extraction was performed using <italic>pliers</italic>’ <italic>ClarifaiAPIImageExtractor</italic>, which wraps Clarifai’s Python API client. We submitted the sampled visual frames from video stimuli to the Clarifai API, and received values representing the model’s predicted probability of each concept for that frame.</p></sec><sec id="s4-4-2-3"><title>Face detection, alignment, and recognition</title><p>Face detection, alignment, and recognition were performed using the <italic>FaceNet</italic> package (<ext-link ext-link-type="uri" xlink:href="https://github.com/davidsandberg/facenet">https://github.com/davidsandberg/facenet</ext-link>; <xref ref-type="bibr" rid="bib67">Sandberg, 2018</xref>), which is an open TensorFlow implementation of state-of-the-art face recognition CNNs. As this feature was not natively available in <italic>pliers</italic>, we computed it offline and uploaded it to Neuroscout using the feature upload portal.</p><p>First, face detection, alignment, and cropping are performed through Multi-task Cascaded Convolutional Networks (MTCNN; <xref ref-type="bibr" rid="bib90">Zhang et al., 2016</xref>). This framework uses unified cascaded CNNs to detect, landmark, and crop the position of a face in an image. We input sampled frames from video stimuli, and the network identified, separated, and cropped individual faces for further processing. At this step, we were able to identify if a given frame in a video contained one or more faces (‘any_faces’).</p><p>Next, cropped faces were input to the <italic>FaceNet</italic> network for facial recognition. <italic>FaceNet</italic> is a face recognition deep CNN based on the Inception ResNet v1 architecture that achieved state-of-the-art performance when released (<xref ref-type="bibr" rid="bib69">Schroff et al., 2015</xref>). The particular recognition model we used was pre-trained on the VGGFace2 dataset (<xref ref-type="bibr" rid="bib19">Cao et al., 2018</xref>), which is composed of over three million faces ‘in the wild’, encompassing a wide range of poses, emotions, lighting, and occlusion conditions. <italic>FaceNet</italic> creates a 512-dimensional embedding vector from cropped faces that represents extracted face features; thus more similar faces are closer in the euclidean embedding space.</p><p>For each dataset separately, we clustered all detected faces’ embedding vectors to group together faces corresponding to distinct characters in the audio-visual videos. We used the Chinese Whispers clustering algorithm, as this algorithm subjectively grouped faces into coherent clusters better than other commonly used algorithms (e.g. k-means clustering). Depending on the dataset, this resulted in 50–200 clusters that subjectively corresponded to readily identifiable characters across the video stimulus. For each dataset, we removed the worst-performing cluster (as for all datasets there was always one with a highly noisy profile) and grouped demonstrably different faces into one cluster. Using the generated face clusters for each dataset, we computed the cumulative time each character had been seen across the stimulus (i.e. entire movie) and log transformed the variable in order to represent the adaptation to specific faces over time. As more than one face could be shown simultaneously, we took the mean for all faces on screen in a given frame.</p></sec><sec id="s4-4-2-4"><title>Google Video Intelligence</title><p>We used the Google Video Intelligence API to identify shot changes in video stimuli. Using the <italic>GoogleVideoAPIShotDetectionExtractor</italic> extractor in <italic>pliers</italic>, we queried the API with complete video clips (typically one video per run). The algorithm separates distinct video segments, by detecting abstract shot changes in the video (i.e. the frames before and after that frame are visually different). The time at which there was a transition between two segments was given a value of 1, while all other time points received a value of 0.</p></sec></sec></sec><sec id="s4-5"><title>Auditory features</title><sec id="s4-5-1"><title>RMS</title><p>We used <italic>librosa</italic> (<xref ref-type="bibr" rid="bib58">McFee et al., 2015</xref>), a python package for music and audio analysis, to compute root-mean-squared (RMS) as a measure of the instantaneous audio power over time, or ‘loudness’.</p></sec><sec id="s4-5-2"><title>Speech forced alignment</title><p>For most datasets, transcripts of the speech with low-resolution or no timing information were available either from the original researcher or via closed captions in the case of commercially produced media. We force aligned the transcripts to extract word-level speech timing, using the Forced Alignment and Vowel Extraction toolkit (FAVE; <xref ref-type="bibr" rid="bib66">Rosenfelder et al., 2014</xref>). FAVE employs Gaussian mixture model based monophone Hidden Markov Models (HMMs) from the Penn Phonetics Lab Forced Aligner for English (p2fa; <xref ref-type="bibr" rid="bib87">Yuan and Liberman, 2008</xref>), which is based on the Hidden Markov Toolkit (<xref ref-type="bibr" rid="bib86">Young, 1994</xref>). The transcripts are mapped to phone sequences with pre-trained HMM acoustic models. Frames of the audio recording are then mapped onto the acoustic models, to determine the most likely sequence. The alignment is constrained by the partial timing information available in closed captions, and the sequence present in the original transcripts. Iterative alignment continues until models converge. Linguistic features are available for all datasets except <italic>studyforrest</italic>, as the movie was presented in German. Transcription and annotation of stimuli in languages other than English are pending.</p></sec><sec id="s4-5-3"><title>Rev.com</title><p>For datasets that had no available transcript (<italic>LearningTemporalStructure</italic>, <italic>SchematicNarrative</italic>), we used a professional speech-to-text service (<ext-link ext-link-type="uri" xlink:href="https://www.rev.com/">Rev.com</ext-link>) to obtain precise transcripts with word-level timing information. <ext-link ext-link-type="uri" xlink:href="https://www.rev.com/">Rev.com</ext-link> provides human-created transcripts which are then force-aligned using proprietary methods to produce a high-quality, aligned transcript, similar to that generated by the FAVE algorithm.</p></sec><sec id="s4-5-4"><title>Speech indicator</title><p>In both cases, we binarized the resulting aligned transcripts based on word onset/offset information to produce a fine-grained speech presence feature (‘speech’). These aligned transcripts served as the input to all subsequent speech-based analyses.</p></sec></sec><sec id="s4-6"><title>Language features</title><sec id="s4-6-1"><title>Word frequency</title><p>Neuroscout includes a variety of frequency norms extracted from different lexical databases. For all the analyses reported here, we used frequency norms from SUBTLEX-US (<xref ref-type="bibr" rid="bib16">Brysbaert and New, 2009</xref>), a 51 million words corpus of American English subtitles. The variable used in the analyses (Log10WF, see <xref ref-type="bibr" rid="bib16">Brysbaert and New, 2009</xref>) is the base 10 logarithm of the number of occurrences of the word in the corpus. In all analyses, this variable was demeaned and rescaled prior to HRF convolution. For a small percentage of words not found in the dictionary, a value of zero was applied after rescaling, effectively imputing the value as the mean word frequency. This feature was extracted using the <italic>subtlexusfrequency dictionary</italic> and the <italic>PredefinedDictionaryExtractor</italic> available in <italic>pliers</italic>.</p></sec><sec id="s4-6-2"><title>Concreteness</title><p>Concreteness norms were extracted from the (<xref ref-type="bibr" rid="bib17">Brysbaert et al., 2014</xref>) concreteness database, which contains norms for over 40,000 English words, obtained from participants’ ratings on a five-point scale. In all analyses, this variable was demeaned and rescaled before HRF convolution. This feature was extracted using the <italic>concreteness</italic> dictionary and the <italic>PredefinedDictionaryExtractor</italic> available in <italic>pliers</italic>.</p></sec><sec id="s4-6-3"><title>Massive auditory lexical decision norms</title><p>The Massive Auditory Lexical Decision (MALD) database (<xref ref-type="bibr" rid="bib74">Tucker et al., 2019</xref>) is a large-scale auditory and production dataset that includes a variety of lexical, orthographic, and phonological descriptors for over 35,000 English words and pseudowords. MALD norms are available in Neuroscout for all words in stimulus transcripts. The analyses reported in this paper make use of the following variables:</p><list list-type="bullet"><list-item><p><italic>Duration</italic>: duration of spoken word in milliseconds;</p></list-item><list-item><p><italic>NumPhones</italic>: number of phones, that is of distinct speech sounds;</p></list-item><list-item><p><italic>NumSylls</italic>: number of syllables;</p></list-item><list-item><p><italic>PhonLev</italic>: mean phone-level Levenshtein distance of the spoken word from all items in the reference pronunciation dictionary, i.e. the CMU pronouncing dictionary with a few additions. This variable quantifies average phonetic similarity with the rest of the lexicon so as to account for neighborhood density and lexical competition effects (<xref ref-type="bibr" rid="bib82">Yarkoni et al., 2008</xref>).</p></list-item></list><p>In all analyses, these variables were demeaned and rescaled before HRF convolution. MALD metrics was extracted using the <italic>massiveauditorylexicaldecision</italic> dictionary and the <italic>PredefinedDictionaryExtractor</italic> available in pliers.</p></sec><sec id="s4-6-4"><title>Text length</title><p>This variable corresponds to the number of characters in a word’s transcription. A <italic>TextLengthExtractor</italic> is available in <italic>pliers</italic>.</p></sec></sec><sec id="s4-7"><title>GLM models</title><p>Neuroscout uses FitLins, a newly developed workflow for executing multi-level fMRI general linear model (GLM) analyses defined by the BIDS StatsModels specification. FitLins uses <italic>pybids</italic> to generate run-level design matrices, and <italic>NiPype</italic> to encapsulate a multi-level GLM workflow. Model estimation at the first level was performed using <italic>AFNI</italic>—in part due to its memory efficiency—and subject and group level summary statistics were fit using the nilearn.glm module.</p><p>For all models, we included a standard set of confounds from <italic>fmriprep</italic>, in addition to the listed features of interest. This set includes 6 rigid-body motion-correction parameters, 6 noise components calculated using CompCor, a cosine drift model, and non-steady state volume detection, if present for that run. Using <italic>pybids</italic>, we convolved the regressors with an implementation of the SPM dispersion derivative haemodynamic response model, and computed first-level design matrices downsampled to the TR. We fit the design matrices to the unsmoothed registered images using a standard AR(1) + noise model.</p><p>Smoothing was applied to the resulting parameter estimate images using a 4 mm FWHM isotropic kernel. For the datasets that had more than one run per subject, we then fit a subject-level fixed-effects model with the smoothed run-level parameter estimates as inputs, resulting in subject-level parameter estimates for each regressor. Finally, we fit a group-level fixed-effects model using the previous level’s parameter estimates and performed a one-sample t-test contrast for each regressor in the model.</p></sec><sec id="s4-8"><title>Meta-analysis</title><p>NiMARE (version 0.0.11rc1; available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/neurostuff/NiMARE">https://github.com/neurostuff/NiMARE</ext-link>; <ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_017398">RRID:SCR_017398</ext-link>) was used to perform meta-analyses across the neuroscout datasets. Typical study harmonization steps (smoothing, design matrix scaling, spatial normalization) were forgone because all group level beta and variance maps were generated using the same GLM pipeline. All group level beta and variance maps were resampled to a 2x2 × 2 mm ICBM 152 Nonlinear Symmetrical gray matter template (downloaded using <italic>nilearn</italic>, version 0.8.0) with linear interpolation. Resampled values were clipped to the minimum and maximum statistical values observed in the original maps. We used the DerSimonian &amp; Laird random effects meta-regression algorithm (<xref ref-type="bibr" rid="bib26">DerSimonian and Laird, 1986</xref>; <xref ref-type="bibr" rid="bib49">Kosmidis et al., 2017</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Resources, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Validation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Software, Formal analysis, Writing – review and editing</p></fn><fn fn-type="con" id="con7"><p>Software, Writing – review and editing</p></fn><fn fn-type="con" id="con8"><p>Conceptualization, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con9"><p>Conceptualization, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Software, Supervision, Funding acquisition, Validation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-79277-mdarchecklist1-v3.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code from our processing pipeline and core infrastructure is available online (<ext-link ext-link-type="uri" xlink:href="https://www.github.com/neuroscout/neuroscout">https://www.github.com/neuroscout/neuroscout</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a358fe02406a82b5e06c79e8ca6edd2b0332f817;origin=https://www.github.com/neuroscout/neuroscout;visit=swh:1:snp:e5c6a790ee1d247df3d49425aae56354b1d7ea25;anchor=swh:1:rev:ed79e9cf4b1ee1320a2d43c72e95f3fd3619c9b7">swh:1:rev:ed79e9cf4b1ee1320a2d43c72e95f3fd3619c9b7</ext-link>). An online supplement including all analysis code and resulting images is available as a public GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/neuroscout/neuroscout-paper">https://github.com/neuroscout/neuroscout-paper</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:fe145e1dee75eed4fd448712fb191bc94b6cc2e6;origin=https://github.com/neuroscout/neuroscout-paper;visit=swh:1:snp:a6ee5f741929e340e9659413c310edd47f4a9bc2;anchor=swh:1:rev:abd64777a96bdcefdfdf40edc70ff6ed937a5bcc">swh:1:rev:abd64777a96bdcefdfdf40edc70ff6ed937a5bcc</ext-link>). All analysis results are made publicly available in a public GitHub repository (<ext-link ext-link-type="uri" xlink:href="https://github.com/neuroscout/neuroscout-paper">https://github.com/neuroscout/neuroscout-paper</ext-link>).</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Hanke</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>studyforrest</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds000113.v1.3.0</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Neural responses to naturalistic clips of animals</data-title><source>DataLad</source><pub-id pub-id-type="accession" xlink:href="https://datasets.datalad.org/?dir=/labs/haxby/life">/labs/haxby/life</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset3"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Dartmouth Raiders Dataset</data-title><source>DataLad</source><pub-id pub-id-type="accession" xlink:href="https://datasets.datalad.org/?dir=/labs/haxby/raiders">/labs/haxby/raiders</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset4"><person-group person-group-type="author"><name><surname>Aly</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Learning Temporal Structure</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds001545.v1.1.1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset5"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Yong</surname><given-names>CH</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>Sherlock</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds001132.v1.0.0</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset6"><person-group person-group-type="author"><name><surname>Zadbood</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017</year><data-title>SherlockMerlin</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds001110</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset7"><person-group person-group-type="author"><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Schematic Narrative</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds001510.v2.0.2</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset8"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Corlett</surname><given-names>PR</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Constable</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>ParanoiaStory</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds001338.v1.0.0</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset9"><person-group person-group-type="author"><collab>Visconti di Oleggio Castello M, Chauhan V, Jiahui G, Gobbini MI</collab></person-group><year iso-8601-date="2020">2020</year><data-title>Budapest</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds003017.v1.0.3</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset10"><person-group person-group-type="author"><name><surname>Aliko</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Gheorghiu</surname><given-names>F</given-names></name><name><surname>Meliss</surname><given-names>S</given-names></name><name><surname>Skipper</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Naturalistic Neuroimaging Database</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds002837.v2.0.0</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset11"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Narratives</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds002345.v1.1.4</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>Neuroscout is made possible by funding from the National Institute of Mental Health (NIMH) of the National Institute of Health (NIH) under award number R01MH109682. In addition, research in this preprint and critical infrastructure was supported by NIMH awards P41EB019936, R24MH117179 and R01MH096906. P.H. was supported in part by the Canada First Research Excellence Fund; the Brain Canada Foundation; and Unifying Neuroscience and Artificial Intelligence - Québec.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Brevdo</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Citro</surname><given-names>C</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Harp</surname><given-names>A</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Kudlur</surname><given-names>M</given-names></name><name><surname>Levenberg</surname><given-names>J</given-names></name><name><surname>Mané</surname><given-names>D</given-names></name><name><surname>Monga</surname><given-names>R</given-names></name><name><surname>Moore</surname><given-names>S</given-names></name><name><surname>Murray</surname><given-names>D</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name><name><surname>Schuster</surname><given-names>M</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Talwar</surname><given-names>K</given-names></name><name><surname>Tucker</surname><given-names>P</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Vasudevan</surname><given-names>V</given-names></name><name><surname>Viégas</surname><given-names>F</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>P</given-names></name><name><surname>Wattenberg</surname><given-names>M</given-names></name><name><surname>Wicke</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1603.04467">https://arxiv.org/abs/1603.04467</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Alejandro de la</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022a</year><data-title>Neuroscout</data-title><version designator="swh:1:rev:ed79e9cf4b1ee1320a2d43c72e95f3fd3619c9b7">swh:1:rev:ed79e9cf4b1ee1320a2d43c72e95f3fd3619c9b7</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:a358fe02406a82b5e06c79e8ca6edd2b0332f817;origin=https://www.github.com/neuroscout/neuroscout;visit=swh:1:snp:e5c6a790ee1d247df3d49425aae56354b1d7ea25;anchor=swh:1:rev:ed79e9cf4b1ee1320a2d43c72e95f3fd3619c9b7">https://archive.softwareheritage.org/swh:1:dir:a358fe02406a82b5e06c79e8ca6edd2b0332f817;origin=https://www.github.com/neuroscout/neuroscout;visit=swh:1:snp:e5c6a790ee1d247df3d49425aae56354b1d7ea25;anchor=swh:1:rev:ed79e9cf4b1ee1320a2d43c72e95f3fd3619c9b7</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Alejandro de la</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022b</year><data-title>PyNS</data-title><version designator="swh:1:rev:db62c4b7eb4d1f23ff5f4e59617a58c4206e68ea">swh:1:rev:db62c4b7eb4d1f23ff5f4e59617a58c4206e68ea</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:9e0df08165a466b72ab1a93b4053217ac0bfa816;origin=https://www.github.com/neuroscout/pyNS;visit=swh:1:snp:cfc6ef33422c927dfec1d14d9afecdc2ccfb4448;anchor=swh:1:rev:db62c4b7eb4d1f23ff5f4e59617a58c4206e68ea">https://archive.softwareheritage.org/swh:1:dir:9e0df08165a466b72ab1a93b4053217ac0bfa816;origin=https://www.github.com/neuroscout/pyNS;visit=swh:1:snp:cfc6ef33422c927dfec1d14d9afecdc2ccfb4448;anchor=swh:1:rev:db62c4b7eb4d1f23ff5f4e59617a58c4206e68ea</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Alejandro de la</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022c</year><data-title>Neuroscout</data-title><version designator="swh:1:rev:d4399ce98ec44f20dcd1072454d0bd57c9c14ac2">swh:1:rev:d4399ce98ec44f20dcd1072454d0bd57c9c14ac2</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:39e35f7067bafa4e1a7ac0fa2c665262dc606069;origin=https://github.com/neuroscout/neuroscout-cli;visit=swh:1:snp:1d3251e6e4ac57b35c8605f574a492a03c31bfad;anchor=swh:1:rev:d4399ce98ec44f20dcd1072454d0bd57c9c14ac2">https://archive.softwareheritage.org/swh:1:dir:39e35f7067bafa4e1a7ac0fa2c665262dc606069;origin=https://github.com/neuroscout/neuroscout-cli;visit=swh:1:snp:1d3251e6e4ac57b35c8605f574a492a03c31bfad;anchor=swh:1:rev:d4399ce98ec44f20dcd1072454d0bd57c9c14ac2</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aliko</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name><name><surname>Gheorghiu</surname><given-names>F</given-names></name><name><surname>Meliss</surname><given-names>S</given-names></name><name><surname>Skipper</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A naturalistic neuroimaging database for understanding the brain using ecological stimuli</article-title><source>Scientific Data</source><volume>7</volume><elocation-id>347</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-020-00680-2</pub-id><pub-id pub-id-type="pmid">33051448</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aly</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Turk-Browne</surname><given-names>NB</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning naturalistic temporal structure in the posterior medial network</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1345</fpage><lpage>1365</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01308</pub-id><pub-id pub-id-type="pmid">30004848</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Andric</surname><given-names>M</given-names></name><name><surname>Small</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>FMRI methods for studying the neurobiology of language under naturalistic conditions</chapter-title><person-group person-group-type="editor"><name><surname>Willems</surname><given-names>RM</given-names></name></person-group><source>Cognitive Neuroscience of Natural Language Use</source><publisher-loc>Cambridge, UK</publisher-loc><publisher-name>Cambridge University Press</publisher-name><fpage>8</fpage><lpage>28</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avesani</surname><given-names>P</given-names></name><name><surname>McPherson</surname><given-names>B</given-names></name><name><surname>Hayashi</surname><given-names>S</given-names></name><name><surname>Caiafa</surname><given-names>CF</given-names></name><name><surname>Henschel</surname><given-names>R</given-names></name><name><surname>Garyfallidis</surname><given-names>E</given-names></name><name><surname>Kitchell</surname><given-names>L</given-names></name><name><surname>Bullock</surname><given-names>D</given-names></name><name><surname>Patterson</surname><given-names>A</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Sporns</surname><given-names>O</given-names></name><name><surname>Saykin</surname><given-names>AJ</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Dinov</surname><given-names>I</given-names></name><name><surname>Hancock</surname><given-names>D</given-names></name><name><surname>Caron</surname><given-names>B</given-names></name><name><surname>Qian</surname><given-names>Y</given-names></name><name><surname>Pestilli</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The open diffusion data derivatives, brain data upcycling via integrated publishing of derivatives and reproducible open cloud services</article-title><source>Scientific Data</source><volume>6</volume><elocation-id>69</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-019-0073-y</pub-id><pub-id pub-id-type="pmid">31123325</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Representation of real-world event schemas during narrative perception</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9689</fpage><lpage>9699</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0251-18.2018</pub-id><pub-id pub-id-type="pmid">30249790</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Y</given-names></name><name><surname>Restom</surname><given-names>K</given-names></name><name><surname>Liau</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A component based noise correction method (compcor) for BOLD and perfusion based fmri</article-title><source>NeuroImage</source><volume>37</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bossier</surname><given-names>H</given-names></name><name><surname>Roels</surname><given-names>SP</given-names></name><name><surname>Seurinck</surname><given-names>R</given-names></name><name><surname>Banaschewski</surname><given-names>T</given-names></name><name><surname>Barker</surname><given-names>GJ</given-names></name><name><surname>Bokde</surname><given-names>ALW</given-names></name><name><surname>Quinlan</surname><given-names>EB</given-names></name><name><surname>Desrivières</surname><given-names>S</given-names></name><name><surname>Flor</surname><given-names>H</given-names></name><name><surname>Grigis</surname><given-names>A</given-names></name><name><surname>Garavan</surname><given-names>H</given-names></name><name><surname>Gowland</surname><given-names>P</given-names></name><name><surname>Heinz</surname><given-names>A</given-names></name><name><surname>Ittermann</surname><given-names>B</given-names></name><name><surname>Martinot</surname><given-names>JL</given-names></name><name><surname>Artiges</surname><given-names>E</given-names></name><name><surname>Nees</surname><given-names>F</given-names></name><name><surname>Orfanos</surname><given-names>DP</given-names></name><name><surname>Poustka</surname><given-names>L</given-names></name><name><surname>Fröhner Dipl-Psych</surname><given-names>JH</given-names></name><name><surname>Smolka</surname><given-names>MN</given-names></name><name><surname>Walter</surname><given-names>H</given-names></name><name><surname>Whelan</surname><given-names>R</given-names></name><name><surname>Schumann</surname><given-names>G</given-names></name><name><surname>Moerkerke</surname><given-names>B</given-names></name><collab>IMAGEN Consortium</collab></person-group><year iso-8601-date="2020">2020</year><article-title>The empirical replicability of task-based fmri as a function of sample size</article-title><source>NeuroImage</source><volume>212</volume><elocation-id>116601</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116601</pub-id><pub-id pub-id-type="pmid">32036019</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinik-Nezer</surname><given-names>R</given-names></name><name><surname>Holzmeister</surname><given-names>F</given-names></name><name><surname>Camerer</surname><given-names>CF</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Huber</surname><given-names>J</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Iwanir</surname><given-names>R</given-names></name><name><surname>Mumford</surname><given-names>JA</given-names></name><name><surname>Adcock</surname><given-names>RA</given-names></name><name><surname>Avesani</surname><given-names>P</given-names></name><name><surname>Baczkowski</surname><given-names>BM</given-names></name><name><surname>Bajracharya</surname><given-names>A</given-names></name><name><surname>Bakst</surname><given-names>L</given-names></name><name><surname>Ball</surname><given-names>S</given-names></name><name><surname>Barilari</surname><given-names>M</given-names></name><name><surname>Bault</surname><given-names>N</given-names></name><name><surname>Beaton</surname><given-names>D</given-names></name><name><surname>Beitner</surname><given-names>J</given-names></name><name><surname>Benoit</surname><given-names>RG</given-names></name><name><surname>Berkers</surname><given-names>RMWJ</given-names></name><name><surname>Bhanji</surname><given-names>JP</given-names></name><name><surname>Biswal</surname><given-names>BB</given-names></name><name><surname>Bobadilla-Suarez</surname><given-names>S</given-names></name><name><surname>Bortolini</surname><given-names>T</given-names></name><name><surname>Bottenhorn</surname><given-names>KL</given-names></name><name><surname>Bowring</surname><given-names>A</given-names></name><name><surname>Braem</surname><given-names>S</given-names></name><name><surname>Brooks</surname><given-names>HR</given-names></name><name><surname>Brudner</surname><given-names>EG</given-names></name><name><surname>Calderon</surname><given-names>CB</given-names></name><name><surname>Camilleri</surname><given-names>JA</given-names></name><name><surname>Castrellon</surname><given-names>JJ</given-names></name><name><surname>Cecchetti</surname><given-names>L</given-names></name><name><surname>Cieslik</surname><given-names>EC</given-names></name><name><surname>Cole</surname><given-names>ZJ</given-names></name><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Cunningham</surname><given-names>WA</given-names></name><name><surname>Czoschke</surname><given-names>S</given-names></name><name><surname>Dadi</surname><given-names>K</given-names></name><name><surname>Davis</surname><given-names>CP</given-names></name><name><surname>Luca</surname><given-names>AD</given-names></name><name><surname>Delgado</surname><given-names>MR</given-names></name><name><surname>Demetriou</surname><given-names>L</given-names></name><name><surname>Dennison</surname><given-names>JB</given-names></name><name><surname>Di</surname><given-names>X</given-names></name><name><surname>Dickie</surname><given-names>EW</given-names></name><name><surname>Dobryakova</surname><given-names>E</given-names></name><name><surname>Donnat</surname><given-names>CL</given-names></name><name><surname>Dukart</surname><given-names>J</given-names></name><name><surname>Duncan</surname><given-names>NW</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Eed</surname><given-names>A</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Erhart</surname><given-names>A</given-names></name><name><surname>Fontanesi</surname><given-names>L</given-names></name><name><surname>Fricke</surname><given-names>GM</given-names></name><name><surname>Fu</surname><given-names>S</given-names></name><name><surname>Galván</surname><given-names>A</given-names></name><name><surname>Gau</surname><given-names>R</given-names></name><name><surname>Genon</surname><given-names>S</given-names></name><name><surname>Glatard</surname><given-names>T</given-names></name><name><surname>Glerean</surname><given-names>E</given-names></name><name><surname>Goeman</surname><given-names>JJ</given-names></name><name><surname>Golowin</surname><given-names>SAE</given-names></name><name><surname>González-García</surname><given-names>C</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Grady</surname><given-names>CL</given-names></name><name><surname>Green</surname><given-names>MA</given-names></name><name><surname>Guassi Moreira</surname><given-names>JF</given-names></name><name><surname>Guest</surname><given-names>O</given-names></name><name><surname>Hakimi</surname><given-names>S</given-names></name><name><surname>Hamilton</surname><given-names>JP</given-names></name><name><surname>Hancock</surname><given-names>R</given-names></name><name><surname>Handjaras</surname><given-names>G</given-names></name><name><surname>Harry</surname><given-names>BB</given-names></name><name><surname>Hawco</surname><given-names>C</given-names></name><name><surname>Herholz</surname><given-names>P</given-names></name><name><surname>Herman</surname><given-names>G</given-names></name><name><surname>Heunis</surname><given-names>S</given-names></name><name><surname>Hoffstaedter</surname><given-names>F</given-names></name><name><surname>Hogeveen</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>C-P</given-names></name><name><surname>Huettel</surname><given-names>SA</given-names></name><name><surname>Hughes</surname><given-names>ME</given-names></name><name><surname>Iacovella</surname><given-names>V</given-names></name><name><surname>Iordan</surname><given-names>AD</given-names></name><name><surname>Isager</surname><given-names>PM</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Jahn</surname><given-names>A</given-names></name><name><surname>Johnson</surname><given-names>MR</given-names></name><name><surname>Johnstone</surname><given-names>T</given-names></name><name><surname>Joseph</surname><given-names>MJE</given-names></name><name><surname>Juliano</surname><given-names>AC</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Kassinopoulos</surname><given-names>M</given-names></name><name><surname>Koba</surname><given-names>C</given-names></name><name><surname>Kong</surname><given-names>X-Z</given-names></name><name><surname>Koscik</surname><given-names>TR</given-names></name><name><surname>Kucukboyaci</surname><given-names>NE</given-names></name><name><surname>Kuhl</surname><given-names>BA</given-names></name><name><surname>Kupek</surname><given-names>S</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Lamm</surname><given-names>C</given-names></name><name><surname>Langner</surname><given-names>R</given-names></name><name><surname>Lauharatanahirun</surname><given-names>N</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>S</given-names></name><name><surname>Leemans</surname><given-names>A</given-names></name><name><surname>Leo</surname><given-names>A</given-names></name><name><surname>Lesage</surname><given-names>E</given-names></name><name><surname>Li</surname><given-names>F</given-names></name><name><surname>Li</surname><given-names>MYC</given-names></name><name><surname>Lim</surname><given-names>PC</given-names></name><name><surname>Lintz</surname><given-names>EN</given-names></name><name><surname>Liphardt</surname><given-names>SW</given-names></name><name><surname>Losecaat Vermeer</surname><given-names>AB</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name><name><surname>Mack</surname><given-names>ML</given-names></name><name><surname>Malpica</surname><given-names>N</given-names></name><name><surname>Marins</surname><given-names>T</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>McDonald</surname><given-names>K</given-names></name><name><surname>McGuire</surname><given-names>JT</given-names></name><name><surname>Melero</surname><given-names>H</given-names></name><name><surname>Méndez Leal</surname><given-names>AS</given-names></name><name><surname>Meyer</surname><given-names>B</given-names></name><name><surname>Meyer</surname><given-names>KN</given-names></name><name><surname>Mihai</surname><given-names>G</given-names></name><name><surname>Mitsis</surname><given-names>GD</given-names></name><name><surname>Moll</surname><given-names>J</given-names></name><name><surname>Nielson</surname><given-names>DM</given-names></name><name><surname>Nilsonne</surname><given-names>G</given-names></name><name><surname>Notter</surname><given-names>MP</given-names></name><name><surname>Olivetti</surname><given-names>E</given-names></name><name><surname>Onicas</surname><given-names>AI</given-names></name><name><surname>Papale</surname><given-names>P</given-names></name><name><surname>Patil</surname><given-names>KR</given-names></name><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Pérez</surname><given-names>A</given-names></name><name><surname>Pischedda</surname><given-names>D</given-names></name><name><surname>Poline</surname><given-names>J-B</given-names></name><name><surname>Prystauka</surname><given-names>Y</given-names></name><name><surname>Ray</surname><given-names>S</given-names></name><name><surname>Reuter-Lorenz</surname><given-names>PA</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Ricciardi</surname><given-names>E</given-names></name><name><surname>Rieck</surname><given-names>JR</given-names></name><name><surname>Rodriguez-Thompson</surname><given-names>AM</given-names></name><name><surname>Romyn</surname><given-names>A</given-names></name><name><surname>Salo</surname><given-names>T</given-names></name><name><surname>Samanez-Larkin</surname><given-names>GR</given-names></name><name><surname>Sanz-Morales</surname><given-names>E</given-names></name><name><surname>Schlichting</surname><given-names>ML</given-names></name><name><surname>Schultz</surname><given-names>DH</given-names></name><name><surname>Shen</surname><given-names>Q</given-names></name><name><surname>Sheridan</surname><given-names>MA</given-names></name><name><surname>Silvers</surname><given-names>JA</given-names></name><name><surname>Skagerlund</surname><given-names>K</given-names></name><name><surname>Smith</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>DV</given-names></name><name><surname>Sokol-Hessner</surname><given-names>P</given-names></name><name><surname>Steinkamp</surname><given-names>SR</given-names></name><name><surname>Tashjian</surname><given-names>SM</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Thorp</surname><given-names>JN</given-names></name><name><surname>Tinghög</surname><given-names>G</given-names></name><name><surname>Tisdall</surname><given-names>L</given-names></name><name><surname>Tompson</surname><given-names>SH</given-names></name><name><surname>Toro-Serey</surname><given-names>C</given-names></name><name><surname>Torre Tresols</surname><given-names>JJ</given-names></name><name><surname>Tozzi</surname><given-names>L</given-names></name><name><surname>Truong</surname><given-names>V</given-names></name><name><surname>Turella</surname><given-names>L</given-names></name><name><surname>van ’t Veer</surname><given-names>AE</given-names></name><name><surname>Verguts</surname><given-names>T</given-names></name><name><surname>Vettel</surname><given-names>JM</given-names></name><name><surname>Vijayarajah</surname><given-names>S</given-names></name><name><surname>Vo</surname><given-names>K</given-names></name><name><surname>Wall</surname><given-names>MB</given-names></name><name><surname>Weeda</surname><given-names>WD</given-names></name><name><surname>Weis</surname><given-names>S</given-names></name><name><surname>White</surname><given-names>DJ</given-names></name><name><surname>Wisniewski</surname><given-names>D</given-names></name><name><surname>Xifra-Porxas</surname><given-names>A</given-names></name><name><surname>Yearling</surname><given-names>EA</given-names></name><name><surname>Yoon</surname><given-names>S</given-names></name><name><surname>Yuan</surname><given-names>R</given-names></name><name><surname>Yuen</surname><given-names>KSL</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Zosky</surname><given-names>JE</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Schonberg</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Variability in the analysis of a single neuroimaging dataset by many teams</article-title><source>Nature</source><volume>582</volume><fpage>84</fpage><lpage>88</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2314-9</pub-id><pub-id pub-id-type="pmid">32483374</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Naturalistic sentence comprehension in the brain</article-title><source>Language and Linguistics Compass</source><volume>10</volume><fpage>299</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1111/lnc3.12198</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>New</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Moving beyond kucera and francis: A critical evaluation of current word frequency norms and the introduction of A new and improved word frequency measure for american english</article-title><source>Behavior Research Methods</source><volume>41</volume><fpage>977</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.3758/BRM.41.4.977</pub-id><pub-id pub-id-type="pmid">19897807</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brysbaert</surname><given-names>M</given-names></name><name><surname>Warriner</surname><given-names>AB</given-names></name><name><surname>Kuperman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Concreteness ratings for 40 thousand generally known english word lemmas</article-title><source>Behavior Research Methods</source><volume>46</volume><fpage>904</fpage><lpage>911</lpage><pub-id pub-id-type="doi">10.3758/s13428-013-0403-5</pub-id><pub-id pub-id-type="pmid">24142837</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Button</surname><given-names>KS</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Mokrysz</surname><given-names>C</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Flint</surname><given-names>J</given-names></name><name><surname>Robinson</surname><given-names>ESJ</given-names></name><name><surname>Munafò</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Power failure: why small sample size undermines the reliability of neuroscience</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>365</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1038/nrn3475</pub-id><pub-id pub-id-type="pmid">23571845</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cao</surname><given-names>Q</given-names></name><name><surname>Shen</surname><given-names>L</given-names></name><name><surname>Xie</surname><given-names>W</given-names></name><name><surname>Parkhi</surname><given-names>OM</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vggface2: A Dataset for Recognising Faces across Pose and Age</article-title><source>arXiv</source><pub-id pub-id-type="doi">10.48550/arXiv.1710.08092</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carp</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>On the plurality of (methodological) worlds: estimating the analytic flexibility of fmri experiments</article-title><source>Frontiers in Neuroscience</source><volume>6</volume><elocation-id>149</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2012.00149</pub-id><pub-id pub-id-type="pmid">23087605</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Yong</surname><given-names>CH</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Shared memories reveal shared structure in neural activity across individuals</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>115</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nn.4450</pub-id><pub-id pub-id-type="pmid">27918531</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Wassermann</surname><given-names>D</given-names></name><name><surname>Abrams</surname><given-names>DA</given-names></name><name><surname>Kochalka</surname><given-names>J</given-names></name><name><surname>Gallardo-Diez</surname><given-names>G</given-names></name><name><surname>Menon</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The visual word form area (VWFA) is part of both language and attention circuitry</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5601</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13634-z</pub-id><pub-id pub-id-type="pmid">31811149</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Akbudak</surname><given-names>E</given-names></name><name><surname>Conturo</surname><given-names>TE</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Ollinger</surname><given-names>JM</given-names></name><name><surname>Drury</surname><given-names>HA</given-names></name><name><surname>Linenweber</surname><given-names>MR</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Shulman</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A common network of functional areas for attention and eye movements</article-title><source>Neuron</source><volume>21</volume><fpage>761</fpage><lpage>773</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80593-0</pub-id><pub-id pub-id-type="pmid">9808463</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Craddock</surname><given-names>C</given-names></name><name><surname>Sikka</surname><given-names>S</given-names></name><name><surname>Cheung</surname><given-names>B</given-names></name><name><surname>Khanuja</surname><given-names>R</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Yan</surname><given-names>C</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Lurie</surname><given-names>D</given-names></name><name><surname>Vogelstein</surname><given-names>J</given-names></name><name><surname>Burns</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Towards automated analysis of connectomes: the configurable pipeline for the analysis of connectomes (c-pac)</article-title><source>Frontiers in Neuroinformatics</source><volume>42</volume><elocation-id>3389</elocation-id><pub-id pub-id-type="doi">10.3389/conf.fninf.2013.09.00042</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cremers</surname><given-names>HR</given-names></name><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The relation between statistical power and inference in fmri</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0184923</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0184923</pub-id><pub-id pub-id-type="pmid">29155843</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DerSimonian</surname><given-names>R</given-names></name><name><surname>Laird</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Meta-analysis in clinical trials</article-title><source>Controlled Clinical Trials</source><volume>7</volume><fpage>177</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/0197-2456(86)90046-2</pub-id><pub-id pub-id-type="pmid">3802833</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietz</surname><given-names>NAE</given-names></name><name><surname>Jones</surname><given-names>KM</given-names></name><name><surname>Gareau</surname><given-names>L</given-names></name><name><surname>Zeffiro</surname><given-names>TA</given-names></name><name><surname>Eden</surname><given-names>GF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Phonological decoding involves left posterior fusiform gyrus</article-title><source>Human Brain Mapping</source><volume>26</volume><fpage>81</fpage><lpage>93</lpage><pub-id pub-id-type="doi">10.1002/hbm.20122</pub-id><pub-id pub-id-type="pmid">15934062</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nature abhors a paywall: how open science can realize the potential of naturalistic stimuli</article-title><source>NeuroImage</source><volume>216</volume><elocation-id>116330</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116330</pub-id><pub-id pub-id-type="pmid">31704292</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Birman</surname><given-names>D</given-names></name><name><surname>Schaer</surname><given-names>M</given-names></name><name><surname>Koyejo</surname><given-names>OO</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>MRIQC: advancing the automatic prediction of image quality in mri from unseen sites</article-title><source>PLOS ONE</source><volume>12</volume><elocation-id>e0184661</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0184661</pub-id><pub-id pub-id-type="pmid">28945803</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>FMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Ciric</surname><given-names>R</given-names></name><name><surname>Finc</surname><given-names>K</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Gomez</surname><given-names>DEP</given-names></name><name><surname>Ye</surname><given-names>Z</given-names></name><name><surname>Salo</surname><given-names>T</given-names></name><name><surname>Valabregue</surname><given-names>R</given-names></name><name><surname>Amlien</surname><given-names>IK</given-names></name><name><surname>Liem</surname><given-names>F</given-names></name><name><surname>Jacoby</surname><given-names>N</given-names></name><name><surname>Stojić</surname><given-names>H</given-names></name><name><surname>Cieslak</surname><given-names>M</given-names></name><name><surname>Urchs</surname><given-names>S</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>De La Vega</surname><given-names>A</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Thompson</surname><given-names>WH</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Analysis of task-based functional MRI data preprocessed with fmriprep</article-title><source>Nature Protocols</source><volume>15</volume><fpage>2186</fpage><lpage>2202</lpage><pub-id pub-id-type="doi">10.1038/s41596-020-0327-3</pub-id><pub-id pub-id-type="pmid">32514178</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>FMRIPrep: a robust preprocessing pipeline for functional MRI</data-title><version designator="2.0">2.0</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5898602">https://doi.org/10.5281/zenodo.5898602</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finn</surname><given-names>ES</given-names></name><name><surname>Corlett</surname><given-names>PR</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Constable</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Trait paranoia shapes inter-subject synchrony in brain activity during an ambiguous social narrative</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2043</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04387-2</pub-id><pub-id pub-id-type="pmid">29795116</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>VS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fox</surname><given-names>MD</given-names></name><name><surname>Corbetta</surname><given-names>M</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Raichle</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Spontaneous neuronal activity distinguishes human dorsal and ventral attention systems</article-title><source>PNAS</source><volume>103</volume><fpage>10046</fpage><lpage>10051</lpage><pub-id pub-id-type="doi">10.1073/pnas.0604187103</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Keator</surname><given-names>DB</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Thomas</surname><given-names>AG</given-names></name><name><surname>Kessler</surname><given-names>DA</given-names></name><name><surname>Kennedy</surname><given-names>DN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A very simple, re-executable neuroimaging publication</article-title><source>F1000Research</source><volume>6</volume><elocation-id>124</elocation-id><pub-id pub-id-type="doi">10.12688/f1000research.10783.2</pub-id><pub-id pub-id-type="pmid">28781753</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Rivera</surname><given-names>G</given-names></name><name><surname>Schwarz</surname><given-names>Y</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>Sochat</surname><given-names>VV</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Poline</surname><given-names>J-B</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>NeuroVault.org: a web-based repository for collecting and sharing unthresholded statistical maps of the human brain</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>8</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00008</pub-id><pub-id pub-id-type="pmid">25914639</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Auer</surname><given-names>T</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name><name><surname>Craddock</surname><given-names>RC</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Duff</surname><given-names>EP</given-names></name><name><surname>Flandin</surname><given-names>G</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Glatard</surname><given-names>T</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Handwerker</surname><given-names>DA</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Keator</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Michael</surname><given-names>Z</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>Nichols</surname><given-names>BN</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Pellman</surname><given-names>J</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Rokem</surname><given-names>A</given-names></name><name><surname>Schaefer</surname><given-names>G</given-names></name><name><surname>Sochat</surname><given-names>V</given-names></name><name><surname>Triplett</surname><given-names>W</given-names></name><name><surname>Turner</surname><given-names>JA</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments</article-title><source>Scientific Data</source><volume>3</volume><elocation-id>160044</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.44</pub-id><pub-id pub-id-type="pmid">27326542</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Grall</surname><given-names>C</given-names></name><name><surname>Finn</surname><given-names>ES</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Leveraging the Power of Media to Drive Cognition: A Media-Informed Approach to Naturalistic Neuroscience</article-title><source>PsyArXiv</source><pub-id pub-id-type="doi">10.31234/osf.io/c8z9t</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id><pub-id pub-id-type="pmid">19573611</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grill-Spector</surname><given-names>K</given-names></name><name><surname>Kushnir</surname><given-names>T</given-names></name><name><surname>Edelman</surname><given-names>S</given-names></name><name><surname>Avidan</surname><given-names>G</given-names></name><name><surname>Itzchak</surname><given-names>Y</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Differential processing of objects under various viewing conditions in the human lateral occipital complex</article-title><source>Neuron</source><volume>24</volume><fpage>187</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)80832-6</pub-id><pub-id pub-id-type="pmid">10677037</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halchenko</surname><given-names>Y</given-names></name><name><surname>Meyer</surname><given-names>K</given-names></name><name><surname>Poldrack</surname><given-names>B</given-names></name><name><surname>Solanky</surname><given-names>D</given-names></name><name><surname>Wagner</surname><given-names>A</given-names></name><name><surname>Gors</surname><given-names>J</given-names></name><name><surname>MacFarlane</surname><given-names>D</given-names></name><name><surname>Pustina</surname><given-names>D</given-names></name><name><surname>Sochat</surname><given-names>V</given-names></name><name><surname>Ghosh</surname><given-names>S</given-names></name><name><surname>Mönch</surname><given-names>C</given-names></name><name><surname>Markiewicz</surname><given-names>C</given-names></name><name><surname>Waite</surname><given-names>L</given-names></name><name><surname>Shlyakhter</surname><given-names>I</given-names></name><name><surname>de la Vega</surname><given-names>A</given-names></name><name><surname>Hayashi</surname><given-names>S</given-names></name><name><surname>Häusler</surname><given-names>C</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Kadelka</surname><given-names>T</given-names></name><name><surname>Skytén</surname><given-names>K</given-names></name><name><surname>Jarecka</surname><given-names>D</given-names></name><name><surname>Kennedy</surname><given-names>D</given-names></name><name><surname>Strauss</surname><given-names>T</given-names></name><name><surname>Cieslak</surname><given-names>M</given-names></name><name><surname>Vavra</surname><given-names>P</given-names></name><name><surname>Ioanas</surname><given-names>HI</given-names></name><name><surname>Schneider</surname><given-names>R</given-names></name><name><surname>Pflüger</surname><given-names>M</given-names></name><name><surname>Haxby</surname><given-names>J</given-names></name><name><surname>Eickhoff</surname><given-names>S</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>DataLad: distributed system for joint management of code, data, and their relationship</article-title><source>Journal of Open Source Software</source><volume>6</volume><elocation-id>3262</elocation-id><pub-id pub-id-type="doi">10.21105/joss.03262</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hamilton</surname><given-names>LS</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The revolution will not be controlled: natural stimuli in speech neuroscience</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>573</fpage><lpage>582</lpage><pub-id pub-id-type="doi">10.1080/23273798.2018.1499946</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Baumgartner</surname><given-names>FJ</given-names></name><name><surname>Ibe</surname><given-names>P</given-names></name><name><surname>Kaule</surname><given-names>FR</given-names></name><name><surname>Pollmann</surname><given-names>S</given-names></name><name><surname>Speck</surname><given-names>O</given-names></name><name><surname>Zinke</surname><given-names>W</given-names></name><name><surname>Stadler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A high-resolution 7-tesla fmri dataset from complex natural stimulation with an audio movie</article-title><source>Scientific Data</source><volume>1</volume><elocation-id>140003</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2014.3</pub-id><pub-id pub-id-type="pmid">25977761</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Häusler</surname><given-names>CO</given-names></name><name><surname>Eickhoff</surname><given-names>SB</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Processing of visual and non-visual naturalistic spatial information in the “parahippocampal place area.”</article-title><source>Scientific Data</source><volume>9</volume><elocation-id>147</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-022-01250-4</pub-id><pub-id pub-id-type="pmid">35365659</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Conroy</surname><given-names>BR</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Ramadge</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A common, high-dimensional model of the representational space in human ventral temporal cortex</article-title><source>Neuron</source><volume>72</volume><fpage>404</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.026</pub-id><pub-id pub-id-type="pmid">22017997</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(02)91132-8</pub-id><pub-id pub-id-type="pmid">12377157</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The fusiform face area: a module in human extrastriate cortex specialized for face perception</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4302</fpage><lpage>4311</lpage><pub-id pub-id-type="pmid">9151747</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kosmidis</surname><given-names>I</given-names></name><name><surname>Guolo</surname><given-names>A</given-names></name><name><surname>Varin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Improving the accuracy of likelihood-based inference in meta-analysis and meta-regression</article-title><source>Biometrika</source><volume>104</volume><fpage>489</fpage><lpage>496</lpage><pub-id pub-id-type="doi">10.1093/biomet/asx001</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kravitz</surname><given-names>DJ</given-names></name><name><surname>Saleem</surname><given-names>KS</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A new neural framework for visuospatial processing</article-title><source>Nature Reviews. Neuroscience</source><volume>12</volume><fpage>217</fpage><lpage>230</lpage><pub-id pub-id-type="doi">10.1038/nrn3008</pub-id><pub-id pub-id-type="pmid">21415848</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langers</surname><given-names>DRM</given-names></name><name><surname>van Dijk</surname><given-names>P</given-names></name><name><surname>Schoenmaker</surname><given-names>ES</given-names></name><name><surname>Backes</surname><given-names>WH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>FMRI activation in relation to sound intensity and loudness</article-title><source>NeuroImage</source><volume>35</volume><fpage>709</fpage><lpage>718</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.12.013</pub-id><pub-id pub-id-type="pmid">17254802</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Latour</surname><given-names>TD</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gallant</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Feature-Space Selection with Banded Ridge Regression</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.05.05.490831</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackenzie-Graham</surname><given-names>AJ</given-names></name><name><surname>Van Horn</surname><given-names>JD</given-names></name><name><surname>Woods</surname><given-names>RP</given-names></name><name><surname>Crawford</surname><given-names>KL</given-names></name><name><surname>Toga</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Provenance in neuroimaging</article-title><source>NeuroImage</source><volume>42</volume><fpage>178</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.04.186</pub-id><pub-id pub-id-type="pmid">18519166</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Bottenhorn</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Maumet</surname><given-names>C</given-names></name><name><surname>Nichols</surname><given-names>T</given-names></name><name><surname>Poldrack</surname><given-names>R</given-names></name><name><surname>Poline</surname><given-names>J</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2021">2021a</year><source>BIDS Statistical Models-An Implementation-Independent Representation of General Linear Models</source><publisher-loc>Oxford</publisher-loc><publisher-name>Oxford University</publisher-name></element-citation></ref><ref id="bib55"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>De La Vega</surname><given-names>A</given-names></name><name><surname>Wagner</surname><given-names>A</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name></person-group><year iso-8601-date="2021">2021b</year><data-title>Poldracklab/fitlins</data-title><version designator="v0.9.2">v0.9.2</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5120201">https://doi.org/10.5281/zenodo.5120201</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Feingold</surname><given-names>F</given-names></name><name><surname>Blair</surname><given-names>R</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Miller</surname><given-names>E</given-names></name><name><surname>Hardcastle</surname><given-names>N</given-names></name><name><surname>Wexler</surname><given-names>J</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Goncavles</surname><given-names>M</given-names></name><name><surname>Jwa</surname><given-names>A</given-names></name><name><surname>Poldrack</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021c</year><article-title>The openneuro resource for sharing of neuroscience data</article-title><source>eLife</source><volume>10</volume><elocation-id>e71774</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71774</pub-id><pub-id pub-id-type="pmid">34658334</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Markiewicz</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Fitlins</data-title><version designator="swh:1:rev:efd85cbeed2069a7f8de835f2d838e96d3f58ea9">swh:1:rev:efd85cbeed2069a7f8de835f2d838e96d3f58ea9</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://github.com/poldracklab/fitlins">https://github.com/poldracklab/fitlins</ext-link></element-citation></ref><ref id="bib58"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McFee</surname><given-names>B</given-names></name><name><surname>Raffel</surname><given-names>C</given-names></name><name><surname>Liang</surname><given-names>D</given-names></name><name><surname>Ellis</surname><given-names>DP</given-names></name><name><surname>McVicar</surname><given-names>M</given-names></name><name><surname>Battenberg</surname><given-names>E</given-names></name><name><surname>Nieto</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>librosa: Audio and Music Signal Analysis in Python</article-title><conf-name>Python in Science Conference</conf-name><conf-loc>Austin, Texas</conf-loc><fpage>18</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.25080/Majora-7b98e3ed-003</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McNamara</surname><given-names>Q</given-names></name><name><surname>De La Vega</surname><given-names>A</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Developing a Comprehensive Framework for Multimodal Feature Extraction</article-title><conf-name>KDD ’17</conf-name><conf-loc>Halifax NS Canada</conf-loc><fpage>1567</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1145/3097983.3098075</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural responses to naturalistic clips of behaving animals in two different task contexts</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>316</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00316</pub-id><pub-id pub-id-type="pmid">29867327</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Goldstein</surname><given-names>A</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Keep it real: rethinking the primacy of experimental control in cognitive neuroscience</article-title><source>NeuroImage</source><volume>222</volume><elocation-id>117254</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117254</pub-id><pub-id pub-id-type="pmid">32800992</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nastase</surname><given-names>SA</given-names></name><name><surname>Liu</surname><given-names>YF</given-names></name><name><surname>Hillman</surname><given-names>H</given-names></name><name><surname>Zadbood</surname><given-names>A</given-names></name><name><surname>Hasenfratz</surname><given-names>L</given-names></name><name><surname>Keshavarzian</surname><given-names>N</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Yeshurun</surname><given-names>Y</given-names></name><name><surname>Regev</surname><given-names>M</given-names></name><name><surname>Nguyen</surname><given-names>M</given-names></name><name><surname>Chang</surname><given-names>CHC</given-names></name><name><surname>Baldassano</surname><given-names>C</given-names></name><name><surname>Lositsky</surname><given-names>O</given-names></name><name><surname>Simony</surname><given-names>E</given-names></name><name><surname>Chow</surname><given-names>MA</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Brooks</surname><given-names>PP</given-names></name><name><surname>Micciche</surname><given-names>E</given-names></name><name><surname>Choe</surname><given-names>G</given-names></name><name><surname>Goldstein</surname><given-names>A</given-names></name><name><surname>Vanderwal</surname><given-names>T</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Narratives: FMRI data for evaluating models of naturalistic language comprehension</article-title><source>Neuroscience</source><volume>8</volume><elocation-id>250</elocation-id><pub-id pub-id-type="doi">10.1101/2020.12.23.424091</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>S</given-names></name><name><surname>Chun</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Different roles of the parahippocampal place area (PPA) and retrosplenial cortex (RSC) in panoramic scene perception</article-title><source>NeuroImage</source><volume>47</volume><fpage>1747</fpage><lpage>1756</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.04.058</pub-id><pub-id pub-id-type="pmid">19398014</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peters</surname><given-names>JC</given-names></name><name><surname>Jans</surname><given-names>B</given-names></name><name><surname>van de Ven</surname><given-names>V</given-names></name><name><surname>De Weerd</surname><given-names>P</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dynamic brightness induction in V1: analyzing simulated and empirically acquired fmri data in a “common brain space” framework</article-title><source>NeuroImage</source><volume>52</volume><fpage>973</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2010.03.070</pub-id><pub-id pub-id-type="pmid">20362062</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rocca</surname><given-names>R</given-names></name><name><surname>Coventry</surname><given-names>KR</given-names></name><name><surname>Tylén</surname><given-names>K</given-names></name><name><surname>Staib</surname><given-names>M</given-names></name><name><surname>Lund</surname><given-names>TE</given-names></name><name><surname>Wallentin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Language beyond the language system: dorsal visuospatial pathways support processing of demonstratives and spatial language during naturalistic fast fmri</article-title><source>NeuroImage</source><volume>216</volume><elocation-id>116128</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.116128</pub-id><pub-id pub-id-type="pmid">31473349</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rosenfelder</surname><given-names>I</given-names></name><name><surname>Fruehwald</surname><given-names>J</given-names></name><name><surname>Evanini</surname><given-names>K</given-names></name><name><surname>Seyfarth</surname><given-names>S</given-names></name><name><surname>Gorman</surname><given-names>K</given-names></name><name><surname>Prichard</surname><given-names>H</given-names></name><name><surname>Yuan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><data-title>FAVE</data-title><version designator="1.1.3">1.1.3</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.9846">https://doi.org/10.5281/zenodo.9846</ext-link></element-citation></ref><ref id="bib67"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Sandberg</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Facenet</data-title><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/davidsandberg/facenet">https://github.com/davidsandberg/facenet</ext-link></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schone</surname><given-names>HR</given-names></name><name><surname>Maimon-Mor</surname><given-names>RO</given-names></name><name><surname>Baker</surname><given-names>CI</given-names></name><name><surname>Makin</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Expert tool users show increased differentiation between visual representations of hands and tools</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>2980</fpage><lpage>2989</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2489-20.2020</pub-id><pub-id pub-id-type="pmid">33563728</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Schroff</surname><given-names>F</given-names></name><name><surname>Kalenichenko</surname><given-names>D</given-names></name><name><surname>Philbin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Facenet: a unified embedding for face recognition and clustering</article-title><conf-name>Proceedings of the IEEE conference on computer vision and pattern recognition</conf-name><fpage>815</fpage><lpage>823</lpage><pub-id pub-id-type="doi">10.1109/CVPR.1997.609286</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sekiyama</surname><given-names>K</given-names></name><name><surname>Kanno</surname><given-names>I</given-names></name><name><surname>Miura</surname><given-names>S</given-names></name><name><surname>Sugita</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Auditory-visual speech perception examined by fmri and PET</article-title><source>Neuroscience Research</source><volume>47</volume><fpage>277</fpage><lpage>287</lpage><pub-id pub-id-type="doi">10.1016/s0168-0102(03)00214-1</pub-id><pub-id pub-id-type="pmid">14568109</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sonkusare</surname><given-names>S</given-names></name><name><surname>Breakspear</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Naturalistic stimuli in neuroscience: critically acclaimed</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>699</fpage><lpage>714</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.05.004</pub-id><pub-id pub-id-type="pmid">31257145</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Szucs</surname><given-names>D</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature</article-title><source>PLOS Biology</source><volume>15</volume><elocation-id>e2000797</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2000797</pub-id><pub-id pub-id-type="pmid">28253258</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname><given-names>WH</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Bissett</surname><given-names>PG</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dataset decay and the problem of sequential analyses on open datasets</article-title><source>eLife</source><volume>9</volume><elocation-id>e53498</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.53498</pub-id><pub-id pub-id-type="pmid">32425159</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tucker</surname><given-names>BV</given-names></name><name><surname>Brenner</surname><given-names>D</given-names></name><name><surname>Danielson</surname><given-names>DK</given-names></name><name><surname>Kelley</surname><given-names>MC</given-names></name><name><surname>Nenadić</surname><given-names>F</given-names></name><name><surname>Sims</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The massive auditory lexical decision (MALD) database</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>1187</fpage><lpage>1204</lpage><pub-id pub-id-type="doi">10.3758/s13428-018-1056-1</pub-id><pub-id pub-id-type="pmid">29916041</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname><given-names>BO</given-names></name><name><surname>Paul</surname><given-names>EJ</given-names></name><name><surname>Miller</surname><given-names>MB</given-names></name><name><surname>Barbey</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Small sample sizes reduce the replicability of task-based fmri studies</article-title><source>Communications Biology</source><volume>1</volume><elocation-id>62</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-018-0073-z</pub-id><pub-id pub-id-type="pmid">30271944</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: improved N3 bias correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><pub-id pub-id-type="pmid">20378467</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Valyear</surname><given-names>KF</given-names></name><name><surname>Cavina-Pratesi</surname><given-names>C</given-names></name><name><surname>Stiglick</surname><given-names>AJ</given-names></name><name><surname>Culham</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Does tool-related fmri activity within the intraparietal sulcus reflect the plan to grasp?</article-title><source>NeuroImage</source><volume>36 Suppl 2</volume><fpage>T94</fpage><lpage>T108</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.03.031</pub-id><pub-id pub-id-type="pmid">17499175</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Visconti</surname><given-names>M</given-names></name><name><surname>Chauhan</surname><given-names>V</given-names></name><name><surname>Jiahui</surname><given-names>G</given-names></name><name><surname>Gobbini</surname><given-names>MI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An fmri dataset in response to “the grand budapest hotel”, a socially-rich, naturalistic movie</article-title><source>Scientific Data</source><volume>7</volume><elocation-id>383</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-020-00735-4</pub-id><pub-id pub-id-type="pmid">33177526</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westfall</surname><given-names>J</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fixing the stimulus-as-fixed-effect fallacy in task fmri</article-title><source>Wellcome Open Research</source><volume>1</volume><elocation-id>23</elocation-id><pub-id pub-id-type="doi">10.12688/wellcomeopenres.10298.2</pub-id><pub-id pub-id-type="pmid">28503664</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilkinson</surname><given-names>MD</given-names></name><name><surname>Dumontier</surname><given-names>M</given-names></name><name><surname>Aalbersberg</surname><given-names>IJJ</given-names></name><name><surname>Appleton</surname><given-names>G</given-names></name><name><surname>Axton</surname><given-names>M</given-names></name><name><surname>Baak</surname><given-names>A</given-names></name><name><surname>Blomberg</surname><given-names>N</given-names></name><name><surname>Boiten</surname><given-names>JW</given-names></name><name><surname>da Silva Santos</surname><given-names>LB</given-names></name><name><surname>Bourne</surname><given-names>PE</given-names></name><name><surname>Bouwman</surname><given-names>J</given-names></name><name><surname>Brookes</surname><given-names>AJ</given-names></name><name><surname>Clark</surname><given-names>T</given-names></name><name><surname>Crosas</surname><given-names>M</given-names></name><name><surname>Dillo</surname><given-names>I</given-names></name><name><surname>Dumon</surname><given-names>O</given-names></name><name><surname>Edmunds</surname><given-names>S</given-names></name><name><surname>Evelo</surname><given-names>CT</given-names></name><name><surname>Finkers</surname><given-names>R</given-names></name><name><surname>Gonzalez-Beltran</surname><given-names>A</given-names></name><name><surname>Gray</surname><given-names>AJG</given-names></name><name><surname>Groth</surname><given-names>P</given-names></name><name><surname>Goble</surname><given-names>C</given-names></name><name><surname>Grethe</surname><given-names>JS</given-names></name><name><surname>Heringa</surname><given-names>J</given-names></name><name><surname>’t Hoen</surname><given-names>PAC</given-names></name><name><surname>Hooft</surname><given-names>R</given-names></name><name><surname>Kuhn</surname><given-names>T</given-names></name><name><surname>Kok</surname><given-names>R</given-names></name><name><surname>Kok</surname><given-names>J</given-names></name><name><surname>Lusher</surname><given-names>SJ</given-names></name><name><surname>Martone</surname><given-names>ME</given-names></name><name><surname>Mons</surname><given-names>A</given-names></name><name><surname>Packer</surname><given-names>AL</given-names></name><name><surname>Persson</surname><given-names>B</given-names></name><name><surname>Rocca-Serra</surname><given-names>P</given-names></name><name><surname>Roos</surname><given-names>M</given-names></name><name><surname>van Schaik</surname><given-names>R</given-names></name><name><surname>Sansone</surname><given-names>SA</given-names></name><name><surname>Schultes</surname><given-names>E</given-names></name><name><surname>Sengstag</surname><given-names>T</given-names></name><name><surname>Slater</surname><given-names>T</given-names></name><name><surname>Strawn</surname><given-names>G</given-names></name><name><surname>Swertz</surname><given-names>MA</given-names></name><name><surname>Thompson</surname><given-names>M</given-names></name><name><surname>van der Lei</surname><given-names>J</given-names></name><name><surname>van Mulligen</surname><given-names>E</given-names></name><name><surname>Velterop</surname><given-names>J</given-names></name><name><surname>Waagmeester</surname><given-names>A</given-names></name><name><surname>Wittenburg</surname><given-names>P</given-names></name><name><surname>Wolstencroft</surname><given-names>K</given-names></name><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Mons</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The FAIR guiding principles for scientific data management and stewardship</article-title><source>Scientific Data</source><volume>3</volume><elocation-id>160018</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2016.18</pub-id><pub-id pub-id-type="pmid">26978244</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wolf</surname><given-names>T</given-names></name><name><surname>Debut</surname><given-names>L</given-names></name><name><surname>Sanh</surname><given-names>V</given-names></name><name><surname>Chaumond</surname><given-names>J</given-names></name><name><surname>Delangue</surname><given-names>C</given-names></name><name><surname>Moi</surname><given-names>A</given-names></name><name><surname>Cistac</surname><given-names>P</given-names></name><name><surname>Rault</surname><given-names>T</given-names></name><name><surname>Louf</surname><given-names>R</given-names></name><name><surname>Funtowicz</surname><given-names>M</given-names></name><name><surname>Davison</surname><given-names>J</given-names></name><name><surname>Shleifer</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>C</given-names></name><name><surname>Jernite</surname><given-names>Y</given-names></name><name><surname>Plu</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>C</given-names></name><name><surname>Scao</surname><given-names>TL</given-names></name><name><surname>Gugger</surname><given-names>S</given-names></name><name><surname>Drame</surname><given-names>M</given-names></name><name><surname>Lhoest</surname><given-names>Q</given-names></name><name><surname>Rush</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Transformers: state-of-the-art natural language processing</article-title><conf-name>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</conf-name><fpage>38</fpage><lpage>45</lpage></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Speer</surname><given-names>NK</given-names></name><name><surname>Zacks</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neural substrates of narrative comprehension and memory</article-title><source>NeuroImage</source><volume>41</volume><fpage>1408</fpage><lpage>1425</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2008.03.062</pub-id><pub-id pub-id-type="pmid">18499478</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Alejandro</surname><given-names>G</given-names></name><name><surname>Krzysztof</surname><given-names>J</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Salo</surname><given-names>T</given-names></name><name><surname>Blair</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019a</year><data-title>Bids-standard/pybids</data-title><version designator="0.9.3">0.9.3</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="http://doi.org/10.5281/zenodo.3363985">http://doi.org/10.5281/zenodo.3363985</ext-link></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>de la Vega</surname><given-names>A</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Salo</surname><given-names>T</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>McNamara</surname><given-names>Q</given-names></name><name><surname>DeStasio</surname><given-names>K</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name><name><surname>Petrov</surname><given-names>D</given-names></name><name><surname>Hayot-Sasson</surname><given-names>V</given-names></name><name><surname>Nielson</surname><given-names>DM</given-names></name><name><surname>Carlin</surname><given-names>J</given-names></name><name><surname>Kiar</surname><given-names>G</given-names></name><name><surname>Whitaker</surname><given-names>K</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Wagner</surname><given-names>A</given-names></name><name><surname>Tirrell</surname><given-names>LS</given-names></name><name><surname>Jas</surname><given-names>M</given-names></name><name><surname>Hanke</surname><given-names>M</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Appelhoff</surname><given-names>S</given-names></name><name><surname>Holdgraf</surname><given-names>C</given-names></name><name><surname>Staden</surname><given-names>I</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Kleinschmidt</surname><given-names>DF</given-names></name><name><surname>Lee</surname><given-names>JA</given-names></name><name><surname>Visconti di Oleggio Castello</surname><given-names>M</given-names></name><name><surname>Notter</surname><given-names>MP</given-names></name><name><surname>Blair</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>PyBIDS: python tools for BIDS datasets</article-title><source>Journal of Open Source Software</source><volume>4</volume><elocation-id>1294</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01294</pub-id><pub-id pub-id-type="pmid">32775955</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The generalizability crisis</article-title><source>The Behavioral and Brain Sciences</source><volume>45</volume><elocation-id>e1</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X20001685</pub-id><pub-id pub-id-type="pmid">33342451</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Young</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>The HTK Hidden Markov Model Toolkit: Design and Philosophy</source><publisher-name>Entropic Cambridge Research Laboratory, Ltd</publisher-name><fpage>2</fpage><lpage>44</lpage></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>J</given-names></name><name><surname>Liberman</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Speaker identification on the SCOTUS corpus</article-title><source>The Journal of the Acoustical Society of America</source><volume>123</volume><elocation-id>3878</elocation-id><pub-id pub-id-type="doi">10.1121/1.2935783</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zadbood</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How we transmit memories to other brains: constructing shared neural representations via communication</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>4988</fpage><lpage>5000</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhx202</pub-id><pub-id pub-id-type="pmid">28922834</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of brain MR images through a hidden markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Qiao</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Joint face detection and alignment using multitask cascaded convolutional networks</article-title><source>IEEE Signal Processing Letters</source><volume>23</volume><fpage>1499</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1109/LSP.2016.2603342</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79277.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hunt</surname><given-names>Laurence Tudor</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.04.05.487222" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.05.487222"/></front-stub><body><p>This is an important, methodologically compelling paper. It describes a powerful new online software platform for analysing data from naturalistic fMRI studies. The paper describes both the philosophy behind and intended usage of the software, and offers several examples of the types of results that can be computed using publicly available datasets. It will provide an important new tool for the open neuroscience community who are seeking to perform standardised and reproducible analyses of naturalistic fMRI datasets.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79277.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Hunt</surname><given-names>Laurence Tudor</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Finn</surname><given-names>Emily</given-names></name><role>Reviewer</role></contrib><contrib contrib-type="reviewer"><name><surname>Baldassano</surname><given-names>Christopher</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Duff</surname><given-names>Eugene P</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/041kmwe10</institution-id><institution>Imperial College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.05.487222">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.04.05.487222v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Neuroscout, a unified platform for generalizable and reproducible fMRI research&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Laurence Hunt as the Reviewing Editor and Tamar Makin as the Senior Editor. The following individuals involved in the review of your submission have agreed to reveal their identity: Emily Finn (Reviewer #1); Christopher Baldassano (Reviewer #2); Eugene P Duff (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>All reviewers were in agreement that Neuroscout will provide a valuable tool to the neuroimaging research community, that the structure of the platform is a novel one (that offers unique functionality not currently available on other Open Neuroimaging platforms), and that the manuscript is well-written. Following discussion with the reviewers we agreed that, while the authors should provide a point-by-point rebuttal to the reviewers' comments below, the three areas that should receive the highest priority in revising the manuscript are:</p><p>1) Provide slightly more in the way of assistance/help for users who are completely new to the platform to ensure that they can 'get started' without encountering errors – e.g. two of the reviewers (#2 and #3) attempted to use the platform but encountered errors when getting started, which could easily put off new users from adopting Neuroscout.</p><p>2) Discuss more extensively whether there are plans for further expansion/what the ultimate 'scope' of the project is beyond GLM specification. In particular, provide a specific discussion of how exactly the authors/developers see the software evolving over the short-, medium-, and long-term, and any plans they have in place to ensure continued development and promote a robust user community. Of course, it's hard to predict uptake with any new software platform, and this manuscript is an important step in publicizing the platform, but where do we go from here? Will there be a dedicated developer or team of developers moving forward? If so, how do they plan to prioritize and execute new features -- i.e., top-down decisions about important extensions (more detail on what these are?) versus bottom-up responses to user requests? If not, how will the authors/original developers ensure the continued health of the software? What kind of user support/promotion will exist -- passive usage monitoring? regular user surveys? active mailing/discussion lists? hackathons? etc. etc.</p><p>3) Address some of the specific technical questions about how Neuroscout handles certain aspects of the GLM, as highlighted in particular by reviewer #1.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The Neuroscout platform and the manuscript are very impressive, and I have no comments on how the presentation of the material could be improved.</p><p>I attempted to perform an analysis in Neuroscout as part of my review and was not successful. I was able to set up a simple analysis which successfully validated and compiled. To run the analysis, I first tried using Docker, as recommended. When doing so, I was prompted to enter my username and password for github.com. This was confusing to me since a github login was not discussed as part of this system. Entering my github credentials produced an error, since &quot;Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.&quot;</p><p>I also tried to run the analysis via singularity. The command recommended in the documentation was &quot;singularity pull oras://ghcr.io/neuroscout/neuroscout-cli:&quot; which yielded the error &quot;FATAL: While pulling image from oci registry: failed to get checksum for //ghcr.io/neuroscout/neuroscout-cli:master: no layer found corresponding to SIF image&quot;. I instead built a singularity image from docker using &quot;singularity build neuroscout.simg docker://neuroscout/neuroscout-cli&quot; which ran successfully. To run the singularity image, the documentation incorrectly left out the second &quot;run&quot; argument to the command, but after adding that in and running &quot;singularity run <monospace>--cleanenv</monospace> neuroscout.simg run [id] testns/&quot; the analysis started. Unfortunately, I then hit the same github issue described above and did not know how to troubleshoot the issue.</p><p>The fact that analyses must be run locally is understandable since it greatly decreases the cost and complexity of running the Neuroscout website. However, it could increase adoption of the platform if a cloud-based execution option were available in some form – I'm unsure if there are options for easily spinning up free (or low-cost) cloud instances that could allow users without local computing resources to run analyses.</p><p>There is a sentence in the manuscript that I did not understand: &quot;Interestingly, these effects were robust to phonological and orthographic covariates, suggesting that the involvement of VWFA in language comprehension may not be specific to reading.&quot; This sentence is referring to a study that is described as a &quot;reading experiment&quot;, so I do not understand how the results could show that VWFA is involved in tasks besides reading.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The manuscript is prepared to a high standard. I have a number of recommendations:</p><p>– It would be nice to see more of a review of similar attempts at web analysis platforms and related software – possibly including those outside of imaging.</p><p>– Furthermore, general readers may appreciate a little more of a survey of the open fMRI analysis ecosystem that this is based upon, in the intro/results. Possibly further more detailed schematic figures could help.</p><p>– The description of the platform as &quot;end-to-end&quot; might be qualified – for me it produced unrealistic expectations for the breadth of the analyses that could be specified.</p><p>– More discussion on how this tool will be maintained and developed, including mechanisms for community contributions to its various elements.</p><p>– The example analyses were well chosen, but I sometimes lost track of what they were demonstrating.</p><p>– My attempt to access the analyses on binder seemed to fail.</p><p>– There was also a possible typo in the git command when I tried to pull a dataset (&quot;fatal: repository 'https://github.com/neuroscout-datasets/SemanticNarrative-/' not found'&quot;).</p><p>– I think the manuscript would be strengthened with a broader discussion of future possibilities and goals for the tool. For example, more detail on the challenges of moving beyond naturalistic stimuli, and extension to modalities other than fMRI.</p><p>– Some discussion of the inferential challenges related to the re-analysis of datasets could be warranted.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79277.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>All reviewers were in agreement that Neuroscout will provide a valuable tool to the neuroimaging research community, that the structure of the platform is a novel one (that offers unique functionality not currently available on other Open Neuroimaging platforms), and that the manuscript is well-written. Following discussion with the reviewers we agreed that, while the authors should provide a point-by-point rebuttal to the reviewers' comments below, the three areas that should receive the highest priority in revising the manuscript are:</p><p>1) Provide slightly more in the way of assistance/help for users who are completely new to the platform to ensure that they can 'get started' without encountering errors – e.g. two of the reviewers (#2 and #3) attempted to use the platform but encountered errors when getting started, which could easily put off new users from adopting Neuroscout.</p></disp-quote><p>We agree that the first-time user experience is critical to the success of any scientific software project. As such, we have launched a comprehensive documentation website for the Neuroscout project (https://neuroscout.org/docs) which integrates our three major ecosystem tools (neuroscout.org; Neuroscout-CLI, and the PyNS Python API). In addition, for Neuroscout-CLI and PyNS, we have launched versioned documentation to ensure up-to-date usage references are available in a user-friendly format</p><p>(<ext-link ext-link-type="uri" xlink:href="https://pyns.readthedocs.io/en/latest/">https://pyns.readthedocs.io/en/latest/</ext-link>; https://neuroscout-cli.readthedocs.io/en/latest/).</p><p>In addition, to help guide new users, we have launched a fully cloud-based workflow using Google Colab— a free Jupyter notebook environment. Users can use this free resource to run analyses designed on neuroscout.org without using any of their local compute resources, or having to install any software. This option is clearly documented in the new Neuroscout documentation, and linked in the “Run” tab of the Neuroscout model builder.</p><p>We mention this new documentation in the introduction to our Results:</p><p>“Complete and up-to-date documentation of all of the platform’s components, including</p><p>Getting Started guides to facilitate first time users, is available in the official Neuroscout</p><p>Documentation (https://neuroscout.org/docs).“ pg. 3</p><p>As well as in the Discussion section in a new section on User support and feedback:</p><p>“A comprehensive overview of the platform and guides for getting started can be found in the integrated Neuroscout documentation (https://neuroscout.org/docs), as well as in each tool’s version-specific automatically generated documentation (hosted by ReadTheDocs, a community-supported documentation platform). We plan to grow the collection of complete tutorials replicating exemplary analyses and host them in the centralized Neuroscout documentation. “ (pg. 12)</p><p>Finally, we’re happy to report that by communicating with the reviewers, we were able to identify a bug which has since been corrected and should no longer be an obstacle to new users.</p><disp-quote content-type="editor-comment"><p>2) Discuss more extensively whether there are plans for further expansion/what the ultimate 'scope' of the project is beyond GLM specification. In particular, provide a specific discussion of how exactly the authors/developers see the software evolving over the short-, medium-, and long-term, and any plans they have in place to ensure continued development and promote a robust user community. Of course, it's hard to predict uptake with any new software platform, and this manuscript is an important step in publicizing the platform, but where do we go from here? Will there be a dedicated developer or team of developers moving forward? If so, how do they plan to prioritize and execute new features -- i.e., top-down decisions about important extensions (more detail on what these are?) versus bottom-up responses to user requests? If not, how will the authors/original developers ensure the continued health of the software? What kind of user support/promotion will exist -- passive usage monitoring? regular user surveys? active mailing/discussion lists? hackathons? etc. etc.</p></disp-quote><p>The reviewers are correct to point out that planning for short and long term extensibility and support of the project is critical to long term success. To address these concerns, we have explicitly addressed this in the Discussion section in three sections.</p><p>First, we address long term sustainability in a new Discussion section:</p><p>“An on-going challenge for scientific software tools—especially those that rely on centralized services—is long-term maintenance, development and user support. On-going upkeep of core tools and development of new features require a non-trivial amount of developer time. This problem is exacerbated for projects primarily supported by government funding, which generally prefers novel research to the on-going maintenance of existing tools. This is particularly challenging for centralized services, such as the Neuroscout server and web application, which require greater maintenance and coordination for upkeep.</p><p>With this in mind, we have designed many of the core components of Neuroscout with modularity as a guiding principle in order to maximize the longevity and impact of the platform. Although components of the platform are tightly integrated, they are also designed to be independently useful, increasing their general utility, and encouraging broader adoption by the community. For example, our feature extraction library (pliers) is designed for general purpose use on multimodal stimuli, and can be easily expanded to adopt novel extractors. On the analysis execution side, rather than implementing a bespoke analysis workflow, we worked to develop a general specification for statistical models under the BIDS standard (BIDS StatsModels; https://bids-standard.github.io/stats-models/) and a compatible execution workflow (FitLins; https://github.com/poldracklab/fitlins). By distributing the technical debt of Neuroscout across various independently used and supported projects, we hope to maximize the robustness and impact of the platform. To ensure the community’s needs are met, users are encouraged to vote on the prioritization of features by voting on issues on Neuroscout’s GitHub repository, and code from new contributors is actively encouraged.” (pg. 12)</p><p>Next, in the aforementioned User support and feedback section we more explicitly discuss resources available to users, and community involvement:</p><p>“Users can ask questions to developers and the community using the topic ‘neuroscout’ on <ext-link ext-link-type="uri" xlink:href="https://neurostars.org/">Neurostars.org</ext-link>— a public forum for neuroscience researchers and neuroinformatics infrastructure maintainers managed by the INCF. In addition, users can provide direct feedback through a form found on all pages in the Neuroscout website, which directly alerts developers to user concerns. A mailing list is also available to stay up to date with the latest feature developments in the platform. Finally, the Neuroscout developer team frequently participates at major neuroinformatics hackathons (such as Brainhack events and at major neuroimaging conferences) and plans on hosting ongoing Neuroscout specific hackathons.” (pg. 12)</p><p>Finally, in the revamped Neuroscout Documentation website, we have added a section “Get involved”, which explicitly documents how to ask questions, report bugs, request features or contribute to the project (https://neuroscout.org/docs/overview/get_involved.html). We plan on keeping this section actively updated, to reflect events where the team will be present, or hackathons hosted specifically for Neuroscout.</p><disp-quote content-type="editor-comment"><p>3) Address some of the specific technical questions about how Neuroscout handles certain aspects of the GLM, as highlighted in particular by reviewer #1.</p></disp-quote><p>See our responses to Reviewer 1 for a detailed response to these points. We have expanded our discussion to better guide users in these difficult methodological issues.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The Neuroscout platform and the manuscript are very impressive, and I have no major comments on how the presentation of the material could be improved.</p><p>I attempted to perform an analysis in Neuroscout as part of my review and was not successful. I was able to set up a simple analysis which successfully validated and compiled. To run the analysis, I first tried using Docker, as recommended. When doing so, I was prompted to enter my username and password for github.com. This was confusing to me since a github login was not discussed as part of this system. Entering my github credentials produced an error, since &quot;Support for password authentication was removed on August 13, 2021. Please use a personal access token instead.&quot;</p><p>I also tried to run the analysis via singularity. The command recommended in the documentation was &quot;singularity pull oras://ghcr.io/neuroscout/neuroscout-cli:&quot; which yielded the error &quot;FATAL: While pulling image from oci registry: failed to get checksum for //ghcr.io/neuroscout/neuroscout-cli:master: no layer found corresponding to SIF image&quot;. I instead built a singularity image from docker using &quot;singularity build neuroscout.simg docker://neuroscout/neuroscout-cli&quot; which ran successfully. To run the singularity image, the documentation incorrectly left out the second &quot;run&quot; argument to the command, but after adding that in and running &quot;singularity run <monospace>--cleanenv</monospace> neuroscout.simg run [id] testns/&quot; the analysis started. Unfortunately, I then hit the same github issue described above and did not know how to troubleshoot the issue.</p><p>The fact that analyses must be run locally is understandable since it greatly decreases the cost and complexity of running the Neuroscout website. However, it could increase adoption of the platform if a cloud-based execution option were available in some form – I'm unsure if there are options for easily spinning up free (or low-cost) cloud instances that could allow users without local computing resources to run analyses.</p></disp-quote><p>We again thank the reviewer for their detailed report that allowed us to address these issues. We are also happy to report that we have developed a notebook workflow for Neuroscout which can be run for free in the Google Collab cloud. This is documented in our revamped documentation, and linked in the Run page of the Neuroscout application now: https://neuroscout.github.io/neuroscout-docs//cli/Neuroscout_CLI_Colab_Demo.html. We are also actively pursuing integration with Brainlife.io, a cloud-based neuroscience analysis platform.</p><p>We mention the future Brainlife integration in the Future Directions section:</p><p>“Other important expansions include facilitating analysis execution by directly integrating with cloud-based neuroscience analysis platforms, such as Brainlife.io (Avesani et al., 2019)…” (pp. 11)</p><disp-quote content-type="editor-comment"><p>There is a sentence in the manuscript that I did not understand: &quot;Interestingly, these effects were robust to phonological and orthographic covariates, suggesting that the involvement of VWFA in language comprehension may not be specific to reading.&quot; This sentence is referring to a study that is described as a &quot;reading experiment&quot;, so I do not understand how the results could show that VWFA is involved in tasks besides reading.</p></disp-quote><p>We thank the reviewer for this comment, and have rephrased as follows for more clarity:</p><p>“Interestingly, these effects were robust to phonological and orthographic covariates, suggesting that VWFA activity may not only be involved in orthographic and phonological reading subprocesses, but also modulated by modality-independent lexical-semantic properties of linguistic input. Yet, as the experiment only involved visual presentation of linguistic stimuli, this hypothesis could not be corroborated empirically.” (pp. 9)</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>The manuscript is prepared to a high standard. I have a number of recommendations:</p><p>– It would be nice to see more of a review of similar attempts at web analysis platforms and related software – possibly including those outside of imaging.</p></disp-quote><p>To our knowledge, there are scant efforts in neuroimaging to provide web-based analysis platforms. The vast majority of work has been to develop data sharing portals (such as OpenNeuro), and reproducible tools that pipelines that can be put together by users on their own computational resources. One notable exception is Brainlife.io, which we have added to the introduction. Although we agree that it might be helpful to review other similar efforts outside of neuroimaging, we feel that we are not sufficiently familiar with them to confidently introduce them here. Instead, we have chosen to review the open fMRI analysis ecosystem as suggested by the reviewer in their second suggestion.</p><disp-quote content-type="editor-comment"><p>– Furthermore, general readers may appreciate a little more of a survey of the open fMRI analysis ecosystem that this is based upon, in the intro/results. Possibly further more detailed schematic figures could help.</p></disp-quote><p>We agree that we did not introduce the ecosystem in sufficient detail. We added a paragraph to the introduction to better review the existing ecosystem, and position Neuroscout within it. We appreciate this suggestion, as it makes it more clear to readers exactly which part of the analysis lifecycle Neuroscout contributes to.</p><p>“The recent proliferation of community-led tools and standards—most notably the Brain Imaging Data Structure (Gorgolewski et al., 2016) standard—has galvanized efforts to foster reproducible practices across the data analysis lifecycle. A growing number of data archives, such as OpenNeuro (Markiewicz, Gorgolewski, et al., 2021), now host hundreds of publicly available neuroimaging datasets, including dozens of naturalistic fMRI datasets. The development of standardized quality control and preprocessing pipelines, such as MRIQC (Esteban et al., 2017), fmriprep (Esteban et al., 2019, 2022), and C-PAC (Craddock et al., 2013), facilitate their analysis and can be launched on emerging cloud-based platforms, such as Brainlife.io (Avesani et al., 2019). However, fMRI model specification and estimation remains challenging to standardize, and typically results in bespoke modeling pipelines that are not often shared, and can be difficult to re-use. Unfortunately, despite the availability of a rich ecosystem of tools, assembling them into a complete and reproducible workflow remains out of reach for many scientists due to substantial technical challenges.” (pp. 2)</p><disp-quote content-type="editor-comment"><p>– The description of the platform as &quot;end-to-end&quot; might be qualified – for me it produced unrealistic expectations for the breadth of the analyses that could be specified.</p></disp-quote><p>We agree with the reviewer that “end-to-end” implies that the entire analysis can be run and finished on the cloud. In order to avoid this implication, we have replaced “end-to-end” with the term “unified”, which is also consistent with our description of the platform in the title.</p><disp-quote content-type="editor-comment"><p>– More discussion on how this tool will be maintained and developed, including mechanisms for community contributions to its various elements.</p></disp-quote><p>Thank you for this important comment. We have added an extensive section in the</p><p>Discussion to discuss the long-term maintainability of the platform, and future directions (quote above in the response to editor). In addition, in our revamped documentation, we dedicate a page to “Getting involved” in order to orient new users and potential contributors to our development practices: https://neuroscout.org/docs/overview/get_involved.html</p><disp-quote content-type="editor-comment"><p>– The example analyses were well chosen, but I sometimes lost track of what they were demonstrating.</p></disp-quote><p>We appreciate this feedback, however we had a difficult time identifying a useful change to make. If the reviewer has any additional feedback here to help us refine our examples, we would be happy to make changes.</p><disp-quote content-type="editor-comment"><p>– My attempt to access the analyses on binder seemed to fail.</p></disp-quote><p>Unfortunately, the Binder service can fail on occasion with no repeatable cause. We have checked the JupyterBook to ensure there are no major outstanding issues that would cause this behavior. Typically, re-launching Binder will solve the problem.</p><disp-quote content-type="editor-comment"><p>– There was also a possible typo in the git command when I tried to pull a dataset (&quot;fatal: repository 'https://github.com/neuroscout-datasets/SemanticNarrative-/' not found'&quot;).</p></disp-quote><p>We thank the reviewer for this issue report. This issue was also noticed by Reviewer 2, and we have since addressed it. Thankfully it only affected a subset of datasets, but we very much appreciate the in-depth testing of the platform by the reviewer.</p><disp-quote content-type="editor-comment"><p>– I think the manuscript would be strengthened with a broader discussion of future possibilities and goals for the tool. For example, more detail on the challenges of moving beyond naturalistic stimuli, and extension to modalities other than fMRI.</p></disp-quote><p>We have substantially expanded the discussion with a section on “Long term sustainability”, as well as a clearly defined “Challenges and future directions” section which details the upcoming priorities for Neuroscout. In particular, we chose to expand on three issues: handling collinearity and implementing more sophisticated statistical modeling workflows that are a better fit for naturalistic data.</p><p>With respect to moving beyond naturalistic stimuli, the primary challenge is related to the availability of pre-processed derivatives at scale on data sharing portals such as OpenNeuro. Neuroscout already has the ability to ingest non-naturalistic datasets, we simply chose to focus on naturalistic datasets as these datasets are inherently more flexible and potentially generalizable.</p><p>We tried to clarify that Neuroscout is compatible with this next step by revising this sentence:</p><p>“Although we have primarily focused on naturalistic datasets—as they intrinsically feature a high degree of reusability and ecological validity—Neuroscout workflows are applicable to any BIDS-compliant dataset due to the flexibility of the BIDS Stats Model specification. Indexing non-naturalistic fMRI datasets will be an important next step, an effort that will require the widespread sharing of harmonized pre-processed derivatives that can be automatically ingested.” (pp. 11)</p><disp-quote content-type="editor-comment"><p>– Some discussion of the inferential challenges related to the re-analysis of datasets could be warranted.</p></disp-quote><p>The reviewer brings up relevant concerns regarding long-term re-use of public datasets, and what is being called “dataset decay”. An important principle guiding Neuroscout is to minimize undocumented researcher degrees of freedom. Unlike traditional re-analysis of a public dataset, in which only the final analysis would be shared with the public, Neuroscout “locks” finalized analyses. If users want to modify their analysis, they must “clone” that analysis, keeping alive a record of their previous analysis attempts. As such, there exists a trail of all analyses that preceded the “final” analysis that is shared and published. Thus, although we have not formally implemented any correction for multiple comparisons to combat dataset decay, by keeping track of all attempted analysis, we may be able to investigate this issue in more detail in the future. In addition, we believe that by placing a focus on meta-analytic approaches, Neuroscout minimizes over-reliance on results from any one dataset.</p><p>We mention this issue as a future challenge under “Challenges and future directions”</p><p>“In addition, as Neuroscout grows to facilitate the re-analysis of a broader set of public datasets, it will be important to reckon with the threat of “dataset decay” which can occur from repeated sequential re-analysis (Thompson, Wright, Bissett, and Poldrack, 2020). By encouraging the central registration of all analysis attempts and the associated results, Neuroscout is designed to minimize undocumented researcher degrees of freedom and link the final published results with all previous attempts. By encouraging the public sharing of all results, we hope to encourage meta-scientists to empirically investigate statistical solutions to the problem of dataset decay and develop methods to minimize the effect of false positives.” (pp. xx)</p></body></sub-article></article>