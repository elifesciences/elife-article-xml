<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">65566</article-id><article-id pub-id-type="doi">10.7554/eLife.65566</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Distinct higher-order representations of natural sounds in human and ferret auditory cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-217829"><name><surname>Landemard</surname><given-names>Agnès</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6081-1014</contrib-id><email>agnes.landemard@ens.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-219238"><name><surname>Bimbard</surname><given-names>Célian</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6380-5856</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-219234"><name><surname>Demené</surname><given-names>Charlie</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-83129"><name><surname>Shamma</surname><given-names>Shihab</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-190996"><name><surname>Norman-Haignere</surname><given-names>Sam</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9342-6868</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-219236"><name><surname>Boubenec</surname><given-names>Yves</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0106-6947</contrib-id><email>yves.boubenec@ens.fr</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Laboratoire des Systèmes Perceptifs, Département d’Études Cognitives, École Normale Supérieure PSL Research University, CNRS</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff2"><label>2</label><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution>Physics for Medicine Paris, Inserm, ESPCI Paris, PSL Research University, CNRS</institution><addr-line><named-content content-type="city">Paris</named-content></addr-line><country>France</country></aff><aff id="aff4"><label>4</label><institution>Institute for Systems Research, Department of Electrical and Computer Engineering, University of Maryland</institution><addr-line><named-content content-type="city">College Park</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution>HHMI Postdoctoral Fellow of the Life Sciences Research Foundation</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role>Reviewing Editor</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>18</day><month>11</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e65566</elocation-id><history><date date-type="received" iso-8601-date="2020-12-08"><day>08</day><month>12</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-10-22"><day>22</day><month>10</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-10-01"><day>01</day><month>10</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.09.30.321695"/></event></pub-history><permissions><copyright-statement>© 2021, Landemard et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Landemard et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-65566-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-65566-figures-v1.pdf"/><abstract><p>Little is known about how neural representations of natural sounds differ across species. For example, speech and music play a unique role in human hearing, yet it is unclear how auditory representations of speech and music differ between humans and other animals. Using functional ultrasound imaging, we measured responses in ferrets to a set of natural and spectrotemporally matched synthetic sounds previously tested in humans. Ferrets showed similar lower-level frequency and modulation tuning to that observed in humans. But while humans showed substantially larger responses to natural vs. synthetic speech and music in non-primary regions, ferret responses to natural and synthetic sounds were closely matched throughout primary and non-primary auditory cortex, even when tested with ferret vocalizations. This finding reveals that auditory representations in humans and ferrets diverge sharply at late stages of cortical processing, potentially driven by higher-order processing demands in speech and music.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>auditory cortex</kwd><kwd>natural sounds</kwd><kwd>functional ultrasound imaging</kwd><kwd>vocalizations</kwd><kwd>sensory coding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Ferret</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-17-EURE-0017 ANR-10-IDEX-0001-02</award-id><principal-award-recipient><name><surname>Landemard</surname><given-names>Agnès</given-names></name><name><surname>Bimbard</surname><given-names>Célian</given-names></name><name><surname>Shamma</surname><given-names>Shihab</given-names></name><name><surname>Boubenec</surname><given-names>Yves</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>787836-NEUME</award-id><principal-award-recipient><name><surname>Shamma</surname><given-names>Shihab</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>K99/R00</award-id><principal-award-recipient><name><surname>Norman-Haignere</surname><given-names>Sam</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000011</institution-id><institution>Howard Hughes Medical Institute</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Norman-Haignere</surname><given-names>Sam</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009559</institution-id><institution>Life Sciences Research Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral Fellowship</award-id><principal-award-recipient><name><surname>Norman-Haignere</surname><given-names>Sam</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NIDCD DC005779</award-id><principal-award-recipient><name><surname>Shamma</surname><given-names>Shihab</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001665</institution-id><institution>Agence Nationale de la Recherche</institution></institution-wrap></funding-source><award-id>ANR-JCJC-DynaMiC</award-id><principal-award-recipient><name><surname>Boubenec</surname><given-names>Yves</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003043</institution-id><institution>EMBO</institution></institution-wrap></funding-source><award-id>ALTF 740-2019</award-id><principal-award-recipient><name><surname>Bimbard</surname><given-names>Célian</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Auditory representations of natural sounds are similar in primary auditory cortex of ferrets and humans, but diverge sharply in non-primary areas for speech and music sounds.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Surprisingly little is known about how sensory representations of natural stimuli differ across species (<xref ref-type="bibr" rid="bib49">Theunissen and Elie, 2014</xref>). This question is central to understanding how evolution and development shape sensory representations (<xref ref-type="bibr" rid="bib33">Moore and Woolley, 2019</xref>) as well as developing animal models of human brain functions. Audition provides a natural test case because speech and music play a unique role in human hearing (<xref ref-type="bibr" rid="bib51">Zatorre et al., 2002</xref>; <xref ref-type="bibr" rid="bib21">Hickok and Poeppel, 2007</xref>; <xref ref-type="bibr" rid="bib40">Patel, 2012</xref>). While human knowledge of speech and music clearly differs from other species (<xref ref-type="bibr" rid="bib42">Pinker and Jackendoff, 2005</xref>), it remains unclear how neural representations of speech and music differ from those in other species, particularly within the auditory cortex. Few studies have directly compared neural responses to natural sounds between humans and other animals, and those that have done so have often observed similar responses. For example, both humans and non-human primates show regions that respond preferentially to conspecific vocalizations (<xref ref-type="bibr" rid="bib2">Belin et al., 2000</xref>; <xref ref-type="bibr" rid="bib41">Petkov et al., 2008</xref>). Human auditory cortex exhibits preferential responses for speech phonemes (<xref ref-type="bibr" rid="bib30">Mesgarani et al., 2014</xref>; <xref ref-type="bibr" rid="bib12">Di Liberto et al., 2015</xref>), but much of this sensitivity can be predicted by simple forms of spectrotemporal modulation tuning (<xref ref-type="bibr" rid="bib30">Mesgarani et al., 2014</xref>), and perhaps as a consequence can be observed in other animals such as ferrets (<xref ref-type="bibr" rid="bib29">Mesgarani et al., 2008</xref>; <xref ref-type="bibr" rid="bib48">Steinschneider et al., 2013</xref>). Consistent with this finding, maps of spectrotemporal modulation, measured using natural sounds, appear coarsely similar between humans and macaques (<xref ref-type="bibr" rid="bib18">Erb et al., 2019</xref>), although temporal modulations present in speech may be over-represented in humans. Thus, it remains unclear if the representation of natural sounds in auditory cortex differs substantially between humans and other animals, and if so, how.</p><p>A key challenge is that representations of natural stimuli are transformed across different stages of sensory processing, and species may share some but not all representational stages. Moreover, responses at different sensory stages are often correlated across natural stimuli (<xref ref-type="bibr" rid="bib10">de Heer et al., 2017</xref>), making them difficult to disentangle. Speech and music, for example, have distinctive patterns of spectrotemporal modulation energy (<xref ref-type="bibr" rid="bib46">Singh and Theunissen, 2003</xref>; <xref ref-type="bibr" rid="bib15">Ding et al., 2017</xref>), as well as higher-order structure (e.g., syllabic and harmonic structure) that is not well captured by modulation (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). To isolate neural sensitivity for higher-order structure, we recently developed a method for synthesizing sounds whose spectrotemporal modulation statistics are closely matched to a corresponding set of natural sounds (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). Because the synthetic sounds are otherwise unconstrained, they lack perceptually salient higher-order structure, which is particularly true for complex natural sounds like speech and music that are poorly captured by modulation statistics, unlike many other natural sounds (<xref ref-type="bibr" rid="bib28">McDermott and Simoncelli, 2011</xref>). We found that human primary auditory cortex responds similarly to natural and spectrotemporally matched synthetic sounds, while non-primary regions respond preferentially to the natural sounds. Most of this response enhancement is driven by preferential responses to natural vs. synthetic speech and music in non-primary auditory cortex. The specificity for speech and music could be due to their ecological relevance in humans and/or the fact that speech and music are more complex than other sounds, and thus perceptually differ more from their synthetic counterparts. But notably, the response preference for natural speech and music cannot be explained by speech semantics since similar responses are observed for native and foreign speech (<xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Overath et al., 2015</xref>), or explicit musical training, since humans without any training show similar response preferences for music in their non-primary auditory cortex (<xref ref-type="bibr" rid="bib4">Boebinger et al., 2020</xref>). These findings suggest that human non-primary regions respond to higher-order acoustic features that both cannot be explained by lower-level modulation statistics and do not yet reflect explicit semantic knowledge.</p><p>The goal of the present study was to test whether such higher-order sensitivity is present in another species. We test three key hypotheses: (1) higher-order sensitivity in humans reflects a generic mechanism present across species for analyzing complex sounds like speech and music; (2) higher-order sensitivity reflects an adaptation to ecologically relevant sounds such as speech and music in humans or vocalizations in other species; and (3) higher-order sensitivity reflects a specific adaptation in humans, potentially driven by the unique demands of speech and music perception, that is not generically present in other species even for ecologically relevant sounds. We addressed this question by measuring cortical responses in ferrets – one of the most common animal models used to study auditory cortex (<xref ref-type="bibr" rid="bib34">Nelken et al., 2008</xref>) – to the same set of natural and synthetic sounds previously tested in humans, as well as natural and synthetic ferret vocalizations. Responses were measured using functional ultrasound imaging (fUS) (<xref ref-type="bibr" rid="bib27">Macé et al., 2011</xref>; <xref ref-type="bibr" rid="bib3">Bimbard et al., 2018</xref>), a recently developed wide-field imaging technique that like fMRI detects changes in neural activity via changes in blood flow (movement of blood induces a Doppler effect detectable with ultrasound). fUS has substantially better spatial resolution than fMRI, making it applicable to small animals like ferrets. We found that tuning for spectrotemporal modulations present in both natural and synthetic sounds was similar between humans and animals, and could be quantitatively predicted across species, consistent with prior findings (<xref ref-type="bibr" rid="bib29">Mesgarani et al., 2008</xref>; <xref ref-type="bibr" rid="bib18">Erb et al., 2019</xref>). But unlike humans, ferret responses to natural and synthetic sounds were similar throughout primary and non-primary auditory cortex even when comparing natural and synthetic ferret vocalizations; and the small differences that were present in ferrets were weak and spatially scattered. This finding suggests that representations of natural sounds in humans and ferrets diverge substantially at the final stages of acoustic processing.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Experiment I: Comparing ferret cortical responses to natural vs. synthetic sounds</title><p>We measured cortical responses with fUS to the same 36 natural sounds tested previously in humans plus four additional ferret vocalizations (experiment II tested many more ferret vocalizations). The 36 natural sounds included speech, music, and other environmental sounds (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). For each natural sound, we synthesized four sounds that were matched on acoustic statistics of increasing complexity (<xref ref-type="fig" rid="fig1">Figure 1A</xref>): (1) cochlear energy statistics, (2) temporal modulation statistics, (3) spectral modulation statistics, and (4) spectrotemporal modulation statistics. Cochlear-matched sounds had a similar frequency spectrum, but their modulation content was unconstrained and thus differed from the natural sounds. Modulation-matched sounds were additionally constrained in their temporal and/or spectral modulation rates, measured by linearly filtering a cochleagram representation with filters tuned to different modulation rates (modulation-matched sounds also had matched cochlear statistics so as to isolate the contribution of modulation sensitivity). The modulation-matched sounds audibly differ from their natural counterparts, particularly for complex sounds like speech and music that contain higher-order structure not captured by frequency and modulation statistics (listen to example sounds <ext-link ext-link-type="uri" xlink:href="http://mcdermottlab.mit.edu/svnh/model-matching/Stimuli_from_Model-Matching_Experiment.html">here</ext-link>). We focused on time-averaged statistics because the hemodynamic response measured by both fMRI and fUS reflects a time-averaged measure of neural activity. As a consequence, each of the synthetic sounds can be thought of as being matched under a different model of the hemodynamic response (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic of stimuli and imaging protocol.</title><p>(<bold>A</bold>) Cochleagrams for two example natural sounds (left column) and corresponding synthetic sounds (right four columns) that were matched to the natural sounds along a set of acoustic statistics of increasing complexity. Statistics were measured by filtering a cochleagram with filters tuned to temporal, spectral, or joint spectrotemporal modulations. (<bold>B</bold>) Schematic of the imaging procedure. A three-dimensional volume, covering all of ferret auditory cortex, was acquired through successive coronal slices. Auditory cortical regions (colored regions) were mapped with anatomical and functional markers (<xref ref-type="bibr" rid="bib44">Radtke-Schuller, 2018</xref>). The rightmost image shows a single ultrasound image with overlaid region boundaries. Auditory regions: dPEG: dorsal posterior ectosylvian gyrus; AEG: anterior ectosylvian gyrus; VP: ventral posterior auditory field; ADF: anterior dorsal field; AAF: anterior auditory field. Non-auditory regions: hpc: hippocampus; SSG: suprasylvian gyrus; LG: lateral gyrus. Anatomical markers: pss: posterior sylvian sulcus; sss: superior sylvian sulcus. (<bold>C</bold>) Response timecourse of a single voxel to all natural sounds, before (left) and after (right) denoising. Each line reflects a different sound, and its color indicates its membership in one of 10 different categories. English and non-English speech are separated out because all of the human subjects tested in our prior study were native English speakers, and so the distinction is meaningful in humans. The gray region shows the time window when sound was present. We summarized the response of each voxel by measuring its average response to each sound between 3 and 11 s post-stimulus onset. The location of this voxel corresponds to the highlighted voxel in panel B. (<bold>D</bold>) We measured the correlation across sounds between pairs of voxels as a function or their distance using two independent measurements of the response (odd vs. even repetitions). Results are plotted separately for ferret fUS data (left) and human fMRI data (right). The 0 mm datapoint provides a measure of test–retest reliability and the fall-off with distance provides a measure of spatial precision. Results are shown before and after component denoising. Note that in our prior fMRI study we did not use component denoising because the voxels were sufficiently reliable; we used component-denoised human data here to make the human and ferret analyses more similar (findings did not depend on this choice: see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The distance needed for the correlation to decay by 75% is shown above each plot (<inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>75</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>). The human data were smoothed using a 5 MM FWHM kernel, the same amount used in our prior study, but fMRI responses were still coarser when using unsmoothed data (<inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mn>75</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> = 6.5 mm; findings did not depend on the presence/absence of smoothing). Thin lines show data from individual human (N = 8) and ferret (N = 2) subjects, and thick lines show the average across subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>The effect of enhancing reliable signal using a procedure similar to ‘denoising source separation (DSS)’ (see ‘Denoising part II’ in Materials and methods) (<xref ref-type="bibr" rid="bib8">de Cheveigné and Parra, 2014</xref>).</title><p>(<bold>A</bold>) Voxel responses were denoised by projecting their timecourse onto components that were reliably present across repetitions and slices. This figure plots the test–retest correlation across independent splits of data before (x-axis) and after (y-axis) denoising (data from experiment I). Each dot corresponds to a single voxel. We denoised either one split of data (blue dots) or both splits of data (green dots). Denoising one split provides a fairer test of whether the denoising procedure enhances SNR. Denoising both splits shows the overall effect on response reliability. The theoretical upper bound for denoising one split of data is shown by the black line. The denoising procedure substantially increased data reliability, with the one-split correlations hugging the upper bound. This plot shows results from an eight-component model. (<bold>B</bold>) This figure plots split-half correlations for denoised data (one split) as a map (upper panel), along with a map showing the upper bound (lower panel). Denoised correlations were close to their upper bound throughout auditory cortex. (<bold>C</bold>) This figure plots the median denoised correlation across voxels (one split) as a function of the number of components used in the denoising procedure. Gray line plots the upper bound. Shaded areas indicate the 95% confidence interval, computed via bootstrapping across the sound set. Results are shown for both experiments I (left) and II (right). Predictions were near their maximum using approximately eight components in both experiments (the eight-component mark is shown by the vertical dashed line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Effect of component denoising on human fMRI results.</title><p>This figure plots normalized squared error (NSE) maps comparing natural and synthetic sounds in humans both before (top) and after denoising (bottom) by projecting onto the six reliable components identified in our prior work (<xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>). We used component-denoised data for all species comparisons to make the analyses more similar, but results were similar without denoising. The bottom panel is the same as that shown in <xref ref-type="fig" rid="fig2">Figure 2E</xref> and is reproduced here for ease of comparison. Results are based on 12 human subjects.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig1-figsupp2-v1.tif"/></fig></fig-group><p>We measured fUS responses throughout primary and non-primary ferret auditory cortex (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We first plot the response timecourse to all 40 natural sounds for one example voxel in non-primary auditory cortex (dPEG) (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). We plot the original timecourse of the voxel as well as a denoised version, computed by projecting the timecourse onto a small number of reliable components (see Materials and methods). Our denoising procedure substantially boosted the SNR of the measurements (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) and made it possible to analyze individual voxels, as opposed to averaging responses across a large region of interest (ROI), which could potentially wash out heterogeneity present at the single-voxel level. As expected and similar to fMRI, we observed a gradual build-up of the hemodynamic response after stimulus onset. The shape of the response timecourse was similar across stimuli, but the magnitude of the response varied. We thus summarized the response of each voxel to each sound by its time-averaged response magnitude (the same approach used in our prior fMRI study). We found that the denoised fUS responses were substantially more reliable and precise than the fMRI voxels from our prior study (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) (test–retest correlation: 0.93 vs. 0.44, Wilcoxon rank-sum test across subjects, p&lt;0.01). To make our human and ferret analyses more similar, we used component-denoised fMRI data in this study, which had similar reliability to the denoised fUS data (<xref ref-type="fig" rid="fig1">Figure 1D</xref>; results were similar without denoising, see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>We next plot the response of two example fUS voxels – one in primary auditory cortex (A1) and one in a non-primary area (dPEG) – to natural and corresponding synthetic sounds that have been matched on the full spectrotemporal modulation model (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; results were similar when averaging responses within anatomical regions of interest, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). For comparison, we plot the test–retest reliability of each voxel across repeated presentations of the same sound (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), as well as corresponding figures from two example voxels in human primary/non-primary auditory cortex (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref>). As in our prior study, we quantified the similarity of responses to natural and synthetic sounds using the normalized squared error (NSE). The NSE takes a value of 0 if responses to natural and synthetic sounds are the same, and 1 if there is no correspondence between the two (see Materials and methods for details).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Dissimilarity of responses to natural vs. synthetic sounds in ferrets and humans.</title><p>(<bold>A</bold>) Response of two example fUS voxels to natural and corresponding synthetic sounds with matched spectrotemporal modulation statistics. Each dot shows the time-averaged response to a single pair of natural/synthetic sounds (after denoising), with colors indicating the sound category. The example voxels come from primary (top, A1) and non-primary (bottom, dPEG) regions of the ferret auditory cortex (locations shown in panel E). The normalized squared error (NSE) quantifies the dissimilarity of responses. (<bold>B</bold>) Test–retest response of the example voxels across all natural (o) and synthetic (+) sounds (odd vs. even repetitions). The responses were highly reliable due to the denoising procedure. (<bold>C, D</bold>) Same as panels (<bold>A, B</bold>), but showing two example voxels from human primary/non-primary auditory cortex. (<bold>E</bold>) Maps plotting the dissimilarity of responses to natural vs. synthetic sounds from one ferret hemisphere (top row) and from humans (bottom row). Each column shows results for a different set of synthetic sounds. The synthetic sounds were constrained by statistics of increasing complexity (from left to right): just cochlear statistics, cochlear + temporal modulation statistics, cochlear + spectral modulation statistics, and cochlear + spectrotemporal modulation statistics. Dissimilarity was quantified using the NSE, corrected for noise using the test–retest reliability of the voxel responses. Ferret maps show a ‘surface’ view from above of the sylvian gyri, similar to the map in humans. Surface views were computed by averaging activity perpendicular to the cortical surface. The border between primary and non-primary auditory cortex is shown with a white line in both species and was defined using tonotopic gradients. Areal boundaries in the ferret are also shown (dashed thin lines). This panel shows results from one hemisphere of one animal (ferret T, left hemisphere), but results were similar in other animals/hemispheres (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). The human map is a group map averaged across 12 subjects, but results were similar in individual subjects (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). (<bold>F</bold>) Voxels were binned based on their distance to primary auditory cortex (defined tonotopically). This figure plots the median NSE value in each bin. Each thin line corresponds to a single ferret (gray) or a single human subject (gold). Thick lines show the average across all subjects. The ferret and human data were rescaled so that they could be plotted on the same figure, using a scaling factor of 10, which roughly corresponds to the difference in the radius of primary auditory cortex between ferrets and humans. The corresponding unit is plotted on the x-axis below. The number of human subjects varied by condition (see Materials and methods for details) and is indicated on each plot. (<bold>G</bold>) The slope of NSE vs. distance-to-primary auditory cortex (PAC) curve (<bold>F</bold>) from individual ferret and human subjects using responses to the spectrotemporally matched synthetic sounds. We used absolute distances to quantify the slope, which is conservative with respect to the hypothesis since correcting for brain size would differentially increase the ferret slopes.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Responses to natural and synthetic sounds in standard anatomical regions of interest (ROIs).</title><p>Format is analogous to <xref ref-type="fig" rid="fig2">Figure 2A and B</xref>. (<bold>A</bold>) Cartoon showing the location of three ROIs spanning primary (MEG) and non-primary (AEG, PEG) ferret auditory cortex. (<bold>B</bold>) Response to natural and spectrotemporally matched synthetic sounds averaged across all voxels in each ROI. Each circle corresponds to a single pair of natural/synthetic sounds, with colors indicating the sound category. The normalized squared error (NSE) between natural and synthetic sounds is shown above each plot. (<bold>C</bold>) Test–retest response of the ROI across all natural (o) and synthetic (+) sounds (odd vs. even repetitions). The test–retest NSE provides a noise floor for the natural vs. synthetic NSE.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Dissimilarity maps for all hemispheres and animals.</title><p>Same format as <xref ref-type="fig" rid="fig2">Figure 2E</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Uncorrected normalized squared error (NSE) values.</title><p>This figure plots the uncorrected NSE between natural and synthetic sounds as a function of distance to primary auditory cortex (PAC) for humans (<bold>A</bold>) and ferrets (<bold>B</bold>). The test–retest NSE value, which provides a noise floor for the natural vs. synthetic NSE, is plotted below each set of curves using dashed lines. Each thin line corresponds to a single ferret (gray) or a single human subject (gold). Thick lines show the average across all subjects. Format is the same as <xref ref-type="fig" rid="fig2">Figure 2F</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig2-figsupp3-v1.tif"/></fig></fig-group><p>Both the primary and non-primary ferret voxels produced similar responses to natural and corresponding synthetic sounds (NSEs: 0.084, 0.13), suggesting that spectrotemporal modulations are sufficient to account for most of the response variance in these voxels. The human primary voxel also showed similar responses to natural and synthetic responses (NSE: 0.080). In contrast, the human non-primary voxel responded substantially more to the natural speech (green) and music (blue) than matched synthetics, yielding a high NSE value (0.67). This pattern demonstrates that spectrotemporal modulations are insufficient to drive the response of the human non-primary voxel, plausibly because it responds to higher-order features that are not captured by modulation statistics.</p><p>We quantified this trend across voxels by plotting maps of the NSE between natural and synthetic sounds (<xref ref-type="fig" rid="fig2">Figure 2E</xref> shows one hemisphere of one animal, but results were very similar in other hemispheres of other animals, see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). We used the test–retest reliability of the responses to noise-correct the measured NSE values such that the effective noise floor given the reliability of the measurements is zero. We show separate maps for each of the different sets of statistics used to constrain the synthetic sounds (cochlear, temporal modulation, spectral modulation, and spectrotemporal modulation). Each map shows a view from above auditory cortex, computed by averaging NSE values perpendicular to the cortical sheet. We summarized the data in this way because we found that maps were similar across the different layers within a cortical column. Below we plot corresponding maps from humans. The human maps are based on data averaged across subjects, but similar results were observed in individual subjects (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>).</p><p>In ferrets, we found that responses became more similar as we matched additional acoustic features, as expected (NSE spectrotemporal &lt; NSE temporal &lt; NSE spectral &lt; NSE cochlear, p&lt;0.01 in every ferret; significance computed via bootstrapping across sounds the median NSE value across all voxels in auditory cortex). Notably, we observed similar NSE values in primary and non-primary regions for all conditions, and for sounds matched on joint spectrotemporal statistics, NSE values were close to 0 throughout most of auditory cortex. This pattern contrasts sharply with that observed in humans, where we observed a clear and substantial rise in NSE values when moving from primary to non-primary regions even for sounds matched on joint spectrotemporal modulations statistics. We quantified these effects by binning voxels based on their distance to primary auditory cortex, as was done previously in humans (<xref ref-type="fig" rid="fig2">Figure 2F</xref>; see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> for results without noise correction), and then measuring the slope of the NSE-vs.-distance curve for each human subject and each ferret tested (<xref ref-type="fig" rid="fig2">Figure 2G</xref>). We used absolute distances for calculating the slopes, which is a highly conservative choice given our findings since correcting for brain size would enhance the slopes of ferrets relative to humans. Despite this choice, we found that the slope of every ferret was well below that of all 12 human subjects tested, and thus significantly different from the human group via a non-parametric sign test (p&lt;0.001). This finding demonstrates that the higher-order sensitivity we previously observed for natural sounds in human non-primary auditory cortex is not a generic feature of higher-order processing in mammals.</p></sec><sec id="s2-2"><title>Assessing and comparing sensitivity for frequency and modulation across species</title><p>Our NSE maps suggest that ferret cortical responses are tuned for frequency and modulation, but do not reveal how this tuning is organized or whether it is similar to that in humans. While it is not feasible to inspect or plot all individual voxels, we found that fUS responses like human fMRI responses are low-dimensional and can be explained as the weighted sum of a small number of component response patterns. This observation served as the basis for our denoising procedure, as well as a useful way to examine ferret cortical responses and to compare those responses with humans. We found that we could discriminate approximately eight distinct component response patterns before overfitting to noise.</p><p>We first examined the inferred response patterns and their anatomical distribution of weights in the brain (<xref ref-type="fig" rid="fig3">Figure 3</xref> shows three example components; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> shows all eight components). All of the component response profiles showed significant correlations with measures of energy at different cochlear frequencies and spectrotemporal modulation rates (<xref ref-type="fig" rid="fig3">Figure 3D and E</xref>) (p&lt;0.01 for all components for both frequency and modulation features; statistics computed via a permutation test across the sound set). Two components (f1 and f2) had responses that correlated with energy at high and low frequencies, respectively, with voxel weights that mirrored the tonotopic gradients measured in these animals (compare <xref ref-type="fig" rid="fig3">Figure 3A</xref> with <xref ref-type="fig" rid="fig3">Figure 3B</xref>; see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for all hemispheres/animals), similar to the tonotopic components previously identified in humans (<xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>; <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>, components h1 and h2). We also observed components with weak frequency tuning but prominent tuning for spectrotemporal modulations (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>), again similar to humans. Perhaps surprisingly, one component (f3) responded preferentially to speech sounds, and its response correlated with energy at frequency and modulation rates characteristic of speech (insets in <xref ref-type="fig" rid="fig3">Figure 3D and E</xref>, bottom row). But notably, all of the inferred components, including the speech-preferring component, produced very similar responses to natural and synthetic sounds (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), suggesting that their response can be explained by tuning for frequency and modulation. This contrasts with the speech- and music-preferring components previously observed in humans, which showed a clear response preference for natural speech and music, respectively, and which clustered in distinct non-primary regions of human auditory cortex (see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>, components h5 and h6). This finding shows that preferential responses for natural speech compared with other natural sounds are not unique to humans, and thus that comparing responses to natural vs. synthetic sounds is critical to revealing representational differences between species.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Organization of frequency and modulation tuning in ferret auditory cortex, as revealed by component analysis.</title><p>(<bold>A</bold>) For reference with the weight maps in panel (<bold>B</bold>), a tonotopic map is shown, measured using pure tones. The map is from one hemisphere of one animal (ferret T, left). (<bold>B</bold>) Voxel weight maps from three components, inferred using responses to natural and synthetic sounds (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for all eight components and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for all hemispheres). The maps for components f1 and f2 closely mirrored the high- and low-frequency tonotopic gradients, respectively. (<bold>C</bold>) Component response to natural and spectrotemporally matched synthetic sounds, colored based on category labels (labels shown at the bottom left of the figure). Component f3 responded preferentially to speech sounds. (<bold>D</bold>) Correlation of component responses with energy at different audio frequencies, measured from a cochleagram. Inset for f3 shows the correlation pattern that would be expected from a response that was perfectly speech selective (i.e., 1 for speech, 0 for all other sounds). (<bold>E</bold>) Correlations with modulation energy at different temporal and spectral rates. Inset shows the correlation pattern that would be expected for a speech-selective response. Results suggest that f3 responds to particular frequency and modulation statistics that happen to differ between speech and other sounds.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Results from all eight ferret components.</title><p>(<bold>A</bold>) Voxel weight map for each component. (<bold>B</bold>) The temporal response of each component. Black line shows the average timecourse across all natural sounds. Colored lines correspond to major categories (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>): speech (green), music (blue), vocalizations (pink), and other sounds (brown). Note that the temporal shape varies across components, but is very similar across sounds/categories within a component, which is why we summarized component responses by their time-averaged response to each sound. (<bold>C</bold>) Time-averaged component responses to natural and spectrotemporally matched synthetic sounds, colored based on category labels. (<bold>D</bold>) Correlation of component responses with energy at different audio frequencies, measured from a cochleagram. (<bold>E</bold>) Correlations with modulation energy at different temporal and spectral rates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Component weight maps from all hemispheres and ferrets.</title><p>(<bold>A</bold>) For reference with the weight maps in panel (<bold>B</bold>), tonotopic maps measured using pure tones are shown for all hemispheres. (<bold>B</bold>) Voxel weight maps from the three components shown in <xref ref-type="fig" rid="fig3">Figure 3</xref> for all hemispheres of all ferrets tested. (<bold>C</bold>) Voxel weights for three example coronal slices from ferret T, left hemisphere. Gray outlines in panel (<bold>B</bold>) indicate their location in the ‘surface’ view. Each slice corresponds to one vertical strip from the maps in panel (<bold>B</bold>). The same slices are shown for all three components.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Human components.</title><p>This figure shows the anatomy and response properties of the six human components inferred in prior work (<xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). (<bold>A</bold>) Voxel weight map for each component, averaged across subjects. (<bold>B</bold>) Component responses to natural and spectrotemporally matched synthetic sounds, colored based on category labels. (<bold>C</bold>) Correlation of component responses with energy at different audio frequencies, measured from a cochleagram. (<bold>D</bold>) Correlations with modulation energy at different temporal and spectral rates.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Predicting human component responses from ferret components.</title><p>This figure plots the results of trying to predict the six human components inferred from our prior work (<xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>) from the eight ferret components inferred here (see <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref> for the reverse). (<bold>A</bold>) For reference, the response of the six human components to natural and spectrotemporally matched synthetic sounds is re-plotted here. Components h1–h4 produced similar responses to natural and synthetic sounds and had weights that clustered in and around primary auditory cortex (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Components h5 and h6 responded selectively to natural speech and natural music, respectively, and had weights that clustered in non-primary regions. (<bold>B</bold>) This panel plots the measured response of each human component to just the spectrotemporally matched synthetic sounds, along with the predicted response from ferrets. (<bold>C</bold>) This panel plots the difference between responses to natural and spectrotemporally matched synthetic sounds along with the predicted difference from the ferret components. (<bold>D</bold>) This panel plots the total response variance (white bars) of each human component to synthetic sounds (left) and to the difference between natural and synthetic sounds (right) along with the fraction of that total response variance predictable from ferrets (gray bars) (all variance measures are noise-corrected). Error bars show the 95% confidence interval, computed via bootstrapping across the sound set. (<bold>E</bold>) Same as (<bold>D</bold>), but averaged across components.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Predicting ferret component responses from human components.</title><p>(<bold>A</bold>) This panel plots the measured response of each ferret component to just the spectrotemporally matched synthetic sounds, along with the predicted response from humans. (<bold>B</bold>) This panel plots the difference between responses to natural and spectrotemporally matched synthetic sounds along with the predicted difference from the human components. (<bold>C-D</bold>) This panel plots the total response variance (white bars) of each ferret component to synthetic sounds (<bold>C</bold>) and to the difference between natural and synthetic sounds (<bold>D</bold>) along with the fraction of that total response variance predictable from humans (gray bars) (all variance measures are noise-corrected). Error bars show the 95% confidence interval, computed via bootstrapping across the sound set. (<bold>E</bold>) Same as (<bold>C-D</bold>), but averaged across components.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig3-figsupp5-v1.tif"/></fig></fig-group><p>Overall, the frequency and modulation tuning evident in the ferret components appeared similar to that in humans (<xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>). To quantitatively evaluate similarity, we attempted to predict the response of each human component, inferred from our prior work, from those in the ferrets (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>) and vice versa (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>). We found that much of the component response variation to synthetic sounds could be predicted across species (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4B,D and E</xref>, <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5A, C and D</xref>). This finding is consistent with the hypothesis that tuning for frequency and modulation is similar across species since the synthetic sounds only varied in their frequency and modulation statistics. In contrast, differences between natural vs. synthetic sounds were only robust in humans and as a consequence could not be predicted from responses in ferrets (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4C, D and E</xref>). Thus, frequency and modulation tuning are both qualitatively and quantitatively similar across species, despite substantial differences in higher-order sensitivity.</p></sec><sec id="s2-3"><title>Experiment II: Testing the importance of ecological relevance</title><p>The results of experiment I show that higher-order sensitivity in humans is not a generic feature of auditory processing for complex sounds. However, the results could still be explained by a difference in ecological relevance since differences between natural and synthetic sounds in humans are mostly driven by speech and music (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>) and experiment I included more speech (8) and music (10) sounds than ferret vocalizations (4). To test this possibility, we performed a second experiment that included many more ferret vocalizations (30), as well as a smaller number of speech (14) and music (16) sounds to allow comparison with experiment I. We only synthesized sounds matched in their full spectrotemporal modulation statistics to be able to test a broader sound set.</p><p>Despite testing many more ferret vocalizations, results were nonetheless similar to those of experiment I: voxel responses to natural and synthetic sounds were similar throughout primary and non-primary auditory cortex, yielding low NSE values everywhere (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We also observed similar component responses to those observed in experiment I (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). To directly test if ferrets showed preferential responses to natural vs. synthetic ferret vocalizations, we computed maps plotting the average difference between natural vs. synthetic sounds for different categories, using data from both experiments I and II (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). We also separately measured the NSE for sounds from different categories, again plotting NSE values as a function of distance to PAC (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>). The differences that we observed between natural and synthetic sounds were small and scattered throughout primary and non-primary auditory cortex, even for ferret vocalizations. In one animal, we observed significantly larger NSE values for ferret vocalizations compared with speech and music (ferret A, Md<sub>voc</sub> = 0.14 vs. Md<sub>SpM</sub> = 0.042, Wilcoxon rank-sum test: T = 1138, z = 3.29, p&lt;0.01). But this difference was not present in the other two ferrets tested (p&gt;0.55) and was also not present when we averaged NSE values across animals (Md<sub>voc</sub> = 0.053 vs. Md<sub>SpM</sub> = 0.033, Wilcoxon rank-sum test: T = 1016, z = 1.49, p=0.27). Moreover, the slope of the NSE vs. distance-to-PAC curve was near 0 for all animals and sound categories, even for ferret vocalizations, and was substantially lower than the slopes measured in all 12 human subjects (<xref ref-type="fig" rid="fig4">Figure 4E</xref>) (vocalizations in ferrets vs. speech in humans: p&lt;0.001 via a sign test; speech in ferrets vs. speech in humans: p&lt;0.001). In contrast, human cortical responses were substantially larger for natural vs. synthetic speech and music, and these response enhancements were concentrated in distinct non-primary regions (lateral for speech and anterior/posterior for music) and different from those for other natural sounds (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Thus, ferrets do not show any of the neural signatures of higher-order sensitivity that we previously identified in humans (large effect size, spatially clustered responses, and a clear non-primary bias), even for conspecific vocalizations.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Testing the importance of ecological relevance.</title><p>Experiment II measured responses to a larger number of ferret vocalizations (30 compared with 4 in experiment I), as well as speech (14) and music (16) sounds. (<bold>A</bold>) Map showing the dissimilarity between natural and spectrotemporally matched synthetic sounds from experiment II for each recorded hemisphere, measured using the noise-corrected normalized squared error (NSE). NSE values were low across auditory cortex, replicating experiment I. (<bold>B</bold>) Maps showing the average difference between responses to natural and synthetic sounds for vocalizations, speech, music, and others sounds, normalized for each voxel by the standard deviation across all sounds. Results are shown for ferret T, left hemisphere for both experiments I and II (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref> for all hemispheres). For comparison, the same difference maps are shown for the human subjects, who were only tested in experiment I. (<bold>C</bold>) NSE for different sound categories, plotted as a function of distance to primary auditory cortex (binned as in <xref ref-type="fig" rid="fig2">Figure 2F</xref>). Shaded area represents 1 standard error of the mean across sounds within each category (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D</xref> plots NSEs for individual sounds). (<bold>D</bold>) Same as panel (<bold>C</bold>) but showing results from experiment II. (<bold>E</bold>) The slope of NSE vs. distance-to-primary auditory cortex (PAC) curves for individual ferrets and human subjects. Ferret slopes were measured separately for ferret vocalizations (black lines) and speech (gray lines) (animal indicated by line style). For comparison, human slopes are plotted for speech (each yellow line corresponds to a different human subject).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Results of experiment II from other hemispheres.</title><p>(<bold>A</bold>) The animal’s spontaneous movements were monitored with a video recording of the animal’s face. Motion was measured as the mean absolute deviation between adjacent video frames, averaged across pixels. (<bold>B</bold>) Average evoked movement amplitude for natural (shaded) and synthetic (unshaded) sounds broken down by category. Each dot represents one sound. Significant differences between natural and synthetic sounds, and between categories of natural sounds are plotted (Wilcoxon signed-rank test, *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001). Evoked movement amplitude was normalized by the standard deviation across sounds for each recording session prior to averaging across sound category (necessary because absolute pixel deviations cannot be meaningfully compared across sessions). Movement amplitude is shown for each animal separately. (<bold>C</bold>) Same format as <xref ref-type="fig" rid="fig4">Figure 4B</xref> but showing results from additional hemispheres/animals. (<bold>D</bold>) This panel shows the distribution of normalized squared error (NSE) values for all pairs of natural and synthetic sounds (median across all voxels; averaged across subjects for humans), grouped by category. Dots show individual sound pairs and boxplots show the median, central 50%, and central 92% (whiskers) of the distribution. Humans were only tested in experiment I. Note that the two outliers for the human music plot are sound clips that contain singing, and thus are a mixture of both speech and music, which likely explains the particularly divergent responses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Components from experiment II.</title><p>The components derived from experiment II were similar to those from experiment I, shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>. This figure plots similar low-frequency, high-frequency, and speech-preferring components from experiment II. <xref ref-type="fig" rid="fig3">Figure 3</xref> (<bold>A</bold>) For reference with the weight maps in panel (<bold>B</bold>), a tonotopic map is shown, measured using pure tones. The map is from one hemisphere of one animal (ferret T, left). (<bold>B</bold>) Voxel weight maps. (<bold>C</bold>) Component response to natural and spectrotemporally matched synthetic sounds, colored based on category labels (labels shown at the bottom left of the figure). (<bold>D</bold>) Correlation of component responses with energy at different audio frequencies, measured from a cochleagram. Inset for f3 shows the correlation pattern that would be expected from a response that was perfectly speech selective (i.e., 1 for speech, 0 for all other sounds). (<bold>E</bold>) Correlations with modulation energy at different temporal and spectral rates. Inset shows the correlation pattern that would be expected for a speech-selective response.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>The effect of removing outside-of-cortex components on motion correlations.</title><p>Voxel responses were denoised by removing components from outside of cortex, which are likely to reflect artifacts like motion (see ‘Denoising part I’ in Materials and methods). (<bold>A</bold>) Effect of removing components from outside of cortex on correlations with movement. We measured the correlation of each voxel’s response with movement, measured from a video recording of the animal’s face (absolute deviation between adjacent frames). Each line shows the average absolute correlation across voxels for a single recording session/slice. Correlation values are plotted as a function of the number of removed components. Motion correlations were substantially reduced by removing the top 20 components (vertical dotted line). (<bold>B</bold>) The average difference between responses to natural vs. synthetic sounds for an example slice (ferret A) before and after removing the top 20 out-of-cortex components. Motion induces a stereotyped ‘striping’ pattern due to its effect on blood vessels, which is evident in the map computed from raw data, likely because this ferret moved substantially more during natural vs. synthetic sounds (in particular for ferret vocalizations; <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). The striping pattern is unlikely to reflect genuine neural activity and is largely removed by the denoising procedure.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-fig4-figsupp3-v1.tif"/></fig></fig-group><p>Given the weak neural differences between natural and synthetic sounds, we wondered if any of the animals could perceive the difference between natural and synthetic sounds. Using a video recording of the animals’ face (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>), we found that one ferret (ferret A) spontaneously moved more during the presentation of the natural ferret vocalizations compared with both the synthetic sounds (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>, Md<sub>voc, nat</sub> = 1.77 vs. Md<sub>voc, synth</sub> = 1.07, Wilcoxon signed-rank test across sounds: T = 464, z = 4.76, p&lt;0.001) and the other natural sounds (Md<sub>voc, nat</sub> = 1.8 vs. Md<sub>others, nat</sub> = 0.65, Wilcoxon rank-sum test across sounds T = 1301, z = 5.70, p&lt;0.001). There was a similar trend in a second animal (ferret T; Md<sub>voc, nat</sub> = 1.68 vs. Md<sub>voc, synth</sub> = 1.44, T = 335, z = 2.11, p=0.07; Md<sub>voc, nat</sub> = 1.6 vs. Md<sub>others, nat</sub> = 0.97, T = 1269, z = 5.23, p&lt;0.001), but not in the third (ferret C; Md<sub>voc, nat</sub> = 0.41 vs. Md<sub>voc, synth</sub> = 0.47, T = 202, z = –0.62, p=0.53), likely because the animal did not move very much for any of the sounds. This finding demonstrates that ferrets are perceptually capable of detecting the difference between natural and synthetic sounds without any overt training and that this difference is more salient for ferret vocalizations, consistent with their greater ecological relevance. Since our key neural findings were present in all animals tested, including ferret A, we conclude that our results cannot be explained by an inability to perceptually detect differences between natural and synthetic vocalizations.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Our study reveals a prominent divergence in the representation of natural sounds between humans and ferrets. Using a recently developed wide-field imaging technique (fUS), we measured cortical responses in the ferret to a set of natural and spectrotemporally matched synthetic sounds previously tested in humans. We found that tuning for frequency and modulation statistics in the synthetic sounds was similar across species. But unlike humans, who showed preferential responses to natural vs. synthetic speech and music in non-primary regions, ferret cortical responses to natural and synthetic sounds were similar throughout primary and non-primary auditory cortex, even when tested with ferret vocalizations. This finding suggests that higher-order sensitivity in humans for natural vs. synthetic speech/music (1) does not reflect a species-generic mechanism for analyzing complex sounds and (2) does not reflect a species-generic adaptation for coding ecologically relevant sounds like conspecific vocalizations. Instead, our findings suggest that auditory representations in humans diverge from ferrets at higher-order processing stages, plausibly driven by the unique demands of speech and music perception.</p><sec id="s3-1"><title>Species differences in the representation of natural sounds</title><p>The central challenge of sensory coding is that behaviorally relevant information is often not explicit in the inputs to sensory systems. As a consequence, sensory systems transform their inputs into higher-order representations that expose behaviorally relevant properties of stimuli (<xref ref-type="bibr" rid="bib14">DiCarlo and Cox, 2007</xref>; <xref ref-type="bibr" rid="bib32">Mizrahi et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Theunissen and Elie, 2014</xref>). The early stages of this transformation are thought to be conserved across many species. For example, all mammals transduce sound pressure waveforms into a frequency-specific representation of sound energy in the cochlea, although the resolution and frequency range of cochlear tuning differ across species (<xref ref-type="bibr" rid="bib6">Bruns and Schmieszek, 1980</xref>; <xref ref-type="bibr" rid="bib24">Koppl et al., 1993</xref>; <xref ref-type="bibr" rid="bib22">Joris et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Walker et al., 2019</xref>). But it has remained unclear whether representations at later stages are similarly conserved across species.</p><p>Only a few studies have attempted to compare cortical representations of natural sounds between humans and other animals, and these studies have typically found similar representations in auditory cortex. Studies of speech phonemes in ferrets (<xref ref-type="bibr" rid="bib29">Mesgarani et al., 2008</xref>) and macaques (<xref ref-type="bibr" rid="bib48">Steinschneider et al., 2013</xref>) have replicated neural phenomena observed in humans (<xref ref-type="bibr" rid="bib30">Mesgarani et al., 2014</xref>). A recent fMRI study found that maps of spectrotemporal modulation tuning, measured using natural sounds, are coarsely similar between humans and macaques, although slow temporal modulations that are prominent in speech were better decoded in humans compared with macaques (<xref ref-type="bibr" rid="bib18">Erb et al., 2019</xref>), potentially analogous to prior findings of enhanced cochlear frequency tuning for behaviorally relevant sound frequencies (<xref ref-type="bibr" rid="bib6">Bruns and Schmieszek, 1980</xref>; <xref ref-type="bibr" rid="bib24">Koppl et al., 1993</xref>). Thus, prior work has revealed differences in the extent and resolution of neural tuning for different acoustic frequencies and modulation rates.</p><p>Our study demonstrates that human non-primary regions exhibit a form of higher-order acoustic sensitivity that is almost completely absent in ferrets. Ferret cortical responses to natural and spectrotemporally matched synthetic sounds were closely matched throughout their auditory cortex, and the small differences that we observed were scattered throughout primary and non-primary regions (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), unlike the pattern observed in humans. As a consequence, the differences that we observed between natural and synthetic sounds in humans were not predictable from cortical responses in ferrets, even though we could predict responses to synthetic sounds across species (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). This higher-order sensitivity is unlikely to be explained by explicit semantic knowledge about speech or music since similar responses are observed for foreign speech (<xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>) and music sensitivity is robust in listeners without musical training (<xref ref-type="bibr" rid="bib4">Boebinger et al., 2020</xref>). These results suggest that humans develop or have evolved a higher-order stage of acoustic analysis, potentially specific to speech and music, that cannot be explained by standard frequency and modulation statistics and is largely absent from the ferret brain. This specificity for speech and music could be due to their acoustic complexity, their behavioral relevance to humans, or a combination of the two.</p><p>By comparison, our study suggests that there is a substantial amount of cross-species overlap in the cortical representation of frequency and modulation features. Both humans and ferrets exhibited tonotopically organized tuning for different audio frequencies. Like humans, ferrets showed spatially organized sensitivity for different temporal and spectral modulation rates that coarsely mimicked the types of tuning we have previously observed in humans, replicating prior findings (<xref ref-type="bibr" rid="bib18">Erb et al., 2019</xref>). And this tuning was sufficiently similar that we could quantitatively predict response patterns to the synthetic sounds across species (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). These results do not imply that frequency and modulation tuning is the same across species, but do suggest that the organization is similar.</p><p>Our results also do not imply that ferrets lack higher-order acoustic representations. Indeed, we found that one ferret’s spontaneous movements robustly discriminated between natural and synthetic ferret vocalizations, demonstrating behavioral sensitivity to the features that distinguish these sound sets. But how species-relevant higher-order features are represented is likely distinct between humans and ferrets. Consistent with this idea, we found that differences between natural and synthetic sounds are weak, distributed throughout primary and non-primary regions, and show a mix of enhanced and suppressive responses (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), unlike the strong response enhancements we observed for natural speech and music in distinct regions of human non-primary auditory cortex.</p><p>The species differences we observed are unlikely to be driven by differences in the method used to record brain responses (fUS vs. fMRI) for several reasons. First, both methods detect changes in neural responses driven by hemodynamic activity. Second, the denoised fUS responses were both more reliable and more spatially precise than our previously analyzed fMRI voxels. Higher SNR and spatial precision should make it easier, not harder, to detect response differences between sounds, like the natural and synthetic sounds tested here. Third, all of our measures were noise-corrected and thus any residual differences in SNR between species or brain regions should have minimal effect on our measures. Fourth, human non-primary regions show a strong response preference for natural vs. synthetic sounds that is absent in ferrets, and there is no reason why methodological differences should produce a greater response to one set of sounds over another in a specific anatomical region of one species. Fifth, ferrets’ cortical responses show clear selectivity for standard frequency and modulation features of sound, and this selectivity is qualitatively similar to that observed in humans. Sixth, the differences we observed between humans and ferrets are not subtle: humans show a substantial change across their auditory cortex in sensitivity for natural vs. synthetic sounds while ferrets show no detectable change across their auditory cortex. We quantified this change by measuring the slope of the NSE-vs.-distance curve and found that the slopes in ferrets were close to zero and differed substantially from every human subject tested.</p><p>A recent study also found evidence for a species difference in auditory cortical organization by comparing responses to tone and noise stimuli between humans and macaques (<xref ref-type="bibr" rid="bib37">Norman-Haignere et al., 2019</xref>). This study found that preferential responses to tones vs. noise were larger in both primary and non-primary regions of human auditory cortex compared with macaques, which might reflect the importance of speech and music in humans where harmonic structure plays a central role. Our findings are unlikely to reflect greater tone sensitivity in humans because the differences that we observed between natural and synthetic sounds were not limited to tone-selective regions. Here, we tested a mucher wider range of natural and synthetic sounds that differ on many different ecologically relevant dimensions and we could thus compare the overall functional organization between humans and ferrets. As a consequence, we were able to identify a substantial divergence in neural representations at a specific point in the cortical hierarchy.</p></sec><sec id="s3-2"><title>Methodological advances</title><p>Our findings were enabled by a recently developed synthesis method that makes it possible to synthesize sounds with frequency and modulation statistics that are closely matched to those in natural sounds (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). Because the synthetics are otherwise unconstrained, they lack higher-order acoustic properties present in complex natural sounds like speech and music (e.g., syllabic structure; musical notes, harmonies, and rhythms). Comparing neural responses to natural and synthetic sounds thus provides a way to isolate responses to higher-order properties of natural stimuli that cannot be accounted for by modulation statistics. This methodological advance was critical to differentiating human and ferret cortical responses. Indeed, when considering natural or synthetic sounds alone, we observed similar responses between species. We even observed preferential responses to speech compared with other natural sounds in the ferret auditory cortex due to the fact that speech has a unique range of spectrotemporal modulations. Thus, if we had only tested natural sounds, we might have concluded that speech-sensitive responses in the human non-primary auditory cortex reflect the same types of acoustic representations present in ferrets.</p><p>Our study illustrates the utility of wide-field imaging methods in comparing the brain organization of different species (<xref ref-type="bibr" rid="bib3">Bimbard et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Milham et al., 2018</xref>). Most animal physiology studies focus on measuring responses from single neurons or small clusters of neurons in a single brain region. While this approach is essential to understanding the neural code at a fine grain, studying a single brain region can obscure larger-scale trends that are evident across the cortex. Indeed, if we had only measured responses in a single region of auditory cortex, we would have missed the most striking difference between humans and ferrets: the emergence of preferential responses to natural sounds in non-primary regions of humans but not ferrets (<xref ref-type="fig" rid="fig2">Figure 2E</xref>).</p><p>fUS imaging provides a powerful way of studying large-scale functional organization in small animals such as ferrets since it has better spatial resolution than fMRI (<xref ref-type="bibr" rid="bib27">Macé et al., 2011</xref>; <xref ref-type="bibr" rid="bib3">Bimbard et al., 2018</xref>). Because fUS responses are noisy, prior studies, including those from our lab, have only been able to characterize responses to a single stimulus dimension, such as frequency, typically using a small stimulus set (<xref ref-type="bibr" rid="bib19">Gesnik et al., 2017</xref>; <xref ref-type="bibr" rid="bib3">Bimbard et al., 2018</xref>). Here, we developed a denoising method that made it possible to measure highly reliable responses to over a hundred stimuli in a single experiment. We were able to recover at least as many response dimensions as those detectable with fMRI in humans, and those response dimensions exhibited sensitivity for a wide range of frequencies and modulation rates. Our study thus pushes the limits of what is possible using ultrasound imaging and establishes fUS as an ideal method for studying the large-scale functional organization of the animal brain.</p></sec><sec id="s3-3"><title>Assumptions and limitations</title><p>The natural and synthetic sounds we tested were closely matched in their time-averaged cochlear frequency and modulation statistics, measured using a standard model of cochlear and cortical modulation tuning (<xref ref-type="bibr" rid="bib7">Chi et al., 2005</xref>; <xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). We focused on time-averaged statistics because fMRI and fUS reflect time-averaged measures of neural activity due to the temporally slow nature of hemodynamic responses. Thus, a similar response to natural and synthetic sounds indicates that the statistics being matched are sufficient to explain the voxel response. By contrast, a divergent voxel response indicates that the voxel responds to features of sound that are not captured by the model.</p><p>While divergent responses by themselves do not demonstrate a higher-order response, there are several reasons to think that the sensitivity we observed in human non-primary regions is due to higher-order tuning. First, the fact that differences between natural and synthetic speech/music were much larger in non-primary regions suggests that these differences are driven by higher-order processing above and beyond that present in primary auditory cortex, where spectrotemporal modulations appear to explain much of the voxel response. Second, the natural and synthetic sounds produced by our synthesis procedure are in practice closely matched on a wide variety of spectrotemporal filterbank models (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). As a consequence, highly divergent responses to natural and synthetic sounds rule out many such models. Third, the fact that responses were consistently larger for natural speech/music vs. synthetic speech/music suggests that these non-primary regions respond preferentially to features in natural sounds that are not explicitly captured by spectrotemporal modulations and are thus absent from the synthetic sounds.</p><p>Our findings show that a prominent signature of hierarchical functional organization present in humans – preferential responses for natural vs. spectrotemporal structure – is largely absent in ferret auditory cortex. But this finding does not imply that there is no functional differentiation between primary and non-primary regions in ferrets. For example, ferret non-primary regions show longer latencies, greater spectral integration bandwidths, and stronger task-modulated responses compared with primary regions (<xref ref-type="bibr" rid="bib16">Elgueda et al., 2019</xref>). The fact that we did not observe differences between primary and non-primary regions is not because the acoustic features manipulated are irrelevant to ferret auditory cortex, since our analysis shows that matching frequency and modulation statistics is sufficient to match the ferret cortical response, at least as measured by ultrasound. Indeed, if anything, it appears that modulation features are more relevant to the ferret auditory cortex since these features appear to drive responses throughout primary and non-primary regions, unlike human auditory cortex where we only observed strong, matched responses in primary regions.</p><p>As with any study, our conclusions are limited by the precision and coverage of our neural measurements. For example, fine-grained temporal codes, which have been suggested to play an important role in vocalization coding (<xref ref-type="bibr" rid="bib45">Schnupp et al., 2006</xref>), cannot be detected with fUS. However, we note that the resolution of fUS is substantially better than fMRI, particularly in the spatial dimension and thus the species differences we observed are unlikely to be explained by differences in the resolution of fUS vs. fMRI. It is also possible that ferrets might show more prominent differences between natural and synthetic sounds outside of auditory cortex. But even if this were true, it would still demonstrate a clear species difference because humans show robust sensitivity for natural sounds in non-primary regions just outside of primary auditory cortex, while ferrets apparently do not.</p></sec><sec id="s3-4"><title>Possible nature and causes of differences in higher-order sensitivity</title><p>What features might non-primary human auditory cortex represent, given that spectrotemporal modulations fail to explain much of the response? Although these regions respond preferentially to speech and music, they are not driven by semantic meaning or explicit musical training (<xref ref-type="bibr" rid="bib39">Overath et al., 2015</xref>; <xref ref-type="bibr" rid="bib4">Boebinger et al., 2020</xref>), are located just beyond primary auditory cortex, and show evidence of having short integration windows on the scale of hundreds of milliseconds (<xref ref-type="bibr" rid="bib39">Overath et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Norman-Haignere et al., 2020</xref>). This pattern suggests nonlinear sensitivity for short-term temporal and spectral structure present in speech syllables or musical notes (e.g., harmonic structure, pitch contours, and local periodicity). This hypothesis is consistent with recent work showing sensitivity to phonotactics in non-primary regions of the superior temporal gyrus (<xref ref-type="bibr" rid="bib26">Leonard et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Brodbeck et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Di Liberto et al., 2019</xref>), and with a recent study showing that deep neural networks trained to perform challenging speech and music tasks are better able to predict responses in non-primary regions of human auditory cortex (<xref ref-type="bibr" rid="bib23">Kell et al., 2018</xref>).</p><p>Why don’t we observe similar neural sensitivity in ferrets for vocalizations? Ferret vocalizations exhibit additional structure not captured by spectrotemporal modulations since at least one ferret was able to detect the difference between natural and synthetic sounds. However, this additional structure may play a less-essential role in their everyday hearing compared with that of speech and music in humans. Other animals that depend more on higher-order acoustic representations might show more human-like sensitivity in non-primary regions. For example, marmosets have a relatively complex vocal repertoire (<xref ref-type="bibr" rid="bib1">Agamaite et al., 2015</xref>) and depend more heavily on vocalizations than many other species (<xref ref-type="bibr" rid="bib17">Eliades and Miller, 2017</xref>), and thus might exhibit more prominent sensitivity for higher-order properties in their calls. It may also be possible to experimentally enhance sensitivity for higher-order properties via extensive exposure and training, particularly at an early age of development (<xref ref-type="bibr" rid="bib43">Polley et al., 2006</xref>; <xref ref-type="bibr" rid="bib47">Srihasam et al., 2014</xref>). All of these questions could be addressed in future work using the methods developed here.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animal preparation</title><p>Experiments were performed in three head-fixed awake ferrets (A, T, and C), across one or both hemispheres (study 1: A<sub>left</sub>, A<sub>right</sub>, T<sub>left</sub>, T<sub>right</sub>; study 2: A<sub>left</sub>, T<sub>left</sub>, T<sub>right</sub>, C<sub>left</sub>). Ferrets A and C were mothers (had one litter of pups), while ferret T was a virgin. Experiments were approved by the French Ministry of Agriculture (protocol authorization: 21022) and strictly comply with the European directives on the protection of animals used for scientific purposes (2010/63/EU). Animal preparation and fUS imaging were performed as in <xref ref-type="bibr" rid="bib3">Bimbard et al., 2018</xref>. Briefly, a metal headpost was surgically implanted on the skull under anesthesia. After recovery from surgery, a craniotomy was performed over auditory cortex and then sealed with an ultrasound-transparent Polymethylpentene (TPX) cover, embedded in an implant of dental cement. Animals could then recover for 1 week, with unrestricted access to food, water, and environmental enrichment. Imaging windows were maintained across weeks with appropriate interventions when tissue and bone regrowth were shadowing brain areas of interest.</p><p>Ultrasound imaging fUS data are collected as a series of 2D images or ‘slices.’ Slices were collected in the coronal plane and were spaced 0.4 mm apart. The slice plane was varied across sessions to cover the ROI, which included both primary and non-primary regions of auditory cortex. We did not collect data from non-auditory regions due to limited time/coverage. One or two sessions were performed on each day of recording. The resolution of each voxel was 0.1 × 0.1 × ~0.4 mm (the latter dimension, called elevation, being slightly dependent on the depth of the voxel). The overall voxel volume (0.004 mm<sup>3</sup>) was more than a thousand times smaller than the voxel volume used in our human study (which was either 8 or 17.64 mm<sup>3</sup> depending on the subjects/paradigm), which helps to account for their smaller brain.</p><p>A separate ‘power Doppler’ image/slice was acquired every second. Each of these images was computed by first collecting 300 sub-images or ‘frames’ in a short 600 ms time interval (500 Hz sampling rate). Those 300 frames were then filtered to discard global tissue motion from the signal (<xref ref-type="bibr" rid="bib11">Demené et al., 2015</xref>) (the first 55 principal components (PCs) were discarded because they mainly reflect motion; see <xref ref-type="bibr" rid="bib11">Demené et al., 2015</xref> for details). The blood signal energy, also known as power Doppler, was computed for each voxel by summing the squared magnitudes across the 300 frames separately for each pixel (<xref ref-type="bibr" rid="bib27">Macé et al., 2011</xref>). Power Doppler is approximately proportional to blood volume (<xref ref-type="bibr" rid="bib27">Macé et al., 2011</xref>).</p><p>Each of the 300 frames was itself computed from 11 tilted plane wave emissions (–10° to 10° with 2° steps) fired at a pulse repetition frequency of 5500 Hz. Frames were reconstructed from these plane wave emissions using an in-house, GPU-parallelized delay-and-sum beamforming algorithm (<xref ref-type="bibr" rid="bib27">Macé et al., 2011</xref>).</p></sec><sec id="s4-2"><title>Stimuli for experiment I</title><p>We tested 40 natural sounds: 36 sounds from our prior experiment plus 4 ferret vocalizations (fight call, pup call, fear vocalization, and play call). Each natural sound was 10 s in duration. For each natural sound, we synthesized four synthetic sounds, matched on a different set of acoustic statistics of increasing complexity: cochlear, temporal modulation, spectral modulation, and spectrotemporal modulation. The modulation-matched synthetics were also matched in their cochlear statistics to ensure that differences between cochlear and modulation-matched sounds must be due to the addition of modulation statistics. The natural and synthetic sounds were identical to those in our prior paper, except for the four additional ferret vocalizations, which were synthesized using the same algorithm. We briefly review the algorithm below.</p><p>Cochlear statistics were measured from a cochleagram representation of sound, computed by convolving the sound waveform with filters designed to mimic the pseudo-logarithmic frequency resolution of cochlear responses (<xref ref-type="bibr" rid="bib28">McDermott and Simoncelli, 2011</xref>). The cochleagram for each sound was composed of the compressed envelopes of these filter responses (compression is designed to mimic the effects of cochlear amplification at low sound levels). Modulation statistics were measured from filtered cochleagrams, computed by convolving each cochleagram in time and frequency with a filter designed to highlight modulations at a particular temporal rate and/or spectral scale (<xref ref-type="bibr" rid="bib7">Chi et al., 2005</xref>). The temporal and spectral modulation filters were only modulated in time or frequency, respectively. There were nine temporal filters (best rates: 0.5, 1, 2, 4, 8, 16, 32, 64, and 128 Hz) and six spectral filters (best scales: 0.25, 0.5, 1, 2, 4, and 8 cycles per octave). Spectrotemporal filters were created by taking the outer product of all pairs of temporal and spectral filters in the 2D Fourier domain, which results in oriented, gabor-like filters.</p><p>Our synthesis algorithm matches time-averaged statistics of the cochleagrams and filtered cochleagrams via a histogram-matching procedure that implicitly matches all time-averaged statistics of the responses (separately for each frequency channel of the cochleagrams and filtered cochleagrams). This choice is motivated by the fact that both fMRI and fUS reflect time-averaged measures of neural activity because the temporal resolution of hemodynamic changes is much slower than the underlying neuronal activity. As a consequence, if the fMRI or fUS response is driven by a particular set of acoustic features, we would expect two sounds with similar time-averaged statistics for those features to yield a similar response. We can therefore think of the natural and synthetic sounds as being matched under a particular model of the fMRI or fUS response (a formal derivation of this idea is given in <xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>).</p><p>We note that the filters used to compute the cochleagram were designed to match the frequency resolution of the human cochlea, which is thought to be somewhat finer than the frequency resolution of the ferret cochlea (<xref ref-type="bibr" rid="bib50">Walker et al., 2019</xref>). In general, synthesizing sounds from broader filters results in synthetics that differ slightly more from the originals. And thus if we had used cochlear filters designed to mimic the frequency tuning of the ferret cochlea, we would expect the cochlear-matched synthetic sounds to differ slightly more from the natural sounds. However, given that we already observed highly divergent responses to natural and cochlear-matched synthetic sounds in both species, it is unlikely that using broader cochlear filters would change our findings. In general, we have found that the matching procedure is not highly sensitive to the details of the filters used. For example, we have found that sounds matched on the spectrotemporal filters used here and taken from <xref ref-type="bibr" rid="bib7">Chi et al., 2005</xref> are also well matched on filters with half the bandwidth, with phases that have been randomized, and with completely random filters (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>).</p></sec><sec id="s4-3"><title>Stimuli for experiment II</title><p>Experiment II tested a larger set of 30 ferret vocalizations (5 fight calls, 17 single-pup calls, and 8 multi-pup calls where the calls from different pups overlapped in time). The vocalizations consisted of recordings from several labs (our own, Stephen David’s and Andrew King’s laboratories). For comparison, we also tested 14 speech sounds and 16 music sounds, yielding 60 natural sounds in total. For each natural sound, we created a synthetic sound matched on the full spectrotemporal model. We did not synthesize sounds for the sub-models (cochlear, temporal modulation, and spectral modulation) since our goal was to test if there were divergent responses to natural and synthetic ferret vocalizations for spectrotemporally matched sounds, like those present in human non-primary auditory cortex for speech and music sounds.</p></sec><sec id="s4-4"><title>Procedure for presenting stimuli and measuring voxel responses</title><p>Sounds were played through calibrated earphones (Sennheiser IE800 earphones, HDVA 600 amplifier, 65 dB) while recording hemodynamic responses via fUS imaging. In our prior fMRI experiments in humans, we had to chop the 10 s stimuli into 2 s excerpts to present the sounds in between scan acquisitions because MRI acquisitions produce a loud sound that would otherwise interfere with hearing the stimuli. Because fUS imaging produces no audible noise, we were able to present the entire 10 s sound without interruption. The experiment was composed of a series of 20 s trials, and fUS acquisitions were synchronized to trial onset. On each trial, a single 10 s sound was played, with 7 s of silence before the sound to establish a response baseline, and 3 s of post-stimulus silence to allow the response to return to baseline. There was a randomly chosen 3–5 s gap between each trial. Sounds were presented in random order, and each sound was repeated four times.</p><p>Like fMRI, the response timecourse of each fUS voxel shows a gradual build-up of activity after a stimulus due to the slow and gradual nature of blood flow changes. The shape of this response timecourse is similar across different sounds, but the magnitude varies (<xref ref-type="fig" rid="fig1">Figure 1C</xref>) (fMRI responses show the same pattern). We therefore measured the response magnitude of each voxel by averaging the response to each sound across time (from 3 to 11 s post-stimulus onset; results were robust to the particular time window used), yielding one number per sound. Before this step, we normalized responses by the prestimulus baseline for each voxel in order to account for differences in voxel perfusion levels. Specifically, we removed the mean baseline signal for each trial and then divided by the mean baseline signal across the whole session. Responses were measured from denoised data. We describe the denoising procedure at the end of Materials and methods because it is more involved than our other analyses.</p></sec><sec id="s4-5"><title>Procedure for presenting stimuli in humans</title><p>The human data collection procedures have been described in detail previously (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). Here, we give a brief overview, noting aspects of the design that are relevant to understanding the analyses.</p><p>Stimuli were presented using two slightly different paradigms. In paradigm I, we presented all four synthesis conditions in six subjects and three synthesis conditions in the other six subjects (the spectral modulation condition was missing). The natural sounds were presented twice per scan, but the synthetic sounds were only presented once to fit all of the stimuli into a single 2 hr scan. In paradigm II, we just tested the natural and fully matched synthetic sounds, which allowed us to repeat both sets of sounds 3–4 times per scan. Four subjects in paradigm I were scanned multiple times so that we could more reliably measure responses from their individual brain (three subjects completed five scans, one subject completed three scans). Five subjects were scanned in paradigm II (one subject was scanned in both paradigms), and all were scanned multiple times (one subject completed four scans, two subjects completed three scans, and one subject completed two scans).</p><p>fMRI scan acquisitions produce a loud noise due to rapid gradient switching. To prevent these noises from interfering with subjects’ ability to hear the sounds, we used a ‘sparse’ scanning paradigm (<xref ref-type="bibr" rid="bib20">Hall et al., 1999</xref>) that alternated between presenting sounds and acquiring scans. This was achieved by dividing each 10 s stimulus into five 2 s segments (windowed with 25 ms linear ramps). These five segments were presented sequentially with a single scan acquired after each segment. Each scan acquisition lasted 1 s in paradigm I and 1.05 s in paradigm II. There was a 200 ms buffer of silence before and after each acquisition. The total duration of each five-segment block was 17 s in paradigm I and 17.25 s in paradigm II. We averaged the responses of the second through fifth acquisitions after the onset of each stimulus block. The first acquisition was discarded to account for the hemodynamic delay.</p></sec><sec id="s4-6"><title>Mapping of tonotopic organization with pure tones</title><p>Tonotopic organization was assessed using previously described methods (<xref ref-type="bibr" rid="bib3">Bimbard et al., 2018</xref>). In short, responses were measured to 2 s long pure tones from five different frequencies (602 Hz, 1430 Hz, 3400 Hz, 8087 Hz, 19,234 Hz). The tones were played in random order, with 20 trials/frequency. Data were denoised using the same method described in ‘Denoising part I: removing components outside of cortex.’ Tonotopic maps were created by determining the best frequency of each voxel, defined as the tone evoking the largest power Doppler response. Voxel responses were measured as the average response between 3 and 5 s after tone onset. We used a shorter window because the stimuli were shorter (2 s vs. 10 s). We then used these functional landmarks in combination with brain and vascular anatomy to establish the borders between primary and non-primary areas in all hemispheres, as well as to compare them to those obtained with natural sounds (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>).</p></sec><sec id="s4-7"><title>Brain map display</title><p>Views from above were obtained by computing the average of the variable of interest in each vertical column of voxels from the upper part of the manually defined cortical mask. All of our measures were similar across depth (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref> for examples). We note that having a three-dimensional dataset was important to measuring responses from throughout the highly folded cortical ribbon.</p></sec><sec id="s4-8"><title>Spatial correlation analysis</title><p>We compared the precision and reliability of the fUS and fMRI data by measuring the correlation between all pairs of voxels and binning the results based on their distance (<xref ref-type="fig" rid="fig1">Figure 1D</xref> plots the mean correlation within each bin; ferret bin size was 0.5 mm; human bin size was 3 mm). The correlation was computed across two independent measurements of each voxel’s response (odd vs. even repetitions). As a measure of spatial precision, we computed the distance needed for the correlation to decay by 75%:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>75</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>0.75</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf3"><mml:mi>f</mml:mi><mml:mo>(</mml:mo><mml:mo>.</mml:mo><mml:mo>)</mml:mo></mml:math></inline-formula> is the correlation vs. distance function and <inline-formula><mml:math id="inf4"><mml:mi>τ</mml:mi></mml:math></inline-formula> is the 75% decay rate, computed by solving the above equation (via linear interpolation). The human data showed an above 0 correlation at very long distances, suggesting that there is a shared response pattern present across all voxels. To prevent this baseline difference from affecting the decay rate, we first normalized the correlation by subtracting the minimum correlation across all distances before applying the above equation. We statistically compared the reliability (0 mm correlation) and 75% decay rate of the spatial correlation function across species using a Wilcoxon rank-sum test across subjects.</p></sec><sec id="s4-9"><title>NSE maps</title><p>We compared the response magnitude to natural and corresponding synthetic sounds using the NSE, the same metric used in humans. The NSE is defined as<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <bold>x</bold> and <bold>y</bold> are response vectors across the sounds being compared (i.e., natural and synthetic). The squares in the above equation indicate that each element of the vector is being squared. μ(.) indicates the mean across all elements in the vector.</p><p>The NSE takes a value of 0 if the response to natural and synthetic sounds is identical and 1 if there is no correspondence between responses to natural and synthetic sounds (i.e., they are independent). For anticorrelated signals, the NSE can exceed 1 with a maximum value of 2 for signals that are zero-mean and perfectly anticorrelated. This is analogous to the correlation coefficient, which has a maximum value of 1 for identical signals, a minimum value of –1 for anticorrelated signals, and a value of 0 for independent signals.</p><p>Unlike the correlation coefficient, the NSE is sensitive to differences in the mean and scale of the responses being compared, in addition to differences in the response pattern. This property is useful because the model predicts that the responses to natural and synthetic sounds should be matched (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>), and thus any divergence in the response to natural vs. synthetic sounds reflects a model failure, regardless of whether that divergence is driven by the pattern, mean, or scale of the response. In ferrets, we observed NSE values near 0 throughout ferret auditory cortex, indicating that responses are approximately matched in all respects. In contrast, humans showed large NSE values in non-primary auditory cortex, which could in principle be driven by differences in the mean, scale, or response pattern. In our prior work, we showed that these high NSE values are primarily driven by stronger responses to natural vs. synthetic sounds, which manifests as a downward scaling of the response to synthetic sounds. The stronger responses to natural sounds are presumably driven by sensitivity to higher-order structure that is absent from the synthetic sounds.</p><p>We noise-corrected the NSE to prevent differences in SNR from affecting our estimates, although we note that the denoised responses were highly reliable and thus correction had relatively little effect on the measured values. We used a noise-correction procedure that we previously derived and validated in simulations (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). Here, we give a brief description of the method. As is evident in the equation below (an expanded version of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), the NSE can be written as a function of three statistics: (1) the power (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>); (2) mean (<inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>); and (3) cross-product (<inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∘</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) of the responses being compared.<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mfrac><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>∘</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi>u</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The means and cross-products are unbiased by noise as long as the noise is zero-mean, which is a trivial assumption (e.g., if we define the noise-free signal as the average response in the limit of infinite measurements, then the noise around this average is by definition zero-mean). The response power however is biased upward by noise. We can estimate the magnitude of this upward bias by calculating the power of the residual error between two independent measurements of the response (i.e., two different repetitions of the same stimuli), which is equal to twice the noise power in expectation. By subtracting off half the residual power, we can thus noise-correct our power estimates:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>μ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where, for example, <inline-formula><mml:math id="inf8"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are two independent measurements of the response to natural sounds and <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are two independent measurements of the response to synthetic sounds.</p><p>We only analyzed voxels that had a test–retest NSE less than 0.4, which we previously found in simulations was sufficient to achieve reliable noise-corrected measures (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). Most voxels in auditory cortex passed this threshold since the denoised voxel responses were highly reliable.</p></sec><sec id="s4-10"><title>Annular ROI analyses</title><p>We used the same annular ROI analyses from our prior paper to quantify the change in NSE values (or lack thereof) across the cortex. We binned voxels based on their distance to the center of primary auditory cortex, defined tonotopically. We used smaller bin sizes in ferrets (0.5 mm) than humans (5 mm) due to their smaller brains (results were not sensitive to the choice of bin size). <xref ref-type="fig" rid="fig2">Figure 2F</xref> plots the median NSE value in each bin, plotted separately for each human and ferret subject. To statistically compare different models (e.g., cochlear vs. spectrotemporal), for each animal, we computed the median NSE value across all voxels separately for each model, bootstrapped the resulting statistics by resampling across the sound set (1000 times), and counted the fraction of samples that overlapped between models (multiplying by 2 to arrive at a two-sided p-value). To compare species, we measured the slope of the NSE vs. distance curve for the fully matched synthetic sounds separately for each human and ferret subject. We then compared each ferret slope to the distribution of human slopes using a sign test to evaluate if that individual ferret differed significantly from the human population.</p></sec><sec id="s4-11"><title>Human analyses</title><p>The details of the human analyses very were similar to those in our prior paper (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>). To make the human and ferret analyses more similar, we used component-denoised fMRI data. Results were similar without denoising (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). Denoising was accomplished by projecting the response of each voxel of each subject onto the six reliable components inferred in our prior studies (see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>; <xref ref-type="bibr" rid="bib35">Norman-Haignere et al., 2015</xref>; <xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>).</p><p>Whole-brain NSE maps are based on data for paradigm I and were computed by simply averaging responses across voxels in standardized coordinates (FsAverage template brain distributed by Freesurfer) and applying our NSE measures to these group averaged responses. For individual subject analyses, we used all of the available data for a given condition and the number of subjects is indicated in all relevant plots. Unlike in our prior study, we were able to get reliable NSE estimates from individual subjects with just a single scan of data because of our denoising procedure. Note that some subjects were not included in the annular ROI analyses because we did not have tonotopy data for them and thus could not functionally identify the center of PAC. When using data for paradigm I, we used just the natural sounds to estimate the noise power and correct our NSE measures since only those were presented multiple times in each scan (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>) (note that we have no reason to expect fMRI noise to differ across stimuli).</p></sec><sec id="s4-12"><title>Component analyses</title><p>To investigate the organization of fUS responses to the sound set, we applied the same voxel decomposition used in our prior work in humans to identify a small number of component response patterns that explained a large fraction of the response variance. Like all factorization methods, each voxel is modeled as the weighted sum of a set of canonical response patterns that are shared across voxels. The decomposition algorithm is similar to standard algorithms for independent component analysis (ICA) in that it identifies components that have a non-Gaussian distribution of weights across voxels by minimizing the entropy of the weights (the Gaussian distribution has the highest entropy of any distribution with fixed variance). This optimization criterion is motivated by the fact that independent variables become more Gaussian when they are linearly mixed, and non-Gaussianity thus provides a statistical signature that can be used to unmix the latent variables. Our algorithm differs from standard algorithms for ICA in that it estimates entropy using a histogram, which is effective if there are many voxels, as is the case with fMRI and fUS (40,882 fUS voxels for experiment I, 38,366 fUS voxels for experiment II).</p><p>We applied our analyses to the denoised response timecourse of each voxel across all sounds (each column of the data matrix contained the concatenated response timecourse of one voxel across all sounds). Our main analysis was performed on voxels concatenated across both animals tested. The results however were similar when the analysis was performed on data from each animal separately. The number of components was determined via a cross-validation procedure described in the section on denoising.</p><p>We examined the inferred components by plotting and comparing their response profiles to the natural and synthetic sounds, as well as plotting their anatomical weights in the brain. We also correlated the response profiles across all sounds with measures of cochlear and spectrotemporal modulation energy. Cochlear energy was computed by averaging the cochleagram for each sound across time. Spectrotemporal modulation energy was calculated by measuring the strength of modulations in the filtered cochleagrams (which highlight modulations at a particular temporal rate and/or spectral scale). Modulation strength was computed as the standard deviation across time of each frequency channel of the filtered cochleagram. The channel-specific energies were then averaged across frequency, yielding one number per sound and spectrotemporal modulation rate.</p><p>We used a permutation test across the sound set to assess the significance of correlations with frequency and modulation features. Specifically, we measured the maximum correlation across all frequencies and all modulation rates tested, and we compared these values with those from a null distribution computed by permuting the correspondence across sounds between the features and the component responses (1000 permutations). We counted the fraction of samples that overlapped the null distribution and multiplied by 2 to get a two-sided p-value. For every component, we found that correlations with frequency and modulation features were significant (p&lt;0.01).</p><p>We separately analyzed responses from experiments I (<xref ref-type="fig" rid="fig3">Figure 3</xref>) and II (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>) because there was no simple way to combine the data across experiments since the stimuli were distinct and there was no obvious correspondence across voxels since the data were collected from different slices on different days.</p></sec><sec id="s4-13"><title>Predicting human components from ferret responses</title><p>To quantify which component response patterns were shared across species, we tried to linearly predict components across species (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>, <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>). Each component was defined by its average response to the 36 natural and corresponding synthetic sounds, matched on the full spectrotemporal model. We attempted to predict each human component from all of the ferret components and vice versa, using cross-validated ridge regression (9 folds). The ridge parameter was chosen using nested cross-validation within the training set (also 9 folds; testing a very wide range from 2<sup>–100</sup> to 2<sup>100</sup>). Each fold contained pairs of corresponding natural and synthetic sounds so that there would be no dependencies between the train and test sounds (i.e., the natural and synthetic version of a sound could not straddle the train and test set).</p><p>For each component, we separately measured how well we could predict the response to synthetic sounds (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4B</xref>, <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5A</xref>) – which isolates tuning for frequency and modulation statistics present in natural sounds – as well as how well we could predict the difference between responses to natural vs. synthetic sounds (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4C</xref>, <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5B</xref>) – which isolates sensitivity for features in natural sounds that are not explained by frequency and modulation statistics. We quantified prediction accuracy using the NSE and used <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as a measure of explained variance. This choice is motivated by the fact that <inline-formula><mml:math id="inf13"><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is equivalent to the Pearson correlation for signals with equal mean and variance and <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> is therefore analogous to the squared Pearson correlation, which is a standard measure of explained variance. We multiplied these explained variance estimates by the total response variance of each component for either synthetic sounds or for the difference between natural and synthetic sounds (<xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4D,E</xref> and <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5C,D</xref> show the total variance alongside the fraction of that total variance explained by the cross-species prediction).</p><p>When possible, we noise-corrected both the NSE and the total variance to provide the best possible estimate of their true values. Results were similar without correction. We did not noise-correct the NSE when the component responses being predicted were themselves unreliable (test–retest NSE &gt;0.4) since that makes the correction unreliable (<xref ref-type="bibr" rid="bib36">Norman-Haignere et al., 2018</xref>); this occurred, for example, when attempting to predict the natural vs. synthetic differences in ferrets for which there was virtually no reliable variance (see <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5D</xref>).</p><p>We noise-corrected the total variance using the equation below:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are two independent response measurements. Below, we give a brief derivation of this equation, where <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are expressed as the sum of a shared signal (<inline-formula><mml:math id="inf19"><mml:mi>s</mml:mi></mml:math></inline-formula>) that is repeated across measurements plus independent noise (<inline-formula><mml:math id="inf20"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>) which is not. This derivation utilizes the fact that the variance of independent signals that are summed or subtracted is equal to the sum of their respective variances.<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>s</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The two independent measurements used for noise correction were derived from different human or ferret subjects. The measurements were computed by attempting to predict group components from each subject using the same cross-validated regression procedure described above. The two measurements in ferrets came from the two animals tested (A and T). And the two measurements in humans came from averaging the predictions across two non-overlapping sets of subjects (four in each group; groups chosen to have similar SNR).</p><p>For this analysis, the components were normalized so that the RMS magnitude of their weights was equal. As a consequence, components that explained more response variance also had larger response magnitudes. We also adjusted the total variance across all components to equal 1.</p><p>We computed error bars by bootstrapping across sounds (1000 samples). Specifically, we sampled sounds with replacement and then re-computed the NSE and total variance using the sampled sounds. Note that we did not allow squaring to make negative values positive (i.e., in <inline-formula><mml:math id="inf22"><mml:msup><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) since that would bias the distribution.</p></sec><sec id="s4-14"><title>Comparing the similarity of natural and synthetic sounds from different categories</title><p>We computed maps showing the average difference between natural and synthetic sounds from different categories (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). So that the scale of the differences could be compared across species, we divided the measured differences by the standard deviation of each voxel’s response across all sounds. We also separately measured the NSE for individual sounds (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D</xref>) and sound categories (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>). For this analysis, the numerator of the NSE (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) was computed in the normal way by measuring the error between natural and synthetic sounds for the particular sounds/categories of interest. The denominator/normalization term was computed using all sounds to ensure that the normalization was the same for all sounds/categories and thus that we were not inadvertently normalizing away meaningful differences. To statistically compare the categories, we applied a Wilcoxon rank-sum test to the distribution of NSE values across sounds from the categories being compared.</p></sec><sec id="s4-15"><title>Video recording</title><p>We measured the motion of the animal using a video recording of the animal’s face (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A and B</xref>). Specifically, we measured the absolute value of the frame-to-frame deviations in the video and summed these differences across all pixels within an ROI centered on the ferret’s face. We computed evoked movement in a similar way as for fUS signals. Specifically, we removed the mean movement during the baseline for each trial and then divided by the mean baseline movement across the whole session. We computed the average motion evoked by each sound by averaging across recording sessions, separately for each animal. Before averaging, to account for different camera angles across recording sessions, we divided the movement by the standard deviation across sounds in each session. We statistically compared motion between different sound categories using a Wilcoxon rank-sum test across the sounds from each category.</p></sec><sec id="s4-16"><title>Denoising part I: Removing components outside of cortex</title><p>Ultrasound responses in awake animals are noisy, which has limited its usage to mapping simple stimulus dimensions (e.g., frequency) where a single stimulus can be repeated many times (<xref ref-type="bibr" rid="bib3">Bimbard et al., 2018</xref>). To overcome this issue, we developed a denoising procedure that substantially increased the reliability of the voxel responses (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The procedure had two parts. The first part, described in this section, removed prominent signals outside of cortex, which are likely to reflect movement or other sources of noise. The second part enhanced reliable signals. Code implementing the denoising procedures is publicly available (<ext-link ext-link-type="uri" xlink:href="https://github.com/agneslandemard/naturalsounds_analysis">https://github.com/agneslandemard/naturalsounds_analysis</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3d57d695a35922cc9a01768aca6ab229c40b4ab4;origin=https://github.com/agneslandemard/naturalsounds_analysis;visit=swh:1:snp:dadceb4e34d53e58827ee98c3928af1f349a3011;anchor=swh:1:rev:89466e7b5492553d3af314b7d4fff6d059445588">swh:1:rev:89466e7b5492553d3af314b7d4fff6d059445588</ext-link>; <xref ref-type="bibr" rid="bib25">Landemard, 2021</xref>).</p><p>We separated voxels into those inside and outside of cortex since responses outside of the cortex by definition do not contain stimulus-driven cortical responses, but do contain sources of noise like motion. We then used canonical correlation analysis (CCA) to find a set of response timecourses that were robustly present both inside and outside of cortex since such timecourses are both likely to reflect noise and likely to distort the responses of interest (<xref ref-type="bibr" rid="bib9">de Cheveigné et al., 2019</xref>). We projected out the top 20 canonical components (CCs) from the dataset, which we found scrubbed the data of motion-related signals (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>; motion described below).</p><p>This analysis was complicated by one key fact: the animals reliably moved more during the presentation of some sounds (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). Thus, noise-induced activity outside of cortex is likely to be correlated with sound-driven neural responses inside of cortex, and removing CCs will thus remove both noise and genuine sound-driven activity. To overcome this issue, we took advantage of the fact that sound-driven responses will by definition be reliable across repeated presentations of the same sound, while motion-induced activity will vary from trial to trial for the same sound. We thus found CCs where the residual activity after removing trial-averaged responses was shared between responses inside and outside of cortex, and we then removed the contribution of these components from the data. We give a detailed description and motivation of this procedure in Appendix 1 and show the results of a simple simulation demonstrating its efficacy.</p><p>To assess the effect of this procedure on our fUS data, we measured how well it removed signals that were correlated with motion (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A</xref>). Motion was measured using a video recording of the animal’s face. We measured the motion energy in the video as the average absolute deviation across adjacent frames, summed across all pixels. We correlated this motion timecourse with the timecourse of every voxel. <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A</xref> plots the mean absolute correlation value across voxels as a function of the number of CCs removed (motion can induce both increased and decreased fUS signal, and thus it was necessary to take the absolute value of the correlation before averaging). We found that removing the top 20 CCs substantially reduced motion correlations.</p><p>We also found that removing the top 20 CCs removed spatial striping in the voxel responses, which is a stereotyped feature of motion due to the interaction between motion and blood vessels. To illustrate this effect, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3B</xref> shows the average difference between responses to natural vs. synthetic sounds in experiment II (vocalization experiment). Before denoising, this difference map shows a clear striping pattern likely due to the fact that the animals moved more during the presentation of the natural vs. synthetic sounds. The denoising procedure largely eliminated this striping pattern.</p></sec><sec id="s4-17"><title>Denoising part II: Enhancing signal using DSS</title><p>After removing components likely to be driven by noise, we applied a second procedure designed to enhance reliable components in the data. Our procedure is a variant of a method that is often referred to as ‘denoising source separation’ (DSS) or ‘joint decorrelation’ (<xref ref-type="bibr" rid="bib8">de Cheveigné and Parra, 2014</xref>). In contrast with principal component analysis (PCA), which finds components that have high variance, DSS emphasizes components that have high variance after applying a ‘biasing’ operation that is designed to enhance some aspect of the data. The procedure begins by whitening the data such that all response dimensions have equal variance, the biasing operation is applied, and PCA is then used to extract the components with the highest variance after biasing. In our case, we biased the data to enhance response components that were reliable across stimulus repetitions and slices. This procedure was done for each animal independently. We note that unlike fMRI, data from different slices come from different sessions. As a consequence, the noise from different slices will be independent. Thus, any response components that are consistent across slices are likely to reflect true, stimulus-driven responses.</p><p>The input to our analysis was a set of matrices. Each matrix contained data from a single stimulus repetition and slice. Only voxels from inside of cortex were analyzed. Each column of each matrix contained the response timecourse of one voxel to all of the sounds (concatenated), denoised using the procedure described in part I. The response of each voxel was converted to units of percent signal change (the same units used for fMRI analyses) by subtracting and dividing by the pre-stimulus period (also known as percent cerebral blood volume [%CBV] in the fUS literature).</p><p>Our analysis involved five steps:</p><p>1. We whitened each matrix individually.</p><p>2. We averaged the whitened response timecourses across repetitions, thus enhancing responses that are reliable across repetitions.</p><p>3. We concatenated the repetition-averaged matrices for all slices across the voxel dimension, thus boosting signal that is shared across slices.</p><p>4. We extracted the top N principal components (PCs) with the highest variance from the concatenated data matrix. The number of components was selected using cross-validation (described below). Because the matrices for each repetition and slice have been whitened, the PCs extracted in this step will <italic>not</italic> reflect the components with highest variance, but will instead reflect the components that are the most reliable across repetitions and slices. We thus refer to these components as ‘reliable components’ (<inline-formula><mml:math id="inf23"><mml:mi>R</mml:mi></mml:math></inline-formula>).</p><p>5. We then projected the data onto the top N reliable components (<inline-formula><mml:math id="inf24"><mml:mi>R</mml:mi></mml:math></inline-formula>):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>R</mml:mi><mml:msup><mml:mi>R</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mi>D</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf25"><mml:mi>D</mml:mi></mml:math></inline-formula> is the denoised response matrix from part I and + indicates the matrix pseudoinverse.</p><p>We used cross-validation to test the efficacy of this denoising procedure and select the number of components (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The same number of components was selected across animals. This analysis involved the following steps:</p><p>1. We divided the sound set into training (75%) and test (25%) sounds. Each set contained corresponding natural and synthetic sounds so that there would be no overlap between train and test sets. We attempted to balance the train and test sets across categories such that each split had the same number of sounds from each category.</p><p>2. Using responses to just the train sounds (<inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), we computed reliable components (<inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) using the procedure just described (steps 1–4 in the above section).</p><p>3. We calculated voxel weights for these components:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>W</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>4. We used this weight matrix, which was derived entirely from train data, to denoise responses to the test sounds:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>W</mml:mi></mml:mrow></mml:math></disp-formula><disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>To evaluate whether the denoising procedure improved predictions, we measured responses to the test sound set using two independent splits of data (odd or even repetitions). We then correlated the responses across the two splits either before or after denoising.</p><p><xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref> plots the split-half correlation of each voxel before vs. after denoising for every voxel in the cortex (using an eight-component model). For this analysis, we either denoised one split of data (blue dots) or both splits of data (green dots). Denoising one split provides a fairer test of whether the denoising procedure enhances SNR, while denoising both splits demonstrates the overall boost in reliability. We also plot the upper bound on the split-half correlation when denoising one split of data (black line), which is given by the square root of the split-half reliability of the original data. We found that our denoising procedure substantially increased reliability with the denoised correlations remaining close to the upper bound. When denoising both splits, the split-half correlations were near 1, indicating a highly reliable response.</p><p><xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref> plots a map in one animal of the split-half correlations when denoising one split of data along with a map of the upper bound. As is evident, the denoised correlations remain close to the upper bound throughout primary and non-primary auditory cortex.</p><p><xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref> shows the median split-half correlation across voxels as a function of the number of components. Performance was best using approximately eight components in both experiments.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Investigation, Methodology, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Resources, Software</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Methodology, Supervision, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Funding acquisition, Methodology, Project administration, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Experiments were approved by the French Ministry of Agriculture (protocol authorization: 21022) and strictly comply with the European directives on the protection of animals used for scientific purposes (2010/63/EU).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>List of sounds used in both experiments.</title><p>Names of sounds used in experiments I and II, grouped by category at both fine and coarse scales.</p></caption><media mime-subtype="postscript" mimetype="application" xlink:href="elife-65566-supp1-v1.ai"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-65566-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Our data is publicly available on Zenodo at the following link: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5493682">https://doi.org/10.5281/zenodo.5493682</ext-link> We provide ferret fUS data, before and after denoising, as well as additional files necessary to run our analyses. Source code for our denoising procedure and production of main figures is available on <ext-link ext-link-type="uri" xlink:href="https://github.com/agneslandemard/naturalsounds_analysis">https://github.com/agneslandemard/naturalsounds_analysis</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Landemard</surname><given-names>A</given-names></name><name><surname>Bimbard</surname><given-names>C</given-names></name><name><surname>Demené</surname><given-names>C</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Norman-Haigneré</surname><given-names>S</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>fUS imaging of ferret auditory cortex during passive listening of natural sounds</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.4349594</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Sophie Bagur for careful reading of the manuscript and precious comments.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agamaite</surname><given-names>JA</given-names></name><name><surname>Chang</surname><given-names>CJ</given-names></name><name><surname>Osmanski</surname><given-names>MS</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A quantitative acoustic analysis of the vocal repertoire of the common marmoset (Callithrix jacchus)</article-title><source>The Journal of the Acoustical Society of America</source><volume>138</volume><fpage>2906</fpage><lpage>2928</lpage><pub-id pub-id-type="doi">10.1121/1.4934268</pub-id><pub-id pub-id-type="pmid">26627765</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Lafaille</surname><given-names>P</given-names></name><name><surname>Ahad</surname><given-names>P</given-names></name><name><surname>Pike</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Voice-selective areas in human auditory cortex</article-title><source>Nature</source><volume>403</volume><fpage>309</fpage><lpage>312</lpage><pub-id pub-id-type="doi">10.1038/35002078</pub-id><pub-id pub-id-type="pmid">10659849</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bimbard</surname><given-names>C</given-names></name><name><surname>Demene</surname><given-names>C</given-names></name><name><surname>Girard</surname><given-names>C</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Tanter</surname><given-names>M</given-names></name><name><surname>Boubenec</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Multi-scale mapping along the auditory hierarchy using high-resolution functional UltraSound in the awake ferret</article-title><source>eLife</source><volume>7</volume><elocation-id>e35028</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.35028</pub-id><pub-id pub-id-type="pmid">29952750</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Boebinger</surname><given-names>D</given-names></name><name><surname>Norman-Haignere</surname><given-names>S</given-names></name><name><surname>McDermott</surname><given-names>J</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Cortical Music Selectivity Does Not Require Musical Training</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.10.902189</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brodbeck</surname><given-names>C</given-names></name><name><surname>Hong</surname><given-names>LE</given-names></name><name><surname>Simon</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Rapid transformation from auditory to linguistic representations of continuous speech</article-title><source>Current Biology</source><volume>28</volume><fpage>3976</fpage><lpage>3983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.10.042</pub-id><pub-id pub-id-type="pmid">30503620</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruns</surname><given-names>V</given-names></name><name><surname>Schmieszek</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Cochlear innervation in the greater horseshoe bat: demonstration of an acoustic fovea</article-title><source>Hearing Research</source><volume>3</volume><fpage>27</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1016/0378-5955(80)90006-4</pub-id><pub-id pub-id-type="pmid">7400046</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chi</surname><given-names>T</given-names></name><name><surname>Ru</surname><given-names>P</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multiresolution spectrotemporal analysis of complex sounds</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>887</fpage><lpage>906</lpage><pub-id pub-id-type="doi">10.1121/1.1945807</pub-id><pub-id pub-id-type="pmid">16158645</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Joint decorrelation, a versatile tool for multichannel data analysis</article-title><source>NeuroImage</source><volume>98</volume><fpage>487</fpage><lpage>505</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2014.05.068</pub-id><pub-id pub-id-type="pmid">24990357</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Cheveigné</surname><given-names>A</given-names></name><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Arzounian</surname><given-names>D</given-names></name><name><surname>Wong</surname><given-names>DDE</given-names></name><name><surname>Hjortkjær</surname><given-names>J</given-names></name><name><surname>Fuglsang</surname><given-names>S</given-names></name><name><surname>Parra</surname><given-names>LC</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Multiway canonical correlation analysis of brain data</article-title><source>NeuroImage</source><volume>186</volume><fpage>728</fpage><lpage>740</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.11.026</pub-id><pub-id pub-id-type="pmid">30496819</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Heer</surname><given-names>WA</given-names></name><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The hierarchical cortical organization of human speech processing</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6539</fpage><lpage>6557</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3267-16.2017</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demené</surname><given-names>C</given-names></name><name><surname>Deffieux</surname><given-names>T</given-names></name><name><surname>Pernot</surname><given-names>M</given-names></name><name><surname>Osmanski</surname><given-names>B-F</given-names></name><name><surname>Biran</surname><given-names>V</given-names></name><name><surname>Gennisson</surname><given-names>J-L</given-names></name><name><surname>Sieu</surname><given-names>L-A</given-names></name><name><surname>Bergel</surname><given-names>A</given-names></name><name><surname>Franqui</surname><given-names>S</given-names></name><name><surname>Correas</surname><given-names>J-M</given-names></name><name><surname>Cohen</surname><given-names>I</given-names></name><name><surname>Baud</surname><given-names>O</given-names></name><name><surname>Tanter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Spatiotemporal clutter filtering of ultrafast ultrasound data highly increases Doppler and fUltrasound sensitivity</article-title><source>IEEE Transactions on Medical Imaging</source><volume>34</volume><fpage>2271</fpage><lpage>2285</lpage><pub-id pub-id-type="doi">10.1109/TMI.2015.2428634</pub-id><pub-id pub-id-type="pmid">25955583</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>O’Sullivan</surname><given-names>JA</given-names></name><name><surname>Lalor</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Low-Frequency Cortical Entrainment to Speech Reflects Phoneme-Level Processing</article-title><source>Current Biology</source><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname><given-names>GM</given-names></name><name><surname>Wong</surname><given-names>D</given-names></name><name><surname>Melnik</surname><given-names>GA</given-names></name><name><surname>de Cheveigné</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Low-frequency cortical responses to natural speech reflect probabilistic phonotactics</article-title><source>NeuroImage</source><volume>196</volume><fpage>237</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.04.037</pub-id><pub-id pub-id-type="pmid">30991126</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id><pub-id pub-id-type="pmid">17631409</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname><given-names>N</given-names></name><name><surname>Patel</surname><given-names>AD</given-names></name><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Butler</surname><given-names>H</given-names></name><name><surname>Luo</surname><given-names>C</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal modulations in speech and music</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>81</volume><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.02.011</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elgueda</surname><given-names>D</given-names></name><name><surname>Duque</surname><given-names>D</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>State-dependent encoding of sound and behavioral meaning in a tertiary region of the ferret auditory cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>447</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0317-8</pub-id><pub-id pub-id-type="pmid">30692690</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eliades</surname><given-names>SJ</given-names></name><name><surname>Miller</surname><given-names>CT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Marmoset vocal communication: behavior and neurobiology</article-title><source>Developmental Neurobiology</source><volume>77</volume><fpage>286</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1002/dneu.22464</pub-id><pub-id pub-id-type="pmid">27739195</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erb</surname><given-names>J</given-names></name><name><surname>Armendariz</surname><given-names>M</given-names></name><name><surname>De Martino</surname><given-names>F</given-names></name><name><surname>Goebel</surname><given-names>R</given-names></name><name><surname>Vanduffel</surname><given-names>W</given-names></name><name><surname>Formisano</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Homology and specificity of natural sound-encoding in human and monkey auditory cortex</article-title><source>Cerebral Cortex</source><volume>29</volume><fpage>3636</fpage><lpage>3650</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhy243</pub-id><pub-id pub-id-type="pmid">30395192</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gesnik</surname><given-names>M</given-names></name><name><surname>Blaize</surname><given-names>K</given-names></name><name><surname>Deffieux</surname><given-names>T</given-names></name><name><surname>Gennisson</surname><given-names>JL</given-names></name><name><surname>Sahel</surname><given-names>JA</given-names></name><name><surname>Fink</surname><given-names>M</given-names></name><name><surname>Picaud</surname><given-names>S</given-names></name><name><surname>Tanter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>3D functional ultrasound imaging of the cerebral visual system in rodents</article-title><source>NeuroImage</source><volume>149</volume><fpage>267</fpage><lpage>274</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.01.071</pub-id><pub-id pub-id-type="pmid">28167348</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hall</surname><given-names>DA</given-names></name><name><surname>Haggard</surname><given-names>MP</given-names></name><name><surname>Akeroyd</surname><given-names>MA</given-names></name><name><surname>Palmer</surname><given-names>AR</given-names></name><name><surname>Summerfield</surname><given-names>AQ</given-names></name><name><surname>Elliott</surname><given-names>MR</given-names></name><name><surname>Gurney</surname><given-names>EM</given-names></name><name><surname>Bowtell</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>“Sparse” temporal sampling in auditory fMRI</article-title><source>Human Brain Mapping</source><volume>7</volume><fpage>213</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1002/(sici)1097-0193(1999)7:3&lt;213::aid-hbm5&gt;3.0.co;2-n</pub-id><pub-id pub-id-type="pmid">10194620</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hickok</surname><given-names>G</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The cortical organization of speech processing</article-title><source>Nature Reviews. Neuroscience</source><volume>8</volume><fpage>393</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1038/nrn2113</pub-id><pub-id pub-id-type="pmid">17431404</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joris</surname><given-names>PX</given-names></name><name><surname>Bergevin</surname><given-names>C</given-names></name><name><surname>Kalluri</surname><given-names>R</given-names></name><name><surname>Mc Laughlin</surname><given-names>M</given-names></name><name><surname>Michelet</surname><given-names>P</given-names></name><name><surname>van der Heijden</surname><given-names>M</given-names></name><name><surname>Shera</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Frequency selectivity in Old-World monkeys corroborates sharp cochlear tuning in humans</article-title><source>PNAS</source><volume>108</volume><fpage>17516</fpage><lpage>17520</lpage><pub-id pub-id-type="doi">10.1073/pnas.1105867108</pub-id><pub-id pub-id-type="pmid">21987783</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Shook</surname><given-names>EN</given-names></name><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy</article-title><source>Neuron</source><volume>98</volume><fpage>630</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.03.044</pub-id><pub-id pub-id-type="pmid">29681533</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koppl</surname><given-names>C</given-names></name><name><surname>Gleich</surname><given-names>O</given-names></name><name><surname>Manley</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>An auditory fovea in the barn owl cochlea</article-title><source>Journal of Comparative Physiology A</source><volume>171</volume><fpage>695</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1007/BF00213066</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Landemard</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>naturalsounds_analysis</data-title><version designator="swh:1:dir:3d57d695a35922cc9a01768aca6ab229c40b4ab4">swh:1:dir:3d57d695a35922cc9a01768aca6ab229c40b4ab4</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:3d57d695a35922cc9a01768aca6ab229c40b4ab4;origin=https://github.com/agneslandemard/naturalsounds_analysis;visit=swh:1:snp:dadceb4e34d53e58827ee98c3928af1f349a3011;anchor=swh:1:rev:89466e7b5492553d3af314b7d4fff6d059445588">https://archive.softwareheritage.org/swh:1:dir:3d57d695a35922cc9a01768aca6ab229c40b4ab4;origin=https://github.com/agneslandemard/naturalsounds_analysis;visit=swh:1:snp:dadceb4e34d53e58827ee98c3928af1f349a3011;anchor=swh:1:rev:89466e7b5492553d3af314b7d4fff6d059445588</ext-link></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonard</surname><given-names>MK</given-names></name><name><surname>Bouchard</surname><given-names>KE</given-names></name><name><surname>Tang</surname><given-names>C</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamic encoding of speech sequence probability in human temporal cortex</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>7203</fpage><lpage>7214</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4100-14.2015</pub-id><pub-id pub-id-type="pmid">25948269</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macé</surname><given-names>E</given-names></name><name><surname>Montaldo</surname><given-names>G</given-names></name><name><surname>Cohen</surname><given-names>I</given-names></name><name><surname>Baulac</surname><given-names>M</given-names></name><name><surname>Fink</surname><given-names>M</given-names></name><name><surname>Tanter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional ultrasound imaging of the brain</article-title><source>Nature Methods</source><volume>8</volume><fpage>662</fpage><lpage>664</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1641</pub-id><pub-id pub-id-type="pmid">21725300</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Sound texture perception via statistics of the auditory periphery: evidence from sound synthesis</article-title><source>Neuron</source><volume>71</volume><fpage>926</fpage><lpage>940</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.06.032</pub-id><pub-id pub-id-type="pmid">21903084</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Phoneme representation and classification in primary auditory cortex</article-title><source>The Journal of the Acoustical Society of America</source><volume>123</volume><fpage>899</fpage><lpage>909</lpage><pub-id pub-id-type="doi">10.1121/1.2816572</pub-id><pub-id pub-id-type="pmid">18247893</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mesgarani</surname><given-names>N</given-names></name><name><surname>Cheung</surname><given-names>C</given-names></name><name><surname>Johnson</surname><given-names>K</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Phonetic feature encoding in human superior temporal gyrus</article-title><source>Science</source><volume>343</volume><fpage>1006</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1126/science.1245994</pub-id><pub-id pub-id-type="pmid">24482117</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milham</surname><given-names>MP</given-names></name><name><surname>Ai</surname><given-names>L</given-names></name><name><surname>Koo</surname><given-names>B</given-names></name><name><surname>Xu</surname><given-names>T</given-names></name><name><surname>Amiez</surname><given-names>C</given-names></name><name><surname>Balezeau</surname><given-names>F</given-names></name><name><surname>Baxter</surname><given-names>MG</given-names></name><name><surname>Blezer</surname><given-names>ELA</given-names></name><name><surname>Brochier</surname><given-names>T</given-names></name><name><surname>Chen</surname><given-names>A</given-names></name><name><surname>Croxson</surname><given-names>PL</given-names></name><name><surname>Damatac</surname><given-names>CG</given-names></name><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Everling</surname><given-names>S</given-names></name><name><surname>Fair</surname><given-names>DA</given-names></name><name><surname>Fleysher</surname><given-names>L</given-names></name><name><surname>Freiwald</surname><given-names>W</given-names></name><name><surname>Froudist-Walsh</surname><given-names>S</given-names></name><name><surname>Griffiths</surname><given-names>TD</given-names></name><name><surname>Guedj</surname><given-names>C</given-names></name><name><surname>Hadj-Bouziane</surname><given-names>F</given-names></name><name><surname>Ben Hamed</surname><given-names>S</given-names></name><name><surname>Harel</surname><given-names>N</given-names></name><name><surname>Hiba</surname><given-names>B</given-names></name><name><surname>Jarraya</surname><given-names>B</given-names></name><name><surname>Jung</surname><given-names>B</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name><name><surname>Klink</surname><given-names>PC</given-names></name><name><surname>Kwok</surname><given-names>SC</given-names></name><name><surname>Laland</surname><given-names>KN</given-names></name><name><surname>Leopold</surname><given-names>DA</given-names></name><name><surname>Lindenfors</surname><given-names>P</given-names></name><name><surname>Mars</surname><given-names>RB</given-names></name><name><surname>Menon</surname><given-names>RS</given-names></name><name><surname>Messinger</surname><given-names>A</given-names></name><name><surname>Meunier</surname><given-names>M</given-names></name><name><surname>Mok</surname><given-names>K</given-names></name><name><surname>Morrison</surname><given-names>JH</given-names></name><name><surname>Nacef</surname><given-names>J</given-names></name><name><surname>Nagy</surname><given-names>J</given-names></name><name><surname>Rios</surname><given-names>MO</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Pinsk</surname><given-names>M</given-names></name><name><surname>Poirier</surname><given-names>C</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Rajimehr</surname><given-names>R</given-names></name><name><surname>Reader</surname><given-names>SM</given-names></name><name><surname>Roelfsema</surname><given-names>PR</given-names></name><name><surname>Rudko</surname><given-names>DA</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Russ</surname><given-names>BE</given-names></name><name><surname>Sallet</surname><given-names>J</given-names></name><name><surname>Schmid</surname><given-names>MC</given-names></name><name><surname>Schwiedrzik</surname><given-names>CM</given-names></name><name><surname>Seidlitz</surname><given-names>J</given-names></name><name><surname>Sein</surname><given-names>J</given-names></name><name><surname>Shmuel</surname><given-names>A</given-names></name><name><surname>Sullivan</surname><given-names>EL</given-names></name><name><surname>Ungerleider</surname><given-names>L</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Todorov</surname><given-names>OS</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wilson</surname><given-names>CRE</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ye</surname><given-names>FQ</given-names></name><name><surname>Zarco</surname><given-names>W</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An open resource for non-human primate imaging</article-title><source>Neuron</source><volume>100</volume><fpage>61</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.039</pub-id><pub-id pub-id-type="pmid">30269990</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizrahi</surname><given-names>A</given-names></name><name><surname>Shalev</surname><given-names>A</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Single neuron and population coding of natural sounds in auditory cortex</article-title><source>Current Opinion in Neurobiology</source><volume>24</volume><fpage>103</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2013.09.007</pub-id><pub-id pub-id-type="pmid">24492086</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>JM</given-names></name><name><surname>Woolley</surname><given-names>SMN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Emergent tuning for learned vocalizations in auditory cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1469</fpage><lpage>1476</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0458-4</pub-id><pub-id pub-id-type="pmid">31406364</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Nodal</surname><given-names>FR</given-names></name><name><surname>Ahmed</surname><given-names>B</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Responses of auditory cortex to complex stimuli: functional organization revealed using intrinsic optical signals</article-title><source>Journal of Neurophysiology</source><volume>99</volume><fpage>1928</fpage><lpage>1941</lpage><pub-id pub-id-type="doi">10.1152/jn.00469.2007</pub-id><pub-id pub-id-type="pmid">18272880</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S.V</given-names></name><name><surname>Kanwisher</surname><given-names>NG</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct cortical pathways for music and speech revealed by hypothesis-free voxel decomposition</article-title><source>Neuron</source><volume>88</volume><fpage>1281</fpage><lpage>1296</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.11.035</pub-id><pub-id pub-id-type="pmid">26687225</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Davis</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural responses to natural and model-matched stimuli reveal distinct computations in primary and nonprimary auditory cortex</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2005127</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2005127</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>S.V</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Conway</surname><given-names>BR</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Divergence in the functional organization of human and macaque auditory cortex revealed by fMRI responses to harmonic tones</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1057</fpage><lpage>1060</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0410-7</pub-id><pub-id pub-id-type="pmid">31182868</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Norman-Haignere</surname><given-names>SV</given-names></name><name><surname>Long</surname><given-names>LK</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Doyle</surname><given-names>W</given-names></name><name><surname>Irobunda</surname><given-names>I</given-names></name><name><surname>Merricks</surname><given-names>EM</given-names></name><name><surname>Feldstein</surname><given-names>NA</given-names></name><name><surname>McKhann</surname><given-names>GM</given-names></name><name><surname>Schevon</surname><given-names>CA</given-names></name><name><surname>Flinker</surname><given-names>A</given-names></name><name><surname>Mesgarani</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multiscale Integration Organizes Hierarchical Computation in Human Auditory Cortex</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.09.30.321687</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Overath</surname><given-names>T</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>Zarate</surname><given-names>JM</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The cortical analysis of speech-specific temporal structure revealed by responses to sound quilts</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>903</fpage><lpage>911</lpage><pub-id pub-id-type="doi">10.1038/nn.4021</pub-id><pub-id pub-id-type="pmid">25984889</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Language, music, and the brain: a resource-sharing framework</article-title><source>Language and Music as Cognitive</source><volume>5</volume><fpage>204</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1093/acprof:oso/9780199553426.001.0001</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Steudel</surname><given-names>T</given-names></name><name><surname>Whittingstall</surname><given-names>K</given-names></name><name><surname>Augath</surname><given-names>M</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A voice region in the monkey brain</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>367</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1038/nn2043</pub-id><pub-id pub-id-type="pmid">18264095</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinker</surname><given-names>S</given-names></name><name><surname>Jackendoff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The faculty of language: what’s special about it?</article-title><source>Cognition</source><volume>95</volume><fpage>201</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2004.08.004</pub-id><pub-id pub-id-type="pmid">15694646</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polley</surname><given-names>DB</given-names></name><name><surname>Steinberg</surname><given-names>EE</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perceptual Learning Directs Auditory Cortical Map Reorganization through Top-Down Influences</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>4970</fpage><lpage>4982</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3771-05.2006</pub-id><pub-id pub-id-type="pmid">16672673</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Cyto- and Myeloarchitectural Brain Atlas of the Ferret (Mustela Putorius) in MRI Aided Stereotaxic Coordinates</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-319-76626-3</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schnupp</surname><given-names>JWH</given-names></name><name><surname>Hall</surname><given-names>TM</given-names></name><name><surname>Kokelaar</surname><given-names>RF</given-names></name><name><surname>Ahmed</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Plasticity of temporal pattern codes for vocalization stimuli in primary auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>4785</fpage><lpage>4795</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4330-05.2006</pub-id><pub-id pub-id-type="pmid">16672651</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Singh</surname><given-names>NC</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Modulation spectra of natural sounds and ethological theories of auditory processing</article-title><source>The Journal of the Acoustical Society of America</source><volume>114</volume><fpage>3394</fpage><lpage>3411</lpage><pub-id pub-id-type="doi">10.1121/1.1624067</pub-id><pub-id pub-id-type="pmid">14714819</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srihasam</surname><given-names>K</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Novel domain formation reveals proto-architecture in inferotemporal cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1776</fpage><lpage>1783</lpage><pub-id pub-id-type="doi">10.1038/nn.3855</pub-id><pub-id pub-id-type="pmid">25362472</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinschneider</surname><given-names>M</given-names></name><name><surname>Nourski</surname><given-names>KV</given-names></name><name><surname>Fishman</surname><given-names>YI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Representation of speech in human auditory cortex: is it special?</article-title><source>Hearing Research</source><volume>305</volume><fpage>57</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2013.05.013</pub-id><pub-id pub-id-type="pmid">23792076</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Elie</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural processing of natural sounds</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>355</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1038/nrn3731</pub-id><pub-id pub-id-type="pmid">24840800</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker</surname><given-names>KM</given-names></name><name><surname>Gonzalez</surname><given-names>R</given-names></name><name><surname>Kang</surname><given-names>JZ</given-names></name><name><surname>McDermott</surname><given-names>JH</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Across-species differences in pitch perception are consistent with differences in cochlear filtering</article-title><source>eLife</source><volume>8</volume><elocation-id>e41626</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.41626</pub-id><pub-id pub-id-type="pmid">30874501</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zatorre</surname><given-names>RJ</given-names></name><name><surname>Belin</surname><given-names>P</given-names></name><name><surname>Penhune</surname><given-names>VB</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Structure and function of auditory cortex: music and speech</article-title><source>Trends in Cognitive Sciences</source><volume>6</volume><fpage>37</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(00)01816-7</pub-id><pub-id pub-id-type="pmid">11849614</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Recentered CCA</title><sec id="s8-1" sec-type="appendix"><title>Derivation</title><p>The goal of the denoising procedure described in part I was to remove artifactual components that were present both inside and outside of cortex since such components are both likely to be artifactual and likely to distort the responses of interest. The key complication was that motion-induced artifacts are likely to be correlated with true sound-driven neural activity because the animals reliably moved more during the presentation of some sounds. To deal with this issue, we used the fact that motion will vary from trial-to-trial for repeated presentations of the same sound, while sound-driven responses by definition will not. Here, we give a more formal derivation of our procedure. We refer to our method as ‘recentered canonical correlation analysis’ (rCCA) for reasons that will become clear below.</p><p>We represent the data for each voxel as an unrolled vector (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) that contains its response timecourse across all sounds and repetitions. We assume that these voxel responses are contaminated by a set of <italic>K</italic> artifactual component timecourses <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We thus model each voxel as a weighted sum of these artifactual components plus a sound-driven response timecourse (<inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>):<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Actual voxel responses are also corrupted by voxel-specific noise, which would add an additional error term to the above equation. In practice, the error term has no effect on our derivation so we omit it for simplicity (we verified our analysis was robust to voxel-specific noise using simulations, which are described below).</p><p>To denoise our data, we need to estimate the artifactual timecourses <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and their weights (<inline-formula><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) so that we can subtract them out. If the artifactual components <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> were uncorrelated with the sound-driven responses (<inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), we could estimate them by performing CCA on voxel responses from inside and outside of cortex since only the artifacts would be correlated. However, we expect sound-driven responses to be correlated with motion artifacts, and the components inferred by CCA will thus reflect a mixture of sound-driven and artifactual activity.</p><p>To overcome this problem, we first subtract-out the average response of each voxel across repeated presentations of the same sound. This ‘recentering’ operation removes sound-driven activity, which by definition is the same across repeated presentations of the same sound:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi mathvariant="bold-italic">k</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where the dot above a variable indicates its response after recentering (not its time derivative). Because sound-driven responses have been eliminated, applying CCA to the recentered voxel responses should yield an estimate of the recentered artifacts (<inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and their weights (<inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) (note that CCA actually yields a set of components that span a similar subspace as the artifactual components, which is equivalent from the perspective of denoising). To simplify notation in the equations below, we assume this estimate is exact (i.e., CCA exactly returns <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p><p>Since the weights (<inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) are the same for original (<inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and recentered (<inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) data, we are halfway done. All that is left is to estimate the original artifact components before recentering (<inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), which can be done using the original data before recentering (<inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). To see this, first note that canonical components (CCs) are by construction a linear projection of the data used to compute them, and thus, we can write<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>We can use the reconstruction weights (<inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) in the above equation to get an estimate of the original artifactual components by applying them to the original data before recentering:<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mtext> </mml:mtext><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>To see this, we expand the above equation:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mrow></mml:msub><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>The first term in the above equation exactly equals <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> because <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are by construction pseudoinverses of each other (i.e., <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is 1 when <inline-formula><mml:math id="inf49"><mml:msup><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mi>k</mml:mi></mml:math></inline-formula> and 0 otherwise). The second term can be made small by estimating and applying reconstruction weights using only data from outside of cortex, where sound-driven responses are weak.</p><p>We thus have a procedure for estimating both the original artifactual responses (<inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and their weights (<inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), and can denoise our data by simply subtracting them out:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p></sec><sec id="s8-2" sec-type="appendix"><title>Procedure</title><p>We now give the specific steps used to implement the above idea using matrix notation. The inputs to the analysis were two matrices (<inline-formula><mml:math id="inf52"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf53"><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), each of which contained voxel responses from inside and outside of cortex. Each column of each matrix contained the response timecourse of a single voxel, concatenated across all sounds and repetitions (i.e., <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in the above derivation). We also computed recentered data matrices (<inline-formula><mml:math id="inf55"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf56"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) by subtracting out trial-averaged activity (i.e., <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>CCA can be performed by whitening each input matrix individually, concatenating the whitened data matrices, and then computing the PCs of the concatenated matrices (<xref ref-type="bibr" rid="bib9">de Cheveigné et al., 2019</xref>). Our procedure is an elaborated version of this basic design:</p><p>1. The recentered data matrices were reduced in dimensionality and whitened. We implemented this step using the singular value decomposition (SVD), which factors the data matrix as the product of two orthonormal matrices (<inline-formula><mml:math id="inf58"><mml:mi>U</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf59"><mml:mi>V</mml:mi></mml:math></inline-formula>), scaled by a diagonal matrix of singular values (<inline-formula><mml:math id="inf60"><mml:mi>S</mml:mi></mml:math></inline-formula>):<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>S</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mi>V</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula><disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>U</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi>S</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mover><mml:mi>V</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>The reduced and whitened data were given by selecting the top 250 components and removing the diagonal S matrix:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>250</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>V</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>250</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>U</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>250</mml:mn><mml:mo stretchy="false">]</mml:mo><mml:msub><mml:mover><mml:mi>V</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mn>250</mml:mn><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:math></disp-formula></p><p>2. We concatenated the whitened data matrices from inside and outside of cortex across the voxel dimension:<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover><mml:mi>D</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>3. We computed the top N PCs from the concatenated matrix using the SVD:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:msub><mml:mover><mml:mi>D</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover><mml:mi>U</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mover><mml:mi>S</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mover><mml:mi>V</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf61"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> contains the timecourses of the CCs, ordered by variance, which provide an estimate of the artifactual components after recentering (i.e., <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mover><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). The corresponding weights (i.e., <inline-formula><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) for voxels inside of cortex were computed by projecting the recentered data onto <inline-formula><mml:math id="inf64"><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> :<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">n</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:msubsup><mml:mover><mml:mi>U</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:msub><mml:mover><mml:mi>D</mml:mi><mml:mrow><mml:mo>˙</mml:mo></mml:mrow></mml:mover><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where + indicates the matrix pseudoinverse.</p><p>4. The original artifactual components before recentering (i.e., <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) were estimated by learning a set of reconstruction weights (<inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) using recentered data from outside of cortex, and then applying these weights to the original data before recentering:<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mover><mml:mi>D</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:msub><mml:mrow><mml:mover><mml:mi>U</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>B</mml:mi></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>U</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is an estimate of the artifactual components before recentering (i.e., <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>5. Finally, we subtracted out the contribution of the artifactual components to each voxel inside of cortex, estimated by simply multiplying the component responses and weights:<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p></sec><sec id="s8-3" sec-type="appendix"><title>Simulation</title><p>We created a simulation to test our method. We simulated 1000 voxel responses, both inside and outside of cortex, using <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>. For voxels outside of cortex, we set the sound-driven responses to 0. We also added voxel-specific noise to make the denoising task more realistic/difficult (sampled from a Gaussian). Results were very similar across a variety of noise levels.</p><p>To induce correlations between the artifactual (<inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and sound-driven responses (<inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), we forced them to share a subspace. Specifically, we computed the sound-driven responses as a weighted sum of a set of 10 component timecourses (results did not depend on this parameter), thus forcing the responses to be low-dimensional, as we found to be the case:<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>The artifactual timecourses were then computed as a weighted sum of these same 10 components timecourses plus a timecourse that was unique to each artifactual component:<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>p</mml:mi><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf71"><mml:mi>p</mml:mi></mml:math></inline-formula> controls the strength of the dependence between the sound-driven and artifactual components with a value of 1 indicating complete dependence and 0 indicating no dependence. All of responses and weights (<inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , <inline-formula><mml:math id="inf75"><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) were sampled from a unit-variance Gaussian. Sound-driven responses were constrained to be the same across repetitions by sampling the latent timecourses <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> once, and then simply repeating the sampled values across repetitions. In contrast, a unique <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was sampled for every repetition to account for the fact that the artifacts like motion will vary from trial-to-trial. We sampled 20 artifactual timecourses using <xref ref-type="disp-formula" rid="equ30">Equation 30</xref>.</p><p>We applied both standard CCA and our modified rCCA method to the simulated data. We measured the median NSE between the true and estimated sound-driven responses (<inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), computed using the two methods as a function of the strength of the dependence (<inline-formula><mml:math id="inf79"><mml:mi>p</mml:mi></mml:math></inline-formula>) between sound-driven and artifactual timecourses (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). For comparison, we also plot the NSE for raw voxels (i.e., before any denoising) as well as the minimum possible NSE (noise floor) given the voxel-specific noise (which cannot possibly be removed using CCA or rCCA). When the dependence factor (<inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is low, both CCA and rCCA yield similarly good results, as expected. As the dependence increases, CCA performs substantially worse, while rCCA continues to perform well up until the point when the dependence becomes so strong that sound-driven and artifactual timecourses are nearly indistinguishable. Results were not highly sensitive to the number of components removed as long as the number of removed components was equal to or greater than the number of artifactual components (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Simulation results.</title><p>(<bold>A</bold>) Median normalized squared error (NSE) across simulated voxels between the true and estimated sound-driven responses (<inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), computed using raw/undenoised data (light green line), standard canonical correlation analysis (CCA) (dark green line), and recentered CCA (red line). Results are shown as a function of the strength of the dependence (<inline-formula><mml:math id="inf82"><mml:mi>p</mml:mi></mml:math></inline-formula>) between sound-driven and artifactual timecourses. The minimum possible NSE (noise floor) given the level of voxel-specific noise is also shown. (<bold>B</bold>) Same as panel (<bold>A</bold>), but showing results as a function of the number of components removed for a fixed value of <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (set to 0.5).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-app1-fig1-v1.tif"/></fig></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65566.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2020.09.30.321695" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2020.09.30.321695"/></front-stub><body><p>How the auditory system encodes speech sounds is not well understood, and animal models have a lot to offer in investigating such questions. This study evaluated the representations of a variety of natural and synthetic sounds in both ferrets and humans, and reported that humans differed from ferrets in the manner in which speech and music were represented, despite controlling for the spectrotemporal content of the sounds. This work makes an important contribution to our understanding of how the coding of such sounds differs across species.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65566.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Groh</surname><given-names>Jennifer M</given-names></name><role>Reviewing Editor</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Cogan</surname><given-names>Greg</given-names></name><role>Reviewer</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Overath</surname><given-names>Tobias</given-names></name><role>Reviewer</role><aff><institution>Duke University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.09.30.321695">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.09.30.321695v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Distinct higher-order representations of natural sounds in human and ferret auditory cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Andrew King as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Greg Cogan (Reviewer #2); Tobias Overath (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Both reviewers found the work to be interesting and important, but the concerns about the reproducibility of the finding given the small number of animals tested weighed heavily. Given that both reviewers found merit in the work, we encourage a revised submission provided the concerns about reproducibility can be satisfactorily addressed.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. Fig 2F: It would be useful here to quantify the slope as this appears to be a relevant feature of this figure.</p><p>2. Figure 2F: Are the distances for ferret vs. human chosen for a particular reason? Is it just a simple linear scaling based on brain size?</p><p>3. Is the unit size of one voxel a reasonable analysis size? If you average over all voxels in a particular region, do the results from figure 1A-D change?</p><p>4. Does the dimension reduction/component analysis (Figure 3) contain data from experiment 2 or just experiment 1? If only 1, do the results change by including the data from experiment 2?</p><p>5. While I am sure that the difference between methods of acquisition cannot fully explain your results (fUS vs. fMRI), is would be useful to comment on the relative SNR of the methods and how this would or would not influence your results.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1) Selectivity vs. sensitivity</p><p>The authors use the term selectivity, which implies exclusivity: e.g., response to a certain sound characteristic, but no response to any other sound characteristic. Given the actual data that the authors show, a more appropriate term to use would be sensitivity. For example, the f3 component's response profile in Figure S3 clearly shows the strongest response to speech sounds, but there is also substantial (though weaker) response to the other types of sounds. In that sense, f3 is not selective, but rather shows that it reflects maximal sensitivity to speech sounds (or, even more precisely, particular spectrotemporal characteristics of speech sounds). The authors should adjust the terminology accordingly throughout their manuscript.</p><p>2) Generalizing from 2 ferrets to all ferrets seems 'courageous' to me, especially given the replicability crisis in the human neuroimaging community. For what it's worth, the mean signal prior to denoising (Figure 1C) looks about as noisy as human fMRI data. I understand the invasive nature of fUS imaging, but I would feel much more comfortable seeing these results replicated in more animals.</p><p>3) Can the authors expand a bit on their reasoning for choosing a 3-11 s time window (line 701)? Looking at Figure 1c, it seems that this includes data from the initial rise period (which is not of interest), rather than just the (more) steady part of the response. I would have expected the authors to focus on the sustained, steady part response (e.g. 6-11 s), which presumably best reflects processing of the summary statistics of the input sound. The authors should show that their results are insensitive to (reasonable) variations in the time window.</p><p>4) NSE. I implemented the NSE formula in Matlab via</p><p>x = rand(1,40);</p><p>y = rand(1,40);</p><p>NSE = mean((x-y).^2) / (mean(x.^2) + mean(y.^2) – 2*mean(x)*mean(y))</p><p>However, the values I get for this implementation are not bounded between 0 and 1. Perhaps my implementation is wrong, or there is an error in the formula?</p><p>Also, after clarifying their NSE measure (or pointing out the mistake in the above implementation), can the authors elaborate on how NSE can distinguish between the cases (A) where a voxel has different response profiles for natural vs. model-matched sounds (e.g. x = 1:40; y = 40:-1:1;) vs. (B) where the response difference between natural and model-matched sounds is simply additive (or multiplicative) in nature (e.g. x = 1:40; y = x*2), vs. (C) when they are anticorrelated (x = [1 -1 1 -1]; y = [-1 1 -1 1])?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65566.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Both reviewers found the work to be interesting and important, but the concerns about the reproducibility of the finding given the small number of animals tested weighed heavily. Given that both reviewers found merit in the work, we encourage a revised submission provided the concerns about reproducibility can be satisfactorily addressed.</p><p>Reviewer #2 (Recommendations for the authors):</p><p>1. Fig 2F: It would be useful here to quantify the slope as this appears to be a relevant feature of this figure.</p></disp-quote><p>Thank you for this suggestion. As noted in our general response to the editor and all reviewers, we now plot slopes for all individual human subjects and ferrets for both Experiment I (Figure 2G) and Experiment II (Figure 4F). See our note at the beginning of this response for details.</p><disp-quote content-type="editor-comment"><p>2. Figure 2F: Are the distances for ferret vs. human chosen for a particular reason? Is it just a simple linear scaling based on brain size?</p></disp-quote><p>The x-axis plots distances in millimeters, but in order to show both ferrets and humans on the same plot we had to rescale the axes. The corresponding unit is shown for both humans and ferrets below the axis. The 10x rescaling corresponds loosely to the difference in the radius of primary auditory cortex across species. However, our results are not sensitive to this scaling factor. Indeed, even if we use absolute distances without any rescaling, as was done to quantify the slope (Figure 2G), we still observe much more prominent changes in humans compared with ferrets. Using absolute distances substantially biases against our findings, since correcting for brain size would differentially inflate the ferret slopes relative to the human slopes. We have clarified this point in the Results (page 5):</p><p>“We used absolute distance for calculating the slopes, which is a highly conservative choice given our findings, since correcting for brain size would enhance the slopes of ferrets relative to humans. Despite this conservative choice, we found that the slope of every ferret was well below that of all 12 human subjects tested, and thus significantly different from the human group via a non-parametric sign test (p &lt; 0.001).”</p><p>We have clarified the scaling factor in the legend of Figure 2F:</p><p>“The ferret and human data were rescaled so they could be plotted on the same figure, using a scaling factor of 10, which roughly corresponds to the difference in the radius of primary auditory cortex between ferrets and humans. The corresponding unit is plotted on the x-axis below.”</p><disp-quote content-type="editor-comment"><p>3. Is the unit size of one voxel a reasonable analysis size? If you average over all voxels in a particular region, do the results from figure 1A-D change?</p></disp-quote><p>In Figure 2—figure supplement 1, we plot results averaged across all voxels within standard anatomical regions-of-interest (ROIs). The results are very similar to those we find in individual voxels, with closely matched responses to natural and synthetic sounds. This result is expected since we found that individual voxels had closely matched responses and if a set of voxels have matched responses, then their average response must also be matched. We note however that the opposite is not true: if an ROI shows a matched response, there is no guarantee that the individual voxels have matched responses, since averaging across voxels could wash out heterogeneous and divergent response patterns. Thus, we believe it is appropriate to analyze individual voxels. We also note that one reason for averaging responses within an ROI is that individual voxel responses are typically quite noisy. Our denoising method however substantially boosted the reliability of our voxel responses (Figure 2 - figure supplement 1), which made it possible to analyze individual voxels.</p><p>We have clarified these points in the Results when describing our single voxel analyses (page 4):</p><p>“Our denoising procedure substantially boosted the SNR of the measurements (Figure 1 - figure supplement 1) and made it possible to analyze individual voxels, as opposed to averaging responses across a large region-of-interest (ROI), which could potentially wash out heterogeneity present at the single voxel level. […] (results were similar when averaging responses within anatomical regions of interest, see Figure 2 - figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>4. Does the dimension reduction/component analysis (Figure 3) contain data from experiment 2 or just experiment 1? If only 1, do the results change by including the data from experiment 2?</p></disp-quote><p>Figure 3 shows results from applying component analyses to data from Experiment I. Results are very similar to Experiment II. We have added a supplemental figure (Figure 4 - figure supplement 2), which shows components from Experiment II that responded preferentially to low frequencies (top panel), high-frequencies (middle panel), and speech (bottom panel).</p><p>We note that it is not straightforward to combine data from the two experiments because the stimuli are different and the voxels are not identical because the data were recorded on different days using different slices. We have clarified this point in the Methods (page 19):</p><p>“We separately analyzed responses from Experiment I (Figure 3) and Experiment II (Figure 4 - figure supplement 2) because there was no simple way to combine the data across experiments, since the stimuli were distinct and there was no simple correspondence across voxels since the data were collected from different slices on different days.”</p><disp-quote content-type="editor-comment"><p>5. While I am sure that the difference between methods of acquisition cannot fully explain your results (fUS vs. fMRI), is would be useful to comment on the relative SNR of the methods and how this would or would not influence your results.</p></disp-quote><p>We have performed a new analysis to directly address this question, the results of which are shown in Figure 1D, reproduced below. We measured the correlation between pairs of fUS or fMRI voxels as a function of their distance using two independent measures of each voxel’s response (odd vs. even repetitions). As a consequence, the 0-mm datapoint provides a measure of test-retest reliability (i.e. SNR) and the fall-off with distance provides a measure of spatial precision. Results are shown separately before and after applying our component denoising method. As is evident, our denoising procedure substantially boosts the reliability of the data, which made it possible to analyze individual fUS voxels, which had low reliability before denoising. The reliability of the denoised fUS data is substantially higher than the fMRI data used in our prior study, which were not denoised, since the voxels were reliable enough to perform all of our key analyses. This finding suggests that the denoised fUS data should be more than reliable enough to detect the kinds of effects we observed previously with fMRI in humans. To make the fUS and fMRI analyses more similar, we now use component-denoised fMRI data which had similar reliability to the denoised fUS data, but our findings did not depend on this choice (see Figure 1 - figure supplement 2 if interested, which shows that results are similar for raw and denoised fMRI data).</p><p>The second noteworthy feature of this plot is that the correlation falls off more sharply for the fUS data (note the different x-axes), which we quantified by measuring the distance needed for the correlation to drop by 75% (<inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>75</mml:mn><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 9.3 mm vs <inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>75</mml:mn><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> 1.2 mm, Wilcoxon rank-sum test across subjects, p &lt; 0.05). This plot shows fMRI data smoothed with a 5 mm FWHM kernel, which is the same kernel we used in our prior study, but the fMRI data is still substantially coarser when not smoothed (<inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>75</mml:mn><mml:mo>,</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> = 6.5 mm vs <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mn>75</mml:mn><mml:mo>,</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> 1.2 mm, Wilcoxon rank-sum test across subjects, p &lt; 0.05). Our human findings were very similar across different smoothing levels, indicating that the organization we detected in humans does not depend sensitively on the spatial precision of the method. These analyses suggest that our denoised fUS data is sufficiently reliable and precise to observe the kinds of functional organization we observed in humans were that organization present in ferrets.</p><p>We have added the above figure to the manuscript. The analysis is described briefly in the Results (page 4):</p><p>“We found that the denoised fUS responses were substantially more reliable and precise than the fMRI voxels from our prior study (Figure 1D) (Test-retest correlation: 0.93 vs 0.44, Wilcoxon rank-sum test across subjects, p &lt; 0.01). To make our human and ferret analyses more similar, we used component-denoised fMRI data in this study, which had similar reliability to the denoised fUS data (Figure 1D; results were similar without denoising, see Figure 1 - figure supplement 2).”</p><p>More detail is given in the legend (above) and Methods (page 16):</p><p>“We compared the precision and reliability of the fUS and fMRI data by measuring the correlation between all pairs of voxels and binning the results based on their distance (Figure 1D plots the mean correlation within each bin; ferret bin size was 0.5 mm; human bin size was 3 mm). […] We statistically compared the reliability (0-mm correlation) and decay rate of the spatial correlation function across species using a Wilcoxon rank-sum test across subjects.”</p><p>Finally, we have included a paragraph in the Discussion that enumerates the reasons why we believe our findings are unlikely to be due to methodological differences (page 9):</p><p>“The species differences we observed are unlikely to be driven by differences in the method used to record brain responses (fUS vs. fMRI) for several reasons. […] We quantified this change by measuring the slope of the NSE-vs-distance curve and found that the slopes in ferrets were close to zero and differed substantially from every human subject tested.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1) Selectivity vs. sensitivity</p><p>The authors use the term selectivity, which implies exclusivity: e.g., response to a certain sound characteristic, but no response to any other sound characteristic. Given the actual data that the authors show, a more appropriate term to use would be sensitivity. For example, the f3 component's response profile in Figure S3 clearly shows the strongest response to speech sounds, but there is also substantial (though weaker) response to the other types of sounds. In that sense, f3 is not selective, but rather shows that it reflects maximal sensitivity to speech sounds (or, even more precisely, particular spectrotemporal characteristics of speech sounds). The authors should adjust the terminology accordingly throughout their manuscript.</p></disp-quote><p>We have largely removed the word “selectivity” from the manuscript and now use terms like “sensitivity” or “speech-preferring”.</p><disp-quote content-type="editor-comment"><p>2) Generalizing from 2 ferrets to all ferrets seems 'courageous' to me, especially given the replicability crisis in the human neuroimaging community. For what it's worth, the mean signal prior to denoising (Figure 1C) looks about as noisy as human fMRI data. I understand the invasive nature of fUS imaging, but I would feel much more comfortable seeing these results replicated in more animals.</p></disp-quote><p>See our note at the very beginning of this response which describes how we have addressed this important concern.</p><disp-quote content-type="editor-comment"><p>3) Can the authors expand a bit on their reasoning for choosing a 3-11 s time window (line 701)? Looking at Figure 1c, it seems that this includes data from the initial rise period (which is not of interest), rather than just the (more) steady part of the response. I would have expected the authors to focus on the sustained, steady part response (e.g. 6-11 s), which presumably best reflects processing of the summary statistics of the input sound. The authors should show that their results are insensitive to (reasonable) variations in the time window.</p></disp-quote><p><xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> shows NSE maps for two different windows. The results are virtually identical. In general, the results are highly robust to the exact window used.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65566-sa2-fig1-v1.tif"/></fig><p>We have clarified this point in the Methods (page 15):“We therefore measured the response magnitude of each voxel by averaging the response to each sound across time (from 3 to 11 seconds post-stimulus onset; results were robust to the window size), yielding one number per sound.”</p><p>There are already a large number of supplemental figures, but we would be happy to add this figure to supplemental if you feel it’s important.</p><disp-quote content-type="editor-comment"><p>4) NSE. I implemented the NSE formula in Matlab via</p><p>x = rand(1,40);</p><p>y = rand(1,40);</p><p>NSE = mean((x-y).^2) / (mean(x.^2) + mean(y.^2) – 2*mean(x)*mean(y))</p><p>However, the values I get for this implementation are not bounded between 0 and 1. Perhaps my implementation is wrong, or there is an error in the formula?</p></disp-quote><p>We greatly appreciate you for taking the time to investigate the properties of the NSE measure. Your implementation is correct and we apologize for lack of clarity. The NSE is bounded between 0 and 2, but has an expected value of 1 for independent signals with no dependency, which in most scenarios is the de facto null/upper-bound – in the same way that zero is typically the de facto null when comparing two signals even though anti-correlations are possible. Below, we include MATLAB code, which demonstrates this fact for large samples, for which the measured NSE approaches its expected value (for smaller samples, there will of course be more variation around the expected value):</p><p>N = 100000;</p><p>x = rand(1,N);</p><p>y = rand(1,N);</p><p>NSE = mean((x-y).^2) / (mean(x.^2) + mean(y.^2) – 2*mean(x)*mean(y))</p><p>The NSE can take a larger value if the signals are anticorrelated. For example, if we have two zero-mean signals that are inverses of each other, then the NSE is exactly 2:</p><p>N = 100000;</p><p>x = rand(1,N);</p><p>x = x – mean(x);</p><p>y = -x;</p><p>NSE = mean((x-y).^2) / (mean(x.^2) + mean(y.^2) – 2*mean(x)*mean(y))</p><p>This is analogous to the correlation coefficient which has a maximal value of 1 for identical signals, -1 for anticorrelated signals, and 0 for independent signals (in expectation).</p><p>We have clarified this point in the Methods (page 17):</p><p>“The NSE takes a value of 0 if the response to natural and synthetic sounds is identical and 1 if there is no correspondence between responses to natural and synthetic sounds (i.e. they are independent). For anticorrelated signals, the NSE can exceed 1 with a maximum value of 2 for signals that are zero-mean and perfectly anti-correlated. This is analogous to the correlation coefficient, which has a maximum value of 1 for identical signals, a minimum value of -1 for anticorrelated signals, and a value of 0 for independent signals.”</p><disp-quote content-type="editor-comment"><p>Also, after clarifying their NSE measure (or pointing out the mistake in the above implementation), can the authors elaborate on how NSE can distinguish between the cases (A) where a voxel has different response profiles for natural vs. model-matched sounds (e.g. x = 1:40; y = 40:-1:1;) vs. (B) where the response difference between natural and model-matched sounds is simply additive (or multiplicative) in nature (e.g. x = 1:40; y = x*2), vs. (C) when they are anticorrelated (x = [1 -1 1 -1]; y = [-1 1 -1 1])?</p></disp-quote><p>The NSE value is a summary measure that takes a value of 0 if the responses are identical and a higher value if the response diverges in any way, whether that be due to differences in the mean, scale, or response pattern.</p><p>As noted above, the NSE is 1 if the two responses are independent (irrespective of mean or scale):</p><p>N = 100000;</p><p>x = rand(1,N);</p><p>y = rand(1,N);</p><p>NSE = mean((x-y).^2) / (mean(x.^2) + mean(y.^2) – 2*mean(x)*mean(y))</p><p>Mean and scale differences both cause a rise in NSEs with values approaching 1 as the means and scales diverge even if the response pattern is identical:</p><p>N = 100000;</p><p>x = rand(1,N);</p><p>y = x + 1000;</p><p>NSE = mean((x-y).^2) / (mean(x.^2) + mean(y.^2) – 2*mean(x)*mean(y))</p><p>y = x*1000;</p><p>NSE = mean((x-y).^2) / (mean(x.^2) + mean(y.^2) – 2*mean(x)*mean(y))</p><p>The primary difference between the NSE and the correlation coefficient is that the correlation coefficient is insensitive to mean and scale. This property is problematic because the model predicts that the responses should be the same if the model is accurate and thus any divergence, whether it be due to mean, scale, or pattern, reflects a model failure. In ferrets, the NSE values are near 0 for fully matched synthetic sounds, which guarantees that the mean, scale, and pattern are all similar. For humans, the NSE values are large in non-primary regions which indicates a divergent response, but does not say anything about whether it is the mean, scale or pattern that differs. In our prior paper, we showed that these high NSE values in humans are primarily driven by stronger responses to natural vs. synthetic sounds, which manifests as a downward scaling of the responses to synthetic sounds. We have clarified all of these points in the Methods when describing the NSE (page 17):</p><p>“Unlike the correlation coefficient, the NSE is sensitive to differences in the mean and scale of the responses being compared, in addition to differences in the response pattern. This property is useful because the model predicts that the responses to natural and synthetic sounds should be matched (Norman-Haignere and McDermott, 2018), and thus any divergence in the response to natural vs. synthetic sounds reflects a model failure, regardless of whether that divergence is driven by the pattern, mean, or scale of the response. In ferrets, we observed NSE values near 0 throughout ferret auditory cortex, indicating that responses are approximately matched in all respects. In contrast, humans showed large NSE values in non-primary auditory cortex, which could in principle be driven by differences in the mean, scale, or response pattern. In our prior work, we showed that these high NSE values are primarily driven by stronger responses to natural vs. synthetic sounds, which manifests as a downward scaling of the response to synthetic sounds. The stronger responses to natural sounds are presumably driven by sensitivity to higher-order structure that is absent from the synthetic sounds.”</p></body></sub-article></article>