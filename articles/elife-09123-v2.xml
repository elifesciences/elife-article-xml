<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">09123</article-id><article-id pub-id-type="doi">10.7554/eLife.09123</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Nonlinear circuits for naturalistic visual motion estimation</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-28900"><name><surname>Fitzgerald</surname><given-names>James E</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0949-4188</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-27474"><name><surname>Clark</surname><given-names>Damon A</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-8487-700X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution content-type="dept">Center for Brain Science</institution>, <institution>Harvard University</institution>, <addr-line><named-content content-type="city">Cambridge</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><label>2</label><institution content-type="dept">Department of Molecular, Cellular and Developmental Biology</institution>, <institution>Yale University</institution>, <addr-line><named-content content-type="city">New Haven</named-content></addr-line>, <country>United States</country></aff><aff id="aff3"><label>3</label><institution content-type="dept">Department of Physics</institution>, <institution>Yale University</institution>, <addr-line><named-content content-type="city">New Haven</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-13133"><name><surname>Carandini</surname><given-names>Matteo</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>jamesfitzgerald@fas.harvard.edu</email> (JEF);</corresp><corresp id="cor2"><email>damon.clark@yale.edu</email> (DAC)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>24</day><month>10</month><year>2015</year></pub-date><pub-date pub-type="collection"><year>2015</year></pub-date><volume>4</volume><elocation-id>e09123</elocation-id><history><date date-type="received"><day>01</day><month>06</month><year>2015</year></date><date date-type="accepted"><day>23</day><month>10</month><year>2015</year></date></history><permissions><copyright-statement>© 2015, Fitzgerald and Clark</copyright-statement><copyright-year>2015</copyright-year><copyright-holder>Fitzgerald and Clark</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-09123-v2.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.09123.001</object-id><p>Many animals use visual signals to estimate motion. Canonical models suppose that animals estimate motion by cross-correlating pairs of spatiotemporally separated visual signals, but recent experiments indicate that humans and flies perceive motion from higher-order correlations that signify motion in natural environments. Here we show how biologically plausible processing motifs in neural circuits could be tuned to extract this information. We emphasize how known aspects of <italic>Drosophila</italic>'s visual circuitry could embody this tuning and predict fly behavior. We find that segregating motion signals into ON/OFF channels can enhance estimation accuracy by accounting for natural light/dark asymmetries. Furthermore, a diversity of inputs to motion detecting neurons can provide access to more complex higher-order correlations. Collectively, these results illustrate how non-canonical computations improve motion estimation with naturalistic inputs. This argues that the complexity of the fly's motion computations, implemented in its elaborate circuits, represents a valuable feature of its visual motion estimator.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.001">http://dx.doi.org/10.7554/eLife.09123.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.09123.002</object-id><title>eLife digest</title><p>Many animals have evolved the ability to estimate the speed and direction of visual motion. They use these estimates to judge their own motion, so that they can navigate through an environment, and to judge how other animals are moving, which allows them to avoid predators or detect prey.</p><p>In the 1950s, a physicist and a biologist used measurements of beetle behavior in response to visual stimuli to develop a model for how the brain estimates motion. The model became known as the Hassenstein-Reichardt correlator (HRC). The HRC and related models accurately predict the behavioral and neural responses of insects and mammals to many types of motion stimuli.</p><p>However, there are visual stimuli that generate motion percepts in fruit flies (and humans) that cannot be accounted for by the HRC. Are these differences between real brains and the HRC simply imperfections in visual circuits, whose neurons cannot perform idealized mathematical operations, or are these deviations intentional, somehow improving motion estimates? In other words: are the observed deviations a bug or a feature of visual circuits?</p><p>To address this question, Fitzgerald and Clark evaluated how different models of motion detection performed when presented with natural scenes. Natural scenes are fundamentally different from most stimuli used in lab, since they contain a rich set of regularities that are not present in simple stimuli. Fitzgerald and Clark compared the ability of the HRC, along with new, more general models, to estimate the speed and direction at which images moved across a screen. This revealed that many models could out-perform the HRC by taking advantage of regularities in natural scenes. Those models that were tuned to perform well with natural scenes could also predict the paradoxical motion percepts that were not predicted by the HRC. This suggests that visual circuits may have evolved to perform well with natural inputs, and the paradoxical motion percepts represent a feature of the real circuit, rather than a bug.</p><p>Models that performed well with natural inputs treated light and dark visual information differently. This different treatment of light and dark is a property of most visual systems, but not of the HRC or related models. In the future, these models of motion processing may help us understand how biological details of the fruit fly's visual circuits help it to estimate motion.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.002">http://dx.doi.org/10.7554/eLife.09123.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>vision</kwd><kwd>motion perception</kwd><kwd>natural scenes</kwd><kwd>circuit</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>D. melanogaster</italic></kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution>Swartz Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral Fellowship</award-id><principal-award-recipient><name><surname>Fitzgerald</surname><given-names>James E</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution>Searle Scholars Program</institution></institution-wrap></funding-source><award-id>Scholar Award</award-id><principal-award-recipient><name><surname>Clark</surname><given-names>Damon A</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001341</institution-id><institution>Richard and Susan Smith Family Foundation</institution></institution-wrap></funding-source><award-id>Scholar Award</award-id><principal-award-recipient><name><surname>Clark</surname><given-names>Damon A</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000879</institution-id><institution>Alfred P. Sloan Foundation</institution></institution-wrap></funding-source><award-id>Sloan Research Fellowship in Neuroscience</award-id><principal-award-recipient><name><surname>Clark</surname><given-names>Damon A</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2.3</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>New computational models provide insights into how the insect brain estimates the speed and direction of movement.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A major goal in neuroscience is to understand how the brain computes behaviorally relevant stimulus properties from streams of incoming sensory data (<xref ref-type="bibr" rid="bib57">Sejnowski et al., 1988</xref>). Visual motion guides behaviors across the animal kingdom. To navigate, many vertebrates and invertebrates use visual data to estimate the velocity of full field motion, and they use that estimate to judge their motion with respect to their environment (<xref ref-type="bibr" rid="bib61">Sperry, 1950</xref>; <xref ref-type="bibr" rid="bib33">Kalmus, 1964</xref>; <xref ref-type="bibr" rid="bib52">Reichardt and Poggio, 1976</xref>; <xref ref-type="bibr" rid="bib46">Orger et al., 2000</xref>). Spatially localized motion perception (<xref ref-type="bibr" rid="bib28">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="bib3">Barlow and Hill, 1963</xref>; <xref ref-type="bibr" rid="bib9">Buchner, 1976</xref>) is also important, as it can indicate the presence of predators or prey in the environment (<xref ref-type="bibr" rid="bib53">Reichardt et al., 1983</xref>; <xref ref-type="bibr" rid="bib20">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib44">Nordström et al., 2006</xref>; <xref ref-type="bibr" rid="bib75">Zhang et al., 2012</xref>), and spatial velocity gradients allow animals to judge relative distances (<xref ref-type="bibr" rid="bib54">Rogers and Graham, 1979</xref>; <xref ref-type="bibr" rid="bib62">Srinivasan et al., 1991</xref>; <xref ref-type="bibr" rid="bib36">Kral, 2003</xref>; <xref ref-type="bibr" rid="bib47">Pick and Strauss, 2005</xref>). In principle, different algorithms could be used to estimate different types of motion. However, data suggest that many animals compute local motion over an array of spatially localized elementary motion detectors, or EMDs, and then differentially pool those signals for use in different behaviors and neural operations (<xref ref-type="bibr" rid="bib28">Hubel and Wiesel, 1962</xref>; <xref ref-type="bibr" rid="bib3">Barlow and Hill, 1963</xref>; <xref ref-type="bibr" rid="bib9">Buchner, 1976</xref>; <xref ref-type="bibr" rid="bib8">Britten et al., 1992</xref>; <xref ref-type="bibr" rid="bib20">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib19">Franz and Krapp, 2000</xref>; <xref ref-type="bibr" rid="bib55">Rust et al., 2006</xref>).</p><p>The Hassenstein-Reichardt correlator (HRC) was introduced nearly sixty years ago to model the EMD underlying the beetle's optomotor response (<xref ref-type="bibr" rid="bib25">Hassenstein and Reichardt, 1956</xref>). It has since provided numerous insights into motion-guided behaviors across a variety of insect species. The HRC's successes are most striking in flies, where the HRC accurately predicts a wide variety of behavioral and neural responses (<xref ref-type="bibr" rid="bib22">Götz, 1968</xref>; <xref ref-type="bibr" rid="bib9">Buchner, 1976</xref>; <xref ref-type="bibr" rid="bib16">Egelhaaf and Borst, 1989</xref>; <xref ref-type="bibr" rid="bib24">Haag et al., 2004</xref>), and even adaptation to stimulus statistics (<xref ref-type="bibr" rid="bib6">Borst et al., 2005</xref>). The HRC's importance also extends to primates and vertebrates, where the EMDs are often described in terms of the motion energy model (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>). In particular, although the HRC and motion energy models differ in terms of their intuition and neuronal bases, both models rely on the same mathematical fact about moving visual stimuli—motion causes pairs of spatially separated points to become correlated when a stimulus moves from one location towards the other (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib71">van Santen and Sperling, 1985</xref>). A simple algebraic identity shows that the HRC and motion energy models are computationally equivalent.</p><p>Research on <italic>Drosophila</italic>'s motion detection system has progressed quickly in recent years. With an influx of novel genetic, anatomical, and physiological tools, <italic>Drosophila</italic> researchers are able to perform experiments that have revealed an intricate neural circuit whose details were not anticipated. For example, separate pathways process the motion of light and dark moving edges (<xref ref-type="bibr" rid="bib29">Joesch et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="bib39">Maisak et al., 2013</xref>), and different neurons within these pathways coordinate the motion response depending on the velocity of motion (<xref ref-type="bibr" rid="bib2">Ammer et al., 2015</xref>). Furthermore, connectomic analysis has revealed that more spatial and temporal channels converge onto the fly's motion computing neurons than had been predicted by the HRC's two-input architecture (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>). Going forward, it is critical that the field discovers which of these circuit details are computationally relevant and which are not. Since many of these details go beyond the HRC's premise, we must consider alternate theories if we hope to understand how circuit details contribute to motion estimation.</p><p>A large body of theoretical and experimental work supports the hypothesis that visual systems are tailored for functionality in the animal's natural behavioral context (<xref ref-type="bibr" rid="bib60">Simoncelli and Olshausen, 2001</xref>). For example, photoreceptors adapt effectively across the ecological range of light levels (<xref ref-type="bibr" rid="bib31">Juusola and Hardie, 2001</xref>), the excess number of OFF vs ON retinal ganglion cells matches the excess information of dark vs light contrasts in natural images (<xref ref-type="bibr" rid="bib51">Ratliff et al., 2010</xref>), and several learning algorithms predict receptive fields similar to early cortical neurons when applied to natural images (<xref ref-type="bibr" rid="bib45">Olshausen and Field, 1996</xref>; <xref ref-type="bibr" rid="bib5">Bell and Sejnowski, 1997</xref>). These examples are special cases of the general hypothesis that the early visual system provides an efficient code for the natural visual environment, and recent research suggests that efficient coding accounts for certain aspects of higher-level coding and perception as well (<xref ref-type="bibr" rid="bib67">Tkačik et al., 2010</xref>; <xref ref-type="bibr" rid="bib26">Hermundstad et al., 2014</xref>; <xref ref-type="bibr" rid="bib73">Yu et al., 2015</xref>).</p><p>Several recent studies have established connections between the biological algorithms used for visual motion estimation and the statistical demands of naturalistic motion estimation. Natural stimuli are intricately structured and light–dark asymmetric (<xref ref-type="bibr" rid="bib21">Geisler, 2008</xref>), and a variety of low and high order correlations characterize motion in such an environment. Although the HRC and motion energy models only respond to pairwise correlations in their inputs (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib71">van Santen and Sperling, 1985</xref>), the Bayes optimal visual motion estimator also incorporates a variety of higher-order correlations of both even and odd order (<xref ref-type="bibr" rid="bib49">Potters and Bialek, 1994</xref>; <xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>). Accordingly, certain visual stimuli that contain only higher-order correlations induce motion percepts in both vertebrates and insects (<xref ref-type="bibr" rid="bib11">Chubb and Sperling, 1988</xref>; <xref ref-type="bibr" rid="bib50">Quenzer and Zanker, 1991</xref>; <xref ref-type="bibr" rid="bib74">Zanker, 1993</xref>; <xref ref-type="bibr" rid="bib46">Orger et al., 2000</xref>; <xref ref-type="bibr" rid="bib27">Hu and Victor, 2010</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>), and theoretical work shows that the correlations that characterize these stimuli can also improve motion estimation in natural environments (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>). This demonstrates that neither the HRC nor the motion energy model can account for the totality of experimentally observed motion percepts and suggests that departures from these canonical models might improve motion estimation accuracy. Relatively little is known about the neural basis of these higher-order motion percepts, although several studies have suggested intriguing commonalities across insect and primate species (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Nitzany et al., 2014</xref>).</p><p>Here we investigate whether the computational demands imposed by accurate motion estimation in natural environments can illuminate the unexpected details of <italic>Drosophila</italic>'s motion estimation circuit or account for non-Reichardtian motion perception in flies. We study a sequence of five computational models, each of which considers a conceptually new aspect of the motion estimation problem. Since each model succeeds in improving estimation accuracy, these results provide a range of nonlinear circuit mechanisms that flies and other animals might incorporate into their motion estimators. We describe how observed elements of <italic>Drosophila</italic>'s motion estimation circuitry could support such computations (<xref ref-type="table" rid="tbl1">Table 1</xref>). Importantly, four of the five models also predict the signs and approximate magnitudes of known non-Reichardtian motion percepts in flies. Since the models were tuned exclusively for estimation accuracy, these results support the view that non-Reichardtian motion percepts probe ethologically relevant aspects of biological motion estimators. More generally, our results posit normative interpretations for some unexpected aspects of the fly's motion estimation circuit and behavior and suggest that non-Reichardtian aspects of fly circuitry and behavior might be closely linked through the statistics of natural scenes.<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.003</object-id><label>Table 1.</label><caption><p>The different models used in this paper, experimental results that support each model, and references for those results</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.003">http://dx.doi.org/10.7554/eLife.09123.003</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Supporting evidence</th></tr></thead><tbody><tr><td rowspan="2">Front-end nonlinearity</td><td>• Photoreceptors show nonlinear responses to contrast changes (<xref ref-type="bibr" rid="bib38">Laughlin, 1989</xref>; <xref ref-type="bibr" rid="bib32">Juusola and Hardie, 2001</xref>; <xref ref-type="bibr" rid="bib31">Juusola and Hardie, 2001</xref>; <xref ref-type="bibr" rid="bib69">van Hateren and Snippe, 2006</xref>)</td></tr><tr><td>• Some neurons in the early visual system have nonlinear responses that make their output signals nearly uniform (<xref ref-type="bibr" rid="bib37">Laughlin, 1981</xref>)</td></tr><tr><td rowspan="3">Weighted 4-quadrant model</td><td>• Visual processing is divided early into ON and OFF channels (<xref ref-type="bibr" rid="bib29">Joesch et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Meier et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Strother et al., 2014</xref>)</td></tr><tr><td>• The two output channels (T4/T5) are sensitive to light and dark edges (<xref ref-type="bibr" rid="bib39">Maisak et al., 2013</xref>), but their inputs are incompletely rectified (<xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>)</td></tr><tr><td>• Stimuli targeting the four quadrants are differentially represented in neural substrates (<xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Joesch et al., 2013</xref>)</td></tr><tr><td rowspan="2">Non-multiplicative nonlinearity</td><td>• Pure multiplication is not a trivial neural operation (<xref ref-type="bibr" rid="bib35">Koch, 2004</xref>)</td></tr><tr><td>• Inputs to T4/T5 are nonlinearly transformed (<xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>), which also contributes to the biologically implemented non-multiplicative nonlinearity</td></tr><tr><td rowspan="2">Unrestricted nonlinearity</td><td>• The direction-selective neurons T4 receive inputs from more than two types of neurons (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>; <xref ref-type="bibr" rid="bib2">Ammer et al., 2015</xref>)</td></tr><tr><td>• T4 receives inputs from both its major input channels at overlapping points in space (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>)</td></tr><tr><td>Extra input nonlinearity</td><td>• The direction-selective neuron T4 receives inputs from more than two discrete retinotopic locations (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>)</td></tr></tbody></table></table-wrap></p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Flies incorporate motion signals that the HRC neglects</title><p>The HRC is the dominant model of motion computation in flies and other insects. In this paper we describe several generalizations of the HRC, but it is helpful to first review this canonical model. The HRC comprises three stages of processing. First, two different temporal filters (here, a low-pass filter <inline-formula><mml:math id="inf51"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and a high-pass filter <inline-formula><mml:math id="inf52"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) are applied to each of two spatially filtered visual input streams (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, ‘Materials and methods’). These four filtered signals are then paired and multiplied (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Finally, the HRC takes the difference between the two multiplied signals to obtain a mirror anti-symmetric motion estimator (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Because the HRC combines its two input channels via a multiplication operation, the average output of the HRC depends only on 2-point correlations in the visual stimulus. We thus refer to the HRC as a 2-point correlator, and we will return to this mathematical characterization of the HRC repeatedly throughout this work.<fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.004</object-id><label>Figure 1.</label><caption><title>The Hassenstein-Reichardt correlator (HRC) model is an incomplete description of <italic>Drosophila</italic>'s motion estimator.</title><p>(<bold>A</bold>) Diagram of the HRC model. (<bold>B</bold>) We assessed motion estimation performance across an ensemble of naturalistic motions, each of which consisted of a natural image (<xref ref-type="bibr" rid="bib70">van Hateren and van der Schaaf, 1998</xref>) and a velocity chosen from a normal distribution. (<bold>C</bold>) We quantified model accuracy by comparing the model response to the true velocity using the mean squared error. (<bold>D</bold>) We summarized the error with the correlation coefficient between the model output and the true velocity. (<bold>E</bold>) In previous work (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>), we used a panoramic display and spherical treadmill to measure the rotational responses of <italic>Drosophila</italic> to visual stimuli. (<bold>F</bold>) We presented flies with binary stimuli called gliders (<xref ref-type="bibr" rid="bib27">Hu and Victor, 2010</xref>), which imposed specific 2-point and 3-point correlations (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>). (<bold>G</bold>) Flies turned in response to 3-point glider stimuli, but these responses cannot be predicted by the standard HRC. (<bold>H</bold>) Diagram of the converging 3-point correlator, which is designed to detect higher-order motion signals like those found in 3-point glider stimuli. (<bold>I</bold>) Adding the converging 3-point correlator to the HRC improved motion estimation performance with naturalistic inputs. We optimized weighting coefficients to minimize the mean squared error over the ensemble of naturalistic motions and used cross-validation to protect against over-fitting. (<bold>J</bold>) This model predicted that <italic>Drosophila</italic> would weakly turn in response to 3-point glider stimuli.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.004">http://dx.doi.org/10.7554/eLife.09123.004</ext-link></p></caption><graphic xlink:href="elife-09123-fig1-v2.tif"/></fig></p><p>No motion estimator is perfect for every stimulus, and this paper explores the hypothesis that evolution has tuned <italic>Drosophila</italic>'s motion estimator for visual experiences that are likely to result from ordinary behavior in natural environments (<xref ref-type="app" rid="app1">Appendix 1</xref>). We assessed the accuracy of the HRC and other motion estimators by approximating naturalistic motion as the rigid translation of natural images (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>), with a velocity distribution that mimicked <italic>Drosophila</italic>'s natural behavior (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, ‘Materials and methods’) (<xref ref-type="bibr" rid="bib34">Katsov and Clandinin, 2008</xref>). We spatiotemporally filtered the input signals to simulate the responses of two neighboring photoreceptors (‘Materials and methods’). We quantified the performance of each model as the mean squared error between the input velocity and model output. However, we report each model's accuracy as the correlation coefficient between its output and the true velocity (<xref ref-type="fig" rid="fig1">Figure 1C</xref>), an intuitive metric that is equivalent to the mean squared error for correctly scaled model outputs (‘Materials and methods’). In isolation, the local HRC was weakly correlated with the velocity of motion (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). Although the HRC's performance can be improved by averaging over space and time (<xref ref-type="bibr" rid="bib15">Dror et al., 2001</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>), this study explores how alternate nonlinear processing can improve motion estimation accuracy without sacrificing spatial or temporal resolution (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>).</p><p>Researchers can probe a fly's motion estimate by measuring its behavioral optomotor turning response (<xref ref-type="bibr" rid="bib25">Hassenstein and Reichardt, 1956</xref>; <xref ref-type="bibr" rid="bib23">Götz and Wenking, 1973</xref>; <xref ref-type="bibr" rid="bib9">Buchner, 1976</xref>; <xref ref-type="bibr" rid="bib52">Reichardt and Poggio, 1976</xref>). We previously measured optomotor responses from flies walking on a spherical treadmill by recording their turning responses to various visual stimuli (<xref ref-type="fig" rid="fig1">Figure 1E</xref>) (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>). We emphasized binary stimuli called gliders (<xref ref-type="bibr" rid="bib27">Hu and Victor, 2010</xref>) (<xref ref-type="fig" rid="fig1">Figure 1F</xref>), which enforce spatiotemporal correlations to interrogate the fly's motion estimation algorithm. For example, 2-point gliders contain only 2-point correlations (<italic>first two stimuli</italic>, <xref ref-type="fig" rid="fig1">Figure 1F</xref>). <italic>Drosophila</italic> turned in response to these stimuli (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>) (<italic>black bars</italic>, <italic>left</italic>, <xref ref-type="fig" rid="fig1">Figure 1G</xref>), and the HRC correctly predicted that flies would respond to both positive and negative 2-point correlations (<italic>gray bars, left,</italic> <xref ref-type="fig" rid="fig1">Figure 1G</xref>). On the other hand, 3-point gliders contain 3-point correlations without 2-point correlations (<italic>last four stimuli</italic>, <xref ref-type="fig" rid="fig1">Figure 1F</xref>). These stimuli generated motion responses in flies (<italic>black bars</italic>, <italic>right</italic>, <xref ref-type="fig" rid="fig1">Figure 1G</xref>) that the HRC could not explain (<italic>gray bars</italic>, <italic>right</italic>, <xref ref-type="fig" rid="fig1">Figure 1G</xref>). Thus, behavioral responses to glider stimuli show that the HRC is an incomplete description of fly motion estimation and provide a useful benchmark for evaluating alternate models.</p><p>In this study, we tune our models to optimize motion estimation accuracy, rather than to fit the behavioral data, for two main reasons. First, we want to explore the hypothesis that <italic>Drosophila</italic>'s glider responses follow from performance optimization within biologically plausible circuit architectures. Second, we seek models that will generalize well across visual stimuli, and the measured glider responses under-constrain possible motion estimation models. It's useful to illustrate our procedure with a simple example. The HRC does not account for 3-point glider responses because it is insensitive to 3-point correlations. Nevertheless, 3-point correlations are present in natural stimuli (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Nitzany and Victor, 2014</xref>), and their use might facilitate accurate motion estimation. We can explore this hypothesis by summing the HRC with a motion estimator designed to respond specifically to 3-point correlations. For instance, the mirror anti-symmetric ‘converging’ 3-point correlator multiplies one high-pass filtered signal with two low-pass filtered signals (<xref ref-type="fig" rid="fig1">Figure 1H</xref>) and mimics the converging structure present in certain glider stimuli (<italic>last two stimuli</italic>, <xref ref-type="fig" rid="fig1">Figure 1F</xref>). We tune the model for motion estimation accuracy by choosing the weights of the HRC and the converging 3-point correlator to minimize the mean squared error (‘Materials and methods’). The resulting model is more accurate than the HRC (<xref ref-type="fig" rid="fig1">Figure 1I</xref>) and it predicts that flies should respond to glider stimuli in the observed directions (<xref ref-type="fig" rid="fig1">Figure 1J</xref>, ‘Materials and methods’). Nevertheless, this simple model underestimates 3-point turning magnitudes (<xref ref-type="fig" rid="fig1">Figure 1J</xref>), indicating a discrepancy between the fly's motion estimator and this performance-optimized model.</p><p>In this study, we apply this same basic model building procedure to a series of increasingly general model architectures. There are four benefits to this approach. First, each model incorporates a type of computation that was neglected by earlier models. Thus, we can compare model accuracies to quantify how important various computations are for naturalistic motion estimation. Second, each model has a distinct biological interpretation in terms of <italic>Drosophila</italic>'s motion estimation circuit (<xref ref-type="table" rid="tbl1">Table 1</xref>). This allows us to enumerate many directions for future experimental and computational research. Third, this set of models reveals several distinct principles of accurate naturalistic motion estimators, yet no single model illustrates every principle. Finally, by comparing the glider predictions of each model to behavioral data, we can gain insight into which principles underlie <italic>Drosophila</italic>'s known glider responses.</p></sec><sec id="s2-2"><title>Nonlinear preprocessing of HRC inputs improves estimation but poorly predicts responses to gliders</title><p>The HRC correlates pairs of photoreceptor signals (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We previously assumed that each photoreceptor's response was generated from incoming contrast signals through linear spatiotemporal filtering. However, real photoreceptors are linear only over a limited range of inputs (<xref ref-type="bibr" rid="bib37">Laughlin, 1981</xref>; <xref ref-type="bibr" rid="bib31">Juusola and Hardie, 2001</xref>) (<xref ref-type="table" rid="tbl1">Table 1</xref>). Our first model thus modifies the HRC by allowing the photoreceptor responses to become nonlinear (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). More specifically, we consider models in which a static nonlinearity transforms the filtered contrast signals before a standard HRC is applied to the two input streams (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, ‘Materials and methods’). Since the nonlinearity occurs before the HRC, we refer to this model as the <italic>front-end nonlinearity</italic> model. By nonlinearly transforming the contrast signals, the front-end nonlinearity model is able to reshape natural sensory statistics. In particular, linear photoreceptor signals inherit complex non-Gaussian statistics from their natural inputs (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), but front-end nonlinearities (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) can produce transformed signals with alternate statistics (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, ‘Materials and methods’). Thus, optimal front-end nonlinearity models should reshape natural statistics into those that best suit the HRC. Previous studies have already demonstrated example front-end nonlinearity models that improve naturalistic motion processing by the HRC (<xref ref-type="bibr" rid="bib15">Dror et al., 2001</xref>; <xref ref-type="bibr" rid="bib7">Brinkworth and O'Carroll, 2009</xref>). Here we provide new theoretical insight into these improvements and their consequences for glider responses.<fig-group><fig id="fig2" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.005</object-id><label>Figure 2.</label><caption><title>Front-end nonlinearities improved naturalistic motion estimation but did not reproduce the psychophysical results.</title><p>(<bold>A</bold>) Diagram of the front-end nonlinearity model. The nonlinearity occurs after the spatiotemporal filtering of photoreceptors but before the temporal filtering of the HRC. (<bold>B</bold>) The distribution of contrast signals after photoreceptor filtering had a kurtosis of 9.6. The kurtosis of unfiltered pixels in the image database was 7.8. (<bold>C</bold>) Three different nonlinearities that transformed this input distribution into a Gaussian distribution, a uniform distribution, and a binary distribution. (<bold>D</bold>) After these transformations, the kurtosis of the contrast signal was reduced to 3, 1.8, and 1, respectively. (<bold>E</bold>) Each front-end nonlinearity model improved the HRC's estimation accuracy, and uniform output signals worked best. (<bold>F</bold>, <bold>G</bold>) The front-end nonlinearity models reproduced the sign of the negative 2-point glider psychophysical responses but did not reproduce the pattern of psychophysical responses to 3-point gliders.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.005">http://dx.doi.org/10.7554/eLife.09123.005</ext-link></p></caption><graphic xlink:href="elife-09123-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09123.006</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Front-end nonlinearities modify the correlations present in natural scenes.</title><p>(<bold>A</bold>) Example images with no front-end nonlinearity (top), with an equalizing front-end nonlinearity (middle), and with a binarizing front-end nonlinearity (bottom). (<bold>B</bold>) The covariance between contrasts at 2 horizontally separated points is plotted as a function of distance between the points. The binary nonlinearity attenuated spatial correlations.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.006">http://dx.doi.org/10.7554/eLife.09123.006</ext-link></p></caption><graphic xlink:href="elife-09123-fig2-figsupp1-v2.tif"/></fig></fig-group></p><p>Although the statistics of natural images are complicated, the mean squared error between the HRC's output and the velocity of motion depends only on a few statistical quantities. Since the HRC is a 2-point correlator, the mean velocity signal decoded by an HRC is determined by the second-order statistics of the image ensemble (<xref ref-type="bibr" rid="bib15">Dror et al., 2001</xref>). The variance of the motion signal comes from the square of a quadratic signal, and thus the noise statistics of the HRC depend on the fourth-order statistics of the image ensemble (<xref ref-type="app" rid="app2">Appendix 2</xref>). If the image ensemble is spatially uncorrelated, the situation simplifies further and the correlation between the estimated and true image velocity is determined entirely by the standardized fourth central moment of the input streams, a quantity known as kurtosis (<xref ref-type="app" rid="app3">Appendix 3</xref>). A larger kurtosis results in a larger error in the motion estimate. Note that some authors use ‘kurtosis’ to refer to the ‘excess kurtosis’, which shifts kurtosis values such that the Gaussian distribution has zero excess kurtosis. This shift is not relevant for our purposes. Because large positive contrasts are relatively probable, naturalistic inputs are highly kurtotic (kurtosis = 9.6 for the spatiotemporal filtering in our simulations) and are thus expected to hinder HRC performance (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><p>The Gaussian, uniform, and symmetric Bernoulli distributions have much lower kurtosis values (kurtosis = 3.0, 1.8, 1.0, respectively, <xref ref-type="fig" rid="fig2">Figure 2D</xref>). In fact, the symmetric Bernoulli distribution has the lowest kurtosis of any probability distribution (<xref ref-type="bibr" rid="bib14">DeCarlo, 1997</xref>). When we transformed the HRC's inputs to have these statistics (‘Materials and methods’), we found that each nonlinearity substantially improved the accuracy of the HRC (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). The <italic>contrast equalizing</italic> nonlinearity, which produces uniform outputs, performed best and also plays a prominent role in efficient coding theory (<xref ref-type="bibr" rid="bib37">Laughlin, 1981</xref>). It is interesting that contrast equalization improved the accuracy of the HRC more than binarization (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), even though it produced outputs with greater kurtosis. The reason for this is that natural images are spatially correlated, and the accuracy of the HRC over a general image ensemble depends on the ensemble's spatial correlation structure (<xref ref-type="app" rid="app2">Appendix 2</xref>). Binarization attenuated spatial correlations more strongly than contrast equalization over the natural image ensemble (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), and spatial correlations can enhance the performance of the HRC (<xref ref-type="app" rid="app4 app5">Appendices 4, 5</xref>). Designing a nonlinearity that optimally sculpts the correlation structure of natural images is not simple and goes beyond the scope of this study.</p><p>Each front-end nonlinearity model is sensitive to a variety of higher-order correlations (<xref ref-type="app" rid="app6">Appendix 6</xref>). We thus tested whether accurate front-end nonlinearity models would predict <italic>Drosophila</italic>'s glider response pattern. However, each front-end nonlinearity model performed poorly at this task (<xref ref-type="fig" rid="fig2">Figure 2F,G</xref>). None of the three models predicted that <italic>Drosophila</italic> would invert its response to positive and negative 3-point gliders (<xref ref-type="fig" rid="fig2">Figure 2G</xref>), even though they predicted that the 3-point glider responses would be nonzero. The simplest explanation for this observation is that the front-end nonlinearity models responded to fourth-order correlations that are common to the stimuli, rather than the third-order correlations that defined the glider stimuli and primarily drove the experimental response (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>). Mechanistically, this result follows from the fact that the nonlinearities that reduced kurtosis (<xref ref-type="fig" rid="fig2">Figure 2C</xref>) were not strongly asymmetric around zero contrast (<xref ref-type="app" rid="app6">Appendix 6</xref>). The binarizing front-end nonlinearity model also failed to predict that <italic>Drosophila</italic> would respond less to negative 2-point glider stimuli than positive 2-point glider stimuli (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). Since this effect was correctly predicted by the standard HRC (<xref ref-type="fig" rid="fig1">Figure 1G</xref>), this observation shows that accurate front-end nonlinearity models can distort the processing of 2-point correlations. Although the front-end nonlinearity model did not explain the phenomenon of fly glider perception, future work should investigate whether its merits make it functionally relevant for motion processing in other contexts or species.</p></sec><sec id="s2-3"><title>Separating ON and OFF signals improves motion estimation and predicts responses to gliders</title><p>Instead of a front-end nonlinearity, <italic>Drosophila</italic> could use an alternative non-Reichardtian motion estimation strategy that reflects natural sensory statistics, without necessarily requiring nonlinear preprocessing. Previous computational analyses show that motion estimation strategies that distinguish light and dark information can enhance motion processing with natural inputs (<xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>; <xref ref-type="bibr" rid="bib42">Nitzany and Victor, 2014</xref>), and recent experiments indicate that flies use separate channels to process the motion of light and dark edges (<xref ref-type="bibr" rid="bib29">Joesch et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Meier et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Strother et al., 2014</xref>) (<xref ref-type="table" rid="tbl1">Table 1</xref>). Our next model explores the hypothesis that <italic>Drosophila</italic> segregates ON and OFF signals in order to facilitate naturalistic motion estimation (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>) (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, ‘Materials and methods’). There are four ways to pair the ON and OFF components of the two filtered signals that enter the HRC's multiplier. For example, one possibility is to pair the ON component of the low-pass filtered signals with the OFF component of the high-pass filtered signal. Since each pairing restricts the HRC's multiplier to a single quadrant of the Cartesian plane, we refer to these four signals as HRC-quadrants. If the quadrants are summed with equal weights, then this model is mathematically identical to the HRC (<xref ref-type="bibr" rid="bib25">Hassenstein and Reichardt, 1956</xref>; <xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>). Unequal weighting coefficients enable the motion estimator to prioritize some quadrants over others, and here we select quadrant weightings that minimize the mean squared error between the model output and velocity (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, ‘Materials and methods’). More generally, we refer to any model that linearly combines the four HRC-quadrants as a <italic>weighted 4-quadrant</italic> model. The precise manner in which the four HRC-quadrants might map onto circuitry remains unclear; we do not suggest there exists separate circuitry for each quadrant. For instance, studies have identified only two motion-processing channels in the <italic>Drosophila</italic> brain, which might suggest that the fly only uses a subset of the quadrants (<xref ref-type="bibr" rid="bib17">Eichner et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Joesch et al., 2013</xref>; <xref ref-type="bibr" rid="bib39">Maisak et al., 2013</xref>). On the other hand, each channel appears imperfectly selective for light vs dark signals (<xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>), which in principle enables these two channels to access all four quadrants (<xref ref-type="table" rid="tbl1">Table 1</xref>).<fig-group><fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.007</object-id><label>Figure 3.</label><caption><title>The weighted 4-quadrant model improved estimation performance and reproduced the directionality of psychophysical results.</title><p>(<bold>A</bold>) Diagram of the weighted 4-quadrant model. Similar to ON/OFF processing in the visual system, the weighted 4-quadrant model splits the four differentially filtered signals into positive and negative components. As in the HRC, these component signals are paired, multiplied, and subtracted to produce four mirror anti-symmetric signals. We refer to these signals as HRC-quadrants. The model output is a weighted sum of the quadrant signals. We identify quadrants by whether they respond to the positive or negative components of each filtered signal and denote the four quadrants as (+ +), (+ −), (− +), and (− −). In this notation, the first index refers to the sign of the low-pass filtered signal (emanating from <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>), and the second refers to the high-pass filtered signal (emanating from <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) We measured the response of each quadrant to naturalistic motions and chose the quadrant weightings to minimize the mean squared error between the model output and the true velocity. (<bold>C</bold>) Comparison of the estimation performance of individual quadrants, multiple quadrants, and the HRC. The best two quadrants were (− −) and (− +); the best three also included (+ −). (<bold>D</bold>) The performance-optimized weighted 4-quadrant model reproduced the signs and approximate magnitudes of the psychophysical results.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.007">http://dx.doi.org/10.7554/eLife.09123.007</ext-link></p></caption><graphic xlink:href="elife-09123-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09123.008</object-id><label>Figure 3—figure supplement 1.</label><caption><title>Separate ON and OFF processing improved motion estimation by supplementing the HRC with odd-ordered correlations.</title><p>(<bold>A</bold>) By summing and subtracting the four quadrants (top labels, e.g., ‘++’) in four different patterns, we isolated the contributions of various correlation types (side labels, e.g., ‘odd’) to the weighted 4-quadrant model (<xref ref-type="app" rid="app7">Appendix 7</xref>). For example, the uniform sum of the four quadrants is the HRC, and we denote this quadrant combination as ‘even = 2’ (top row of matrix). The other three rows of the matrix define quadrant combinations that are sensitive to two different classes of third and higher odd-ordered correlations (‘odd’ and ‘odd*’ rows) and to fourth and higher even-ordered correlations (‘even &gt;2’ row). The factor of 1/4 merely sets the magnitude of the quadrant contributions to match the formulas in <xref ref-type="app" rid="app7">Appendix 7</xref> and is without conceptual importance. (<bold>B</bold>) The ‘even = 2’ correlation class worked best in isolation. Nevertheless, the ‘even = 2’ and ‘odd’ classes were highly synergistic (their weighted sum is notated ‘best 2’), and these classes together made the ‘odd*’ and the ‘even &gt;2’ classes irrelevant.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.008">http://dx.doi.org/10.7554/eLife.09123.008</ext-link></p></caption><graphic xlink:href="elife-09123-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09123.009</object-id><label>Figure 3—figure supplement 2.</label><caption><title>The weighted 4-quadrant model cannot reproduce the positive-negative parity asymmetry in the psychophysical data.</title><p>In this numerical experiment, we tuned the coefficients of the weighted 4-quadrant model to optimize a fit to the psychophysical data. (<bold>A</bold>) The tuned model could reproduce the 2-point glider data well. (<bold>B</bold>) Although the tuned weighted 4-quadrant model could reproduce the signs of the 3-point glider data, it could not reproduce the differential amplitudes of the positive and negative parity responses. This demonstrates that the architecture of the weighted 4-quadrant model is too limited to reproduce the experimental response pattern.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.009">http://dx.doi.org/10.7554/eLife.09123.009</ext-link></p></caption><graphic xlink:href="elife-09123-fig3-figsupp2-v2.tif"/></fig></fig-group></p><p>We began by examining how well individual quadrants predicted the velocity of motion. The four quadrants provided motion signals of strikingly different quality (<italic>first four red bars</italic>, <xref ref-type="fig" rid="fig3">Figure 3C</xref>). The most accurate quadrant correlated negative low-pass filtered signals with negative high-pass filtered signals ((− −) <italic>bar</italic>, <xref ref-type="fig" rid="fig3">Figure 3C</xref>). This isolated quadrant already outperformed the full HRC. The quadrant that correlated negative low-pass filtered signals with positive high-pass filtered signals also performed relatively well ((− +) <italic>bar</italic>, <xref ref-type="fig" rid="fig3">Figure 3C</xref>), whereas the quadrants that involved positive low-pass filtered signals performed poorly ((+ +) and (+ −) <italic>bars</italic>, <xref ref-type="fig" rid="fig3">Figure 3C</xref>). This shows that negative signals emanating from the low-pass filter better facilitate motion estimation, and the HRC's uniform weighting of all four quadrants is computationally detrimental.</p><p>We next considered all subsets of two, three, or four quadrants. The best subsets for each number of predictors were nested, and the quadrants were incorporated in the order (i) (− −); (ii) (− +); (iii) (+ +); (iv) (+ −). Although all four quadrants enhanced the accuracy of the weighted 4-quadrant model, the benefit of each added quadrant decreased with the number of quadrants (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). It is possible to reparameterize the weighted 4-quadrant model in a form that isolates the contributions of various higher-order correlations to the model's accuracy (<xref ref-type="app" rid="app7">Appendix 7</xref>). Interestingly, this parameterization showed that nearly all the accuracy of the weighted 4-quadrant model can be obtained by supplementing the HRC with a set of odd-ordered correlations that account for the asymmetry between positive and negative low-pass filtered signals (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="app" rid="app8">Appendix 8</xref>). Principal component analysis (PCA) did not reveal this simple interpretation of the model's computation (<xref ref-type="app" rid="app9">Appendix 9</xref>).</p><p>The performance-optimized weighted 4-quadrant model also offered an interesting interpretation of <italic>Drosophila</italic>'s glider response pattern. First note that the model preserved the HRC's response pattern to 2-point glider stimuli (compare <italic>left</italic> subpanels of <xref ref-type="fig" rid="fig3">Figure 3D</xref> and <xref ref-type="fig" rid="fig1">Figure 1G</xref>). More interestingly, the model predicted behavioral responses to 3-point glider stimuli that matched the experimentally observed turning directions, and even the response magnitudes were similar between the model and the data (<italic>right</italic>, <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Nevertheless, the model's predictions were imperfect. The primary qualitative discrepancy was that the model failed to predict that positive 3-point glider stimuli would generate smaller turning responses than negative 3-point glider stimuli. The simplest interpretation for this experimental result is that flies might incorporate both 3-point correlations and 4-point correlations into their motion estimation strategy. In particular, since the positive and negative 3-point glider stimuli have inverted 3-point correlations and matched 4-point correlations, third-order and fourth-order correlations would have the same sign for one parity and opposite signs for the other parity. This observation makes it easier to understand the glider predictions of the weighted 4-quadrant model. The optimized model does a good job accounting for the direction and approximate magnitude of the glider responses because it draws heavily on second-order and odd-order correlations, but it fails to predict the 3-point glider magnitude asymmetry because it finds little added utility in higher-order even correlations (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, <xref ref-type="app" rid="app8">Appendix 8</xref>). This failure stems from architectural limitations in the weighted 4-quadrant model (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>), so it is important to consider alternate model classes.</p></sec><sec id="s2-4"><title><italic>Drosophila</italic> circuitry contains additional elements that might facilitate motion estimation</title><p>The previous section suggested that the segregation of light and dark signals by <italic>Drosophila</italic>'s motion estimation circuitry might enhance naturalistic motion estimation in a manner that also generates the observed glider responses. In this section, we introduce three hierarchical models to investigate other features of <italic>Drosophila</italic>'s circuit that might have functional consequences for the processing of natural stimuli and gliders (<xref ref-type="table" rid="tbl1">Table 1</xref>). We refrain from modifying the temporal filtering of the motion estimator, and we focus on its nonlinear architecture.</p><p>The first of these models recasts the HRC and the weighted 4-quadrant model in a more general architecture. This model is the class of mirror anti-symmetric models that apply a 2-dimensional nonlinearity to the low-pass filtered signal from one point in space and the high-pass filtered signal from a neighboring point in space (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Since the observed glider responses indicate that flies use higher-order correlations of both even and odd order, we model this 2-dimensional nonlinearity as a fourth-order polynomial (‘Materials and methods’). The HRC corresponds to the special case of this nonlinearity that multiplies the two inputs (<italic>left</italic>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>). To emphasize how the model class in <xref ref-type="fig" rid="fig4">Figure 4A</xref> generalizes the HRC, we refer to it as the <italic>non-multiplicative nonlinearity</italic> model. In comparison, the weighted 4-quadrant model corresponds to a different nonlinearity that separately scales a pure multiplication in each quadrant of the Cartesian plane. Compared to the HRC, the optimized forms of both the weighted 4-quadrant model and the non-multiplicative nonlinearity model substantially attenuated positive low-pass filtered signals (<italic>middle</italic> and <italic>right</italic>, <xref ref-type="fig" rid="fig4">Figure 4B</xref>), though the non-multiplicative nonlinearity shows less attenuation. This model architecture provides enough flexibility to generate the glider response pattern (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.010</object-id><label>Figure 4.</label><caption><title>Several biologically motivated generalizations of the motion estimator further improved estimation performance without sacrificing glider responses.</title><p>See <xref ref-type="table" rid="tbl1">Table 1</xref> for a description of the biological rationales behind these models. (<bold>A</bold>) The ‘non-multiplicative nonlinearity’ model substitutes a 2-dimensional nonlinearity for the pure multiplication of the HRC. Here, we approximated the nonlinearity with a fourth order polynomial. (<bold>B</bold>) Two-dimensional nonlinearities underlying the HRC, the weighted 4-quadrant model, and the non-multiplicative nonlinearity model. The latter models reflect optimized cases, in which the weighting coefficients maximized estimation performance with natural inputs. Iso-output lines are shown in each plot, and the horizontal and vertical limits are chosen to include 95% of the naturalistic input signals. (<bold>C</bold>) Another generalization, the ‘unrestricted nonlinearity’ model allows all 4 input signals to be combined nonlinearly. We approximate this 4-dimensional nonlinearity with a fourth-order polynomial. (<bold>D</bold>) A final generalization, the ‘extra input nonlinearity’ model, relaxes the restriction that the motion estimator only uses 2 spatial inputs. We approximate this 6-dimensional nonlinearity with a fourth-order polynomial. (<bold>E</bold>) Comparison of the estimation performance of these models to the HRC. We compare the extra input nonlinearity model to the average of two neighboring motion estimators. (<bold>F</bold>, <bold>G</bold>) The three models correctly predicted the directions of psychophysical responses. The pattern of 3-point responses differed somewhat across the models, and the extra input nonlinearity model was the first to predict a large asymmetry between positive and negative 3-point glider responses.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.010">http://dx.doi.org/10.7554/eLife.09123.010</ext-link></p></caption><graphic xlink:href="elife-09123-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09123.011</object-id><label>Figure 4—figure supplement 1.</label><caption><title>The non-multiplicative nonlinearity model can be tuned to account for the positive-negative parity asymmetry in the psychophysical data.</title><p>In this numerical experiment, we tuned the model nonlinearity to optimize a fit to the psychophysical data. (<bold>A</bold>) In this case, the tuned model could reproduce the 2-point glider data well. (<bold>B</bold>) This tuned model could also reproduce the differential amplitudes of the positive and negative parity responses. Thus, the non-multiplicative nonlinearity model repairs an architectural defect of the weighted 4-quadrant model (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.011">http://dx.doi.org/10.7554/eLife.09123.011</ext-link></p></caption><graphic xlink:href="elife-09123-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09123.012</object-id><label>Figure 4—figure supplement 2.</label><caption><title>The performance of the non-multiplicative nonlinearity model is plotted against the order of the fitted polynomial.</title><p>With only zeroth or first-order terms, the model cannot predict motion. With second-order terms, it can perform slightly better than the HRC (<xref ref-type="app" rid="app10">Appendix 10</xref>). The biggest performance increase occurred when third-order terms were included, and the fourth-order terms also improved performance.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.012">http://dx.doi.org/10.7554/eLife.09123.012</ext-link></p></caption><graphic xlink:href="elife-09123-fig4-figsupp2-v2.tif"/></fig></fig-group></p><p>The non-multiplicative nonlinearity model relaxes some restrictions of the 4-quadrant model. This is prudent because the exact nonlinear transformations implemented by neural circuits in the <italic>Drosophila</italic> brain remain poorly understood. For example, T4 and T5 are the first direction-selective neurons in the fly brain (<xref ref-type="bibr" rid="bib39">Maisak et al., 2013</xref>), but the mechanism by which they become direction-selective is not yet known. Furthermore, neurons upstream of T4 and T5 imperfectly segregate light and dark information (<xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>) and show overlap between the two motion pathways (<xref ref-type="bibr" rid="bib58">Silies et al., 2013</xref>), suggesting that ON/OFF segregation may not be crisply realized. We will discuss this model's estimation accuracy and glider performance in the next section.</p><p><italic>Drosophila</italic>'s motion processing circuitry suggests two more generalizations of the non-multiplicative nonlinearity model. First, note that the non-multiplicative nonlinearity model inherits the HRC's assumption that each nonlinear unit only acts upon the low-pass filtered signal from one point in space and the high-pass filtered signal from the neighboring point (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). In contrast, the converging 3-point correlator (<xref ref-type="fig" rid="fig1">Figure 1H</xref>) shows that the accuracy of motion estimation can sometimes be enhanced by nonlinearly combining both low-pass filtered signals (<xref ref-type="fig" rid="fig1">Figure 1I</xref>). Moreover, connectomic evidence conflicts with the non-multiplicative nonlinearity model's constraints, because each T4 cell receives synaptic connections from both the Mi1 cell and the Tm3 cells (T4's two major input channels) at overlapping points in space (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>). The <italic>unrestricted nonlinearity</italic> model removes this restriction of the non-multiplicative nonlinearity model by allowing a 4-dimensional nonlinearity to act on all four filtered signals (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). Here, we again model this nonlinearity as a fourth-order polynomial (‘Materials and methods’). The unrestricted nonlinearity allows the motion estimator to nonlinearly combine multiple temporal channels from the same point in space. Recent experiments indicate that the Mi1 and Tm3 cells alone are insufficient to account for the motion processing of the T4 channel (<xref ref-type="bibr" rid="bib2">Ammer et al., 2015</xref>). Future work might generalize the unrestricted nonlinearity model to include three or more temporal channels at each point in space.</p><p>The models presented so far operate only on a pair of neighboring photoreceptors, and the final generalization incorporates a third point in space. Averaging EMDs over space improves the accuracy of whole-field motion estimation (<xref ref-type="bibr" rid="bib15">Dror et al., 2001</xref>), but <italic>Drosophila</italic>'s neural circuitry suggests that it might adopt a more sophisticated strategy to combine signals across space. In particular, single T4 cells receive synaptic inputs from Mi1 cells and Tm3 cells from more than two retinotopic columns (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>). This arrangement could allow the circuit to incorporate higher-order correlations that are distributed across three or more spatial input channels. To explore whether this possibility has computational significance, we generalized the unrestricted nonlinearity model to provide unrestricted access to six temporal channels distributed across three points in space (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). We refer to this model as the <italic>extra input nonlinearity</italic> model. We approximate its 6-dimensional nonlinearity as a fourth-order polynomial (‘Materials and methods’).</p></sec><sec id="s2-5"><title>Elaborated circuit architectures improve motion estimation without sacrificing glider responses</title><p>Having introduced the rationale behind the non-multiplicative, unrestricted, and extra input nonlinearity models, it is straightforward to examine their performance as motion estimators. First note that the polynomial non-multiplicative nonlinearity model was a better motion estimator (<xref ref-type="fig" rid="fig4">Figure 4E</xref>) than the weighted 4-quadrant model (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This implies that some useful signatures of naturalistic motion are not made accessible by simply segregating ON and OFF motion signals. Interestingly, this performance improvement is largely due to 3-point correlations, and models that exclude fourth-order polynomial terms still outperform the weighted 4-quadrant model (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Third-order correlations are only useful for motion estimation because of light–dark asymmetries in natural stimulus statistics (<xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>), so this result implies that ON/OFF segregation provides an imperfect way to account for the complexity of light–dark asymmetries found in the natural world. The non-multiplicative nonlinearity model also made novel use of low-order correlations to improve its motion estimate (<xref ref-type="app" rid="app10">Appendix 10</xref>).</p><p>The three models are hierarchical because the non-multiplicative nonlinearity model is a special case of the unrestricted nonlinearity model, which is itself a special case of the extra input nonlinearity model. Thus, we expect each model to perform at least as well as its predecessor, but it is possible that some circuit elaborations will not introduce useful computational cues. Nevertheless, we found that that the unrestricted nonlinearity model performed better than the non-multiplicative nonlinearity model, and the extra input model performed better than the average of two neighboring unrestricted nonlinearity models (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Therefore, both models incorporated novel computational signatures with relevance for visual motion estimation. Although the relative improvements were fairly small, it's worth noting that the improvement from spatial averaging is also small, and it is possible that the fly brain builds an accurate motion estimator by combining a large number of weak predictors of motion.</p><p>Each of these three generalized models predicted 2-point glider responses (<xref ref-type="fig" rid="fig4">Figure 4F</xref>) that closely resembled the standard HRC (<italic>left</italic>, <xref ref-type="fig" rid="fig1">Figure 1G</xref>). Each model also correctly predicted the experimental turning directions to each of the 3-point glider stimuli (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). The magnitudes of the 3-point glider turning responses did not unambiguously favor any of the three hierarchical models (<xref ref-type="fig" rid="fig4">Figure 4G</xref>) or the weighted 4-quadrant model (<italic>right</italic>, <xref ref-type="fig" rid="fig3">Figure 3D</xref>). Each model did better on some stimuli and worse on others. Nevertheless, the predicted glider responses did make several interesting points. First, the extra input nonlinearity model predicted a clear asymmetry between positive and negative 3-point gliders (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). This shows that some of the even-ordered correlations found in 3-point glider stimuli have relevance for naturalistic motion estimation. Second, the observation that each model provides qualitatively similar glider response patterns illustrates that animals could use multiple nonlinear mechanisms to access ethologically relevant higher-order correlations. Future experiments should directly assess the functional relevance of the different models in the hierarchy. Finally, the qualitative agreement between all of these predictions and the experimental data supports the general hypothesis that glider responses could reflect underlying nonlinear mechanisms that facilitate motion estimation in natural environments.</p></sec><sec id="s2-6"><title>The extra input nonlinearity model contains the conceptual content of the other considered models</title><p>In this paper, we sequentially introduced several models in order to isolate specific ideas about the relationships between <italic>Drosophila</italic>'s behavior, its motion estimation circuit, and the statistical demands of accurate motion estimation in natural environments. The front-end nonlinearity model explored an interesting candidate principle for visual motion estimation, but it conflicted sharply with fly behavior (<xref ref-type="fig" rid="fig2">Figure 2G</xref>) and excluded the conceptual insights offered by other models. For example, the front-end nonlinearities we considered eliminated the asymmetry between light and dark contrasts (<xref ref-type="fig" rid="fig2">Figure 2D</xref>), removing the need for separate ON and OFF processing. However, the remaining models embodied ideas that are complementary rather than exclusive, and these models should not be thought of as competitors. Instead, we will show here that the final, most general model incorporates the variety of conceptual points that were initially illustrated by specific models.</p><p>The structure of the non-multiplicative nonlinearity models can be directly plotted (<xref ref-type="fig" rid="fig4">Figure 4B</xref>), but is not easy to visualize the 6-dimensional nonlinearity that defines the extra input nonlinearity model. We therefore need an alternate technique to illustrate its computations. We proceed by leveraging three ideas. First, a wide variety of visual motion estimators can be expanded as an abstract series of multipoint correlators (e.g., see <xref ref-type="bibr" rid="bib48">Poggio and Reichardt, 1980</xref>, <xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>, and <xref ref-type="app" rid="app6 app7 app11">Appendices 6, 7, 11</xref>), and it is straightforward to pictorially represent a multipoint correlator (e.g., see <xref ref-type="fig" rid="fig1">Figure 1A,H</xref>, and more to come). In the extra input nonlinearity model, this expansion is immediate because we have already parameterized its 6-dimensional nonlinearity as a polynomial. Importantly, this expansion should be considered at the algorithmic level (<xref ref-type="bibr" rid="bib40">Marr and Poggio, 1976</xref>), and we do not suggest that the wiring of brain circuits will reflect a large number of higher-order correlators. To the contrary, a large number of higher-order multipoint correlators may be implemented implicitly by high-dimensional nonlinearities suggested by <italic>Drosophila</italic>'s visual circuitry. Second, we note that certain multipoint correlators can be recombined into a 2-dimensional non-multiplicative nonlinearity that facilitates easy comparisons with the HRC and weighted 4-quadrant models (e.g., see <xref ref-type="fig" rid="fig4">Figure 4B</xref>). Taken together, these two points mean that we can represent the performance-optimized extra input nonlinearity model in terms of non-multiplicative nonlinearity models and multipoint correlators, each of which are easy to represent graphically.</p><p>This graphical representation could be unwieldy because of the shear number of higher-order correlators in the model. Thus the third and final point is that we need a way to identify a relatively small number of terms that substantially improve the accuracy of motion estimation and illustrate the conceptual content of the model. To achieve this, we used lasso regression (<xref ref-type="bibr" rid="bib66">Tibshirani, 1996</xref>) to identify models with fewer multipoint correlators that still enabled accurate motion estimation (‘Materials and methods’). This analysis revealed that fewer than half of all multipoint correlators were needed to account for the full accuracy of the extra input nonlinearity model (<italic>rightmost bars</italic>, <xref ref-type="fig" rid="fig5">Figure 5A</xref>). In fact, the accuracy of naturalistic motion estimation increased rapidly as the few correlators were sequentially added (<italic>left bars</italic>, <xref ref-type="fig" rid="fig5">Figure 5A</xref>), and a model that used 16 out of the 209 possible predictors was already able to produce 74% of the gain offered by the full extra input nonlinearity model (<italic>red bar</italic>, <xref ref-type="fig" rid="fig5">Figure 5A</xref>).<fig-group><fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.013</object-id><label>Figure 5.</label><caption><title>Computational interpretation of the extra input nonlinearity model.</title><p>(<bold>A</bold>) We used lasso regression to select subsets of predictors that might enable accurate estimation (see ‘Materials and methods’). With only 16 predictors, the model improved naturalistic performance over the HRC by 68%, and including fewer than half of the predictors improved it by the full 92%. The maximum number of predictors corresponds to the number of polynomial coefficients that were fit in the full model. (<bold>B</bold>) We visualized the 6-dimensional nonlinearity as the sum of several simpler computational modules. When only 16 predictors were used (red bar in (<bold>A</bold>)), the model used four distinct types of computations. In particular, the model included nearest-neighbor and next-nearest-neighbor non-multiplicative nonlinearities (<italic>top row</italic>). It also included a converging 3-point correlator from the two furthest photoreceptors and a 4-point correlator that combined three spatial inputs (<italic>bottom row</italic>). (<bold>C</bold>) Venn diagram illustrating the hierarchical nesting of models used in this paper. All models in this paper contain sets of parameters that reproduce the HRC (gray dot). The weighted 4-quadrant model is a subset of non-multiplicative nonlinearity models, which are themselves a subset of unrestricted nonlinearity models. The extra input nonlinearity encompasses all the models. When we approximated the nonlinearites with fourth order polynomials, we restricted them to a smaller portion of the model space. The 4-quadrant nonlinearities only overlapped with the fourth-order polynomial approximation at the HRC, because the weighted 4-quadrant model is infinite order when expanded as a polynomial (see <xref ref-type="app" rid="app7">Appendix 7</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.013">http://dx.doi.org/10.7554/eLife.09123.013</ext-link></p></caption><graphic xlink:href="elife-09123-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.09123.014</object-id><label>Figure 5—figure supplement 1.</label><caption><title>Structure of non-multiplicative nonlinearities in the extra input model of <xref ref-type="fig" rid="fig5">Figure 5B</xref>.</title><p>(<bold>A</bold>) The nearest-neighbor non-multiplicative nonlinearity was made up of a standard HRC and a 3-point correlator in which the low-pass filtered input was squared before being multiplied by the adjacent receptor's high-pass filtered signal. (<bold>B</bold>) The next-nearest-neighbor non-multiplicative nonlinearity combined the analogous long-range terms. (<bold>C</bold>) Structure of the 2-dimensional nonlinearities, shown according to the same conventions as <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.014">http://dx.doi.org/10.7554/eLife.09123.014</ext-link></p></caption><graphic xlink:href="elife-09123-fig5-figsupp1-v2.tif"/></fig></fig-group></p><p>The leading 16 predictors compactly illustrated how the extra input nonlinearity model recapitulates the conceptual advances offered by the other models (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Four of the predictors combine to implement a mirror-symmetric non-multiplicative nonlinearity model that acts on the first and second points in space (<italic>first term</italic>, <xref ref-type="fig" rid="fig5">Figure 5B</xref>). The dominant contribution to the nonlinearity is the HRC's multiplier, but an additional third-order term breaks the symmetry between positive and negative low-pass filtered signals (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). Thus, the extra input nonlinearity model approximately correlates neighboring points in space, as the HRC would suggest, but it differentially weights positive and negative low-pass filtered signals, like the weighted 4-quadrant model. It also replicates the main insight from the non-multiplicative nonlinearity model: the best treatment of asymmetric light and dark information need not be as simple as pure ON/OFF segregation. The model used another eight predictors to construct two more non-multiplicative nonlinearity models, one that surveyed the second and third points in space and another that surveyed the first and third points (<italic>first and second terms</italic>, <xref ref-type="fig" rid="fig5">Figure 5B</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). These components make the previously highlighted conceptual points and add the observation that spatial averaging improves estimates.</p><p>The final four predictors implemented two mirror anti-symmetric multipoint correlators (<italic>third and fourth terms</italic>, <xref ref-type="fig" rid="fig5">Figure 5B</xref>). In particular, two predictors went towards implementing a converging 3-point correlator that spanned the first and third spatial points (<italic>third term</italic>, <xref ref-type="fig" rid="fig5">Figure 5B</xref>). This estimator made the model's asymmetric treatment of light and dark signals more nuanced than permitted by the non-multiplicative nonlinearity model, and it also incorporated motion signals that combine multiple temporal signals from the same point in space. This latter point was the main conceptual motivation for the unrestricted nonlinearity model. Finally, the last two predictors implemented a 4-point correlator that combined temporal signals from three distinct points in space (<italic>fourth term</italic>, <xref ref-type="fig" rid="fig5">Figure 5B</xref>). This component reinforces the conceptual motivation for the extra input nonlinearity model and gives a concrete example of a computationally relevant higher-order correlator that is distributed across three points in space. It's interesting that the leading fourth-order correlator spanned three spatial points, because the extra input nonlinearity model was the first performance-optimized model that generated a substantially asymmetric response to positive and negative 3-point gliders (<xref ref-type="fig" rid="fig4">Figure 4G</xref>).</p><p>This paper set out with the goal of exploring whether the statistical demands of naturalistic motion estimation could provide a useful lens for interpreting features of <italic>Drosophila</italic>'s behavior and neural circuitry that push beyond the canonical HRC. Although we have considered several interesting classes of visual motion estimators, the space of possible motion estimators is much larger (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). For instance, these models have not explored the impact of temporal filter choice on naturalistic motion estimation. Nor have they assessed the possibility of more than two temporal filters, which is be suggested by anatomical (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>) and physiological (<xref ref-type="bibr" rid="bib2">Ammer et al., 2015</xref>) experiments. More generally, the neural circuits contributing to <italic>Drosophila</italic>'s motion estimator are still incompletely known, and the extent to which the fly brain's biological complexity reflects computational sophistication remains an open question. Theoretical considerations will be critical for resolving that question and pinpointing the most relevant principles underlying visual motion estimation.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Ongoing research is providing an increasingly detailed picture of the anatomy and physiology of the visual circuitry that implements motion processing in <italic>Drosophila</italic>. Through the combination of genetic silencing experiments, connectomic analysis, and functional recordings, researchers have identified many individual neurons in the fly brain that contribute to visual motion processing (<xref ref-type="bibr" rid="bib59">Silies et al., 2014</xref>). Although the HRC provided the initial theoretical impetus for these experiments, specific experimental outcomes have often been unanticipated. For instance, the fly brain contains multiple pathways that segregate different types of motion information (<xref ref-type="bibr" rid="bib29">Joesch et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="bib58">Silies et al., 2013</xref>); its direction-selective neurons receive inputs from more than two neighboring points in visual space (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>); and the biological substrates for reverse-phi signals, which were fundamental to the formulation of the HRC, remain poorly understood (<xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="bib68">Tuthill et al., 2011</xref>; <xref ref-type="bibr" rid="bib30">Joesch et al., 2013</xref>). Theoretical work to illuminate the computational significance of these various discrepancies is critical for understanding <italic>Drosophila</italic>'s motion estimator.</p><p>The results presented in this paper provide a new theoretical perspective on these experimental results. While previous research has addressed how neural circuits could use four quadrants to carry out algebraic multiplication, here, the recurring theme of our models was that motion-processing circuits should treat light and dark signals differently for functional reasons. We first showed that visual systems could use ON and OFF processing channels that separately correlate light and dark signals to improve the accuracy of motion estimation (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This model was inspired by the experimental observation that <italic>Drosophila</italic>'s motion processing channels distinguish between light increments and decrements (<xref ref-type="bibr" rid="bib29">Joesch et al., 2010</xref>; <xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>), but this study is the first to explicitly demonstrate how such processing channels can improve the accuracy of motion estimation. Furthermore, our model shows that both the phi channels (i.e., the (+ +) and (− −) and quadrants) and the reverse-phi channels (i.e., the (+ −) and (− +) quadrants) can contribute productively to motion estimation in natural environments. Since many animals experience similar sensory statistics and ON and OFF visual processing channels are pervasive across visual systems (<xref ref-type="bibr" rid="bib56">Schiller, 1992</xref>; <xref ref-type="bibr" rid="bib72">Westheimer, 2007</xref>), these mechanisms might be very general. Ultimately, the performance gains from weighted quadrants were a consequence of statistical asymmetries between light and dark contrasts in natural images, and our models showed that neural circuits could perform even better if they made distinctions between light and dark signals that were subtler than simple ON/OFF segregation (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Recent experimental evidence indicates that the fly's motion processing channels are imperfectly selective for ON vs OFF information (<xref ref-type="bibr" rid="bib58">Silies et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Strother et al., 2014</xref>), and it is important that future experiments characterize such subtleties in the computations performed by these circuits.</p><p>Our most general model contained three spatial inputs and showed that spatial averaging of local motion detectors was suboptimal (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Anatomy suggests that single T4 cells receive inputs from several different retinotopic columns, and also from multiple neuron types in a single retinotopic column (<xref ref-type="bibr" rid="bib64">Takemura et al., 2013</xref>). Our modeling suggests that these two forms of circuit heterogeneity could enhance motion estimation by facilitating computations that go beyond averaging to compute higher-order correlations that are distributed across multiple points in space (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Overall, our results demonstrate how the subtleties of neural circuit nonlinearities can improve motion detection with naturalistic inputs. It therefore seems likely that some of the complexities of <italic>Drosophila</italic>'s circuitry are critical to its performance under natural conditions.</p><p>It is remarkable that our approximation of natural motion by the rigid translation of natural images revealed substantial utility for higher-order correlations in motion processing. Truly naturalistic motion would include spatial velocity gradients, occlusion, expansion, and contraction, yet the simplified naturalism we used to optimize our models already sufficed to account for many aspects of the fly's glider responses. This may be because the rotational optomotor response measured in the fly experiments is thought to be sensitive primarily to full-field rotations, which our naturalism emulates well. However, since other higher-order correlations may be associated with non-rigid translation (<xref ref-type="bibr" rid="bib42">Nitzany and Victor, 2014</xref>), one might expect a different set of glider sensitivities to be optimal in the context of other motion-guided behaviors, such as looming responses (<xref ref-type="bibr" rid="bib20">Gabbiani et al., 1999</xref>; <xref ref-type="bibr" rid="bib65">Tammero and Dickinson, 2002</xref>; <xref ref-type="bibr" rid="bib10">Card and Dickinson, 2008</xref>). Since a common elementary motion detector might underlie many or all motion-guided behaviors, incorporating more complex optic flow patterns may even diminish discrepancies between our models and <italic>Drosophila</italic>'s behavior.</p><p>The approach of this study is also relevant to vertebrate vision, where researchers typically model motion estimation using the motion energy model (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>). Like the HRC, the motion energy model only responds to 2-point correlations in the visual stimulus. Consequently, many of the theoretical considerations in this paper apply directly to the motion energy model. Furthermore, each of our computational models can be straightforwardly generalized to the architecture of the motion energy model. For example, one could incorporate non-multiplicative nonlinearities by replacing the squaring operation of the motion energy model with a more flexible nonlinearity. Nevertheless, the numerical benefits offered by each modification to the motion energy model might differ from those found for the HRC because the motion energy model and HRC use distinct spatial and temporal filtering. Such differences could in principle manifest themselves as a different pattern of predicted glider responses (<xref ref-type="bibr" rid="bib27">Hu and Victor, 2010</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>), but comparative electrophysiology experiments in macaques and dragonflies currently suggest that similarities between primate and insect motion processing are abundant (<xref ref-type="bibr" rid="bib43">Nitzany et al., 2014</xref>).</p><p>Our models make predictions that are testable with new experiments. Researchers hypothesize that the T4 and T5 neurons in the fly lobula nonlinearly combine visual inputs across space and time to become the first direction-selective neurons in <italic>Drosophila</italic>'s visual system (<xref ref-type="bibr" rid="bib39">Maisak et al., 2013</xref>). In accordance with the HRC model, conventional wisdom says that these neurons will multiply their input channels. In contrast, we predict that T4 and T5 will combine their visual input streams with non-multiplicative nonlinearities that facilitate accurate motion estimation in natural sensory environments. It's crucial to note that subtle differences between biology's nonlinearity and a pure multiplication can correspond to substantial functional effects. In particular, the optimized nonlinearity that we found here (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) is superficially similar a simple multiplication, yet its subtle distinctions manifest themselves by improving the local estimation accuracy of the HRC by an impressive margin (<xref ref-type="fig" rid="fig2">Figure 2D</xref>).</p><p>In this paper, we studied several simple models to most clearly illustrate the computational consequences of fundamental nonlinear circuit operations. Each of these operations individually provided a way for <italic>Drosophila</italic> to improve their motion estimation accuracy in natural environments, but they are not necessarily exclusive. For example, if a front-end nonlinearity does not fully remove the asymmetry between light and dark contrasts, then subsequent ON and OFF processing might further improve estimation accuracy. Similarly, non-multiplicative nonlinearities might enable an even better combination of ON and OFF signals for motion estimation. The general approach that we adopted here is to restrict the space of candidate models to those that have immediate biological relevance and to identify interesting models by optimizing the model's estimation accuracy over naturalistic stimuli. Future models should incorporate more biological details to better emulate the specifics of <italic>Drosophila</italic>'s visual circuitry, which is rapidly being dissected through unprecedented anatomical, functional, and behavioral experiments (<xref ref-type="bibr" rid="bib59">Silies et al., 2014</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Simulated ensemble of naturalistic motions</title><p>We simulated the linear responses of neighboring photoreceptors to naturalistic motion using methods similar to previous work (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>). We began with a database of natural images (<xref ref-type="bibr" rid="bib70">van Hateren and van der Schaaf, 1998</xref>). We converted each natural image to a contrast scale, <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the contrast at the spatial point <inline-formula><mml:math id="inf3"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is its intensity, and <inline-formula><mml:math id="inf189"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the average intensity across the image. Since we only consider horizontal motion, we emulated the spatial blurring of <italic>Drosophila</italic>'s photoreceptors in the vertical dimension by filtering across rows with a Gaussian kernel (FWHM = 5.7°). We then took the central row of each filtered image to represent a one-dimension variant of the natural image, denoted <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We applied reflective boundary conditions to generate images that covered 360° and down-sampled each resulting image to 1° pixels by averaging. Photoreceptor blurring from signals in the horizontal dimension depends on the velocity of motion. In particular, we model the response of the <italic>i</italic>th photoreceptor as<disp-formula id="equ84"><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is a causal exponential kernel (timescale = 10 ms), <inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> is a Gaussian kernel (FWHM = 5.7°), <mml:math id="inf58"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math> is the location of the <italic>i</italic>th photoreceptor, and <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>ν</mml:mi></mml:mrow></mml:math></inline-formula> is the velocity of motion.</p><p>Each naturalistic motion comprised a randomly selected one-dimensional natural image, an offset to set the initial location of the photoreceptors, and a velocity drawn from a zero-mean normal distribution with a standard deviation of 90°/s. In this manner, we simulated the responses of three horizontally adjacent photoreceptors (spaced by 5.1°) to 5 × 10<sup>5</sup> naturalistic motions (each with duration = 800 ms, time step = 5 ms). We then explicitly enforced left-right symmetry in the naturalistic ensemble by pairing each naturalistic motion with a new simulated motion, in which the natural image is reflected, the velocity is inverted, and the offset is chosen such that <inline-formula><mml:math id="inf140"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the new naturalistic motion is exactly <inline-formula><mml:math id="inf141"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from its partner. The final symmetric ensemble thus consists of 10<sup>6</sup> naturalistic motions.</p></sec><sec id="s4-2"><title>The HRC</title><p>The HRC applies two temporal filters to its photoreceptor inputs. We denote the kernels of the low-pass and high-pass filters as <italic>f</italic> and <italic>g</italic>, respectively, such that the output of a local HRC is <disp-formula id="equ85"><mml:math id="m2"><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi mathvariant="italic">V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where * denotes convolution (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). We consider the HRC's velocity estimate for a given naturalistic motion as its value at the final time point of the simulation. We model the filter kernels as<disp-formula id="equ86"><mml:math id="m3"><mml:mrow><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula>and<disp-formula id="equ87"><mml:math id="m4"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:math></inline-formula> = 20 ms and <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is comparable to lamina monopolar cell responses (<xref ref-type="bibr" rid="bib12">Clark et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Behnia et al., 2014</xref>). We built the alternate motion estimators considered in this work from the same four filtered signals, <inline-formula><mml:math id="inf142"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, always considering the estimator's output at the final time point as its velocity estimate. Thus, none of our models modified the spatial or temporal processing of the HRC, reflecting our emphasis on how nonlinear processing might be tuned for naturalistic motion estimation. The global output of an array of HRCs would be obtained by pooling signals across space. Here we focus on spatiotemporally local strategies for motion estimation and at most pool motion signals across two neighboring motion detectors.</p></sec><sec id="s4-3"><title>Relationship between the mean squared error and the correlation coefficient</title><p>We evaluate motion estimators by the mean squared error between their output and the true velocity. To minimize the mean squared error of the HRC, we scale its output by <inline-formula><mml:math id="inf143"><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf62"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is the correlation coefficient between the HRC's output and the velocity of motion, <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the standard deviation of the velocity distribution, and <inline-formula><mml:math id="inf64"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the standard deviation of the HRC's output. Once the HRC is scaled in this manner, its mean squared error is<disp-formula id="equ88"><mml:math id="m5"><mml:mrow><mml:mi mathvariant="italic">ϵ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>More generally, this equation rewrites the mean squared error of any optimally scaled motion estimator in terms of its correlation coefficient with the velocity. All motion estimators considered in this paper are optimally scaled, and we find the correlation coefficient to be more intuitive than the mean squared error. We thus always report the performance of each motion estimator in terms of the correlation coefficient between the true and estimated velocity.</p></sec><sec id="s4-4"><title>Model fitting procedure</title><p>We fit the linear weighting parameters in the models of <xref ref-type="fig" rid="fig1 fig3 fig4">Figures 1I, 3A, 4A,C,D</xref> to maximize the estimation accuracy over a simulated ensemble of naturalistic motions. The formulas provided in subsequent sections of the ‘Materials and methods’ will cast each motion estimation scheme as a linear combination of a variety of motion predictors,<disp-formula id="equ89"><mml:math id="m6"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the <inline-formula><mml:math id="inf65"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are nonlinear combinations of <inline-formula><mml:math id="inf5"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that depend on the model architecture, and the <italic>w</italic><sub><italic>i</italic></sub> are associated weighting coefficients. We chose the weights to minimize the mean squared error between the true and predicted velocity, which is the standard scenario considered by ordinary least-squares regression. The same weights maximize the correlation coefficient between the true and predicted velocity, and we typically present model accuracies as correlation coefficients.</p><p>We used twofold cross-validation to protect against over-fitting. In particular, we randomly divided the ensemble of naturalistic motions into a training set of 500,000 symmetrically paired examples and a testing set of the remaining 500,000 examples. We determined the weighting coefficients by minimizing the empirical error over the training set, and we reported accuracies over the test set. To estimate error bars for each model's accuracy, we computed twenty random divisions of the naturalistic motion ensemble and calculated the standard deviation of the estimation accuracy.</p></sec><sec id="s4-5"><title>Model responses to glider stimuli</title><p>We generated 25 random instantiations of each glider stimulus considered by our previous experimental work (<xref ref-type="fig" rid="fig1">Figure 1F</xref>, duration = 3 s, update rate = 40 Hz, pixel size = 5°) (<xref ref-type="bibr" rid="bib27">Hu and Victor, 2010</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>). We evaluated the response of each model to these stimuli by averaging the outputs of 60 identical local motion estimators (each separated by 5.1°) over the last two seconds of visual stimulation. Glider predictions were equal and opposite for the left and right variants of the stimuli, so we pooled leftward and rightward stimuli in all figures (<xref ref-type="fig" rid="fig1">Figure 1F</xref> shows the rightward variants). We scaled each model's output such that the average response to the positive 2-point glider was 1. All figures associated with glider responses show the mean and standard error of each model's response across the 25 glider instantiations.</p></sec><sec id="s4-6"><title>Front-end nonlinearity model</title><p>The model in <xref ref-type="fig" rid="fig2">Figure 2A</xref> replaces the linear photoreceptor signals, <inline-formula><mml:math id="inf66"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>1</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>2</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, with nonlinear photoreceptor signals<disp-formula id="equ90"><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula> is some nonlinear function. Thus, the motion estimate from the front-end nonlinearity model is<disp-formula id="equ91"><mml:math id="m8"><mml:mrow><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To implement the contrast equalizing nonlinearity, we replaced values of <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by their rank-order (scaled and shifted to range between −1 and +1). Note that all <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> were sorted together (i.e., including all spatial points, temporal points, and simulated naturalistic motions). When multiple <inline-formula><mml:math id="inf138"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> had the same value, they were given the same rank. To implement binarizing nonlinearities, we again sorted the <inline-formula><mml:math id="inf139"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and found the values corresponding to the threshold locations. For example, to calculate the binarizing nonlinearity with two steps (<xref ref-type="app" rid="app4">Appendix 4</xref>): (i) we found the <inline-formula><mml:math id="inf73"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> values corresponding to the 25th and 75th percentiles; (ii) signals below the 25th percentile or above the 75th percentiles were assigned the value of −1; and (iii) signals between 25th and 75th percentiles were assigned the value of +1. To implement the Gaussianizing nonlinearity, we again rank-ordered the <inline-formula><mml:math id="inf74"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (scaled to range between 0 and 1) and applied the inverse Gaussian cumulative distribution function to these ranks. The HRC is the special case of this model where the front-end nonlinearity is linear.</p></sec><sec id="s4-7"><title>Weighted 4-quadrant model</title><p>The weighted 4-quadrant model in <xref ref-type="fig" rid="fig3">Figure 3A</xref> separately correlates bright and dark signals. Mathematically, it is<disp-formula id="equ92"><mml:math id="m9"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf144"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are adjustable weights that parameterize the model,<disp-formula id="equ93"><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is equal to <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> is positive and zero otherwise, and <inline-formula><mml:math id="inf7"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is equal to <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> is negative and zero otherwise. The HRC is the special case of this model where <inline-formula><mml:math id="inf8"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-8"><title>Non-multiplicative nonlinearity model</title><p>The non-multiplicative nonlinearity model in <xref ref-type="fig" rid="fig4">Figure 4A</xref> replaces the HRC's multiplication step with a more flexible two-dimension nonlinearity. In particular, it is<disp-formula id="equ94"><mml:math id="m11"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where we approximate the nonlinearity, <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula>, as a fourth-order polynomial<disp-formula id="equ95"><mml:math id="m12"><mml:mrow><mml:mi>η</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mi>j</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and <inline-formula><mml:math id="inf9"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are adjustable weights that parameterize the model. We include terms up to fourth order in this model to ensure that it is flexible enough to describe the published glider response data. In particular: (i) the second-order terms accommodate responses to 2-point glider stimuli; (ii) the third-order terms accommodate parity-inverting responses to 3-point glider stimuli; and (iii) the fourth-order terms enable the model to respond with unequal magnitude to positive and negative parity 3-point glider stimuli (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Thus, this model has 14 parameters. The HRC is the special case of this model where only <inline-formula><mml:math id="inf10"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>N</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> is nonzero.</p></sec><sec id="s4-9"><title>Unrestricted nonlinearity model</title><p>Here we model the 4-dimensional nonlinearity in <xref ref-type="fig" rid="fig4">Figure 4C</xref> as a fourth-order polynomial of the four filtered signals in the HRC. In general, this motion estimator is<disp-formula id="equ96"><mml:math id="m13"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf11"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are adjustable weights that parameterize the model, and we set <inline-formula><mml:math id="inf12"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>0000</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> because this term has no utility for naturalistic motion estimation. Thus, this model has 69 parameters. The HRC is the special case of this model where <inline-formula><mml:math id="inf13"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>1001</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>0110</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>S</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and all other parameters are zero.</p></sec><sec id="s4-10"><title>Extra input nonlinearity model</title><p>Here we model the 6-dimensional nonlinearity in <xref ref-type="fig" rid="fig4">Figure 4D</xref> as a fourth-order polynomial of the six filtered signals in two neighboring HRCs. In general, this motion estimator is<disp-formula id="equ97"><mml:math id="m14"><mml:mrow><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mn>4</mml:mn></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>−</mml:mo><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>j</mml:mi></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>k</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>l</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>m</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf14"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi><mml:mi>k</mml:mi><mml:mi>l</mml:mi><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are adjustable weights that parameterize the model, and we set <inline-formula><mml:math id="inf15"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>000000</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> because this term has no utility for naturalistic motion estimation. Thus, this model has 209 parameters. The average of two neighboring HRCs is the special case of this model where <inline-formula><mml:math id="inf16"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>100100</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo> </mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>011000</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>001001</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mn>000110</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>E</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and all other parameters are zero.</p></sec><sec id="s4-11"><title>Lasso regression for predictor selection</title><p>Lasso regression augments the squared error with an <italic>L</italic><sub>1</sub> penalty on nonzero weighting coefficients that favors sparse solutions (<xref ref-type="bibr" rid="bib66">Tibshirani, 1996</xref>). We used lasso regression to identify subsets of predictors that might enable accurate motion estimation (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). Once we identified a predictor subset using lasso regression, we refit the nonzero model weights using ordinary least squares regression (i.e., without the weight penalty).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>The authors thank Ruben Portugues, Haim Sompolinsky, and Florian Engert for helpful conversations. JEF acknowledges fellowship support from the Swartz Foundation. DAC acknowledges support from a Searle Scholar Award, a Sloan Research Fellowship, and the Smith Family Foundation.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>JEF, Conceived of research, designed research, performed research analyzed data, and wrote paper</p></fn><fn fn-type="con" id="con2"><p>DAC, Conceived of research, designed research, and wrote paper</p></fn></fn-group></sec><sec sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.09123.021</object-id><label>Source code 1.</label><caption><p>Code used to generate figures.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.021">http://dx.doi.org/10.7554/eLife.09123.021</ext-link></p></caption><media mime-subtype="zip" mimetype="application" xlink:href="elife-09123-code1-v2.zip"/></supplementary-material></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>E</given-names></name><name><surname>Bergen</surname><given-names>J</given-names></name></person-group><year>1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>2</volume><fpage>284</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ammer</surname><given-names>G</given-names></name><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Dickson</surname><given-names>BJ</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>2015</year><article-title>Functional specialization of neural input elements to the <italic>Drosophila</italic> ON motion detector</article-title><source>Current Biology</source><volume>25</volume><fpage>2247</fpage><lpage>2253</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.07.014</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barlow</surname><given-names>HB</given-names></name><name><surname>Hill</surname><given-names>RM</given-names></name></person-group><year>1963</year><article-title>Selective sensitivity to direction of movement in ganglion cells of the rabbit retina</article-title><source>Science</source><volume>139</volume><fpage>412</fpage><lpage>414</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behnia</surname><given-names>R</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Carter</surname><given-names>AG</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name><name><surname>Desplan</surname><given-names>C</given-names></name></person-group><year>2014</year><article-title>Processing properties of ON and OFF pathways for <italic>Drosophila</italic> motion detection</article-title><source>Nature</source><volume>512</volume><fpage>427</fpage><lpage>430</lpage><pub-id pub-id-type="doi">10.1038/nature13427</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>A</given-names></name><name><surname>Sejnowski</surname><given-names>T</given-names></name></person-group><year>1997</year><article-title>The “independent components” of natural scenes are edge filters</article-title><source>Vision Research</source><volume>37</volume><fpage>3327</fpage><pub-id pub-id-type="doi">10.1016/S0042-6989(97)00121-1</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Flanagin</surname><given-names>VL</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year>2005</year><article-title>Adaptation without parameter change: dynamic gain control in motion detection</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>102</volume><fpage>6172</fpage><lpage>6176</lpage><pub-id pub-id-type="doi">10.1073/pnas.0500491102</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brinkworth</surname><given-names>RS</given-names></name><name><surname>O'Carroll</surname><given-names>DC</given-names></name></person-group><year>2009</year><article-title>Robust models for optic flow coding in natural scenes inspired by insect biology</article-title><source>PLOS Computational Biology</source><volume>5</volume><fpage>e1000555</fpage><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000555</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year>1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buchner</surname><given-names>E</given-names></name></person-group><year>1976</year><article-title>Elementary movement detectors in an insect visual system</article-title><source>Biological Cybernetics</source><volume>24</volume><fpage>85</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1007/BF00360648</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Card</surname><given-names>G</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year>2008</year><article-title>Visually mediated motor planning in the escape response of <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>18</volume><fpage>1300</fpage><lpage>1307</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.07.094</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chubb</surname><given-names>C</given-names></name><name><surname>Sperling</surname><given-names>G</given-names></name></person-group><year>1988</year><article-title>Drift-balanced random stimuli: a general basis for studying non-Fourier motion perception</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>5</volume><fpage>1986</fpage><lpage>2007</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.5.001986</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Bursztyn</surname><given-names>L</given-names></name><name><surname>Horowitz</surname><given-names>MA</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year>2011</year><article-title>Defining the computational structure of the motion detector in <italic>Drosophila</italic></article-title><source>Neuron</source><volume>70</volume><fpage>1165</fpage><lpage>1177</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.023</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Ales</surname><given-names>JM</given-names></name><name><surname>Gohl</surname><given-names>DM</given-names></name><name><surname>Silies</surname><given-names>MA</given-names></name><name><surname>Norcia</surname><given-names>AM</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year>2014</year><article-title>Flies and humans share a motion estimation strategy that exploits natural scene statistics</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>296</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nn.3600</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeCarlo</surname><given-names>LT</given-names></name></person-group><year>1997</year><article-title>On the meaning and use of kurtosis</article-title><source>Psychological Methods</source><volume>2</volume><fpage>292</fpage><pub-id pub-id-type="doi">10.1037/1082-989X.2.3.292</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dror</surname><given-names>DO</given-names></name><name><surname>O'Carroll</surname><given-names>DC</given-names></name><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year>2001</year><article-title>Accuracy of velocity estimation by Reichardt correlates</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>18</volume><fpage>241</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.18.000241</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egelhaaf</surname><given-names>M</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>1989</year><article-title>Transient and steady-state response properties of movement detectors</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>6</volume><fpage>116</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.6.000116</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egelhaaf</surname><given-names>M</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name><name><surname>Reichardt</surname><given-names>W</given-names></name></person-group><year>1989</year><article-title>Computational structure of a biological motion-detection system as revealed by local detector analysis in the fly's nervous system</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>6</volume><fpage>1070</fpage><lpage>1087</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.6.001070</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichner</surname><given-names>H</given-names></name><name><surname>Joesch</surname><given-names>M</given-names></name><name><surname>Schnell</surname><given-names>B</given-names></name><name><surname>Reiff</surname><given-names>DF</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>2011</year><article-title>Internal structure of the fly elementary motion detector</article-title><source>Neuron</source><volume>70</volume><fpage>1155</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.03.028</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fitzgerald</surname><given-names>JE</given-names></name><name><surname>Katsov</surname><given-names>AY</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name><name><surname>Schnitzer</surname><given-names>MJ</given-names></name></person-group><year>2011</year><article-title>Symmetries in stimulus statistics shape the form of visual motion estimators</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>108</volume><fpage>12909</fpage><lpage>12914</lpage><pub-id pub-id-type="doi">10.1073/pnas.1015680108</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franz</surname><given-names>MO</given-names></name><name><surname>Krapp</surname><given-names>HG</given-names></name></person-group><year>2000</year><article-title>Wide-field, motion-sensitive neurons and matched filters for optic flow fields</article-title><source>Biological Cybernetics</source><volume>83</volume><fpage>185</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1007/s004220000163</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabbiani</surname><given-names>F</given-names></name><name><surname>Krapp</surname><given-names>HG</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year>1999</year><article-title>Computation of object approach by a wide-field, motion-sensitive neuron</article-title><source>The Journal of Neuroscience</source><volume>19</volume><fpage>1122</fpage><lpage>1141</lpage></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geisler</surname><given-names>WS</given-names></name></person-group><year>2008</year><article-title>Visual perception and the statistical properties of natural scenes</article-title><source>Annual Review of Psychology</source><volume>59</volume><fpage>167</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.58.110405.085632</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Götz</surname><given-names>KG</given-names></name></person-group><year>1968</year><article-title>Flight control in <italic>Drosophila</italic> by visual perception of motion</article-title><source>Biological Cybernetics</source><volume>4</volume><fpage>199</fpage><lpage>208</lpage></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Götz</surname><given-names>K</given-names></name><name><surname>Wenking</surname><given-names>H</given-names></name></person-group><year>1973</year><article-title>Visual control of locomotion in the walking fruitfly <italic>Drosophila</italic></article-title><source>Journal of Comparative Physiology. A, Sensory, Neural, and Behavioral Physiology</source><volume>85</volume><fpage>235</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1007/BF00694232</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haag</surname><given-names>J</given-names></name><name><surname>Denk</surname><given-names>W</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>2004</year><article-title>Fly motion vision is based on Reichardt detectors regardless of the signal-to-noise ratio</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>101</volume><fpage>16333</fpage><pub-id pub-id-type="doi">10.1073/pnas.0407368101</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hassenstein</surname><given-names>B</given-names></name><name><surname>Reichardt</surname><given-names>W</given-names></name></person-group><year>1956</year><article-title>Systemtheoretische Analyse der Zeit-, Reihenfolgen-und Vorzeichenauswertung bei der Bewegungsperzeption des Rüsselkäfers Chlorophanus</article-title><source>Zeitschrift für Naturforschung B</source><volume>11</volume><fpage>513</fpage><lpage>524</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hermundstad</surname><given-names>AM</given-names></name><name><surname>Briguglio</surname><given-names>JJ</given-names></name><name><surname>Conte</surname><given-names>MM</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name><name><surname>Tkačik</surname><given-names>G</given-names></name></person-group><year>2014</year><article-title>Variance predicts salience in central sensory processing</article-title><source>eLife</source><volume>3</volume><fpage>e03722</fpage><pub-id pub-id-type="doi">10.7554/eLife.03722</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>Q</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year>2010</year><article-title>A set of high-order spatiotemporal stimuli that elicit motion and reverse-phi percepts</article-title><source>Journal of Vision</source><volume>10</volume><pub-id pub-id-type="doi">10.1167/10.3.9</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>DH</given-names></name><name><surname>Wiesel</surname><given-names>TN</given-names></name></person-group><year>1962</year><article-title>Receptive fields, binocular interaction and functional architecture in the cat's visual cortex</article-title><source>The Journal of Physiology</source><volume>160</volume><fpage>106</fpage><pub-id pub-id-type="doi">10.1113/jphysiol.1962.sp006837</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joesch</surname><given-names>M</given-names></name><name><surname>Schnell</surname><given-names>B</given-names></name><name><surname>Raghu</surname><given-names>SV</given-names></name><name><surname>Reiff</surname><given-names>DF</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>2010</year><article-title>ON and OFF pathways in <italic>Drosophila</italic> motion vision</article-title><source>Nature</source><volume>468</volume><fpage>300</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1038/nature09545</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joesch</surname><given-names>M</given-names></name><name><surname>Weber</surname><given-names>F</given-names></name><name><surname>Eichner</surname><given-names>H</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>2013</year><article-title>Functional specialization of parallel motion detection circuits in the fly</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>902</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3374-12.2013</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name></person-group><year>2001</year><article-title>Light adaptation in <italic>Drosophila</italic> photoreceptors I. Response dynamics and signaling efficiency at 25° C</article-title><source>The Journal of General Physiology</source><volume>117</volume><fpage>3</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1085/jgp.117.1.3</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juusola</surname><given-names>M</given-names></name><name><surname>Hardie</surname><given-names>RC</given-names></name></person-group><year>2001</year><article-title>Light adaptation in <italic>Drosophila</italic> photoreceptors II. Rising temperature increases the bandwidth of reliable signaling</article-title><source>The Journal of General Physiology</source><volume>117</volume><fpage>27</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1085/jgp.117.1.27</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kalmus</surname><given-names>H</given-names></name></person-group><year>1964</year><source>Animals as mathematicians</source></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Katsov</surname><given-names>A</given-names></name><name><surname>Clandinin</surname><given-names>T</given-names></name></person-group><year>2008</year><article-title>Motion processing streams in <italic>Drosophila</italic> are behaviorally specialized</article-title><source>Neuron</source><volume>59</volume><fpage>322</fpage><lpage>335</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.05.022</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Koch</surname><given-names>C</given-names></name></person-group><year>2004</year><source>Biophysics of computation: information processing in single neurons</source><publisher-name>Oxford university press</publisher-name></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kral</surname><given-names>K</given-names></name></person-group><year>2003</year><article-title>Behavioural–analytical studies of the role of head movements in depth perception in insects, birds and mammals</article-title><source>Behavioural Processes</source><volume>64</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/S0376-6357(03)00054-8</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year>1981</year><article-title>A simple coding procedure enhances a neuron's information capacity</article-title><source>Zeitschrift für Naturforschung C</source><volume>36</volume><fpage>51</fpage></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name></person-group><year>1989</year><article-title>The role of sensory adaptation in the retina</article-title><source>The Journal of Experimental Biology</source><volume>146</volume><fpage>39</fpage><lpage>62</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maisak</surname><given-names>MS</given-names></name><name><surname>Haag</surname><given-names>J</given-names></name><name><surname>Ammer</surname><given-names>G</given-names></name><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Schilling</surname><given-names>T</given-names></name><name><surname>Bahl</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Dickson</surname><given-names>BJ</given-names></name><name><surname>Reiff</surname><given-names>DF</given-names></name><name><surname>Hopp</surname><given-names>E</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>2013</year><article-title>A directional tuning map of <italic>Drosophila</italic> elementary motion detectors</article-title><source>Nature</source><volume>500</volume><fpage>212</fpage><lpage>216</lpage><pub-id pub-id-type="doi">10.1038/nature12320</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year>1976</year><article-title>From understanding computation to understanding neural circuitry</article-title><comment>Massachusetts Institute of Technology Artificial Intelligence Laboratory A.I. Memo 357</comment></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Maisak</surname><given-names>MS</given-names></name><name><surname>Haag</surname><given-names>J</given-names></name><name><surname>Dickson</surname><given-names>BJ</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year>2014</year><article-title>Neural circuit components of the <italic>Drosophila</italic> OFF motion vision pathway</article-title><source>Current Biology</source><volume>24</volume><fpage>385</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.01.006</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nitzany</surname><given-names>EI</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year>2014</year><article-title>The statistics of local motion signals in naturalistic movies</article-title><source>Journal of Vision</source><volume>14</volume><fpage>10</fpage><pub-id pub-id-type="doi">10.1167/14.4.10</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nitzany</surname><given-names>EI</given-names></name><name><surname>Menda</surname><given-names>G</given-names></name><name><surname>Shamble</surname><given-names>PS</given-names></name><name><surname>Golden</surname><given-names>JR</given-names></name><name><surname>Hoy</surname><given-names>RR</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year>2014</year><article-title>Evolutionary convergence in computation of local motion signals in monkey and dragonfly</article-title><source>Computational and Systems Neuroscience (CoSyNe)</source></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nordström</surname><given-names>K</given-names></name><name><surname>Barnett</surname><given-names>PD</given-names></name><name><surname>O'Carroll</surname><given-names>DC</given-names></name></person-group><year>2006</year><article-title>Insect detection of small targets moving in visual clutter</article-title><source>PLOS Biology</source><volume>4</volume><fpage>e54</fpage><pub-id pub-id-type="doi">10.1371/journal.pbio.0040054</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olshausen</surname><given-names>BA</given-names></name><name><surname>Field</surname><given-names>DJ</given-names></name></person-group><year>1996</year><article-title>Emergence of simple-cell receptive field properties by learning a sparse code for natural images</article-title><source>Nature</source><volume>381</volume><fpage>607</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1038/381607a0</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orger</surname><given-names>MB</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name><name><surname>Anstis</surname><given-names>SM</given-names></name><name><surname>Baier</surname><given-names>H</given-names></name></person-group><year>2000</year><article-title>Perception of Fourier and non-Fourier motion by larval zebrafish</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>1128</fpage><lpage>1133</lpage><pub-id pub-id-type="doi">10.1038/80649</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pick</surname><given-names>S</given-names></name><name><surname>Strauss</surname><given-names>R</given-names></name></person-group><year>2005</year><article-title>Goal-driven behavioral adaptations in gap-climbing <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>15</volume><fpage>1473</fpage><lpage>1478</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2005.07.022</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Reichardt</surname><given-names>W</given-names></name></person-group><year>1973</year><article-title>Considerations of models of movement detection</article-title><source>Kybernetik</source><volume>13</volume><fpage>223</fpage><lpage>227</lpage><pub-id pub-id-type="doi">10.1007/BF00274887</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Reichardt</surname><given-names>W</given-names></name></person-group><year>1980</year><article-title>On the representation of multi-input systems: computational properties of polynomial algorithms</article-title><source>Biological Cybernetics</source><volume>37</volume><fpage>167</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1007/BF00355455</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potters</surname><given-names>M</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year>1994</year><article-title>Statistical mechanics and visual signal processing</article-title><source>Journal de Physique I</source><volume>4</volume><fpage>1755</fpage><lpage>1775</lpage><pub-id pub-id-type="doi">10.1051/jp1:1994219</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quenzer</surname><given-names>T</given-names></name><name><surname>Zanker</surname><given-names>J</given-names></name></person-group><year>1991</year><article-title>Visual detection of paradoxical motion in flies</article-title><source>Journal of Comparative Physiology A</source><volume>169</volume><fpage>331</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1007/BF00206997</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratliff</surname><given-names>CP</given-names></name><name><surname>Borghuis</surname><given-names>BG</given-names></name><name><surname>Kao</surname><given-names>YH</given-names></name><name><surname>Sterling</surname><given-names>P</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year>2010</year><article-title>Retina is structured to process an excess of darkness in natural scenes</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>107</volume><fpage>17368</fpage><lpage>17373</lpage><pub-id pub-id-type="doi">10.1073/pnas.1005846107</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichardt</surname><given-names>W</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year>1976</year><article-title>Visual control of orientation behaviour in the fly: part I. A quantitative analysis</article-title><source>Quarterly Reviews of Biophysics</source><volume>9</volume><fpage>311</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1017/S0033583500002523</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reichardt</surname><given-names>W</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name><name><surname>Hausen</surname><given-names>K</given-names></name></person-group><year>1983</year><article-title>Figure-ground discrimination by relative movement in the visual system of the fly</article-title><source>Biological Cybernetics</source><volume>46</volume><fpage>1</fpage><lpage>30</lpage><pub-id pub-id-type="doi">10.1007/BF00595226</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogers</surname><given-names>B</given-names></name><name><surname>Graham</surname><given-names>M</given-names></name></person-group><year>1979</year><article-title>Motion parallax as an independent cue for depth perception</article-title><source>Perception</source><volume>8</volume><fpage>125</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1068/p080125</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruderman</surname><given-names>DL</given-names></name><name><surname>Bialek</surname><given-names>W</given-names></name></person-group><year>1994</year><article-title>Statistics of natural images: scaling in the woods</article-title><source>Physical Review Letters</source><volume>73</volume><fpage>814</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.73.814</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rust</surname><given-names>NC</given-names></name><name><surname>Mante</surname><given-names>V</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year>2006</year><article-title>How MT cells analyze the motion of visual patterns</article-title><source>Nature Neuroscience</source><volume>9</volume><fpage>1421</fpage><lpage>1431</lpage><pub-id pub-id-type="doi">10.1038/nn1786</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schiller</surname><given-names>PH</given-names></name></person-group><year>1992</year><article-title>The ON and OFF channels of the visual system</article-title><source>Trends in Neurosciences</source><volume>15</volume><fpage>86</fpage><lpage>92</lpage><pub-id pub-id-type="doi">10.1016/0166-2236(92)90017-3</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Churchland</surname><given-names>PS</given-names></name></person-group><year>1988</year><article-title>Computational neuroscience</article-title><source>Science</source><volume>241</volume><fpage>1299</fpage><lpage>1306</lpage><pub-id pub-id-type="doi">10.1126/science.3045969</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silies</surname><given-names>M</given-names></name><name><surname>Gohl</surname><given-names>DM</given-names></name><name><surname>Fisher</surname><given-names>YE</given-names></name><name><surname>Freifeld</surname><given-names>L</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year>2013</year><article-title>Modular use of peripheral input channels tunes motion-detecting circuitry</article-title><source>Neuron</source><volume>79</volume><fpage>111</fpage><lpage>127</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.04.029</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silies</surname><given-names>M</given-names></name><name><surname>Gohl</surname><given-names>DM</given-names></name><name><surname>Clandinin</surname><given-names>TR</given-names></name></person-group><year>2014</year><article-title>Motion-detecting circuits in flies: coming into view</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>307</fpage><lpage>327</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-013931</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Olshausen</surname><given-names>BA</given-names></name></person-group><year>2001</year><article-title>Natural image statistics and neural representation</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>1193</fpage><lpage>1216</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.1193</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sperry</surname><given-names>RW</given-names></name></person-group><year>1950</year><article-title>Neural basis of the spontaneous optokinetic response produced by visual inversion</article-title><source>Journal of Comparative and Physiological Psychology</source><volume>43</volume><fpage>482</fpage><pub-id pub-id-type="doi">10.1037/h0055479</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinivasan</surname><given-names>MV</given-names></name><name><surname>Lehrer</surname><given-names>M</given-names></name><name><surname>Kirchner</surname><given-names>WH</given-names></name><name><surname>Zhang</surname><given-names>SW</given-names></name></person-group><year>1991</year><article-title>Range perception through apparent image speed in freely flying honeybees</article-title><source>Visual Neuroscience</source><volume>6</volume><fpage>519</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1017/S095252380000136X</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strother</surname><given-names>JA</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year>2014</year><article-title>Direct observation of ON and OFF pathways in the <italic>Drosophila</italic> visual system</article-title><source>Current Biology</source><volume>24</volume><fpage>976</fpage><lpage>983</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.03.017</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takemura</surname><given-names>SY</given-names></name><name><surname>Bharioke</surname><given-names>A</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Nern</surname><given-names>A</given-names></name><name><surname>Vitaladevuni</surname><given-names>S</given-names></name><name><surname>Rivlin</surname><given-names>PK</given-names></name><name><surname>Katz</surname><given-names>WT</given-names></name><name><surname>Olbris</surname><given-names>DJ</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name><name><surname>Winston</surname><given-names>P</given-names></name><name><surname>Zhao</surname><given-names>T</given-names></name><name><surname>Horne</surname><given-names>JA</given-names></name><name><surname>Fetter</surname><given-names>RD</given-names></name><name><surname>Takemura</surname><given-names>S</given-names></name><name><surname>Blazek</surname><given-names>K</given-names></name><name><surname>Chang</surname><given-names>LA</given-names></name><name><surname>Ogundeyi</surname><given-names>O</given-names></name><name><surname>Saunders</surname><given-names>MA</given-names></name><name><surname>Shapiro</surname><given-names>V</given-names></name><name><surname>Sigmund</surname><given-names>C</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Meinertzhagen</surname><given-names>IA</given-names></name><name><surname>Chklovskii</surname><given-names>DB</given-names></name></person-group><year>2013</year><article-title>A visual motion detection circuit suggested by <italic>Drosophila</italic> connectomics</article-title><source>Nature</source><volume>500</volume><fpage>175</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1038/nature12450</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tammero</surname><given-names>LF</given-names></name><name><surname>Dickinson</surname><given-names>MH</given-names></name></person-group><year>2002</year><article-title>Collision-avoidance and landing responses are mediated by separate pathways in the fruit fly, <italic>Drosophila melanogaster</italic></article-title><source>The Journal of Experimental Biology</source><volume>205</volume><fpage>2785</fpage><lpage>2798</lpage></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tibshirani</surname><given-names>R</given-names></name></person-group><year>1996</year><article-title>Regression shrinkage and selection via the lasso</article-title><source>Journal of the Royal Statistical Society. Series B (Methodological)</source><comment>267–288</comment></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tkačik</surname><given-names>G</given-names></name><name><surname>Prentice</surname><given-names>JS</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name><name><surname>Balasubramanian</surname><given-names>V</given-names></name></person-group><year>2010</year><article-title>Local statistics in natural scenes predict the saliency of synthetic textures</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>107</volume><fpage>18149</fpage><lpage>18154</lpage><pub-id pub-id-type="doi">10.1073/pnas.0914916107</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuthill</surname><given-names>JC</given-names></name><name><surname>Chiappe</surname><given-names>ME</given-names></name><name><surname>Reiser</surname><given-names>MB</given-names></name></person-group><year>2011</year><article-title>Neural correlates of illusory motion perception in <italic>Drosophila</italic></article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>108</volume><fpage>9685</fpage><lpage>9690</lpage><pub-id pub-id-type="doi">10.1073/pnas.1100062108</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>J</given-names></name><name><surname>Snippe</surname><given-names>H</given-names></name></person-group><year>2006</year><article-title>Phototransduction in primate cones and blowfly photoreceptors: different mechanisms, different algorithms, similar response</article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>192</volume><fpage>187</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1007/s00359-005-0060-y</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Hateren</surname><given-names>JH</given-names></name><name><surname>van der Schaaf</surname><given-names>A</given-names></name></person-group><year>1998</year><article-title>Independent component filters of natural images compared with simple cells in primary visual cortex</article-title><source>Proceedings of the Royal Society of London. Series B</source><volume>265</volume><fpage>359</fpage><lpage>366</lpage><pub-id pub-id-type="doi">10.1098/rspb.1998.0303</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Santen</surname><given-names>JPH</given-names></name><name><surname>Sperling</surname><given-names>G</given-names></name></person-group><year>1985</year><article-title>Elaborated reichardt detectors</article-title><source>Journal of the Optical Society of America. A, Optics and Image Science</source><volume>2</volume><fpage>300</fpage><lpage>320</lpage><pub-id pub-id-type="doi">10.1364/JOSAA.2.000300</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Westheimer</surname><given-names>G</given-names></name></person-group><year>2007</year><article-title>The ON-OFF dichotomy in visual processing: from receptors to perception</article-title><source>Progress in Retinal and Eye Research</source><volume>26</volume><fpage>636</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.preteyeres.2007.07.003</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Schmid</surname><given-names>AM</given-names></name><name><surname>Victor</surname><given-names>JD</given-names></name></person-group><year>2015</year><article-title>Visual processing of informative multipoint correlations arises primarily in V2</article-title><source>eLife</source><volume>4</volume><fpage>e06604</fpage><pub-id pub-id-type="doi">10.7554/eLife.06604</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zanker</surname><given-names>JM</given-names></name></person-group><year>1993</year><article-title>Theta motion: a paradoxical stimulus to explore higher order motion extraction</article-title><source>Vision Research</source><volume>33</volume><fpage>553</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(93)90258-X</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>IJ</given-names></name><name><surname>Sanes</surname><given-names>JR</given-names></name><name><surname>Meister</surname><given-names>M</given-names></name></person-group><year>2012</year><article-title>The most numerous ganglion cell type of the mouse retina is a selective feature detector</article-title><source>Proceedings of the National Academy of Sciences of USA</source><volume>109</volume><fpage>E2391</fpage><lpage>E2398</lpage><pub-id pub-id-type="doi">10.1073/pnas.1211547109</pub-id></element-citation></ref></ref-list><app-group><app id="app1"><title>Appendix 1</title><boxed-text><sec id="s6" sec-type="appendix"><title>Visual signatures of motion.</title><p>The pattern of light that stimulates the retina encodes information about the relative motion between the retina and its visual environment. The manner in which this information is encoded depends on the geometry of the photoreceptor array, the statistics of self-motion, and the statistics of the visual environment. The principal goal of this paper is to illustrate several ways that the brain's nonlinear processing of visual motion signals might be tuned to reflect specific features of the natural visual environment. We thus begin by enumerating some computational signatures of visual motion in natural environments, thereby exposing a diversity of stimulus features that visual system nonlinearities might aim to extract.</p><p>In the real world, animals encounter visual environments that are intricately structured and far from random (<xref ref-type="fig" rid="fig6">Appendix figure 1A</xref>) (<xref ref-type="bibr" rid="bib12">Ruderman and Bialek, 1994</xref>; <xref ref-type="bibr" rid="bib70">van Hateren and van der Schaaf, 1998</xref>; <xref ref-type="bibr" rid="bib21">Geisler, 2008</xref>). When an animal rotates with constant angular velocity through the environment, the spatiotemporal response profile of the photoreceptor array encodes the velocity of self-motion through the slope of oriented streaks in space-time (<italic>front face</italic>, <xref ref-type="fig" rid="fig6">Appendix figure 1B</xref>) (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>). Thus, a visual system with a dense array of noiseless photoreceptors could extract the angular velocity of an arbitrary image by computing the ratio of temporal and spatial derivatives (<xref ref-type="bibr" rid="bib49">Potters and Bialek, 1994</xref>). The statistics of the image ensemble become relevant once multiple interpretations of the sensory world become logically consistent with the photoreceptor data. In particular, the optimal motion estimator depends on the statistics of the image ensemble when photoreceptors have noise (<xref ref-type="bibr" rid="bib49">Potters and Bialek, 1994</xref>; <xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>), and a nonzero spacing between photoreceptors introduces ambiguity via aliasing (<xref ref-type="bibr" rid="bib49">Potters and Bialek, 1994</xref>). In these cases, the animal can use prior information regarding the sensory environment and its motion in order to weigh the plausibility of each sensory interpretation.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.015</object-id><label>Appendix figure 1.</label><caption><title>Motion transforms spatial correlations into temporal correlations.</title><p>(<bold>A</bold>) An example natural image (<xref ref-type="bibr" rid="bib70">van Hateren and van der Schaaf, 1998</xref>). (<bold>B</bold>) When a natural image (<italic>top face</italic>) moves to the right, streaks in space-time (<italic>front face</italic>) indicate the direction and speed of the motion. Alternatively, motion influences the temporal correlation structure of visual signals (<italic>side face</italic>). (<bold>C</bold>) Second-order correlation function between pairs of spatially separated contrast signals (across the natural image ensemble [<xref ref-type="bibr" rid="bib70">van Hateren and van der Schaaf, 1998</xref>]). (<bold>D</bold>) For constant velocity motion, the temporal correlation function between a pair of spatially separated points is shifted and stretched relative to the spatial correlation function. We separated the two points by <italic>Drosophila</italic>'s photoreceptor spacing (5.1°). (<bold>E</bold>) Example third-order spatial correlation function involving two points in space. (<bold>F</bold>) As with pairwise correlations, higher-order temporal correlations between spatially separated visual signals are shifted and stretched (relative to higher-order spatial correlation functions) in a manner that indicates the speed and direction of motion.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.015">http://dx.doi.org/10.7554/eLife.09123.015</ext-link></p></caption><graphic xlink:href="elife-09123-app1-fig1-v2.tif"/></fig></p><p>Full field motion transforms spatial features (<italic>top face</italic>, <xref ref-type="fig" rid="fig6">Appendix figure 1B</xref>) into temporal features (<italic>side face</italic>, <xref ref-type="fig" rid="fig6">Appendix figure 1B</xref>) in a manner that depends upon the velocity of motion. Consequently, one can also think about the visual signatures of motion in terms of spatiotemporal correlations between photoreceptors. The luminance contrast encoded by the <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>th photoreceptor is <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf81"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the luminance intensity seen by the <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>th photoreceptor at time <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:mrow><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the average luminance intensity over the visual field. Thus, the average contrast is zero, and the simplest correlation function corresponds to the product of two spatially separated contrast signals. Measured over an ensemble of natural images, this 2-point correlation function had a global maximum at zero spatial offset (<xref ref-type="fig" rid="fig6">Appendix figure 1C</xref>). Consequently, the velocity of motion is encoded by the peak of the temporal cross-correlation function between two neighboring photoreceptors, which occurs at the temporal offset that equals the photoreceptor spacing (5.1° for <italic>Drosophila</italic>) divided by the velocity of motion (<xref ref-type="fig" rid="fig6">Appendix figure 1D</xref>). Natural images also contain many higher-order correlations (<xref ref-type="bibr" rid="bib12">Ruderman and Bialek, 1994</xref>; <xref ref-type="bibr" rid="bib21">Geisler, 2008</xref>). For instance, the nonzero skewness of natural images implies that the third-order correlation that multiplies the contrast at one point with the squared contrast at a neighboring point also has a peak at zero spatial offset (<xref ref-type="fig" rid="fig6">Appendix figure 1E</xref>). Correspondingly, the peak of the temporal 3-point correlation function between neighboring photoreceptors encodes the velocity of motion (<xref ref-type="fig" rid="fig6">Appendix figure 1F</xref>). This argument generalizes to <italic>n</italic>th-order correlation functions when the ensemble of natural images has a nonzero <italic>n</italic>th moment. Note that this argument does not necessarily imply that a motion estimator would benefit from the incorporation of all nonzero correlation functions, because the velocity signals provided by one correlation function could be redundant with those provided by others.</p><p>Importantly, photoreceptor correlation functions also encode velocity information away from their peaks. For example, the velocity of motion influences the widths of the temporal cross-correlation functions between pairs of photoreceptors (<xref ref-type="fig" rid="fig6">Appendix figure 1D,F</xref>). To see this, note that the values of the temporal correlation functions at zero temporal offsets are velocity independent, whereas the peak locations are closer to zero for larger speeds (<xref ref-type="fig" rid="fig6">Appendix figure 1D,F</xref>). This implies a more rapid falloff for higher speeds. This fundamental effect occurs because nearby points are more correlated in natural environments and photoreceptors rapidly survey distant points when the speed of motion is high.</p><p>The description above illustrates how visual motion becomes encoded in photoreceptor correlations. A central goal of research in visual motion estimation is to understand how neural circuits invert (or decode) that encoding of velocity. Just as a broad class of functions can be represented as a power series, a broad class of motion estimators can be represented as a Volterra series (<xref ref-type="bibr" rid="bib48">Poggio and Reichardt, 1980</xref>; <xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>). Each term in the Volterra series can be interpreted as a multipoint correlator that decodes velocity information from a specific correlation function (<xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>). For example, the HRC and the motion energy model are 2-point correlators that decode velocity from 2-point correlations, whereas the Bayes optimal motion estimator capitalizes on a wider variety of correlation functions (<xref ref-type="bibr" rid="bib49">Potters and Bialek, 1994</xref>; <xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>). Because multipoint correlators relate intuitively to measurable properties of the image ensemble, we will find that decomposing a motion estimator in terms of multipoint correlators is often illuminating. Moreover, we will use multipoint correlators as a common basis to compare the computations performed by mechanistically distinct models.</p></sec></boxed-text></app><app id="app2"><title>Appendix 2</title><boxed-text><sec id="s7" sec-type="appendix"><title>Accuracy of 2-point correlators.</title><p>In this section we derive an expression for the accuracy of a general 2-point correlator in terms of the statistics of naturalistic motion.</p><p>We consider a general 2-point correlator that temporally correlates visual signals from the spatial points <italic>i</italic> and <italic>j</italic>. Mathematically, this estimator has the form<disp-formula id="equ1"><label>(1)</label><mml:math id="m15"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where the 2-point kernel, <inline-formula><mml:math id="inf17"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, defines the correlator by specifying how each 2-point correlation contributes to the motion estimate. We model the response of the <italic>i</italic>th photoreceptor as<disp-formula id="equ2"><label>(2)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:mi>τ</mml:mi><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:mi>θ</mml:mi><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf85"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is a temporal integration kernel, <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> is the photoreceptor's spatial acceptance profile, <inline-formula><mml:math id="inf87"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the location of the <italic>i</italic>th photoreceptor, <inline-formula><mml:math id="inf88"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>θ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the spatial contrast pattern of the visual world, and <inline-formula><mml:math id="inf89"><mml:mrow><mml:mrow><mml:mi>ν</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the time-dependent velocity. This formula simplifies to the formula in the ‘Materials and methods’ when <inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:mi>ν</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is time-independent. If <inline-formula><mml:math id="inf91"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> is an invertible linear filter, then a more convenient representation of the photoreceptor signals is<disp-formula id="equ3"><label>(3)</label><mml:math id="m17"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>t</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:mrow></mml:mstyle><mml:mtext> </mml:mtext><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>∗</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf18"><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mo>*</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf92"><mml:mrow><mml:mo>*</mml:mo></mml:mrow></mml:math></inline-formula> is the convolution operator (<xref ref-type="bibr" rid="bib49">Potters and Bialek, 1994</xref>). We can rewrite the 2-point correlator in this representation as<disp-formula id="equ4"><label>(4)</label><mml:math id="m18"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where<disp-formula id="equ5"><label>(5)</label><mml:math id="m19"><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>is the 2-point kernel that converts correlations in the <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>U</mml:mi></mml:mrow></mml:math></inline-formula> variables to a velocity estimate.</p><p>Recall that we quantify the performance of visual motion estimators based on the mean squared error between the true and estimated velocities<disp-formula id="equ6"><label>(6)</label><mml:math id="m20"><mml:mrow><mml:mi mathvariant="italic">ϵ</mml:mi><mml:mo>≡</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf94"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ν</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> = 90°/s is the standard deviation of the velocity distribution. For estimators that are scaled to minimize their mean squared error (‘Materials and methods’), this formula can be rewritten as<disp-formula id="equ7"><label>(7)</label><mml:math id="m21"><mml:mrow><mml:mi mathvariant="italic">ϵ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where<disp-formula id="equ8"><label>(8)</label><mml:math id="m22"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>is the correlation coefficient between the estimated and true velocities. Thus, minimizing the mean squared error is mathematically equivalent to maximizing the correlation coefficient if all motion estimators are correctly scaled. We find the correlation coefficient to be a more intuitive error metric than the mean squared error, so many of our results will be presented in terms of correlation coefficients.</p><p>The numerator of the correlation coefficient is determined by the second-order statistics of the image ensemble,<disp-formula id="equ9"><label>(9)</label><mml:math id="m23"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where Δ<sub><italic>ij</italic></sub> is the angular separation between the <italic>i</italic>th and <italic>j</italic>th photoreceptors, and<disp-formula id="equ10"><label>(10)</label><mml:math id="m24"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≡</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>is the 2-point correlation function over the ensemble of spatially filtered natural scenes. Note that <inline-formula><mml:math id="inf19"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is independent of <inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> because reasonable image ensembles are translationally invariant. Also note that the 2-point correlation function of filtered natural images is related to the correlation function of unfiltered images by<disp-formula id="equ11"><label>(11)</label><mml:math id="m25"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo>′</mml:mo><mml:mi>M</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo>′</mml:mo><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mo>*</mml:mo><mml:mi>M</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>*</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf20"><mml:mrow><mml:msup><mml:mi>C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the correlation function of unfiltered images, and we've assumed that <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> is a symmetric function. We model <inline-formula><mml:math id="inf97"><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> as Gaussian with FWHM of 5.7°, so <inline-formula><mml:math id="inf98"><mml:mrow><mml:mi>M</mml:mi><mml:mo>*</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> is also Gaussian with FWHM of <inline-formula><mml:math id="inf21"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mn>5.7</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mn>8.1</mml:mn></mml:mrow><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>On the other hand, the denominator of the correlation coefficient is determined by fourth-order statistics of the image ensemble,<disp-formula id="equ12"><label>(12)</label><mml:math id="m26"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>×</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>×</mml:mo><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where<disp-formula id="equ13"><label>(13)</label><mml:math id="m27"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi mathvariant="script">C</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula></p><p>is the 4-point correlation function of the ensemble of filtered natural images. Notice that the second argument of <inline-formula><mml:math id="inf22"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> lacks the additive factor of <inline-formula><mml:math id="inf99"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> because <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> correspond to the same point in space. As above, <inline-formula><mml:math id="inf23"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> is related to the unfiltered 4-point correlation function through a fourfold application of the photoreceptor spatial acceptance filter.</p><p>The preceding analysis shows that only the second-order and fourth-order statistics of the natural image ensemble contribute to the correlation coefficient between an arbitrary 2-point correlator and the true velocity. The same quantities also determine the mean squared error. Thus, the second-order and fourth-order statistics of the image ensembles are the critical determinants of a 2-point correlator's motion estimation accuracy. Note that both the HRC and the motion energy model fall into this important class of visual motion estimators, so our analysis is also important for understanding visual motion estimation by vertebrates.</p></sec></boxed-text></app><app id="app3"><title>Appendix 3</title><boxed-text><sec id="s8" sec-type="appendix"><title>Motion estimation without spatial correlations—the role of kurtosis on the accuracy of 2-point correlators.</title><p>In this section, we apply the results of <xref ref-type="app" rid="app2">Appendix 2</xref> to the special case of normally distributed velocities and spatially uncorrelated image ensembles. This calculation reveals an important role for kurtosis in motion estimation, and we discuss how nonlinearities in the early visual system could cope with highly kurtotic naturalistic inputs.</p><p>In this section, we assume that the velocity is time-independent (i.e., <inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mi>ν</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ν</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and normally distributed<disp-formula id="equ14"><label>(14)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>v</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We also assume that the image ensemble is spatially uncorrelated. By this, we mean that the luminance contrast at each point in space is statistically independent of the luminance contrast at all other points in space. Thus, the second-order correlation function is<disp-formula id="equ15"><label>(15)</label><mml:math id="m29"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf101"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>C</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the standard deviation of the luminance contrast, and <inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Dirac delta-function. The fourth-order correlation function is<disp-formula id="equ16"><label>(16)</label><mml:math id="m30"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>4</mml:mn></mml:msubsup><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>4</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>4</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the excess kurtosis of the contrast distribution. The excess kurtosis is zero for normally distributed contrasts. It can either be positive or negative for other contrast distributions. Note that we define the <italic>kurtosis</italic> of a probability distribution to be its fourth central moment normalized by the square of its second central moment. Thus, the kurtosis of a normal distribution is 3. We caution readers that some other sources use ‘kurtosis’ to refer to the excess kurtosis.</p><p>With these assumptions, the signal term represented by <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> is<disp-formula id="equ17"><label>(17)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and the noise term represented by <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> is<disp-formula id="equ18"><label>(18)</label><mml:math id="m32"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>4</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>κ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where we've assumed that the 2-point correlator is mirror anti-symmetric,<disp-formula id="equ19"><label>(19)</label><mml:math id="m33"><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>in order to ignore contributions from static signals. This mirror-symmetry assumption holds for the HRC and the motion energy model. Since the denominator of the correlation coefficient is set by <inline-formula><mml:math id="inf24"><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>, both the signal and the noise are proportional to <inline-formula><mml:math id="inf25"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula>. Thus, the only remaining dependence on the image ensemble is through the excess kurtosis. Note that<disp-formula id="equ20"><label>(20)</label><mml:math id="m34"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>κ</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>C</mml:mi><mml:mn>4</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>|</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>|</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Thus, the correlation coefficient is maximized by making <inline-formula><mml:math id="inf104"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mi>4</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> as small as possible.</p><p>In conclusion, if the image ensemble is spatially uncorrelated (at second and fourth-order), then the image ensemble only affects the correlation coefficient between the velocity and a 2-point correlator through its kurtosis. The best accuracy is achieved when the kurtosis is minimized. In reality, the assumption that the image ensemble is spatially uncorrelated is clearly wrong. Natural images are strongly correlated, and even if they weren't, they'd become correlated once they are filtered by the photoreceptors' spatial acceptance filter. Nevertheless, <xref ref-type="fig" rid="fig2">Figure 2E</xref> empirically shows that introducing several front-end nonlinearities that decrease the kurtosis also improve the accuracy of naturalistic motion estimation. Thus, kurtosis provides a useful guide for the design of neuronal nonlinearities. On the other hand, <xref ref-type="fig" rid="fig2">Figure 2D,E</xref> demonstrate that it's too simplistic to assume that the kurtosis is the only relevant factor for the accuracy of a 2-point correlator. As we'll discuss in the next section, spatial correlations in the image ensemble also affect the accuracy of 2-point correlators.</p></sec></boxed-text></app><app id="app4"><title>Appendix 4</title><boxed-text><sec id="s9" sec-type="appendix"><title>The HRC benefits from spatially correlated input signals.</title><p>When we applied a contrast-equalizing or binarizing nonlinearity to naturalistic inputs before evaluating the HRC, we found that both nonlinearities substantially improved the accuracy of the HRC (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Interestingly, contrast equalization improved the accuracy of the HRC more than binarization (<xref ref-type="fig" rid="fig2">Figure 2E</xref>), even though it produced outputs with greater kurtosis. The reason for this is that natural images are correlated (<xref ref-type="fig" rid="fig6">Appendix figure 1</xref>), and the accuracy of the HRC over a general image ensemble depends on the ensemble's spatial correlation structure (Appendix 2). Binarization attenuated spatial correlations more strongly than contrast equalization over the natural image ensemble (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), which leads us to hypothesize that correlations present in the natural image ensemble might benefit the HRC's performance. In <xref ref-type="app" rid="app5">Appendix 5</xref> we will provide theoretical support for this idea. Here we begin with a less mathematical argument that also supports our hypothesis.</p><p>A comparison between the estimation performance of binarizing and equalizing front-end nonlinearities was complicated by the fact that the models produced outputs that differed in both their point statistics and their correlation structures. To gain more direct insight into how spatial correlations affect motion estimation performance, it would be helpful to compare front-end nonlinearity models that differ <italic>only</italic> through their output correlation structures. We implemented this comparison using a family of binarizing front-end nonlinearities that undergo multiple steps between +1 and −1 (<xref ref-type="fig" rid="fig7">Appendix figure 2A</xref>). Although these nonlinearities are not physiologically realistic, they are conceptually useful because they each produced a stimulus ensemble that minimized the kurtosis yet achieved distinct correlation structures (<xref ref-type="fig" rid="fig7">Appendix figure 2B</xref>). These nonlinearities thus allow us to assess directly whether spatial decorrelation of inputs degrades the motion estimation performance of the HRC. We found that each binarizing front-end nonlinearity model outperformed the original HRC (<xref ref-type="fig" rid="fig7">Appendix figure 2C</xref>). However, we found that the magnitude of the improvement decreased with the number of steps (<xref ref-type="fig" rid="fig7">Appendix figure 2C</xref>). Since spatial cross-correlations also decreased as a function of the number of steps (<xref ref-type="fig" rid="fig7">Appendix figure 2B</xref>), these results support our hypothesis that the correlations present in natural visual inputs aid the functionality of the standard HRC.<fig id="fig7" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.016</object-id><label>Appendix figure 2.</label><caption><title>Correlations in binarized natural images.</title><p>(<bold>A</bold>) We transformed each image in the van Hateren natural image database (<xref ref-type="bibr" rid="bib70">van Hateren and van der Schaaf, 1998</xref>) with several binarizing nonlinearities. To implement the simplest binarizing nonlinearity, we set all pixels to +1 or −1 depending on whether that pixel exceeded or fell below the median intensity in the image. For the nonlinearity with two steps, the thresholds were at the 25th and 75th intensity percentiles. For the nonlinearity with three steps, the thresholds were at the 25th, 50th, and 75th intensity percentiles. When a pixel intensity exactly equaled a threshold, we considered its value below threshold. Binary nonlinearities with a larger number of steps produced grainier images that indicate a spatial decorrelation of the transformed image. (<bold>B</bold>) We computed second-order spatial correlation functions across the nonlinearly transformed natural image ensemble. This confirmed that each step in the binarizing nonlinearity further decorrelated the image ensemble. (<bold>C</bold>) In addition to decreasing the spatial extent of correlations, a larger number of transitions also degraded the performance of the front-end nonlinearity model.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.016">http://dx.doi.org/10.7554/eLife.09123.016</ext-link></p></caption><graphic xlink:href="elife-09123-app4-fig1-v2.tif"/></fig></p><p>The HRC correlates two signals that are offset in space and differentially delayed in time. One intuition that researchers often apply to this computation is that the correlation operation effectively detects times when two signals that are offset in space and time are equal. However, a motion estimator that strictly obeyed this intuition would be agnostic to the spatial correlation structures present in the input signals, and our results show that the HRC is not (see also <xref ref-type="app" rid="app5">Appendix 5</xref>). Instead, the HRC also generates motion signals when its two input channels are imperfectly aligned, and these signals depend strongly on the correlation structure of the inputs (<xref ref-type="fig" rid="fig6">Appendix figure 1D</xref>). Our results thus show that the HRC's ability to detect imperfect coincidences contributes significantly to its performance as a motion estimator, as was suggested intuitively in <xref ref-type="app" rid="app1">Appendix 1</xref>.</p></sec></boxed-text></app><app id="app5"><title>Appendix 5</title><boxed-text><sec id="s10" sec-type="appendix"><title>Motion estimation with Gaussian image statistics—the role of spatial correlations on the accuracy of 2-point correlators.</title><p>In this section, we apply the results of <xref ref-type="app" rid="app2">Appendix 2</xref> to the special case of normally distributed velocities and normally distributed image ensembles. This model formalizes how spatial correlations in the natural world affect the accuracy of motion estimation by 2-point correlators and shows how spatial decorrelation can adversely affect estimation accuracy. For example, we'll show that the simplest HRC is unable to extract motion signals from high frequency components of the image ensemble, yet those components still lead to variability in the motion estimator. Thus, this HRC works best when the image ensemble is correlated in a manner that avoids high-frequency components in the signal, and spatial low-pass filtering at the photoreceptor level can help to eliminate the high-frequency image components that hurt the HRC's accuracy.</p><p>Here we use the same velocity distribution that we used in <xref ref-type="app" rid="app3">Appendix 3</xref> (i.e., <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>). However, we now allow the two point correlation function to have arbitrary structure<disp-formula id="equ21"><label>(21)</label><mml:math id="m35"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf105"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the Fourier coefficients for <inline-formula><mml:math id="inf26"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and we have noted that the image ensemble is <inline-formula><mml:math id="inf108"><mml:mrow><mml:mo>2</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula>-periodic. Note that <inline-formula><mml:math id="inf106"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is called the power spectrum of the image ensemble, and uncorrelated ensembles correspond to the special case where <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> = constant. With these assumptions<disp-formula id="equ22"><label>(22)</label><mml:math id="m36"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>By evaluating the integral, we find that this velocity expectation is<disp-formula id="equ23"><label>(23)</label><mml:math id="m37"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, if we define<disp-formula id="equ24"><label>(24)</label><mml:math id="m38"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>then<disp-formula id="equ25"><label>(25)</label><mml:math id="m39"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Each frequency component of the image ensemble linearly contributes to the correlation between the 2-point correlator's response and the velocity. The weight of each frequency component is determined by the structure of the 2-point correlator and the width of the velocity distribution.</p><p>We compute the fourth-order moment of the image ensemble using Wick's theorem for Gaussian moments, which says<disp-formula id="equ26"><label>(26)</label><mml:math id="m40"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This immediately implies that<disp-formula id="equ27"><label>(27)</label><mml:math id="m41"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Once again, it's convenient to rewrite this expression in the Fourier domain<disp-formula id="equ28"><label>(28)</label><mml:math id="m42"><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>With these assumptions<disp-formula id="equ29"><label>(29)</label><mml:math id="m43"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="script">C</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>4</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo><mml:mtext> </mml:mtext></mml:mrow></mml:mrow></mml:mstyle><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>We evaluate the expectations over velocity by noting that each has the form<disp-formula id="equ30"><label>(30)</label><mml:math id="m44"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>+</mml:mo><mml:mi>v</mml:mi><mml:msub><mml:mi>δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>for some spatial offset <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow></mml:math></inline-formula> and temporal offsets <inline-formula><mml:math id="inf149"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>δ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, if we define<disp-formula id="equ31"><label>(31)</label><mml:math id="m45"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mstyle displaystyle="true"><mml:mo>∫</mml:mo></mml:mstyle><mml:mi>d</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>then<disp-formula id="equ32"><label>(32)</label><mml:math id="m46"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Power spectrum components contribute to the 2-point correlator's variance quadratically.</p><p>Putting these pieces together, the expected squared error achieved by a 2-point correlator is a quadratic function of the power spectrum<disp-formula id="equ33"><label>(33)</label><mml:math id="m47"><mml:mrow><mml:mi mathvariant="italic">ϵ</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We're interested to know whether spatial correlations can enhance the accuracy of the 2-point correlator. This will be the case unless a uniform power spectrum minimizes <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula>. Note that every physically meaningful power spectrum is non-negative<disp-formula id="equ34"><label>(34)</label><mml:math id="m48"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Thus, the minimum of <inline-formula><mml:math id="inf111"><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></inline-formula> either occurs at an extremum point or on the boundary of admissible solutions. If the minimum occurs on the boundary, then a subset of the <inline-formula><mml:math id="inf112"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are exactly equal to zero. In particular, the power spectrum would not be constant, which implies that the image ensemble would be spatially correlated. At an extremum point, we must find<disp-formula id="equ35"><label>(35)</label><mml:math id="m49"><mml:mrow><mml:mn>0</mml:mn><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi mathvariant="italic">ϵ</mml:mi></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>for every <inline-formula><mml:math id="inf113"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula>. A uniform power spectrum can only satisfy this condition if<disp-formula id="equ36"><label>(36)</label><mml:math id="m50"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>β</mml:mi><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf114"><mml:mrow><mml:mi>β</mml:mi><mml:mo>&gt;</mml:mo><mml:mo>0</mml:mo></mml:mrow></mml:math></inline-formula> is the (constant) value of each power spectrum component. This is generally not the case, so correlations exist that would help typical 2-point correlators.</p><p>For example, the simplest HRC, which replaces the low-pass and high-pass filters with pure time delays, is<disp-formula id="equ37"><label>(37)</label><mml:math id="m51"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf115"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:math></inline-formula> is a constant with units of °/s. For this model,<disp-formula id="equ38"><label>(38)</label><mml:math id="m52"><mml:mrow><mml:msubsup><mml:mi>κ</mml:mi><mml:mrow><mml:mn>1,2</mml:mn></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mn>2</mml:mn><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Substituting this expression into the above formulas, we find<disp-formula id="equ39"><label>(39)</label><mml:math id="m53"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>A</mml:mi><mml:mi>k</mml:mi><mml:mi>τ</mml:mi><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ40"><label>(40)</label><mml:math id="m54"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>3</mml:mn><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sin</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>τ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math id="inf116"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> is the spacing between adjacent photoreceptors. Note that<disp-formula id="equ41"><label>(41)</label><mml:math id="m55"><mml:mrow><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.</mml:mn></mml:mrow></mml:math></disp-formula></p><p>On the other hand,<disp-formula id="equ42"><label>(42)</label><mml:math id="m56"><mml:mrow><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>A</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">cos</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:msub><mml:mi mathvariant="normal">Δ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This does not approach zero, even for large values of <inline-formula><mml:math id="inf117"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>1</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula>. Therefore, <inline-formula><mml:math id="inf27"><mml:mrow><mml:msubsup><mml:mrow><mml:msup><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> diverges and <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>≠</mml:mo><mml:mi>β</mml:mi><mml:msubsup><mml:mrow><mml:msup><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mtext>​</mml:mtext></mml:msup></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>′</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mi>∞</mml:mi></mml:msubsup><mml:msub><mml:mi mathvariant="normal">Γ</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mi>k</mml:mi><mml:mo>′</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. In this model, high frequency components lack signal but contribute noise. It's helpful if these frequency components are absent from the image ensemble. Future work should more fully investigate the role of spatial correlations in naturalistic motion estimation.</p></sec></boxed-text></app><app id="app6"><title>Appendix 6</title><boxed-text><sec id="s11" sec-type="appendix"><title>Front-end nonlinearities give the HRC access to higher-order correlations.</title><p>The response of the front-end nonlinearity model to a 3-point glider stimulus is determined by the higher-order correlations that it detects in the stimulus. Furthermore, we argued in <xref ref-type="app" rid="app1">Appendix 1</xref> and <xref ref-type="fig" rid="fig1">Figure 1I</xref> that higher-order correlations can contribute to the accuracy of visual motion estimators. We now describe how front-end nonlinearities provide pair-correlation mechanisms with access to certain types of higher-order correlations.</p><p>We suppose that the front-end nonlinearity, denoted <inline-formula><mml:math id="inf118"><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula>, has a power series expansion:<disp-formula id="equ43"><label>(43)</label><mml:math id="m57"><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msup><mml:mi>x</mml:mi><mml:mi>n</mml:mi></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Then the cross-correlation function between two non-linearly transformed input streams, denoted <inline-formula><mml:math id="inf119"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>1</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf120"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>2</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, is<disp-formula id="equ44"><label>(44)</label><mml:math id="m58"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:msub><mml:mi>h</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>〈</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>〉</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <mml:math id="inf121"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow><mml:mi>1</mml:mi></mml:msub></mml:mrow></mml:math> and <mml:math id="inf122"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:mrow><mml:mi>2</mml:mi></mml:msub></mml:mrow></mml:math> are linear photoreceptor signals. This substitution explicitly demonstrates that the front-end nonlinear transformation enables pair correlation mechanisms to incorporate higher-order correlations of the form <inline-formula><mml:math id="inf29"><mml:mrow><mml:mo>〈</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:math></inline-formula>. The choice of nonlinearity specifies the expansion coefficients, <mml:math id="inf123"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math>, which in turn determines the pattern of higher-order correlations that the pair correlator incorporates into its velocity estimate. For example, sensitivity to odd-ordered correlations demands that <mml:math id="inf124"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>h</mml:mi></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math> be large for some even values of <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula>. These expansion coefficients would manifest themselves in the structure of the front-end nonlinearity as asymmetries between positive and negative contrasts, but strong asymmetries were not needed to eliminate kurtosis in natural image ensembles (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Inversely, one could use this equation to determine whether a set of expansion coefficients exist that would implement a desired series of multipoint correlators. The preceding argument implies that strongly asymmetric front-end nonlinearities would be needed to account for the 3-point glider responses.</p></sec></boxed-text></app><app id="app7"><title>Appendix 7</title><boxed-text><sec id="s12" sec-type="appendix"><title>Expansion of the weighted 4-quadrant model.</title><p>In this Appendix, we rewrite the weighted 4-quadrant model in a basis that isolates its dependence on 2-point correlations, on higher-even-ordered correlations, and on two types of odd-ordered correlations. In <xref ref-type="app" rid="app8">Appendix 8</xref>, we'll discuss the motion estimation performance of the weighted 4-quadrant model in this basis in order to gain insight into why performance-optimized weighted 4-quadrant models also predict 3-point glider responses that resemble <italic>Drosophila</italic> behavior.</p><p>The weighted 4-quadrant model supposes that the input signals are segregated into four separate streams:<disp-formula id="equ45"><label>(45)</label><mml:math id="m59"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>where <inline-formula><mml:math id="inf126"><mml:mrow><mml:msub><mml:mi>Q</mml:mi><mml:mi mathvariant="italic">ab</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> denotes the (<inline-formula><mml:math id="inf127"><mml:mrow><mml:mi mathvariant="italic">ab</mml:mi></mml:mrow></mml:math></inline-formula>) quadrant for <inline-formula><mml:math id="inf30"><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf129"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&gt;</mml:mo><mml:mo>0</mml:mo></mml:mrow></mml:math></inline-formula> and is zero otherwise, and <inline-formula><mml:math id="inf32"><mml:mrow><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> is <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>x</mml:mi><mml:mo>&lt;</mml:mo><mml:mo>0</mml:mo></mml:mrow></mml:math></inline-formula> and is zero otherwise. The HRC is equal to<disp-formula id="equ46"><label>(46)</label><mml:math id="m60"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>More generally, we suppose that <italic>Drosophila</italic> could estimate motion as any linear combination of these signals, and we define the weighted 4-quadrant model as<disp-formula id="equ47"><label>(47)</label><mml:math id="m61"><mml:mrow><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf33"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf34"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf35"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf36"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula> are linear weighting coefficients that specify the computation performed by the model. Since this section, and the next two, focus entirely on the weighted 4-quadrant model, we simplify notation by dropping the superscript (<inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:math></inline-formula>).</p><p>The weighted 4-quadrant model can be rewritten in an alternate form that facilitates an understanding of how various correlation types contribute to its motion estimates. We begin by noting that<disp-formula id="equ48"><label>(48)</label><mml:math id="m62"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ49"><label>(49)</label><mml:math id="m63"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mi>x</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>−</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf190"><mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is +1 for positive arguments and −1 for negative arguments. We thus see that<disp-formula id="equ50"><label>(50)</label><mml:math id="m64"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mi>Q</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>a</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>b</mml:mi></mml:msub><mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mi>a</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi>b</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Therefore, the complete weighted 4-quadrant model is<disp-formula id="equ51"><label>(51)</label><mml:math id="m65"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>Q</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>This expression for the weighted 4-quadrant model groups the four weighting coefficients into four alternate terms. The first term is proportional to a standard HRC, which computes second-order correlations. We denote its associated coefficient as<disp-formula id="equ52"><label>(52)</label><mml:math id="m66"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The second and third terms invert sign and retain magnitude under contrast inversion. Therefore, they only compute odd-ordered correlations:<disp-formula id="equ53"><label>(53)</label><mml:math id="m67"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>odd</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ54"><label>(54)</label><mml:math id="m68"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>odd*</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The fourth term is unaffected by contrast inversion. Thus, it only computes even-ordered correlations. We'll soon see that the lowest-order contribution from this term is fourth-order, so we denote its coefficient as<disp-formula id="equ55"><label>(55)</label><mml:math id="m69"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>4</mml:mn></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>These four coefficients define the correlational basis considered in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>. For example, note that <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref> shows the transformation defined by <xref ref-type="disp-formula" rid="equ52">Equations 52</xref>–<xref ref-type="disp-formula" rid="equ55">55</xref>.</p><p>Because <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a non-analytic function, it is still somewhat opaque how the weighted 4-quadrant model relates to specific higher-order correlations in the visual stimulus. We thus rewrite <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as the limit of an analytic function:<disp-formula id="equ56"><label>(56)</label><mml:math id="m70"><mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:mtext>lim</mml:mtext></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>→</mml:mo><mml:mi>∞</mml:mi></mml:mrow></mml:munder><mml:mtext>erf</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where<disp-formula id="equ57"><label>(57)</label><mml:math id="m71"><mml:mrow><mml:mtext>erf</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mn>0</mml:mn><mml:mi>x</mml:mi></mml:msubsup><mml:mrow><mml:mi>d</mml:mi><mml:mi>y</mml:mi></mml:mrow></mml:mrow></mml:mstyle><mml:mtext> </mml:mtext><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula>is the Gauss error function. The Gauss error function is entire, which means that it has a power series expansion for any value <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula>. Also note that since real biological nonlinearities are not infinitely sharp, a more realistic weighted 4-quadrant model would fix <inline-formula><mml:math id="inf135"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula> at a finite value. We thus consider the follow approximation,<disp-formula id="equ58"><label>(58)</label><mml:math id="m72"><mml:mrow><mml:mi mathvariant="normal">sgn</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder></mml:mrow><mml:mi>∞</mml:mi></mml:mover></mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>2</mml:mn><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>3</mml:mn></mml:msup></mml:mrow><mml:mn>3</mml:mn></mml:mfrac><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>5</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf39"><mml:mrow><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>∞</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Although high-order terms might not be negligible in this expansion, the contributions of low-order correlations to visual motion estimation are set by low-order terms. In particular, the contributions of second, third, and fourth-order correlations to the weighted four quadrant model are determined by the leading terms in the expansion,<disp-formula id="equ59"><label>(59)</label><mml:math id="m73"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mi>F</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>odd</mml:mtext></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>odd*</mml:mtext></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mi>π</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mn>4</mml:mn><mml:msup><mml:mi>β</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mi>π</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>O</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:msup><mml:mi>V</mml:mi><mml:mn>5</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Thus, the third-order term associated with <inline-formula><mml:math id="inf136"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>odd</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> squares the low-pass filtered signal and might help to account for light–dark asymmetries in the low-pass filtered signal. The third-order term associated with <inline-formula><mml:math id="inf185"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mtext>odd</mml:mtext><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> squares the high-pass filtered signal. Finally, note that this formula confirms that the lowest-order term associated with <inline-formula><mml:math id="inf137"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mi>even</mml:mi><mml:mo>&gt;</mml:mo><mml:mo>2</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is fourth-order.</p></sec></boxed-text></app><app id="app8"><title>Appendix 8</title><boxed-text><sec id="s13" sec-type="appendix"><title>The weighted 4-quadrant model improves motion estimation with odd-ordered correlations.</title><p>In the main text we quantitatively characterized the weighted 4-quadrant model by discussing its accuracy given various subsets of the four quadrants (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Here we consider the performance of the weighted 4-quadrant model in the correlational basis defined in <xref ref-type="app" rid="app7">Appendix 7</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>. These results lead to a simple interpretation of the computation performed by performance optimized weighted 4-quadrant models.</p><p>Models that oriented all of their weight along the even = 2 axis outperformed models that focused their weight along any other correlational axis (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). This reinforces the foremost importance of second-order correlations for motion estimation. In isolation, odd-ordered correlations were weaker predictors of motion than second-order correlations (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). Nevertheless, the odd class well complemented the HRC, and the full accuracy of the weighted 4-quadrant model was obtained by linearly combining the even = 2 and odd correlation classes (<italic>best 2 bar</italic>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). This result suggests that the weighted 4-quadrant model has two relevant dimensions. In particular, accurate models combine an HRC with odd-ordered correlations that account for statistical light–dark asymmetries in the HRC's low-pass filtered branch.</p><p>Since the weighted 4-quadrant model only has four parameters, it's possible to exhaustively study its parameter dependence. We have in mind models that are correctly scaled, in which case the mean squared error is determined by the correlation coefficient (<xref ref-type="app" rid="app2">Appendix 2</xref>). Since the value of the correlation coefficient is unchanged when all four weighting coefficients are scaled by the same positive factor, it suffices to consider weighting coefficients drawn from the 3-sphere, such that <inline-formula><mml:math id="inf40"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>+</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>+</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo>−</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Because the 3-sphere has a finite volume, we were able to densely sample the correlation coefficient for all parameter values (<xref ref-type="fig" rid="fig8">Appendix figure 3</xref>). This function has one global maximum, corresponding to the optimal weight vector discussed in the main text. Its global minimum occurs on the polar opposite side of the 3-sphere, where the weighted 4-quadrant model is most strongly anti-correlated with the velocity. More generally, correlation coefficients corresponding to model parameters on opposite poles of the 3-sphere always have the same magnitude and opposite sign. Both models explain the same amount of variance about the velocity, and they become equivalent after they're correctly scaled. Thus, we henceforth focus our discussion on the hemisphere where the correlation coefficient was positive.<fig id="fig8" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.017</object-id><label>Appendix figure 3.</label><caption><title>Accuracy of the weighted 4-quadrant model across model parameters.</title><p>(<bold>A</bold>, <bold>B</bold>) We computed the correlation coefficient between the velocity and the response of the weighted 4-quadrant model for all possible sets of model parameters. Since rescaling the weight vector does not affect the correlation coefficient, we assumed that all model parameters satisfy <inline-formula><mml:math id="inf41"><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mo>∑</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mo>+</mml:mo><mml:mo>,</mml:mo><mml:mo>−</mml:mo><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>Q</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:mrow></mml:math></inline-formula>. We color-coded each set of model parameters by its accuracy and projected the parameter space onto various subspaces. (<bold>A</bold>) We first examined the quadrant basis by projecting onto the {(− −), (− +)} (<italic>left</italic>) and {(+ −), (+ +)} (<italic>right</italic>) subspaces. (<bold>B</bold>) We next examined the correlational basis by projecting onto the {even = 2, odd} (<italic>left</italic>) and {odd*, even &gt;2} (<italic>right</italic>) subspaces. These project into different linear combinations of the original quadrant weightings. One of the projections is the pure HRC (even = 2), while the other projections contain only odd correlations, of two different types (odd and odd*), or only even correlations of order greater than 2 (even &gt;2). These projections show that accurate weighted 4-quadrant models always put positive weight into 2-point correlations and negative weight into odd-ordered correlations. Note that the glider responses predicted by the weighted 4-quadrant model mirror this pattern (<xref ref-type="fig" rid="fig3">Figure 3D</xref>).</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.017">http://dx.doi.org/10.7554/eLife.09123.017</ext-link></p></caption><graphic xlink:href="elife-09123-app8-fig1-v2.tif"/></fig></p><p>Weighted 4-quadrant models were most accurate when <inline-formula><mml:math id="inf154"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mo>−+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf155"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mo>−−</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> were large (<xref ref-type="fig" rid="fig8">Appendix figure 3A</xref>, <italic>left</italic>) and <inline-formula><mml:math id="inf150"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mo>++</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf151"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mo>−+</mml:mo></mml:msub></mml:mrow></mml:math></inline-formula> were small (<xref ref-type="fig" rid="fig8">Appendix figure 3A</xref>, <italic>right</italic>). In the correlational basis, the HRC is the model with maximum weight in <mml:math id="inf152"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:mrow><mml:mi>even=2</mml:mi></mml:msub></mml:mrow></mml:math> and with zero weight in <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>odd</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf186"><mml:mrow><mml:msub><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mtext>odd</mml:mtext><mml:mo>*</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf187"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Thus, this basis makes it easy to compare the accuracy of the HRC to other weighted 4-quadrant models (<xref ref-type="fig" rid="fig8">Appendix figure 3B</xref>). Furthermore, this basis clearly sorts the weighted 4-quadrant models according to their accuracy and confirms that that the accuracy of a weighted 4-quadrant model is largely determined by <inline-formula><mml:math id="inf156"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>even</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf157"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mi>odd</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig8">Appendix figure 3B</xref>, <italic>left</italic>). Higher even-ordered correlations and odd-ordered correlations that account for light–dark asymmetries in the high-pass filtered visual signals did not contribute prominently to the accuracy of the weighted 4-quadrant model (<xref ref-type="fig" rid="fig8">Appendix figure 3B</xref>, <italic>right</italic>). Interestingly, <xref ref-type="fig" rid="fig8">Appendix figure 3A</xref> shows that there is a diversity of ways to combine the four quadrants in order to improve the accuracy of the HRC, which translates into a diversity of correlational responses (<xref ref-type="fig" rid="fig8">Appendix figure 3B</xref>). Similarly, the HRC is only one of many models that achieve a comparable level of accuracy. Every other motion estimator that achieves the HRC's performance level incorporates higher-order correlations into its estimate.</p></sec></boxed-text></app><app id="app9"><title>Appendix 9</title><boxed-text><sec id="s14" sec-type="appendix"><title>The weighted 4-quadrant model in the basis of PCs.</title><p>PCA is a popular method to reduce the dimensionality of neural population recordings. In this section, we conceptualize the four quadrants as a small neural population and study how each PC accounts for variance in the system and contributes to motion estimation. We show that most of the weighted 4-quadrant model's variance is due to two of the four PCs. Interestingly, most of this variance is not velocity-related, and we show that the two low-variance PCs are the ones that dominate motion estimation.</p><p>We began by directly applying PCA to the weighted 4-quadrant model. We computed the 4 × 4 covariance matrix of the four quadrants over the ensemble of simulated motions (<xref ref-type="fig" rid="fig9">Appendix figure 4A</xref>). The eigenvectors of the covariance matrix are called the PCs (<xref ref-type="fig" rid="fig9">Appendix figure 4B</xref>), and the associated eigenvalues specify the amount of variance accounted for by each PC (<xref ref-type="fig" rid="fig9">Appendix figure 4C</xref>). We found that the first two PCs accounted for 86.3% of the variance, whereas the third and fourth PCs each contributed about 7% of the variance (<xref ref-type="fig" rid="fig9">Appendix figure 4C</xref>). The high-variance eigenvectors roughly corresponded to a sum and a difference of the (+ +) and (+ −) quadrants, whereas the low-variance PCs roughly corresponded to a sum and a difference of the (− +) and (− −) quadrants (<xref ref-type="fig" rid="fig9">Appendix figure 4B</xref>). The (− −) and (− +) quadrants best facilitated motion estimation (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). Thus, the low-variance PCs were most important for motion estimation.<fig id="fig9" position="float"><object-id pub-id-type="doi">10.7554/eLife.09123.018</object-id><label>Appendix figure 4.</label><caption><title>The weighted 4-quadrant model in the basis of principal components (PCs).</title><p>(<bold>A</bold>) We computed the covariance matrix of quadrant responses across the simulated ensemble of naturalistic motions. (<bold>B</bold>) The eigenvectors of the covariance matrix are called PCs. Signals from the (+ +) and (+ −) quadrants primarily comprised the first two PCs, whereas the (− +) and (− −) components comprised the third and fourth PCs. (<bold>C</bold>) The first two PCs accounted for the vast majority of the weighted 4-quadrant model's response variance. (<bold>D</bold>) Each member of the ensemble of naturalistic motions comprised a velocity and a natural image, and both components contributed variance to the model response. Although the first two PCs accounted for most of the variance, little of that variance was associated with the velocity of motion. Instead, the third and fourth PCs best aided motion estimation, because they contributed the vast majority of the velocity-associated variance.</p><p><bold>DOI:</bold> <ext-link ext-link-type="doi" xlink:href="10.7554/eLife.09123.018">http://dx.doi.org/10.7554/eLife.09123.018</ext-link></p></caption><graphic xlink:href="elife-09123-app9-fig1-v2.tif"/></fig></p><p>This result is counter to one's usual intuition, but it is a straightforward consequence of the mathematics of linear regression and PCA. We want to linearly combine the PC signals to best predict the velocity:<disp-formula id="equ60"><label>(60)</label><mml:math id="m90"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mtext>argmin</mml:mtext><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:math></inline-formula> is a four-dimensional column vector of weights, <inline-formula><mml:math id="inf188"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> denotes the velocity, the superscript <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> denotes the matrix transpose, and <inline-formula><mml:math id="inf160"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> is the 4-vector of PC signals. The solution to this problem is well-known from the theory of linear regression:<disp-formula id="equ61"><label>(61)</label><mml:math id="m91"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mi>M</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mi>U</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the covariance matrix of the predictors, and <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the covariance of each predictor with the velocity. In practice, we estimate these expectations from the empirical data, and PCs are uncorrelated over the naturalistic motion ensemble by construction<disp-formula id="equ62"><label>(62)</label><mml:math id="m92"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <mml:math id="inf161"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math> is the variance associated with <inline-formula><mml:math id="inf162"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>th PC, and <inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the Kronecker <italic>δ</italic>-function. Thus,<disp-formula id="equ63"><label>(63)</label><mml:math id="m93"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msubsup><mml:mi>x</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <mml:math id="inf164"><mml:mrow><mml:msub><mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mrow><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:math> is the standard deviation of the velocity signal, and <inline-formula><mml:math id="inf165"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is the correlation coefficient between the velocity and the <inline-formula><mml:math id="inf166"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>th PC.</p><p>It is also easy to calculate the correlation coefficient between the true velocity and the estimated velocity. First note that<disp-formula id="equ64"><label>(64)</label><mml:math id="m94"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:msqrt><mml:mi>λ</mml:mi></mml:msqrt></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msqrt><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msqrt><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ65"><label>(65)</label><mml:math id="m95"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>β</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus the square of the correlation coefficient between the true and estimated velocities is<disp-formula id="equ66"><label>(66)</label><mml:math id="m96"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Because the PCs are uncorrelated, each contributes independently to the motion estimator's accuracy. The amount that each PC contributes to the estimation accuracy is determined by its correlation with the velocity, and all dependence on the total amount of variance associated with the PC has dropped out entirely. These conclusions are also true when we look at the squared error directly<disp-formula id="equ67"><label>(67)</label><mml:math id="m97"><mml:mrow><mml:mi mathvariant="italic">ϵ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:msup><mml:mi>β</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mi>x</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:munder><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mi>i</mml:mi></mml:munder><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As would be expected from this formula, the third and fourth PCs account for much more of the velocity-associated variance than the first and second PCs (<xref ref-type="fig" rid="fig9">Appendix figure 4D</xref>). Nevertheless, the first PC does account for a significant portion of the velocity-associated variance (<xref ref-type="fig" rid="fig9">Appendix figure 4D</xref>), so the basis of PCs does not fully reveal the structure that was apparent in the correlational basis (<xref ref-type="app" rid="app8">Appendix 8</xref>).</p></sec></boxed-text></app><app id="app10"><title>Appendix 10</title><boxed-text><sec id="s15" sec-type="appendix"><title>Novel use of low-order signatures for motion estimation.</title><p>The non-multiplicative nonlinearity model (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) relaxed the assumption that <italic>Drosophila</italic>'s motion estimator multiplies its inputs and substantially improved the accuracy of visual motion estimation (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). Surprisingly, the non-multiplicative nonlinearity model slightly outperformed the HRC when we parameterized it as a second-order polynomial (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). This indicates that there are useful low-order correlations that the HRC neglects. In this section, we will explain how visual motion estimators can sometimes productively incorporate computational signatures that do not nonlinearly combine signals across space.</p><p>This section considers computational signatures that clash harshly with our usual intuition for visual motion estimation, and we need to unpack <italic>how</italic> the motion estimator in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> works before we can understand <italic>why</italic> it works. The observed improvement results from a linear combination of the HRC<disp-formula id="equ68"><label>(68)</label><mml:math id="m98"><mml:mrow><mml:mi>R</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with a linear transformation of the photoreceptor signals<disp-formula id="equ69"><label>(69)</label><mml:math id="m99"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We thus must consider the motion estimator<disp-formula id="equ70"><label>(70)</label><mml:math id="m100"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>low</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mi>R</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mi>L</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf167"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> are the weighting coefficients that minimize the mean-squared error. Note that <inline-formula><mml:math id="inf169"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> linearly combines signals from multiple points in space. Like the HRC, it is mirror anti-symmetric:<disp-formula id="equ71"><label>(71)</label><mml:math id="m101"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>↦</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>⇒</mml:mo><mml:mi>L</mml:mi><mml:mo>↦</mml:mo><mml:mo>−</mml:mo><mml:mi>L</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>It is useful to take a detour to abstractly consider how motion estimation performance depends on the joint statistics of <inline-formula><mml:math id="inf170"><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf171"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula>, and the velocity of motion, <inline-formula><mml:math id="inf172"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula>. All three of these quantities are zero mean. We denote their variances as<disp-formula id="equ72"><label>(72)</label><mml:math id="m102"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and their cross-correlation coefficients as<disp-formula id="equ73"><label>(73)</label><mml:math id="m123"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>R</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:msub><mml:mi>σ</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The optimal weighting coefficients are determined by these quantities (see <xref ref-type="disp-formula" rid="equ61">Equation 61</xref>):<disp-formula id="equ74"><label>(74)</label><mml:math id="m80"><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>β</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>;</mml:mo></mml:mrow></mml:math></disp-formula>as is the correlation coefficient between the true velocity and <inline-formula><mml:math id="inf42"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>auto</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ75"><label>(75)</label><mml:math id="m81"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>low</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Across the simulated ensemble of naturalistic motions we empirically found that <inline-formula><mml:math id="inf43"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mn>0.24</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf44"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:mn>0.0017</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf45"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mo>−</mml:mo><mml:mn>0.28</mml:mn></mml:mrow></mml:math></inline-formula>. Thus, we note that <inline-formula><mml:math id="inf46"><mml:mrow><mml:mo>|</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>L</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>|</mml:mo><mml:mo>≪</mml:mo><mml:mo>|</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>|</mml:mo></mml:mrow></mml:math></inline-formula> and approximate the correlation coefficient as<disp-formula id="equ76"><label>(76)</label><mml:math id="m82"><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>low</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>≈</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:msqrt><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Thus, we expect the inclusion of the linear term <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> to improve the accuracy of motion estimation by about 4.3% (compare to <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). Interested readers can find a complete derivation of these equations in section V of the supplemental materials for (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>).</p><p>With this machinery in hand, we can start to understand the utility of the linear term. First, note that this term was only weakly correlated with the velocity across the simulated ensemble of motions. Furthermore, the correlation would have been <italic>exactly</italic> zero if <inline-formula><mml:math id="inf47"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> had been equal to <inline-formula><mml:math id="inf48"><mml:mrow><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as would have been the case for an ensemble that was perfectly translationally invariant. So the small correlation we observed between <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf175"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> is nothing more than residual noise resulting from a finitely sized data sample that did not explicitly enforce translation invariance. Nevertheless, it's critical to realize that <xref ref-type="disp-formula" rid="equ76">Equation 76</xref> treated <inline-formula><mml:math id="inf176"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> as if it <italic>were</italic> zero, yet it still managed to account for the results of <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. Thus, this residual sampling noise has nothing to do with the improvements offered by the hybrid estimator. As intuitively expected, the linear term is completely uncorrelated with the velocity of motion.</p><p><xref ref-type="disp-formula" rid="equ76">Equation 76</xref> suggests that a linear term, which is itself uncorrelated with the velocity of motion, can nevertheless help velocity estimation. However, this improvement demands that it be combined with another motion estimator that: (i) is correlated with the velocity (i.e., <inline-formula><mml:math id="inf49"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi>R</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>); and (ii) is correlated with the linear term (i.e., <inline-formula><mml:math id="inf50"><mml:mrow><mml:msup><mml:mi>c</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). Our numerical results indicate that the HRC is an example of such a motion estimator. The HRC obviously satisfies the first condition. To examine the second condition, we note that correlation between the HRC and the linear term is nonzero if and only if<disp-formula id="equ77"><label>(77)</label><mml:math id="m83"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>is nonzero. As long as the image ensemble is light–dark asymmetric, there are no symmetry principles that force this number to vanish for a general choice of <inline-formula><mml:math id="inf177"><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf178"><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:math></inline-formula>. Our numerical results show that the associated correlation coefficient is far from zero for natural inputs and our choices of filters. Fundamentally, this correlation can be nonzero because the HRC's response depends on the pattern that is moving, as does the linear response. Because image-induced variability is partially shared between the HRC and the linear term, the latter can help to eliminate image-induced noise from the HRC, thereby improving the motion estimate.</p><p>Although our results indicate that a linear term can improve local motion estimation, its benefits do not sum over space. In particular, imagine an ensemble of elementary motion detectors that combine a local HRC and a local linear estimator:<disp-formula id="equ78"><label>(78)</label><mml:math id="m84"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>low</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf179"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> indexes the first point in space surveyed by the <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>th local estimator. A whole field motion percept could be found by averaging these local motion signals over space<disp-formula id="equ79"><label>(79)</label><mml:math id="m85"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>low</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>low</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf181"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula> denotes the total number of local motion detectors. However, the second term in the linear estimator at point <inline-formula><mml:math id="inf182"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> cancels the first term in the linear estimator at point <inline-formula><mml:math id="inf183"><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Thus, spatial averaging eliminates most of the dependence on the linear term<disp-formula id="equ80"><label>(80)</label><mml:math id="m86"><mml:mrow><mml:msubsup><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>low</mml:mtext></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>−</mml:mo><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>All that remains of the linear term is a boundary term that depends on photoreceptor activity at the edges of the visual field. Furthermore, the magnitude of this contribution decreases with <inline-formula><mml:math id="inf184"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:math></inline-formula>. Thus, linear estimators have little utility for full-field motion estimation. Nevertheless, it's conceivable that such terms could play a role in <italic>Drosophila</italic>'s motion estimation circuit, because the same elementary motion detector is thought to underlie a wide variety of motion-guided behaviors, and the inclusion of this locally beneficial term is not detrimental to whole field motion estimation.</p><p>Finally we note that the principles discussed in the context of linear motion estimators also apply in other counterintuitive contexts. For example, consider an autocorrelator,<disp-formula id="equ81"><label>(81)</label><mml:math id="m87"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>which correlates visual signals from the same point in space. Like the HRC, it is mirror anti-symmetric:<disp-formula id="equ82"><label>(82)</label><mml:math id="m88"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>↦</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>⇒</mml:mo><mml:mi>A</mml:mi><mml:mo>↦</mml:mo><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>but it is uncorrelated with the velocity. Nevertheless, the autocorrelator's correlation with the HRC is determined by<disp-formula id="equ83"><label>(83)</label><mml:math id="m89"><mml:mtable columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>A</mml:mi></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>〉</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>〈</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>*</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>〉</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula>and need not be zero. Empirically, we find the relevant correlation coefficient to be −0.40 across the ensemble of naturalistic motions, so <xref ref-type="disp-formula" rid="equ76">Equation 76</xref> implies that this autocorrelator would enhance the HRC by 8.9%. However, such improvements do not sum over space. Thus, autocorrelators might be relevant for local motion estimates, but not for motion estimates that average over space.</p></sec></boxed-text></app><app id="app11"><title>Appendix 11</title><boxed-text><sec id="s16" sec-type="appendix"><title>Regarding the computational problem of visual motion estimation.</title><p>Throughout this paper, we have illustrated connections between the computations performed by our models and spatiotemporal correlations. These links are important for both practical and theoretical reasons. First, the many experimental successes of the HRC already suggest that the fly's computation of motion is organized around spatiotemporal correlations in the stimulus (<xref ref-type="bibr" rid="bib59">Silies et al., 2014</xref>). Thus, by relating our models to spatiotemporal correlations, we were able to discern how each model generalizes this canonical model. For example, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref> shows that the optimal weighted 4-quadrant model supplements the standard HRC with a specific subclass of odd-ordered correlations, an observation that both reiterates the importance of the HRC and highlights the most critical signals that it lacks. Second, spatiotemporal correlations provide a fundamental connection between the motion estimation strategies used by invertebrates and vertebrates (<xref ref-type="bibr" rid="bib1">Adelson and Bergen, 1985</xref>; <xref ref-type="bibr" rid="bib71">van Santen and Sperling, 1985</xref>). In particular, although the HRC and motion energy models differ in their architectural details, both models are ultimately driven by 2-point correlations in the stimulus. Therefore, general arguments framed in terms of spatiotemporal correlations are easy to investigate in the specific context of either the HRC or motion energy model. Third, an understanding of the spatiotemporal correlations computed by each model facilitates the design of psychophysical experiments that test the models. For example, glider stimuli (<xref ref-type="bibr" rid="bib27">Hu and Victor, 2010</xref>) provide flexible experimental tools to probe how specific correlations contribute to motion percepts. Future work will lead to a variety of more realistic models that can also be characterized by the stimulus correlations that they detect. These models can be distinguished by carefully designed glider experiments.</p><p>From a theoretical point of view, correlation functions are important because they provide a mathematical basis in which to decompose neural computations (<xref ref-type="bibr" rid="bib48">Poggio and Reichardt, 1973, 1980</xref>; <xref ref-type="bibr" rid="bib18">Fitzgerald et al., 2011</xref>). David Marr famously proposed that neural computation must be understood at several levels (<xref ref-type="bibr" rid="bib8">Marr and Poggio, 1976</xref>). He described his second level as “that at which the algorithms that implement a computation are characterized.” Our emphasis on correlation functions is directed towards unraveling motion estimation at this algorithmic level. As illustrated concretely by <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, it's possible for an algorithm to have a simple characterization in terms of correlation functions, even when the fundamental computational units (e.g., the quadrants) do not actually compute correlations. Furthermore, correlation functions intuitively relate the visual signatures of motion to measurable features of natural visual environments (<xref ref-type="fig" rid="fig6">Appendix figure 1</xref>). Nevertheless, it's possible that correlation functions will ultimately provide an inefficient basis for representing the algorithms of visual motion estimation. For example, although the weighted 4-quadrant model is well understood in terms of the correlations that it detects, it would be nontrivial to discern its underlying simplicity based solely on its responses to glider stimuli, because the constraints relating various higher order correlators would be <italic>a priori</italic> unknown. Overall, we consider correlation functions to provide a useful lens for characterizing and understanding the algorithms of visual motion estimation, but research should also consider visual motion estimation in alternate bases that might reflect the brain's biological substrates more directly (<xref ref-type="bibr" rid="bib55">Rust et al., 2006</xref>).</p><p>Our characterization of visual motion estimation in terms of correlation functions provides an interesting perspective on the computational problem faced by <italic>Drosophila</italic>'s visual motion estimator in natural environments. Natural images contain many low and high-order correlations (<xref ref-type="bibr" rid="bib21">Geisler, 2008</xref>), and this implies that the fly brain could in principle use a wide array of correlations for visual motion estimation (<xref ref-type="fig" rid="fig6">Appendix figure 1</xref>). However, each correlation is only weakly associated with the velocity of motion in naturalistic settings (<xref ref-type="bibr" rid="bib15">Dror et al., 2001</xref>; <xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>). The reason for this is that the specific structure of the scene that is moving acts as a nuisance parameter that hinders the unambiguous assignment of a velocity to pattern of light input. For example, it's well known that the temporal frequency of a moving sinusoidal grating shapes the HRC's output (<xref ref-type="bibr" rid="bib4">Egelhaaf et al., 1989</xref>), thereby conflating the velocity with the grating's spatial frequency. More generally, the variability of a multipoint correlator across an ensemble of moving scenes is determined by higher-order statistics of the image ensemble (e.g., see <xref ref-type="app" rid="app2">Appendix 2</xref>). The fact that the same natural image drives every multipoint correlator also implies that the correlators co-vary with each other across the naturalistic motion ensemble. This shared variability can sometimes enable higher-order multipoint correlators to compensate effectively for image-induced noise that contaminates the HRC (<xref ref-type="bibr" rid="bib13">Clark et al., 2014</xref>).</p><p>Questions of how brains compute behaviorally relevant stimulus features from sensory inputs are central to neuroscience, but they are extraordinarily difficult to answer, even in principle. In the context of <italic>Drosophila</italic>'s visual motion estimator, the ensemble of photoreceptor signals contains many nonlinear cues that are weakly correlated with the stimulus velocity and with each other under naturalistic conditions. There are many ways to pool these signals into an improved motion estimate. The space of possible stimuli is astronomically large, so it is impossible for experiments to sample it completely. Nevertheless, synthetic laboratory stimuli can be designed to rule out specific algorithms that the brain might use to estimate motion. Thus, to deconstruct a neural computation, one must find ways to dramatically restrict the space of candidate models and to identify interesting models that can be experimentally ruled out. It's important to note that we did not construct our models to reproduce the behavioral data, even though this is a straightforward exercise (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Instead we aimed for a predictive framework that can relate behavioral responses to the statistics of natural sensory inputs, the statistics of natural behavior, and the constraints imposed by the neural circuits that implement the computation. Such constructions are complicated and depend on features of neural circuits that are incompletely known. Nevertheless, we hope that this added complexity will eventually pay off in computational models that have a rational structure from the viewpoint of the stimulus, the animal, and the brain.</p></sec></boxed-text></app></app-group></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.09123.019</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Carandini</surname><given-names>Matteo</given-names></name><role>Reviewing editor</role><aff><institution>University College London</institution>, <country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for submitting your work entitled “Naturalistic visual motion estimation by <italic>Drosophila</italic>” for peer review at <italic>eLife</italic>. Your submission has been favorably evaluated by Eve Marder (Senior Editor), Matteo Carandini (Reviewing Editor), Jonathan Victor (peer reviewer), and two other reviewers.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>This is an important computational and mathematical study of visual motion analysis: what kinds of motion signals are present in the natural environment, how they can be extracted by biologically-plausible neural circuitry, and the features that govern their performance. The point of reference for the paper is the Hassenstein-Reichardt correlator (HRC), the canonical model for motion signal detection, and the starting point for a great deal of important work on how motion computations are reduced to circuitry. The main point of the paper is that this model fails to account for some qualitative aspects of motion processing (responses to glider stimuli), and that relatively simple extensions of the model enable it to properly predict responses to glider stimuli, and also, substantially improve its accuracy for the naturalistic motion stimuli that are the focus of the paper.</p><p>The big-picture findings (which at the moment are very difficult to grasp, see below) are important for the community at large: simple multiplication is not the best approach, and deviations from multiplication – which are being revealed by circuit-level analysis – should likely be viewed as a feature, rather than a bug. This changes the way we view the analysis of the implementation of the HRC in circuitry – rather than attempting to understand how neurons carry out a multiplication, the focus is shifted into how they make use of nonlinearities to do a computation that is, perhaps surprisingly, even better for the task.</p><p>Essential revisions:</p><p>The paper is currently written in such a way that only an exquisitely trained and alert specialist can appreciate these points. It is essential that the paper be reorganized so that there are clear questions set in context of the literature, and the paper and its logic become easier to follow for someone who is not working on motion detection in fly.</p><p>The current organization of the paper, indeed, is obscure. The paper starts by introducing 4 models that are elaborations of the Reichardt detector: (1) multiple channels that explicitly construct higher order correlation statistics, (2) A static front end nonlinearity, (3) separate correlations between lights and darks, (4) nonlinear combination through operations other than multiplication. This is a lot to take in, and only the most motivated readers will get past this point. Most readers, instead, will stop there. It would be much better to introduce the Reichardt detector (and ideally an energy model), show their failings (e.g. with glider stimuli) and then motivate ways in which they could be improved, and introduce a few of the alternative models (do we need all 4?). Otherwise, introducing the 4 models right away feels like introducing solutions in search of a problem.</p><p>A key problem is that all the models considered seem to work better than the Reichardt model, with no compelling argument for one vs. the other. Possibly this should be regarded as generating a set of alternate hypotheses for physiologists and anatomists to investigate, but it would be useful to understand better what separates these models.</p><p>Because of this, and because the style of writing is opaque, it is currently difficult to discern the “bottom line” in this paper. It reads as a survey of many possible models and some of their virtues and discontents.</p><p>It is essential for the authors to decide what exactly they want the paper to convey, and set out a clear set of questions in Introduction, answer them in Results, and return to them in Discussion. The current paper, by contrast, seems to change its mind along the way as to what those questions are, and the reader is left grasping for a specific set of questions that need addressing. For instance, the Discussion (second paragraph) emphasizes processing in segregated ON and OFF channels. Is this the main take-home message? If so, it should be clearly set out as a question in the Introduction.</p><p>More generally, it seems essential that the authors introduce the problem in a broader context. As written, the paper uses computational methods to specifically examine some possible mechanisms of motion estimation in flies in view of the inability of the traditional Reichardt model to explain how animals use higher order motion cues. These are important issues, but to engage that broader interest it would be good to relate more substantially with the literature on higher order statistics in natural scenes and adaptation of circuit structure and perceptual phenomena to these statistics. Some suggestions appear in Reviewer 3's specific comments, appended below.</p><p>The language needs to become clearer. For instance, at the end of the subsection “Model responses to glider stimuli”, you state, “utility of higher-order correlations for naturalistic motion processing is not restricted to a specific neural circuit implementation.” What does this mean?</p><p>Also, while it is understandable that the authors may want to protect the reader from too much detail, at the moment the paper seems to put too much away into the appendices for the reader to be able to follow exactly what is going on. In turn, the material in the appendices is written in an extremely mathematical style, too much so for the readers of this journal – for example, “simply” is overused.</p><p>In addition to these key issues of organization and style, there are also some conceptual aspects that need work:</p><p>1) The paper seems to concern only rigid, constant, fronto-parallel motion. While this limitation needs to be explicitly stated, it also increases the strength of the findings. That is, there are many ways in which real motion estimation differ from this simplified scenario: for example, there are objects that may be moving independently of the visual flow, and objects may move in depth. Getting rid of these confounds will further complicate algorithms for extraction of visual flow, and influence what is “optimal”. So it is really quite remarkable that even with these real-world complications neglected, the authors still find that extraction of motion is benefited by mechanisms that make use of high-order nonlinearities.</p><p>2) The authors' choice of a way to evaluate performance, i.e. correlation of the inferred motion signal with the veridical motion, is problematic for two related reasons. Firstly, performance can be increased merely by attenuating large-magnitude outliers in the estimate (even simply by passing the output through a sigmoidal nonlinearity, independent of the stimulus). Secondly, the mathematical analysis is complicated because of the need to compute the denominator. There is no single best measure of performance, but for the above reasons, correlation is suboptimal. Two possible alternatives are: (a) to use the covariance (or, equivalently, mean-squared error) more extensively – this gets rid of the denominator problem, and may also simplify the understanding of the role of the autocorrelators; (b) to use mutual information between true and estimated velocity. The advantage of this is that it will not depend on any output transformation. There's a good argument that this is biologically appropriate, since after all, the fly does not need to know the velocity, it just needs to control its behavior – and there may well be nonlinear transformations that intervene between the motion signal output, and the motor command. In sum, it would be useful to know whether maximizing covariance, or maximizing information about velocity, would lead to the same conclusions as maximizing correlation. Note that the authors don't necessarily need to refit the models to maximize mutual information. Rather, all that is needed is to take the existing fitted models and calculate the mutual information from the scatter plot of true velocity vs. calculated velocity – a one dimensional calculation that can be done with binning in, say, 16 velocity bins of approximately equal occupancy.</p><p>3) The benefits of the autocorrelator are not clear, despite the paper's lengthy material on this point. To get off the ground: is it the case that the stimulus set is symmetric with respect to velocity – i.e., that each spatial profile is presented as moving both with positive and negative velocity? If this is not the case, then it would seem important to explain why this is justified, and of course, autocorrelators may help. And unless this asymmetry is present in natural stimuli, it would seem to be a flaw in the analysis that needs to be fixed.</p><p>Indeed, perhaps the stimulus set is actually not symmetric with respect to velocity. This is suggested by the sentence following <xref ref-type="disp-formula" rid="equ7">Eq. 7</xref> of <xref ref-type="app" rid="app4">Appendix 4</xref>: “The performance of the combined motion estimator is superior to the original direction selective estimator whenever the latter [i.e., the direction selective estimator] has a nonzero correlation with the autocorrelator.”</p><p>But if the stimulus set is symmetric in this regard, the result is quite puzzling. Let's say a particular estimator A that was optimal had the following behavior. For a particular stimulus moving with a positive velocity (say, S<sup>+</sup>, with signed velocity v(S<sup>+</sup>)) gave a result E<sup>+</sup>, and for the same stimulus moving with a negative velocity (say, S<sup>-</sup>, with signed velocity v(S<sup>-</sup>)=-v(S<sup>+</sup>)) gave a result E<sup>-</sup>. We could also construct an estimator A*, which first inverted the stimulus, and then applied A, and then inverted the result – so that A* would give the result of-(E-) for S<sup>+</sup>, and -(E<sup>+</sup>) for S<sup>-</sup>. If A is optimal, then (by symmetry) A* would also have to be optimal. But then it would seem that a new estimator, B=(A+A*)/2, would be better than both, unless A and A* are identical. The reason for this is that B removes any bias (towards either positive or negative velocities) that A or A* might have, and one can always decompose the error (across the entire ensemble) into the error for the summed velocity (v(S<sup>+</sup>)+v(S<sup>-</sup>)) and for the difference velocity (v(S<sup>+</sup>)-v(S<sup>-</sup>)), for each pair of stimuli. B reduces the error for the summed velocity to zero, and does not influence the error for the difference velocity. Is there a problem with this argument?</p><p>Assuming that the stimulus set is symmetric w.r.t. velocity, and, the above argument is correct, then it is hard to understand how linear addition of a signal Z that is generated in a spatially-symmetric fashion, including an autocorrelator, can improve the performance of an estimator if measured by covariance. But perhaps it might help by improving the correlation (see item 2 above) – and this should be clarified. On the other hand, using a symmetric signal Z in a nonlinear way (e.g., dividing by (1+Z<sup>2</sup>)) could improve the performance as measured by covariance or correlation, by reducing outliers. However, it is not clear that this is what the authors are doing.</p><p>In sum – the paper should clarify whether the stimulus set is symmetric with respect to velocity sign. If it is asymmetric, it should justify this asymmetry, and, be clear whether it is responsible for the utility of autocorrelators. If it is symmetric, then further explanation is needed as to why autocorrelators are useful.</p><p>4) Regarding the Discussion section concerning motion energy computations in vertebrates: In considering how the conclusions might apply to the motion energy model, it looks like the paper is suggesting that vertebrates might use different kinds of deviations from strict multiplication, because of the different linear structure of the motion energy model. In this regard, it would be useful to point out that cellular-level analysis in the macaque (Nitzany et al., Evolutionary convergence in computation of local motion signals in monkey and dragonfly. CoSyNe , 2014) shows that the consequences of these deviations is very similar in terms of the motion signal that is extracted – at least in terms of detection of three-point correlations, which are crucial to this paper.</p><p>5) Nesting of models. A diagram that shows the “nesting” relationships of the models – which ones are special cases of others, and that the HRC is a special case of all – would be helpful.</p><p>6) Static nonlinearities at the front end: the paper considers binarization and histogram-equalization. Might it be useful to add a front-end nonlinearity that converts the intensity distribution to a Gaussian? Kurtosis would be higher than either, but this is the distribution that maximizes information (entropy) for a given variance – so it would be interesting to see how it does.</p><p>7) The authors must make their program code for the results available either on the journal web site or on a publicly accessible data base. Please add details to the Method on this last point.</p><p><italic>Reviewer #3:</italic></p><p>1) At the end of the Introduction, the authors say that the perceptual measurements are consistent with only a subset of their models. It would be useful to be clear up front here about which models worked and which ones didn't and in what ways.</p><p>2) A comment about writing style. The paper frequently has expressions like “we hypothesize that biology tunes its motion estimators[…]” (subsection “Strategies for visual motion estimation”, first paragraph). Personally, I find it a bit grating to read this sort of broad (over-)generalization about “biology”. Are there motion detectors of some kind in plants, cyanobacteria and tube-worms at deep-sea vents? Maybe, but the authors are not saying anything that would convince me about how they work. There are many such occurrences of the “biology does X or Y” phrasing, which the authors would be well-advised to remove. (Another example is in the third paragraph, but there are many others.) In any case, the hypothesis of tuning of circuits to natural scenes statistics arising from ordinary behavior is hardly new, so a nod to the venerable history of this idea would be a good thing here.</p><p>3) In reading the third paragraph of the subsecton “Strategies for visual motion estimation” and <xref ref-type="fig" rid="fig1">Figure 1</xref>, I could not understand how the static front-end nonlinearity allowed the circuit compute higher order spatial correlations more easily. Or maybe that is not being implied, but it seemed to be. Later on it becomes more clear that the nonlinearity is supposed to remove kurtosis, but I was confused at this stage in the paper. More generally, it would be helpful to get some conceptual sense of why these particular models are considered – otherwise one gets a sense of a bit of a grab-bag, especially since many of these models could be combined with each other, no doubt giving improvements in each case.</p><p>4) In the subsection entitled “Each mechanisms outperforms the HRC[…]”, the authors show that their various models work better than the HRC. They then say that they can gain insight because their models are theoretical tractable. As an example, they say that their front-end nonlinearity does contrast equalization. But this is the first time they say anything about the nature of the nonlinearity, and they don't say why this kind of normalization helps. So I am afraid that the insight does not come through. Continuing in this vein, they say that “a large fraction of […] performance was afforded by a small number of correlation types”. But there has been no discussion in the text at this point of the possible correlation types and which ones are being used, so the comment remains opaque. Then they write “binarizing nonlinearities also offered certain […] advantages”. Again, this is the first time binarizing nonlinearities are mentioned and it is not clear what the advantages are. It seems clear from the opacity of the text that more details of the model variations should be described earlier, in order to make the paper easier to read. Looking at <xref ref-type="fig" rid="fig2">Figure 2</xref> it is clear that everything outperforms the HRC. But the HRC really does poorly – a correlation coefficient of 0.25 or so. The best mechanism has a correlation coefficient of 0.5. This leads me to worry that none of these proposed mechanisms actually works that well. Also, a main message of this paper is that multi-point correlations are important to motion estimation. But it seems in this Figure that the front-end nonlinearity has the biggest effect. Why didn't the authors combine this nonlinearity with the other mechanisms they consider?</p><p>5) The authors cite their own work for the glider stimuli. Don't these stimuli and associated analysis come from the decades of work by J. Victor and collaborators? I might be mistaken about this – I am most familiar with the spatial stimuli created with gliders that I learned about from those papers. I understand how these stimuli are constructed from those works, but I suspect that the general reader will need a brief introduction at this stage in the paper, even though this is covered in previous works.</p><p>6) I appreciated the clear statement in the first paragraph of subsection “Model responses to glider stimuli” about what the HRC model fails to predict. In the next paragraph (and <xref ref-type="fig" rid="fig3">Figure 3</xref>) the authors discuss how the front-end nonlinearity increases accuracy compared to HRC broadly speaking, but fail to match the responses to negative 2-point correlations and some 3-point gliders. Then they show that models that explicitly compute higher-order correlations correctly predicted the sign of all glider responses, but did not predict the detailed response amplitudes. They try different architectures and find that several architectures show similar performance overall. Of course the output of any of these circuits need not be directly equal to the turning rate of the animal. Am I correct in understanding that the model is that a single gain parameter should relate all of the turning rates under different conditions to the output of the circuit, and that this single gain is fixed by normalizing the positive two-point glider? Also, in <xref ref-type="fig" rid="fig3">Figure 3</xref>, is the “equalized” model the same as the front-end nonlinearity model in <xref ref-type="fig" rid="fig1">Figure 1</xref>? In what sense is it “equalized”?</p><p>7) In the subsection “Improving motion estimation by accounting for natural light-dark asymmetries”, the authors describe results that show that accounting for bright-dark asymmetry improved the results of all of the mechanisms that got the sign of glider responses right. While the text has a technical discussion how various combinations of signals work, I did not understand from the text the conceptual reason why the bright-dark separation helps. At some broad level it is not surprising that adapting to the natural statistics helps with the detection of signals, and indeed it seems here that all the mechanisms are helped by incorporating bright-dark asymmetries. Is there a deeper insight here? Or maybe the point is that the authors are simply making a prediction that the higher order motion detection circuits in flies will be discovered to segregate ON and OFF pathways and then recombine them, independently of which detailed mechanism and nonlinearities are being used? Now in <xref ref-type="fig" rid="fig4">Figure 4A</xref>, most of the quadrant models seem to do worse than HRC, and the (— —) quadrant seems not to be significantly better. And all of these models have a very low correlation with velocity. In view of this, could the authors please clarify why they are saying that accounting for bright-dark asymmetry in their model improves things?</p><p>8) In the subsection “Improving motion estimation by reducing kurtosis” the authors explain that their front end nonlinearities improved motion estimation by reducing the kurtosis in the inputs, even though they did not help with predicting glider responses. Would it help to have these nonlinearities along with the correlation detectors and the ON-OFF segregation discussed in previous sections?</p><p>9) As written, the paper uses computational methods to specifically examine some possible mechanisms of motion estimation in flies in view of the inability of the traditional Reichardt model to explain how animals use higher order motion cues. I agree with the authors that the work has potentially broader significance beyond the specific example discussed here. But in order to engage that broader interest it would be really good if the authors would engage more substantially with the literature on higher order statistics in natural scenes and adaptation of circuit structure and perceptual phenomena to these statistics. At the moment the engagement is cursory. Some suggestions appear below. None of the papers mentioned below is specifically about motion estimation and the suggestion to engage a bit more with all this is not intended to detract from the novelty of the present work; rather it will likely help readers to find the work situated better against this well-known context.</p><p>A) Light-dark asymmetry of natural stimuli and consequences for visual cicrcuits:</p><p>(i)Ratliff, Charles P., et al. “Retina is structured to process an excess of darkness in natural scenes.” Proceedings of the National Academy of Sciences 107.40 (2010): 17368-17373, and references therein on light-dark statistics;</p><p>(ii) Komban, Stanley Jose, Jose-Manuel Alonso, and Qasim Zaidi. “Darks are processed faster than lights.” The Journal of Neuroscience 31.23 (2011): 8654-865, and references therein.</p><p>B) Kurtosis and higher moments of the distribution of light:</p><p>(i) Bonin, Vincent, Valerio Mante, and Matteo Carandini. “The statistical computation underlying contrast gain control.” The Journal of neuroscience 26.23 (2006): 6346-6353, and references therein.</p><p>C) Higher order spatial correlations and perception in the “glider” formulation used here:</p><p>(i) A long history of studies by Victor, and specifically the recent work: Tkačik, Gašper, et al. “Local statistics in natural scenes predict the saliency of synthetic textures.” Proceedings of the National Academy of Sciences 107.42 (2010): 18149-18154; Hermundstad, Ann M., et al. “Variance predicts salience in central sensory processing.” <italic>eLife</italic> 3 (2014): e03722, and references therein;</p><p>(ii) Many papers by the Simoncelli group about visual textures.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.09123.020</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p><italic>Essential revisions</italic>:</p><p><italic>The paper is currently written in such a way that only an exquisitely trained and alert specialist can appreciate these points. It is essential that the paper be reorganized so that there are clear questions set in context of the literature, and the paper and its logic become easier to follow for someone who is not working on motion detection in fly</italic>.</p><p><italic>The current organization of the paper, indeed, is obscure. The paper starts by introducing 4 models that are elaborations of the Reichardt detector: (1) multiple channels that explicitly construct higher order correlation statistics, (2) A static front end nonlinearity, (3) separate correlations between lights and darks, (4) nonlinear combination through operations other than multiplication. This is a lot to take in, and only the most motivated readers will get past this point. Most readers, instead, will stop there. It would be much better to introduce the Reichardt detector (and ideally an energy model), show their failings (e.g. with glider stimuli) and then motivate ways in which they could be improved, and introduce a few of the alternative models (do we need all 4?). Otherwise, introducing the 4 models right away feels like introducing solutions in search of a problem</italic>.</p><p>In response to this suggestion and others below, we have fully restructured our paper. Instead of introducing all models simultaneously at the beginning of the paper, we now first discuss experimental and theoretical reasons that the pure HRC might be disfavored. This includes introducing the puzzling glider psychophysical results. We then show that front-end nonlinearities, while able to improve motion signals with natural inputs, are unable to recapitulate the glider response patterns. We next explore the ON/OFF model, which is probably most proximal to the field’s current conception of the circuit, and show that it improves natural motion estimation and matches experimental glider responses. However, it is important not to get stuck on the first model that fits the glider data reasonably well, because other aspects of <italic>Drosophila</italic>’s circuitry still conflict with the HRC’s hypotheses. We thus continue to explore the space of models with other plausible nonlinear modifications to the HRC: the non-multiplicative nonlinearity model, an unrestricted nonlinearity model, which is a circuit-motivated renaming of the former “explicit multipoint correlator model,” and an extra-input nonlinearity model. This last model is new to this revision, but we believe it is informative because it moves us further beyond the HRC and is motivated by EM reconstructions that show wide-field inputs to the fly’s motion detectors.</p><p>As hinted above, we now focus much more on explaining the biological motivation for each model. This is aided by a new table in the manuscript, which gives experimental justifications for each generalization. One of our goals in this paper is to move away from orthodox models by emphasizing the potential of more complex circuit computations. However, the models we consider are roughly hierarchical, moving to ever more general conceptualizations. This enables us to conclude the paper with our bottom line – the final model incorporates the variety of conceptual advances that were initially illustrated with specific models. This includes, but goes beyond, our point that many of our models improve natural motion estimation through asymmetric treatment of light and dark by the motion estimator. We believe this conceptual hierarchy of models clarifies why it is helpful to consider multiple models, rather than just one: the computational structure of the extra input nonlinearity model is too complicated to take in at once, and the other models in the hierarchy help to dissect it.</p><p><italic>A key problem is that all the models considered seem to work better than the Reichardt model, with no compelling argument for one vs. the other. Possibly this should be regarded as generating a set of alternate hypotheses for physiologists and anatomists to investigate, but it would be useful to understand better what separates these models</italic>.</p><p>Our new structure, described above, provides a stronger rationale for each of the models, introducing them one by one and in a logical progression, from the most conceptually-proximal to the most generalized. <xref ref-type="table" rid="tbl1">Table 1</xref> now gives experimental rationales for examining each model type. We do not intend to present our models as competitors. Rather, each illustrates something new about naturalistic motion estimation. As the reviewers say, each generates a set of hypotheses for experiments. Our new hierarchical organization should make this clearer.</p><p><italic>Because of this, and because the style of writing is opaque, it is currently difficult to discern the “bottom line” in this paper. It reads as a survey of many possible models and some of their virtues and discontents</italic>.</p><p><italic>It is essential for the authors to decide what exactly they want the paper to convey, and set out a clear set of questions in Introduction, answer them in Results, and return to them in Discussion. The current paper, by contrast, seems to change its mind along the way as to what those questions are, and the reader is left grasping for a specific set of questions that need addressing. For instance, the Discussion (second paragraph) emphasizes processing in segregated ON and OFF channels. Is this the main take-home message? If so, it should be clearly set out as a question in the Introduction</italic>.</p><p>As described above, we have restructured the paper to better tease out the bottom line of our paper. More generally, we have highlighted several points that are most critical to the paper: (a) In the motion estimation circuit, nonlinearities are a feature, not a bug, and can be tuned to improve motion estimation in natural scenes. Such tuning also accounts for puzzling psychophysical results. (b) We must explore the space of models to discern critical details from non-critical ones, and make sure our results aren’t the product of idiosyncracies of one particular model. (c) The critical detail of the models that reproduce the behavioral data is that they treat light and dark asymmetrically. (d) The models are conceptually hierarchical, and the most general model recapitulates the insights provided by its predecessors.</p><p><italic>More generally, it seems essential that the authors introduce the problem in a broader context. As written, the paper uses computational methods to specifically examine some possible mechanisms of motion estimation in flies in view of the inability of the traditional Reichardt model to explain how animals use higher order motion cues. These are important issues, but to engage that broader interest it would be good to relate more substantially with the literature on higher order statistics in natural scenes and adaptation of circuit structure and perceptual phenomena to these statistics. Some suggestions appear in Reviewer 3's specific comments, appended below</italic>.</p><p>Thank you for this comment. Our revised manuscript rewrites the Introduction and parts of the Discussion to place our work in the broader contexts suggested by the reviewers.</p><p><italic>The language needs to become clearer. For instance, at the end of the subsection “Model responses to glider stimuli”, you state, “utility of higher-order correlations for naturalistic motion processing is not restricted to a specific neural circuit implementation.” What does this mean</italic>?</p><p>In response to this comment we have tried to clarify and simplify our writing. (In this particular case, we meant that many models were sufficient to make use of HOCs to improve motion estimates. This is now clarified in the text.)</p><p><italic>Also, while it is understandable that the authors may want to protect the reader from too much detail, at the moment the paper seems to put too much away into the appendices for the reader to be able to follow exactly what is going on. In turn, the material in the appendices is written in an extremely mathematical style, too much so for the readers of this journal – for example, “simply” is overused</italic>.</p><p>We have moved some of the material from the appendices to the main text to make the paper easier to follow. All mathematics remains in the appendices. We have gone through all of the appendices to clarify our arguments and simplify our prose. This word “simply” no longer appears. We have added explanatory prose to help ease the reader through the math. More importantly, we think it’s now possible for the reader to understand the paper without ever looking at the math, which is an important improvement.</p><p><italic>In addition to these key issues of organization and style, there are also some conceptual aspects that need work</italic>:</p><p><italic>1) The paper seems to concern only rigid, constant, fronto-parallel motion. While this limitation needs to be explicitly stated, it also increases the strength of the findings. That is, there are many ways in which real motion estimation differ from this simplified scenario: for example, there are objects that may be moving independently of the visual flow, and objects may move in depth. Getting rid of these confounds will further complicate algorithms for extraction of visual flow, and influence what is “optimal”. So it is really quite remarkable that even with these real-world complications neglected, the authors still find that extraction of motion is benefited by mechanisms that make use of high-order nonlinearities</italic>.</p><p>We agree that this is remarkable and interesting, and we have added a Discussion paragraph about this point.</p><p><italic>2) The authors' choice of a way to evaluate performance, i.e. correlation of the inferred motion signal with the veridical motion, is problematic for two related reasons. Firstly, performance can be increased merely by attenuating large-magnitude outliers in the estimate (even simply by passing the output through a sigmoidal nonlinearity, independent of the stimulus). Secondly, the mathematical analysis is complicated because of the need to compute the denominator. There is no single best measure of performance, but for the above reasons, correlation is suboptimal. Two possible alternatives are: (a) to use the covariance (or, equivalently, mean-squared error) more extensively – this gets rid of the denominator problem, and may also simplify the understanding of the role of the autocorrelators; (b) to use mutual information between true and estimated velocity. The advantage of this is that it will not depend on any output transformation. There's a good argument that this is biologically appropriate, since after all, the fly does not need to know the velocity, it just needs to control its behavior – and there may well be nonlinear transformations that intervene between the motion signal output, and the motor command. In sum, it would be useful to know whether maximizing covariance, or maximizing information about velocity, would lead to the same conclusions as maximizing correlation. Note that the authors don't necessarily need to refit the models to maximize mutual information. Rather, all that is needed is to take the existing fitted models and calculate the mutual information from the scatter plot of true velocity vs. calculated velocity – a one dimensional calculation that can be done with binning in, say, 16 velocity bins of approximately equal occupancy</italic>.</p><p>We thank the reviewers for this comment. We emphasized the correlation coefficient because it is intuitive and has a general relationship with the mean- squared error. In particular, we’ve always been choosing parameters that minimize the mean-squared error, and we’re only using the correlation coefficient as a more intuitive metric to present those results. This was not clear in the previous draft. To make it clear in the new draft, we now introduce our optimization procedures in terms of the mean squared error, and we explicitly describe the relationship between the correlation coefficient and the mean squared error in the Methods and in <xref ref-type="app" rid="app2">Appendix 2</xref>. We also use this relationship to explicitly motivate our continued graphical reliance on the correlation coefficient. We hope that the situation is clear now. Simply put, everything in this paper is in terms of the mean squared error, and the correlation coefficient is merely an intuitive way to signify the mean squared error of correctly scaled models.</p><p>We have refrained from using mutual information to assess our motion estimators for both practical and conceptual reasons. Practically speaking, it is more difficult to tune high-dimensional models for mutual information than for the mean-squared error. The referees already allude to this practical problem by not asking us to refit the models. More importantly, we fear that the mutual information could be conceptually misleading in the present context. For example, the referees’ comment about autocorrelators clearly indicates that they would not consider the speed of motion to be a good estimator of its velocity. We agree. Nevertheless, the mutual information between the velocity of motion and its speed is the entropy of the velocity distribution minus one bit, where the “minus one” corresponds to the missing information about the motion’s direction. Thus, the mutual information between the speed and velocity can be huge for broad velocity distributions. For example, it can certainly be greater than 1 bit, in which case the mutual information metric would claim that the speed of motion is a better estimator of the velocity than the direction of motion. We think this is the wrong conclusion. As the referees say, what ultimately matters is how well the animal can control its behavior, and the direction of motion is clearly more useful for orienting behaviors than the speed of motion, independently of how broad the velocity distribution is. Furthermore, we would not consider the ensemble of raw photoreceptor signals to be a good estimator of the velocity of motion, but these signals have at least as much mutual information with the velocity as any motion estimator we can hope to dream up. Overall, we believe that both the mutual information and the mean squared error have their own domains of superiority, and we think that the mean squared error is a more appropriate fit for this paper. Both the mean squared error and the mutual information are standard metrics, and we hope the referees will permit us to emphasize the metric that we strongly prefer.</p><p><italic>3) The benefits of the autocorrelator are not clear, despite the paper's lengthy material on this point</italic>.</p><p>Thank you for pointing out that this section was unclear. Your comments lead us to carefully revisit the utility of the autocorrelator, and these efforts have led us to clearer understanding. As described below, we still argue that autocorrelators have utility for motion estimation. We have fully rewritten the associated Appendix (now <xref ref-type="app" rid="app10">Appendix 10</xref>) to enhance its clarity in light of the referees’ specific concerns. Note that the new appendix discusses the mechanisms underlying the autocorrelator’s utility in a more general setting. The new paper organization leads us to emphasize a different computational example in <xref ref-type="app" rid="app10">Appendix 10</xref>, but we still comment on autocorrelators at the end. Below we will discuss <xref ref-type="app" rid="app10">Appendix 10</xref> as if it had emphasized an autocorrelator example, because we want our response to most directly connect to the referees’ specific comments. We hope the combined concreteness and generality of the Appendix will help to clarify the situation for the referees and future readers.</p><p><italic>To get off the ground: is it the case that the stimulus set is symmetric with respect to velocity – i.e., that each spatial profile is presented as moving both with positive and negative velocity? If this is not the case, then it would seem important to explain why this is justified, and of course, autocorrelators may help. And unless this asymmetry is present in natural stimuli, it would seem to be a flaw in the analysis that needs to be fixed</italic>.</p><p>The previous ensemble was statistically symmetric, in the sense that each spatial profile was equally <italic>likely</italic> to be presented with velocity <italic>v</italic> and –<italic>v</italic>. However, it is conceivable that our finite ensemble of simulated motions was too small to have achieved the desired level of left-right symmetry. To eliminate this possibility, we generated a new ensemble that explicitly enforced left-right symmetry: whenever a pattern with velocity <italic>v</italic> was randomly sampled, we reflected the pattern and presented it with velocity –v. We numerically confirmed that mirror-paired photoreceptor signals were exactly equal between the two simulations. This change didn’t affect the results, which suggests that our original ensemble was large enough to safely consider as left-right symmetric. Nevertheless, we now use the new ensemble with enforced left-right symmetry throughout the paper.</p><p><italic>Indeed, perhaps the stimulus set is actually not symmetric with respect to velocity. This is suggested by the sentence following</italic> <xref ref-type="disp-formula" rid="equ7"><italic>Eq. 7</italic></xref> <italic>of</italic> <xref ref-type="app" rid="app4"><italic>Appendix 4</italic></xref><italic>: “The performance of the combined motion estimator is superior to the original direction selective estimator whenever the latter [i.e., the direction selective estimator] has a nonzero correlation with the autocorrelator.”</italic></p><p>It is mathematically possible for the autocorrelator to be correlated with a direction- selective motion estimator (e.g. the HRC) without being correlated with velocity of motion. Indeed, this is what happens. The Appendix now discusses this initially counter-intuitive point at greater length. Basically, this result occurs because the HRC’s response depends on the pattern that is moving, as does the autocorrelator’s response. Because image-induced variability is partially shared between the HRC and the autocorrelator, the autocorrelator can help to eliminate image-induced noise from the HRC, thereby improving the motion estimate.</p><p><italic>Let's say a particular estimator A that was optimal had the following behavior. For a particular stimulus moving with a positive velocity (say, S</italic><sup><italic>+</italic></sup><italic>, with signed velocity v(S</italic><sup><italic>+</italic></sup><italic>)) gave a result E</italic><sup><italic>+</italic></sup><italic>, and for the same stimulus moving with a negative velocity (say, S</italic><sup><italic>-</italic></sup><italic>, with signed velocity v(S</italic><sup><italic>-</italic></sup><italic>)=-v(S</italic><sup><italic>+</italic></sup><italic>)) gave a result E</italic><sup><italic>-</italic></sup><italic>. We could also construct an estimator A*, which first inverted the stimulus, and then applied A, and then inverted the result – so that A* would give the result of-(E-) for S</italic><sup><italic>+</italic></sup><italic>, and -(E</italic><sup><italic>+</italic></sup><italic>) for S</italic><sup><italic>-</italic></sup><italic>. If A is optimal, then (by symmetry) A* would also have to be optimal. But then it would seem that a new estimator, B=(A+A*)/2, would be better than both, unless A and A* are identical. The reason for this is that B removes any bias (towards either positive or negative velocities) that A or A* might have, and one can always decompose the error (across the entire ensemble) into the error for the summed velocity (v(S</italic><sup><italic>+</italic></sup><italic>)+v(S</italic><sup><italic>-</italic></sup><italic>)) and for the difference velocity (v(S</italic><sup><italic>+</italic></sup><italic>)-v(S</italic><sup><italic>-</italic></sup><italic>)), for each pair of stimuli. B reduces the error for the summed velocity to zero, and does not influence the error for the difference velocity. Is there a problem with this argument?</italic></p><p>We think this argument is elegant and 100% correct.</p><p><italic>Assuming that the stimulus set is symmetric w.r.t. velocity, and, the above argument is correct, then it is hard to understand how linear addition of a signal Z that is generated in a spatially-symmetric fashion, including an autocorrelator, can improve the performance of an estimator if measured by covariance</italic>.</p><p>Your argument helped us to realize that autocorrelators are only useful when they are combined in a mirror anti-symmetric way across space, but this is what they had been doing all along. For example, in <xref ref-type="app" rid="app10">Appendix 10</xref> we discuss the usefulness of the autocorrelator: (f*V_1)(g*V_1) – (f*V_2)(g*V_2). When the referees consider this specific form of autocorrelator, we think they will understand why its utility doesn’t contradict their argument. Nevertheless, its utility might still be opaque, and we hope the material <xref ref-type="app" rid="app10">Appendix 10</xref> will make its utility easier to understand.</p><p><italic>But perhaps it might help by improving the correlation (see item 2 above) – and this should be clarified</italic>.</p><p>As described above, we do not think that our choice of error function is a cause for concern.</p><p><italic>On the other hand, using a symmetric signal Z in a nonlinear way (e.g., dividing by (1+Z</italic><sup><italic>2</italic></sup><italic>)) could improve the performance as measured by covariance or correlation, by reducing outliers. However, it is not clear that this is what the authors are doing.</italic></p><p>Part of the confusion regarding “what the authors are doing” probably relates to the fact that the old version of the Appendix had some discussion of autocorrelators that went beyond trying to explain the specific results that we observed. For example, we described some models that utilized autocorrelation functions in a nonlinear way. We have now deleted that material from the Appendix entirely.</p><p><italic>In sum – the paper should clarify whether the stimulus set is symmetric with respect to velocity sign. If it is asymmetric, it should justify this asymmetry, and, be clear whether it is responsible for the utility of autocorrelators. If it is symmetric, then further explanation is needed as to why autocorrelators are useful</italic>.</p><p>We hope <xref ref-type="app" rid="app10">Appendix 10</xref> has succeeded in this regard.</p><p><italic>4) Regarding the Discussion section concerning motion energy computations in vertebrates: In considering how the conclusions might apply to the motion energy model, it looks like the paper is suggesting that vertebrates might use different kinds of deviations from strict multiplication, because of the different linear structure of the motion energy model. In this regard, it would be useful to point out that cellular-level analysis in the macaque (Nitzany et al., Evolutionary convergence in computation of local motion signals in monkey and dragonfly. CoSyNe , 2014) shows that the consequences of these deviations is very similar in terms of the motion signal that is extracted – at least in terms of detection of three-point correlations, which are crucial to this paper</italic>.</p><p>We thank the reviewers for this citation, which is now referenced appropriately in that Discussion paragraph.</p><p><italic>5) Nesting of models. A diagram that shows the “nesting” relationships of the models – which ones are special cases of others, and that the HRC is a special case of all – would be helpful</italic>.</p><p>This was a fun diagram to think about, and we agree it is helpful. We have included it in <xref ref-type="fig" rid="fig5">Figure 5</xref>, where we discuss the most general model, and its relationship to the others. In fact, this conception of the hierarchy of models was key to reorganizing the paper.</p><p><italic>6) Static nonlinearities at the front end: the paper considers binarization and histogram-equalization. Might it be useful to add a front-end nonlinearity that converts the intensity distribution to a Gaussian? Kurtosis would be higher than either, but this is the distribution that maximizes information (entropy) for a given variance – so it would be interesting to see how it does</italic>.</p><p>We simulated this Gaussianizing nonlinearity and found that it performed less well in natural scenes than the binarizing or equalizing nonlinearities. Like the other front-end nonlinearities, it didn’t generate glider responses that matched the data. This information is fully included in the results section about front-end nonlinearities and in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p><p><italic>7) The authors must make their program code for the results available either on the journal web site or on a publicly accessible data base. Please add details to the methods on this last point</italic>.</p><p>We agree to provide code that will be posted on the journal website. However, we want to first ensure that the referees and editors are satisfied by our new paper organization. The process of assembling the code is time-consuming, so we want to do it only once in a way that reflects the final published paper’s organization.</p><p><italic>Reviewer #3</italic>:</p><p>We thank Reviewer 3 for these specific comments. We have addressed them all in our revision. In particular, we have included an Introduction paragraph that puts our work in the broader context of natural scene statistics and visual processing.</p><p><italic>1) At the end of the Introduction, the authors say that the perceptual measurements are consistent with only a subset of their models. It would be useful to be clear up front here about which models worked and which ones didn't and in what ways</italic>.</p><p>We think this should be clear in the revised manuscript, because we now go through the models more sequentially.</p><p><italic>2) A comment about writing style. The paper frequently has expressions like “we hypothesize that biology tunes its motion estimators[…]” (subsection “Strategies for visual motion estimation”, first paragraph). Personally, I find it a bit grating to read this sort of broad (over-)generalization about “biology”. Are there motion detectors of some kind in plants, cyanobacteria and tube-worms at deep-sea vents? Maybe, but the authors are not saying anything that would convince me about how they work. There are many such occurrences of the “biology does X or Y” phrasing, which the authors would be well-advised to remove. (Another example is in the third paragraph, but there are many others.) In any case, the hypothesis of tuning of circuits to natural scenes statistics arising from ordinary behavior is hardly new, so a nod to the venerable history of this idea would be a good thing here</italic>.</p><p>We agree with this comment and have removed references to what biology does. In a few places, we have emphasized that evolution might tune specific parameters, and we have added appropriate citations for this idea.</p><p><italic>3) In reading the third paragraph of the subsecton “Strategies for visual motion estimation” and</italic> <xref ref-type="fig" rid="fig1"><italic>Figure 1</italic></xref><italic>, I could not understand how the static front-end nonlinearity allowed the circuit compute higher order spatial correlations more easily. Or maybe that is not being implied, but it seemed to be. Later on it becomes more clear that the nonlinearity is supposed to remove kurtosis, but I was confused at this stage in the paper. More generally, it would be helpful to get some conceptual sense of why these particular models are considered – otherwise one gets a sense of a bit of a grab-bag, especially since many of these models could be combined with each other, no doubt giving improvements in each case.</italic></p><p>In the new structure of the paper, we believe it is clearer why we chose these particular models. We have added <xref ref-type="table" rid="tbl1">Table 1</xref>, which gives clear experimental rationales (and citations) for each of the models and generalized models we consider. We have also changed our Introduction to give more background for why these models are chosen.</p><p><italic>4) In the subsection entitled “Each mechanisms outperforms the HRC[…]”, the authors show that their various models work better than the HRC. They then say that they can gain insight because their models are theoretical tractable. As an example, they say that their front-end nonlinearity does contrast equalization. But this is the first time they say anything about the nature of the nonlinearity, and they don't say why this kind of normalization helps. So I am afraid that the insight does not come through. Continuing in this vein, they say that “a large fraction of […] performance was afforded by a small number of correlation types”. But there has been no discussion in the text at this point of the possible correlation types and which ones are being used, so the comment remains opaque. Then they write “binarizing nonlinearities also offered certain […] advantages”. Again, this is the first time binarizing nonlinearities are mentioned and it is not clear what the advantages are. It seems clear from the opacity of the text that more details of the model variations should be described earlier, in order to make the paper easier to read. Looking at</italic> <xref ref-type="fig" rid="fig2"><italic>Figure 2</italic></xref> <italic>it is clear that everything outperforms the HRC. But the HRC really does poorly – a correlation coefficient of 0.25 or so. The best mechanism has a correlation coefficient of 0.5. This leads me to worry that none of these proposed mechanisms actually works that well. Also, a main message of this paper is that multi-point correlations are important to motion estimation. But it seems in this Figure that the front-end nonlinearity has the biggest effect. Why didn't the authors combine this nonlinearity with the other mechanisms they consider?</italic></p><p>To address this comment, we have first clarified our prose surrounding the introduction of the front-end nonlinearities, and we describe more details earlier in the new structure of the manuscript.</p><p>The second point about the low correlation coefficient of the HRC is worth exploring. Our models perform no averaging in time or space, so their performance is worse than one could do if one pooled signals over space. The HRC performance we measure is reminiscent of the noisy performance reported in <xref ref-type="bibr" rid="bib15">Dror et al. (2001)</xref>. The fact that the HRC doesn’t work very well for natural inputs is part of the reason that we’re looking for alternatives.</p><p>It is true that the front-end nonlinearity had the largest effect of any nonlinearity in improving natural motion estimates. Yet it poorly predicted the psychophysics. We agree that stringing models together may improve motion estimates, but our goal here was to first understand how the different mechanisms work in isolation. We believe that combining these nonlinearities is fodder for future work, but it is not appropriate in this paper, in part because of the combinatorial explosion expected when we start combining models, and in part because combining models makes it far more difficult to understand how they function to improve estimates. That said, we do now emphasize the hierarchical nature of our models, and the final figure of the paper should help future reader think about how these models might be combined.</p><p>It’s also interesting to note that the front-end nonlinearities considered here eliminate the light-dark asymmetry from their natural inputs. Thus, such non- linearly preprocessed signals cannot combine effectively with some of the other mechanisms that we describe, such as ON/OFF processing. We now point this out in the text. Our overall view is that tuning frontend nonlinearities for motion estimation is a good way to improve estimation performance, but it doesn’t provide a good way of understanding <italic>Drosophila</italic>’s computation and circuitry.</p><p><italic>5) The authors cite their own work for the glider stimuli. Don't these stimuli and associated analysis come from the decades of work by J. Victor and collaborators? I might be mistaken about this – I am most familiar with the spatial stimuli created with gliders that I learned about from those papers. I understand how these stimuli are constructed from those works, but I suspect that the general reader will need a brief introduction at this stage in the paper, even though this is covered in previous works</italic>.</p><p>We have introduced the gliders more prominently in the new structure of the manuscript (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We continue to cite Hu and Victor prominently as the originator of these spatiotemporal stimuli. We also now cite some of Victor’s references containing spatial gliders in the Introduction.</p><p><italic>6) I appreciated the clear statement in the first paragraph of subsection “Model responses to glider stimuli” about what the HRC model fails to predict. In the next paragraph (and</italic> <xref ref-type="fig" rid="fig3"><italic>Figure 3</italic></xref><italic>) the authors discuss how the front-end nonlinearity increases accuracy compared to HRC broadly speaking, but fail to match the responses to negative 2-point correlations and some 3-point gliders. Then they show that models that explicitly compute higher-order correlations correctly predicted the sign of all glider responses, but did not predict the detailed response amplitudes. They try different architectures and find that several architectures show similar performance overall. Of course the output of any of these circuits need not be directly equal to the turning rate of the animal. Am I correct in understanding that the model is that a single gain parameter should relate all of the turning rates under different conditions to the output of the circuit, and that this single gain is fixed by normalizing the positive two-point glider? Also, in</italic> <xref ref-type="fig" rid="fig3"><italic>Figure 3</italic></xref><italic>, is the “equalized” model the same as the front-end nonlinearity model in</italic> <xref ref-type="fig" rid="fig1"><italic>Figure 1</italic></xref><italic>? In what sense is it “equalized”?</italic></p><p>In response to the first question, the answer is yes: we are assuming that the behavior is proportional to the model output. This is in line with typical models of the optomotor response, which were first modeled as proportional to the HRC output.</p><p>In response to the second question, the equalized model is a specific front-end nonlinearity that transforms the natural input distribution into a uniform distribution. This is in contrast to the binarizing nonlinearity, which turns the natural input distribution into a binary distribution. We have clarified this point when we introduce the front-end nonlinearity models. Furthermore, the new paper architecture avoids the specific issue that the referee is referring to.</p><p><italic>7) In the subsection “Improving motion estimation by accounting for natural light-dark asymmetries”, the authors describe results that show that accounting for bright-dark asymmetry improved the results of all of the mechanisms that got the sign of glider responses right. While the text has a technical discussion how various combinations of signals work, I did not understand from the text the conceptual reason why the bright-dark separation helps. At some broad level it is not surprising that adapting to the natural statistics helps with the detection of signals, and indeed it seems here that all the mechanisms are helped by incorporating bright-dark asymmetries. Is there a deeper insight here? Or maybe the point is that the authors are simply making a prediction that the higher order motion detection circuits in flies will be discovered to segregate ON and OFF pathways and then recombine them, independently of which detailed mechanism and nonlinearities are being used? Now in</italic> <xref ref-type="fig" rid="fig4"><italic>Figure 4A</italic></xref><italic>, most of the quadrant models seem to do worse than HRC, and the (</italic>— —<italic>) quadrant seems not to be significantly better. And all of these models have a very low correlation with velocity. In view of this, could the authors please clarify why they are saying that accounting for bright-dark asymmetry in their model improves things?</italic></p><p>We apologize that this section was unclear. In the new manuscript, the presentation of this material is very different from the previous draft. For example, the results that were compressed into <xref ref-type="fig" rid="fig4">Figure 4</xref> are now more distributed throughout the narrative, which affords us the opportunity to discuss how each model incorporates light and dark information differently than prior models. We hope that our presentation is substantially clearer this time around.</p><p>When we say that accounting for light-dark asymmetry is helpful in the weighted 4- quadrant model, we meant to emphasize that the four quadrants can be combined in a better manner than the HRC predicts. We did not mean to say that the benefits of ON/OFF processing could be achieved by the isolated quadrants.</p><p>In terms of whether there is a deeper insight, we now include some discussion of the many different ways that the models utilized light-dark asymmetries. For example, we discuss how simple ON/OFF misses certain useful cues that are captured by other models. We also try to emphasize a broader set of conceptual points that go beyond the asymmetry between light and dark.</p><p><italic>8) In the subsection “Improving motion estimation by reducing kurtosis\ the authors explain that their front end nonlinearities improved motion estimation by reducing the kurtosis in the inputs, even though they did not help with predicting glider responses. Would it help to have these nonlinearities along with the correlation detectors and the ON-OFF segregation discussed in previous sections</italic>?</p><p>As discussed previously (see response to comment 4), the frontend nonlinearities that we use will eliminate the benefit of subsequent ON/OFF processing. It is less obvious how effectively frontend nonlinearities could be combined with the non- multiplicative nonlinearity model. However, the situation will certainly be much more complicated, because the signal and noise of the non-multiplicative nonlinearity model depend on more than the second and fourth-order statistics of the image ensemble. We point out in our discussion that several of these models could be combined in interesting ways, though combinations would change the optimal weightings. We consider pursuing this to be beyond the scope of the current work.</p><p><italic>9) As written, the paper uses computational methods to specifically examine some possible mechanisms of motion estimation in flies in view of the inability of the traditional Reichardt model to explain how animals use higher order motion cues. I agree with the authors that the work has potentially broader significance beyond the specific example discussed here. But in order to engage that broader interest it would be really good if the authors would engage more substantially with the literature on higher order statistics in natural scenes and adaptation of circuit structure and perceptual phenomena to these statistics. At the moment the engagement is cursory. Some suggestions appear below. None of the papers mentioned below is specifically about motion estimation and the suggestion to engage a bit more with all this is not intended to detract from the novelty of the present work; rather it will likely help readers to find the work situated better against this well-known context.</italic></p><p>We thank the reviewer for the suggestion to add broader context. We have modified the Introduction and Discussion sections substantially to situate the reader within the broader context of natural scene regularities and their influence on visual processing. Through these changes, we have included many of the references suggested.</p></body></sub-article></article>