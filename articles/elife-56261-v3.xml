<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56261</article-id><article-id pub-id-type="doi">10.7554/eLife.56261</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Training deep neural density estimators to identify mechanistic models of neural dynamics</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-177423"><name><surname>Gonçalves</surname><given-names>Pedro J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6987-4836</contrib-id><email>pedro.goncalves@caesar.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-177424"><name><surname>Lueckmann</surname><given-names>Jan-Matthis</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4320-4663</contrib-id><email>jan-matthis.lueckmann@tum.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-177425"><name><surname>Deistler</surname><given-names>Michael</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3573-0404</contrib-id><email>michael.deistler@tum.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177426"><name><surname>Nonnenmacher</surname><given-names>Marcel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177427"><name><surname>Öcal</surname><given-names>Kaan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-8528-6858</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177428"><name><surname>Bassetto</surname><given-names>Giacomo</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-150473"><name><surname>Chintaluri</surname><given-names>Chaitanya</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-4252-1608</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund8"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177429"><name><surname>Podlaski</surname><given-names>William F</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0001-6619-7502</contrib-id><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund10"/><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177431"><name><surname>Haddad</surname><given-names>Sara A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0807-0823</contrib-id><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-148436"><name><surname>Vogels</surname><given-names>Tim P</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund6"/><xref ref-type="other" rid="fund10"/><xref ref-type="other" rid="fund7"/><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-177674"><name><surname>Greenberg</surname><given-names>David S</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-176198"><name><surname>Macke</surname><given-names>Jakob H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5154-8912</contrib-id><email>Jakob.Macke@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff9">9</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund9"/><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Computational Neuroengineering, Department of Electrical and Computer Engineering, Technical University of Munich</institution><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>Max Planck Research Group Neural Systems Analysis, Center of Advanced European Studies and Research (caesar)</institution><addr-line><named-content content-type="city">Bonn</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution>Machine Learning in Science, Excellence Cluster Machine Learning, Tübingen University</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution>Model-Driven Machine Learning, Institute of Coastal Research, Helmholtz Centre Geesthacht</institution><addr-line><named-content content-type="city">Geesthacht</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution>Mathematical Institute, University of Bonn</institution><addr-line><named-content content-type="city">Bonn</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution>Centre for Neural Circuits and Behaviour, University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff7"><label>7</label><institution>Institute of Science and Technology Austria</institution><addr-line><named-content content-type="city">Klosterneuburg</named-content></addr-line><country>Austria</country></aff><aff id="aff8"><label>8</label><institution>Max Planck Institute for Brain Research</institution><addr-line><named-content content-type="city">Frankfurt</named-content></addr-line><country>Germany</country></aff><aff id="aff9"><label>9</label><institution>Max Planck Institute for Intelligent Systems</institution><addr-line><named-content content-type="city">Tübingen</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Huguenard</surname><given-names>John R</given-names></name><role>Senior Editor</role><aff><institution>Stanford University School of Medicine</institution><country>United States</country></aff></contrib><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>09</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e56261</elocation-id><history><date date-type="received" iso-8601-date="2020-02-21"><day>21</day><month>02</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-09-16"><day>16</day><month>09</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Gonçalves et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Gonçalves et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56261-v3.pdf"/><abstract><p>Mechanistic modeling in neuroscience aims to explain observed phenomena in terms of underlying causes. However, determining which model parameters agree with complex and stochastic neural data presents a significant challenge. We address this challenge with a machine learning tool which uses deep neural density estimators—trained using model simulations—to carry out Bayesian inference and retrieve the full space of parameters compatible with raw data or selected data features. Our method is scalable in parameters and data features and can rapidly analyze new data after initial training. We demonstrate the power and flexibility of our approach on receptive fields, ion channels, and Hodgkin–Huxley models. We also characterize the space of circuit configurations giving rise to rhythmic activity in the crustacean stomatogastric ganglion, and use these results to derive hypotheses for underlying compensation mechanisms. Our approach will help close the gap between data-driven and theory-driven models of neural dynamics.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Computational neuroscientists use mathematical models built on observational data to investigate what’s happening in the brain. Models can simulate brain activity from the behavior of a single neuron right through to the patterns of collective activity in whole neural networks. Collecting the experimental data is the first step, then the challenge becomes deciding which computer models best represent the data and can explain the underlying causes of how the brain behaves.</p><p>Researchers usually find the right model for their data through trial and error. This involves tweaking a model’s parameters until the model can reproduce the data of interest. But this process is laborious and not systematic. Moreover, with the ever-increasing complexity of both data and computer models in neuroscience, the old-school approach of building models is starting to show its limitations.</p><p>Now, Gonçalves, Lueckmann, Deistler et al. have designed an algorithm that makes it easier for researchers to fit mathematical models to experimental data. First, the algorithm trains an artificial neural network to predict which models are compatible with simulated data. After initial training, the method can rapidly be applied to either raw experimental data or selected data features. The algorithm then returns the models that generate the best match.</p><p>This newly developed machine learning tool was able to automatically identify models which can replicate the observed data from a diverse set of neuroscience problems. Importantly, further experiments showed that this new approach can be scaled up to complex mechanisms, such as how a neural network in crabs maintains its rhythm of activity. This tool could be applied to a wide range of computational investigations in neuroscience and other fields of biology, which may help bridge the gap between ‘data-driven’ and ‘theory-driven’ approaches.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>bayesian inference</kwd><kwd>deep learning</kwd><kwd>stomatogastric ganglion</kwd><kwd>model identification</kwd><kwd>neural dynamics</kwd><kwd>mechanistic models</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd><kwd>Rat</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB 1233</award-id><principal-award-recipient><name><surname>Lueckmann</surname><given-names>Jan-Matthis</given-names></name><name><surname>Nonnenmacher</surname><given-names>Marcel</given-names></name><name><surname>Bassetto</surname><given-names>Giacomo</given-names></name><name><surname>Macke</surname><given-names>Jakob H</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SFB 1089</award-id><principal-award-recipient><name><surname>Gonçalves</surname><given-names>Pedro J</given-names></name><name><surname>Macke</surname><given-names>Jakob H</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>SPP 2041</award-id><principal-award-recipient><name><surname>Macke</surname><given-names>Jakob H</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>01IS18052 A-D</award-id><principal-award-recipient><name><surname>Deistler</surname><given-names>Michael</given-names></name><name><surname>Nonnenmacher</surname><given-names>Marcel</given-names></name><name><surname>Macke</surname><given-names>Jakob H</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010663</institution-id><institution>H2020 European Research Council</institution></institution-wrap></funding-source><award-id>SYNAPSEEK</award-id><principal-award-recipient><name><surname>Chintaluri</surname><given-names>Chaitanya</given-names></name><name><surname>Podlaski</surname><given-names>William F</given-names></name><name><surname>Vogels</surname><given-names>Tim P</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution>Wellcome Trust Senior Research Fellowship</institution></institution-wrap></funding-source><award-id>214316/Z/18/Z</award-id><principal-award-recipient><name><surname>Vogels</surname><given-names>Tim P</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100014013</institution-id><institution>UK Research and Innovation</institution></institution-wrap></funding-source><award-id>UKRI-BBSRC BB/N019512/1</award-id><principal-award-recipient><name><surname>Chintaluri</surname><given-names>Chaitanya</given-names></name></principal-award-recipient></award-group><award-group id="fund9"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Germany's Excellence Strategy - EXC-Number 2064/1 610 - Project number 390727645</award-id><principal-award-recipient><name><surname>Macke</surname><given-names>Jakob H</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship WT100000</award-id><principal-award-recipient><name><surname>Podlaski</surname><given-names>William F</given-names></name><name><surname>Vogels</surname><given-names>Tim P</given-names></name></principal-award-recipient></award-group><award-group id="fund10"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship WT100000</award-id><principal-award-recipient><name><surname>Podlaski</surname><given-names>William F</given-names></name><name><surname>Vogels</surname><given-names>Tim P</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Deep neural networks can be trained to automatically find mechanistic models which quantitatively agree with experimental data, providing new opportunities for building and visualizing interpretable models of neural dynamics.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>New experimental technologies allow us to observe neurons, networks, brain regions, and entire systems at unprecedented scale and resolution, but using these data to understand how behavior arises from neural processes remains a challenge. To test our understanding of a phenomenon, we often take to rebuilding it in the form of a computational model that incorporates the mechanisms we believe to be at play, based on scientific knowledge, intuition, and hypotheses about the components of a system and the laws governing their relationships. The goal of such mechanistic models is to investigate whether a proposed mechanism can explain experimental data, uncover details that may have been missed, inspire new experiments, and eventually provide insights into the inner workings of an observed neural or behavioral phenomenon (<xref ref-type="bibr" rid="bib53">Herz et al., 2006</xref>; <xref ref-type="bibr" rid="bib38">Gerstner et al., 2012</xref>; <xref ref-type="bibr" rid="bib89">O'Leary et al., 2015</xref>; <xref ref-type="bibr" rid="bib5">Baker et al., 2018</xref>). Examples for such a symbiotic relationship between model and experiments range from the now classical work of <xref ref-type="bibr" rid="bib54">Hodgkin and Huxley, 1952</xref>, to population models investigating rules of connectivity, plasticity and network dynamics (<xref ref-type="bibr" rid="bib137">van Vreeswijk and Sompolinsky, 1996</xref>; <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib138">Vogels et al., 2005</xref>; <xref ref-type="bibr" rid="bib106">Potjans and Diesmann, 2014</xref>; <xref ref-type="bibr" rid="bib69">Litwin-Kumar and Doiron, 2012</xref>), network models of inter-area interactions (<xref ref-type="bibr" rid="bib124">Sporns, 2014</xref>; <xref ref-type="bibr" rid="bib7">Bassett et al., 2018</xref>), and models of decision making (<xref ref-type="bibr" rid="bib40">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib139">Wang, 2008</xref>).</p><p>A crucial step in building a model is adjusting its free parameters to be consistent with experimental observations. This is essential both for investigating whether the model agrees with reality and for gaining insight into processes which cannot be measured experimentally. For some models in neuroscience, it is possible to identify the relevant parameter regimes from careful mathematical analysis of the model equations. But as the complexity of both neural data and neural models increases, it becomes very difficult to find well-fitting parameters by inspection, and <italic>automated</italic> identification of data-consistent parameters is required.</p><p>Furthermore, to understand how a model quantitatively explains data, it is necessary to find not only the <italic>best</italic>, but <italic>all</italic> parameter settings consistent with experimental observations. This is especially important when modeling neural data, where highly variable observations can lead to broad ranges of data-consistent parameters. Moreover, many models in biology are inherently robust to some perturbations of parameters, but highly sensitive to others (<xref ref-type="bibr" rid="bib46">Gutenkunst et al., 2007</xref>; <xref ref-type="bibr" rid="bib89">O'Leary et al., 2015</xref>), for example because of processes such as homeostastic regulation. For these systems, identifying the full range of data-consistent parameters can reveal how multiple distinct parameter settings give rise to the same model behavior (<xref ref-type="bibr" rid="bib37">Foster et al., 1993</xref>; <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib2">Achard and De Schutter, 2006</xref>; <xref ref-type="bibr" rid="bib4">Alonso and Marder, 2019</xref>). Yet, despite the clear benefits of mechanistic models in providing scientific insight, identifying their parameters given data remains a challenging open problem that demands new algorithmic strategies.</p><p>The gold standard for automated parameter identification is <italic>statistical inference</italic>, which uses the likelihood <inline-formula><mml:math id="inf1"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to quantify the match between parameters <inline-formula><mml:math id="inf2"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> and data <inline-formula><mml:math id="inf3"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula>. Likelihoods can be efficiently computed for purely statistical models commonly used in neuroscience (<xref ref-type="bibr" rid="bib135">Truccolo et al., 2005</xref>; <xref ref-type="bibr" rid="bib119">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib101">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib143">Yu et al., 2009</xref>; <xref ref-type="bibr" rid="bib74">Macke et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Cunningham and Yu, 2014</xref>; <xref ref-type="bibr" rid="bib93">Pandarinath et al., 2018</xref>), but are computationally intractable for most mechanistic models. Mechanistic models are designed to reflect knowledge about biological mechanisms, and not necessarily to be amenable to efficient inference: many mechanistic models are defined implicitly through stochastic computer simulations (e.g. a simulation of a network of spiking neurons), and likelihood calculation would require the ability to integrate over all potential paths through the simulator code. Similarly, a common goal of mechanistic modeling is to capture selected summary features of the data (e.g. a certain firing rate, bursting behavior, etc…), <italic>not</italic> the full dataset in all its details. The same feature (such as a particular average firing rate) can be produced by infinitely many realizations of the simulated process (such as a time-series of membrane potential). This makes it impractical to compute likelihoods, as one would have to average over all possible realizations which produce the same output.</p><p>Since the toolkit of (likelihood-based) statistical inference is inaccessible for mechanistic models, parameters are typically tuned ad-hoc (often through laborious, and subjective, trial-and-error), or by computationally expensive parameter search: a large set of models is generated, and grid search (<xref ref-type="bibr" rid="bib108">Prinz et al., 2003</xref>; <xref ref-type="bibr" rid="bib134">Tomm et al., 2011</xref>; <xref ref-type="bibr" rid="bib125">Stringer et al., 2016</xref>) or a genetic algorithm (<xref ref-type="bibr" rid="bib31">Druckmann et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Hay et al., 2011</xref>; <xref ref-type="bibr" rid="bib116">Rossant et al., 2011</xref>; <xref ref-type="bibr" rid="bib136">Van Geit et al., 2016</xref>) is used to filter out simulations which do not match the data. However, these approaches require the user to define a heuristic rejection criterion on which simulations to keep (which can be challenging when observations have many dimensions or multiple units of measurement), and typically end up discarding most simulations. Furthermore, they lack the advantages of statistical inference, which provides principled approaches for handling variability, quantifying uncertainty, incorporating prior knowledge and integrating multiple data sources. Approximate Bayesian Computation (ABC) (<xref ref-type="bibr" rid="bib8">Beaumont et al., 2002</xref>; <xref ref-type="bibr" rid="bib83">Marjoram et al., 2003</xref>; <xref ref-type="bibr" rid="bib122">Sisson et al., 2007</xref>) is a parameter-search technique which aims to perform statistical inference, but still requires definition of a rejection criterion and struggles in high-dimensional problems. Thus, computational neuroscientists face a dilemma: either create carefully designed, highly interpretable mechanistic models (but rely on ad-hoc parameter tuning), or resort to purely statistical models offering sophisticated parameter inference but limited mechanistic insight.</p><p>Here, we propose a new approach using machine learning to combine the advantages of mechanistic and statistical modeling. We present SNPE (Sequential Neural Posterior Estimation), a tool that makes it possible to perform Bayesian inference on mechanistic models in neuroscience without requiring access to likelihoods. SNPE identifies all mechanistic model parameters consistent with observed experimental data (or summary features). It builds on recent advances in simulation-based Bayesian inference (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Cranmer et al., 2020</xref>): given observed experimental data (or summary features) <inline-formula><mml:math id="inf4"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>, and a mechanistic model with parameters <inline-formula><mml:math id="inf5"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula>, it expresses both prior knowledge and the range of data-compatible parameters through probability distributions. SNPE returns a posterior distribution <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which is high for parameters <inline-formula><mml:math id="inf7"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> consistent with both the data <inline-formula><mml:math id="inf8"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> and prior knowledge, but approaches zero for <inline-formula><mml:math id="inf9"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> inconsistent with either (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Goal: algorithmically identify mechanistic models which are consistent with data.</title><p>Our algorithm (SNPE) takes three inputs: a candidate mechanistic model, prior knowledge or constraints on model parameters, and data (or summary statistics). SNPE proceeds by (1) sampling parameters from the prior and simulating synthetic datasets from these parameters, and (2) using a deep density estimation neural network to learn the (probabilistic) association between data (or data features) and underlying parameters, that is to learn statistical inference from simulated data. (3) This density estimation network is then applied to empirical data to derive the full space of parameters consistent with the data and the prior, that is, the posterior distribution. High posterior probability is assigned to parameters which are consistent with both the data and the prior, low probability to inconsistent parameters. (4) If needed, an initial estimate of the posterior can be used to adaptively guide further simulations to produce data-consistent results.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-fig1-v3.tif"/></fig><p>Similar to parameter search methods, SNPE uses simulations instead of likelihood calculations, but instead of filtering out simulations, it uses <italic>all</italic> simulations to train a multilayer artificial neural network to identify admissible parameters (<xref ref-type="fig" rid="fig1">Figure 1</xref>). By incorporating modern deep neural networks for conditional density estimation (<xref ref-type="bibr" rid="bib114">Rezende and Mohamed, 2015</xref>; <xref ref-type="bibr" rid="bib95">Papamakarios et al., 2017</xref>), it can capture the full <italic>distribution</italic> of parameters consistent with the data, even when this distribution has multiple peaks or lies on curved manifolds. Critically, SNPE decouples the design of the model and design of the inference approach, giving the investigator maximal flexibility to design and modify mechanistic models. Our method makes minimal assumptions about the model or its implementation, and can for example also be applied to non-differentiable models, such as networks of spiking neurons. Its only requirement is that one can run model simulations for different parameters, and collect the resulting synthetic data or summary features of interest.</p><p>While the theoretical foundations of SNPE were originally developed and tested using simple inference problems on small models (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>), here we show that SNPE can scale to complex mechanistic models in neuroscience, provide an accessible and powerful implementation, and develop validation and visualization techniques for exploring the derived posteriors. We illustrate SNPE using mechanistic models expressing key neuroscientific concepts: beginning with a simple neural encoding problem with a known solution, we progress to more complex data types, large datasets and many-parameter models inaccessible to previous methods. We estimate visual receptive fields using many data features, demonstrate rapid inference of ion channel properties from high-throughput voltage-clamp protocols, and show how Hodgkin–Huxley models are more tightly constrained by increasing numbers of data features. Finally, we showcase the power of SNPE by using it to identify the parameters of a network model which can explain an experimentally observed pyloric rhythm in the stomatogastric ganglion (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>)–in contrast to previous approaches, SNPE allows us to search over the full space of both single-neuron and synaptic parameters, allowing us to study the geometry of the parameter space, as well as to provide new hypotheses for which compensation mechanisms might be at play.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Training neural networks to perform Bayesian inference without likelihood evaluations</title><p>SNPE performs Bayesian inference on mechanistic models using only model-simulations, without requiring likelihood evaluations. It requires three inputs: a model (i.e. computer code to simulate data from parameters), prior knowledge or constraints on parameters, and data (outputs from the model or the real system it describes, <xref ref-type="fig" rid="fig1">Figure 1</xref>). SNPE runs simulations for a range of parameter values, and trains an artificial neural network to map any simulation result onto a range of possible parameters. Importantly, a network trained to maximize log-probability (of parameters given simulation results) will learn to approximate the posterior distribution as given by Bayes rule (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>) (see Materials and methods for details, <xref ref-type="fig" rid="fig1">Figure 1</xref>). After training on <italic>simulated</italic> data with known model parameters, SNPE can perform Bayesian inference of unknown parameters for <italic>empirical</italic> data. This approach to Bayesian inference never requires evaluating likelihoods. SNPE’s efficiency can be further improved by using the running estimate of the posterior distribution to guide further simulations toward data-compatible regions of the parameter space (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>). Below, we apply SNPE to a range of stochastic models in neuroscience.</p></sec><sec id="s2-2"><title>Estimating stimulus-selectivity in linear-nonlinear encoding models</title><p>We first illustrate SNPE on linear-nonlinear (LN) encoding models, a special case of generalized linear models (GLMs). These are simple, commonly used phenomenological models for which likelihood-based parameter estimation is feasible (<xref ref-type="bibr" rid="bib18">Brown et al., 1998</xref>; <xref ref-type="bibr" rid="bib94">Paninski, 2004</xref>; <xref ref-type="bibr" rid="bib100">Pillow, 2007</xref>; <xref ref-type="bibr" rid="bib39">Gerwinn et al., 2010</xref>; <xref ref-type="bibr" rid="bib104">Polson et al., 2013</xref>; <xref ref-type="bibr" rid="bib102">Pillow and Scott, 2012</xref>), and which can be used to validate the accuracy of our approach, before applying SNPE to more complex models for which the likelihood is unavailable. We will show that SNPE returns the correct posterior distribution over parameters, that it can cope with high-dimensional observation data, that it can recover multiple solutions to parameter inference problems, and that it is substantially more simulation efficient than conventional rejection-based ABC methods.</p><p>An LN model describes how a neuron’s firing rate is modulated by a sensory stimulus through a linear filter <inline-formula><mml:math id="inf10"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula>, often referred to as the <italic>receptive field</italic> (<xref ref-type="bibr" rid="bib99">Pillow et al., 2005</xref>; <xref ref-type="bibr" rid="bib23">Chichilnisky, 2001</xref>). We first considered a model of a retinal ganglion cell (RGC) driven by full-field flicker (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). A statistic that is often used to characterize such a neuron is the <italic>spike-triggered average</italic> (STA) (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, right). We therefore used the STA, as well as the firing rate of the neuron, as input <inline-formula><mml:math id="inf11"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> to SNPE. (Note that, in the limit of infinite data, and for white noise stimuli, the STA will converge to the receptive field [<xref ref-type="bibr" rid="bib94">Paninski, 2004</xref>]–for finite, and non-white data, the two will in general be different.) Starting with random receptive fields <inline-formula><mml:math id="inf12"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula>, we generated synthetic spike trains and calculated STAs from them (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). We then trained a neural conditional density estimator to recover the receptive fields from the STAs and firing rates (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). This allowed us to estimate the posterior distribution over receptive fields, that is to estimate which receptive fields are consistent with the data (and prior) (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). For LN models, likelihood-based inference is possible, allowing us to validate the SNPE posterior by comparing it to a reference posterior obtained via Markov Chain Monte Carlo (MCMC) sampling (<xref ref-type="bibr" rid="bib104">Polson et al., 2013</xref>; <xref ref-type="bibr" rid="bib102">Pillow and Scott, 2012</xref>). We found that SNPE accurately estimates the posterior distribution (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref> and <xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>), and substantially outperforms Sequential Monte Carlo (SMC) ABC methods (<xref ref-type="bibr" rid="bib122">Sisson et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Beaumont et al., 2009</xref>; <xref ref-type="fig" rid="fig2">Figure 2d</xref>). If SNPE works correctly, its posterior mean filter will match that of the reference posterior – however, it is not to be expected that either of them precisely matches the ground-truth filter (<xref ref-type="fig" rid="fig2">Figure 2c</xref> and <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>): In the presence of finite sampling and stochasticity, multiple different filters could have plausibly given rise to the observed data. A properly inferred posterior will reflect this uncertainty, and include the true filters as one of many plausible explanations of the data (but not necessarily as the ‘mean’ of all plausible explanations) (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>). Increasing the number of Bernoulli samples in the observed data leads to progressively tighter posteriors, with posterior samples closer to the true filter (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>). Furthermore, SNPE closely agrees with the MCMC reference solution in all these cases, further emphasizing the correctness of the posteriors inferred with SNPE.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Estimating receptive fields in linear-nonlinear models of single neurons with statistical inference.</title><p>(<bold>a</bold>) Schematic of a time-varying stimulus, associated observed spike train and resulting spike-triggered average (STA) (<bold>b</bold>) Sequential Neural Posterior Estimation (SNPE) proceeds by first randomly generating simulated receptive fields θ, and using the mechanistic model (here an LN model) to generate simulated spike trains and simulated STAs. (<bold>c</bold>) These simulated STAs and receptive fields are then used to train a deep neural density estimator to identify the distribution of receptive fields consistent with a given observed STA <inline-formula><mml:math id="inf13"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>. (<bold>d</bold>) Relative error in posterior estimation of SNPE and alternative methods (mean and 95% CI; 0 corresponds to perfect estimation, one to prior-level, details in Materials and methods). (<bold>e</bold>) Example of spatial receptive field. We simulated responses and an STA of an LN-model with oriented receptive field. (<bold>f</bold>) We used SNPE to recover the distribution of receptive-field parameters. Univariate and pairwise marginals for four parameters of the spatial filter (MCMC, yellow histograms; SNPE, blue lines; ground truth, green; full posterior in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). Non-identifiabilities of the Gabor parameterization lead to multimodal posteriors. (<bold>g</bold>) Average correlation (± SD) between ground-truth receptive field and receptive field samples from posteriors inferred with SMC-ABC, SNPE, and MCMC (which provides an upper bound given the inherent stochasticity of the data). (<bold>h</bold>) Posterior samples from SNPE posterior (SNPE, blue) compared to ground-truth receptive field (green; see panel (<bold>e</bold>)), overlaid on STA. (<bold>i</bold>) Posterior samples for V1 data; full posterior in <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-fig2-v3.tif"/></fig><p>As a more challenging problem, we inferred the receptive field of a neuron in primary visual cortex (V1) (<xref ref-type="bibr" rid="bib87">Niell and Stryker, 2008</xref>; <xref ref-type="bibr" rid="bib35">Dyballa et al., 2018</xref>). Using a model composed of a bias (related to the spontaneous firing rate) and a Gabor function with eight parameters (<xref ref-type="bibr" rid="bib59">Jones and Palmer, 1987</xref>) describing the receptive field’s location, shape and strength, we simulated responses to 5 min random noise movies of 41 × 41 pixels, such that the STA is high-dimensional, with a total of 1681 dimensions (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). This problem admits multiple solutions (as e.g. rotating the receptive field by 180°). As a result, the posterior distribution has multiple peaks (‘modes’). Starting from a simulation result <inline-formula><mml:math id="inf14"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> with known parameters, we used SNPE to estimate the posterior distribution <inline-formula><mml:math id="inf15"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To deal with the high-dimensional data <inline-formula><mml:math id="inf16"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> in this problem, we used a convolutional neural network (CNN), as this architecture excels at learning relevant features from image data (<xref ref-type="bibr" rid="bib63">Krizhevsky et al., 2012</xref>; <xref ref-type="bibr" rid="bib121">Simonyan and Zisserman, 2015</xref>). To deal with the multiple peaks in the posterior, we fed the CNN’s output into a mixture density network (MDN) (<xref ref-type="bibr" rid="bib11">Bishop, 1994</xref>), which can learn to assign probability distributions with multiple peaks as a function of its inputs (details in Materials and methods). Using this strategy, SNPE was able to infer a posterior distribution that tightly enclosed the ground truth simulation parameters which generated the original simulated data <inline-formula><mml:math id="inf17"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>, and matched a reference MCMC posterior (<xref ref-type="fig" rid="fig2">Figure 2f</xref>, posterior over all parameters in <xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4</xref>). For this challenging estimation problem with high-dimensional summary features, an SMC-ABC algorithm with the same simulation-budget failed to identify the correct receptive fields (<xref ref-type="fig" rid="fig2">Figure 2g</xref>) and posterior distributions (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5</xref>). We also applied this approach to electrophysiological data from a V1 cell (<xref ref-type="bibr" rid="bib35">Dyballa et al., 2018</xref>), identifying a sine-shaped Gabor receptive field consistent with the original spike-triggered average (<xref ref-type="fig" rid="fig2">Figure 2i</xref>; posterior distribution in <xref ref-type="fig" rid="app1fig6">Appendix 1—figure 6</xref>).</p></sec><sec id="s2-3"><title>Functional diversity of ion channels: efficient high-throughput inference</title><p>We next show how SNPE can be efficiently applied to estimation problems in which we want to identify a large number of models for different observations in a database. We considered a flexible model of ion channels (<xref ref-type="bibr" rid="bib30">Destexhe and Huguenard, 2000</xref>), which we here refer to as the <italic>Omnimodel</italic>. This model uses eight parameters to describe how the dynamics of currents through non-inactivating potassium channels depend on membrane voltage (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). For various choices of its parameters <inline-formula><mml:math id="inf18"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula>, it can capture 350 specific models in publications describing this channel type, cataloged in the IonChannelGenealogy (ICG) database (<xref ref-type="bibr" rid="bib103">Podlaski et al., 2017</xref>). We aimed to identify these ion channel parameters <inline-formula><mml:math id="inf19"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> for each ICG model, based on 11 features of the model’s response to a sequence of five noisy voltage clamp protocols, resulting in a total of 55 different characteristic features per model (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, see Materials and methods for details).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Inference on a database of ion-channel models.</title><p>(<bold>a</bold>) We perform inference over the parameters of non-inactivating potassium channel models. Channel kinetics are described by steady-state activation curves, <inline-formula><mml:math id="inf20"><mml:msub><mml:mi mathvariant="normal">∞</mml:mi><mml:mtext>gate</mml:mtext></mml:msub></mml:math></inline-formula>, and time-constant curves, <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>gate</mml:mtext></mml:msub></mml:math></inline-formula>. (<bold>b</bold>) Observation generated from a channel model from ICG database: normalized current responses to three (out of five) voltage-clamp protocols (action potentials, activation, and ramping). Details in <xref ref-type="bibr" rid="bib103">Podlaski et al., 2017</xref>. (<bold>c</bold>) Classical approach to parameter identification: inference is optimized on each datum separately, requiring new computations for each new datum. (<bold>d</bold>) Amortized inference: an inference network is learned which can be applied to multiple data, enabling rapid inference on new data. (<bold>e</bold>) Posterior distribution over eight model parameters, <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf23"><mml:msub><mml:mi>θ</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:math></inline-formula>. Ground truth parameters in green, high-probability parameters in purple, low-probability parameters in magenta. (<bold>f</bold>) Traces obtained by sampling from the posterior in (<bold>e</bold>). Purple: traces sampled from posterior, that is, with high posterior probability. Magenta: trace from parameters with low probability. (<bold>g</bold>) Observations (green) and traces generated by posterior samples (purple) for four models from the database.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-fig3-v3.tif"/></fig><p>Because this model’s output is a typical format for functional characterization of ion channels both in simulations (<xref ref-type="bibr" rid="bib103">Podlaski et al., 2017</xref>) and in high-throughput electrophysiological experiments (<xref ref-type="bibr" rid="bib32">Dunlop et al., 2008</xref>; <xref ref-type="bibr" rid="bib126">Suk et al., 2019</xref>; <xref ref-type="bibr" rid="bib111">Ranjan et al., 2019</xref>), the ability to rapidly infer different parameters for many separate experiments is advantageous. Existing fitting approaches based on numerical optimization (<xref ref-type="bibr" rid="bib30">Destexhe and Huguenard, 2000</xref>; <xref ref-type="bibr" rid="bib111">Ranjan et al., 2019</xref>) must repeat all computations anew for a new experiment or data point (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). However, for SNPE the only heavy computational tasks are carrying out simulations to generate training data, and training the neural network. We therefore reasoned that by training a network once using a large number of simulations, we could subsequently carry out rapid ‘amortized’ parameter inference on new data using a single pass through the network (<xref ref-type="fig" rid="fig3">Figure 3d</xref>; <xref ref-type="bibr" rid="bib123">Speiser et al., 2017</xref>; <xref ref-type="bibr" rid="bib140">Webb et al., 2018</xref>). To test this idea, we used SNPE to train a neural network to infer the posterior from any data <inline-formula><mml:math id="inf24"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula>. To generate training data, we carried out 1 million Omnimodel simulations, with parameters randomly chosen across ranges large enough to capture the models in the ICG database (<xref ref-type="bibr" rid="bib103">Podlaski et al., 2017</xref>). SNPE was run using a single round, that is, it learned to perform inference for all data from the prior (rather than a specific observed datum). Generating these simulations took around 1000 CPU-hours and training the network 150 CPU-hours, but afterwards a full posterior distribution could be inferred for new data in less than 10 ms.</p><p>As a first test, SNPE was run on simulation data, generated by a previously published model of a non-inactivating potassium channel (<xref ref-type="bibr" rid="bib84">McTavish et al., 2012</xref>; <xref ref-type="fig" rid="fig3">Figure 3b</xref>). Simulations of the Omnimodel using parameter sets sampled from the obtained posterior distribution (<xref ref-type="fig" rid="fig3">Figure 3e</xref>) closely resembled the input data on which the SNPE-based inference had been carried out, while simulations using ‘outlier’ parameter sets with low probability under the posterior generated current responses that were markedly different from the data <inline-formula><mml:math id="inf25"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig3">Figure 3f</xref>). Taking advantage of SNPE’s capability for rapid amortized inference, we further evaluated its performance on all 350 non-inactivating potassium channel models in ICG. In each case, we carried out a simulation to generate initial data from the original ICG model, used SNPE to calculate the posterior given the Omnimodel, and then generated a new simulation <inline-formula><mml:math id="inf26"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> using parameters sampled from the posterior (<xref ref-type="fig" rid="fig3">Figure 3f</xref>). This resulted in high correlation between the original ICG model response and the Omnimodel response, in every case (&gt;0.98 for more than 90% of models, see <xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>). However, this approach was not able to capture all traces perfectly, as for example it failed to capture the shape of the onset of the bottom right model in <xref ref-type="fig" rid="fig3">Figure 3g</xref>. Additional analysis of this example revealed that this example is not a failure of SNPE, but rather a limitation of the Omnimodel: in particular, directly fitting the steady-state activation and time-constant curves on this specific example yielded no further quantitative or qualitative improvement, suggesting that the limitation is in the model, not the fit. Thus, SNPE can be used to reveal limitations of candidate models and aid the development of more verisimilar mechanistic models.</p><p>Calculating the posterior for all 350 ICG models took only a few seconds, and was fully automated, that is, did not require user interactions. These results show how SNPE allows fast and accurate identification of biophysical model parameters on new data, and how SNPE can be deployed for applications requiring rapid automated inference, such as high-throughput screening-assays, closed-loop paradigms (e.g. for adaptive experimental manipulations or stimulus-selection [<xref ref-type="bibr" rid="bib62">Kleinegesse and Gutmann, 2019</xref>]), or interactive software tools.</p></sec><sec id="s2-4"><title>Hodgkin–Huxley model: stronger constraints from additional data features</title><p>The Hodgkin–Huxley (HH) model (<xref ref-type="bibr" rid="bib54">Hodgkin and Huxley, 1952</xref>) of action potential generation through ion channel dynamics is a highly influential mechanistic model in neuroscience. A number of algorithms have been proposed for fitting HH models to electrophysiological data (<xref ref-type="bibr" rid="bib108">Prinz et al., 2003</xref>; <xref ref-type="bibr" rid="bib55">Huys et al., 2006</xref>; <xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>; <xref ref-type="bibr" rid="bib116">Rossant et al., 2011</xref>; <xref ref-type="bibr" rid="bib86">Meliza et al., 2014</xref>; <xref ref-type="bibr" rid="bib136">Van Geit et al., 2016</xref>; <xref ref-type="bibr" rid="bib10">Ben-Shalom et al., 2019</xref>), but (with the exception of <xref ref-type="bibr" rid="bib28">Daly et al., 2015</xref>) these approaches do not attempt to estimate the full posterior. Given the central importance of the HH model in neuroscience, we sought to test how SNPE would cope with this challenging non-linear model.</p><p>As previous approaches for HH models concentrated on reproducing specified features (e.g. the number of spikes, [<xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>]), we also sought to determine how various features provide different constraints. We considered the problem of inferring eight biophysical parameters in a HH single-compartment model, describing voltage-dependent sodium and potassium conductances and other intrinsic membrane properties, including neural noise, making the model stochastic by nature (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, left). We simulated the neuron’s voltage response to the injection of a square wave of depolarizing current, and defined the model output <inline-formula><mml:math id="inf27"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> used for inference as the number of evoked action potentials along with six additional features of the voltage response (<xref ref-type="fig" rid="fig4">Figure 4a</xref>, right, details in Materials and methods). We first applied SNPE to observed data <inline-formula><mml:math id="inf28"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> created by simulation from the model, calculating the posterior distribution using all seven features in the observed data (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). The posterior contained the ground truth parameters in a high probability-region, as in previous applications, indicating the consistency of parameter identification. The variance of the posterior was narrower for some parameters than for others, indicating that the seven data features constrain some parameters strongly (such as the potassium conductance), but others only weakly (such as the adaptation time constant). Additional simulations with parameters sampled from the posterior closely resembled the observed data <inline-formula><mml:math id="inf29"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>, in terms of both the raw membrane voltage over time and the seven data features (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, purple and green). Parameters with low posterior probability (outliers) generated simulations that markedly differed from <inline-formula><mml:math id="inf30"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, magenta).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Inference for single compartment Hodgkin–Huxley model.</title><p>(<bold>a</bold>) Circuit diagram describing the Hodgkin–Huxley model (left), and simulated voltage-trace given a current input (right). Three out of 7 voltage features are depicted: (1) number of spikes, (2) mean resting potential, and (3) standard deviation of the pre-stimulus resting potential. (<bold>b</bold>) Inferred posterior for 8 parameters given seven voltage features. Ground truth parameters in green, high-probability parameters in purple, low-probability parameters in magenta. (<bold>c</bold>) Traces (left) and associated features <italic>f</italic> (right) for the desired output (observation), the mode of the inferred posterior, and a sample with low posterior probability. The voltage features are: number of spikes <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:math></inline-formula>, mean resting potential <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, standard deviation of the resting potential <inline-formula><mml:math id="inf33"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>rpot</mml:mtext></mml:msub></mml:math></inline-formula>, and the first four voltage moments, mean <inline-formula><mml:math id="inf34"><mml:msub><mml:mi>m</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula>, standard deviation <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, skewness <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msub></mml:math></inline-formula> and kurtosis <inline-formula><mml:math id="inf37"><mml:msub><mml:mi>m</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:math></inline-formula>. Each value plotted is the mean feature ± standard deviation across 100 simulations with the same parameter set. Each feature is normalized by <inline-formula><mml:math id="inf38"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>f PRIOR</mml:mtext></mml:msub></mml:math></inline-formula>, the standard deviation of the respective feature of simulations sampled from the prior. (<bold>d</bold>) Partial view of the inferred posteriors (4 out of 8 parameters) given 1, 4 and 7 features (full posteriors over eight parameters in <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref>). (<bold>e</bold>) Traces for posterior modes given 1, 4 and 7 features. Increasing the number of features leads to posterior traces that are closer to the observed data. (<bold>f</bold>) Observations from Allen Cell Types Database (green) and corresponding mode samples (purple). Posteriors in <xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-fig4-v3.tif"/></fig><p>Genetic algorithms are commonly used to fit parameters of deterministic biophysical models (<xref ref-type="bibr" rid="bib31">Druckmann et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Hay et al., 2011</xref>; <xref ref-type="bibr" rid="bib136">Van Geit et al., 2016</xref>; <xref ref-type="bibr" rid="bib43">Gouwens et al., 2018</xref>). While genetic algorithms can also return multiple data-compatible parameters, they do not perform inference (i.e. find the posterior distribution), and their outputs depend strongly on user-defined goodness-of-fit criteria. When comparing a state-of-the-art genetic algorithm (Indicator Based Evolutionary Algorithm, IBEA, [<xref ref-type="bibr" rid="bib13">Bleuler et al., 2003</xref>; <xref ref-type="bibr" rid="bib144">Zitzler and Künzli, 2004</xref>; <xref ref-type="bibr" rid="bib136">Van Geit et al., 2016</xref>]) to SNPE, we found that the parameter-settings favored by IBEA produced simulations whose summary features were as similar to the observed data as those obtained by SNPE high-probability samples (<xref ref-type="fig" rid="app1fig10">Appendix 1—figure 10</xref>). However, high-scoring IBEA parameters were concentrated in small regions of the posterior, that is, IBEA did not identify the full space of data-compatible models.</p><p>To investigate how individual data features constrain parameters, we compared SNPE-estimated posteriors based (1) solely on the spike count, (2) on the spike count and three voltage-features, or (3) on all 7 features of <inline-formula><mml:math id="inf39"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>. As more features were taken into account, the posterior became narrower and centered more closely on the ground truth parameters (<xref ref-type="fig" rid="fig4">Figure 4d</xref>, <xref ref-type="fig" rid="app1fig8">Appendix 1—figure 8</xref>). Posterior simulations matched the observed data only in those features that had been used for inference (e.g. applying SNPE to spike counts alone identified parameters that generated the correct number of spikes, but for which spike timing and subthreshold voltage time course were off, <xref ref-type="fig" rid="fig4">Figure 4e</xref>). For some parameters, such as the potassium conductance, providing more data features brought the peak of the posterior (the <italic>posterior mode</italic>) closer to the ground truth and also decreased uncertainty. For other parameters, such as <inline-formula><mml:math id="inf40"><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula>, a parameter adjusting the spike threshold (<xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>), the peak of the posterior was already close to the correct value with spike counts alone, but adding additional features reduced uncertainty. While SNPE can be used to study the effect of additional data features in reducing parameter uncertainty, this would not be the case for methods that only return a single best-guess estimate of parameters. These results show that SNPE can reveal how information from multiple data features imposes collective constraints on channel and membrane properties in the HH model.</p><p>We also inferred HH parameters for eight in vitro recordings from the Allen Cell Types database using the same current-clamp stimulation protocol as in our model (<xref ref-type="bibr" rid="bib3">Allen Institute for Brain Science, 2016</xref>; <xref ref-type="bibr" rid="bib132">Teeter et al., 2018</xref>; <xref ref-type="fig" rid="fig4">Figure 4f</xref>, <xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>). In each case, simulations based on the SNPE-inferred posterior closely resembled the original data (<xref ref-type="fig" rid="fig4">Figure 4f</xref>). We note that while inferred parameters differed across recordings, some parameters (the spike threshold, the density of sodium channels, the membrane reversal potential and the density of potassium channels) were consistently more strongly constrained than others (the intrinsic neural noise, the adaptation time constant, the density of slow voltage-dependent channels and the leak conductance) (<xref ref-type="fig" rid="app1fig9">Appendix 1—figure 9</xref>). Overall, these results suggest that the electrophysiological responses measured by this current-clamp protocol can be approximated by a single-compartment HH model, and that SNPE can identify the admissible parameters.</p></sec><sec id="s2-5"><title>Crustacean stomatogastric ganglion: sensitivity to perturbations</title><p>We next aimed to demonstrate how the full posterior distribution obtained with SNPE can lead to novel scientific insights. To do so, we used the pyloric network of the stomatogastric ganglion (STG) of the crab <italic>Cancer borealis</italic>, a well-characterized neural circuit producing rhythmic activity. In this circuit, similar network activity can arise from vastly different sets of membrane and synaptic conductances (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>). We first investigated whether data-consistent sets of membrane and synaptic conductances are connected in parameter space, as has been demonstrated for single neurons (<xref ref-type="bibr" rid="bib130">Taylor et al., 2006</xref>), and, second, which compensation mechanisms between parameters of this circuit allow the neural system to maintain its activity despite parameter variations. While this model has been studied extensively, answering these questions requires characterizing higher dimensional parameter spaces than those accessed previously. We demonstrate how SNPE can be used to identify the posterior distribution over both membrane and synaptic conductances of the STG (31 parameters total) and how the full posterior distribution can be used to study the above questions at the circuit level.</p><p>For some biological systems, multiple parameter sets give rise to the same system behavior (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib81">Marder and Goaillard, 2006</xref>; <xref ref-type="bibr" rid="bib47">Gutierrez et al., 2013</xref>; <xref ref-type="bibr" rid="bib36">Fisher et al., 2013</xref>; <xref ref-type="bibr" rid="bib80">Marder et al., 2015</xref>; <xref ref-type="bibr" rid="bib4">Alonso and Marder, 2019</xref>). In particular, neural systems can be robust to specific perturbations of parameters (<xref ref-type="bibr" rid="bib88">O'Leary et al., 2014</xref>; <xref ref-type="bibr" rid="bib80">Marder et al., 2015</xref>; <xref ref-type="bibr" rid="bib90">O'Leary and Marder, 2016</xref>), yet highly sensitive to others, properties referred to as <italic>sloppiness</italic> and <italic>stiffness</italic> (<xref ref-type="bibr" rid="bib41">Goldman et al., 2001</xref>; <xref ref-type="bibr" rid="bib46">Gutenkunst et al., 2007</xref>; <xref ref-type="bibr" rid="bib73">Machta et al., 2013</xref>; <xref ref-type="bibr" rid="bib89">O'Leary et al., 2015</xref>). We studied how perturbations affect model output using a model (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>) and data (<xref ref-type="bibr" rid="bib49">Haddad and Marder, 2018</xref>) of the pyloric rhythm in the crustacean stomatogastric ganglion (STG). This model describes a triphasic motor pattern generated by a well-characterized circuit (<xref ref-type="fig" rid="fig5">Figure 5a</xref>). The circuit consists of two electrically coupled pacemaker neurons (anterior burster and pyloric dilator, AB/PD), modeled as a single neuron, as well as two types of follower neurons (lateral pyloric (LP) and pyloric (PY)), all connected through inhibitory synapses (details in Materials and methods). Eight membrane conductances are included for each modeled neuron, along with seven synaptic conductances, for a total of 31 parameters. This model has been used to demonstrate that virtually indistinguishable activity can arise from vastly different membrane and synaptic conductances in the STG (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib4">Alonso and Marder, 2019</xref>). Here, we build on these studies and extend the model to include intrinsic neural noise on each neuron (see Materials and methods).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Identifying network models underlying an experimentally observed pyloric rhythm in the crustacean stomatogastric ganglion.</title><p>(<bold>a</bold>) Simplified circuit diagram of the pyloric network from the stomatogastric ganglion. Thin connections are fast glutamatergic, thick connections are slow cholinergic. (<bold>b</bold>) Extracellular recordings from nerves of pyloric motor neurons of the crab <italic>Cancer borealis</italic> (<xref ref-type="bibr" rid="bib49">Haddad and Marder, 2018</xref>). Numbers indicate some of the used summary features, namely cycle period (1), phase delays (2), phase gaps (3), and burst durations (4) (see Materials and methods for details). (<bold>c</bold>) Posterior over 24 membrane and seven synaptic conductances given the experimental observation shown in panel b (eight parameters shown, full posterior in <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>). Two high-probability parameter sets in purple. Inset: magnified marginal posterior for the synaptic strengths AB to LP neuron vs. PD to LP neuron. (<bold>d</bold>) Identifying directions of sloppiness and stiffness. Two samples from the posterior both show similar network activity as the experimental observation (top left and top right), but have very different parameters (purple dots in panel c). Along the high-probability path between these samples, network activity is preserved (trace 1). When perturbing the parameters orthogonally off the path, network activity changes abruptly and becomes non-pyloric (trace 2).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-fig5-v3.tif"/></fig><p>We applied SNPE to an extracellular recording from the STG of the crab <italic>Cancer borealis</italic> (<xref ref-type="bibr" rid="bib49">Haddad and Marder, 2018</xref>) which exhibited pyloric activity (<xref ref-type="fig" rid="fig5">Figure 5b</xref>), and inferred the posterior distribution over all 31 parameters based on 18 salient features of the voltage traces, including cycle period, phase delays, phase gaps, and burst durations (features in <xref ref-type="fig" rid="fig5">Figure 5B</xref>, posterior in <xref ref-type="fig" rid="fig5">Figure 5c</xref>, posterior over all parameters in <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>, details in Materials and methods). Consistent with previous reports, the posterior distribution has high probability over extended value ranges for many membrane and synaptic conductances. To verify that parameter settings across these extended ranges are indeed capable of generating the experimentally observed network activity, we sampled two sets of membrane and synaptic conductances from the posterior distribution. These two samples have widely disparate parameters from each other (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, purple dots, details in Materials and methods), but both exhibit activity highly similar to the experimental observation (<xref ref-type="fig" rid="fig5">Figure 5d</xref>, top left and top right).</p><p>We then investigated the geometry of the parameter space producing these rhythms (<xref ref-type="bibr" rid="bib2">Achard and De Schutter, 2006</xref>; <xref ref-type="bibr" rid="bib4">Alonso and Marder, 2019</xref>). First, we wanted to identify directions of sloppiness, and we were interested in whether parameter settings producing pyloric rhythms form a single connected region, as has been shown for single neurons (<xref ref-type="bibr" rid="bib130">Taylor et al., 2006</xref>), or whether they lie on separate ‘islands’. Starting from the two parameter settings showing similar activity above, we examined whether they were connected by searching for a path through parameter space along which pyloric activity was maintained. To do this, we algorithmically identified a path lying only in regions of high posterior probability (<xref ref-type="fig" rid="fig5">Figure 5c</xref>, white, details in Materials and methods). Along the path, network output was tightly preserved, despite a substantial variation of the parameters (voltage trace 1 in <xref ref-type="fig" rid="fig5">Figure 5d</xref>, <xref ref-type="fig" rid="app1fig12">Appendix 1—figure 12</xref>). Second, we inspected directions of stiffness by perturbing parameters off the path. We applied perturbations that yield maximal drops in posterior probability (see Materials and methods for details), and found that the network quickly produced non-pyloric activity (voltage trace 2, <xref ref-type="fig" rid="fig5">Figure 5d</xref>; <xref ref-type="bibr" rid="bib41">Goldman et al., 2001</xref>). Note that, while parameter set 2 seems to lie in regions of high probability when inspecting pairwise marginals, it in fact has low probability under the full posterior distribution (<xref ref-type="fig" rid="app1fig13">Appendix 1—figure 13</xref>). In identifying these paths and perturbations, we exploited the fact that SNPE provides a differentiable estimate of the posterior, as opposed to parameter search methods which provide only discrete samples.</p><p>Overall, these results show that the pyloric network can be robust to specific perturbations in parameter space, but sensitive to others, and that one can interpolate between disparate solutions while preserving network activity. This analysis demonstrates the flexibility of SNPE in capturing complex posterior distributions, and shows how the differentiable posterior can be used to study directions of sloppiness and stiffness.</p></sec><sec id="s2-6"><title>Predicting compensation mechanisms from posterior distributions</title><p>Experimental and computational studies have shown that stable neural activity can be maintained despite variable circuit parameters (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>; <xref ref-type="bibr" rid="bib82">Marder and Taylor, 2011</xref>; <xref ref-type="bibr" rid="bib92">O’Leary, 2018</xref>). This behavior can emerge from two sources (<xref ref-type="bibr" rid="bib82">Marder and Taylor, 2011</xref>): either, the variation of a certain parameter barely influences network activity at all, or alternatively, variations of several parameters influence network activity, but their effects compensate for one another. Here, we investigated these possibilities by using the posterior distribution over membrane and synaptic conductances of the STG.</p><p>We began by drawing samples from the posterior and inspecting their pairwise histograms (i.e. the pairwise marginals, <xref ref-type="fig" rid="fig6">Figure 6a</xref>, posterior over all parameters in <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>). Consistent with previously reported results (<xref ref-type="bibr" rid="bib131">Taylor et al., 2009</xref>), many parameters seem only weakly constrained and only weakly correlated (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). However, this observation does not imply that the parameters of the network do not have to be finely tuned: pairwise marginals are averages over many network configurations, where all other parameters may take on diverse values, which could disguise that each individual configuration is finely tuned. Indeed, when we sampled parameters independently from their posterior histograms, the resulting circuit configurations rarely produced pyloric activity, indicating that parameters have to be tuned relative to each other (<xref ref-type="fig" rid="app1fig14">Appendix 1—figure 14</xref>). This analysis also illustrates that the (common) approach of independently setting parameters can be problematic: although each parameter individually is in a realistic range, the network as a whole is not (<xref ref-type="bibr" rid="bib42">Golowasch et al., 2002</xref>). Finally, it shows the importance of identifying the full posterior distribution, which is far more informative than just finding individual parameters and assigning error bars.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Predicting compensation mechanisms in the stomatogastric ganglion.</title><p>(<bold>a</bold>) Inferred posterior. We show a subset of parameters which are weakly constrained (full posterior in <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>). Pyloric activity can emerge from a wide range of maximal membrane conductances, as the 1D and 2D posterior marginals cover almost the entire extent of the prior. (<bold>b</bold>) Correlation matrix, based on the samples shown in panel (a). Almost all correlations are weak. Ordering of membrane and synaptic conductances as in <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>. (<bold>c</bold>) Conditional distributions given a particular circuit configuration: for the plots on the diagonal, we keep all but one parameter fixed. For plots above the diagonal, we keep all but two parameters fixed. The remaining parameter(s) are narrowly tuned; tuning across parameters is often highly correlated. When conditioning on a different parameter setting (right plot), the conditional posteriors change, but correlations are often maintained. (<bold>d</bold>) Conditional correlation matrix, averaged over 500 conditional distributions like the ones shown in panel (c). Black squares highlight parameter-pairs within the same model neuron. (<bold>e</bold>) Consistency with experimental observations. Top: maximal conductance of the fast transient potassium current and the maximal conductance of the hyperpolarization current are positively correlated for all three neurons. This has also been experimentally observed in the PD and the LP neuron (<xref ref-type="bibr" rid="bib76">MacLean et al., 2005</xref>). Bottom: the maximal conductance of the hyperpolarization current of the postsynaptic neuron can compensate the strength of the synaptic input, as experimentally observed in the PD and the LP neuron (<xref ref-type="bibr" rid="bib44">Grashow et al., 2010</xref>; <xref ref-type="bibr" rid="bib79">Marder, 2011</xref>). The boxplots indicate the maximum, 75% quantile, median, 25% quantile, and minimum across 500 conditional correlations for different parameter pairs. Face color indicates mean correlation using the colorbar shown in panel (b).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-fig6-v3.tif"/></fig><p>In order to investigate the need for tuning between pairs of parameters, we held all but two parameters constant at a given consistent circuit configuration (sampled from the posterior), and observed the network activity across different values of the remaining pair of parameters. We can do so by calculating the conditional posterior distribution (details in Materials and methods), and do not have to generate additional simulations (as would be required by parameter search methods). Doing so has a simple interpretation: when all but two parameters are fixed, what values of the remaining two parameters can then lead to the desired network activity? We found that the desired pattern of pyloric activity can emerge only from narrowly tuned and often highly correlated combinations of the remaining two parameters, showing how these parameters can compensate for one another (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). When repeating this analysis across multiple network configurations, we found that these ‘conditional correlations’ are often preserved (<xref ref-type="fig" rid="fig6">Figure 6c</xref>, left and right). This demonstrates that pairs of parameters can compensate for each other in a similar way, independently of the values taken by other parameters. This observation about compensation could be interpreted as an instance of modularity, a widespread underlying principle of biological robustness (<xref ref-type="bibr" rid="bib61">Kitano, 2004</xref>).</p><p>We calculated conditional correlations for each parameter pair using 500 different circuit configurations sampled from the posterior (<xref ref-type="fig" rid="fig6">Figure 6d</xref>). Compared to correlations based on the pairwise marginals (<xref ref-type="fig" rid="fig6">Figure 6b</xref>), these conditional correlations were substantially stronger. They were particularly strong across membrane conductances of the same neuron, but primarily weak across different neurons (black boxes in <xref ref-type="fig" rid="fig6">Figure 6d</xref>).</p><p>Finally, we tested whether the conditional correlations were in line with experimental observations. For the PD and the LP neuron, it has been reported that overexpression of the fast transient potassium current (<inline-formula><mml:math id="inf41"><mml:msub><mml:mi>I</mml:mi><mml:mtext>A</mml:mtext></mml:msub></mml:math></inline-formula>) leads to a compensating increase of the hyperpolarization current (<inline-formula><mml:math id="inf42"><mml:msub><mml:mi>I</mml:mi><mml:mtext>H</mml:mtext></mml:msub></mml:math></inline-formula>), suggesting a positive correlation between these two currents (<xref ref-type="bibr" rid="bib75">MacLean et al., 2003</xref>; <xref ref-type="bibr" rid="bib76">MacLean et al., 2005</xref>). These results are qualitatively consistent with the positive conditional correlations between the maximal conductances of <inline-formula><mml:math id="inf43"><mml:msub><mml:mi>I</mml:mi><mml:mtext>A</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:msub><mml:mi>I</mml:mi><mml:mtext>H</mml:mtext></mml:msub></mml:math></inline-formula> for all three model neurons (<xref ref-type="fig" rid="fig6">Figure 6e</xref> top). In addition, using the dynamic clamp, it has been shown that diverse combinations of the synaptic input strength and the maximal conductance of <inline-formula><mml:math id="inf45"><mml:msub><mml:mi>I</mml:mi><mml:mtext>H</mml:mtext></mml:msub></mml:math></inline-formula> lead to similar activity in the LP and the PD neuron (<xref ref-type="bibr" rid="bib44">Grashow et al., 2010</xref>; <xref ref-type="bibr" rid="bib79">Marder, 2011</xref>). Consistent with these findings, the non-zero conditional correlations reveal that there can indeed be compensation mechanisms between the synaptic strength and the maximal conductance of <inline-formula><mml:math id="inf46"><mml:msub><mml:mi>I</mml:mi><mml:mtext>H</mml:mtext></mml:msub></mml:math></inline-formula> of the postsynaptic neuron (<xref ref-type="fig" rid="fig6">Figure 6e</xref> bottom).</p><p>Overall, we showed how SNPE can be used to study parameter dependencies, and how the posterior distribution can be used to efficiently explore potential compensation mechanisms. We found that our method can predict compensation mechanisms which are qualitatively consistent with experimental studies. We emphasize that these findings would not have been possible with a direct grid-search over all parameters: defining a grid in a 31-dimensional parameter space would require more than 2<sup>31</sup> &gt; 2 billion simulations, even if one were to use the coarsest-possible grid with only two values per dimension.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>How can we build models which give insights into the causal mechanisms underlying neural or behavioral dynamics? The cycle of building mechanistic models, generating predictions, comparing them to empirical data, and rejecting or refining models has been of crucial importance in the empirical sciences. However, a key challenge has been the difficulty of identifying mechanistic models which can quantitatively capture observed phenomena. We suggest that a generally applicable tool to constrain mechanistic models by data would expedite progress in neuroscience. While many considerations should go into designing a model that is appropriate for a given question and level of description (<xref ref-type="bibr" rid="bib53">Herz et al., 2006</xref>; <xref ref-type="bibr" rid="bib16">Brette, 2015</xref>; <xref ref-type="bibr" rid="bib38">Gerstner et al., 2012</xref>; <xref ref-type="bibr" rid="bib89">O'Leary et al., 2015</xref>), the question of whether and how one can perform statistical inference should not compromise model design. In our tool, SNPE, the process of model building and parameter inference are entirely decoupled. SNPE can be applied to <italic>any</italic> simulation-based model (requiring neither model nor summary features to be differentiable) and gives full flexibility on defining a prior. We illustrated the power of our approach on a diverse set of applications, highlighting the potential of SNPE to rapidly identify data-compatible mechanistic models, to investigate which data-features effectively constrain parameters, and to reveal shortcomings of candidate-models.</p><p>Finally, we used a model of the stomatogastric ganglion to show how SNPE can identify complex, high-dimensional parameter landscapes of neural systems. We analyzed the geometrical structure of the parameter landscape and confirmed that circuit configurations need to be finely tuned, even if individual parameters can take on a broad range of values. We showed that different configurations are connected in parameter space, and provided hypotheses for compensation mechanisms. These analyses were made possible by SNPE’s ability to estimate full parameter posteriors, rather than just constraints on individual parameters, as is common in many statistical parameter-identification approaches.</p><sec id="s3-1"><title>Related work</title><p>SNPE builds on recent advances in machine learning and in particular in density-estimation approaches to likelihood-free inference (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib66">Le et al., 2017a</xref>; <xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Chan et al., 2018</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>, reviewed in <xref ref-type="bibr" rid="bib26">Cranmer et al., 2020</xref>). We here scaled these approaches to canonical mechanistic models of neural dynamics and provided methods and software-tools for inference, visualization, and analysis of the resulting posteriors (e.g. the high-probability paths and conditional correlations presented here).</p><p>The idea of learning inference networks on simulated data can be traced back to <italic>regression-adjustment</italic> methods in ABC (<xref ref-type="bibr" rid="bib8">Beaumont et al., 2002</xref>; <xref ref-type="bibr" rid="bib15">Blum and François, 2010</xref>). <xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref> first proposed to use expressive conditional density estimators in the form of deep neural networks (<xref ref-type="bibr" rid="bib11">Bishop, 1994</xref>; <xref ref-type="bibr" rid="bib95">Papamakarios et al., 2017</xref>), and to optimize them sequentially over multiple rounds with cost-functions derived from Bayesian inference principles. Compared to commonly used rejection-based ABC methods (<xref ref-type="bibr" rid="bib117">Rubin, 1984</xref>; <xref ref-type="bibr" rid="bib110">Pritchard et al., 1999</xref>), such as MCMC-ABC (<xref ref-type="bibr" rid="bib83">Marjoram et al., 2003</xref>), SMC-ABC (<xref ref-type="bibr" rid="bib122">Sisson et al., 2007</xref>; <xref ref-type="bibr" rid="bib68">Liepe et al., 2014</xref>), Bayesian-Optimization ABC (<xref ref-type="bibr" rid="bib48">Gutmann and Corander, 2016</xref>), or ensemble methods (<xref ref-type="bibr" rid="bib17">Britton et al., 2013</xref>; <xref ref-type="bibr" rid="bib65">Lawson et al., 2018</xref>), SNPE approaches do not require one to define a distance function in data space. In addition, by leveraging the ability of neural networks to learn informative features, they enable scaling to problems with high-dimensional observations, as are common in neuroscience and other fields in biology. We have illustrated this capability in the context of receptive field estimation, where a convolutional neural network extracts summary features from a 1681 dimensional spike-triggered average. Alternative likelihood-free approaches include <italic>synthetic likelihood</italic> methods (<xref ref-type="bibr" rid="bib142">Wood, 2010</xref>; <xref ref-type="bibr" rid="bib25">Costa et al., 2013</xref>; <xref ref-type="bibr" rid="bib141">Wilkinson, 2014</xref>; <xref ref-type="bibr" rid="bib85">Meeds and Welling, 2014</xref>; <xref ref-type="bibr" rid="bib96">Papamakarios et al., 2019a</xref>; <xref ref-type="bibr" rid="bib72">Lueckmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib33">Durkan et al., 2018</xref>), moment-based approximations of the posterior (<xref ref-type="bibr" rid="bib6">Barthelmé and Chopin, 2014</xref>; <xref ref-type="bibr" rid="bib120">Schröder et al., 2019</xref>), inference compilation (<xref ref-type="bibr" rid="bib67">Le et al., 2017b</xref>; <xref ref-type="bibr" rid="bib20">Casado et al., 2017</xref>), and density-ratio estimation (<xref ref-type="bibr" rid="bib51">Hermans et al., 2020</xref>). For some mechanistic models in neuroscience (e.g. for integrate-and-fire neurons), likelihoods can be computed via stochastic numerical approximations (<xref ref-type="bibr" rid="bib22">Chen, 2003</xref>; <xref ref-type="bibr" rid="bib56">Huys and Paninski, 2009</xref>; <xref ref-type="bibr" rid="bib86">Meliza et al., 2014</xref>) or model-specific analytical approaches (<xref ref-type="bibr" rid="bib55">Huys et al., 2006</xref>; <xref ref-type="bibr" rid="bib52">Hertäg et al., 2012</xref>; <xref ref-type="bibr" rid="bib107">Pozzorini et al., 2015</xref>; <xref ref-type="bibr" rid="bib64">Ladenbauer et al., 2018</xref>; <xref ref-type="bibr" rid="bib113">René et al., 2020</xref>).</p><p>How big is the advance brought by SNPE relative to ‘conventional’ brute-force approaches that aim to exhaustively explore parameter space? A fundamental difference from grid search approaches that have been applied to neuroscientific models (<xref ref-type="bibr" rid="bib108">Prinz et al., 2003</xref>; <xref ref-type="bibr" rid="bib19">Caplan et al., 2014</xref>; <xref ref-type="bibr" rid="bib125">Stringer et al., 2016</xref>) is that SNPE can perform Bayesian inference for <italic>stochastic</italic> models, whereas previous approaches identified parameters whose <italic>deterministic</italic> model-outputs were heuristically ‘close’ to empirical data. Depending on the goal of the analysis, either approach might be preferable. SNPE, and Bayesian inference more generally, is derived for stochastic models. SNPE can, in principle, also be applied to deterministic models, but a rigorous mathematical interpretation or empirical evaluation in this regime is beyond the scope of this study. SNPE also differs conceptually and quantitatively from rejection-ABC, in which random parameters are accepted or rejected based on a distance-criterion. SNPE uses <italic>all</italic> simulations during training instead of rejecting some, learns to identify data features informative about model parameters rather than relying on the user to choose the correct data features and distance metric, and performs considerably better than rejection-ABC, in particular for problems with high-dimensional observations (<xref ref-type="fig" rid="fig2">Figure 2</xref>). Another advantage over grid search and rejection-ABC is that SNPE can ‘amortize’ inference of parameter posteriors, so that one can quickly perform inference on new data, or explore compensation mechanisms, without having to carry out new simulations, or repeatedly search a simulation database. We should still note that SNPE can require the generation of large sets of simulations, which can be viewed as a brute-force step, emphasising that one of the main strengths of SNPE over conventional brute-force approaches relies on the processing of these simulations via deep neural density estimators.</p><p>Our approach is already finding its first applications in neuroscience–for example, <xref ref-type="bibr" rid="bib91">Oesterle et al., 2020</xref> have used a variant of SNPE to constrain biophysical models of retinal neurons, with the goal of optimizing stimulation approaches for neuroprosthetics. Concurrently with our work, <xref ref-type="bibr" rid="bib12">Bittner et al., 2019</xref> developed an alternative approach to parameter identification for mechanistic models and showed how it can be used to characterize neural population models which exhibit specific emergent computational properties. Both studies differ in their methodology and domain of applicability (see descriptions of underlying algorithms in our prior work [<xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>] and theirs [<xref ref-type="bibr" rid="bib70">Loaiza-Ganem et al., 2017</xref>]), as well in the focus of their neuroscientific contributions. Both approaches share the overall goal of using deep probabilistic inference tools to build more interpretable models of neural data. These complementary and concurrent advances will expedite the cycle of building, adjusting and selecting mechanistic models in neuroscience.</p><p>Finally, a complementary approach to mechanistic modeling is to pursue purely phenomenological models, which are designed to have favorable statistical and computational properties: these data-driven models can be efficiently fit to neural data (<xref ref-type="bibr" rid="bib18">Brown et al., 1998</xref>; <xref ref-type="bibr" rid="bib135">Truccolo et al., 2005</xref>; <xref ref-type="bibr" rid="bib100">Pillow, 2007</xref>; <xref ref-type="bibr" rid="bib101">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib119">Schneidman et al., 2006</xref>; <xref ref-type="bibr" rid="bib74">Macke et al., 2011</xref>; <xref ref-type="bibr" rid="bib143">Yu et al., 2009</xref>; <xref ref-type="bibr" rid="bib93">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib27">Cunningham and Yu, 2014</xref>) or to implement desired computations (<xref ref-type="bibr" rid="bib127">Sussillo and Abbott, 2009</xref>). Although tremendously useful for a quantitative characterization of neural dynamics, these models typically have a large number of parameters, which rarely correspond to physically measurable or mechanistically interpretable quantities, and thus it can be challenging to derive mechanistic insights or causal hypotheses from them (but see e.g. <xref ref-type="bibr" rid="bib78">Mante et al., 2013</xref>; <xref ref-type="bibr" rid="bib128">Sussillo and Barak, 2013</xref>; <xref ref-type="bibr" rid="bib77">Maheswaranathan et al., 2019</xref>).</p></sec><sec id="s3-2"><title>Use of summary features</title><p>When fitting mechanistic models to data, it is common to target summary features to isolate specific behaviors, rather than the full data. For example, the spike shape is known to constrain sodium and potassium conductances (<xref ref-type="bibr" rid="bib31">Druckmann et al., 2007</xref>; <xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>; <xref ref-type="bibr" rid="bib50">Hay et al., 2011</xref>). When modeling population dynamics, it is often desirable to achieve realistic firing rates, rate-correlations and response nonlinearities (<xref ref-type="bibr" rid="bib118">Rubin et al., 2015</xref>; <xref ref-type="bibr" rid="bib12">Bittner et al., 2019</xref>), or specified oscillations (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>). In models of decision making, one is often interested in reproducing psychometric functions or reaction-time distributions (<xref ref-type="bibr" rid="bib112">Ratcliff and McKoon, 2008</xref>). Choice of summary features might also be guided by known limitations of either the model or the measurement approach, or necessitated by the fact that published data are only available in summarized form. Several methods have been proposed to automatically construct informative summary features (<xref ref-type="bibr" rid="bib14">Blum et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Jiang et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Izbicki et al., 2019</xref>). SNPE can be applied to, and might benefit from the use of summary features, but it also makes use of the ability of neural networks to automatically learn informative features in high-dimensional data. Thus, SNPE can also be applied directly to raw data (e.g. using recurrent neural networks [<xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>]), or to high-dimensional summary features which are challenging for ABC approaches (<xref ref-type="fig" rid="fig2">Figure 2</xref>). In all cases, care is needed when interpreting models fit to summary features, as choice of features can influence the results (<xref ref-type="bibr" rid="bib14">Blum et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Jiang et al., 2017</xref>; <xref ref-type="bibr" rid="bib57">Izbicki et al., 2019</xref>).</p></sec><sec id="s3-3"><title>Applicability and limitations</title><p>A key advantage of SNPE is its general applicability: it can be applied whenever one has a simulator that allows to stochastically generate model outputs from specific parameters. Furthermore, it can be applied in a fully ‘black-box manner’, that is, does not require access to the internal workings of the simulator, its model equations, likelihoods or gradients. It does not impose any other limitations on the model or the summary features, and in particular does not require them to be differentiable. However, it also has limitations which we enumerate below.</p><p>First, current implementations of SNPE scale well to high-dimensional observations (∼1000s of dimensions, also see <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>), but scaling SNPE to even higher-dimensional parameter spaces (above 30) is challenging (note that previous approaches were generally limited to less than 10 dimensions). Given that the difficulty of estimating full posteriors scales exponentially with dimensionality, this is an inherent challenge for all approaches that aim at full inference (in contrast to just identifying a single, or a few heuristically chosen parameter fits).</p><p>Second, while it is a long-term goal for these approaches to be made fully automatic, our current implementation still requires choices by the user: as described in Materials and methods, one needs to choose the type of the density estimation network, and specify settings related to network-optimization, and the number of simulations and inference rounds. These settings depend on the complexity of the relation between summary features and model parameters, and the number of simulations that can be afforded. In the documentation accompanying our code-package, we provide examples and guidance. For small-scale problems, we have found SNPE to be robust to these settings. However, for challenging, high-dimensional applications, SNPE might currently require substantial user interaction.</p><p>Third, the power of SNPE crucially rests on the ability of deep neural networks to perform density estimation. While deep nets have had ample empirical success, we still have an incomplete understanding of their limitations, in particular in cases where the mapping between data and parameters might not be smooth (e.g. near phase transitions).</p><p>Fourth, when applying SNPE (or any other model-identification approach), validation of the results is of crucial importance, both to assess the accuracy of the inference procedure, as well as to identify possible limitations of the mechanistic model itself. In the example applications, we used several procedures for assessing the quality of the inferred posteriors. One common ingredient of these approaches is to sample from the inferred model, and search for systematic differences between observed and simulated data, e.g. to perform <italic>posterior predictive checks</italic> (<xref ref-type="bibr" rid="bib24">Cook et al., 2006</xref>; <xref ref-type="bibr" rid="bib129">Talts et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Liepe et al., 2014</xref>; <xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>; <xref ref-type="fig" rid="fig2">Figure 2g</xref>, <xref ref-type="fig" rid="fig3">Figure 3f,g</xref>, <xref ref-type="fig" rid="fig4">Figure 4c</xref>, and <xref ref-type="fig" rid="fig5">Figure 5d</xref>). These approaches allow one to detect ‘failures’ of SNPE, that is, cases in which samples from the posterior do not reproduce the data. However, when diagnosing any Bayesian inference approach, it is challenging to rigorously rule out the possibility that additional parameter-settings (e.g. in an isolated ‘island’) would also explain the data. Thus, it is good practice to use multiple initializations of SNPE, and/or a large number of simulations in the initial round. There are challenges and opportunities ahead in further scaling and automating simulation-based inference approaches. However, in its current form, SNPE will be a powerful tool for quantitatively evaluating mechanistic hypotheses on neural data, and for designing better models of neural dynamics.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Code availability</title><p>Code implementing SNPE based on Theano, is available at <ext-link ext-link-type="uri" xlink:href="http://www.mackelab.org/delfi/">http://www.mackelab.org/delfi/</ext-link>. An extended toolbox based on PyTorch is available at <ext-link ext-link-type="uri" xlink:href="http://www.mackelab.org/sbi/">http://www.mackelab.org/sbi/</ext-link> (<xref ref-type="bibr" rid="bib133">Tejero-Cantero et al., 2020</xref>).</p></sec><sec id="s4-2"><title>Simulation-based inference</title><p>To perform Bayesian parameter identification with SNPE, three types of input need to be specified:</p><list list-type="order"><list-item><p>A mechanistic model. The model only needs to be specified through a simulator, that is that one can generate a simulation result <inline-formula><mml:math id="inf47"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> for any parameters <inline-formula><mml:math id="inf48"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula>. We do not assume access to the likelihood <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> or the equations or internals of the code defining the model, nor do we require the model to be differentiable. This is in contrast to many alternative approaches (including <xref ref-type="bibr" rid="bib12">Bittner et al., 2019</xref>), which require the model to be differentiable and to be implemented in a software code that is amenable to automatic differentiation packages. Finally, SNPE can both deal with inputs <inline-formula><mml:math id="inf50"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> which resemble ‘raw’ outputs of the model, or summary features calculated from data.</p></list-item><list-item><p>Observed data <inline-formula><mml:math id="inf51"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> of the same form as the results <inline-formula><mml:math id="inf52"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> produced by model simulations.</p></list-item><list-item><p>A prior distribution <inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> describing the range of possible parameters. <inline-formula><mml:math id="inf54"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> could consist of upper and lower bounds for each parameter, or a more complex distribution incorporating mechanistic first principles or knowledge gained from previous inference procedures on other data. In our applications, we chose priors deemed reasonable or informed by previous studies (see Materials and methods), although setting such priors is an open problem in itself, and outside of the scope of this study.</p></list-item></list><p>For each problem, our goal was to estimate the posterior distribution <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To do this, we used SNPE (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>). Setting up the inference procedure required three design choices:</p><list list-type="order"><list-item><p>A network architecture, including number of layers, units per layer, layer type (feedforward or convolutional), activation function and skip connections.</p></list-item><list-item><p>A parametric family of probability densities <inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi>ψ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to represent inferred posteriors, to be used as conditional density estimator. We used either a mixture of Gaussians (MoG) or a masked autoregressive flow (MAF) (<xref ref-type="bibr" rid="bib95">Papamakarios et al., 2017</xref>). In the former case, the number of components <italic>K</italic> must be specified; in the latter the number of <italic>MADES</italic> (Masked Autoencoder for Distribution Estimation) <inline-formula><mml:math id="inf57"><mml:msub><mml:mi>n</mml:mi><mml:mtext>MADES</mml:mtext></mml:msub></mml:math></inline-formula>. Both choices are able to represent richly structured, and multimodal posterior distributions (more details on neural density estimation below).</p></list-item><list-item><p>A simulation budget, that is, number of rounds <italic>R</italic> and simulations per round <inline-formula><mml:math id="inf58"><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula>. The required number of simulations depends on both the dimensionality and complexity of the function between summary statistics and model parameters. While the number of parameters and summary-features can easily be determined, it can be hard to determine how ‘complex’ (or nonlinear) this mapping is. This makes it difficult to give general guidelines on how many simulations will be required. A practical approach is to choose a simulation-budget based on the computational cost of the simulation, inspect the results (e.g. with posterior predictive checks), and add more simulations when it seems necessary.</p></list-item></list><p>We emphasize that SNPE is highly modular, that is, that the the inputs (data, the prior over parameter, the mechanistic model), and algorithmic components (network architecture, probability density, optimization approach) can all be modified and chosen independently. This allows neuroscientists to work with models which are designed with mechanistic principles—and not convenience of inference—in mind. Furthermore, it allows SNPE to benefit from advances in more flexible density estimators, more powerful network architectures, or optimization strategies.</p><p>With the problem and inference settings specified, SNPE adjusts the network weights <inline-formula><mml:math id="inf59"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> based on simulation results, so that <inline-formula><mml:math id="inf60"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for any <inline-formula><mml:math id="inf61"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula>. In the first round of SNPE, simulation parameters are drawn from the prior <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If a single round of inference is not sufficient, SNPE can be run in multiple rounds, in which samples are drawn from the version of <inline-formula><mml:math id="inf63"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at the beginning of the round. After the last round, <inline-formula><mml:math id="inf64"><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> is returned as the inferred posterior on parameters <inline-formula><mml:math id="inf65"><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:math></inline-formula> given observed data <inline-formula><mml:math id="inf66"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>. If SNPE is only run for a single round, then the generated samples only depend on the prior, but not on <inline-formula><mml:math id="inf67"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>: in this case, the inference network is applicable to any data (covered by the prior ranges), and can be used for rapid amortized inference.</p><p>SNPE learns the correct network weights <inline-formula><mml:math id="inf68"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> by minimizing the objective function <inline-formula><mml:math id="inf69"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where the simulation with parameters <inline-formula><mml:math id="inf70"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> produced result <inline-formula><mml:math id="inf71"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>. For the first round of SNPE <inline-formula><mml:math id="inf72"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, while in subsequent rounds a different loss function accounts for the fact that simulation parameters were not sampled from the prior. Different choices of the loss function for later rounds result in SNPE-A (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>), SNPE-B (<xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>) or SNPE-C algorithm (<xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>). To optimize the networks, we used ADAM with default settings (<xref ref-type="bibr" rid="bib60">Kingma and Ba, 2014</xref>).</p><p>The details of the algorithm are below:</p><p><table-wrap id="inlinetable1" position="anchor"><table frame="hsides" rules="groups"><thead><tr><th>Algorithm 1: SNPE</th></tr></thead><tbody><tr><td>  <bold>Input</bold>: simulator with (implicit) density <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, observed data <inline-formula><mml:math id="inf74"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>, prior <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, density family <inline-formula><mml:math id="inf76"><mml:msub><mml:mi>q</mml:mi><mml:mi>ψ</mml:mi></mml:msub></mml:math></inline-formula>, <break/>  neural network <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, number of rounds , simulation count for each round <inline-formula><mml:math id="inf78"><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:math></inline-formula> <break/>  randomly initialize <inline-formula><mml:math id="inf79"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> <break/>  <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>  <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>N</mml:mi><mml:mo>:=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> <break/>  <bold>for</bold> <inline-formula><mml:math id="inf82"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> <bold>to <italic>R</italic> do</bold> <break/>    <bold>for</bold> <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> <bold>do</bold> <break/>      sample <inline-formula><mml:math id="inf84"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>r</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>      simulate <inline-formula><mml:math id="inf85"><mml:mrow><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>    <inline-formula><mml:math id="inf86"><mml:mrow><mml:mi>N</mml:mi><mml:mo>←</mml:mo><mml:mrow><mml:mi>N</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>r</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>    train <inline-formula><mml:math id="inf87"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>←</mml:mo><mml:mrow><mml:munder><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>arg</mml:mi></mml:mpadded><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>min</mml:mi></mml:mrow><mml:mi>ϕ</mml:mi></mml:munder><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>N</mml:mi></mml:munderover></mml:mstyle><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>    <inline-formula><mml:math id="inf88"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>r</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> <break/>  <bold>return</bold> <inline-formula><mml:math id="inf89"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap></p></sec><sec id="s4-3"><title>Bayesian inference without likelihood-evaluations with SNPE</title><p>In <xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>, it was shown that the procedure described above (i.e. sample from the prior, train a flexible density estimator by minimizing the log-loss <inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℒ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) can be used to perform Bayesian inference without likelihood evaluations.</p><p>For the multi-round case, in which samples are no longer drawn from the prior, but adaptively generated from a (generally more focused) proposal distribution, the loss function needs to be modified. Different variants of SNPE differ in how exactly this is done:</p><list list-type="bullet"><list-item><p>SNPE-A minimizes the same loss function as in the first round, but applies a post-hoc analytical correction (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>)</p></list-item><list-item><p>SNPE-B minimizes an importance-weighted loss function, directly approximating the posterior and therefore not requiring a post-hoc correction (<xref ref-type="bibr" rid="bib71">Lueckmann et al., 2017</xref>)</p></list-item><list-item><p>SNPE-C avoids importance weights (which can have high variance), by either calculating normalization constants in closed-form or using a classifier-based loss (<xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>)</p></list-item></list></sec><sec id="s4-4"><title>Neural density estimation</title><p>As described above, SNPE approximates the posterior distribution with flexible neural density estimators: either a mixture density network (MDN) or a masked autoregressive flow (MAF). Below, we provide a few more details about these density estimators, how we chose their respective architectures, and when to choose one or the other.</p><p>The MDN outputs the parameters of a mixture of Gaussians (i.e. mixture weights, and for each component of the mixture, the mean vector and covariance entries). Thus, for an MDN composed of <italic>K</italic> components, we chose an architecture with at least as many units per layer as <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>K</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>θ</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the number of parameters to infer, to ensure enough flexibility to approximate well the parameters of the mixture of Gaussians. For example, when inferring the parameters of the Hodgkin-Huxley model given in vitro recordings from mouse cortex (Allen Cell Types Database, <ext-link ext-link-type="uri" xlink:href="https://celltypes.brain-map.org/data">https://celltypes.brain-map.org/data</ext-link>), we infer the posterior over eight parameters with a mixture of two Gaussians, and the MDN needs at least 89 units per layer. Across applications, we found two layers to be sufficient to appropriately approximate the posterior distribution.</p><p>MAF is a specific type of normalizing flow, which is a highly flexible density estimator (<xref ref-type="bibr" rid="bib114">Rezende and Mohamed, 2015</xref>; <xref ref-type="bibr" rid="bib95">Papamakarios et al., 2017</xref>; <xref ref-type="bibr" rid="bib97">Papamakarios et al., 2019b</xref>). Normalizing flows consist of a stack of bijections which transform a simple distribution (usually a multivariate Gaussian distribution) into the target distribution. Each bijection is parameterized by a specific type of neural network (for MAF: a Masked Autoencoder for Distribution Estimation, or MADE). In our experiments, five stacked bijections are enough to approximate even complex posterior distributions. Depending on the size of the parameter and data space, each neural network had between [50,50] and [100,100,100] hidden units.</p><p>When using SNPE in a single-round, we generally found superior performance for MAFs as compared to MDNs. When running inference across multiple rounds, training MAFs leads to additional challenges which might impede the quality of inference (<xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>; <xref ref-type="bibr" rid="bib34">Durkan et al., 2020</xref>).</p></sec><sec id="s4-5"><title>Linear-nonlinear encoding models</title><p>We used a Linear-Nonlinear (LN) encoding model (a special case of a generalized linear model, GLM, [<xref ref-type="bibr" rid="bib18">Brown et al., 1998</xref>; <xref ref-type="bibr" rid="bib94">Paninski, 2004</xref>; <xref ref-type="bibr" rid="bib135">Truccolo et al., 2005</xref>; <xref ref-type="bibr" rid="bib100">Pillow, 2007</xref>; <xref ref-type="bibr" rid="bib101">Pillow et al., 2008</xref>; <xref ref-type="bibr" rid="bib39">Gerwinn et al., 2010</xref>]) to simulate the activity of a neuron in response to a univariate time-varying stimulus. Neural activity <inline-formula><mml:math id="inf93"><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> was subdivided in <inline-formula><mml:math id="inf94"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> bins and, within each bin <italic>i</italic>, spikes were generated according to a Bernoulli observation model,<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>Bern</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mi>i</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">𝒇</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf95"><mml:msub><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is a vector of white noise inputs between time bins <inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mn>8</mml:mn></mml:mrow></mml:math></inline-formula> and <italic>i</italic>, <inline-formula><mml:math id="inf97"><mml:mi mathvariant="bold-italic">𝒇</mml:mi></mml:math></inline-formula> a length-9 linear filter, β is the bias, and <inline-formula><mml:math id="inf98"><mml:mrow><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the canonical inverse link function for a Bernoulli GLM. As summary features, we used the total number of spikes <italic>N</italic> and the spike-triggered average <inline-formula><mml:math id="inf99"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐕𝐳</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf100"><mml:mrow><mml:mi mathvariant="bold">𝐕</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the so-called design matrix of size <inline-formula><mml:math id="inf101"><mml:mrow><mml:mn>9</mml:mn><mml:mo>×</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>. We note that the spike-triggered sum <inline-formula><mml:math id="inf102"><mml:mi mathvariant="bold">𝐕𝐳</mml:mi></mml:math></inline-formula> constitutes sufficient statistics for this GLM, that is that selecting the STA and <italic>N</italic> together as summary features does not lead to loss of model relevant information over the full input-output dataset <inline-formula><mml:math id="inf103"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi mathvariant="bold">𝐕</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐳</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. We used a Gaussian prior with zero mean and covariance matrix <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf105"><mml:mi mathvariant="bold">𝐅</mml:mi></mml:math></inline-formula> encourages smoothness by penalizing the second-order differences in the vector of parameters (<xref ref-type="bibr" rid="bib29">De Nicolao et al., 1997</xref>).</p><p>For inference, we used a single round of 10,000 simulations, and the posterior was approximated with a Gaussian distribution (<inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>10</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>10</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). We used a feedforward neural network with two hidden layers of 50 units each. We used a Polya Gamma Markov Chain Monte Carlo sampling scheme (<xref ref-type="bibr" rid="bib104">Polson et al., 2013</xref>) to estimate a reference posterior.</p><p>In <xref ref-type="fig" rid="fig2">Figure 2d</xref>, we compare the performance of SNPE with two classical ABC algorithms, rejection ABC and Sequential Monte Carlo ABC as a function of the number of simulations. We report the relative error in Kullback-Leibler divergence, which is defined as:<disp-formula id="equ2"><label>(1)</label><mml:math id="m2"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo rspace="7.5pt" stretchy="false">|</mml:mo><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mtext>KL</mml:mtext></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo rspace="7.5pt" stretchy="false">|</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and which ranges between 0 (perfect recovery of the posterior) and 1 (estimated posterior no better than the prior). Here, <inline-formula><mml:math id="inf107"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the ground-truth posterior estimated via Markov Chain Monte Carlo sampling, <inline-formula><mml:math id="inf108"><mml:mrow><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the estimated posterior via SNPE, rejection ABC or Sequential Monte Carlo ABC, and <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the prior.</p><p>For the spatial receptive field model of a cell in primary visual cortex, we simulated the activity of a neuron depending on an image-valued stimulus. Neural activity was subdivided in bins of length <inline-formula><mml:math id="inf110"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.025</mml:mn><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and within each bin <italic>i</italic>, spikes were generated according to a Poisson observation model, <disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>Poiss</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mi>i</mml:mi><mml:mo>⊤</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">𝒉</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf111"><mml:msub><mml:mi mathvariant="bold">𝐯</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the vectorized white noise stimulus at time bin <italic>i</italic>, <inline-formula><mml:math id="inf112"><mml:mi mathvariant="bold-italic">𝒉</mml:mi></mml:math></inline-formula> a 41 × 41 linear filter, β is the bias, and <inline-formula><mml:math id="inf113"><mml:mrow><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the canonical inverse link function for a Poisson GLM. The receptive field <inline-formula><mml:math id="inf114"><mml:mi mathvariant="bold-italic">𝒉</mml:mi></mml:math></inline-formula> is constrained to be a Gabor filter:<disp-formula id="equ4"><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>′</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:msup><mml:mi>y</mml:mi><mml:mrow><mml:mo>′</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>−</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>x</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ψ</mml:mi><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ψ</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>y</mml:mi><mml:mo>′</mml:mo></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ψ</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mi>ψ</mml:mi></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>σ</mml:mi></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>w</mml:mi></mml:msup><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mi>w</mml:mi></mml:msup><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf115"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>x</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> is a regular grid of 41 × 41 positions spanning the 2D image-valued stimulus. The parameters of the Gabor are gain <italic>g</italic>, spatial frequency <italic>f</italic>, aspect-ratio <italic>r</italic>, width <italic>w</italic>, phase <inline-formula><mml:math id="inf116"><mml:mi>ϕ</mml:mi></mml:math></inline-formula> (between 0 and π), angle <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ψ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (between 0 and <inline-formula><mml:math id="inf118"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:math></inline-formula>) and location <inline-formula><mml:math id="inf119"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula> (assumed within the stimulated area, scaled to be between −1 and 1). Bounded parameters were transformed with a log-, or logit-transform, to yield unconstrained parameters. After applying SNPE, we back-transformed both the parameters and the estimated posteriors in closed form, as shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. We did not transform the bias β.</p><p>We used a factorizing Gaussian prior for the vector of transformed Gabor parameters<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mrow><mml:mo rspace="7.5pt" stretchy="false">[</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>g</mml:mi></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>r</mml:mi></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ψ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo rspace="7.5pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where transforms <inline-formula><mml:math id="inf120"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf121"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>π</mml:mi><mml:mo>-</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf122"><mml:mrow><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> ensured the assumed ranges for the Gabor parameters <inline-formula><mml:math id="inf123"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo>,</mml:mo><mml:mi>ψ</mml:mi><mml:mo>,</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:math></inline-formula>. Our Gaussian prior had zero mean and standard deviations <inline-formula><mml:math id="inf124"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1.9</mml:mn><mml:mo>,</mml:mo><mml:mn>1.78</mml:mn><mml:mo>,</mml:mo><mml:mn>1.78</mml:mn><mml:mo>,</mml:mo><mml:mn>1.78</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>. We note that a Gaussian prior on a logit-transformed random variable <inline-formula><mml:math id="inf125"><mml:mrow><mml:mtext>logit</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:math></inline-formula> with zero mean and standard deviation around 1.78 is close to a uniform prior over the original variable <italic>X</italic>. For the bias β, we used a Gaussian prior with mean −0.57 and variance 1.63, which approximately corresponds to an exponential prior <inline-formula><mml:math id="inf126"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with rate <inline-formula><mml:math id="inf127"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> on the baseline firing rate <inline-formula><mml:math id="inf128"><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in absence of any stimulus.</p><p>The ground-truth parameters for the demonstration in <xref ref-type="fig" rid="fig2">Figure 2</xref> were chosen to give an asymptotic firing rate of 1 Hz for 5 min stimulation, resulting in 299 spikes, and a signal-to-noise ratio of −12dB.</p><p>As summary features, we used the total number of spikes <italic>N</italic> and the spike-triggered average <inline-formula><mml:math id="inf129"><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">𝐕𝐳</mml:mi></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf130"><mml:mrow><mml:mi mathvariant="bold">𝐕</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>v</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the stimulation video of length <inline-formula><mml:math id="inf131"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>300</mml:mn><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>12000</mml:mn></mml:mrow></mml:math></inline-formula>. As for the GLM with a temporal filter, the spike-triggered sum <inline-formula><mml:math id="inf132"><mml:mi mathvariant="bold">𝐕𝐳</mml:mi></mml:math></inline-formula> constitutes sufficient statistics for this GLM.</p><p>For inference, we applied SNPE-A with in total two rounds: an initial round serves to first roughly identify the relevant region of parameter space. Here we used a Gaussian distribution to approximate the posterior from 100,000 simulations. A second round then used a mixture of eight Gaussian components to estimate the exact shape of the posterior from another 100,000 simulations (<inline-formula><mml:math id="inf133"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>9</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>1682</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). We used a convolutional network with five convolutional layers with 16 to 32 convolutional filters followed by two fully connected layers with 50 units each. The total number of spikes <italic>N</italic> within a simulated experiment was passed as an additional input directly to the fully-connected layers of the network. Similar to the previous GLM, this model has a tractable likelihood, so we use MCMC to obtain a reference posterior.</p><p>We applied this approach to extracelullar recordings from primary visual cortex of alert mice obtained using silicon microelectrodes in response to colored-noise visual stimulation. Experimental methods are described in <xref ref-type="bibr" rid="bib35">Dyballa et al., 2018</xref>.</p></sec><sec id="s4-6"><title>Comparison with Sequential Monte Carlo (SMC) ABC</title><p>In order to illustrate the competitive performance of SNPE, we obtained a posterior estimate with a classical ABC method, Sequential Monte Carlo (SMC) ABC (<xref ref-type="bibr" rid="bib122">Sisson et al., 2007</xref>; <xref ref-type="bibr" rid="bib9">Beaumont et al., 2009</xref>). Likelihood-free inference methods from the ABC family require a distance function <inline-formula><mml:math id="inf134"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> between observed data <inline-formula><mml:math id="inf135"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula> and possible simulation outputs <inline-formula><mml:math id="inf136"><mml:mi mathvariant="bold">𝐱</mml:mi></mml:math></inline-formula> to characterize dissimilarity between simulations and data. A common choice is the (scaled) Euclidean distance <inline-formula><mml:math id="inf137"><mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>. The Euclidean distance here was computed over 1681 summary features given by the spike-triggered average (one per pixel) and a single summary feature given by the ‘spike count’. To ensure that the distance measure was sensitive to differences in both STA and spike count, we scaled the summary feature ‘spike count’ to account for about 20% of the average total distance (other values did not yield better results). The other 80% were computed from the remaining 1681 summary features given by spike-triggered averages.</p><p>To showcase how this situation is challenging for ABC approaches, we generated 10,000 input-output pairs <inline-formula><mml:math id="inf138"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> with the prior and simulator used above, and illustrate the 10 STAs and spike counts with closest <inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5a</xref>. Spike counts were comparable to the observed data (299 spikes), but STAs were noise-dominated and the 10 ‘closest’ underlying receptive fields (orange contours) showed substantial variability in location and shape of the receptive field. If even the ‘closest’ samples do not show any visible receptive field, then there is little hope that even an appropriately chosen acceptance threshold will yield a good approximation to the posterior. These findings were also reflected in the results from SMC-ABC with a total simulation budget of 10<sup>6</sup> simulations (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5b</xref>). The estimated posterior marginals for ‘bias’ and ‘gain’ parameters show that the parameters related to the firing rate were constrained by the data <inline-formula><mml:math id="inf140"><mml:msub><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:math></inline-formula>, but marginals of parameters related to shape and location of the receptive field did not differ from the prior, highlighting that SMC-ABC was not able to identify the posterior distribution. The low correlations between the ground-truth receptive field and receptive fields sampled from SMC-ABC posterior further highlight the failure of SMC-ABC to infer the ground-truth posterior (<xref ref-type="fig" rid="app1fig5">Appendix 1—figure 5c</xref>). Further comparisons of neural-density estimation approaches with ABC-methods can be found in the studies describing the underlying machine-learning methodologies (<xref ref-type="bibr" rid="bib98">Papamakarios and Murray, 2016</xref>; <xref ref-type="bibr" rid="bib72">Lueckmann et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Greenberg et al., 2019</xref>).</p></sec><sec id="s4-7"><title>Ion channel models</title><p>We simulated non-inactivating potassium channel currents subject to voltage-clamp protocols as:<disp-formula id="equ6"><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>I</mml:mi><mml:mi>K</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mtext>K</mml:mtext></mml:msub><mml:mi>m</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mtext>K</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>V</italic> is the membrane potential, <inline-formula><mml:math id="inf141"><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>K</mml:mtext></mml:msub></mml:math></inline-formula> is the density of potassium channels, <inline-formula><mml:math id="inf142"><mml:msub><mml:mi>E</mml:mi><mml:mtext>K</mml:mtext></mml:msub></mml:math></inline-formula> is the reversal potential of potassium, and <italic>m</italic> is the gating variable for potassium channel activation. <italic>m</italic> is modeled according to the first-order kinetic equation<disp-formula id="equ7"><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf143"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the steady-state activation, and <inline-formula><mml:math id="inf144"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the respective time constant. We used a general formulation of <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib30">Destexhe and Huguenard, 2000</xref>), where the steady-state activation curve has two parameters (slope and offset) and the time constant curve has six parameters, amounting to a total of 8 parameters (<inline-formula><mml:math id="inf147"><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> to <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>θ</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:math></inline-formula>): <disp-formula id="equ8"><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>θ</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Since this model can be used to describe the dynamics of a wide variety of channel models, we refer to it as <italic>Omnimodel</italic>.</p><p>We modeled responses of the Omnimodel to a set of five noisy voltage-clamp protocols (<xref ref-type="bibr" rid="bib103">Podlaski et al., 2017</xref>): as described in <xref ref-type="bibr" rid="bib103">Podlaski et al., 2017</xref>, the original voltage-clamp protocols correspond to standard protocols of activation, inactivation, deactivation, ramp and action potential, to which we added Gaussian noise with zero mean and standard deviation 0.5 mV. Current responses were reduced to 55 summary features (11 per protocol). Summary features were coefficients to basis functions derived via Principal Components Analysis (PCA) (10 per protocol) plus a linear offset (one per protocol) found via least-squares fitting. PCA basis functions were found by simulating responses of the non-inactivating potassium channel models to the five voltage-clamp protocols and reducing responses to each protocol to 10 dimensions (explaining 99.9% of the variance).</p><p>To amortize inference on the model, we specified a wide uniform prior over the parameters: <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>10.</mml:mn><mml:mo>,</mml:mo><mml:mn>10.</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>120.</mml:mn><mml:mo>,</mml:mo><mml:mn>120.</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.</mml:mn><mml:mo>,</mml:mo><mml:mn>2000</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.05</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒰</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.05</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For inference, we trained a shared inference network in a single round of 10<sup>6</sup> simulations generated by sampling from the prior (<inline-formula><mml:math id="inf153"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>8</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>55</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). The density estimator was a masked autoregressive flow (MAF) (<xref ref-type="bibr" rid="bib95">Papamakarios et al., 2017</xref>) with five MADES with [250,250] hidden units each.</p><p>We evaluated performance on 350 non-inactivating potassium ion channels selected from IonChannelGenealogy (ICG) by calculating the correlation coefficient between traces generated by the original model and traces from the Omnimodel using the posterior mode (<xref ref-type="fig" rid="app1fig7">Appendix 1—figure 7</xref>).</p></sec><sec id="s4-8"><title>Single-compartment Hodgkin–Huxley neurons</title><p>We simulated a single-compartment Hodgkin–Huxley type neuron with channel kinetics as in <xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>,<disp-formula id="equ9"><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mtext>l</mml:mtext></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>l</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>Na</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mi>m</mml:mi><mml:mn>3</mml:mn></mml:msup><mml:mi>h</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>Na</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mtext>K</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mtext>K</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mtext>M</mml:mtext></mml:msub><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mtext>K</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mtext>inj</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>σ</mml:mi><mml:mi>η</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>q</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>m</mml:mi><mml:mo>,</mml:mo><mml:mi>h</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>p</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>V</italic> is the membrane potential, <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>C</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is the membrane capacitance, <inline-formula><mml:math id="inf155"><mml:msub><mml:mi>g</mml:mi><mml:mtext>l</mml:mtext></mml:msub></mml:math></inline-formula> is the leak conductance, <inline-formula><mml:math id="inf156"><mml:msub><mml:mi>E</mml:mi><mml:mtext>l</mml:mtext></mml:msub></mml:math></inline-formula> is the membrane reversal potential, <inline-formula><mml:math id="inf157"><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the density of channels of type <italic>c</italic> (Na<sup>+</sup>, K<sup>+</sup>, M), <inline-formula><mml:math id="inf158"><mml:msub><mml:mi>E</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the reversal potential of <italic>c</italic>, (<italic>m</italic>, <italic>h</italic>, <italic>n</italic>, <italic>p</italic>) are the respective channel gating kinetic variables, and <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the intrinsic neural Gaussian noise. The right hand side of the voltage dynamics is composed of a leak current, a voltage-dependent Na<sup>+</sup> current, a delayed-rectifier K<sup>+</sup> current, a slow voltage-dependent K<sup>+</sup> current responsible for spike-frequency adaptation, and an injected current <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>I</mml:mi><mml:mtext>inj</mml:mtext></mml:msub></mml:math></inline-formula>. Channel gating variables <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> have dynamics fully characterized by the neuron membrane potential <italic>V</italic>, given the respective steady-state <inline-formula><mml:math id="inf162"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and time constant <inline-formula><mml:math id="inf163"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>q</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (details in <xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>). Two additional parameters are implicit in the functions <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>q</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>: <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> adjusts the spike threshold through <inline-formula><mml:math id="inf167"><mml:msub><mml:mi>m</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf168"><mml:msub><mml:mi>h</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf169"><mml:msub><mml:mi>n</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf170"><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf171"><mml:msub><mml:mi>τ</mml:mi><mml:mi>h</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>; <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> scales the time constant of adaptation through <inline-formula><mml:math id="inf174"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (details in <xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>). We set <inline-formula><mml:math id="inf175"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mtext>Na</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>53</mml:mn></mml:mrow></mml:math></inline-formula> mV and <inline-formula><mml:math id="inf176"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mtext>K</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>107</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mV, similar to the values used for simulations in Allen Cell Types Database (<ext-link ext-link-type="uri" xlink:href="http://help.brain-map.org/download/attachments/8323525/BiophysModelPeri.pdf">http://help.brain-map.org/download/attachments/8323525/BiophysModelPeri.pdf</ext-link>).</p><p>We applied SNPE to infer the posterior over eight parameters (<inline-formula><mml:math id="inf177"><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>Na</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf178"><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>K</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>g</mml:mi><mml:mtext>l</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf180"><mml:msub><mml:mover accent="true"><mml:mi>g</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mtext>M</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>τ</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf182"><mml:msub><mml:mi>V</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula>, σ, <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>E</mml:mi><mml:mtext>l</mml:mtext></mml:msub></mml:math></inline-formula>), given seven voltage features (number of spikes, mean resting potential, standard deviation of the resting potential, and the first four voltage moments, mean, standard deviation, skewness and kurtosis).</p><p>The prior distribution over the parameters was uniform,<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">𝒰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext>low</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mtext>high</mml:mtext></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf184"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>low</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>40</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mn>35</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf185"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>high</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>80</mml:mn><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>3000</mml:mn><mml:mo>,</mml:mo><mml:mn>90</mml:mn><mml:mo>,</mml:mo><mml:mn>0.15</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. These ranges are similar to the ones obtained in <xref ref-type="bibr" rid="bib105">Pospischil et al., 2008</xref>, when fitting the above model to a set of electrophysiological recordings.</p><p>For inference in simulated data, we used a single round of 100,000 simulations (<inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). The density estimator was a masked autoregressive flow (MAF) (<xref ref-type="bibr" rid="bib95">Papamakarios et al., 2017</xref>) with five MADES with [50,50] hidden units each.</p><p>For the inference on in vitro recordings from mouse cortex (Allen Cell Types Database, <ext-link ext-link-type="uri" xlink:href="https://celltypes.brain-map.org/data">https://celltypes.brain-map.org/data</ext-link>), we selected eight recordings corresponding to spiny neurons with at least 10 spikes during the current-clamp stimulation. The respective cell identities and sweeps are: (518290966,57), (509881736,39), (566517779,46), (567399060,38), (569469018,44), (532571720,42), (555060623,34), (534524026,29). For each recording, SNPE-B was run for two rounds with 125,000 Hodgkin–Huxley simulations each, and the posterior was approximated by a mixture of two Gaussians. In this case, the density estimator was composed of two fully connected layers of 100 units each.</p></sec><sec id="s4-9"><title>Comparison with genetic algorithm</title><p>We compared SNPE posterior with a state-of-the-art genetic algorithm (Indicator Based Evolutionary Algorithm IBEA, [<xref ref-type="bibr" rid="bib13">Bleuler et al., 2003</xref>; <xref ref-type="bibr" rid="bib144">Zitzler and Künzli, 2004</xref>] from the BluePyOpt package [<xref ref-type="bibr" rid="bib136">Van Geit et al., 2016</xref>]), in the context of the Hodgkin-Huxley model with 8 parameters and seven features (<xref ref-type="fig" rid="app1fig10">Appendix 1—figure 10</xref>). For each Hodgkin-Huxley model simulation <italic>i</italic> and summary feature <italic>j</italic>, we used the following objective score:<disp-formula id="equ11"><mml:math id="m11"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mfrac><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="7.5pt">,</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf187"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the value of summary feature <italic>j</italic> for simulation <italic>i</italic>, <inline-formula><mml:math id="inf188"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the observed summary feature <italic>j</italic>, and <inline-formula><mml:math id="inf189"><mml:msub><mml:mi>σ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is the standard deviation of the summary feature <italic>j</italic> computed across 1000 previously simulated datasets. IBEA outputs the hall-of-fame, which corresponds to the 10 parameter sets with the lowest sum of objectives <inline-formula><mml:math id="inf190"><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi><mml:mn>7</mml:mn></mml:msubsup><mml:msub><mml:mi>ϵ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. We ran IBEA with 100 generations and an offspring size of 1000 individuals, corresponding to a total of 100,000 simulations.</p></sec><sec id="s4-10"><title>Circuit model of the crustacean stomatogastric ganglion</title><p>We used extracellular nerve recordings made from the stomatogastric motor neurons that principally comprise the triphasic pyloric rhythm in the crab <italic>Cancer borealis</italic> (<xref ref-type="bibr" rid="bib49">Haddad and Marder, 2018</xref>). The preparations were decentralized, that is, the axons of the descending modulatory inputs were severed. The data was recorded at a temperature of 11°C. See <xref ref-type="bibr" rid="bib49">Haddad and Marder, 2018</xref> for full experimental details.</p><p>We simulated the circuit model of the crustacean stomatogastric ganglion by adapting a model described in <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>. The model is composed of three single-compartment neurons, AB/PD, LP, and PD, where the electrically coupled AB and PD neurons are modeled as a single neuron. Each of the model neurons contains eight currents, a Na<sup>+</sup> current <inline-formula><mml:math id="inf191"><mml:msub><mml:mi>I</mml:mi><mml:mtext>Na</mml:mtext></mml:msub></mml:math></inline-formula>, a fast and a slow transient Ca<sup>2+</sup> current <inline-formula><mml:math id="inf192"><mml:msub><mml:mi>I</mml:mi><mml:mtext>CaT</mml:mtext></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf193"><mml:msub><mml:mi>I</mml:mi><mml:mtext>CaS</mml:mtext></mml:msub></mml:math></inline-formula>, a transient K<sup>+</sup> current <inline-formula><mml:math id="inf194"><mml:msub><mml:mi>I</mml:mi><mml:mtext>A</mml:mtext></mml:msub></mml:math></inline-formula>, a Ca<sup>2+</sup>-dependent K<sup>+</sup> current <inline-formula><mml:math id="inf195"><mml:msub><mml:mi>I</mml:mi><mml:mtext>KCa</mml:mtext></mml:msub></mml:math></inline-formula>, a delayed rectifier K<sup>+</sup> current <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>Kd</mml:mtext></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, a hyperpolarization-activated inward current <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>H</mml:mtext></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a leak current <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>leak</mml:mtext></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In addition, the model contains seven synapses. As in <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>, these synapses were simulated using a standard model of synaptic dynamics (<xref ref-type="bibr" rid="bib1">Abbott and Marder, 1998</xref>). The synaptic input current into the neurons is given by <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mi>s</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mtext>s</mml:mtext></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the maximal synapse conductance, <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mtext>post</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the membrane potential of the postsynaptic neuron, and <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mtext>s</mml:mtext></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the reversal potential of the synapse. The evolution of the activation variable <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by<disp-formula id="equ12"><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mtext>pre</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula>with<disp-formula id="equ13"><mml:math id="m13"><mml:mrow><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mtext>pre</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+8.4pt"><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:mi>δ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mpadded><mml:mo>⁢</mml:mo><mml:mpadded width="+8.4pt"><mml:mtext>and</mml:mtext></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>s</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mtext>pre</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf204"><mml:msub><mml:mi>V</mml:mi><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula> is the membrane potential of the presynaptic neuron, <inline-formula><mml:math id="inf205"><mml:msub><mml:mi>V</mml:mi><mml:mtext>th</mml:mtext></mml:msub></mml:math></inline-formula> is the half-activation voltage of the synapse, δ sets the slope of the activation curve, and <inline-formula><mml:math id="inf206"><mml:msub><mml:mi>k</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula> is the rate constant for transmitter-receptor dissociation rate.</p><p>As in <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>, two types of synapses were modeled since AB, LP, and PY are glutamatergic neurons whereas PD is cholinergic. We set <inline-formula><mml:math id="inf207"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mV and <inline-formula><mml:math id="inf208"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>40</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms for all glutamatergic synapses and <inline-formula><mml:math id="inf209"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>80</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mV and <inline-formula><mml:math id="inf210"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> ms for all cholinergic synapses. For both synapse types, we set <inline-formula><mml:math id="inf211"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mtext>th</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>35</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> mV and <inline-formula><mml:math id="inf212"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> mV.</p><p>For each set of membrane and synaptic conductances, we numerically simulated the rhythm for 10 s with a step size of 0.025 ms. At each time step, each neuron received Gaussian noise with mean zero and standard deviation 0.001 mV.ms<sup>−0.5</sup>.</p><p>We applied SNPE to infer the posterior over 24 membrane parameters and 7 synaptic parameters, that is, 31 parameters in total. The seven synaptic parameters were the maximal conductances <inline-formula><mml:math id="inf213"><mml:msub><mml:mi>g</mml:mi><mml:mtext>s</mml:mtext></mml:msub></mml:math></inline-formula> of all synapses in the circuit, each of which was varied uniformly in logarithmic domain from <inline-formula><mml:math id="inf214"><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>⁢</mml:mo> <mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf215"><mml:mrow><mml:mn>1000</mml:mn><mml:mo>⁢</mml:mo> <mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula>, with the exception of the synapse from AB to LP, which was varied uniformly in logarithmic domain from <inline-formula><mml:math id="inf216"><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>⁢</mml:mo> <mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula> to <inline-formula><mml:math id="inf217"><mml:mrow><mml:mn>10000</mml:mn><mml:mo>⁢</mml:mo> <mml:mtext>nS</mml:mtext></mml:mrow></mml:math></inline-formula>. The membrane parameters were the maximal membrane conductances for each of the neurons. The membrane conductances were varied over an extended range of previously reported values (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>), which led us to the uniform prior bounds <inline-formula><mml:math id="inf218"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>low</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>25</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>mS cm</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf219"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>high</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>500</mml:mn><mml:mo>,</mml:mo><mml:mn>7.5</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>60</mml:mn><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>150</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>mS cm</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the maximal membrane conductances of the AB neuron, <inline-formula><mml:math id="inf220"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>low</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>mS cm</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf221"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>high</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>200</mml:mn><mml:mo>,</mml:mo><mml:mn>2.5</mml:mn><mml:mo>,</mml:mo><mml:mn>12</mml:mn><mml:mo>,</mml:mo><mml:mn>60</mml:mn><mml:mo>,</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>125</mml:mn><mml:mo>,</mml:mo><mml:mn>0.06</mml:mn><mml:mo>,</mml:mo><mml:mn>0.04</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>mS cm</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the maximal membrane conductances of the LP neuron, and <inline-formula><mml:math id="inf222"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>low</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>50</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>mS cm</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf223"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mtext>high</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>600</mml:mn><mml:mo>,</mml:mo><mml:mn>12.5</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mn>60</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>150</mml:mn><mml:mo>,</mml:mo><mml:mn>0.06</mml:mn><mml:mo>,</mml:mo><mml:mn>0.04</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup> <mml:mtext>mS cm</mml:mtext><mml:mrow><mml:mo>-</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for the maximal membrane conductances of the PY neuron. The order of the membrane currents was: [Na, CaT, CaS, A, KCa, Kd, H, leak].</p><p>We used the 15 summary features proposed by <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>, and extended them by three additional features. The features proposed by <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref> are 15 salient features of the pyloric rhythm, namely: cycle period <italic>T</italic> (s), AB/PD burst duration <inline-formula><mml:math id="inf224"><mml:msubsup><mml:mi>d</mml:mi><mml:mtext>AB</mml:mtext><mml:mtext>b</mml:mtext></mml:msubsup></mml:math></inline-formula> (s), LP burst duration <inline-formula><mml:math id="inf225"><mml:msubsup><mml:mi>d</mml:mi><mml:mtext>LP</mml:mtext><mml:mtext>b</mml:mtext></mml:msubsup></mml:math></inline-formula> (s), PY burst duration <inline-formula><mml:math id="inf226"><mml:msubsup><mml:mi>d</mml:mi><mml:mtext>PY</mml:mtext><mml:mtext>b</mml:mtext></mml:msubsup></mml:math></inline-formula> (s), gap AB/PD end to LP start <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mtext>AB-LP</mml:mtext><mml:mtext>es</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula> (s), gap LP end to PY start <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mtext>LP-PY</mml:mtext><mml:mtext>es</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula> (s), delay AB/PD start to LP start <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mtext>AB-LP</mml:mtext><mml:mtext>ss</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula> (s), delay LP start to PY start <inline-formula><mml:math id="inf230"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mtext>LP-PY</mml:mtext><mml:mtext>ss</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula> (s), AB/PD duty cycle <inline-formula><mml:math id="inf231"><mml:msub><mml:mi>d</mml:mi><mml:mtext>AB</mml:mtext></mml:msub></mml:math></inline-formula>, LP duty cycle <inline-formula><mml:math id="inf232"><mml:msub><mml:mi>d</mml:mi><mml:mtext>LP</mml:mtext></mml:msub></mml:math></inline-formula>, PY duty cycle <inline-formula><mml:math id="inf233"><mml:msub><mml:mi>d</mml:mi><mml:mtext>PY</mml:mtext></mml:msub></mml:math></inline-formula>, phase gap AB/PD end to LP start <inline-formula><mml:math id="inf234"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mtext>AB-LP</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>, phase gap LP end to PY start <inline-formula><mml:math id="inf235"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ϕ</mml:mi><mml:mtext>LP-PY</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>, LP start phase <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>ϕ</mml:mi><mml:mtext>LP</mml:mtext></mml:msub></mml:math></inline-formula>, and PY start phase <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>ϕ</mml:mi><mml:mtext>PY</mml:mtext></mml:msub></mml:math></inline-formula>. Note that several of these values are only defined if each neuron produces rhythmic bursting behavior. In addition, for each of the three neurons, we used one feature that describes the maximal duration of its voltage being above −30 mV. We did this as we observed plateaus at around −10 mV during the onset of bursts, and wanted to distinguish such traces from others. If the maximal duration was below 5 ms, we set this feature to 5 ms. To extract the summary features from the observed experimental data, we first found spikes by searching for local maxima above a hand-picked voltage threshold, and then extracted the 15 above described features. We set the additional 3 features to 5 ms.</p><p>We used SNPE to infer the posterior distribution over the 18 summary features from experimental data. For inference, we used a single round with 18.5 million samples, out of which 174,000 samples contain bursts in all neurons. We therefore used these 174,000 samples with well defined summary features for training the inference network (<inline-formula><mml:math id="inf238"><mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>31</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">𝐱</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>18</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). The density estimator was a masked autoregressive flow (MAF) (<xref ref-type="bibr" rid="bib95">Papamakarios et al., 2017</xref>) with five MADES with [100,100,100] hidden units each. The synaptic conductances were transformed into logarithmic space before training and for the entire analysis.</p><p>Previous approaches for fitting the STG circuit (<xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>) first fit individual neuron features and reduce the number of possible neuron models (<xref ref-type="bibr" rid="bib108">Prinz et al., 2003</xref>), and then fit the whole circuit model. While powerful, this approach both requires the availability of single-neuron data, and cannot give access to potential compensation mechanisms between single-neuron and synaptic parameters. Unlike <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>, we apply SNPE to directly identify the full 31 dimensional parameter space without requiring experimental measurements of each individual neuron in the circuit. Despite the high-dimensional parameter space, SNPE can identify the posterior distribution using 18 million samples, whereas a direct application of a full-grid method would require <inline-formula><mml:math id="inf239"><mml:mrow><mml:mn>4.65</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>21</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> samples to fill the 31 dimensional parameter space on a grid with five values per dimension.</p></sec><sec id="s4-11"><title>Finding paths in the posterior</title><p>In order to find directions of robust network output, we searched for a path of high posterior probability. First, as in <xref ref-type="bibr" rid="bib109">Prinz et al., 2004</xref>, we aimed to find two similar model outputs with disparate parameters. To do so, we sampled from the posterior and searched for two parameter sets whose summary features were within 0.1 standard deviations of all 174,000 samples from the observed experimental data, but that had strongly disparate parameters from each other. In the following, we denote the obtained parameter sets by <inline-formula><mml:math id="inf240"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf241"><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:math></inline-formula>.</p><p>Second, in order to identify whether network output can be maintained along a continuous path between these two samples, we searched for a connection in parameter space lying in regions of high posterior probability. To do so, we considered the connection between the samples as a path and minimized the following path integral:<disp-formula id="equ14"><label>(2)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">x</mml:mi><mml:mi mathvariant="bold">o</mml:mi></mml:msub><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo>˙</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>To minimize this term, we parameterized the path <inline-formula><mml:math id="inf242"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> using sinusoidal basis-functions with coefficients <inline-formula><mml:math id="inf243"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>:<disp-formula id="equ15"><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>K</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mi>sin</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mi>K</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mi>sin</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>π</mml:mi><mml:mi>k</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:msub><mml:mi>θ</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>These basis functions are defined such that, for any coefficients <inline-formula><mml:math id="inf244"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the start and end points of the path are exactly the two parameter sets defined above:<disp-formula id="equ16"><mml:math id="m16"><mml:mrow><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mi>g</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>With this formulation, we have framed the problem of finding the path as an unconstrained optimization problem over the parameters <inline-formula><mml:math id="inf245"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. We can therefore minimize the path integral <inline-formula><mml:math id="inf246"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">ℒ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> using gradient descent over <inline-formula><mml:math id="inf247"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. For numerical simulations, we approximated the integral in <xref ref-type="disp-formula" rid="equ14">Equation 2</xref> as a sum over 80 points along the path and use two basis functions for each of the 31 dimensions, that is, <inline-formula><mml:math id="inf248"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>In order to demonstrate the sensitivity of the pyloric network, we aimed to find a path along which the circuit output quickly breaks down. For this, we picked a starting point along the high-probability path and then minimized the posterior probability. In addition, we enforced that the orthogonal path lies within an orthogonal disk to the high-probability path, leading to the following constrained optimization problem:<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mrow><mml:mrow><mml:munder><mml:mi>min</mml:mi><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:munder><mml:mo>⁡</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mtext>s.t.</mml:mtext><mml:mo mathvariant="italic" separator="true"> </mml:mo><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula>where <italic>n</italic> is the tangent vector along the path of high probability. This optimization problem can be solved using the gradient projection method (<xref ref-type="bibr" rid="bib115">Rosen, 1960</xref>):<disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold-italic">𝜽</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>∇</mml:mo><mml:mo>⁡</mml:mo><mml:mi>log</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula>with projection matrix <inline-formula><mml:math id="inf249"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>𝟙</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf250"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mn mathvariant="double-struck">𝟙</mml:mn></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> indicating the identity matrix. Each gradient update is a step along the orthogonal path. We let the optimization run until the distance along the path is 1/27 of the distance along the high-probability path.</p></sec><sec id="s4-12"><title>Identifying conditional correlations</title><p>In order to investigate compensation mechanisms in the STG, we compared marginal and conditional correlations. For the marginal correlation matrix in <xref ref-type="fig" rid="fig6">Figure 6b</xref>, we calculated the Pearson correlation coefficient based on 1.26 million samples from the posterior distribution <inline-formula><mml:math id="inf251"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">𝜽</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi mathvariant="bold">𝐱</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. To find the two-dimensional conditional distribution for any pair of parameters, we fixed all other parameters to values taken from an arbitrary posterior sample, and varied the remaining two on an evenly spaced grid with 50 points along each dimension, covering the entire prior space. We evaluated the posterior distribution at every value on this grid. We then calculated the conditional correlation as the Pearson correlation coefficient over this distribution. For the 1-dimensional conditional distribution, we varied only one parameter and kept all others fixed. Lastly, in <xref ref-type="fig" rid="fig6">Figure 6d</xref>, we sampled 500 parameter sets from the posterior, computed the respective conditional posteriors and conditional correlation matrices, and took the average over the conditional correlation matrices.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Mahmood S Hoseini and Michael Stryker for sharing their data for <xref ref-type="fig" rid="fig2">Figure 2</xref>, and Philipp Berens, Sean Bittner, Jan Boelts, John Cunningham, Richard Gao, Scott Linderman, Eve Marder, Iain Murray, George Papamakarios, Astrid Prinz, Auguste Schulz and Srinivas Turaga for discussions and/or comments on the manuscript. This work was supported by the German Research Foundation (DFG) through SFB 1233 ‘Robust Vision’, (276693517), SFB 1089 ‘Synaptic Microcircuits’, SPP 2041 ‘Computational Connectomics’ and Germany's Excellence Strategy – EXC-Number 2064/1 – Project number 390727645 and the German Federal Ministry of Education and Research (BMBF, project ‘ADIMEM’, FKZ 01IS18052 A-D) to JHM, a Sir Henry Dale Fellowship by the Wellcome Trust and the Royal Society (WT100000; WFP and TPV), a Wellcome Trust Senior Research Fellowship (214316/Z/18/Z; TPV), a ERC Consolidator Grant (SYNAPSEEK; WPF and CC), and a UK Research and Innovation, Biotechnology and Biological Sciences Research Council (CC, UKRI-BBSRC BB/N019512/1). We gratefully acknowledge the Leibniz Supercomputing Centre for funding this project by providing computing time on its Linux-Cluster.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Software, Investigation, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Software, Visualization, Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Resources, Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Resources, Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con9"><p>Resources, Data curation, Writing - review and editing</p></fn><fn fn-type="con" id="con10"><p>Conceptualization, Resources, Supervision, Funding acquisition, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con11"><p>Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con12"><p>Conceptualization, Supervision, Funding acquisition, Validation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-56261-transrepform-v3.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The contributions of the work are primarily the development and application of computational models, no new data has been obtained or is being published. All code and associated data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/mackelab/delfi/">https://github.com/mackelab/delfi/</ext-link> (archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:62a99a879145bdc675917fc33eed69293b964048;origin=https://github.com/mackelab/delfi/;visit=swh:1:snp:95a69f58c195feb5660760cd4ab833de366f5441/">https://archive.softwareheritage.org/swh:1:rev:62a99a879145bdc675917fc33eed69293b964048;origin=https://github.com/mackelab/delfi/;visit=swh:1:snp:95a69f58c195feb5660760cd4ab833de366f5441/</ext-link>) and <ext-link ext-link-type="uri" xlink:href="https://github.com/mackelab/IdentifyMechanisticModels_2020">https://github.com/mackelab/IdentifyMechanisticModels_2020</ext-link> (archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:b93c90ec6156ae5f8afee6aaac7317373e9caf5e;origin=https://github.com/mackelab/IdentifyMechanisticModels_2020;visit=swh:1:snp:20bfde9fbbb134657b75bc29ab4905a2ef3d5b17/">https://archive.softwareheritage.org/swh:1:rev:b93c90ec6156ae5f8afee6aaac7317373e9caf5e;origin=https://github.com/mackelab/IdentifyMechanisticModels_2020;visit=swh:1:snp:20bfde9fbbb134657b75bc29ab4905a2ef3d5b17/</ext-link>).</p><p>The following datasets were generated:</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Abbott</surname> <given-names>L</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Modeling Small Networks</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Achard</surname> <given-names>P</given-names></name><name><surname>De Schutter</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Complex parameter landscape for a complex neuron model</article-title><source>PLOS Computational Biology</source><volume>2</volume><elocation-id>e94</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020094</pub-id><pub-id pub-id-type="pmid">16848639</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Allen Institute for Brain Science</collab></person-group><year iso-8601-date="2016">2016</year><article-title>Allen cell types database</article-title><ext-link ext-link-type="uri" xlink:href="http://celltypes.brain-map.org/">http://celltypes.brain-map.org/</ext-link><date-in-citation iso-8601-date="2018-06-08">June 8, 2018</date-in-citation></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname> <given-names>LM</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visualization of currents in neural models with similar behavior and different conductance densities</article-title><source>eLife</source><volume>8</volume><elocation-id>e42722</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.42722</pub-id><pub-id pub-id-type="pmid">30702427</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname> <given-names>RE</given-names></name><name><surname>Peña</surname> <given-names>JM</given-names></name><name><surname>Jayamohan</surname> <given-names>J</given-names></name><name><surname>Jérusalem</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Mechanistic models versus machine learning, a fight worth fighting for the biological community?</article-title><source>Biology Letters</source><volume>14</volume><elocation-id>20170660</elocation-id><pub-id pub-id-type="doi">10.1098/rsbl.2017.0660</pub-id><pub-id pub-id-type="pmid">29769297</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barthelmé</surname> <given-names>S</given-names></name><name><surname>Chopin</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Expectation propagation for Likelihood-Free inference</article-title><source>Journal of the American Statistical Association</source><volume>109</volume><fpage>315</fpage><lpage>333</lpage><pub-id pub-id-type="doi">10.1080/01621459.2013.864178</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bassett</surname> <given-names>DS</given-names></name><name><surname>Zurn</surname> <given-names>P</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>On the nature and use of models in network neuroscience</article-title><source>Nature Reviews Neuroscience</source><volume>19</volume><fpage>566</fpage><lpage>578</lpage><pub-id pub-id-type="doi">10.1038/s41583-018-0038-8</pub-id><pub-id pub-id-type="pmid">30002509</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaumont</surname> <given-names>MA</given-names></name><name><surname>Zhang</surname> <given-names>W</given-names></name><name><surname>Balding</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Approximate bayesian computation in population genetics</article-title><source>Genetics</source><volume>162</volume><fpage>2025</fpage><lpage>2035</lpage><pub-id pub-id-type="pmid">12524368</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beaumont</surname> <given-names>MA</given-names></name><name><surname>Cornuet</surname> <given-names>J-M</given-names></name><name><surname>Marin</surname> <given-names>J-M</given-names></name><name><surname>Robert</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Adaptive approximate bayesian computation</article-title><source>Biometrika</source><volume>96</volume><fpage>983</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1093/biomet/asp052</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ben-Shalom</surname> <given-names>R</given-names></name><name><surname>Balewski</surname> <given-names>J</given-names></name><name><surname>Siththaranjan</surname> <given-names>A</given-names></name><name><surname>Baratham</surname> <given-names>V</given-names></name><name><surname>Kyoung</surname> <given-names>H</given-names></name><name><surname>Kim</surname> <given-names>KG</given-names></name><name><surname>Bender</surname> <given-names>KJ</given-names></name><name><surname>Bouchard</surname> <given-names>KE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Inferring neuronal ionic conductances from membrane potentials using cnns</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/727974</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Bishop</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="1994">1994</year><source>Mixture Density Networks, Technical Report</source><publisher-name>Aston University</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bittner</surname> <given-names>SR</given-names></name><name><surname>Palmigiano</surname> <given-names>A</given-names></name><name><surname>Piet</surname> <given-names>AT</given-names></name><name><surname>Duan</surname> <given-names>CA</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Interrogating theoretical models of neural computation with deep inference</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/837567</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bleuler</surname> <given-names>S</given-names></name><name><surname>Laumanns</surname> <given-names>M</given-names></name><name><surname>Thiele</surname> <given-names>L</given-names></name><name><surname>Zitzler</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pisa'a platform and programming language independent interface for search algorithms</article-title><conf-name>International Conference on Evolutionary Multi-Criterion Optimization</conf-name><fpage>494</fpage><lpage>508</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname> <given-names>MGB</given-names></name><name><surname>Nunes</surname> <given-names>MA</given-names></name><name><surname>Prangle</surname> <given-names>D</given-names></name><name><surname>Sisson</surname> <given-names>SA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A comparative review of dimension reduction methods in approximate bayesian computation</article-title><source>Statistical Science</source><volume>28</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1214/12-STS406</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blum</surname> <given-names>MGB</given-names></name><name><surname>François</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Non-linear regression models for approximate bayesian computation</article-title><source>Statistics and Computing</source><volume>20</volume><fpage>63</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1007/s11222-009-9116-0</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>What is the most realistic single-compartment model of spike initiation?</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004114</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004114</pub-id><pub-id pub-id-type="pmid">25856629</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britton</surname> <given-names>OJ</given-names></name><name><surname>Bueno-Orovio</surname> <given-names>A</given-names></name><name><surname>Van Ammel</surname> <given-names>K</given-names></name><name><surname>Lu</surname> <given-names>HR</given-names></name><name><surname>Towart</surname> <given-names>R</given-names></name><name><surname>Gallacher</surname> <given-names>DJ</given-names></name><name><surname>Rodriguez</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Experimentally calibrated population of models predicts and explains intersubject variability in cardiac cellular electrophysiology</article-title><source>PNAS</source><volume>110</volume><fpage>E2098</fpage><lpage>E2105</lpage><pub-id pub-id-type="doi">10.1073/pnas.1304382110</pub-id><pub-id pub-id-type="pmid">23690584</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>EN</given-names></name><name><surname>Frank</surname> <given-names>LM</given-names></name><name><surname>Tang</surname> <given-names>D</given-names></name><name><surname>Quirk</surname> <given-names>MC</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A statistical paradigm for neural spike train decoding applied to position prediction from ensemble firing patterns of rat hippocampal place cells</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>7411</fpage><lpage>7425</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-18-07411.1998</pub-id><pub-id pub-id-type="pmid">9736661</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caplan</surname> <given-names>JS</given-names></name><name><surname>Williams</surname> <given-names>AH</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Many parameter sets in a multicompartment model oscillator are robust to temperature perturbations</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>4963</fpage><lpage>4975</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0280-14.2014</pub-id><pub-id pub-id-type="pmid">24695714</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Casado</surname> <given-names>ML</given-names></name><name><surname>Baydin</surname> <given-names>AG</given-names></name><name><surname>Rubio</surname> <given-names>DM</given-names></name><name><surname>Le</surname> <given-names>TA</given-names></name><name><surname>Wood</surname> <given-names>F</given-names></name><name><surname>Heinrich</surname> <given-names>L</given-names></name><name><surname>Louppe</surname> <given-names>G</given-names></name><name><surname>Cranmer</surname> <given-names>K</given-names></name><name><surname>Ng</surname> <given-names>K</given-names></name><name><surname>Bhimji</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Improvements to inference compilation for probabilistic programming in large-scale scientific simulators</article-title><conf-name>NeurIPS Workshop on Deep Learning for Physical Sciences</conf-name></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Chan</surname> <given-names>J</given-names></name><name><surname>Perrone</surname> <given-names>V</given-names></name><name><surname>Spence</surname> <given-names>J</given-names></name><name><surname>Jenkins</surname> <given-names>P</given-names></name><name><surname>Mathieson</surname> <given-names>S</given-names></name><name><surname>Song</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A likelihood-free inference framework for population genetic data using exchangeable neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>8594</fpage><lpage>8605</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Bayesian filtering: from Kalman filters to particle filters, and beyond</article-title><source>Statistics</source><volume>182</volume><fpage>1</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1080/02331880309257</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A simple white noise analysis of neuronal light responses</article-title><source>Network: Computation in Neural Systems</source><volume>12</volume><fpage>199</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1080/713663221</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cook</surname> <given-names>SR</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Rubin</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Validation of software for bayesian models using posterior quantiles</article-title><source>Journal of Computational and Graphical Statistics</source><volume>15</volume><fpage>675</fpage><lpage>692</lpage><pub-id pub-id-type="doi">10.1198/106186006X136976</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Costa</surname> <given-names>RP</given-names></name><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>van Rossum</surname> <given-names>MC</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Probabilistic inference of short-term synaptic plasticity in neocortical microcircuits</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>75</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00075</pub-id><pub-id pub-id-type="pmid">23761760</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cranmer</surname> <given-names>K</given-names></name><name><surname>Brehmer</surname> <given-names>J</given-names></name><name><surname>Louppe</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The frontier of simulation-based inference</article-title><source>PNAS</source><volume>46</volume><elocation-id>201912789</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.1912789117</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dimensionality reduction for large-scale neural recordings</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1500</fpage><lpage>1509</lpage><pub-id pub-id-type="doi">10.1038/nn.3776</pub-id><pub-id pub-id-type="pmid">25151264</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Daly</surname> <given-names>AC</given-names></name><name><surname>Gavaghan</surname> <given-names>DJ</given-names></name><name><surname>Holmes</surname> <given-names>C</given-names></name><name><surname>Cooper</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Hodgkin-Huxley revisited: reparametrization and identifiability analysis of the classic action potential model with approximate bayesian methods</article-title><source>Royal Society Open Science</source><volume>2</volume><elocation-id>150499</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.150499</pub-id><pub-id pub-id-type="pmid">27019736</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Nicolao</surname> <given-names>G</given-names></name><name><surname>Sparacino</surname> <given-names>G</given-names></name><name><surname>Cobelli</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Nonparametric input estimation in physiological systems: problems, methods, and case studies</article-title><source>Automatica</source><volume>33</volume><fpage>851</fpage><lpage>870</lpage><pub-id pub-id-type="doi">10.1016/S0005-1098(96)00254-3</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Destexhe</surname> <given-names>A</given-names></name><name><surname>Huguenard</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Nonlinear thermodynamic models of voltage-dependent currents</article-title><source>Journal of Computational Neuroscience</source><volume>9</volume><fpage>259</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1023/a:1026535704537</pub-id><pub-id pub-id-type="pmid">11139042</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Druckmann</surname> <given-names>S</given-names></name><name><surname>Banitt</surname> <given-names>Y</given-names></name><name><surname>Gidon</surname> <given-names>A</given-names></name><name><surname>Schürmann</surname> <given-names>F</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name><name><surname>Segev</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A novel multiple objective optimization framework for constraining conductance-based neuron models by experimental data</article-title><source>Frontiers in Neuroscience</source><volume>1</volume><fpage>7</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.3389/neuro.01.1.1.001.2007</pub-id><pub-id pub-id-type="pmid">18982116</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dunlop</surname> <given-names>J</given-names></name><name><surname>Bowlby</surname> <given-names>M</given-names></name><name><surname>Peri</surname> <given-names>R</given-names></name><name><surname>Vasilyev</surname> <given-names>D</given-names></name><name><surname>Arias</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>High-throughput electrophysiology: an emerging paradigm for ion-channel screening and physiology</article-title><source>Nature Reviews Drug Discovery</source><volume>7</volume><fpage>358</fpage><lpage>368</lpage><pub-id pub-id-type="doi">10.1038/nrd2552</pub-id><pub-id pub-id-type="pmid">18356919</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Durkan</surname> <given-names>C</given-names></name><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sequential neural methods for likelihood-free inference</article-title><conf-name>NeurIPS Bayesian Deep Learning Workshop</conf-name></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Durkan</surname> <given-names>C</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name><name><surname>Papamakarios</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On contrastive learning for likelihood-free inference</article-title><conf-name>International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyballa</surname> <given-names>L</given-names></name><name><surname>Hoseini</surname> <given-names>MS</given-names></name><name><surname>Dadarlat</surname> <given-names>MC</given-names></name><name><surname>Zucker</surname> <given-names>SW</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flow stimuli reveal ecologically appropriate responses in mouse visual cortex</article-title><source>PNAS</source><volume>115</volume><fpage>11304</fpage><lpage>11309</lpage><pub-id pub-id-type="doi">10.1073/pnas.1811265115</pub-id><pub-id pub-id-type="pmid">30327345</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname> <given-names>D</given-names></name><name><surname>Olasagasti</surname> <given-names>I</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Aksay</surname> <given-names>ER</given-names></name><name><surname>Goldman</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A modeling framework for deriving the structural and functional architecture of a short-term memory microcircuit</article-title><source>Neuron</source><volume>79</volume><fpage>987</fpage><lpage>1000</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.06.041</pub-id><pub-id pub-id-type="pmid">24012010</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foster</surname> <given-names>WR</given-names></name><name><surname>Ungar</surname> <given-names>LH</given-names></name><name><surname>Schwaber</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Significance of conductances in Hodgkin-Huxley models</article-title><source>Journal of Neurophysiology</source><volume>70</volume><fpage>2502</fpage><lpage>2518</lpage><pub-id pub-id-type="doi">10.1152/jn.1993.70.6.2502</pub-id><pub-id pub-id-type="pmid">7509859</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerstner</surname> <given-names>W</given-names></name><name><surname>Sprekeler</surname> <given-names>H</given-names></name><name><surname>Deco</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Theory and simulation in neuroscience</article-title><source>Science</source><volume>338</volume><fpage>60</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1126/science.1227356</pub-id><pub-id pub-id-type="pmid">23042882</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gerwinn</surname> <given-names>S</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Bayesian inference for generalized linear models for spiking neurons</article-title><source>Frontiers in Computational Neuroscience</source><volume>4</volume><elocation-id>12</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2010.00012</pub-id><pub-id pub-id-type="pmid">20577627</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman</surname> <given-names>MS</given-names></name><name><surname>Golowasch</surname> <given-names>J</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Global structure, robustness, and modulation of neuronal models</article-title><source>The Journal of Neuroscience</source><volume>21</volume><fpage>5229</fpage><lpage>5238</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.21-14-05229.2001</pub-id><pub-id pub-id-type="pmid">11438598</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golowasch</surname> <given-names>J</given-names></name><name><surname>Goldman</surname> <given-names>MS</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Failure of averaging in the construction of a conductance-based neuron model</article-title><source>Journal of Neurophysiology</source><volume>87</volume><fpage>1129</fpage><lpage>1131</lpage><pub-id pub-id-type="doi">10.1152/jn.00412.2001</pub-id><pub-id pub-id-type="pmid">11826077</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname> <given-names>NW</given-names></name><name><surname>Berg</surname> <given-names>J</given-names></name><name><surname>Feng</surname> <given-names>D</given-names></name><name><surname>Sorensen</surname> <given-names>SA</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name><name><surname>Hawrylycz</surname> <given-names>MJ</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Arkhipov</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Systematic generation of biophysically detailed models for diverse cortical neuron types</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>710</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02718-3</pub-id><pub-id pub-id-type="pmid">29459718</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grashow</surname> <given-names>R</given-names></name><name><surname>Brookings</surname> <given-names>T</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Compensation for variable intrinsic neuronal excitability by circuit-synaptic interactions</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>9145</fpage><lpage>9156</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0980-10.2010</pub-id><pub-id pub-id-type="pmid">20610748</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Greenberg</surname> <given-names>D</given-names></name><name><surname>Nonnenmacher</surname> <given-names>M</given-names></name><name><surname>Macke</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Automatic posterior transformation for likelihood-free inference</article-title><conf-name>International Conference on Machine Learning</conf-name><fpage>2404</fpage><lpage>2414</lpage></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutenkunst</surname> <given-names>RN</given-names></name><name><surname>Waterfall</surname> <given-names>JJ</given-names></name><name><surname>Casey</surname> <given-names>FP</given-names></name><name><surname>Brown</surname> <given-names>KS</given-names></name><name><surname>Myers</surname> <given-names>CR</given-names></name><name><surname>Sethna</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Universally sloppy parameter sensitivities in systems biology models</article-title><source>PLOS Computational Biology</source><volume>3</volume><elocation-id>e189</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0030189</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutierrez</surname> <given-names>GJ</given-names></name><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multiple mechanisms switch an electrically coupled, synaptically inhibited neuron between competing rhythmic oscillators</article-title><source>Neuron</source><volume>77</volume><fpage>845</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.016</pub-id><pub-id pub-id-type="pmid">23473315</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gutmann</surname> <given-names>MU</given-names></name><name><surname>Corander</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Bayesian optimization for likelihood-free inference of simulator-based statistical models</article-title><source>The Journal of Machine Learning Research</source><volume>17</volume><fpage>4256</fpage><lpage>4302</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haddad</surname> <given-names>SA</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Circuit robustness to temperature perturbation is altered by neuromodulators</article-title><source>Neuron</source><volume>100</volume><fpage>609</fpage><lpage>623</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.08.035</pub-id><pub-id pub-id-type="pmid">30244886</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hay</surname> <given-names>E</given-names></name><name><surname>Hill</surname> <given-names>S</given-names></name><name><surname>Schürmann</surname> <given-names>F</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name><name><surname>Segev</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Models of neocortical layer 5b pyramidal cells capturing a wide range of dendritic and perisomatic active properties</article-title><source>PLOS Computational Biology</source><volume>7</volume><elocation-id>e1002107</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002107</pub-id><pub-id pub-id-type="pmid">21829333</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hermans</surname> <given-names>J</given-names></name><name><surname>Begy</surname> <given-names>V</given-names></name><name><surname>Louppe</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Likelihood-free mcmc with approximate likelihood ratios</article-title><conf-name>International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertäg</surname> <given-names>L</given-names></name><name><surname>Hass</surname> <given-names>J</given-names></name><name><surname>Golovko</surname> <given-names>T</given-names></name><name><surname>Durstewitz</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An approximation to the adaptive exponential Integrate-and-Fire neuron model allows fast and predictive fitting to physiological data</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>62</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00062</pub-id><pub-id pub-id-type="pmid">22973220</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herz</surname> <given-names>AV</given-names></name><name><surname>Gollisch</surname> <given-names>T</given-names></name><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Jaeger</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Modeling single-neuron dynamics and computations: a balance of detail and abstraction</article-title><source>Science</source><volume>314</volume><fpage>80</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1126/science.1127240</pub-id><pub-id pub-id-type="pmid">17023649</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hodgkin</surname> <given-names>AL</given-names></name><name><surname>Huxley</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="1952">1952</year><article-title>A quantitative description of membrane current and its application to conduction and excitation in nerve</article-title><source>The Journal of Physiology</source><volume>117</volume><fpage>500</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1952.sp004764</pub-id><pub-id pub-id-type="pmid">12991237</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJ</given-names></name><name><surname>Ahrens</surname> <given-names>MB</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Efficient estimation of detailed single-neuron models</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>872</fpage><lpage>890</lpage><pub-id pub-id-type="doi">10.1152/jn.00079.2006</pub-id><pub-id pub-id-type="pmid">16624998</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huys</surname> <given-names>QJ</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Smoothing of, and parameter estimation from, noisy biophysical recordings</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000379</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000379</pub-id><pub-id pub-id-type="pmid">19424506</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Izbicki</surname> <given-names>R</given-names></name><name><surname>Lee</surname> <given-names>AB</given-names></name><name><surname>Pospisil</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ABC–CDE: Toward Approximate Bayesian Computation With Complex High-Dimensional Data and Limited Simulations</article-title><source>Journal of Computational and Graphical Statistics</source><volume>28</volume><fpage>481</fpage><lpage>492</lpage><pub-id pub-id-type="doi">10.1080/10618600.2018.1546594</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname> <given-names>B</given-names></name><name><surname>Wu</surname> <given-names>T-y</given-names></name><name><surname>Zheng</surname> <given-names>C</given-names></name><name><surname>Wong</surname> <given-names>WH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning summary statistic for approximate bayesian computation via deep neural network</article-title><source>Statistica Sinica</source><volume>27</volume><fpage>1595</fpage><lpage>1618</lpage><pub-id pub-id-type="doi">10.5705/ss.202015.0340</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>JP</given-names></name><name><surname>Palmer</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>An evaluation of the two-dimensional gabor filter model of simple receptive fields in cat striate cortex</article-title><source>Journal of Neurophysiology</source><volume>58</volume><fpage>1233</fpage><lpage>1258</lpage><pub-id pub-id-type="doi">10.1152/jn.1987.58.6.1233</pub-id><pub-id pub-id-type="pmid">3437332</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname> <given-names>DP</given-names></name><name><surname>Ba</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: a method for stochastic optimization</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kitano</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Biological robustness</article-title><source>Nature Reviews Genetics</source><volume>5</volume><fpage>826</fpage><lpage>837</lpage><pub-id pub-id-type="doi">10.1038/nrg1471</pub-id><pub-id pub-id-type="pmid">15520792</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kleinegesse</surname> <given-names>S</given-names></name><name><surname>Gutmann</surname> <given-names>MU</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Efficient bayesian experimental design for implicit models</article-title><conf-name>The 22nd International Conference on Artificial Intelligence and Statistics</conf-name><fpage>476</fpage><lpage>485</lpage></element-citation></ref><ref id="bib63"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname> <given-names>A</given-names></name><name><surname>Sutskever</surname> <given-names>I</given-names></name><name><surname>Hinton</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Imagenet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ladenbauer</surname> <given-names>J</given-names></name><name><surname>McKenzie</surname> <given-names>S</given-names></name><name><surname>English</surname> <given-names>DF</given-names></name><name><surname>Hagens</surname> <given-names>O</given-names></name><name><surname>Ostojic</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring and validating mechanistic models of neural microcircuits based on spike-train data</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/261016</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawson</surname> <given-names>BAJ</given-names></name><name><surname>Drovandi</surname> <given-names>CC</given-names></name><name><surname>Cusimano</surname> <given-names>N</given-names></name><name><surname>Burrage</surname> <given-names>P</given-names></name><name><surname>Rodriguez</surname> <given-names>B</given-names></name><name><surname>Burrage</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Unlocking data sets by calibrating populations of models to data density: a study in atrial electrophysiology</article-title><source>Science Advances</source><volume>4</volume><elocation-id>e1701676</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.1701676</pub-id><pub-id pub-id-type="pmid">29349296</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Le</surname> <given-names>TA</given-names></name><name><surname>Baydin</surname> <given-names>AG</given-names></name><name><surname>Zinkov</surname> <given-names>R</given-names></name><name><surname>Wood</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Using synthetic data to train neural networks is model-based reasoning, in 2017</article-title><conf-name>International Joint Conference on Neural Networks (IJCNN) IEEE</conf-name><fpage>3514</fpage><lpage>3521</lpage><pub-id pub-id-type="doi">10.1109/IJCNN.2017.7966298</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Le</surname> <given-names>TA</given-names></name><name><surname>Baydin</surname> <given-names>AG</given-names></name><name><surname>Wood</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Inference compilation and universal probabilistic programming</article-title><conf-name>Artificial Intelligence and Statistics</conf-name><fpage>1338</fpage><lpage>1348</lpage></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liepe</surname> <given-names>J</given-names></name><name><surname>Kirk</surname> <given-names>P</given-names></name><name><surname>Filippi</surname> <given-names>S</given-names></name><name><surname>Toni</surname> <given-names>T</given-names></name><name><surname>Barnes</surname> <given-names>CP</given-names></name><name><surname>Stumpf</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A framework for parameter estimation and model selection from experimental data in systems biology using approximate bayesian computation</article-title><source>Nature Protocols</source><volume>9</volume><fpage>439</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1038/nprot.2014.025</pub-id><pub-id pub-id-type="pmid">24457334</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Doiron</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Slow dynamics and high variability in balanced cortical networks with clustered connections</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1498</fpage><lpage>1505</lpage><pub-id pub-id-type="doi">10.1038/nn.3220</pub-id><pub-id pub-id-type="pmid">23001062</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Loaiza-Ganem</surname> <given-names>G</given-names></name><name><surname>Gao</surname> <given-names>Y</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Maximum entropy flow networks</article-title><conf-name>5th International Conference on Learning Representations, ICLR</conf-name></element-citation></ref><ref id="bib71"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname> <given-names>J-M</given-names></name><name><surname>Goncalves</surname> <given-names>PJ</given-names></name><name><surname>Bassetto</surname> <given-names>G</given-names></name><name><surname>Öcal</surname> <given-names>K</given-names></name><name><surname>Nonnenmacher</surname> <given-names>M</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Flexible statistical inference for mechanistic models of neural dynamics</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1289</fpage><lpage>1299</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lueckmann</surname> <given-names>J-M</given-names></name><name><surname>Bassetto</surname> <given-names>G</given-names></name><name><surname>Karaletsos</surname> <given-names>T</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Likelihood-free inference with emulator networks</article-title><conf-name>Proceedings of the 1st Symposium on Advances in Approximate Bayesian Inference, Volume 96 of Proceedings of Machine Learning Research</conf-name><fpage>32</fpage><lpage>53</lpage></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machta</surname> <given-names>BB</given-names></name><name><surname>Chachra</surname> <given-names>R</given-names></name><name><surname>Transtrum</surname> <given-names>MK</given-names></name><name><surname>Sethna</surname> <given-names>JP</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Parameter space compression underlies emergent theories and predictive models</article-title><source>Science</source><volume>342</volume><fpage>604</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1126/science.1238723</pub-id><pub-id pub-id-type="pmid">24179222</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Macke</surname> <given-names>JH</given-names></name><name><surname>Buesing</surname> <given-names>L</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Empirical models of spiking in neural populations</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1350</fpage><lpage>1358</lpage></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLean</surname> <given-names>JN</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Johnson</surname> <given-names>BR</given-names></name><name><surname>Harris-Warrick</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Activity-independent homeostasis in rhythmically active neurons</article-title><source>Neuron</source><volume>37</volume><fpage>109</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(02)01104-2</pub-id><pub-id pub-id-type="pmid">12526777</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLean</surname> <given-names>JN</given-names></name><name><surname>Zhang</surname> <given-names>Y</given-names></name><name><surname>Goeritz</surname> <given-names>ML</given-names></name><name><surname>Casey</surname> <given-names>R</given-names></name><name><surname>Oliva</surname> <given-names>R</given-names></name><name><surname>Guckenheimer</surname> <given-names>J</given-names></name><name><surname>Harris-Warrick</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Activity-independent coregulation of IA and ih in rhythmically active neurons</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>3601</fpage><lpage>3617</lpage><pub-id pub-id-type="doi">10.1152/jn.00281.2005</pub-id><pub-id pub-id-type="pmid">16049145</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Maheswaranathan</surname> <given-names>N</given-names></name><name><surname>Williams</surname> <given-names>A</given-names></name><name><surname>Golub</surname> <given-names>MD</given-names></name><name><surname>Ganguli</surname> <given-names>S</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>15696</fpage><lpage>15705</lpage></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mante</surname> <given-names>V</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Newsome</surname> <given-names>WT</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Context-dependent computation by recurrent dynamics in prefrontal cortex</article-title><source>Nature</source><volume>503</volume><fpage>78</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1038/nature12742</pub-id><pub-id pub-id-type="pmid">24201281</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Variability, compensation, and modulation in neurons and circuits</article-title><source>PNAS</source><volume>108 Suppl 3</volume><fpage>15542</fpage><lpage>15548</lpage><pub-id pub-id-type="doi">10.1073/pnas.1010674108</pub-id><pub-id pub-id-type="pmid">21383190</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name><name><surname>Goeritz</surname> <given-names>ML</given-names></name><name><surname>Otopalik</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Robust circuit rhythms in small circuits arise from variable circuit components and mechanisms</article-title><source>Current Opinion in Neurobiology</source><volume>31</volume><fpage>156</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.10.012</pub-id><pub-id pub-id-type="pmid">25460072</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name><name><surname>Goaillard</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Variability, compensation and homeostasis in neuron and network function</article-title><source>Nature Reviews Neuroscience</source><volume>7</volume><fpage>563</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1038/nrn1949</pub-id><pub-id pub-id-type="pmid">16791145</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marder</surname> <given-names>E</given-names></name><name><surname>Taylor</surname> <given-names>AL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multiple models to capture the variability in biological neurons and networks</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>133</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nn.2735</pub-id><pub-id pub-id-type="pmid">21270780</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marjoram</surname> <given-names>P</given-names></name><name><surname>Molitor</surname> <given-names>J</given-names></name><name><surname>Plagnol</surname> <given-names>V</given-names></name><name><surname>Tavare</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Markov chain monte carlo without likelihoods</article-title><source>PNAS</source><volume>100</volume><fpage>15324</fpage><lpage>15328</lpage><pub-id pub-id-type="doi">10.1073/pnas.0306899100</pub-id><pub-id pub-id-type="pmid">14663152</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McTavish</surname> <given-names>TS</given-names></name><name><surname>Migliore</surname> <given-names>M</given-names></name><name><surname>Shepherd</surname> <given-names>GM</given-names></name><name><surname>Hines</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mitral cell spike synchrony modulated by dendrodendritic synapse location</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00003</pub-id><pub-id pub-id-type="pmid">22319487</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Meeds</surname> <given-names>E</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Gps-abc: gaussian process surrogate approximate bayesian computation</article-title><conf-name>Conference on Uncertainty in Artificial Intelligence</conf-name></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meliza</surname> <given-names>CD</given-names></name><name><surname>Kostuk</surname> <given-names>M</given-names></name><name><surname>Huang</surname> <given-names>H</given-names></name><name><surname>Nogaret</surname> <given-names>A</given-names></name><name><surname>Margoliash</surname> <given-names>D</given-names></name><name><surname>Abarbanel</surname> <given-names>HD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Estimating parameters and predicting membrane voltages with conductance-based neuron models</article-title><source>Biological Cybernetics</source><volume>108</volume><fpage>495</fpage><lpage>516</lpage><pub-id pub-id-type="doi">10.1007/s00422-014-0615-5</pub-id><pub-id pub-id-type="pmid">24962080</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname> <given-names>CM</given-names></name><name><surname>Stryker</surname> <given-names>MP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Highly selective receptive fields in mouse visual cortex</article-title><source>Journal of Neuroscience</source><volume>28</volume><fpage>7520</fpage><lpage>7536</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0623-08.2008</pub-id><pub-id pub-id-type="pmid">18650330</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Williams</surname> <given-names>AH</given-names></name><name><surname>Franci</surname> <given-names>A</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Cell types, network homeostasis, and pathological compensation from a biologically plausible ion channel expression model</article-title><source>Neuron</source><volume>82</volume><fpage>809</fpage><lpage>821</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.002</pub-id><pub-id pub-id-type="pmid">24853940</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Sutton</surname> <given-names>AC</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Computational models in the age of large datasets</article-title><source>Current Opinion in Neurobiology</source><volume>32</volume><fpage>87</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.01.006</pub-id><pub-id pub-id-type="pmid">25637959</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Leary</surname> <given-names>T</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Temperature-Robust neural function from Activity-Dependent ion channel regulation</article-title><source>Current Biology</source><volume>26</volume><fpage>2935</fpage><lpage>2941</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.08.061</pub-id><pub-id pub-id-type="pmid">27746024</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Oesterle</surname> <given-names>J</given-names></name><name><surname>Behrens</surname> <given-names>C</given-names></name><name><surname>Schroeder</surname> <given-names>C</given-names></name><name><surname>Herrmann</surname> <given-names>T</given-names></name><name><surname>Euler</surname> <given-names>T</given-names></name><name><surname>Franke</surname> <given-names>K</given-names></name><name><surname>Smith</surname> <given-names>RG</given-names></name><name><surname>Zeck</surname> <given-names>G</given-names></name><name><surname>Berens</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Bayesian inference for biophysical neuron models enables stimulus optimization for retinal neuroprosthetics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.08.898759</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Leary</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Homeostasis, failure of homeostasis and degenerate ion channel regulation</article-title><source>Current Opinion in Physiology</source><volume>2</volume><fpage>129</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1016/j.cophys.2018.01.006</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname> <given-names>C</given-names></name><name><surname>O'Shea</surname> <given-names>DJ</given-names></name><name><surname>Collins</surname> <given-names>J</given-names></name><name><surname>Jozefowicz</surname> <given-names>R</given-names></name><name><surname>Stavisky</surname> <given-names>SD</given-names></name><name><surname>Kao</surname> <given-names>JC</given-names></name><name><surname>Trautmann</surname> <given-names>EM</given-names></name><name><surname>Kaufman</surname> <given-names>MT</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Hochberg</surname> <given-names>LR</given-names></name><name><surname>Henderson</surname> <given-names>JM</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><volume>15</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id><pub-id pub-id-type="pmid">30224673</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Maximum likelihood estimation of cascade point-process neural encoding models</article-title><source>Network: Computation in Neural Systems</source><volume>15</volume><fpage>243</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_15_4_002</pub-id><pub-id pub-id-type="pmid">15600233</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Pavlakou</surname> <given-names>T</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Masked autoregressive flow for density estimation</article-title><conf-name> Advances in Neural Information Processing Systems</conf-name><fpage>2338</fpage><lpage>2347</lpage></element-citation></ref><ref id="bib96"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Sterratt</surname> <given-names>D</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019a</year><article-title>Sequential neural likelihood: fast likelihood-free inference with autoregressive flows</article-title><conf-name>The 22nd International Conference on Artificial Intelligence and Statistics</conf-name><fpage>837</fpage><lpage>848</lpage></element-citation></ref><ref id="bib97"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Nalisnick</surname> <given-names>E</given-names></name><name><surname>Rezende</surname> <given-names>DJ</given-names></name><name><surname>Mohamed</surname> <given-names>S</given-names></name><name><surname>Lakshminarayanan</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019b</year><article-title>Normalizing flows for probabilistic modeling and inference</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1912.02762">https://arxiv.org/abs/1912.02762</ext-link></element-citation></ref><ref id="bib98"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Papamakarios</surname> <given-names>G</given-names></name><name><surname>Murray</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast ε-free inference of simulation models with bayesian conditional density estimation</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1028</fpage><lpage>1036</lpage></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Uzzell</surname> <given-names>VJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Prediction and decoding of retinal ganglion cell responses with a probabilistic spiking model</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>11003</fpage><lpage>11013</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3305-05.2005</pub-id><pub-id pub-id-type="pmid">16306413</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Likelihood-based approaches to modeling the neural code</article-title><conf-name>Bayesian Brain: Probabilistic Approaches to Neural Coding</conf-name><fpage>53</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.7551/mitpress/9780262042383.003.0003</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Shlens</surname> <given-names>J</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name><name><surname>Sher</surname> <given-names>A</given-names></name><name><surname>Litke</surname> <given-names>AM</given-names></name><name><surname>Chichilnisky</surname> <given-names>EJ</given-names></name><name><surname>Simoncelli</surname> <given-names>EP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Spatio-temporal correlations and visual signalling in a complete neuronal population</article-title><source>Nature</source><volume>454</volume><fpage>995</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/nature07140</pub-id><pub-id pub-id-type="pmid">18650810</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Scott</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fully bayesian inference for neural models with negative-binomial spiking</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1898</fpage><lpage>1906</lpage></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Podlaski</surname> <given-names>WF</given-names></name><name><surname>Seeholzer</surname> <given-names>A</given-names></name><name><surname>Groschner</surname> <given-names>LN</given-names></name><name><surname>Miesenböck</surname> <given-names>G</given-names></name><name><surname>Ranjan</surname> <given-names>R</given-names></name><name><surname>Vogels</surname> <given-names>TP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mapping the function of neuronal ion channels in model and experiment</article-title><source>eLife</source><volume>6</volume><elocation-id>e22152</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22152</pub-id><pub-id pub-id-type="pmid">28267430</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polson</surname> <given-names>NG</given-names></name><name><surname>Scott</surname> <given-names>JG</given-names></name><name><surname>Windle</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Bayesian inference for logistic models using Pólya–Gamma Latent Variables</article-title><source>Journal of the American Statistical Association</source><volume>108</volume><fpage>1339</fpage><lpage>1349</lpage><pub-id pub-id-type="doi">10.1080/01621459.2013.829001</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pospischil</surname> <given-names>M</given-names></name><name><surname>Toledo-Rodriguez</surname> <given-names>M</given-names></name><name><surname>Monier</surname> <given-names>C</given-names></name><name><surname>Piwkowska</surname> <given-names>Z</given-names></name><name><surname>Bal</surname> <given-names>T</given-names></name><name><surname>Frégnac</surname> <given-names>Y</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name><name><surname>Destexhe</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Minimal Hodgkin-Huxley type models for different classes of cortical and thalamic neurons</article-title><source>Biological Cybernetics</source><volume>99</volume><fpage>427</fpage><lpage>441</lpage><pub-id pub-id-type="doi">10.1007/s00422-008-0263-8</pub-id><pub-id pub-id-type="pmid">19011929</pub-id></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Potjans</surname> <given-names>TC</given-names></name><name><surname>Diesmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The cell-type specific cortical microcircuit: relating structure and activity in a full-scale spiking network model</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>785</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs358</pub-id><pub-id pub-id-type="pmid">23203991</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pozzorini</surname> <given-names>C</given-names></name><name><surname>Mensi</surname> <given-names>S</given-names></name><name><surname>Hagens</surname> <given-names>O</given-names></name><name><surname>Naud</surname> <given-names>R</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Automated High-Throughput characterization of single neurons by means of simplified spiking models</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004275</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004275</pub-id><pub-id pub-id-type="pmid">26083597</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname> <given-names>AA</given-names></name><name><surname>Billimoria</surname> <given-names>CP</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Alternative to hand-tuning conductance-based models: construction and analysis of databases of model neurons</article-title><source>Journal of Neurophysiology</source><volume>90</volume><fpage>3998</fpage><lpage>4015</lpage><pub-id pub-id-type="doi">10.1152/jn.00641.2003</pub-id><pub-id pub-id-type="pmid">12944532</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prinz</surname> <given-names>AA</given-names></name><name><surname>Bucher</surname> <given-names>D</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Similar network activity from disparate circuit parameters</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>1345</fpage><lpage>1352</lpage><pub-id pub-id-type="doi">10.1038/nn1352</pub-id><pub-id pub-id-type="pmid">15558066</pub-id></element-citation></ref><ref id="bib110"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pritchard</surname> <given-names>JK</given-names></name><name><surname>Seielstad</surname> <given-names>MT</given-names></name><name><surname>Perez-Lezaun</surname> <given-names>A</given-names></name><name><surname>Feldman</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Population growth of human Y chromosomes: a study of Y chromosome microsatellites</article-title><source>Molecular Biology and Evolution</source><volume>16</volume><fpage>1791</fpage><lpage>1798</lpage><pub-id pub-id-type="doi">10.1093/oxfordjournals.molbev.a026091</pub-id><pub-id pub-id-type="pmid">10605120</pub-id></element-citation></ref><ref id="bib111"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranjan</surname> <given-names>R</given-names></name><name><surname>Logette</surname> <given-names>E</given-names></name><name><surname>Marani</surname> <given-names>M</given-names></name><name><surname>Herzog</surname> <given-names>M</given-names></name><name><surname>Tâche</surname> <given-names>V</given-names></name><name><surname>Scantamburlo</surname> <given-names>E</given-names></name><name><surname>Buchillier</surname> <given-names>V</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A kinetic map of the homomeric Voltage-Gated potassium channel (Kv) Family</article-title><source>Frontiers in Cellular Neuroscience</source><volume>13</volume><elocation-id>358</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2019.00358</pub-id><pub-id pub-id-type="pmid">31481875</pub-id></element-citation></ref><ref id="bib112"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The diffusion decision model: theory and data for two-choice decision tasks</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1162/neco.2008.12-06-420</pub-id><pub-id pub-id-type="pmid">18085991</pub-id></element-citation></ref><ref id="bib113"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>René</surname> <given-names>A</given-names></name><name><surname>Longtin</surname> <given-names>A</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Inference of a mesoscopic population model from population spike trains</article-title><source>Neural Computation</source><volume>32</volume><fpage>1448</fpage><lpage>1498</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01292</pub-id></element-citation></ref><ref id="bib114"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Rezende</surname> <given-names>DJ</given-names></name><name><surname>Mohamed</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Variational inference with normalizing flows</article-title><conf-name>Proceedings of the 32nd International Conference on International Conference on Machine Learning</conf-name><fpage>1530</fpage><lpage>1538</lpage></element-citation></ref><ref id="bib115"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname> <given-names>JB</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>The gradient projection method for nonlinear programming. Part I. Linear constraints</article-title><source>Journal of the Society for Industrial and Applied Mathematics</source><volume>8</volume><fpage>181</fpage><lpage>217</lpage><pub-id pub-id-type="doi">10.1137/0108011</pub-id></element-citation></ref><ref id="bib116"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Goodman</surname> <given-names>DF</given-names></name><name><surname>Fontaine</surname> <given-names>B</given-names></name><name><surname>Platkiewicz</surname> <given-names>J</given-names></name><name><surname>Magnusson</surname> <given-names>AK</given-names></name><name><surname>Brette</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Fitting neuron models to spike trains</article-title><source>Frontiers in Neuroscience</source><volume>5</volume><elocation-id>9</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2011.00009</pub-id><pub-id pub-id-type="pmid">21415925</pub-id></element-citation></ref><ref id="bib117"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname> <given-names>DB</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Bayesianly justifiable and relevant frequency calculations for the applied statistician</article-title><source>The Annals of Statistics</source><volume>12</volume><fpage>1151</fpage><lpage>1172</lpage><pub-id pub-id-type="doi">10.1214/aos/1176346785</pub-id></element-citation></ref><ref id="bib118"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname> <given-names>DB</given-names></name><name><surname>Van Hooser</surname> <given-names>SD</given-names></name><name><surname>Miller</surname> <given-names>KD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The stabilized supralinear network: a unifying circuit motif underlying multi-input integration in sensory cortex</article-title><source>Neuron</source><volume>85</volume><fpage>402</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.026</pub-id><pub-id pub-id-type="pmid">25611511</pub-id></element-citation></ref><ref id="bib119"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schneidman</surname> <given-names>E</given-names></name><name><surname>Berry</surname> <given-names>MJ</given-names></name><name><surname>Segev</surname> <given-names>R</given-names></name><name><surname>Bialek</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Weak pairwise correlations imply strongly correlated network states in a neural population</article-title><source>Nature</source><volume>440</volume><fpage>1007</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1038/nature04701</pub-id><pub-id pub-id-type="pmid">16625187</pub-id></element-citation></ref><ref id="bib120"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schröder</surname> <given-names>C</given-names></name><name><surname>Lagnado</surname> <given-names>L</given-names></name><name><surname>James</surname> <given-names>B</given-names></name><name><surname>Berens</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Approximate bayesian inference for a mechanistic model of vesicle release at a ribbon synapse</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/669218</pub-id></element-citation></ref><ref id="bib121"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simonyan</surname> <given-names>K</given-names></name><name><surname>Zisserman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Very deep convolutional networks for large-scale image recognition</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib122"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sisson</surname> <given-names>SA</given-names></name><name><surname>Fan</surname> <given-names>Y</given-names></name><name><surname>Tanaka</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sequential monte carlo without likelihoods</article-title><source>PNAS</source><volume>104</volume><fpage>1760</fpage><lpage>1765</lpage><pub-id pub-id-type="doi">10.1073/pnas.0607208104</pub-id><pub-id pub-id-type="pmid">17264216</pub-id></element-citation></ref><ref id="bib123"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Speiser</surname> <given-names>A</given-names></name><name><surname>Yan</surname> <given-names>J</given-names></name><name><surname>Archer</surname> <given-names>EW</given-names></name><name><surname>Buesing</surname> <given-names>L</given-names></name><name><surname>Turaga</surname> <given-names>SC</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fast amortized inference of neural activity from calcium imaging data with variational autoencoders</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>4024</fpage><lpage>4034</lpage></element-citation></ref><ref id="bib124"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sporns</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Contributions and challenges for network models in cognitive neuroscience</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>652</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1038/nn.3690</pub-id><pub-id pub-id-type="pmid">24686784</pub-id></element-citation></ref><ref id="bib125"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname> <given-names>C</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Bartho</surname> <given-names>P</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name><name><surname>Lesica</surname> <given-names>NA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Inhibitory control of correlated intrinsic variability in cortical networks</article-title><source>eLife</source><volume>5</volume><elocation-id>e19695</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.19695</pub-id><pub-id pub-id-type="pmid">27926356</pub-id></element-citation></ref><ref id="bib126"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suk</surname> <given-names>HJ</given-names></name><name><surname>Boyden</surname> <given-names>ES</given-names></name><name><surname>van Welie</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Advances in the automation of whole-cell patch clamp technology</article-title><source>Journal of Neuroscience Methods</source><volume>326</volume><elocation-id>108357</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108357</pub-id><pub-id pub-id-type="pmid">31336060</pub-id></element-citation></ref><ref id="bib127"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Generating coherent patterns of activity from chaotic neural networks</article-title><source>Neuron</source><volume>63</volume><fpage>544</fpage><lpage>557</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.07.018</pub-id><pub-id pub-id-type="pmid">19709635</pub-id></element-citation></ref><ref id="bib128"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Opening the black box: low-dimensional dynamics in high-dimensional recurrent neural networks</article-title><source>Neural Computation</source><volume>25</volume><fpage>626</fpage><lpage>649</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00409</pub-id><pub-id pub-id-type="pmid">23272922</pub-id></element-citation></ref><ref id="bib129"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Talts</surname> <given-names>S</given-names></name><name><surname>Betancourt</surname> <given-names>M</given-names></name><name><surname>Simpson</surname> <given-names>D</given-names></name><name><surname>Vehtari</surname> <given-names>A</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Validating bayesian inference algorithms with simulation-based calibration</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1804.06788">https://arxiv.org/abs/1804.06788</ext-link></element-citation></ref><ref id="bib130"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname> <given-names>AL</given-names></name><name><surname>Hickey</surname> <given-names>TJ</given-names></name><name><surname>Prinz</surname> <given-names>AA</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Structure and visualization of high-dimensional conductance spaces</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>891</fpage><lpage>905</lpage><pub-id pub-id-type="doi">10.1152/jn.00367.2006</pub-id><pub-id pub-id-type="pmid">16687617</pub-id></element-citation></ref><ref id="bib131"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taylor</surname> <given-names>AL</given-names></name><name><surname>Goaillard</surname> <given-names>JM</given-names></name><name><surname>Marder</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How multiple conductances determine electrophysiological properties in a multicompartment model</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>5573</fpage><lpage>5586</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4438-08.2009</pub-id><pub-id pub-id-type="pmid">19403824</pub-id></element-citation></ref><ref id="bib132"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Teeter</surname> <given-names>C</given-names></name><name><surname>Iyer</surname> <given-names>R</given-names></name><name><surname>Menon</surname> <given-names>V</given-names></name><name><surname>Gouwens</surname> <given-names>N</given-names></name><name><surname>Feng</surname> <given-names>D</given-names></name><name><surname>Berg</surname> <given-names>J</given-names></name><name><surname>Szafer</surname> <given-names>A</given-names></name><name><surname>Cain</surname> <given-names>N</given-names></name><name><surname>Zeng</surname> <given-names>H</given-names></name><name><surname>Hawrylycz</surname> <given-names>M</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>Mihalas</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Generalized leaky integrate-and-fire models classify multiple neuron types</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>709</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-017-02717-4</pub-id><pub-id pub-id-type="pmid">29459723</pub-id></element-citation></ref><ref id="bib133"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tejero-Cantero</surname> <given-names>A</given-names></name><name><surname>Boelts</surname> <given-names>J</given-names></name><name><surname>Deistler</surname> <given-names>M</given-names></name><name><surname>Lueckmann</surname> <given-names>J-M</given-names></name><name><surname>Durkan</surname> <given-names>C</given-names></name><name><surname>Gonçalves</surname> <given-names>PJ</given-names></name><name><surname>Greenberg</surname> <given-names>DS</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Sbi: a toolkit for simulation-based inference</article-title><source>Journal of Open Source Software</source><volume>5</volume><elocation-id>2505</elocation-id><pub-id pub-id-type="doi">10.21105/joss.02505</pub-id></element-citation></ref><ref id="bib134"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tomm</surname> <given-names>C</given-names></name><name><surname>Avermann</surname> <given-names>M</given-names></name><name><surname>Vogels</surname> <given-names>T</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name><name><surname>Petersen</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The influence of structure on the response properties of biologically plausible neural network models</article-title><source>BMC Neuroscience</source><volume>12</volume><elocation-id>P30</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-12-S1-P30</pub-id></element-citation></ref><ref id="bib135"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Truccolo</surname> <given-names>W</given-names></name><name><surname>Eden</surname> <given-names>UT</given-names></name><name><surname>Fellows</surname> <given-names>MR</given-names></name><name><surname>Donoghue</surname> <given-names>JP</given-names></name><name><surname>Brown</surname> <given-names>EN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A point process framework for relating neural spiking activity to spiking history, neural ensemble, and extrinsic covariate effects</article-title><source>Journal of Neurophysiology</source><volume>93</volume><fpage>1074</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1152/jn.00697.2004</pub-id><pub-id pub-id-type="pmid">15356183</pub-id></element-citation></ref><ref id="bib136"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Geit</surname> <given-names>W</given-names></name><name><surname>Gevaert</surname> <given-names>M</given-names></name><name><surname>Chindemi</surname> <given-names>G</given-names></name><name><surname>Rössert</surname> <given-names>C</given-names></name><name><surname>Courcol</surname> <given-names>JD</given-names></name><name><surname>Muller</surname> <given-names>EB</given-names></name><name><surname>Schürmann</surname> <given-names>F</given-names></name><name><surname>Segev</surname> <given-names>I</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>BluePyOpt: leveraging open source software and cloud infrastructure to optimise model parameters in neuroscience</article-title><source>Frontiers in Neuroinformatics</source><volume>10</volume><elocation-id>17</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2016.00017</pub-id><pub-id pub-id-type="pmid">27375471</pub-id></element-citation></ref><ref id="bib137"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname> <given-names>C</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="bib138"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname> <given-names>TP</given-names></name><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural network dynamics</article-title><source>Annual Review of Neuroscience</source><volume>28</volume><fpage>357</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.28.061604.135637</pub-id><pub-id pub-id-type="pmid">16022600</pub-id></element-citation></ref><ref id="bib139"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision making in recurrent neuronal circuits</article-title><source>Neuron</source><volume>60</volume><fpage>215</fpage><lpage>234</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.034</pub-id><pub-id pub-id-type="pmid">18957215</pub-id></element-citation></ref><ref id="bib140"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Webb</surname> <given-names>S</given-names></name><name><surname>Golinski</surname> <given-names>A</given-names></name><name><surname>Zinkov</surname> <given-names>R</given-names></name><name><surname>Narayanaswamy</surname> <given-names>S</given-names></name><name><surname>Rainforth</surname> <given-names>T</given-names></name><name><surname>Teh</surname> <given-names>YW</given-names></name><name><surname>Wood</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Faithful inversion of generative models for effective amortized inference</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>3070</fpage><lpage>3080</lpage></element-citation></ref><ref id="bib141"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wilkinson</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Accelerating abc methods using gaussian processes</article-title><conf-name>AISTATS</conf-name></element-citation></ref><ref id="bib142"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wood</surname> <given-names>SN</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Statistical inference for noisy nonlinear ecological dynamic systems</article-title><source>Nature</source><volume>466</volume><fpage>1102</fpage><lpage>1104</lpage><pub-id pub-id-type="doi">10.1038/nature09319</pub-id><pub-id pub-id-type="pmid">20703226</pub-id></element-citation></ref><ref id="bib143"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname> <given-names>BM</given-names></name><name><surname>Cunningham</surname> <given-names>JP</given-names></name><name><surname>Santhanam</surname> <given-names>G</given-names></name><name><surname>Ryu</surname> <given-names>SI</given-names></name><name><surname>Shenoy</surname> <given-names>KV</given-names></name><name><surname>Sahani</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gaussian-process factor analysis for low-dimensional single-trial analysis of neural population activity</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>614</fpage><lpage>635</lpage><pub-id pub-id-type="doi">10.1152/jn.90941.2008</pub-id><pub-id pub-id-type="pmid">19357332</pub-id></element-citation></ref><ref id="bib144"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zitzler</surname> <given-names>E</given-names></name><name><surname>Künzli</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Indicator-based selection in multiobjective search</article-title><conf-name>International Conference on Parallel Problem Solving From Nature</conf-name><fpage>832</fpage><lpage>842</lpage></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Comparison between SNPE-estimated posterior and reference posterior (obtained via MCMC) on LN model.</title><p>(<bold>a</bold>) Posterior mean ± one standard deviation of temporal filter (receptive field) from SNPE posterior (SNPE, blue) and reference posterior (MCMC, yellow). (<bold>b</bold>) Full covariance matrices from SNPE posterior (left) and reference (MCMC, right).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig1-v3.tif"/></fig><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Full posterior for LN model.</title><p>In green, ground-truth parameters. Marginals (blue lines) and 2D marginals for SNPE (contour lines correspond to 95% of the mass) and MCMC (yellow histograms).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig2-v3.tif"/></fig><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>LN model with additional data.</title><p>With additional data, posterior samples cluster more tightly around the true filter. From left to right and top to bottom, SNPE (blue) and MCMC (yellow, for reference) are applied to observations with more independent Bernoulli trials, leading to progressively tighter posteriors and posterior samples closer to the true filter (which is the same across panels). Mean ± one standard deviation is shown. Note that SNPE closely agrees with the MCMC reference solution in all cases (<bold>a-d</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig3-v3.tif"/></fig><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Full posterior for Gabor GLM receptive field model.</title><p>SNPE posterior estimate (blue lines) compared to reference posterior (MCMC, histograms). Ground-truth parameters used to simulate the data in green. We depict the distributions over the original receptive field parameters, whereas we estimate the posterior as a Gaussian mixture over transformed parameters, see Materials and methods for details. We find that a (back-transformed) Gaussian mixture with four components approximates the posterior well in this case.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig4-v3.tif"/></fig><fig id="app1fig5" position="float"><label>Appendix 1—figure 5.</label><caption><title>SMC-ABC posterior estimate for Gabor GLM receptive field model.</title><p>(<bold>a</bold>) Spike-triggered averages (STAs) and spike counts with closest distance to the observed data out of 10000 simulations with parameters sampled from the prior. Spike counts are comparable to the observed data (299 spikes), but receptive fields (contours) are not well constrained. (<bold>b</bold>) Results for SMC-ABC with a total of 10<sup>6</sup> simulations. Histograms of 1000 particles (orange) returned in the final iteration of SMC-ABC, compared to prior (red contour lines) and ground-truth parameters (green). Distributions over (log-/logit-)transformed parameters, axis limits scaled to mean ± 3 standard deviations of the prior. (<bold>c</bold>) Correlations between ground-truth receptive field and receptive fields sampled from SMC-ABC posterior (orange), SNPE posterior (blue), reference MCMC posterior (yellow) and prior (red). The SNPE-estimated receptive fields are almost as good as those of the reference posterior, the SMC-ABC estimated ones no better than the prior.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig5-v3.tif"/></fig><fig id="app1fig6" position="float"><label>Appendix 1—figure 6.</label><caption><title>Full posterior for Gabor LN receptive field model on V1 recordings.</title><p>We depict the distributions over the receptive field parameters, derived from the Gaussian mixture over transformed-parameters (see Materials and methods for details).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig6-v3.tif"/></fig><fig id="app1fig7" position="float"><label>Appendix 1—figure 7.</label><caption><title>Summary results on 350 ICG channel models, and comparison with direct fits.</title><p>We generate predictions either with the posterior mode (blue) or with parameters obtained by directly fitting steady-state activation and time-constant curves (yellow). We calculate the correlation coefficient (CC) between observation and prediction. The distribution of CCs is similar for both approaches.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig7-v3.tif"/></fig><fig id="app1fig8" position="float"><label>Appendix 1—figure 8.</label><caption><title>Full posteriors for Hodgkin-Huxley model for 1, 4, and 7 features.</title><p>Images show the pairwise marginals for 7 features. Each contour line corresponds to 68% density mass for a different inferred posterior. Light blue corresponds to 1 feature and dark blue to 7 features. Ground truth parameters in green.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig8-v3.tif"/></fig><fig id="app1fig9" position="float"><label>Appendix 1—figure 9.</label><caption><title>Full posteriors for Hodgkin-Huxley model on 8 different recordings from Allen Cell Type Database.</title><p>Images show the pairwise marginals for 7 features. Each contour line corresponds to 68% density mass for a different inferred posterior.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig9-v3.tif"/></fig><fig id="app1fig10" position="float"><label>Appendix 1—figure 10.</label><caption><title>Comparison between SNPE posterior and IBEA samples for Hodgkin-Huxley model with 8 parameters and 7 features.</title><p>(<bold>a</bold>) Full SNPE posterior distribution. Ground truth parameters in green and IBEA 10 parameters with highest fitness (‘hall-of-fame’) in orange. (<bold>b</bold>) Blue contour line corresponds to 68% density mass for SNPE posterior. Light orange corresponds to IBEA sampled parameters with lowest IBEA fitness and dark orange to IBEA sampled parameters with highest IBEA fitness. This plot shows that, in general, SNPE and IBEA can return very different answers– this is not surprising, as both algorithms have different objectives, but this highlights that genetic algorithms do not in general perform statistical inference. (<bold>c</bold>) Traces for samples with high probability under SNPE posterior (purple), and for samples with high fitness under IBEA objective (hall-of-fame; orange traces). (<bold>d</bold>) Features for the desired output (observation), the mode of the inferred posterior (purple) and the best sample under IBEA objective (orange). Each voltage feature is normalized by <inline-formula><mml:math id="inf252"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>f \! PRIOR</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the standard deviation of the respective feature of simulations sampled from the prior.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig10-v3.tif"/></fig><fig id="app1fig11" position="float"><label>Appendix 1—figure 11.</label><caption><title>Full posterior for the stomatogastric ganglion over 24 membrane and 7 synaptic conductances.</title><p>The first 24 dimensions depict membrane conductances (top left), the last 7 depict synaptic conductances (bottom right). All synaptic conductances are logarithmically spaced. Between two samples from the posterior with high posterior probability (purple dots), there is a path of high posterior probability (white).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig11-v3.tif"/></fig><fig id="app1fig12" position="float"><label>Appendix 1—figure 12.</label><caption><title>Identifying directions of sloppiness and stiffness in the pyloric network of the crustacean stomatogastric ganglion.</title><p>(<bold>a</bold>) Minimal and maximal values of all summary statistics along the path lying in regions of high posterior probability, sampled at 20 evenly spaced points. Summary statistics change only little. The summary statistics are z-scored with the mean and standard deviation of 170,000 bursting samples in the created dataset. (<bold>b</bold>) Summary statistics sampled at 20 evenly spaced points along the orthogonal path. The summary statistics show stronger changes than in panel a and, in particular, often could not be defined because neurons bursted irregularly, as indicated by an ‘x’ above barplots. (<bold>c</bold>) Minimal and maximal values of the circuit parameters along the path lying in regions of high posterior probability. Both membrane conductances (left) and synaptic conductances (right) vary over large ranges. Axes as in panel (<bold>d</bold>). (<bold>d</bold>) Circuit parameters along the orthogonal path. The difference between the minimal and maximal value is much smaller than in panel (<bold>c</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig12-v3.tif"/></fig><fig id="app1fig13" position="float"><label>Appendix 1—figure 13.</label><caption><title>Posterior probability along high probability and orthogonal path.</title><p>Along the path that was optimized to lie in regions of high posterior probability (purple), the posterior probability remains relatively constant. Along the orthogonal path (pink), optimized to quickly reduce posterior probability, the probability quickly drops. The start and end points as well as the points labeled 1 and 2 correspond to the points shown in <xref ref-type="fig" rid="fig5">Figure 5c</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig13-v3.tif"/></fig><fig id="app1fig14" position="float"><label>Appendix 1—figure 14.</label><caption><title>Evaluating circuit configurations in which parameters have been sampled independently.</title><p>(<bold>a</bold>) Factorized posterior, that is, posterior obtained by sampling each parameter independently from the associated marginals. Many of the pairwise marginals look similar to the full posterior shown in <xref ref-type="fig" rid="app1fig11">Appendix 1—figure 11</xref>, as the posterior correlations are low. (<bold>b</bold>) Samples from the factorized posterior– only a minority of these samples produce pyloric activity, highlighting the significance of the posterior correlations between parameters. (<bold>c</bold>) Left: summary features for 500 samples from the posterior. Boxplot for samples where all summary features are well-defined (80% of all samples). Right: summary features for 500 samples from the factorized posterior. Only 23% of these samples have well-defined summary features. The summary features from the factorized posterior have higher variation than the posterior ones. Summary features are z-scored using the mean and standard deviation of all samples in our training dataset obtained from prior samples. The boxplots indicate the maximum, 75% quantile, median, 25% quantile, and minimum. The green ‘x’ indicates the value of the experimental data (the observation, shown in <xref ref-type="fig" rid="fig5">Figure 5b</xref>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56261-app1-fig14-v3.tif"/></fig></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56261.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>O'Leary</surname><given-names>Timothy</given-names></name><role>Reviewing Editor</role><aff><institution>University of Cambridge</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Goldman</surname><given-names>Mark S</given-names></name><role>Reviewer</role><aff><institution>University of California at Davis</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The problem of relating dynamical models to data pervades science and is particularly challenging in neuroscience due to high uncertainty in model parameters, model structure and inherent biological variability. This study presents an approach to performing model parameter inference based on large scale simulation and deep neural networks that permits efficient querying of model behaviour and underlying parameter distribution once simulations have been performed. The usefulness is demonstrated on a number of challenging and relevant models.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Training deep neural density estimators to identify mechanistic models of neural dynamics&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by a Reviewing Editor and John Huguenard as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Mark S. Goldman (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>This is a generally well written paper showcasing methodology for inferring model parameter distributions that are compatible with specified behaviours. The methodology uses recent advances in machine learning to approximate a likelihood function over model parameters that can be reused to perform inference. The method is applied to a number of informative examples. Benchmarks provide a favourable comparison against some existing methods (approximate bayesian computation/sampling) and the required computational resources are described.</p><p>Some of the reviewers found the methodology hard to follow in places; certain details were lacking. Although the authors pointed out limitations, there were also queries about potential failure modes of this methodology and whether there were methodological heuristics that are non-obvious but important, such as a means of assessing how many samples might be required for a particular model.</p><p>The reviewers agreed that a brief revision addressing specific points on methodological clarity (outlined in the reviewer's comments) and commenting on these other points would be worthwhile.</p><p><italic>Reviewer #1:</italic></p><p>General Assessment: This paper introduces a machine learning technique which generalizes Bayesian approaches to mechanistic models which are complex, multiparameter, and which do not necessarily have probability distributions as their key outputs. This is an interesting and likely productive direction to investigate. They claim their method is scalable and adaptable and the paper applies it to several problems of increasing complexity. However, I am unable to judge their method, because almost no details are given. This is the case even for very general questions such as what they use in place of the likelihood function for Bayes rule, and what they mean by a trained density estimator. The method in this paper may or may not be worthy of publication, but – since it is a method paper – I think the paper would need to be rewritten in a manner that readers can understand both in generality and in detail what their method accomplishes.</p><p>Review of Goncalves et al.:</p><p>The authors introduce a machine learning technique for finding ensembles of parameter values of mechanistic models which are consistent with experimental data.</p><p>The core of their method is the “Sequential Neural Posterior Estimation” (SNPE) which takes in a mechanistic model, a (thing which resembles a) prior and summary data, does some mysterious machine-learning processing and spits out something like a posterior probability distribution.</p><p>I agree with the authors that large unwieldy mechanistic models likely require machine learning to find parameter ensembles. I also agree that it should be possible to develop Bayesian like approaches for models which are not naturally probabilistic, a problem they pose nicely, and which I have come across but not quite realized was of such importance. I also find their results on the gastric circuit interesting- particularly the finding that the many disparate parameter values that can all fit data seem to lie on a single plateau, rather than on distinct minima</p><p>However, I don't really know where to begin in evaluating SNPE. It feels as if there is a section missing- the Introduction talks in generalities about large mechanistic models and the need to address their difficulties, and then it jumps to discussing the particular applications of SNPE.</p><p>I don't know what the authors mean by posterior and prior, nor what they mean when they say their method works without a likelihood.</p><p>There is muddling of what their output is and what they do to achieve it, and I don't find either sufficiently explained. For example, machine learning could be used to approximate a posterior in the usual Bayesian sense. Still I would want to know what prior they are using what their likelihood function is and what posterior they are approximating, even if there is a black box in the middle. I don't understand what their p(θ|x) is an approximation to.</p><p><italic>Reviewer #2:</italic></p><p>General assessment</p><p>This is a really clear and well written paper with practical solutions for an important problem in theoretical neuroscience. In addition, the authors provide high quality code and documentation making it fairly straightforward for other groups to make use of these tools. I would expect that these tools would become widely adopted in the community. Indeed, I would love to have had this for many previous modelling studies and will certainly use it in the future.</p><p>Major comments</p><p>1) My main issue is that applying this method depends on how well the deep network does its job of estimating the density (which depends partly on the user specifying the correct architecture and parameters). The authors do discuss this under limitations, but it might be interesting to see what a failure would look like here and what are the scientific consequences of such a failure. I suspect the answer might partly be that you should always verify the resulting posterior distributions, but how practical is this in a 31-dimensional setting, for example? When the authors were working on the examples for this manuscript, how much work did it take to make sure that there were no failures (assuming they verified this) and how did they detect them? (For example, in subsection “Functional diversity of ion channels: efficient high-throughput inference” it's implied that the only failures in this example were because the Omnimodel wasn't able to fit some data.)</p><p>2) Would it be possible to use SNPE to automatically suggest the optimal experiment that would give the narrowest posterior distribution over parameters? This might already be what was suggested when citing papers that &quot;automatically construct informative summary features&quot;, I wasn't entirely sure.</p><p><italic>Reviewer #3:</italic></p><p>This paper addresses a major issue in fitting neuroscience models: how to identify the often degenerate, or nearly degenerate, set of parameters that can underlie a set of experimental observations. Whereas previous techniques often depended upon brute force explorations or special parametric forms or local linearizations to generate sets of parameters consistent with measured properties, the authors take advantage of advances in deep networks for use in estimating probability densities to address this problem. Overall, I think this paper and the complementary submission have the potential to be transformative contributions to model fitting efforts in neuroscience, and the ability of this work to handle cases that are not continuous is a plus. However, I think the paper needs to clarify/quantify how much of an advance this is over brute force methods (plus simple post-hoc analyses on the simulated database), given the heavy brute force aspect of generating the training set, and also whether there are biases in the method. Also, the writing could benefit from more focus on methodology (rather than on examples) to ease adoption of the method.</p><p>Substantive concerns:</p><p>1) In the example of Figure 1, the Appendix 1—figure 1A (which should appear in the main figure rather than the supplement) appears to show that the SNPE's mean temporal filter is considerably off from the true temporal value. Why is this the case? More generally, this suggests that more “ground truth” examples with direct side-by-side comparisons should be provided to assure that the estimates provided by this method are not significantly biased. For example, Figure 5 is wonderful in showing a sloppy/insensitive direction in parameter space that has sharp curvature. Could one do something similar to this and directly compare (for an example with smaller number of parameters, e.g. 4 or 5) the estimated probability distribution to a “ground truth” generated through a sufficiently dense grid search, for different numbers of training samples.</p><p>2) The authors need to clarify, quantitatively, how much of a benefit this is over simple post-hoc analysis of results of brute force methods that just randomly (uniformly) sample parameters, especially given that there is a lot of brute force work in this method as well in generating the samples for the deep network training. For such brute force methods (e.g. rather than a grid, some studies just uniformly randomly generate parameters to use in the simulations), one can also just post-hoc fit the results to a mixture of Gaussians or similar classical density estimator so that other values can be interpolated from the obtained data. How much is the deep network approach needed? My guess is that it does help, potentially a huge amount, but the authors need to show this in a carefully controlled comparison.</p><p>3) Related to #1, given that this paper is primarily a methodological advance, I think the authors could spend more time laying out the essence of the basic algorithm in the beginning of the Results. For example, it's not clear in Figure 1 exactly what is being input and output from the neural density estimator deep network. More generally, it's worth describing more about the algorithm, perhaps in less technical language, in the main results.</p><p>Within the Materials and methods, since the authors indicated some playing around with parameters was needed, it would help a lot for the adoption of this method by others to have more explanation of different features of the method and more intuition behind choices. For example, what is the loss function used after the initial round? Are there any basic constraints on what shape the deep network should have (e.g. should it fan out in the middle layers, fan in, or be approximately uniform in width)? What is the basic idea behind a masked autoregressive flow (MAF) and when should that be chosen versus the mixture of Gaussians model? The algorithm's description could also be expanded (or a short version could be in the main and expanded version in the Materials and methods).</p><p>4) Is there some way to ballpark estimate, or post-hoc characterize, how many samples may be needed? Seemingly, the fact that this works with less than one sample per dimension says something about the geometry of the solution space and its effective dimensionality. (If this seems too hard, the authors can punt on it).</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your article &quot;Training deep neural density estimators to identify mechanistic models of neural dynamics&quot; for consideration by <italic>eLife</italic>. Your revised article has been reviewed by two peer reviewers, and the evaluation has been overseen by a Reviewing Editor and John Huguenard as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Mark S Goldman (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus the revisions requested below only address clarity and presentation.</p><p>Summary</p><p>This is methodology is of potentially wide interest, and although the authors have made efforts to explain the interpretation, applicability and caveats of the methodology, one reviewer feels that the manuscript could benefit from further clarification on:</p><p>1) How one should interpret “posterior probability density” in cases where there is a deterministic relationship between parameters and behaviour, e.g. in extreme cases where there might be a single solution.</p><p>2) The fairness of comparisons with ABC/sampling – are these alternatives being compared in an even-handed way?</p><p>3) It may be helpful to have a very explicit example of the results SNPE returns on a problem where “ground truth” of feasible parameters are known. This could be a simple model dynamical system, used for illustration.</p><p>Full reviewer comments are included below, for reference.</p><p><italic>Reviewer #2:</italic></p><p>I'm very happy with the reviewers response to my previous review, and congratulate them on a great paper!</p><p><italic>Reviewer #3:</italic></p><p>The authors have clarified the methods in a way that I think should be helpful, and I remain optimistic that this will be a very valuable technique for the field. However, I think some of the other concerns raised about rigorously demonstrating the technique, and clarifying the method conceptually were not addressed. I will try to re-state these concerns in a broader scope that I hope strengthens the paper, clarifies potential issues, and gets at some key conceptual and rigor issues that need to be addressed</p><p>1) Conceptual question: The manuscript and reply heavily emphasize that SNPE performs Bayesian inference, as compared to approaches that only are applicable to deterministic modeling (like classic brute force approaches). However, outside of the very simple illustrative example of Figure 2, it appears that all of the remaining examples are done on deterministic models. This leads to a fundamental conceptual question: how should one interpret the resulting posteriors resulting from SNPE?</p><p>To be concrete, if one used the precise output of a simulation (rather than summary statistics), then the actual posterior probability over parameter values is a δ function (assuming the model if identifiable). If there are nearby solutions, this wouldn't change the true posterior probability, i.e. despite being nearby, they would have posterior probability of zero. Thus, while the authors criticize other methods for using “user defined” criteria of being close to a solution, this example illustrates that if one found the exact posterior probability (as the authors state in their reply that the method should ideally do if it works correctly), all information about the existence of nearby (i.e. almost, but not entirely, degenerate) solutions would be lost. I'm guessing the density estimator will somehow tradeoff “posterior probability large” for “close to the correct solution”, but this then requires some interpretation of how “closeness to the correct simulation output” is translated to “posterior probability”. Put in this light, at least a user defined criterion of closeness for a deterministic model, as done in traditional fitting methods, is explicit and easy to interpret. Can the authors justify or provide users with an explanation for how to interpret SNPE when applied to deterministic simulations-my guess is that, when SNPE is working well, it's some sort of smoothed version of the true, very spiky posterior, but is there a way for users to interpret the solution, e.g. how would a user know if there are nearby solutions if the SNPE did a great job of finding the true posterior and therefore didn't indicate these nearby solutions? And how should the user interpret “probabilities” in a deterministic model where there may only be 1 true solution?</p><p>Note: when summary statistics are used rather than exact simulation output, this will likely create some degeneracies. However, this still may end up creating a razor thin slice in parameter space that corresponds to truly, exactly matching the summary statistics, which would then be represented by the true posterior. This scenario then returns to the same fundamental questions of: a) how to interpret the posterior from SNPE, and b) how to interpret a simulation output that is very close to the summary statistics but not equal to them and therefore perhaps lost if SNPE is finding the true posterior of the deterministic model.</p><p>2) On the one example that is stochastic (Figure 2), I am still confused by the results shown. Specifically, in Figure 2C, the posterior samples consistently and systematically undershoot the true receptive field for intermediate parameter values. This is a simple LN model for which classic maximum likelihood models usually doing an excellent job finding the true receptive field, at least with enough data. If one ran standard maximum likelihood estimation on this, would it miss? If so, why? Is this reflecting that the authors are using too broad a prior and it is not converging to the true posterior or that too few samples are used (the number of samples is not indicated)-if the latter, can the authors run with more samples and show that their method then converges to the correct answer? Or is it something else (e.g. see next paragraph)?</p><p>In addition, the authors state that their method works because it gets the same answer as MCMC. However, I'm wondering if both might be making errors due to the use of the Polya-Γ sampling scheme since, at least in the Pillow and Scott article cited, this was used not for a standard LN model but rather to model more complex models in which the variance was greater than for a Poisson spike count. In any case, it seems worth explaining where the systematic error in the posterior samples is coming from, given that the LN model is such a simple classic model fit whose parameters have been successfully identified by simpler maximum likelihood methods.</p><p>3) Related to the Figure 2 example, I was struck by the huge errors and nearly zero correlation for the ABC methods. Is this fundamental to ABC or is this because the authors are hamstringing the ABC method by applying it in a very naïve manner that isn't how someone using ABC would apply it. Specifically, from the methods, it looks like the authors are considering every parameter to be independent, with no smoothing or regularization. I would think that someone using ABC methods would apply regularizations or the like that might dramatically reduce the effective dimensionality of the problem, so it's not clear if the comparison shown in the manuscript is fair. This isn't to say that SNPE won't still work much better than ABC methods, but it's worth checking to make sure the ABC methods are being applied in a realistic manner if the comparison is to be made.</p><p>4) Ground truthing. The authors suggest that they can't ground truth the technique due, in part, to the stochastic nature of it. While I appreciate there may be some challenges, a lot could be done, especially since most of the manuscript focuses on deterministic examples that are easier to check. For example, in Figures 3-5, parameter estimation results that may or may not be correct are shown and the reader is left to assume they are correct by looking at a single or very small number of examples. While I fully expect that it does work reliably, it would be nice to actually run a large set of simulations for posterior checks (even if, e.g., only the mode of the distribution was checked) and then report some statistics.</p><p>5) Related to ground truthing, in Figure 5D and the highlighted 2-D plot of Figure 5C, I am confused as to whether the example shows the success or partial failure of the method. Specifically, the point labeled &quot;2&quot; in the Figures 5C and 5D appears to be in a bright (and therefore presumably high probability?) region of the parameter space, both in an absolute sense and possibly (it's hard to tell) compared to the light purple circle simulation (and also relative to going further down below this light purple trace). However, the authors use point 2 to make the point that the simulation looks very far off from the reference trace. This suggests, possibly, that similar probabilities being output by SNPE is on the one hand (values close to the light purple circle) leading to a very good fit and on the other hand (red point 2) leading to a very bad fit – thus, this suggests that one perhaps can't trust “bright” regions in the SNPE posterior to correspond to “good” fits. Can the authors check this and address the more general concern. I'm guessing this may relate to my larger conceptual question of interpreting the “probabilities” for SNPE in my reviewer comment #1 above.</p><p>6) Difference from “brute force methods”. I thank the authors for trying to generate a comparison to brute force methods by showing a rejection method. However, I don't think this is necessary and I think it's confusing as I don't think “brute force” and “rejection” are the same (i.e. I'd remove this from the paper as being more confusing than helpful). To me at least, “brute force” methods simply generate many example simulations into a database. This is separable from the problem of “what does one then do with this database afterwards”. My point was simply that I think SNPE can be interpreted as being either within the class of brute force searches in generating a very large database of simulations (e.g. ~20 million for Figure 5), or if one focuses on the density estimation component, as being a method that is complementary to the brute force database generation step by addressing the “what does one then do with this database afterwards” component of the problem. For the “what does one do with this database” question, one can certainly imagine many things-one can simply visualize the parameter space through various slices, effectively marginalizing over certain variables and illustrating for example where bifurcations occur that might be difficult to identify mathematically, or can (as the authors highlight has often been done previously…but this should not be equated with parameter search methods, as it's just one possible post-search thing that can be done) implement some sort of selection/rejection criterion. One might also want to interpolate/extrapolate, asking for an estimate of what a new set of parameters would lead to, for which a density estimator such as SNPE would excel.</p><p>This is less a criticism of the approach and more of a clarification for placing the method within existing work. I agree with the authors that current use of brute force generation of databases has not taken advantage of better methods of post-database-generation analysis, as SNPE offers, leading to the likely (and potentially dramatically, as demonstrated in Figure 5) incorrect implicit assumption that one needs to generate many samples per dimension to well-characterize the space. I just wanted to separate the notion of generating a database of many examples (which doesn't need to be done on a grid), from the notion of how one post-hoc analyzes it. And overall, this focus on the power of post-database-generation analysis is why I still think SNPE, and the more general focus on use of density estimators, is a potentially transformative contribution for the practice of model building (in addition to SNPE's applicability to stochastic problems that is already well emphasized in the manuscript).</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56261.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>[…] I agree with the authors that large unwieldy mechanistic models likely require machine learning to find parameter ensembles. I also agree that it should be possible to develop Bayesian like approaches for models which are not naturally probabilistic, a problem they pose nicely, and which I have come across but not quite realized was of such importance. I also find their results on the gastric circuit interesting- particularly the finding that the many disparate parameter values that can all fit data seem to lie on a single plateau, rather than on distinct minima</p><p>However, I don't really know where to begin in evaluating SNPE. It feels as if there is a section missing- the Introduction talks in generalities about large mechanistic models and the need to address their difficulties, and then it jumps to discussing the particular applications of SNPE.</p><p>I don't know what the authors mean by posterior and prior, nor what they mean when they say their method works without a likelihood.</p><p>There is muddling of what their output is and what they do to achieve it, and I don't find either sufficiently explained. For example, machine learning could be used to approximate a posterior in the usual Bayesian sense. Still I would want to know what prior they are using what their likelihood function is and what posterior they are approximating, even if there is a black box in the middle. I don't understand what their p(θ|x) is an approximation to.</p></disp-quote><p>We thank reviewer 1 for the detailed feedback, which has led us to clarify the descriptions of our techniques in both the Results and Materials and methods sections, putting emphasis on (1) the methodological foundation and inner workings of the algorithm and (2) how prior distributions on model parameters are chosen.</p><p>To clarify: SNPE performs Bayesian inference “in the usual sense” of calculating the posterior distribution, that is, the probability distribution of model parameters given observed data. However, in contrast to other approaches based on explicit application of Bayes’ rule, SNPE calculates the posterior distribution without ever having to numerically evaluate or approximate the likelihood of parameters given data. Instead, it trains a neural network using an objective function and learning procedure that have been proved capable, in both theory and practice, of recovering the posterior from model simulations (see first section of Results, and Materials and methods).</p><p>Thus, our definitions and usage of the concepts of “prior” and “posterior” are the standard versions used in statistical data analysis, the only (but important!) difference is that we can approximate the posterior without having to evaluate the likelihood function.</p><p>In order to emphasize and clarify these points, we now start the Results section with a description of SNPE:</p><p>“SNPE performs Bayesian inference on mechanistic models using only model-simulations, without requiring likelihood evaluations. […] SNPE's efficiency can be further improved by using the running estimate of the posterior distribution to guide further simulations towards data-compatible regions of the parameter space (Papamakarios and Murray 2016, Lueckmann et al., 2017, Greenberg et al., 2019).”</p><p>In the Materials and methods, we now have a section (“Bayesian inference without likelihood-evaluations with SNPE”) describing in more detail the loss function and the diverse versions of SNPE algorithms.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>General assessment</p><p>This is a really clear and well written paper with practical solutions for an important problem in theoretical neuroscience. In addition, the authors provide high quality code and documentation making it fairly straightforward for other groups to make use of these tools. I would expect that these tools would become widely adopted in the community. Indeed, I would love to have had this for many previous modelling studies and will certainly use it in the future.</p></disp-quote><p>We thank reviewer 2 for evaluating our paper and are pleased to know that the reviewer considers the paper to be “well-written” and the tools to have the potential to be “widely adopted in the community.”</p><disp-quote content-type="editor-comment"><p>Major comments</p><p>1) My main issue is that applying this method depends on how well the deep network does its job of estimating the density (which depends partly on the user specifying the correct architecture and parameters). The authors do discuss this under limitations, but it might be interesting to see what a failure would look like here and what are the scientific consequences of such a failure. I suspect the answer might partly be that you should always verify the resulting posterior distributions, but how practical is this in a 31-dimensional setting, for example? When the authors were working on the examples for this manuscript, how much work did it take to make sure that there were no failures (assuming they verified this) and how did they detect them? (For example, in subsection “Functional diversity of ion channels: efficient high-throughput inference” it's implied that the only failures in this example were because the Omnimodel wasn't able to fit some data.)</p></disp-quote><p>The reviewer is correct to point out that this methodology has limitations, and this becomes more evident in high-dimensional parameter spaces. In the previous version of the manuscript, we included a Discussion section on limitations, and mentioned the use of posterior predictive checks to diagnose inference-failures:</p><p>“One common ingredient of these approaches is to sample from the inferred model, and search for systematic differences between observed and simulated data, e.g. to perform posterior predictive checks (Cook et al., 2006,Talts et al., 2018, Liepe et al., 2014, Lueckmann et al., 2017, Greenberg et al., 2019) (Figure 2G, Figure 3F,G, Figure 4C, and Figure 5D).”</p><p>In this approach, one samples from the inferred posterior given the observed data, simulates for these sampled parameters, and compares the simulations with the observed data. While “posterior predictive checks” are scalable to higher-dimensional parameter spaces, since one can sample efficiently with SNPE, this approach only allows one to detect whether a mode of the posterior has been wrongly identified, but fails at detecting when there is an “undiscovered” island of the posterior. Such issues are currently unsolved in the field of simulation-based inference and in Bayesian inference in general. For all applications, and especially in high-dimensional problems, it is good practice to resort to multiple initialisations of these algorithms in order to minimise the failure to detect parameter islands.</p><p>We have expanded the Discussion about limitations:</p><p>“These approaches allow one to detect “failures” of SNPE, i.e. cases in which samples from the posterior do not reproduce the data. However, when diagnosing any Bayesian inference approach, it is challenging to rigorously rule out the possibility that additional parameter-settings (e.g. in an isolated “island”) would also explain the data. Thus, it is good practice to use multiple initialisations of SNPE, and/or a large number of simulations in the initial round.”</p><p>We should emphasise that in the manuscript, we analyse a few test cases (Figure 2) where the posterior is known or calculable, cases that have similar dimensionality as other applications where the posterior is not known (the ion-channel model and the Hodgkin-Huxley model). This gives us confidence in the quality of the inferred posteriors.</p><disp-quote content-type="editor-comment"><p>2) Would it be possible to use SNPE to automatically suggest the optimal experiment that would give the narrowest posterior distribution over parameters? This might already be what was suggested when citing papers that &quot;automatically construct informative summary features&quot;, I wasn't entirely sure.</p></disp-quote><p>We thank the reviewer for the excellent point. SNPE could indeed be extended for Bayesian experimental design (see e.g. Kleinegesse and Gutmann, 2019 for a related approach), and we regard this as an exciting avenue for future work. However, we do think that this would constitute a major new research project that would be substantially beyond the scope of this study. We now cite this study when describing the possibilities brought by amortised inference:</p><p>“These results show how SNPE allows fast and accurate identification of biophysical model parameters on new data, and how SNPE can be deployed for applications requiring rapid automated inference, such as high-throughput screening-assays, closed-loop paradigms (e.g. for adaptive experimental manipulations or stimulus-selection (Kleinegesse and Gutmann, 2019)), or interactive software tools.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>This paper addresses a major issue in fitting neuroscience models: how to identify the often degenerate, or nearly degenerate, set of parameters that can underlie a set of experimental observations. Whereas previous techniques often depended upon brute force explorations or special parametric forms or local linearizations to generate sets of parameters consistent with measured properties, the authors take advantage of advances in deep networks for use in estimating probability densities to address this problem. Overall, I think this paper and the complementary submission have the potential to be transformative contributions to model fitting efforts in neuroscience, and the ability of this work to handle cases that are not continuous is a plus. However, I think the paper needs to clarify/quantify how much of an advance this is over brute force methods (plus simple post-hoc analyses on the simulated database), given the heavy brute force aspect of generating the training set, and also whether there are biases in the method. Also, the writing could benefit from more focus on methodology (rather than on examples) to ease adoption of the method.</p></disp-quote><p>We thank reviewer 3 for evaluating our paper and are pleased to know that the reviewer considers the methodology presented to have “the potential to be transformative”. We address the reviewer's specific comments below.</p><disp-quote content-type="editor-comment"><p>Substantive concerns:</p><p>1) In the example of Figure 1, the Appendix 1—figure 1A (which should appear in the main figure rather than the Appendix) appears to show that the SNPE's mean temporal filter is considerably off from the true temporal value. Why is this the case? More generally, this suggests that more “ground truth” examples with direct side-by-side comparisons should be provided to assure that the estimates provided by this method are not significantly biased. For example, Figure 5 is wonderful in showing a sloppy/insensitive direction in parameter space that has sharp curvature. Could one do something similar to this and directly compare (for an example with smaller number of parameters, e.g. 4 or 5) the estimated probability distribution to a “ground truth” generated through a sufficiently dense grid search, for different numbers of training samples.</p></disp-quote><p>The goal of SNPE is to perform Bayesian inference. Thus, if SNPE works correctly, the posterior distribution inferred by SNPE should be the same as the “true” posterior. In particular, the true SNPE-mean filter should be the same as the “true-posterior” mean filter (here computed with MCMC), which is the case in Appendix 1—figure 1A. So, Appendix 1—figure 1A shows that SNPE is working as intended.</p><p>For finite and noisy data, the posterior mean is not necessarily the same as the true parameters. In this setting, it is fundamentally impossible for any estimation procedure to recover the true parameters, and Bayesian inference therefore uses a compromise between the data and the prior. This has advantageous features on average, but for a single example, the posterior mean might deviate from the true parameters. These issues are well understood and widely appreciated in statistics, but we realize that we have not sufficiently made this clear in this figure.</p><p>(As a side note, this issue implies that verifying the correctness of SNPE is not as simple as “comparing true and recovered parameters” is the reason for why we have included extensive analyses on problems with known ground-truth posteriors and additional posterior predictive checks, see e.g. Figure 2C,D,F,G,H and Appendix 1—figures 1,2,4).</p><p>In order to clarify that perfect inference does not necessarily mean recovering the true underlying parameters, we now write in the respective Results sub-section:</p><p>“If SNPE works correctly, its posterior mean filter will match that of the reference posterior, however, it is not to be expected that either of them precisely matches the ground-truth filter (Figure 2c and Appendix 1—figure 1): In the presence of finite sampling and stochasticity, multiple different filters could have plausibly given rise to the observed data. A properly inferred posterior will reflect this uncertainty, and include the true filters as one of many plausible explanations of the data (but not necessarily as the “mean” of all plausible explanations) (Appendix 1—figure 2).”</p><p>Regarding moving Appendix 1—figure 1A to Figure 2, we think that the posterior samples plotted in Figure 2C alongside the true temporal filter, already allow one to appreciate the respective similarities and differences.</p><p>Regarding the comparison of our approach on a (simpler) STG model to a dense grid-search, we would like to point out that the grid search would require the probabilistic mapping of the parameters to the summary statistics, i.e. the likelihood function, which is intractable in the STG case and is therefore not possible to do at this point. Thus, in any problems with intractable likelihoods, it is intractable to compute the “true posterior” to “prove” correctness, but we can do a range of posterior predictive checks (as shown in the manuscript), e.g. showing that for high-probability under the STG posterior, the simulated traces are similar to the observed data (Figure 5C,D, and Appendix 1—figure 12), whereas orthogonal paths lead very quickly to simulated traces different from the observed data. This suggests that the inferred STG posterior is capturing correctly at least one region of high probability.</p><disp-quote content-type="editor-comment"><p>2) The authors need to clarify, quantitatively, how much of a benefit this is over simple post-hoc analysis of results of brute force methods that just randomly (uniformly) sample parameters, especially given that there is a lot of brute force work in this method as well in generating the samples for the deep network training. For such brute force methods (e.g. rather than a grid, some studies just uniformly randomly generate parameters to use in the simulations), one can also just post-hoc fit the results to a mixture of Gaussians or similar classical density estimator so that other values can be interpolated from the obtained data. How much is the deep network approach needed? My guess is that it does help, potentially a huge amount, but the authors need to show this in a carefully controlled comparison.</p></disp-quote><p>We thank the reviewer for the excellent comment. We would like to emphasise that SNPE performs Bayesian inference on stochastic models, whereas “brute-force” approaches are generally only applied to deterministic models, and based on heuristically chosen distance measures. If applied to stochastic models, such brute-force approaches can be interpreted as rejection-ABC, which rejects simulations according to the chosen distance measure, and which we show in Figure 2 to perform poorly compared to SNPE. This suggests that the big advantage of SNPE arises from the fact that (1) it uses all simulations independently of the similarity to the empirical data, and (2) it learns to identify the features most informative about the parameters.</p><p>We now have an extensive Discussion on these aspects:</p><p>“How big is the advance brought by SNPE relative to “conventional” brute-force approaches that aim to exhaustively explore parameter space? […] Finally, we showed above that, on the STG circuit, SNPE also yielded solutions more similar to experimental data than a brute-force approach (based on fitting a mixture-model to accepted simulations).”</p><p>Furthermore, we provide a comparison between SNPE and an approach where we combined rejection ABC with mixture-of-Gaussians density estimation. To do so, we used the dataset of simulations obtained for the crustacean stomatogastric ganglion by​ sampling from the prior and picked the 10000 parameter sets that produced the closest summary features to the observed data. As a distance metric, we used the Euclidean distance over summary features after having standardized them. We then fitted a mixture of Gaussians with 20 components to these 10000 samples using the Python machine learning library scikit-learn. We found that samples from the SNPE posterior have summary statistics that are substantially closer to the observed data than samples from the mixture-of-Gaussians approach. We report these new results as Appendix 1—figure 14 and a respective paragraph in the Results:</p><p>“How much does the conditional density estimation network contribute to posterior estimation? […] In the next section, we will furthermore show how such a powerful posterior representation can be used to explore the properties of neural dynamics.”</p><disp-quote content-type="editor-comment"><p>3) Related to #1, given that this paper is primarily a methodological advance, I think the authors could spend more time laying out the essence of the basic algorithm in the beginning of the Results. For example, it's not clear in Figure 1 exactly what is being input and output from the neural density estimator deep network. More generally, it's worth describing more about the algorithm, perhaps in less technical language, in the main results.</p></disp-quote><p>As replied to reviewer 1’s comments, we have clarified the methodology in Results. We now start the Results section with a description of SNPE:</p><p>“SNPE performs Bayesian inference on mechanistic models using only model-simulations, without requiring likelihood evaluations. […] SNPE's efficiency can be further improved by using the running estimate of the posterior distribution to guide further simulations towards data-compatible regions of the parameter space (Papamakarios and Murray, 2016, Lueckmann et al., 2017, Greenberg et al., 2019).”</p><disp-quote content-type="editor-comment"><p>Within the Materials and methods, since the authors indicated some playing around with parameters was needed, it would help a lot for the adoption of this method by others to have more explanation of different features of the method and more intuition behind choices. For example, what is the loss function used after the initial round? Are there any basic constraints on what shape the deep network should have (e.g. should it fan out in the middle layers, fan in, or be approximately uniform in width)? What is the basic idea behind a masked autoregressive flow (MAF) and when should that be chosen versus the mixture of Gaussians model? The algorithm's description could also be expanded (or a short version could be in the main and expanded version in the Materials and methods).</p></disp-quote><p>We have extended the Materials and methods section to address reviewer’s questions:</p><p>1) We now provide more details on the loss functions for SNPE-A/B/C, and point to previous literature showing how minimising the loss function can be used to perform Bayesian inference without likelihood evaluations;</p><p>2) We include a new section about “Neural density estimation”, where we discuss the constraints on the shape of the mixture density networks:</p><p>“The MDN outputs the parameters of a mixture of Gaussians (i.e. mixture weights, and for each component of the mixture, the mean vector and covariance entries). Thus, for an MDN composed of K components, we chose an architecture with at least as many units per layer as K( 1 + dim(θ) + dim(θ)(dim(θ)+1)/2 ) – 1, to ensure enough flexibility to approximate well the parameters of the mixture of Gaussians. For example, when inferring the parameters of the Hodgkin-Huxley model given in vitro recordings from mouse cortex (Allen Cell Types Database, https://celltypes.brain-map.org/data), we infer the posterior over 8 parameters with a mixture of two Gaussians, and the MDN needs at least 89 units per layer. Across applications, we found 2 layers to be sufficient to appropriately approximate the posterior distribution.”</p><p>3) We also describe the basic idea of MAFs and the respective constraints:</p><p>“MAF is a specific type of normalizing flow, which is a highly flexible density estimator (Rezende et al., 2015; Papamakarios et al., 2017, Papamakarios et al., 2019). Normalizing flows consist of a stack of bijections which transform a simple distribution (usually a multivariate Gaussian distribution) into the target distribution. Each bijection is parameterized by a specific type of neural network (for MAF: a Masked Autoencoder for Distribution Estimation, or MADE). In our experiments, 5 stacked bijections are enough to approximate even complex posterior distributions. Depending on the size of the parameter and data space, each neural network had between (50,50) and (200,400) hidden units.”</p><p>4) Regarding the choice between MDNs and MAFs, we write:</p><p>“When using SNPE in a single-round, we generally found superior performance for MAFs as compared to MDNs. When running inference across multiple rounds, training MAFs leads to additional challenges which might impede the quality of inference (Greenberg et al., 2019; Durkan et al., 2020)”</p><disp-quote content-type="editor-comment"><p>4) Is there some way to ballpark estimate, or post-hoc characterize, how many samples may be needed? Seemingly, the fact that this works with less than one sample per dimension says something about the geometry of the solution space and its effective dimensionality. (If this seems too hard, the authors can punt on it).</p></disp-quote><p>The reviewer is raising an important point. SNPE is using neural density estimators to approximate the function between summary statistics and a distribution over parameters. Thus, in general, the higher the dimensionality and complexity of this function, the more flexible the neural density estimator should be, and therefore the higher the required number of simulations to appropriately fit such a function. Given the unavailability of such a function a priori, it is in general hard to provide a definite answer. In our workflow, we have a few heuristics, which we now clarify in the manuscript, in the Materials and methods, section “Simulation-based inference”:</p><p>“The required number of simulations depends on both the dimensionality and complexity of the function between summary statistics and model parameters. While the number of parameters and summary-features can easily be determined, it can be hard to determine how “complex” (or nonlinear) this mapping is, this makes it difficult to give general guidelines on how many simulations will be required. A practical approach is to choose a simulation-budget based on the computational cost of the simulation, inspect the results (e.g. with posterior predictive checks), and add more simulations when it seems necessary.”</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Summary</p><p>This is methodology is of potentially wide interest, and although the authors have made efforts to explain the interpretation, applicability and caveats of the methodology, one reviewer feels that the manuscript could benefit from further clarification on:</p></disp-quote><p>We thank the editors and reviewers for their constructive comments. We are glad that reviewer 2 was “very happy” with our response and hope to address any remaining concerns by reviewer 3 both in this response and by making the corresponding changes in the manuscript.</p><p>Some main concerns seem to be based on the (incorrect) assumption that we deal with deterministic models—this is not the case, all of our models are stochastic. They are only “likelihood-free” in the sense that the likelihood cannot be evaluated (but mathematically it exists). This is addressed in detail in our response below. In addition, we made sure this is now even more clear from the manuscript. We hope to clear up the confusion about the example with ground truth, showing that SNPE indeed agrees with reference solutions and posterior uncertainty shrinks in light of more data, and we clarify fairness of comparisons with ABC/sampling.</p><disp-quote content-type="editor-comment"><p>1) How one should interpret “posterior probability density” in cases where there is a deterministic relationship between parameters and behaviour, e.g. in extreme cases where there might be a single solution.</p></disp-quote><p>SNPE is designed to perform Bayesian inference, i.e. to calculate the posterior distribution on stochastic models. While, in principle, one can also apply SNPE to deterministic models, we believe that a full investigation of how it behaves under these circumstances, and how to interpret its outputs, is outside of the scope of this paper. Furthermore, we would claim that having some model of variability is paramount for any data analysis, as there can never be a perfect match between model and data.</p><disp-quote content-type="editor-comment"><p>2) The fairness of comparisons with ABC/sampling – are these alternatives being compared in an even-handed way?</p></disp-quote><p>We believe they are—see detailed explanation below. We also remark that the superiority of SNPE-algorithms over rejection ABC, albeit in problems outside of neuroscience, has been demonstrated both by us (Lueckmann, Goncalves et al., 2017, Greenberg et al., 2019) and others (Papamakarios and Murray, 2016).</p><disp-quote content-type="editor-comment"><p>3) It may be helpful to have a very explicit example of the results SNPE returns on a problem where “ground truth” of feasible parameters are known. This could be a simple model dynamical system, used for illustration.</p></disp-quote><p>The manuscript contains two examples (a LN GLM and a Gabor GLM) where the likelihood is known, and as we explain below (see response to reviewer), we can obtain the respective ground-truth posteriors using standard inference algorithms (MCMC). We show that SNPE recovers the correct posterior in both cases. In our detailed response below, we also include additional analyses on the LN GLM model, and clear out concerns about the correctness of the returned posteriors (e.g., the STG).</p><p>We therefore respectfully disagree that the addition of yet another example would strengthen the manuscript. In case the editor still finds it necessary after our detailed response, we would like to clearly understand what precisely the additional value provided by adding one more example application would be.</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>The authors have clarified the methods in a way that I think should be helpful, and I remain optimistic that this will be a very valuable technique for the field. However, I think some of the other concerns raised about rigorously demonstrating the technique, and clarifying the method conceptually were not addressed. I will try to re-state these concerns in a broader scope that I hope strengthens the paper, clarifies potential issues, and gets at some key conceptual and rigor issues that need to be addressed</p><p>1) Conceptual question: The manuscript and reply heavily emphasize that SNPE performs Bayesian inference, as compared to approaches that only are applicable to deterministic modeling (like classic brute force approaches). However, outside of the very simple illustrative example of Figure 2, it appears that all of the remaining examples are done on deterministic models. This leads to a fundamental conceptual question: how should one interpret the resulting posteriors resulting from SNPE?</p><p>To be concrete, if one used the precise output of a simulation (rather than summary statistics), then the actual posterior probability over parameter values is a δ function (assuming the model if identifiable). […] And how should the user interpret “probabilities” in a deterministic model where there may only be 1 true solution?</p><p>Note: when summary statistics are used rather than exact simulation output, this will likely create some degeneracies. However, this still may end up creating a razor thin slice in parameter space that corresponds to truly, exactly matching the summary statistics, which would then be represented by the true posterior. This scenario then returns to the same fundamental questions of: a) how to interpret the posterior from SNPE, and b) how to interpret a simulation output that is very close to the summary statistics but not equal to them and therefore perhaps lost if SNPE is finding the true posterior of the deterministic model.</p></disp-quote><p>All the applications in the manuscript are stochastic, as described in the original version of the manuscript: we write both in Results and Materials and methods that the Hodgkin-Huxley model includes &quot;intrinsic neural noise&quot;; regarding the STG circuit, we write in Materials and methods that &quot;we added Gaussian noise with a standard deviation of 0.001 mV to the input of each neuron&quot;. We note that the information of the stochastic nature of the ion channel model was omitted by mistake in the Materials and methods, and this is now corrected.</p><p>We do, however, realize that this point was not sufficiently clear, so we emphasized it further:</p><p>1) In the ion channel application, we now explicitly write in the Results where the stochasticity comes in:</p><p>“We aimed to identify these ion channel parameters θ for each ICG model, based on 11 features of the model's response to a sequence of 5 noisy voltage clamp protocols, resulting in a total of 55 different characteristic features per model (Figure 3B, see Materials and methods for details).”</p><p>And in the Materials and methods:</p><p>“We modeled responses of the Omnimodel to a set of five noisy voltage-clamp protocols (Podlaski et al., 2017): as described in Podlaski et al., the original voltage-clamp protocols correspond to standard protocols of activation, inactivation, deactivation, ramp and action potential, to which we added Gaussian noise with zero mean and standard deviation 0.5 mV.”</p><p>2) In the Hodgkin-Huxley application:</p><p>“We considered the problem of inferring 8 biophysical parameters in a HH single-compartment model, describing voltage-dependent sodium and potassium conductances and other intrinsic membrane properties, including neural noise, making the model stochastic by nature (Figure 4A, left).”</p><p>3) In the STG application:</p><p>“This model has been used to demonstrate that virtually indistinguishable activity can arise from vastly different membrane and synaptic conductances in the STG (Alonso and Marder, 2019; Prinz et al., 2004). Here, we build on these studies and extend the model to include intrinsic neural noise on each neuron (see Materials and methods).”</p><p>More generally, we hold the view that modelling and explicitly treating <italic>some</italic>​ ​form of noise (either in the underlying system and/or the measurement process) is of critical importance for data analysis in neuroscience. This is (arguably) under-appreciated in computational neuroscience, which often concentrates on reproducing averaged neural activity without taking into account the associated variability or uncertainty. So, we maintain that the restriction of stochastic models is, in our view, not a serious restriction.</p><p>As the reviewer writes, SNPE could also be applied to deterministic models and will also return posteriors in this case. However, a rigorous interpretation of the posteriors in that case is mathematically subtle, and a mathematically rigorous treatment of this case is therefore, in our view, beyond the scope of the manuscript. Somewhat speculatively, we agree that the posterior would likely be a “smoothed out” version of the true (δ-shape) posterior, where the smoothing is dependent on the capacity of the neural network used for inference. We note that there is some recent work in machine learning to deal with such cases rigorously, e.g. by resorting to spread divergences <ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1811.08968">(</ext-link>​<ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1811.08968">https://arxiv.org/abs/1811.08968</ext-link>;​ didactic blog post by David Barber at <ext-link ext-link-type="uri" xlink:href="https://aiucl.github.io/spread-divergence/">https://aiucl.github.io/spread-divergence</ext-link>​ <ext-link ext-link-type="uri" xlink:href="https://aiucl.github.io/spread-divergence/">/</ext-link>).​</p><p>We have added a small Discussion about deterministic models:</p><p>“SNPE, and Bayesian inference more generally, is derived for stochastic models. SNPE can, in principle, also be applied to deterministic models, but a rigorous mathematical interpretation or empirical evaluation in this regime is beyond the scope of this study.”</p><disp-quote content-type="editor-comment"><p>2) On the one example that is stochastic (Figure 2), I am still confused by the results shown. Specifically, in Figure 2C, the posterior samples consistently and systematically undershoot the true receptive field for intermediate parameter values. This is a simple LN model for which classic maximum likelihood models usually doing an excellent job finding the true receptive field, at least with enough data. If one ran standard maximum likelihood estimation on this, would it miss? If so, why? Is this reflecting that the authors are using too broad a prior and it is not converging to the true posterior or that too few samples are used (the number of samples is not indicated)-if the latter, can the authors run with more samples and show that their method then converges to the correct answer? Or is it something else (e.g. see next paragraph)?</p><p>In addition, the authors state that their method works because it gets the same answer as MCMC. However, I'm wondering if both might be making errors due to the use of the Polya-Γ sampling scheme since, at least in the Pillow and Scott article cited, this was used not for a standard LN model but rather to model more complex models in which the variance was greater than for a Poisson spike count. In any case, it seems worth explaining where the systematic error in the posterior samples is coming from, given that the LN model is such a simple classic model fit whose parameters have been successfully identified by simpler maximum likelihood methods.</p></disp-quote><p>This comment contains multiple sub-questions which we would like to answer separately:</p><p>1) Can Polya-Γ MCMC (PG-MCMC) be used to infer a reference posterior for the LN model?</p><p>For the LN model, we use PG-MCMC (a likelihood-based method) to obtain a reference posterior to compare SNPE against. As shown in Polson, Scott and Windle, 2012, (cited in the manuscript), PG-MCMC is applicable to Bernoulli GLMs, which the LN model is one instance of. It is not the case that PG-MCMC is only applicable to overdispersed models.</p><p>2) Is the result obtained by SNPE correct?</p><p>The goal of SNPE is to perform Bayesian inference. Thus, to evaluate whether SNPE “works”, one must not compare its posterior to the ground-truth parameters, but rather to the ground-truth posterior, or at least a proxy thereof. We here used the posterior obtained via PG-MCMC as a reference posterior, which SNPE correctly recovers. We do not have doubts about the correctness of PG-MCMC on this example, in particular as the posterior agrees with summary checks, and this model has a unimodal posterior for which MCMC sampling is well understood and works reliably. (If the reference posterior was wrong, it would also be a bit of a coincidence that SNPE gets the same, but wrong posterior…). The only reason why the reviewer seems to question the correctness of the posterior is that he is surprised by the large posterior variance, see below.</p><p>3) Does posterior uncertainty shrink with more data?</p><p>The posteriors inferred with the PG-MCMC scheme and SNPE are not tightly constrained around the true filters (see Appendix 1—figure 1, showing posteriors means +/- 1 standard deviation). This “systematic undershooting” does not indicate a failure of the PG-MCMC scheme, but is simply a consequence of the fact that we simulated a short experiment consisting of 100 Bernoulli samples (which we call “single-trial” simulation). For short experiments, posterior uncertainty will be large, and a mismatch between true parameters and posterior mean is expected. As expected, increasing the number of single-trials in one observation leads to progressively tighter posteriors, with posterior samples closer to the true filter. We now describe this result in the Results section and Appendix 1—figure 3:</p><p>“Increasing the number of Bernoulli samples in the observed data leads to progressively tighter posteriors, with posterior samples closer to the true filter (Appendix 1—figure 3). Furthermore, SNPE closely agrees with the MCMC reference solution in all these cases, further emphasizing the correctness of the posteriors inferred with SNPE.”</p><p>4) Why are we not as “successful as simple MLE methods”</p><p>We note that the results by Pillow et al., which the reviewer likely refers to, have been obtained in extremely long recordings, which lead to very tightly constrained posteriors: in this regime, both maximum likelihood estimation and Bayesian inference return the same answer, but this also makes it hard to assess whether the posterior distribution, i.e. the estimation uncertainty, is correctly recovered. The fact that our posterior mean, with more samples, converges to the true parameters shows that, as the data increases, all these methods will converge to the same answer, namely the true parameters. In neuroscience, however, it is very common that one is not in this data-regime, making it important that methods perform correct inference even in the small-data regime.</p><disp-quote content-type="editor-comment"><p>3) Related to the Figure 2 example, I was struck by the huge errors and nearly zero correlation for the ABC methods. Is this fundamental to ABC or is this because the authors are hamstringing the ABC method by applying it in a very naïve manner that isn't how someone using ABC would apply it. Specifically, from the Materials and methods, it looks like the authors are considering every parameter to be independent, with no smoothing or regularization. I would think that someone using ABC methods would apply regularizations or the like that might dramatically reduce the effective dimensionality of the problem, so it's not clear if the comparison shown in the manuscript is fair. This isn't to say that SNPE won't still work much better than ABC methods, but it's worth checking to make sure the ABC methods are being applied in a realistic manner if the comparison is to be made.</p></disp-quote><p>Note that we compare SNPE against SMC-ABC in Figure 2 on two different examples: We show a comparison on the LN model example (Figure 2D) as well as a comparison on the Gabor model (Figure 2G). SNPE outperforms rejection methods in both cases, a result which is consistent with previous work (e.g., Lueckmann, Goncalves et al., 2017; Greenberg et al., 2019). Benchmarking against other methods is not the primary focus of this manuscript.</p><p>The LN model comparison is performed on low dimensional summary statistics (10d) rather than raw data traces, so it does not put SMC-ABC to a disadvantage. In the Gabor example, SMC-ABC fails because it is not able to deal with the high-dimensionality of the data. This illustrates an important drawback of SMC-ABC, which would need to be combined with other methods/pre-processing steps, if one wanted to use it for this problem. SNPE allows automatic learning of summary statistics, so that inference on high-dimensional data is possible. We maintain that this is a good illustrative comparison to include.</p><p>We also note that the suggestion to smooth the problem would, in effect, just reduce this high-dimensional estimation to an (effectively) lower-dimensional, and substantially easier estimation problem. Accordingly, all estimation approaches would work better, and the differences between methods would be smaller.</p><disp-quote content-type="editor-comment"><p>4) Ground truthing. The authors suggest that they can't ground truth the technique due, in part, to the stochastic nature of it. While I appreciate there may be some challenges, a lot could be done, especially since most of the manuscript focuses on deterministic examples that are easier to check. For example, in Figures 3-5, parameter estimation results that may or may not be correct are shown and the reader is left to assume they are correct by looking at a single or very small number of examples. While I fully expect that it does work reliably, it would be nice to actually run a large set of simulations for posterior checks (even if, e.g., only the mode of the distribution was checked) and then report some statistics.</p></disp-quote><p>Regarding the LN and Gabor models, and as discussed above, we include comparisons of SNPE posteriors with reference posteriors. Furthermore, we now also have a new supplementary figure for the LN model showing consistency with increases in data (see above).</p><p>All the other applications covered in the manuscript have intractable likelihoods, so ground-truthing is challenging. Therefore, we opted for posterior predictive checks on the data, rather than ground-truthing on the parameters. In addition, we do point out that the manuscript does, in effect, contain multiple quantitative evaluations:</p><p>1) In the ion channel model application, Appendix 1—figure 7 shows the correlation between the observation and the mode of the respective posterior for 350 different observations. We believe this figure demonstrates the reliable quality of the posteriors for this application.</p><p>2) In the Hodgkin-Huxley model applications, Figure 4A-E shows the application of SNPE to a synthetic dataset, Figure 4F shows the application of SNPE to 8 different datasets, and Appendix 1—figure 10C,D show multiple samples from the posterior for the synthetic dataset. We believe this is strong evidence of the success of SNPE on this application.</p><p>3) In the STG application, Figure 5D in combination with Appendix 1—figure 12A, and Appendix 1—figure 14C show that the summary statistics of a large number of samples from the posterior vary little around the observed summary statistics. This is again strong evidence that SNPE performed well in this application.</p><disp-quote content-type="editor-comment"><p>5) Related to ground truthing, in Figure 5D and the highlighted 2-D plot of Figure 5C, I am confused as to whether the example shows the success or partial failure of the method. Specifically, the point labeled &quot;2&quot; in the Figures 5C and 5D appears to be in a bright (and therefore presumably high probability?) region of the parameter space, both in an absolute sense and possibly (it's hard to tell) compared to the light purple circle simulation (and also relative to going further down below this light purple trace). However, the authors use point 2 to make the point that the simulation looks very far off from the reference trace. This suggests, possibly, that similar probabilities being output by SNPE is on the one hand (values close to the light purple circle) leading to a very good fit and on the other hand (red point 2) leading to a very bad fit – thus, this suggests that one perhaps can't trust “bright” regions in the SNPE posterior to correspond to “good” fits. Can the authors check this and address the more general concern. I'm guessing this may relate to my larger conceptual question of interpreting the “probabilities” for SNPE in my reviewer comment #1 above.</p></disp-quote><p>We thank the reviewer for raising this potentially confusing point. As the reviewer points out, when inspecting the 2D-marginal probability distribution, the point labeled “2” seems to lie in regions of high posterior probability. However, this does not imply that the point actually lies in a region of high probability in the full 31-dimensional posterior space (its 2-d project lies in a high probability region, but the full high-d version does not), and thus the visual inspection of 2D-marginals can be misleading. To demonstrate this point, we have added a supplementary figure that shows the posterior probability along both paths, see Appendix 1—figure 13 and refer to this figure in the main text as follows:</p><p>“Note that, while parameter set 2 seems to lie in regions of high probability when inspecting pairwise marginals, it in fact has low probability under the full posterior distribution (see Appendix 1—figure 13).”</p><p>As can be seen, the posterior probability along the orthogonal path drops off very quickly, which matches the off-looking activity and further demonstrates the success of the method.</p><disp-quote content-type="editor-comment"><p>6) Difference from “brute force methods”. I thank the authors for trying to generate a comparison to brute force methods by showing a rejection method. However, I don't think this is necessary and I think it's confusing as I don't think “brute force” and “rejection” are the same (i.e. I'd remove this from the paper as being more confusing than helpful […] And overall, this focus on the power of post-database-generation analysis is why I still think SNPE, and the more general focus on use of density estimators, is a potentially transformative contribution for the practice of model building (in addition to SNPE's applicability to stochastic problems that is already well emphasized in the manuscript).</p></disp-quote><p>We thank the reviewer for the suggestion regarding the clarification of the method in the context of previous approaches. We agree that the process of generating a large set of simulations and analysing it can be decoupled, at least for the case of one round SNPE—we note that in multi-round SNPE, the sampling of parameters is adaptively guided by the posterior from the previous round, and thus this decoupling of sampling and analysis is somewhat debatable. We have added a clarification regarding the brute-force aspect of SNPE in the Discussion subsection “Related Work”:</p><p>“We should still note that SNPE can require the generation of large sets of simulations, which can be viewed as a brute-force step, emphasising that one of the main strengths of SNPE over conventional brute-force approaches relies on the processing of these simulations via deep neural density estimators.”</p><p>We have now removed the analysis we had made and included (at the reviewers request, we were under the impression we were following his instructions) in the previous round. If the reviewer or the editor wants it included again, we are happy to put it back in.</p></body></sub-article></article>