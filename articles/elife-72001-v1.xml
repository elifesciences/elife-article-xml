<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">72001</article-id><article-id pub-id-type="doi">10.7554/eLife.72001</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group><subj-group subj-group-type="heading"><subject>Physics of Living Systems</subject></subj-group></article-categories><title-group><article-title>Multisensory-motor integration in olfactory navigation of silkmoth, <italic>Bombyx mori</italic>, using virtual reality system</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-245674"><name><surname>Yamada</surname><given-names>Mayu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9855-9157</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-245675"><name><surname>Ohashi</surname><given-names>Hirono</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5303-7200</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-245676"><name><surname>Hosoda</surname><given-names>Koh</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8392-1021</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-245677"><name><surname>Kurabayashi</surname><given-names>Daisuke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3186-6531</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib2">‡</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-245226"><name><surname>Shigaki</surname><given-names>Shunsuke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5689-1338</contrib-id><email>shigaki@arl.sys.es.osaka-u.ac.jp</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Graduate School of Engineering Science, Osaka University</institution><addr-line><named-content content-type="city">Osaka</named-content></addr-line><country>Japan</country></aff><aff id="aff2"><label>2</label><institution>Department of Systems and Control Engineering, Tokyo Institute of Technology</institution><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Walczak</surname><given-names>Aleksandra M</given-names></name><role>Senior Editor</role><aff><institution>École Normale Supérieure</institution><country>France</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn><fn fn-type="con" id="equal-contrib2"><label>‡</label><p>These authors also contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>25</day><month>11</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e72001</elocation-id><history><date date-type="received" iso-8601-date="2021-07-07"><day>07</day><month>07</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-11-10"><day>10</day><month>11</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-09-16"><day>16</day><month>09</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.09.14.460318"/></event></pub-history><permissions><copyright-statement>© 2021, Yamada et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Yamada et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-72001-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-72001-figures-v1.pdf"/><abstract><p>Most animals survive and thrive due to navigational behavior to reach their destinations. In order to navigate, it is important for animals to integrate information obtained from multisensory inputs and use that information to modulate their behavior. In this study, by using a virtual reality (VR) system for an insect, we investigated how the adult silkmoth integrates visual and wind direction information during female search behavior (olfactory behavior). According to the behavioral experiments using a VR system, the silkmoth had the highest navigational success rate when odor, vision, and wind information were correctly provided. However, the success rate of the search was reduced if the wind direction information provided was different from the direction actually detected. This indicates that it is important to acquire not only odor information but also wind direction information correctly. When the wind is received from the same direction as the odor, the silkmoth takes positive behavior; if the odor is detected but the wind direction is not in the same direction as the odor, the silkmoth behaves more carefully. This corresponds to a modulation of behavior according to the degree of complexity (turbulence) of the environment. We mathematically modeled the modulation of behavior using multisensory information and evaluated it using simulations. The mathematical model not only succeeded in reproducing the actual silkmoth search behavior but also improved the search success relative to the conventional odor-source search algorithm.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>multisensory-motor integration</kwd><kwd>olfactory navigation</kwd><kwd>virtual reality for insect</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>JP19H04930</award-id><principal-award-recipient><name><surname>Shigaki</surname><given-names>Shunsuke</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>JP19K14943</award-id><principal-award-recipient><name><surname>Shigaki</surname><given-names>Shunsuke</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>JP19H02104</award-id><principal-award-recipient><name><surname>Kurabayashi</surname><given-names>Daisuke</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Silkmoth utilizes multimodal information to modulate behavior according to the degree of turbulence in the environment, enabling it to efficiently an odor source search.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In many organisms, including humans, appropriate behavior is determined based on the integration of different kinds of information from the environment. Examples of information obtained from the environment include light, sound, odor, and wind. Unlike physical signals that are transmitted as waves, it is difficult to obtain precise directional and spatial information for odors because they (chemical substances) are carried by the wind. However, odors are widely utilized as a communication tool by organisms (<xref ref-type="bibr" rid="bib32">Renou, 2014</xref>) because they have good residuality and diffusivity that physical wave signals do not have. Insects, in particular, communicate extensively using odor (e.g. aggregation pheromones, trail pheromones, sex pheromones; <xref ref-type="bibr" rid="bib45">Wyatt, 2014</xref>), despite their small-scale neural systems. Odor information is also largely used to locate feeding sites and flowers (<xref ref-type="bibr" rid="bib32">Renou, 2014</xref>).</p><p>Understanding odor-based search behavior is of great value not only in biology, but also in engineering research. This is because odor-based search can be applied to the use of gas leak source search robots or lifesaving robots in a disaster area instead of dogs. Such odor-based search behavior has been reported to be observed in walking and flying insects such as cockroaches, beetles, moths, and flies, and detailed behavioral analyses have also been conducted (<xref ref-type="bibr" rid="bib44">Willis et al., 2011</xref>; <xref ref-type="bibr" rid="bib4">Burkhardt et al., 1967</xref>; <xref ref-type="bibr" rid="bib35">Ryohei et al., 1992</xref>; <xref ref-type="bibr" rid="bib43">Willis and Arbas, 1991</xref>; <xref ref-type="bibr" rid="bib36">Saxena et al., 2018</xref>; <xref ref-type="bibr" rid="bib28">Pang et al., 2018</xref>). For example, flying moths (<italic>Manduca sexta L</italic>.) or cockroaches (<italic>Periplaneta americana</italic>) move upwind when they encounter an odor, and move in a crosswind direction when they lose the odor. Moreover, walking moths (<italic>Bombyx mori</italic>) and dung beetles (stercorarius) perform a ‘recapture’ or ‘reenter’ of the odor plume by combining straight and zigzag movements. They have excellent performance in searching for feeding grounds, nests, and mating partners using this olfactory behavior (<xref ref-type="bibr" rid="bib31">Reisenman et al., 2016</xref>). Although there have been many attempts to model the excellent odor-source search behavior of insects and implement them in robots (<xref ref-type="bibr" rid="bib6">Chen and Huang, 2019</xref>), artificial systems have not yet achieved the same abilities as insects. One of the reasons for this lack of performance is that models do not incorporate when and under what conditions and which sensory information is fed back to inform subsequent behavior. For example, in order to move into an environment with high uncertainty, an agent needs to modulate its speed of movement according to the type and amount of sensory input, but many models have insufficient discussion on speed modulation. To solve this problem, it is necessary to measure behavioral changes in insects when multiple types of sensory information are presented to them. Previously, <xref ref-type="bibr" rid="bib29">Pansopha et al., 2014</xref> found that the mating behavior of a silkmoth (elicited by odor stimuli) was modulated by visual stimuli. Further, in a study on crickets, <xref ref-type="bibr" rid="bib15">Haberkern and Hedwig, 2016</xref> reported that long-term tactile stimulation suppressed phototaxis, suggesting that behavioral switching between proximal environmental information and phototaxis may occur. <xref ref-type="bibr" rid="bib10">Duistermars and Frye, 2008</xref> showed that by providing visual stimuli using optical flows to <italic>Drosophila</italic> in addition to odor stimuli, spatial information, such as the orientation of the odor source, could be obtained. These results reveal that behavioral modulation and switching mechanisms occur in insects through the acquisition of multiple types of environmental information. Because these experimental results were obtained when controlled stimuli were provided, the mechanisms of behavioral modulation and switching when complex environmental changes are presented are still unknown. Because the search behavior of insects is expected to depend on the environmental dynamics of the search, there should be a difference between search behavior in a relatively well-organized airflow environment and a turbulent environment. However, it is difficult to obtain the relationship between the sensory input and behavioral output of insects moving in a turbulent environment (<xref ref-type="bibr" rid="bib2">Baker et al., 2018</xref>).</p><p>In recent years, the use of virtual reality (VR) systems in insect behavior experiments has attracted attention as a way of presenting complex environmental changes. Generally speaking, VR is a system to achieve (or measure) a purposeful behavior by connecting a device that can provide multisensory stimuli to the organisms and a space in which the organisms can move virtually (<xref ref-type="bibr" rid="bib48">Zhou and Deng, 2009</xref>). Based on the general definition of VR, previous experiments in which stimuli were presented to multi-sensory organs of an organism did not provide a space for the organism to virtually perform the task. Therefore, even if we could measure the behavioral changes when stimuli were input to multi-sensory organs, it would be difficult to directly discuss which part of the task the behavioral changes were contributing to. By converting the behavioral experiment of the organisms into VR, we expect to clarify how the behavioral modulation mechanism using multisensory information contributes to the function of navigation. VR has therefore been proposed to investigate the navigation of mammals and invertebrates (e.g., <xref ref-type="bibr" rid="bib26">Naik et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Radvansky and Dombeck, 2018</xref>). For example, <xref ref-type="bibr" rid="bib18">Kaushik et al., 2020</xref> found, using an insect VR system, that dipterans use airflow and odor information for visual navigation. Moreover, <xref ref-type="bibr" rid="bib30">Radvansky and Dombeck, 2018</xref> succeeded in measuring the olfactory navigation behavior of mammals (mice) using a VR system. All motile organisms use spatially distributed chemical features of their surroundings to guide their behavior; however, investigating the principles of this behavioral elicitation has been difficult because of the technical challenges in controlling chemical concentrations in space and time during behavioral experiments. Moreover, their research has demonstrated that the introduction of VR into the olfactory navigation of organisms can solve the above problem. Thus, VR systems allow for a more natural presentation of environmental changes, as well as a quantitative analysis of the effects of motion modulation and switching mechanisms on functions such as search and navigation. Furthermore, they allow researchers to create and test insects in situations that do not occur in nature and may therefore play an important role in the construction of robust behavioral decision algorithms for unknown environments.</p><p>In order to elucidate the adaptive odor-source search behavior of insects, we constructed a VR system that can present multiple types of environmental information simultaneously and continuously, and we employed it to clarify how sensory information other than odor is used. Our VR system was connected to a virtual field built in silico and presented odor, wind, and visual stimuli according to environmental changes in the virtual search field. We employed an adult male silkmoth (<italic>Bombyx mori</italic>) as our measurement target. Female search behavior of the silkmoth has a stereotypic pattern (<xref ref-type="bibr" rid="bib35">Ryohei et al., 1992</xref>), but the duration and speed of the behavior are regulated by the frequency of odor detection and the input of other sensory information (<xref ref-type="bibr" rid="bib29">Pansopha et al., 2014</xref>; <xref ref-type="bibr" rid="bib40">Shigaki et al., 2019</xref>; <xref ref-type="bibr" rid="bib41">Shigaki et al., 2020</xref>). However, previous studies have observed silkmoth behavior in response to a specified, unchanging amount of the stimulus, and the nature of behavioral changes during an actual odor source search warrants further investigation.</p><p>In this study, we used a VR system to clarify which sensory organs the silkworm moth uses to search for females and how they are used. In addition, we constructed a model from our biological data and tested the validity of the model using a constructive approach.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this study, we analyzed changes in the behavior of an adult male silkmoth in his search for females in response to multiple sensory inputs. To measure behavior, we constructed a novel virtual reality device that presents odor, visual, and wind stimuli (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) to provide the silkmoth with the illusion that it was searching for a female (see <xref ref-type="video" rid="fig1video1">Figure 1—video 1</xref>). We provided the silkmoth with odor, wind, and visual stimuli (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In order to provide the odor stimulus to the left and right antennae independently, we designed two odor discharge ports above each antenna. We integrated these discharge ports with a tethered rod that was fixed to the silkmoth body. We provided the wind stimulus to the silkmoth from four directions: front, back, left, and right. Moreover, the visual stimulus provided an optical flow in the direction opposite to the turn direction of the silkmoth. In this experiment, we focused on odor-based navigation instead of visual object recognition, so that we adopted an optical flow to show which direction the landscape was flowing. The VR device was connected to a virtual field built in silico, and the odor puffs in the virtual field were produced by image processing of the actual odor diffusion using an airflow visualization system (<xref ref-type="bibr" rid="bib46">Yanagawa et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Connor et al., 2018</xref>). In addition, the virtual field was set with wind flowing uniformly from left to right. The search performance of the silkmoth using the constructed VR system was the same as that in free-walking experiments (see Supplementary Materials). We carried out experiments using the VR by changing the number of sensory inputs with (1) two or less types of sensory input (Group 1: odor only/odor and wind/odor vision), and (2) three types of sensory input (Group2: odor wind vision) (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Three repetitions of the experiment were conducted using 10 silkworm moths for each environmental condition (<inline-formula><mml:math id="inf1"><mml:mi>n</mml:mi></mml:math></inline-formula> = 30). We set a time limit of 300 s because theoretically, the silkmoth could reach the odor source in an infinite time. The search was considered a failure if the moth did not enter a radius of 10 mm from the odor source within the time limit.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The virtual reality (VR) system for olfactory navigation of the insect and a list of experimental conditions.</title><p>(<bold>A</bold>) The VR system is equipped with a stimulator of odor, vision, and wind, and is connected to a virtual odor field. The insect on the VR device performs olfactory navigation in a virtual space. (<bold>B</bold>) Definition of the way of presentation of each sensory stimulus. (<bold>C</bold>) The odor is presented under all conditions. ‘Ο’, ‘X’, and ‘<inline-formula><mml:math id="inf2"><mml:mo>∙</mml:mo></mml:math></inline-formula>’ indicate presented, not presented, and presented from a direction opposite to the actual direction, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>System configuration diagram and evaluation of virtual reality (VR) system.</title><p>Data comparison between the free walking experiment and VR experiment. (<bold>A</bold>) VR system configuration diagram. Two microcomputers were used to control the stimulator and measure the amount of rotation of the sphere. (<bold>B—D</bold>) Evaluation results of each stimulator. (<bold>B</bold>) shows the change in the heading angle of the silkmoth. It was confirmed that when the odor was presented from the upper part of the antennae, female search behavior was elicited while changing the heading angle significantly. (<bold>C</bold>) investigated whether the visual motion reflex was elicited by the visual stimulator by observing the change in the angle of the neck. Because the neck tilt was the largest in the same speed band as the angular velocity of the silk moth, it was confirmed that the visual stimulus was presented correctly. (<bold>D</bold>) is a snapshot of the push-pull rectifier, which is a wind stimulator, visualized by PIV. Since the yellow line represents the streamline and does not generate vortices, it was confirmed that rectification could be generated by push-pull. (<bold>E</bold>) Comparison of search success rates between the free walking and VR experiments (Fisher’s exact test, p &lt; 0.05). The free walking experiment was the result of two repeated experiments using 15 silkmoths (<inline-formula><mml:math id="inf3"><mml:mi>n</mml:mi></mml:math></inline-formula> = 30). The environment of the free walking experiment was set to be the same as the virtual environment of the VR system. (<bold>F</bold>) Comparison of relative length. The relative length is an evaluation value that standardizes the distance actually traveled to the shortest distance to the odor source. This makes it possible to evaluate how much action is taken in response to a sensory input. Because there was no difference between free walking and VR (Welch’s t-test, p &lt; 0.05), it is highly possible that the search behavior expressed by a silkmoth in the VR experiment was the same as the behavior in the free walking experiment .</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Plume capturing method and odor frequency property of plume.</title><p>(<bold>A</bold>) The plume model used a video of the behavior of odor particles on a two-dimensional plane using PIV technology. (<bold>B</bold>) Distribution of odor frequency produced by the actual plume photographed in A. We measured the average odor frequency of 10 repetitions of an experiment in which an odor was emitted from the odor source at a frequency of 1 Hz for 5 min. The odor frequency was 1 Hz closer to the odor source, indicating that there was a correlation between the odor frequency and the distance from the odor source.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig1-figsupp2-v1.tif"/></fig><media id="fig1video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-72001-fig1-video1.mp4"><label>Figure 1—video 1.</label><caption><title>A video of a silkmoth behavior experiment using a virtual reality (VR) system.</title></caption></media></fig-group><sec id="s2-1"><title>Wind and visual effects on the olfactory navigation</title><p>We compared the search performance in response to different types of stimuli by presenting wind and visual stimuli in addition to olfactory stimuli in all conditions. We first measured the behavior when wind or visual stimuli were provided in addition to an odor stimulus (Group 1). The blue and black lines in <xref ref-type="fig" rid="fig2">Figure 2B—F</xref> are the trajectories of successful and unsuccessful searches, respectively. The amplitude of the heading angle histogram (<xref ref-type="fig" rid="fig2">Figure 2B—F</xref>; displayed in polar coordinates) indicates the frequency. Judging from the trajectory and heading angle histogram, the trajectories in cond. 1 (only odor) show that although the silkmoth moved toward the upwind direction with a high frequency, it deviated from the range where the odor had a high probability of reaching (high odor area), and the search failed (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). By presenting wind (cond. 2) and visual (cond. 3) information in addition to odor, the silkmoth reduced the deviations from the high odor area, which qualitatively indicates that the odor-source search behavior was modulated by other sensory information (<xref ref-type="fig" rid="fig2">Figure 2CD</xref>). In the case of wind input, even if there was a deviation, modulation was elicited to change the heading angle to the upwind direction. In the case of visual input, although there was no trajectory deviating away from the high-odor area, because the frequency of the heading angle histogram was also high at angles other than <inline-formula><mml:math id="inf4"><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula>, the posture control makes the search behavior more careful. According to the heading angle histogram of cond. 4, which was presented from the direction opposite to the direction in which the wind actually blew, there was also a peak in the downwind direction; This indicates that the behavioral modulation was affected by the wind information (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Even when the visual stimulus direction was reversed, the peak in the upwind direction was not extreme, and a peak was generated in the crosswind direction (<xref ref-type="fig" rid="fig2">Figure 2F</xref>). By presenting the visual information in the opposite direction (the visual information was input as if it were rotating in the opposite direction), the illusion is that it was not rotating correctly, and more rotational behavior was induced. In order to quantitatively evaluate how similar these search trajectories were for each condition, we performed a two-dimensional histogramization of the trajectories and a calculation of similarity.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Definition of odor-source search experimental fields and initial heading angle (<bold>A</bold>), and the result of trajectories and heading angle histograms under each experimental condition (<bold>B—F</bold>).</title><p>The blue and black lines are the trajectories of successful and unsuccessful searches, respectively. Moreover, the amplitude of the heading angle histogram displayed in polar coordinates indicates the frequency. The higher the frequency, the more he silkmoth moved in that direction. Note that the 0°direction is the windward direction and does not necessarily move toward the odor source.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Odor-source search performance of a silkmoth when the direction of odor stimulation is lost.</title><p>Regardless of which insect antennae in the virtual environment detected the odor, we provided the odor stimulus to both antennae of the actual silkmoth. This meant that the silkmoth lost bilateral information of odor. Vision and wind were presented in the same way as in cond. i. (<bold>A, B, and C</bold>) represent the results of the search success rate, search time, and search performance, respectively. (<bold>D and E</bold>) represent the trajectories with and without bilateral information, respectively. The blue and black lines of the trajectories represent success and failure, respectively. When the bilateral information of the odor is lost, the search success rate drops significantly because the momentary direction information is lost. This means that the left and right odor information is important in the odor-source search, and that our odor stimulator correctly provides the odor information into the left and right antennae.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig2-figsupp1-v1.tif"/></fig></fig-group><p>We created a migration probability map in order to visualize the effects of differences in environmental conditions on the behavioral trajectory (<xref ref-type="fig" rid="fig3">Figure 3A—E</xref>). To quantitatively evaluate the similarity between the migration probability maps, we calculated the earth mover’s distance (EMD) (<xref ref-type="bibr" rid="bib33">Rubner et al., 1997</xref>). EMD is an index that calculates whether the distributions of two histograms are similar. The smaller the EMD value, the more similar the two histograms; the larger the value, the less similar they are. <xref ref-type="fig" rid="fig4">Figure 4F</xref> shows the result of calculating EMD based on cond. 1, in which only the odor stimulus was presented. The silkmoths’ trajectories in conds. 3 and 5 (odor + vision) were very similar to the trajectory in cond. 1 (only odor), suggesting that vision did not significantly affect search behavior. In addition, the EMD value when wind stimulus was presented in addition to odor was 10, suggesting that the addition of wind stimulus significantly affected search behavior. We illustrated the navigation success rate and search time (yellow background in <xref ref-type="fig" rid="fig4">Figure 4AB</xref>). Additionally, we calculated the search success rate per unit time (SPT) as a measure of the relationship between the search success rate and search time (yellow background in <xref ref-type="fig" rid="fig4">Figure 4C</xref>). Higher SPT values represent better search performance. The success rate was most improved when the wind was presented correctly (cond. 2) compared to when only the odor stimulus was provided (cond. 1). The success rate was lower under the condition where the wind was presented from the direction opposite to the direction detected in the virtual environment (cond. 4), compared with the condition where only the odor stimulus was presented (cond. 1). Moreover, SPT in conds 3 and 5 (in which the direction of the visual stimulus was changed without presenting the wind stimulus) was almost the same (0.411 vs 0.409), suggesting that the visual stimulus had little effect on search performance. Therefore, the success rate of navigation tends to improve by correctly presenting a wind stimulus in addition to the odor.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The result of visually expressing the trajectory under each experimental condition with a migration probability map (<bold>A—E</bold>).</title><p>The white area in the figure indicates that the moth did not move into that space. A quantitative evaluation using earth mover’s distance (EMD) is illustrated in <bold>F</bold>, which is the result of calculating the similarity (EMD) based on the trajectory of cond. 1 (odor presentation only). The lower the value, the higher the similarity, and the higher the value, the lower the similarity of the trajectories.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig3-v1.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Results of navigation experiments using virtual reality (VR).</title><p>The yellow background is the result of providing two or fewer sensory stimuli, and the red background is the result of providing three sensory stimuli. (<bold>A</bold>) The success rate of the navigation (Fisher’s exact test, p &lt; 0.05). (<bold>B</bold>) The search time at the time of success (Steel-Dwass test, p &lt; 0.05). (<bold>C</bold>) Search performance. The success rate per unit time was calculated based on the results of A and B.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig4-v1.tif"/></fig><p>We found that when three types of sensory stimuli were presented (Group 2), the success rate changed significantly compared to when two types of sensory stimuli were presented (red background in <xref ref-type="fig" rid="fig4">Figure 4</xref>). Focusing on cond. i, the SPT values were also significantly different, suggesting that olfactory navigation can be performed more accurately and efficiently by presenting all sensory stimuli. Additionally, under the condition that the wind stimulus is presented from the direction opposite to the direction received in the virtual environment, the search success rate is significantly reduced, and it is clear that information on wind direction is an important factor. We therefore hypothesized that the silkmoth performs efficient female searches using all sensory inputs: odor, wind, and vision.</p></sec><sec id="s2-2"><title>Extraction of behavioral modulation mechanisms in the odor-source search</title><p>In the previous section, it was found that information on wind direction, in addition to odor, contributed to improving the success rate in searching for the odor source. Here, we analyze in detail how behavior is modulated by visual and wind information using three experimental conditions: a forward condition (cond. i), an inverse condition for wind direction information (cond. ii), and an inverse condition for visual stimuli (cond. iii).</p><p>We first analyzed the effect of information on wind direction on odor-source search behavior. Specifically, we analyzed whether the movement speed changed between cond. i and cond. ii because the wind causes behavioral modulation. We calculated the movement speed change with respect to the odor detection frequency (Hz) because it has been reported that the odor detection frequency is related to the distance from the odor source (<xref ref-type="bibr" rid="bib19">Kikas et al., 2001</xref>; <xref ref-type="fig" rid="fig5">Figure 5AB</xref>). Here, odor detection frequency is defined as the frequency at which the silkmoth receives odors per unit of time. The odor detection frequency at each point of the odor field in this study is listed in the Supplementary Materials (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The velocity distribution at each odor detection frequency is represented by a box plot. The red box plots in each figure represent the maximum values, and the circled dots in each box plot are the average values. The dotted line in <xref ref-type="fig" rid="fig5">Figure 5A/B</xref> is the result of performing a least-square approximation on the average value. The result of this least-square approximation is utilized in the mathematical modeling. According to the results of the speed change of cond. i, the translational speed increases monotonically up to the odor detection frequency of 0.7 Hz and decreases after 0.7 Hz. Moreover, the angular velocity increases monotonically up to 0.4 Hz, and then decreases. Considering that the pheromone release frequency of the silk moth female is about 0.8 Hz, when the wind is received from the same direction as the odor, the male silkmoth may actively move until it approaches the pheromone release frequency. At higher frequencies ( &gt; 0.8 Hz), the male silkmoth reduces its movement speed to facilitate a mate search because there is a possibility that the female is nearby.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Analysis of the effects of wind and vision direction on behavior.</title><p>(<bold>A</bold>) Changes in the translational velocity and angular velocity when the wind direction is presented correctly (cond.<bold>i</bold>). Both translation and angular velocity peak, and the behavior is modulated depending on the odor detection frequency. (<bold>B</bold>) Changes in translational velocity and angular velocity when the wind is presented from the direction opposite to the actual direction (cond. ii). The translational velocity is constant regardless of the odor detection frequency, and the peak position of the angular velocity is 0.5 times that of cond. i. (<bold>C</bold>) Comparison of angular velocities when the visual information is presented correctly (cond. <bold>i</bold>) and when it is presented in the opposite direction (cond. iii). Vision was used to equalize the speed of the left-right rotation.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Histogram of the angular velocity of cond. i and cond. iii.</title><p>The vertical axis is probability and the horizontal axis is the angular velocity (absolute value). The red and blue lines represent the probability density distribution for left or right rotation, respectively. The peak position of the probability density distribution shifts depending on the presentation conditions of the visual stimulus.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig5-figsupp1-v1.tif"/></fig></fig-group><p>According to the results of the translational speed of cond. ii, the moth always moves at a constant speed regardless of the odor detection frequency. We also found that the frequency at which the maximum value occurs for the angular velocity was shifted to 0.2 Hz. This may be because the peak of the angular velocity shifts toward a lower odor detection frequency, and if the odor and wind direction do not match, the silkmoth rotates slowly to carefully search for the odor. This is probably because if the odor and wind direction do not match, the odor is carefully searched for by slowing down the overall movement speed.</p><p>Next, we investigated the effects of visual stimuli on odor-source search behavior. Section 2.1 suggests that visual stimuli do not directly contribute to search performance. However, many flying insects with compound eyes use visual information for their own postural control (<xref ref-type="bibr" rid="bib11">Dyhr and Higgins, 2010</xref>; <xref ref-type="bibr" rid="bib12">Dyhr et al., 2013</xref>). Because the silkmoth, which may retain some vestiges of flying insects (<xref ref-type="bibr" rid="bib17">Kanzaki, 1998</xref>; <xref ref-type="bibr" rid="bib38">Shigaki et al., 2016</xref>), also changes its rotational behavior in response to a visual stimulus presented as an optical flow (<xref ref-type="bibr" rid="bib29">Pansopha et al., 2014</xref>), it is possible that the silkmoth utilizes the visual information as the input amount for postural control. To test this hypothesis, we compared the angular velocities of silkmoths in cond. i (all stimuli in the forward direction) to those in cond. iii (visual stimuli presented in the inverse direction). <xref ref-type="fig" rid="fig5">Figure 5C</xref> shows a histogram of the left and right angular velocities under each experimental condition. The red color in <xref ref-type="fig" rid="fig5">Figure 5C</xref> indicates the angular velocity when rotating counterclockwise, and the blue color shows the angular velocity when rotating clockwise. The position of the vertical bar above the histogram represents the average angular velocity, and the length of the horizontal bar represents the standard deviation. Although the average angular velocities of the left and right rotations are the same in cond. i (visual stimuli presented correctly), the angular velocities of the left and right rotations differed when the visual stimuli were presented in the opposite direction to reality (Wilcoxon rank sum statistical test, p &lt; 0.05). These results indicate that silkmoths, like flying insects, use visual stimuli for postural control.</p><p>We found that the silkmoth modulated its behavior based on whether or not the direction of odor and wind detection coincided.</p></sec><sec id="s2-3"><title>Modeling and validation of behavioral modulation mechanisms</title><p>Here, we investigated how behavioral modulation informed by behavioral experiments using a VR system contributes to the odor-source search. A silkmoth moves by walking on a two-dimensional plane with six legs, but it does not move in a lateral direction. Therefore, we assumed that it has non-holonomic constraints and constructed our model to output straight-ahead and rotational movements. Previous studies have proposed a silkmoth search model called the surge-zigzagging algorithm (<xref ref-type="bibr" rid="bib35">Ryohei et al., 1992</xref>; <xref ref-type="bibr" rid="bib41">Shigaki et al., 2020</xref>), which makes action decisions using only olfactory information. In this study, we updated the surge-zigzagging algorithm to incorporate wind direction information. For convenience, we called this algorithm MiM2 (the multisensory input-based motor modulation) algorithm. A block diagram of the constructed MiM2 algorithm is presented in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. The model has a straight-ahead speed (v) controller (<xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>) and an angular velocity () controller (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>), and each controller changes output depending on the amount of sensory input. The detailed equations for each controller are shown below:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ω</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>×</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>N</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>β</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mi>f</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Block diagram of the proposed model and the simulation results.</title><p>(<bold>A</bold>) Control model that modulates speed according to the degree of coincidence between odor detection direction and wind direction. (<bold>B</bold>) The success rate of navigation. (<bold>C</bold>) The search time at the time of success. (<bold>D—I</bold>) The result of trajectories under each experimental condition. The blue and black lines are the trajectories of successful and unsuccessful searches, respectively. (<bold>J—L</bold>) The result of heading angle histograms under each experimental condition. The amplitude of the heading angle histogram displayed in polar coordinates indicates the frequency: the higher the frequency, the more the agent moves in that direction. Note that the <inline-formula><mml:math id="inf5"><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> direction is the windward direction and does not necessarily move toward the odor source.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Flowchart of other algorithms.</title><p>Flowchart of the algorithm used in the simulation experiment. (<bold>A</bold>) Flowchart of the surge-zigzagging algorithm. The surge state and the zigzag/loop state are switched by the odor stimulus. (<bold>B</bold>) Flowchart of the casting algorithm. By detecting the odor, it moves upwind, and when it loses its odor, it casts in the crosswind direction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Simulation results when the left and right angular velocities are out of balance in rotational motion.</title><p>If the angular velocities in the left and right rotational motions are different, the search trajectory is biased, so that the path becomes longer and the search time is significantly longer. The search success rate also drops, similar to the results of the biological experiments. Accordingly, it is suggested that the balance of left and right rotational motion contributes to an efficient search.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig6-figsupp2-v1.tif"/></fig><media id="fig6video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-72001-fig6-video1.mp4"><label>Figure 6—video 1.</label><caption><title>Experimental video of the simulation.</title></caption></media></fig-group><p>The free parameters were set to <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1000</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.50</mml:mn></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf8"><mml:mrow><mml:msub><mml:mi>ω</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.57</mml:mn></mml:mrow></mml:math></inline-formula>. Here, <inline-formula><mml:math id="inf9"><mml:mi>f</mml:mi></mml:math></inline-formula> denotes the odor detection frequency. Moreover, <inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a linear function that varies with the direction of odor and wind detection. The parameters a and b of <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are shown in <xref ref-type="table" rid="table1">Table 1</xref>. These were obtained by approximating the results of the behavioral experiment using the least-squares method (dotted line in <xref ref-type="fig" rid="fig5">Figure 5A/B</xref>). In addition, <italic>t</italic><sub><italic>d</italic></sub>, <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>R</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf13"><mml:mi>N</mml:mi></mml:math></inline-formula> represent the odor detection timing, the odor detection direction, and the number of turn motions, respectively; <inline-formula><mml:math id="inf14"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents a straight-ahead motion state and takes a value of –1 when the left antenna detects and one when the right antenna detects; if both antennae detect, a value of –1 or one is randomly selected; <inline-formula><mml:math id="inf15"><mml:mi>N</mml:mi></mml:math></inline-formula> increases according to <xref ref-type="disp-formula" rid="equ4">Equation (4)</xref> up to four, but does not take a value of four or more because the number of zigzagging motions is approximately three (<xref ref-type="bibr" rid="bib35">Ryohei et al., 1992</xref>). When it receives an odor stimulus according to <xref ref-type="disp-formula" rid="equ1">Equations (1–3)</xref>, it generates a straight motion for 0.5 s and then makes a turn motion.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0.0116</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>−</mml:mo><mml:mn>0.199</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>1.1971</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mn>0.4482</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>List of parameters for <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>K</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">(a)<break/><inline-formula><mml:math id="inf17"><mml:msub><mml:mi mathsize="90%">k</mml:mi><mml:mi mathsize="90%">v</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"/><th align="left" valign="bottom"><inline-formula><mml:math id="inf18"><mml:msub><mml:mi mathsize="90%">a</mml:mi><mml:mi mathsize="90%">v</mml:mi></mml:msub></mml:math></inline-formula></th><th align="left" valign="bottom"><inline-formula><mml:math id="inf19"><mml:msub><mml:mi mathsize="90%">b</mml:mi><mml:mi mathsize="90%">v</mml:mi></mml:msub></mml:math></inline-formula></th></tr></thead><tbody><tr><td align="left" rowspan="2" valign="bottom">Odor direction = Wind direction</td><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf20"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>≤0.7</td><td align="char" char="." valign="bottom">5.36</td><td align="char" char="." valign="bottom">12.9</td></tr><tr><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf21"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>&gt;0.7</td><td align="char" char="." valign="bottom">–11.4</td><td align="char" char="." valign="bottom">24.6</td></tr><tr><td align="left" rowspan="2" valign="bottom">Odor direction ≠ Wind direction</td><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf22"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>≤0.7</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">10.5</td></tr><tr><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf23"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>&gt;0.7</td><td align="char" char="." valign="bottom">0.0</td><td align="char" char="." valign="bottom">10.5</td></tr><tr><td align="left" valign="bottom">(b)<break/><inline-formula><mml:math id="inf24"><mml:msub><mml:mi mathsize="90%">k</mml:mi><mml:mi mathsize="90%">ω</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom"/><td align="left" valign="bottom"><inline-formula><mml:math id="inf25"><mml:msub><mml:mi mathsize="90%">a</mml:mi><mml:mi mathsize="90%">ω</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf26"><mml:msub><mml:mi mathsize="90%">b</mml:mi><mml:mi mathsize="90%">ω</mml:mi></mml:msub></mml:math></inline-formula></td></tr><tr><td align="left" rowspan="2" valign="bottom">Odor direction = Wind direction</td><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf27"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>≤0.4</td><td align="char" char="." valign="bottom">0.638</td><td align="char" char="." valign="bottom">1.40</td></tr><tr><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf28"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>&gt;0.4</td><td align="char" char="." valign="bottom">–1.22</td><td align="char" char="." valign="bottom">2.16</td></tr><tr><td align="left" rowspan="2" valign="bottom">Odor direction ≠ Wind direction</td><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf29"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>≤0.4</td><td align="char" char="." valign="bottom">0.870</td><td align="char" char="." valign="bottom">1.42</td></tr><tr><td align="char" char="." valign="bottom"><inline-formula><mml:math id="inf30"><mml:mi mathsize="90%">f</mml:mi></mml:math></inline-formula>&gt;0.4</td><td align="char" char="." valign="bottom">–0.997</td><td align="char" char="." valign="bottom">1.80</td></tr></tbody></table></table-wrap><p>By passing through the directional discriminator and frequency counter, the odor is converted into information such as whether it was detected by the left or right antennae and how much odor it was exposed to. The wind is converted into wind direction information. A comparator is used to determine whether the direction of odor detection and the direction of the wind are the same, and the results are input to the straight-ahead speed and angular velocity controllers, which also receive odor detection frequency. Visual information controls the angular velocity of the left and right rotational movements, following which olfactory behavior takes place. From this information, the movement speed output is calculated, and the behavior is generated. We compared the MiM2 algorithm to the previous surge-zigzagging algorithm and the casting algorithm, which uses wind information for searching in a moth-inspired algorithm (<xref ref-type="bibr" rid="bib20">Li et al., 2016</xref>).</p><p>The simulation environment employed a virtual environment similar to that used in the behavioral experiments in the VR system (see <xref ref-type="video" rid="fig6video1">Figure 6—video 1</xref>). For each algorithm, we performed 1000 odor-source search experiments to evaluate search success rate and trajectory. In the simulation experiment, we set two scenarios to verify each algorithm. One was the same initial position (x, y) = (300, 0) [mm] as in the biological experiment, and the other was the initial position moved ± 100 mm in the crosswind direction (x, y) = (300, ± 100) [mm]. Because the latter was near the edge of the area where the odor reaches, deviation behavior possibly occur frequently. <xref ref-type="fig" rid="fig6">Figure 6B and C</xref> show the search success rate and the search time, respectively. The MiM2 algorithm showed a higher search success rate than the other algorithms, regardless of the initial position (Fisher’s exact test, p &lt; 0.01). The casting algorithm showed the shortest search time (Steel-Dwass test, p &lt; 0.01) because it is an algorithm that actively moves upwind using odor detection. However, if the heading angle when moving upwind was incorrect, the search success rate of this algorithm was the lowest of the three, because there was a high possibility that the agent would have moved in the wrong direction. Next, we focused on the trajectory and heading angle change shown in <xref ref-type="fig" rid="fig6">Figure 6D—L</xref>. The blue and black lines in <xref ref-type="fig" rid="fig6">Figure 6D—I</xref> are the trajectories of successful and unsuccessful searches, respectively. Moreover, the amplitude of the heading angle histogram (<xref ref-type="fig" rid="fig6">Figure 6J—L</xref>; displayed in polar coordinates) indicates the frequency. For the readability of the trajectory and heading angle histogram, we randomly selected 100 trials of the 1000 replicates and plotted them. Regardless of the initial position, the search trajectory indicates that the MiM2 algorithm always places the searching agent in the middle of the odor distribution, whereas the conventional surge-zigzagging and surge-casting algorithms tend to place the searching agents near the edges of the odor. According to the heading angle histogram, the MiM2 algorithm resembles the actual silkmoth (<xref ref-type="fig" rid="fig2">Figure 2</xref>), but the other algorithms produced completely different histograms.</p><p>In order to quantitatively evaluate the trajectory of 1,000 simulations, we visualized the trajectory data (<xref ref-type="fig" rid="fig7">Figure 7A—F</xref>). The black line in <xref ref-type="fig" rid="fig7">Figure 7A—F</xref> shows the range that the odor has a high probability of reaching. Moreover, we evaluated the differences in trajectory between algorithms quantitatively by calculating their EDMs (<xref ref-type="fig" rid="fig7">Figure 7H</xref>). The comparison of these EDM values suggests that the MiM2 algorithm most closely resembles the actual silkmoth movements (<xref ref-type="fig" rid="fig7">Figure 7G</xref>) and was the closest to the results of behavioral experiments of the silkmoths while free walking. Thus, the MiM2 algorithm can simulate search behavior similar to that of real silkmoths by modulating the movement speed based on wind information in addition to odor, and equalizes the left-right angular velocities based on visual information.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Evaluation of simulation experiments using migration probability map.</title><p>(<bold>A—F</bold>) The migration probability map at the time of success in the simulation. (<bold>G</bold>) The migration probability of an actual silkmoth when searching for a female by free walking. (<bold>H</bold>) The EMD of each search algorithm, calculated from the migration probability map from the free-walking experiment (<bold>A—C</bold> vs.<bold>G</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-72001-fig7-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>Virtual reality for insect behavioral measurements</title><p>All animals, including insects, need to navigate for survival and reproduction. It is especially important to navigate efficiently in harsh environments and in situations in which there are many competitors. Previous studies have investigated the homing behavior of a desert ant (<italic>Cataglyphis</italic>) (<xref ref-type="bibr" rid="bib42">Wehner, 2003</xref>), the pheromone source localization behavior of a male silkmoth (<italic>Bombyx mori</italic>) (<xref ref-type="bibr" rid="bib27">Obara, 1979</xref>), and the sound source localization behavior of female crickets (<italic>Gryllus campestris L</italic>.) (<xref ref-type="bibr" rid="bib37">Schmitz et al., 1982</xref>). These studies have contributed to our understanding of the sensory-motor integration mechanism that converts sensory input into motor output – an important function of the neural system. However, the insects in these studies may not have displayed navigation behavior as they would have under natural conditions, because stimuli were controlled and presented in a fixed amount and at regular intervals in the behavioral experiments. In addition, the nature of behavioral change in a large-scale space and over a long period is relatively unknown because the behavioral experiments were carried out in a relatively small space and over a relatively short time. Because of this, multimodal virtual reality (VR) systems have attracted a great deal of attention (<xref ref-type="bibr" rid="bib18">Kaushik et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Naik et al., 2020</xref>). The advantage of using a multimodal VR system is that not only can behavior be measured on a large spatiotemporal scale, but the transmission of stimuli can be precisely controlled, and the behavioral output can be precisely measured. Biological data describing the relationship between sensory input and motor output are useful not only for clarifying biological functions, but also for the field of robotics, because these data play a very important role in modeling insect systems. Because the multimodal VR system in previous studies was developed for visually-dominated navigation, it was difficult to measure olfactory-dominated navigation, which was the focus of our research. Therefore, we developed a novel multimodal VR system that allowed us to measure changes in navigational behavior when the olfactory, visual, and wind directions of the silkmoth were modified. Conventionally, previous studies on the silkmoth have shown that (1) a ‘mating dance’ was elicited in response to sexual pheromones (<xref ref-type="bibr" rid="bib27">Obara, 1979</xref>), (2) the condition in which visual stimuli was presented immediately after the reception of sex pheromones influenced the subsequent rotational behavior (<xref ref-type="bibr" rid="bib29">Pansopha et al., 2014</xref>), and (3) behavioral inhibition occurred in response to frontal winds (<xref ref-type="bibr" rid="bib40">Shigaki et al., 2019</xref>). However, the integration of the above phenomena during navigation has not been investigated. By presenting three types of sensory information simultaneously as well as continuously, we were able to clarify the roles of each type of sensory information in navigation. In the behavioral analysis of a flying moth, it was found that when the moth detected the odor, it moved in an upwind direction (<xref ref-type="bibr" rid="bib44">Willis et al., 2011</xref>; <xref ref-type="bibr" rid="bib43">Willis and Arbas, 1991</xref>); and wind information is important for behavior determination. Even in the case of the silkmoth, which is a walking insect, the success rate is higher in the condition that the wind information is correctly provided (cond. 2) than in a non-correct condition (cond. 4) or in a no-wind condition (cond. 1) (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). However, it does not mean that the silkmoth cannot reach the odor source when wind information is not correctly provided, and the search success rate was about the same as in the no-wind condition (approximately 60%). The wind information is therefore important, but if the wind information is unreliable, the priority of the odor information may be increased and the silkmoth performs the navigation. When the phenomena obtained by the VR experiment were mathematically modeled and the performance was confirmed by the simulation experiment, not only the original search trajectory of the silkmoth could be reproduced, but the search performance was improved, compared to the conventional algorithms.</p></sec><sec id="s3-2"><title>Behavioral modulation to odor frequency</title><p>Organisms of all size scales rely on the ability to locate an odor source in space. The chemotaxis of bacteria and a nematode have been determined by measuring and analyzing the relationship between the chemical stimulus input and behavioral output under a controlled environment (<xref ref-type="bibr" rid="bib3">Berg, 2008</xref>; <xref ref-type="bibr" rid="bib22">Lockery, 2011</xref>). These studies found that as the concentration of chemical stimuli increased, the probability of rotating behavior decreased linearly. In other words, bacteria or nematode chemotaxis followed an odor gradient, which allowed them to locate the odor source. In the space where bacteria and the nematode exist, chemotaxis is effective in part because the odor field of the environment does not change significantly due to wind. In the case of larger size animals, the odor field of the environment is quite complex and it is difficult to reach the odor source by following the gradient alone. Odor fields can be complex because odor molecules are transported by airflow and mixed with other molecules at their destinations, forming complex structures (<xref ref-type="bibr" rid="bib8">Crimaldi and Koseff, 2001</xref>; <xref ref-type="bibr" rid="bib24">Murlis et al., 1992</xref>). In addition, the odor molecules themselves are discrete in space, and do not carry information about the source of the odor. However, a study using a sensor array to measure the arrival of odors carried by the wind revealed that the odors emitted from the source arrive periodically (<xref ref-type="bibr" rid="bib19">Kikas et al., 2001</xref>; <xref ref-type="bibr" rid="bib25">Murlis et al., 2000</xref>). Moreover, the periodicity is correlated to some extent with the distance from the source; the closer the source, the shorter the cycle, and the farther the source, the longer the cycle of arrival. Neither the concentration of the odor nor the number of exposures played an important role in the search process in <italic>Drosophila</italic>, but the ‘tempo’ at which flies encountered an odor was an important factor in the decision-making process (<xref ref-type="bibr" rid="bib5">Celani, 2020</xref>; <xref ref-type="bibr" rid="bib9">Demir et al., 2020</xref>). If we assume that the ‘tempo’ is the rate of odor detection per unit time, it is related to the cycle in which the odor arrives. We hypothesized that the silkmoth, like flying insects such as <italic>Drosophila</italic>, modulates its behavior based on the ‘tempo’ of the odor, therefore we included odor frequency in our analyses.</p><p>When the direction of the wind and the odor coincided, movement speed peaked when the odor frequency was 0.7—0.8, which is similar to the frequency at which a female silkmoth releases sex pheromones (0.79 ± 0.05 Hz) (<xref ref-type="bibr" rid="bib13">Fujiwara et al., 2014</xref>). Based on these findings, we hypothesized that if the male silkmoth detects wind and odor from the same direction, it correctly moves in the direction of the female and actively searches the field until it reaches a location where the sex pheromone release frequency approximates that of the female. When the frequency exceeds 0.8 Hz, the male seems to be in the vicinity of the female and therefore increases the spatiotemporal resolution of the search in order to locate the female and prepare for the transition to mating behavior. This might correspond to the silkmoth switching to the odor source declaration algorithm in olfactory navigation. The in vivo data obtained in the current study are consistent with those of earlier research demonstrating that an algorithm that shortens the travel distance of the surge toward the end of the search improves the search performance of the robot (<xref ref-type="bibr" rid="bib39">Shigaki et al., 2018</xref>).</p><p>In an earlier study investigating the direction of wind and odor in the environment, the direction of wind and odor were the same in the open-field experiment (no obstacles) (<xref ref-type="bibr" rid="bib25">Murlis et al., 2000</xref>). However, the direction of wind and odor is not always the same in a complex environment, such as a forest with many trees (<xref ref-type="bibr" rid="bib25">Murlis et al., 2000</xref>). Our data showed that in situations where the wind and odor direction do not match, the silkmoth always moves at the same rate, given any odor detection frequency. This may be a chemical tracking strategy to avoid leaving the odor range by moving at a speed lower than normal speed and suggests that the silkmoth is estimating the degree of environmental turbulence and modulating its behavior based on the odor and wind direction information. This strategy may be applicable to an engineering search system that switches the search strategy according to the environmental conditions.</p></sec><sec id="s3-3"><title>Comparison with conventional CPT algorithm</title><p>Odor sensing plays an important role in situations in which a visual search is difficult (e.g. a space filled with darkness and thick smoke). However, because the technology for the development of an odor sensor is lagging behind that of other sensory sensors (e.g. cameras, microphones), robot olfactory research is still a developing field. Currently, dogs play the role of odor sensors, but due to the high cost of training dogs and the deterioration of their odor sense with physical condition and age, engineering solutions using autonomous mobile robots for searching are needed. Conventional robot olfactory research has emphasized the development of motion planning (a search algorithm). The search algorithm is roughly divided into two fields: one is a bio-inspired algorithm that imitates the search behavior of living organisms, and the other is a search that estimates the position of the odor source using a statistical method (statistical algorithm). The bio-inspired algorithm, which mimics the behavior of organisms that can search in real time, is superior to the statistical algorithm in terms of robot implementation (<xref ref-type="bibr" rid="bib34">Russell et al., 2003</xref>; <xref ref-type="bibr" rid="bib21">Lochmatter et al., 2008</xref>). However, behavior patterns and movement speeds in these bio-inspired algorithms are always constant, regardless of the environmental conditions. Accordingly, there is a problem that the original search performance of living things cannot be replicated. For this reason, different kinds of research have been carried out more recently and applied to the bio-inspired algorithm, such as information-theoretic analysis of the trajectory of an insect (<xref ref-type="bibr" rid="bib16">Hernandez-Reyes et al., 2021</xref>), extraction of adaptability from neuroethology data by fuzzy inference (<xref ref-type="bibr" rid="bib41">Shigaki et al., 2020</xref>), and the acquisition of behavioral switching indices in response to environmental changes by measuring insect behavior while visualizing airflow (<xref ref-type="bibr" rid="bib9">Demir et al., 2020</xref>). In the current study, we found that behavioral modulation occurred based on the relationship between the direction of the odor arrival and the wind, and we used our data to reproduce a behavioral trajectory that was not only better than that of the conventional bio-inspired algorithm but was also more similar to the search behavior of the actual silkmoth. Therefore, it is clear that a probabilistic and time-varying behavior modulation mechanism has a better search performance than a time-invariant search algorithm.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>Silkmoths (<italic>Bombyx mori</italic>; Lepidoptera: Bombycidae) were purchased from Ehine Sansyu Co., Japan. Adult male moths were cooled to 16°C one day after eclosion to reduce their activity and were tested within 2-7 d of eclosion. Before the experiments, the moths were kept at room temperature (25-28°C) for at least 10 min.</p></sec><sec id="s4-2"><title>Virtual odor field</title><p>To generate a virtual odor field closely mimicking reality, smoke was emitted into the actual environment. The diffusion of the smoke in a two-dimensional plane was recorded and implemented into the simulator using image processing. The smoke visualization experiment was conducted in an approximately 2.5 × 0.8 m area in a darkroom. First, the smoke was visualized using a smoke generator (particle diameter: &lt; 10 μm) and a laser sheet (1 mm in thickness) (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). The smoke emitted into the room gleamed when it hit the irradiated laser sheet, and the distribution of smoke was recorded using a high-sensitivity camera. Note that the behavior of smoke reflected by the laser could be observed in two dimensions because the laser was in the form of a sheet. In this experiment, we carried out a visualization experiment in a darkroom because it is important to accurately record the light reflected by the smoke. This method was based on particle image velocimetry (PIV), which measures the velocity of fluid flow in space by visualizing particles and analyzing their movements. PIV accuracy is improved by scattering a large amount of smoke (particles) in space because it focuses on accurately tracking each particle; however, our method did not require following each particle. Instead, our method used a mass of smoke as an odor plume, and we observed how smoke floated and was distributed in space. In the visualization experiment, the flow meter (1.0 L/min) and solenoid valve (1 Hz) were controlled under the same release conditions as in the behavioral experiment. Our method required less smoke than PIV, and the smoke was thinner and more difficult to visualize. For this reason, it was necessary to increase the sensitivity of the camera; however, noise increased accordingly. In PIV, it is not necessary to distinguish between smoke particles and dust because only the movement of particles in space is important, but our method required removing other particles in order to measure only the smoke position as the plume. Hence, we applied image processing to the smoke image. Luminance values can have a large range because the intensity of smoke varies with airflow and time. In addition, it is difficult to extract only smoke with simple thresholding because the noise caused by dust has the same luminance as does smoke. Therefore, we adopted a method that focuses on connected components to remove noise while maintaining the shape of the smoke. In groupings based on connected components, two objects were considered to be connected if adjacent pixels in a binary image take a value of 1. Because smoke exists as a mass, it can be inferred that it has an area above a certain level when divided into connected components. For this reason, we removed the pixels whose connected components were not in adjacent pixels and whose area did not exceed a certain level. These processes were performed with source code using OpenCV.</p></sec><sec id="s4-3"><title>Configuration of virtual reality system</title><p>The behavioral experiment was carried out using a homemade virtual reality system (a photograph of the actual device is shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>). The traditional tethered measurement system was used for behavior measurement, and a stimulator that presented odor, visual, and wind direction information was installed around the tethered measurement system (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). The details of each stimulator were as follows:</p><sec id="s4-3-1"><title>Odor stimulator</title><p>We provided sex pheromone stimulation to both antennae of a silkmoth using Bombykol ((E,Z)–10,12-hexadecadien-1-ol). A cartridge containing 1000 ng of bombykol was placed in a tube in order to present air containing bombykol to the silkmoth. The compressed air from the air compressor (NIP30L, Nihon Denko, Aichi, Japan) passed through three gas washing bottles containing degreased cotton, activated carbon, and water, respectively, and was adjusted to 1.0 L/min using a flow meter (KZ-7002–05 A, AS ONE CORPORATION, Osaka, Japan). The timing of the stimulation was controlled by switching the flow path of the solenoid valve (VT307, SMC Corporation, Tokyo, Japan). The odor stimulus discharge ports were integrated with the tethered rod, and the discharge ports were located above the antennae. The tethered rod with the discharge ports was fabricated using a 3D printer (Guider2, Flashforge 3D Technology Co., Ltd., Zhejiang, China). As a result of presenting a one-shot pheromone stimulus to the silkmoth using this odor stimulator, the odor stimulus was presented correctly because female search behavior was elicited (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref>).</p></sec><sec id="s4-3-2"><title>Vision stimulator</title><p>The VR system in a previous study focused on navigation behaviors including object recognition, so that they provided visual stimuli using a monitor (<xref ref-type="bibr" rid="bib26">Naik et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Radvansky and Dombeck, 2018</xref>; <xref ref-type="bibr" rid="bib18">Kaushik et al., 2020</xref>). However, in this study, we provided the direction in which the landscape was flowing as a visual stimulus, not object recognition using vision. For this reason, we employed an optical flow as a visual stimulus for our VR system. Because the optical flow presentation method using LED arrays was also used in previous studies with flying insects (moth and bee), we used the same method in this study (; <xref ref-type="bibr" rid="bib47">Zheng et al., 2019</xref>). The visual stimulus device was constructed by arranging LEDs (WS2812B, WORLDSEMI CO., LIMITED, GuangDong, China) with a built-in microcomputer in an array around the tethered measurement system. The LED array was comprised of 256 LEDs arranged in 8 (vertical) × 32 (horizontal) panels. The array was presented as an optical flow by controlling the lighting timing of each LED according to the angular velocity of the silkmoth.</p><p>Because it has been reported that the silkmoth causes optomotor reflexes with respect to optical flow and its neck tilts in the direction of optical flow (<xref ref-type="bibr" rid="bib23">Minegishi et al., 2012</xref>), whether or not this vision stimulator was functioning was evaluated by the angle of the neck with respect to optical flow. The vision stimulator was functioning properly because the silkmoth has the largest neck tilt in the range of angular velocity during female search behavior (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1C</xref>), and this result is similar to the past research data (<xref ref-type="bibr" rid="bib29">Pansopha et al., 2014</xref>).</p></sec><sec id="s4-3-3"><title>Wind stimulator</title><p>The wind presented to the silkmoths was generated using a push-pull rectifier (<xref ref-type="bibr" rid="bib14">González et al., 2008</xref>). A push-pull rectifier consists of a push side that sends out the wind and a pull side that draws in the sent wind. Fans (PMD1204PQB1, SUNON, Takao, Taiwan) were installed on both the push and pull sides to generate airflow. Space can be used effectively using this device because air can flow without covering the workspace with a partition. The push-pull rectifier was connected to a hollow motor (DGM85R-AZAK, ORIENTAL MOTOR Co., Ltd., Tokyo, Japan) which rotated to generate wind. The performance of the push-pull rectifier was evaluated using particle image velocimetry (PIV) (<xref ref-type="bibr" rid="bib1">Adrian, 2005</xref>). For PIV analysis, we used videos that were shot at 800 fps with a resolution of 640 × 480 pixels. As a result, it was confirmed that the wind generated by the push-pull rectifier did not generate vortices (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1D</xref>).</p></sec><sec id="s4-3-4"><title>VR system evaluation experiment</title><p>We experimentally verified the extent to which the VR system equipped with odor, vision, and wind direction stimulators could reproduce a free-walking experiment in a real environment. In the free-walking experiment in a real environment, the search field was the same size as the VR system, and an odor emission frequency (1 Hz) was used. Two replicated experiments were carried out using 15 moths, and their behavior was recorded using a 30 fps video camera (BSW200MBK, BUFFALO, Aichi, Japan). The figure shows the quantitative comparison of search success rate and relative distance (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1E</xref>, F). We found that there was no difference between the results of VR and the free walking experiment. In other words, VR was able to reproduce the experiment of free walking, and the behavioral experiment could be performed using the VR device proposed in this study.</p></sec></sec><sec id="s4-4"><title>Migration pathway ratio map</title><p>A migration pathway ratio map was used to statistically process the trajectories of all trials and denoted the points where the quadcopter frequently passed through the experimental field. We created the migration pathway ratio map <inline-formula><mml:math id="inf31"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> based on the rule given by <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref>. Here, <inline-formula><mml:math id="inf32"><mml:mi>n</mml:mi></mml:math></inline-formula> represents the trial number, and the size of the grid cell was 0.1 × 0.1 cm.<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mtext> </mml:mtext><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We applied the equation to all trial trajectories and summarized them, and then calculated the trial average based on <xref ref-type="disp-formula" rid="equ6">Equation (6)</xref> to create the migration pathway ratio map <inline-formula><mml:math id="inf33"><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The migration pathway ratio map was created using MATLAB (2020a, MathWorks, MA, USA).<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>N</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-5"><title>Statistical analysis</title><p>For all data analyses, R version 4.0.3 was used (R Core Team). All earth mover’s distance calculations were performed using the Python 3.7 language.</p></sec><sec id="s4-6"><title>Odor-source search algorithm</title><p>The details of the three algorithms for which simulation experiments were performed are described here:</p><sec id="s4-6-1"><title>Surge-zigzagging</title><p>A surge-zigzagging algorithm models the female search behavior of an adult male silkmoth, which is a walking insect (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). In response to a one-shot sex pheromone stimulus, the silkmoth exhibits a movement that advances in the direction of the odor and a straight movement (surge), followed by a rotational movement (zigzag/loop) to search for further odor information from all directions. The flow chart of the surge-zigzagging algorithm is shown in the supplementary figures. The surge state in the algorithm lasts for 0.5 s because surge behavior lasts for about 0.5 s after the odor stimulus is presented. Because the surge behavior is elicited when the odor stimulus is presented, the trajectory becomes linear when the odor is continuously (high frequency) presented.</p></sec><sec id="s4-6-2"><title>Casting</title><p>A casting algorithm models the odor source localization behavior of a flying moth by mapping it onto a two-dimensional plane (<xref ref-type="bibr" rid="bib20">Li et al., 2016</xref>). A schematic diagram of this algorithm and an implemented flowchart are shown in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. When the agent in this algorithm detects an odor plume, it moves upwind, and when it loses sight of the plume, it moves in the crosswind direction to rediscover the plume. At this time, when moving upwind, it moves at a certain angle <inline-formula><mml:math id="inf34"><mml:mi>β</mml:mi></mml:math></inline-formula> with respect to the upwind direction and continues moving a certain distance <inline-formula><mml:math id="inf35"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> after losing sight of the plume. By repeating these movements in the upwind and crosswind directions, the odor source is localized. The parameters of <inline-formula><mml:math id="inf36"><mml:mi>β</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are <inline-formula><mml:math id="inf38"><mml:msup><mml:mn>30</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:math></inline-formula> and 2.5 cm, respectively, and these coincide with the highest search success rates in previous research (<xref ref-type="bibr" rid="bib21">Lochmatter et al., 2008</xref>).</p></sec></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Data curation, Formal analysis, Software, Validation, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Methodology, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Investigation, Methodology, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Investigation, Resources, Validation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Resources, Software, Validation, Visualization, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The silkmoths experiments in this study were examined and approved by the Osaka University and Tokyo Institute of Technology Gene Recombination Experiments Safety Management Committee.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-72001-transrepform1-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All experimental data are available on Zenodo via <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.5724790">https://doi.org/10.5281/zenodo.5724790</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Shigaki</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Virtual reality system data of silkmoths</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.5724790</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Dr. Takeshi Sakurai (Tokyo University of Agriculture) for providing the sex pheromone, bombykol.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adrian</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Twenty years of particle image velocimetry</article-title><source>Experiments in Fluids</source><volume>39</volume><fpage>159</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1007/s00348-005-0991-7</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>KL</given-names></name><name><surname>Dickinson</surname><given-names>M</given-names></name><name><surname>Findley</surname><given-names>TM</given-names></name><name><surname>Gire</surname><given-names>DH</given-names></name><name><surname>Louis</surname><given-names>M</given-names></name><name><surname>Suver</surname><given-names>MP</given-names></name><name><surname>Verhagen</surname><given-names>JV</given-names></name><name><surname>Nagel</surname><given-names>KI</given-names></name><name><surname>Smear</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Algorithms for Olfactory Search across Species</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>9383</fpage><lpage>9389</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1668-18.2018</pub-id><pub-id pub-id-type="pmid">30381430</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Berg</surname><given-names>HC</given-names></name></person-group><year iso-8601-date="2008">2008</year><source>E. coli in Motion</source><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/b97370</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Burkhardt</surname><given-names>D</given-names></name><name><surname>Schleidt</surname><given-names>W</given-names></name><name><surname>Altner</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1967">1967</year><source>Signals in the Animal World</source><publisher-name>George Allen &amp; Unwin</publisher-name><pub-id pub-id-type="doi">10.1017/S0030605300007122</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Celani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Olfactory navigation: Tempo is the key</article-title><source>eLife</source><volume>9</volume><elocation-id>e63385</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63385</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X -x</given-names></name><name><surname>Huang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Odor source localization algorithms on mobile robots: A review and future outlook</article-title><source>Robotics and Autonomous Systems</source><volume>112</volume><fpage>123</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2018.11.014</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connor</surname><given-names>EG</given-names></name><name><surname>McHugh</surname><given-names>MK</given-names></name><name><surname>Crimaldi</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Quantification of airborne odor plumes using planar laser-induced fluorescence</article-title><source>Experiments in Fluids</source><volume>59</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1007/s00348-018-2591-3</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crimaldi</surname><given-names>JP</given-names></name><name><surname>Koseff</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>High-resolution measurements of the spatial and temporal scalar structure of a turbulent plume</article-title><source>Experiments in Fluids</source><volume>31</volume><fpage>90</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1007/s003480000263</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Demir</surname><given-names>M</given-names></name><name><surname>Kadakia</surname><given-names>N</given-names></name><name><surname>Anderson</surname><given-names>HD</given-names></name><name><surname>Clark</surname><given-names>DA</given-names></name><name><surname>Emonet</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Walking <italic>Drosophila</italic> navigate complex plumes using stochastic decisions biased by the timing of odor encounters</article-title><source>eLife</source><volume>9</volume><elocation-id>e57524</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.57524</pub-id><pub-id pub-id-type="pmid">33140723</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duistermars</surname><given-names>BJ</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Crossmodal visual input for odor tracking during fly flight</article-title><source>Current Biology</source><volume>18</volume><fpage>270</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2008.01.027</pub-id><pub-id pub-id-type="pmid">18280156</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyhr</surname><given-names>JP</given-names></name><name><surname>Higgins</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The spatial frequency tuning of optic-flow-dependent behaviors in the bumblebee Bombus impatiens</article-title><source>The Journal of Experimental Biology</source><volume>213</volume><fpage>1643</fpage><lpage>1650</lpage><pub-id pub-id-type="doi">10.1242/jeb.041426</pub-id><pub-id pub-id-type="pmid">20435814</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dyhr</surname><given-names>JP</given-names></name><name><surname>Morgansen</surname><given-names>KA</given-names></name><name><surname>Daniel</surname><given-names>TL</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Flexible strategies for flight control: an active role for the abdomen</article-title><source>Journal of Experimental Biology</source><volume>216</volume><fpage>1523</fpage><lpage>1536</lpage><pub-id pub-id-type="doi">10.1242/jeb.077644</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujiwara</surname><given-names>T</given-names></name><name><surname>Kazawa</surname><given-names>T</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Fukushima</surname><given-names>R</given-names></name><name><surname>Uchino</surname><given-names>K</given-names></name><name><surname>Yamagata</surname><given-names>T</given-names></name><name><surname>Namiki</surname><given-names>S</given-names></name><name><surname>Haupt</surname><given-names>SS</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Odorant concentration differentiator for intermittent olfactory signals</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>16581</fpage><lpage>16593</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2319-14.2014</pub-id><pub-id pub-id-type="pmid">25505311</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>González</surname><given-names>E</given-names></name><name><surname>Marzal</surname><given-names>F</given-names></name><name><surname>Miñana</surname><given-names>A</given-names></name><name><surname>Doval</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Influence of exhaust hood geometry on the capture efficiency of lateral exhaust and push-pull ventilation systems in surface treatment tanks</article-title><source>Environmental Progress</source><volume>27</volume><fpage>405</fpage><lpage>411</lpage><pub-id pub-id-type="doi">10.1002/ep.10287</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haberkern</surname><given-names>H</given-names></name><name><surname>Hedwig</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Behavioural integration of auditory and antennal stimulation during phonotaxis in the field cricket Gryllus bimaculatus</article-title><source>The Journal of Experimental Biology</source><volume>219</volume><fpage>3575</fpage><lpage>3586</lpage><pub-id pub-id-type="doi">10.1242/jeb.141606</pub-id><pub-id pub-id-type="pmid">27609761</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hernandez-Reyes</surname><given-names>CA</given-names></name><name><surname>Fukushima</surname><given-names>S</given-names></name><name><surname>Shigaki</surname><given-names>S</given-names></name><name><surname>Kurabayashi</surname><given-names>D</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name><name><surname>Sezutsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Identification of Exploration and Exploitation Balance in the Silkmoth Olfactory Search Behavior by Information-Theoretic Modeling</article-title><source>Frontiers in Computational Neuroscience</source><volume>15</volume><elocation-id>629380</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2021.629380</pub-id><pub-id pub-id-type="pmid">33597856</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kanzaki</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Coordination of wing motion and walking suggests common control of zigzag motor program in a male silkworm moth</article-title><source>Journal of Comparative Physiology A</source><volume>182</volume><fpage>267</fpage><lpage>276</lpage><pub-id pub-id-type="doi">10.1007/s003590050177</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaushik</surname><given-names>PK</given-names></name><name><surname>Renz</surname><given-names>M</given-names></name><name><surname>Olsson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Characterizing long-range search behavior in Diptera using complex 3D virtual environments</article-title><source>PNAS</source><volume>117</volume><fpage>12201</fpage><lpage>12207</lpage><pub-id pub-id-type="doi">10.1073/pnas.1912124117</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kikas</surname><given-names>T</given-names></name><name><surname>Ishida</surname><given-names>H</given-names></name><name><surname>Webster</surname><given-names>DR</given-names></name><name><surname>Janata</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Chemical plume tracking. 1. Chemical information encoding</article-title><source>Analytical Chemistry</source><volume>73</volume><fpage>3662</fpage><lpage>3668</lpage><pub-id pub-id-type="doi">10.1021/ac0101813</pub-id><pub-id pub-id-type="pmid">11510831</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>W</given-names></name><name><surname>Farrell</surname><given-names>JA</given-names></name><name><surname>Card</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Tracking of Fluid-Advected Odor Plumes: Strategies Inspired by Insect Orientation to Pheromone</article-title><source>Adaptive Behavior</source><volume>9</volume><fpage>143</fpage><lpage>170</lpage><pub-id pub-id-type="doi">10.1177/10597123010093003</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lochmatter</surname><given-names>T</given-names></name><name><surname>Raemy</surname><given-names>X</given-names></name><name><surname>Matthey</surname><given-names>L</given-names></name><name><surname>Indra</surname><given-names>S</given-names></name><name><surname>Martinoli</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><conf-name>A comparison of casting and spiraling algorithms for odor source localization in laminar flow</conf-name><article-title>2008 IEEE International Conference on Robotics and Automation</article-title><pub-id pub-id-type="doi">10.1109/ROBOT.2008.4543357</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lockery</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The computational worm: spatial orientation and its neuronal basis in <italic>C. elegans</italic></article-title><source>Current Opinion in Neurobiology</source><volume>21</volume><fpage>782</fpage><lpage>790</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2011.06.009</pub-id><pub-id pub-id-type="pmid">21764577</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Minegishi</surname><given-names>R</given-names></name><name><surname>Takashima</surname><given-names>A</given-names></name><name><surname>Kurabayashi</surname><given-names>D</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Construction of a brain–machine hybrid system to evaluate adaptability of an insect</article-title><source>Robotics and Autonomous Systems</source><volume>60</volume><fpage>692</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1016/j.robot.2011.06.012</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murlis</surname><given-names>J</given-names></name><name><surname>Elkinton</surname><given-names>JS</given-names></name><name><surname>Cardé</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Odor Plumes and How Insects Use Them</article-title><source>Annual Review of Entomology</source><volume>37</volume><fpage>505</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1146/annurev.en.37.010192.002445</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murlis</surname><given-names>John</given-names></name><name><surname>Willis</surname><given-names>MA</given-names></name><name><surname>Carde</surname><given-names>RT</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Spatial and temporal structures of pheromone plumes in fields and forests</article-title><source>Physiological Entomology</source><volume>25</volume><fpage>211</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1046/j.1365-3032.2000.00176.x</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naik</surname><given-names>H</given-names></name><name><surname>Bastien</surname><given-names>R</given-names></name><name><surname>Navab</surname><given-names>N</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Animals in Virtual Environments</article-title><source>IEEE Transactions on Visualization and Computer Graphics</source><volume>26</volume><fpage>2073</fpage><lpage>2083</lpage><pub-id pub-id-type="doi">10.1109/TVCG.2020.2973063</pub-id><pub-id pub-id-type="pmid">32070970</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obara</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title><italic>Bombyx mori</italic> Mationg Dance : an Essential in Locationg the Female</article-title><source>Applied Entomology and Zoology</source><volume>14</volume><fpage>130</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1303/aez.14.130</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pang</surname><given-names>R</given-names></name><name><surname>van Breugel</surname><given-names>F</given-names></name><name><surname>Dickinson</surname><given-names>M</given-names></name><name><surname>Riffell</surname><given-names>JA</given-names></name><name><surname>Fairhall</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>History dependence in insect flight decisions during odor tracking</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1005969</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005969</pub-id><pub-id pub-id-type="pmid">29432454</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pansopha</surname><given-names>P</given-names></name><name><surname>Ando</surname><given-names>N</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Dynamic use of optic flow during pheromone tracking by the male silkmoth, <italic>Bombyx mori</italic></article-title><source>The Journal of Experimental Biology</source><volume>217</volume><fpage>1811</fpage><lpage>1820</lpage><pub-id pub-id-type="doi">10.1242/jeb.090266</pub-id><pub-id pub-id-type="pmid">24829328</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Radvansky</surname><given-names>BA</given-names></name><name><surname>Dombeck</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An olfactory virtual reality system for mice</article-title><source>Nature Communications</source><volume>9</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/s41467-018-03262-4</pub-id><pub-id pub-id-type="pmid">29483530</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reisenman</surname><given-names>CE</given-names></name><name><surname>Lei</surname><given-names>H</given-names></name><name><surname>Guerenstein</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Neuroethology of Olfactory-Guided Behavior and Its Potential Application in the Control of Harmful Insects</article-title><source>Frontiers in Physiology</source><volume>7</volume><elocation-id>271</elocation-id><pub-id pub-id-type="doi">10.3389/fphys.2016.00271</pub-id><pub-id pub-id-type="pmid">27445858</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renou</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pheromones and general odor perception in insects</article-title><source>Neurobiology of Chemical Communication</source><volume>1</volume><fpage>23</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1201/b16511-3</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rubner</surname><given-names>Y</given-names></name><name><surname>Guibas</surname><given-names>LJ</given-names></name><name><surname>Tomasi</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>The Earth Mover’s Distance, Multi-Dimensional Scaling, and Color-Based Image Retrieval</source><publisher-name>Stanford University Press</publisher-name></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>RA</given-names></name><name><surname>Bab-Hadiashar</surname><given-names>A</given-names></name><name><surname>Shepherd</surname><given-names>RL</given-names></name><name><surname>Wallace</surname><given-names>GG</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>A comparison of reactive robot chemotaxis algorithms</article-title><source>Robotics and Autonomous Systems</source><volume>45</volume><fpage>83</fpage><lpage>97</lpage><pub-id pub-id-type="doi">10.1016/S0921-8890(03)00120-9</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryohei</surname><given-names>K</given-names></name><name><surname>Naoko</surname><given-names>S</given-names></name><name><surname>Tatsuaki</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Self-generated zigzag turning of <italic>bombyx mori</italic> males during pheromone-mediated upwind walking</article-title><source>Zoological Science</source><volume>9</volume><fpage>515</fpage><lpage>527</lpage></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname><given-names>N</given-names></name><name><surname>Natesan</surname><given-names>D</given-names></name><name><surname>Sane</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Odor source localization in complex visual environments by fruit flies</article-title><source>The Journal of Experimental Biology</source><volume>221</volume><elocation-id>jeb172023</elocation-id><pub-id pub-id-type="doi">10.1242/jeb.172023</pub-id><pub-id pub-id-type="pmid">29146771</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmitz</surname><given-names>B</given-names></name><name><surname>Scharstein</surname><given-names>H</given-names></name><name><surname>Wendler</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Phonotaxis inGryllus campestris L. (Orthoptera, Gryllidae)</article-title><source>Journal of Comparative Physiology</source><volume>148</volume><fpage>431</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1007/BF00619782</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shigaki</surname><given-names>S</given-names></name><name><surname>Fukushima</surname><given-names>S</given-names></name><name><surname>Kurabayashi</surname><given-names>D</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A novel method for full locomotion compensation of an untethered walking insect</article-title><source>Bioinspiration &amp; Biomimetics</source><volume>12</volume><elocation-id>016005</elocation-id><pub-id pub-id-type="doi">10.1088/1748-3190/12/1/016005</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shigaki</surname><given-names>S</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Ando</surname><given-names>N</given-names></name><name><surname>Kurabayashi</surname><given-names>D</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Time-Varying Moth-Inspired Algorithm for Chemical Plume Tracing in Turbulent Environment</article-title><source>IEEE Robotics and Automation Letters</source><volume>3</volume><fpage>76</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1109/LRA.2017.2730361</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shigaki</surname><given-names>S</given-names></name><name><surname>Haigo</surname><given-names>S</given-names></name><name><surname>Hernandez Reyes</surname><given-names>C</given-names></name><name><surname>Sakurai</surname><given-names>T</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name><name><surname>Kurabayashi</surname><given-names>D</given-names></name><name><surname>Sezutsu</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Analysis of the role of wind information for efficient chemical plume tracing based on optogenetic silkworm moth behavior</article-title><source>Bioinspiration &amp; Biomimetics</source><volume>14</volume><elocation-id>046006</elocation-id><pub-id pub-id-type="doi">10.1088/1748-3190/ab1d34</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shigaki</surname><given-names>S</given-names></name><name><surname>Shiota</surname><given-names>Y</given-names></name><name><surname>Kurabayashi</surname><given-names>D</given-names></name><name><surname>Kanzaki</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Modeling of the Adaptive Chemical Plume Tracing Algorithm of an Insect Using Fuzzy Inference</article-title><source>IEEE Transactions on Fuzzy Systems</source><volume>28</volume><fpage>72</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1109/TFUZZ.2019.2915187</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehner</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Desert ant navigation: how miniature brains solve complex tasks</article-title><source>Journal of Comparative Physiology A</source><volume>189</volume><fpage>579</fpage><lpage>588</lpage><pub-id pub-id-type="doi">10.1007/s00359-003-0431-1</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willis</surname><given-names>MA</given-names></name><name><surname>Arbas</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Odor-modulated upwind flight of the sphinx moth, Manduca sexta L</article-title><source>Journal of Comparative Physiology A</source><volume>169</volume><fpage>427</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1007/BF00197655</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willis</surname><given-names>MA</given-names></name><name><surname>Avondet</surname><given-names>JL</given-names></name><name><surname>Zheng</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The role of vision in odor-plume tracking by walking and flying insects</article-title><source>The Journal of Experimental Biology</source><volume>214</volume><fpage>4121</fpage><lpage>4132</lpage><pub-id pub-id-type="doi">10.1242/jeb.036954</pub-id><pub-id pub-id-type="pmid">22116754</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wyatt</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Pheromones and Animal Behavior: Chemical Signals and Signatures</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781139030748</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yanagawa</surname><given-names>R</given-names></name><name><surname>Shigaki</surname><given-names>S</given-names></name><name><surname>Kurabayashi</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><conf-name>Construction of chemical plume tracing simulator in a non-rectifying environment</conf-name><article-title>IEEE</article-title></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>N</given-names></name><name><surname>Ma</surname><given-names>Q</given-names></name><name><surname>Jin</surname><given-names>M</given-names></name><name><surname>Zhang</surname><given-names>S</given-names></name><name><surname>Guan</surname><given-names>N</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name><name><surname>Dai</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Abdominal-Waving Control of Tethered Bumblebees Based on Sarsa With Transformed Reward</article-title><source>IEEE Transactions on Cybernetics</source><volume>49</volume><fpage>3064</fpage><lpage>3073</lpage><pub-id pub-id-type="doi">10.1109/TCYB.2018.2838595</pub-id><pub-id pub-id-type="pmid">29994492</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>NN</given-names></name><name><surname>Deng</surname><given-names>YL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Virtual reality: A state-of-the-art survey</article-title><source>International Journal of Automation and Computing</source><volume>6</volume><fpage>319</fpage><lpage>325</lpage><pub-id pub-id-type="doi">10.1007/s11633-009-0319-9</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72001.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" link-type="continued-by" object-id="10.1101/2021.09.14.460318" object-id-type="id" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.14.460318"/></front-stub><body><p>This paper uses a multi-model virtual reality system to assess which combinations of visual, wind, and olfactory information male silk moths rely on to find a female. The overall conclusion is that for the moths to search effectively, wind direction information is an important input. Vision, on the other hand, while it is used to control angular velocity, does not appear to be important for the moths to search effectively. This paper is of interest to neuroscientists and engineers interested in how multimodal sensory input controls navigational behavior. The experiments and modeling effort provide an advance in our understanding of how odor and wind information is combined in male silkmoths as they search for females.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72001.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Berman</surname><given-names>Gordon J</given-names></name><role>Reviewing Editor</role><aff><institution>Emory University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.09.14.460318">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.09.14.460318v1">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Analysis of Multisensory-Motor Integration in Olfactory Navigation of Silkmoth, <italic>Bombyx mori</italic>, using Virtual Reality System&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Aleksandra Walczak as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) For Figure 1, was the smoke plume measured in 3D? If not, then the 2-dimensional projection of the 3-dimensional plume will create a substantially more continuous (less intermittent) plume than an insect would experience when moving through a 3-dimensional turbulent plume. This substantially limits the realistic nature of the experiments, and therefore the interpretation of the results. In particular, this limits how the frequency of odor detection results can be interpreted since the underlying plume model is to simplified. The idea of the smoke plume measurement to provide a realistic plume model is great, but the analysis of the plume must be done in 3D to provide a realistic experience to the moths.</p><p>2) For Figure 2, it is hard to interpret what the EMD result actually means beyond being similar or different. While this offers a nice summary measurement, to really understand what is going on, a more mechanistic analysis of the individual trajectories is needed. For example, please show individual trajectories of the moths, and make an effort to analyze their turn by turn decisions. Given that a model is presented at the end of the paper, there is a substantial opportunity to provide deeper insight by comparing the individual trajectories with the trajectories generated by the model. Specifically, it would good to see not just a better explanation of what the EMD metric is/means. The EMD metric seems to analyze what results from turn-by turn-decisions, but it would be nice to understand what those turn-by-turn decisions actually are (since there is data for that kind of analysis).</p><p>3) There is some mismatch between the model and the experiments. The model discusses relative timing of odor detections on the left and right antenna versus the direction of the wind. However, in the experiments, it is not clear whether odor stimuli were presented to have different timings for the left and right antenna. Furthermore, if this was included the in experiments, the results do not seem to be analyzed in a clear manner. A more comprehensive and clear analysis of how odor arrival time on the left and right antenna versus the supplied wind direction information influenced mechanistic behavioral decisions is needed.</p><p>4) There are huge swaths of references missing that should be considered to provide better context. Many of these references are for other insect species, namely the fruit fly <italic>Drosophila melanogaster</italic>, for which a great deal is known about both olfactory search, wind sensing, and multi-sensory integration (vision + olfaction, in particular, but also mechano-visual integration). Here are some PI's the authors should read up on, and choose several references from each to include: Rachel Wilson, Kathy Nagel, Mark Willis, Mark Frye, Michael Dickinson, Tom Daniel. A useful review to help collect some of these references: Algorithms for Olfactory Search across Species, Baker et al., 2018, though there are several papers relevant to this study that have been published since this review as well. In addition, the use of VR for exploring insect behaviors like search and navigation is hardly a new development at this point in time. Indeed, work on flies, <italic>Drosophila</italic> and otherwise, going back decades has used VR to extract principles regarding issues such as visually mediated flight control or walking. More pertinent to the ideas of multimodal sensory integration explored here, over the past decade numerous researchers have combined visual input with odor and other cues to discern the relative importance of each of these modalities during search behavior. For example, see Duistermars and Frye, 2008. Generally, the reviewers felt that the paper overemphasizes the technical advance without providing sufficient biological context. So much work has been done on Bombyx that a paper using these methods has the ability to address, but much of that literature is absent from the paper. Focusing more on the behavior will broaden the appeal of the paper by putting it in conversation with a well-established phenomenon.</p><p>5) While the model is well-done and fits with the goals of the paper. Asking about the role of wind direction in this behavior is an important step given the behavioral data presented. However, the reviewers were not convinced, based on the presented data, that the new model developed by the authors is much better than the surge-zigzag model. The success rates are slightly different (is the statistical difference a function of the number of runs or timesteps?) and both models search about the same amount of time before finding the source. Finally, the migration probability maps are rather similar, so it is hard to conclude that factoring in wind direction is necessary to get good performance out of the model.</p><p>6) Overall, the manuscript would greatly benefit from professional editing to improve the writing. Many sections read more as stream of consciousness than tightly edited scientific prose. Some minor writing-related issues are outlined in the comments below, but this list is far from exhaustive. Improved writing would greatly advance the scientific impact of the work.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.72001.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) For Figure 1, was the smoke plume measured in 3D? If not, then the 2-dimensional projection of the 3-dimensional plume will create a substantially more continuous (less intermittent) plume than an insect would experience when moving through a 3-dimensional turbulent plume. This substantially limits the realistic nature of the experiments, and therefore the interpretation of the results. In particular, this limits how the frequency of odor detection results can be interpreted since the underlying plume model is to simplified. The idea of the smoke plume measurement to provide a realistic plume model is great, but the analysis of the plume must be done in 3D to provide a realistic experience to the moths.</p></disp-quote><p>Thank you for your valuable comments. We apologize for the inadequate explanation. We captured the smoke plume movement on a two-dimensional plane as described in the virtual odor field of the “Materials and methods” chapter (section 4.2). We set a laser sheet with a thickness of about 1 mm at a distance of 10 mm from the ground. This height of the laser was approximately equal to the height of the silkmoth antennae from the ground. The smoke particles (&lt; 10 µm) were emitted into the space at a cycle of 1 Hz. The movement of the smoke at the height of the antennae of the silkmoth was reproduced in the virtual environment because only the smoke reflected on this laser sheet was photographed by the high-sensitivity camera. Note that the movement of the silkmoth’s head was just a few millimeters at the maximum; therefore, this was sufficient to reproduce the behavior of smoke reflected on a two-dimensional plane in this paper. Of course, if we want to use the same virtual odor field for flying insects, we can reproduce the three-dimensional realistic smoke behavior by preparing multiple laser sheets and measuring them. We added these explanations and an outline of the experimental setup of capturing the smoke.</p><p>Revised part: “Virtual odor field”, Figure 1—figure supplement 2</p><disp-quote content-type="editor-comment"><p>2) For Figure 2, it is hard to interpret what the EMD result actually means beyond being similar or different. While this offers a nice summary measurement, to really understand what is going on, a more mechanistic analysis of the individual trajectories is needed. For example, please show individual trajectories of the moths, and make an effort to analyze their turn by turn decisions. Given that a model is presented at the end of the paper, there is a substantial opportunity to provide deeper insight by comparing the individual trajectories with the trajectories generated by the model. Specifically, it would good to see not just a better explanation of what the EMD metric is/means. The EMD metric seems to analyze what results from turn-by turn-decisions, but it would be nice to understand what those turn-by-turn decisions actually are (since there is data for that kind of analysis).</p></disp-quote><p>Thank you for your helpful comments. As you pointed out, we added trajectory figures of all the trials. Moreover, the heading angle change during navigation is represented by a histogram of polar coordinates. If the frequency of the heading angle histogram is high, it means that a silkmoth moves in that direction. Please note that the 0° direction is the + x direction (upwind direction), not necessarily the direction of the odor source. Because the silkmoth searches in all directions using a turning motion, a certain frequency occurs in all directions. From trajectories and heading angle histograms, the odor-only trajectory (cond. 1) demonstrated that although the frequency of moving with the heading angle toward the windward direction was high as a whole, leaving behavior from the range with a high probability of odor arrival occurred, so that the search failed. By providing wind (cond. 2) and visual (cond. 3) stimuli in addition to odor, the deviated behaviors from the range with a high probability of odor arrival were extremely reduced; therefore, the silkmoth modulates the odor source search behavior using other sensory information.</p><p>When the wind stimuli were input, the behavior modulation applied so that the heading angle changed in the windward direction even if there was deviated behavior from an area with a high probability of odor arrival. In the case of the visual input, although there was no trajectory that deviated from the odor reach area, there were many angles with a frequency other than 0° in the histogram of the heading angles, which suggests that an omnidirectional search is conducted more carefully using posture control. As a result, we found that the search success rate of cond. 3 was lower than that of cond. 2. The heading angle histogram of cond. 4 (presented from the opposite direction to that actually received by the wind) indicates that a peak appears in the downwind direction, so that wind information has an effect on search behavior. Even when the visual presentation direction was opposite, an extreme peak in the upwind direction did not appear, and the result was that a peak also occurred in the crosswind direction. When the visual information is presented in the opposite direction (the visual information is input as if it rotates in the opposite direction), it gives the illusion that it does not rotate correctly, and it performs more rotational actions. However, because the odor has a higher priority sensory information, the search success rate does not decrease. From this, it is presumed that odor is of course an important sensory input, but information on wind direction is also important and visual information is used only for body control.</p><p>We plotted the trajectories and heading angle histograms of each trial simulated under the same initial condition as the biological experiment. As a result, we found that the search algorithm had a different trajectory from the actual moth. On the other hand, the proposed algorithm (MiM2) was found to be quite similar in trajectory and heading angle histogram. From this, we concluded that the proposed algorithm reproduced the female localization behavior of the silkmoth compared to the conventional algorithm.</p><p>Some tendency appeared in behavioral changes depending on the environmental conditions presented, but it is difficult to evaluate the overlapping parts of trajectories simply by plotting each trajectory. Hence, we visualized trajectories as a two-dimensional histogram in order to comprehensively consider which route was selected. Because a two-dimensional histogram is a kind of probability distribution, we decided to use an earth mover’s distance (EMD) that expresses the similarity between probability distributions by distance in order to quantitatively determine whether the probability of selecting the path differs or is similar depending on the environmental conditions. If the distance of the two-dimensional histogram of the trajectory is very short (EMD value is small), the same route is selected and moved, and if the distance is long, the route is selected differently. In other words, we can quantitatively evaluate the change in behavioral choices depending on the environmental conditions presented. Conventionally, it is determined whether or not the trajectories are qualitatively similar, but now we can evaluate trajectories quantitatively based on the EMD value.</p><p>Revised part: Figure 2 and 6, Line 140—162 and 293—302.</p><disp-quote content-type="editor-comment"><p>3) There is some mismatch between the model and the experiments. The model discusses relative timing of odor detections on the left and right antenna versus the direction of the wind. However, in the experiments, it is not clear whether odor stimuli were presented to have different timings for the left and right antenna. Furthermore, if this was included the in experiments, the results do not seem to be analyzed in a clear manner. A more comprehensive and clear analysis of how odor arrival time on the left and right antenna versus the supplied wind direction information influenced mechanistic behavioral decisions is needed.</p></disp-quote><p>We appreciate the Reviewer’s comment on this point. We utilized two solenoid valves to provide odor stimuli independently to the left and right antennae of a silkmoth. However, as you pointed out, it is not clear whether the silkmoth actually received the odor stimuli independently. Accordingly, we carried out additional behavioral experiments that eliminated the left and right odor stimulus inputs. In other words, we conducted the experiment to provide odor stimulus to both antennae of the actual silkmoth, no matter what odor stimulus was received in the virtual environment. From the results of previous behavioral experiments, the silkmoth utilized the bilateral information of odor to elicit effective female search behavior because the initial rotation direction of the zigzagging motion was determined by the timing of presentation to the left and right antennae. Therefore, we expected that the search behavior would change by eliminating this left and right odor information. We added the experimental results into the supplementary materials. By eliminating bilateral information of odor, the search success rate clearly decreased and the search time increased. From this, we concluded that the VR correctly provided the bilateral odor information and that the silkmoth executed the female search by preferentially using bilateral information. These experimental data and explanations have been added to the manuscript.</p><p>Revised part: Figure 2—figure supplement 1</p><disp-quote content-type="editor-comment"><p>4) There are huge swaths of references missing that should be considered to provide better context. Many of these references are for other insect species, namely the fruit fly <italic>Drosophila melanogaster</italic>, for which a great deal is known about both olfactory search, wind sensing, and multi-sensory integration (vision + olfaction, in particular, but also mechano-visual integration). Here are some PI’s the authors should read up on, and choose several references from each to include: Rachel Wilson, Kathy Nagel, Mark Willis, Mark Frye, Michael Dickinson, Tom Daniel. A useful review to help collect some of these references: Algorithms for Olfactory Search across Species, Baker et al., 2018, though there are several papers relevant to this study that have been published since this review as well. In addition, the use of VR for exploring insect behaviors like search and navigation is hardly a new development at this point in time. Indeed, work on flies, Drosophila and otherwise, going back decades has used VR to extract principles regarding issues such as visually mediated flight control or walking. More pertinent to the ideas of multimodal sensory integration explored here, over the past decade numerous researchers have combined visual input with odor and other cues to discern the relative importance of each of these modalities during search behavior. For example, see Duistermars and Frye, 2008. Generally, the reviewers felt that the paper overemphasizes the technical advance without providing sufficient biological context. So much work has been done on Bombyx that a paper using these methods has the ability to address, but much of that literature is absent from the paper. Focusing more on the behavior will broaden the appeal of the paper by putting it in conversation with a well-established phenomenon.</p></disp-quote><p>We thank the Reviewer for this comment. We added some papers including the one that you presented. It has been reported that fruit flies and flying moths move in an upwind direction when they encounter an odor. However, silkmoths do not necessarily move upwind, but move in the direction of odor detection. In the case of the silkmoth, the female search behavior is modulated by information on wind direction and this behavior may be equivalent to the upwind movement of fruit flies or flying moths during odor detection. The results of the current study and the above results lead us to conclude that our VR system contributed to a certain extent because we were able to deal with the problem of &quot;it is often difficult to measure behavioral responses to controlled turbulent stimuli&quot; in the literature.</p><p>Regarding VR research [1], the recent definition of VR is that &quot;virtual reality creates a physical and a mental space for people.&quot; In other words, we need to connect a device that can provide multisensory stimuli to organisms (physical space) and a mental space where avatars reflect their actions. Based on this definition, we claim that most biological experiments with insects have been conducted in a physical space (just utilizing multisensory stimulators) and have not measured the purposeful behaviors that result from connection to the mental space. Behavioral experiments using a multi-sensory stimulator have revealed how each modality is utilized, but it has not been directly investigated how other modalities affect actual navigation that predominantly utilizes an odor (as in the current study). In fact, the previously proposed insect-inspired algorithm for navigation was modeled based on experimental data using a multisensory stimulus device, but as a result of simulation, it behaves far differently to the actual insect. On the other hand, the model derived from the experimental results using the VR system were able to reproduce the behavior of the insect (silkmoth). In other words, we claim that in order to model the superior functions of an organism, it is necessary to do more than provide multisensory stimuli, and it is important to obtain the relationship between multisensory stimuli and behavioral output when the organism performs the actual purposeful behavior (e.g., navigation behavior).</p><p>[1] Zhou NN, Deng YL. Virtual reality: A state-of-the-art survey. International Journal of Automation and Computing. 672 2009; 6(4):319–325.</p><p>Revised part: “Introduction”</p><disp-quote content-type="editor-comment"><p>5) While the model is well-done and fits with the goals of the paper. Asking about the role of wind direction in this behavior is an important step given the behavioral data presented. However, the reviewers were not convinced, based on the presented data, that the new model developed by the authors is much better than the surge-zigzag model. The success rates are slightly different (is the statistical difference a function of the number of runs or timesteps?) and both models search about the same amount of time before finding the source. Finally, the migration probability maps are rather similar, so it is hard to conclude that factoring in wind direction is necessary to get good performance out of the model.</p></disp-quote><p>Thank you for your valuable comments. Because a surge-zigzagging algorithm utilizes only an odor stimulus to determine actions, it would seem that an agent will move to the edge where the change in odor information occurs most. The behavior of MiM2 is modulated by information on wind direction, and it moves to the areas with a high probability of odor reach. Hence, we thought that the effectiveness of the model could be shown by starting the search for the agent from near the edge. Consequently, we set the initial position (x, y) = (300, ± 100) in the area where the odor is hard to reach and carried out the simulation. As a result, the MiM2 algorithm, which actively moves in the area where the probability of odor reach is high, was significantly better than other conventional algorithms. As for the search time, because MiM2 was based on surge-zigzagging, the movement speeds were almost the same; therefore, both algorithms (MiM2 and surge-zigzagging) required almost the same search time. However, we found that there was a difference in the search success rate because there was a difference in the selected route.</p><p>Revised part: Lines 282—311 at “Modeling and validation of behavioral modulation mechanisms”</p><disp-quote content-type="editor-comment"><p>6) Overall, the manuscript would greatly benefit from professional editing to improve the writing. Many sections read more as stream of consciousness than tightly edited scientific prose. Some minor writing-related issues are outlined in the comments below, but this list is far from exhaustive. Improved writing would greatly advance the scientific impact of the work.</p></disp-quote><p>Thank you for your helpful suggestions. We received English proofreading from an expert to improve the manuscript (Editage).</p></body></sub-article></article>