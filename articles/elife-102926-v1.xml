<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">102926</article-id><article-id pub-id-type="doi">10.7554/eLife.102926</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.102926.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Auditory cortex learns to discriminate audiovisual cues through selective multisensory enhancement</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chang</surname><given-names>Song</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zheng</surname><given-names>Beilin</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Keniston</surname><given-names>Les</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Xu</surname><given-names>Jinghong</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2864-4196</contrib-id><email>jhxu@bio.ecnu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Yu</surname><given-names>Liping</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9771-5971</contrib-id><email>lpyu@bio.ecnu.edu.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02n96ep67</institution-id><institution>Key Laboratory of Brain Functional Genomics (Ministry of Education and Shanghai), School of Life Sciences, East China Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03jjm4b17</institution-id><institution>College of Information Engineering, Hangzhou Vocational and Technical College</institution></institution-wrap><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02fs2ee62</institution-id><institution>Department of Biomedical Sciences, Kentucky College of Osteopathic Medicine, University of Pikeville</institution></institution-wrap><addr-line><named-content content-type="city">Pikeville</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>22</day><month>04</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP102926</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-09-16"><day>16</day><month>09</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-09-16"><day>16</day><month>09</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2024.09.16.613280"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-11-20"><day>20</day><month>11</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102926.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-04-02"><day>02</day><month>04</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.102926.2"/></event></pub-history><permissions><copyright-statement>© 2024, Chang et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Chang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-102926-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-102926-figures-v1.pdf"/><abstract><p>Multisensory object discrimination is essential in everyday life, yet the neural mechanisms underlying this process remain unclear. In this study, we trained rats to perform a two-alternative forced-choice task using both auditory and visual cues. Our findings reveal that multisensory perceptual learning actively engages auditory cortex (AC) neurons in both visual and audiovisual processing. Importantly, many audiovisual neurons in the AC exhibited experience-dependent associations between their visual and auditory preferences, displaying a unique integration model. This model employed selective multisensory enhancement for the auditory-visual pairing guiding the contralateral choice, which correlated with improved multisensory discrimination. Furthermore, AC neurons effectively distinguished whether a preferred auditory stimulus was paired with its associated visual stimulus using this distinct integrative mechanism. Our results highlight the capability of sensory cortices to develop sophisticated integrative strategies, adapting to task demands to enhance multisensory discrimination abilities.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>multisensory integration</kwd><kwd>auditory cortex</kwd><kwd>decision making</kwd><kwd>cortical plasticity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rat</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution>STI2030-major projects</institution></institution-wrap></funding-source><award-id>2021ZD0202600</award-id><principal-award-recipient><name><surname>Yu</surname><given-names>Liping</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32371046</award-id><principal-award-recipient><name><surname>Yu</surname><given-names>Liping</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>31970925</award-id><principal-award-recipient><name><surname>Yu</surname><given-names>Liping</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>32271057</award-id><principal-award-recipient><name><surname>Xu</surname><given-names>Jinghong</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Experience-driven plasticity in auditory cortex neurons establishes cross-modal associations, enabling enhanced discrimination of multisensory cues through specialized integrative strategies.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In our daily lives, the integration of visual and auditory information is crucial for detecting, discriminating, and identifying multisensory objects. When faced with stimuli like seeing an apple while hearing the sound 'apple' or 'peach' simultaneously, our brain must synthesize these inputs to form perceptions and make judgments based on the knowledge of whether the visual information matches the auditory input. To date, it remains unclear exactly where and how the brain integrates across-sensory inputs to benefit cognition and behavior. Traditionally, higher association areas in the temporal, frontal, and parietal lobes were thought to be pivotal for merging visual and auditory signals. However, recent research suggests that even primary sensory cortices, such as auditory and visual cortices, contribute significantly to this integration process (<xref ref-type="bibr" rid="bib38">Perrodin et al., 2015</xref>; <xref ref-type="bibr" rid="bib17">Ghazanfar et al., 2005</xref>; <xref ref-type="bibr" rid="bib23">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib5">Bizley et al., 2007</xref>; <xref ref-type="bibr" rid="bib2">Atilgan et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Meijer et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Ibrahim et al., 2016</xref>).</p><p>Studies have shown that visual stimuli can modulate auditory responses in the AC via pathways involving the lateral posterior nucleus of the thalamus (<xref ref-type="bibr" rid="bib11">Chou et al., 2020</xref>), and the deep layers of the AC serve as crucial hubs for integrating cross-modal contextual information (<xref ref-type="bibr" rid="bib33">Morrill et al., 2022</xref>). Even irrelevant visual cues can affect sound discrimination in AC (<xref ref-type="bibr" rid="bib10">Chang et al., 2022</xref>). Anatomical investigations reveal reciprocal nerve projections between auditory and visual cortices (<xref ref-type="bibr" rid="bib5">Bizley et al., 2007</xref>; <xref ref-type="bibr" rid="bib46">Stehberg et al., 2014</xref>; <xref ref-type="bibr" rid="bib15">Falchier et al., 2002</xref>; <xref ref-type="bibr" rid="bib8">Cappe and Barone, 2005</xref>; <xref ref-type="bibr" rid="bib7">Campi et al., 2010</xref>; <xref ref-type="bibr" rid="bib3">Banks et al., 2011</xref>), highlighting the interconnected nature of these sensory systems. Moreover, two-photon calcium imaging in awake mice has shown that audiovisual encoding in the primary visual cortex depends on the temporal congruency of stimuli, with temporally congruent audiovisual stimuli eliciting balanced enhancement and suppression, whereas incongruent stimuli predominantly result in suppression (<xref ref-type="bibr" rid="bib31">Meijer et al., 2017</xref>). However, despite these findings, the precise mechanisms by which sensory cortices integrate cross-sensory inputs for multisensory object discrimination remain unknown.</p><p>Previous research on cross-modal modulation has predominantly focused on anesthetized or passive animal models (<xref ref-type="bibr" rid="bib23">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib2">Atilgan et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Meijer et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Stein et al., 2014</xref>; <xref ref-type="bibr" rid="bib13">Deneux et al., 2019</xref>), exploring the influence of stimulus properties and spatiotemporal arrangements on sensory interactions (<xref ref-type="bibr" rid="bib23">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib31">Meijer et al., 2017</xref>; <xref ref-type="bibr" rid="bib13">Deneux et al., 2019</xref>; <xref ref-type="bibr" rid="bib52">Wallace et al., 2020</xref>; <xref ref-type="bibr" rid="bib56">Xu et al., 2018</xref>). However, sensory representations, including multisensory processing, are known to be context-dependent (<xref ref-type="bibr" rid="bib14">Elgueda et al., 2019</xref>; <xref ref-type="bibr" rid="bib19">Han et al., 2022</xref>; <xref ref-type="bibr" rid="bib36">Otazu et al., 2009</xref>) and can be shaped by perceptual learning (<xref ref-type="bibr" rid="bib55">Xu et al., 2014</xref>; <xref ref-type="bibr" rid="bib50">Vincis and Fontanini, 2016</xref>; <xref ref-type="bibr" rid="bib25">Knöpfel et al., 2019</xref>). Therefore, the way sensory cortices integrate information during active tasks may differ significantly from what has been observed in passive or anesthetized states. Relatively few studies have investigated cross-modal interactions during the performance of multisensory tasks, regardless of the brain region studied (<xref ref-type="bibr" rid="bib16">Garner and Keller, 2022</xref>; <xref ref-type="bibr" rid="bib40">Raposo et al., 2014</xref>; <xref ref-type="bibr" rid="bib20">Hirokawa et al., 2011</xref>). This limits our understanding of multisensory integration in sensory cortices, particularly regarding: (1) Do neurons in sensory cortices adopt consistent integration strategies across different audiovisual pairings, or do these strategies vary depending on the pairing? (2) How does multisensory perceptual learning reshape cortical representations of audiovisual objects? (3) How does the congruence between auditory and visual features—whether they ‘match’ or ‘mismatch’ based on learned associations—impact neural integration?</p><p>We investigated this by training rats on a multisensory discrimination task involving both auditory and visual stimuli. We then examined cue selectivity and auditory-visual integration in AC neurons of these well-trained rats. Our findings demonstrate that multisensory discrimination training fosters experience-dependent associations between auditory and visual features within AC neurons. During task performance, AC neurons often exhibited multisensory enhancement for the preferred auditory-visual pairing, with no such enhancement observed for the non-preferred pairing. Importantly, this selective enhancement correlated with the animals' ability to discriminate the audiovisual pairings. Furthermore, the degree of auditory-visual integration was linked to the congruence of auditory and visual features. Our findings suggest AC plays a more significant role in multisensory integration than previously thought.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Multisensory discrimination task in freely moving rats</title><p>To investigate how AC neurons integrate visual information into audiovisual processing, we trained 10 adult male Long Evans rats on a multisensory discrimination task (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). During the task, the rat initiated a trial by inserting its nose into the central port, which triggered a randomly selected target stimulus from a pool of six cues: two auditory (3 kHz pure tone, A<sub>3k</sub>;10 kHz pure tone, A<sub>10k</sub>), two visual (horizontal light bar, V<sub>hz</sub>; vertical light bar, V<sub>vt</sub>), and two multisensory cues (A<sub>3k</sub>V<sub>hz</sub>, A<sub>10k</sub>V<sub>vt</sub>). Based on the cue, the rats had to choose the correct left or right port for a water reward within 3 s after the stimulus onset. Incorrect choices or no response resulted in a 5-s timeout. The training proceeded in two stages. In the first stage, which typically lasted 3–5 weeks, rats were trained to discriminate between two audiovisual cues. In the second stage, an additional four unisensory cues were introduced, training the rats to discriminate a total of six cues. To ensure reliable performance, rats needed to achieve 80% accuracy (including at least 70% correct in each modality) for three consecutive sessions (typically taking 2–4 months to achieve this level of accuracy).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Multisensory discrimination task for rats.</title><p>(<bold>a</bold>) Schematic illustration of the behavioral task. A randomly selected stimulus was triggered when a rat placed its nose in the central port. If the triggered stimulus was a 10 kHz pure tone (A<sub>10k</sub>), a vertical light bar (V<sub>vt</sub>), or their combination (A<sub>10k</sub>V<sub>vt</sub>), the rat was rewarded at the left port. Conversely, for other stimuli (a 3 kHz pure tone (A<sub>3k</sub>), a horizontal light bar (V<sub>hz</sub>), or A<sub>3k</sub>V<sub>hz</sub>), the rat was required to move to the right port. Visual stimuli were presented via custom-made LED matrix panels, with one panel for each side. Auditory stimuli were delivered through a centrally located speaker. (<bold>b</bold>) The mean performance for each stimulus condition across rats. Circles connected by a gray line represent data from one individual rat. (<bold>c</bold>) Cumulative frequency distribution of reaction time (time from cue onset to leaving the central port) for one representative rat in auditory, visual and multisensory trials (correct only). (<bold>d</bold>) Comparison of average reaction times across rats in auditory, visual, and multisensory trials (correct only). ***, p&lt;0.001. Error bars represent SDs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig1-v1.tif"/></fig><p>Consistent with previous studies (<xref ref-type="bibr" rid="bib10">Chang et al., 2022</xref>; <xref ref-type="bibr" rid="bib40">Raposo et al., 2014</xref>; <xref ref-type="bibr" rid="bib1">Angelaki et al., 2009</xref>; <xref ref-type="bibr" rid="bib45">Smyre et al., 2021</xref>), rats performed better when responding to combined auditory and visual cues (multisensory trials) compared to trials with only auditory or visual cue (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). This suggests that multisensory cues facilitate decision-making. Reaction time, measured as the time from cue onset to when the rat left the central port, was shorter in multisensory trials (<xref ref-type="fig" rid="fig1">Figure 1c and d</xref>). This indicates that multisensory processing in the brain helps rats discriminate between cues more efficiently (mean reaction time across rats, multisensory, 367±53ms; auditory, 426±60ms; visual, 417±53ms; multisensory vs. each unisensory, p&lt;0.001, paired t-test. <xref ref-type="fig" rid="fig1">Figure 1d</xref>).</p></sec><sec id="s2-2"><title>Auditory, visual, and multisensory discrimination of AC neurons in multisensory discrimination task</title><p>To investigate the discriminative and integrative properties of AC neurons, we implanted tetrodes in the right primary auditory cortex of well-trained rats (n=10; <xref ref-type="fig" rid="fig2">Figure 2a</xref>). Careful implantation procedures were designed and followed to minimize neuron sampling biases. The characteristic frequencies of recorded AC neurons, measured immediately after tetrode implantation, spanned a broad frequency range (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We examined a total of 559 AC neurons (56±29 neurons per rat) that responded to at least one target stimulus during task engagement. Interestingly, a substantial proportion of neurons (35%, 196/559) showed visual responses (<xref ref-type="fig" rid="fig2">Figure 2b</xref>), which was notably higher than the 14% (14%, 39/275, χ2 = 27.5, p &lt; 0.001) recorded in another group of rats (n=8) engaged in a free-choice task where rats were not required to discriminate triggered cues and could receive water rewards with any behavioral response (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). This suggests multisensory discrimination training enhances visual representation in the auditory cortex. To optimize the alignment of auditory and visual responses and reveal the greatest potential for multisensory integration, we used long-ramp pure tone auditory stimuli and quick LED-array-elicited visual stimuli. While auditory responses were still slightly earlier than visual responses (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), the temporal alignment was sufficient to support robust integration. Notably, 27% (150/559) of neurons responded to both auditory and visual stimuli (audiovisual neurons), and a small number (n=7) only responded to the auditory-visual combination (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Auditory, visual and multisensory selectivity in task engagement.</title><p>(<bold>a</bold>) Histological verification of recording locations within the primary auditory cortex (Au1). (<bold>b</bold>) Proportion of neurons categorized based on their responsiveness to auditory-only (A), visual-only (V), both auditory and visual (A, V), or audiovisual-only (AV) stimuli during task engagement. (<bold>c–d</bold>) Rasters (top) and peristimulus time histograms (PSTHs, bottom) show responses of two exemplar neurons in A<sub>3k</sub> (light blue), A<sub>10k</sub> (dark blue), V<sub>hz</sub> (light green), V<sub>vt</sub> (dark green), A<sub>3k</sub>V<sub>hz</sub> (light orange), and A<sub>10k</sub>V<sub>vt</sub> (dark orange) trials (correct only). Mean spike counts in PSTHs were computed in 10 ms time windows and then smoothed with a Gaussian kernel (<bold>σ=50ms</bold>). Black bars indicate the stimulus onset and duration. (<bold>e</bold>) Mean normalized response PSTHs across neurons in auditory (top), visual (middle), and multisensory (bottom) trials (correct only) for multisensory discrimination (left) and free-choice (right) groups. Shaded areas represent mean ± SEM. Black bars indicate stimulus onset and duration. (<bold>f</bold>) Histograms of auditory (top), visual (middle) and multisensory (bottom) selectivity for multisensory discrimination (left) and free-choice (right) groups. Filled bars indicate neurons for which the selectivity index was significantly different from 0 (permutation test, p&lt;0.05, bootstrap n=5000). The dash line represents zero. (<bold>g</bold>) Comparison of visual selectivity distribution between audiovisual (top) and visual (bottom) neurons. (<bold>h</bold>) Comparison of auditory (A), visual (V), and multisensory (AV) selectivity of 150 audiovisual neurons, ordered by auditory selectivity. (<bold>i</bold>) Average absolute auditory, visual and multisensory selectivity across 150 audiovisual neurons. Error bars represent SDs. *, p&lt;0.05; ***, p&lt;0.001. (<bold>j</bold>) Decoding accuracy of populations in the multisensory discrimination group. Decoding accuracy (cross-validation accuracy based on SVM) of populations between responses in two auditory (blue), two visual (green), and two multisensory (red) trials. Each decoding value was calculated in a 100ms temporal window moving at the step of 10ms. Shadowing represents the mean ± SD from bootstrapping of 100 repeats. Two dashed lines represent 90% of decoding accuracy for auditory and multisensory conditions. (<bold>k</bold>) Decoding accuracy of populations in the free-choice group.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Characteristic frequency (CF) and response to target sound stimuli of AC neurons recorded in well-trained rats under anesthesia.</title><p>(<bold>a</bold>) Frequency-intensity tonal receptive fields (TRFs) of three representative AC neurons. The red triangle denotes CF. To create the TRF, we measured responses to a series of pure tone sounds (1–45 kHz in 1 kHz steps) at different intensities (20–60 dB in 10 dB increments). The color bar indicates the range of maximum and minimum spike response counts observed within the TRFs. (<bold>b</bold>) CF distributions of neurons recorded from two representative rats. (<bold>c</bold>) Overall distribution of all recorded neurons. (<bold>d</bold>) Comparison of responses to 3 kHz (A<sub>3k</sub>) and 10 kHz (A<sub>10k</sub>) pure tones. Each circle represents one neuron. (<bold>e</bold>) Comparison of mean response magnitude across populations for A<sub>3k</sub> and A<sub>10k</sub> conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Stimulus-evoked neural responses and latency comparison for auditory and visual stimuli.</title><p>(<bold>a</bold>) Schematic of the auditory and visual stimuli used in the experiment. Auditory stimuli consisted of pure tones with a 15ms ramp and 300ms duration. Visual stimuli, evoked by an LED array, did not include a ramp. (<bold>b</bold>) Example raster plots and PSTHs for a single neuron (the same neuron as shown in <xref ref-type="fig" rid="fig2">Figure 2c</xref>). Auditory responses (blue) began 9ms earlier than visual responses (green). PSTHs were smoothed using a Gaussian kernel (window = 50ms). (<bold>c</bold>) Population mean Z-scored responses for auditory (blue) and visual (green) stimuli. Auditory responses consistently reached 0.5 of the mean amplitude 15ms earlier than visual responses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Cue preference and multisensory integration of AC neurons in well-trained rats under anesthesia.</title><p>(<bold>a</bold>) Comparison of auditory and visual selectivity. Each point represents values for a single neuron. Open circles indicate neurons where neither auditory nor visual selectivity was significant (p&gt;0.05, permutation test). Triangles indicate significant selectivity in either auditory (blue) or visual (green) conditions, while diamonds represent significant selectivity in both auditory and visual conditions. Dashed lines represent zero cue selectivity. (<bold>b</bold>) Mean auditory (filled) and visual (unfilled) selectivity. Error bars represent SEMs. ***, p&lt;0.001. (<bold>c</bold>) MSI in A<sub>3k</sub>-V<sub>hz</sub> and A<sub>10k</sub>-V<sub>vt</sub> pairings. (<bold>d</bold>) Mean MSI in A<sub>3k</sub>-V<sub>hz</sub> (unfilled) and A<sub>10k</sub>-V<sub>vt</sub> (filled) pairings. Error bars represent SEMs. (<bold>e, f</bold>) Comparison of mean multisensory responses (red) with mean corresponding largest unisensory responses (dark blue) across populations for A<sub>3k</sub>-V<sub>hz</sub> (<bold>e</bold>) and A<sub>10k</sub>-V<sub>vt</sub> (<bold>f</bold>) pairings. The conventions used are consistent with those in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>An auditory cortical neuron exhibiting responsiveness to visual targets in the multisensory discrimination task.</title><p>Rasters and PSTHs illustrate the neuron’s responses to auditory (blue), visual (green), and audiovisual (red) target cues. The dashed lines indicate the cue onset (left) and the first spike latency for the visual target (V<sub>hz</sub>) response (right). For this neuron, the first spike latency to the V<sub>hz</sub> response is 44ms. The inset displays 100 recorded spike waveforms from this neuron (blue) with their average waveform overlaid in black.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig2-figsupp4-v1.tif"/></fig></fig-group><p>During task engagement, AC neurons displayed a clear preference for one target sound over the other. As exemplified in <xref ref-type="fig" rid="fig2">Figure 2c and d</xref>, many neurons exhibited a robust response to one target sound while showing a weak or negligible response to the other. We quantified this preference using receiver operating characteristic (ROC) analysis. Since most neurons exhibited their main cue-evoked response within the initial period of cue presentation (<xref ref-type="fig" rid="fig2">Figure 2e</xref>), our analysis focused on responses within the 0–150ms window after cue onset. We found a significant majority (61%, 307/506) of auditory-responsive neurons exhibited this selectivity during the task (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). Notably, most neurons favored the high-frequency tone (A<sub>10k</sub> preferred: 261; A<sub>3k</sub> preferred: 46, <xref ref-type="fig" rid="fig2">Figure 2f</xref>). This aligns with findings that neurons in the AC and medial prefrontal cortex selectively preferred the tone associated with the behavioral choice contralateral to the recorded cortices during sound discrimination tasks (<xref ref-type="bibr" rid="bib10">Chang et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Zheng et al., 2021</xref>), potentially reflecting the formation of sound-to-action associations (<xref ref-type="bibr" rid="bib54">Xiong et al., 2015</xref>). However, this preference represents a neural correlate, and further work is required to establish its causal link to behavioral choices. Such pronounced sound preference and bias were absent in the free-choice group (<xref ref-type="fig" rid="fig2">Figure 2e and f</xref>), suggesting it is directly linked to active discrimination. Anesthesia decreased auditory preference (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), further supporting its dependence on active engagement.</p><p>Regarding the visual modality, 41% (80/196) of visually-responsive neurons showed a significant visual preference (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). The visual responses observed within the 0–150ms window after cue onset were consistent and unlikely to result from visually evoked movement-related activity. This conclusion is supported by the early timing of the response (<xref ref-type="fig" rid="fig2">Figure 2e</xref>) and exemplified by a neuron with a low spontaneous firing rate and a robust, stimulus-evoked response (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). Similar to the auditory selectivity observed, a greater proportion of neurons favored the visual stimulus (V<sub>vt</sub>) associated with the contralateral choice, with a 3:1 ratio of V<sub>vt</sub>-preferred to V<sub>hz</sub>-preferred neurons. This convergence of auditory and visual selectivity likely results from multisensory perceptual learning. Notably, such patterns were absent in neurons recorded from a separate group of rats performing free-choice tasks (<xref ref-type="fig" rid="fig2">Figure 2e</xref>). Further supporting this conclusion is the difference in visual preference between audiovisual and exclusively visual neurons (<xref ref-type="fig" rid="fig2">Figure 2g</xref>). Among audiovisual neurons with a significant visual selectivity, the majority favored V<sub>vt</sub> (V<sub>vt</sub> preferred vs. V<sub>hz</sub> preferred, 48 vs 7; <xref ref-type="fig" rid="fig2">Figure 2g</xref>), aligning with their established auditory selectivity. In contrast, visual neurons did not exhibit this bias (12 preferred V<sub>vt</sub> vs. 13 preferred V<sub>hz</sub>; <xref ref-type="fig" rid="fig2">Figure 2g</xref>). We propose that the auditory input, which dominates within the auditory cortex, acts as a 'teaching signal' that shapes visual processing through the selective reinforcement of specific visual pathways during associative learning. This aligns with Hebbian plasticity, where stronger auditory responses boost the corresponding visual input, ultimately leading visual selectivity to mirror auditory preference.</p><p>Similar to auditory selectivity, the vast majority of neurons (79%, 270/340) showing significant multisensory selectivity exhibited a preference for the multisensory cue (A<sub>10k</sub>V<sub>vt</sub>) guiding the contralateral choice (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). To more clearly highlight the influence of visual input on auditory selectivity, we compared auditory, visual, and multisensory selectivity in 150 audiovisual neurons (<xref ref-type="fig" rid="fig2">Figure 2h</xref>). We found that pairing auditory cues with visual cues significantly improved the neurons' ability to distinguish between auditory stimuli alone (mean absolute auditory selectivity: 0.23±0.11; mean absolute multisensory selectivity: 0.25±0.12; p&lt;0.0001, Wilcoxon Signed Rank Test; <xref ref-type="fig" rid="fig2">Figure 2i</xref>).</p><p>Our multichannel recordings enabled us to decode sensory information from a pseudo-population of AC neurons on a single-trial basis. Using cross-validated support vector machine (SVM) classifiers, we examined how this pseudo-population discriminates stimulus identity within the same modality (e.g., A<sub>3k</sub> vs. A<sub>10k</sub> for auditory stimuli, V<sub>hz</sub> vs. V<sub>vt</sub> for visual stimuli, A<sub>3k</sub>V<sub>hz</sub> vs. A<sub>10k</sub>V<sub>vt</sub> for multisensory stimuli). While decoding accuracy was similar for auditory and multisensory conditions, the presence of visual cues accelerated the decoding process, with AC neurons reaching 90% accuracy approximately 18ms earlier in multisensory trials (<xref ref-type="fig" rid="fig2">Figure 2j</xref>), aligning with behavioral data. Interestingly, AC neurons could discriminate between two visual targets with around 80% accuracy (<xref ref-type="fig" rid="fig2">Figure 2j</xref>), demonstrating a meaningful incorporation of visual information into auditory cortical processing. However, AC neurons in the free-choice group lacked visual discrimination ability and showed lower accuracy for auditory cues (<xref ref-type="fig" rid="fig2">Figure 2k</xref>), suggesting that actively engaging multiple senses is crucial for the benefits of multisensory integration.</p></sec><sec id="s2-3"><title>Audiovisual integration of AC neurons during the multisensory discrimination task</title><p>To understand how AC neurons integrate auditory and visual inputs during task engagement, we compared the multisensory response of each neuron to its strongest corresponding unisensory response using ROC analysis to quantify the difference, termed ‘multisensory interactive index’ (MSI), where an index value greater than 0 indicates a stronger multisensory response. Over a third (34%, 192 of 559) of AC neurons displayed significant visual modulation of their auditory responses in one or both audiovisual pairings (p&lt;0.05, permutation test), including some neurons with no detectable visual response (104/356). <xref ref-type="fig" rid="fig3">Figure 3a</xref> exemplifies this, where the multisensory response exceeded the auditory response, a phenomenon termed ‘multisensory enhancement’ (<xref ref-type="bibr" rid="bib47">Stein and Meredith, 1993</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Auditory and visual integration in the multisensory discrimination task.</title><p>(<bold>a–c</bold>) Rasters and PSTHs showing responses of three typical neurons recorded in well-trained rats performing the multisensory discrimination task. (<bold>d</bold>) Population-averaged multisensory responses (red) compared to the corresponding stronger unisensory responses (dark blue) in A<sub>3k</sub>V<sub>hz</sub> (top) and A<sub>10k</sub>V<sub>vt</sub> (bottom) pairings for multisensory discrimination (left) and free-choice (right) groups. (<bold>e</bold>) Comparison of MSI between A<sub>3k</sub>V<sub>hz</sub> (x-axis) and A<sub>10k</sub>V<sub>vt</sub> (y-axis) conditions. Each point represents values for a single neuron. Open circles: MSI in either condition was not significant (p&gt;0.05, permutation test); triangles: significant selectivity in either A<sub>3k</sub>-V<sub>hz</sub> (blue) or A<sub>10k</sub>-V<sub>vt</sub> (green) condition; Diamonds: significant selectivity in both conditions. Dashed lines show zero MSI. Points labeled a-c correspond to the neurons in panels a–c. (<bold>f</bold>) Mean MSI for A<sub>10k</sub>-V<sub>vt</sub> and A<sub>3k</sub>-V<sub>hz</sub> pairings across audiovisual neurons in the multisensory discrimination group. (<bold>g</bold>) Comparison of MSI for the free-choice group. (<bold>h</bold>) Mean MSI across audiovisual neurons for the free-choice group. (<bold>i</bold>) SVM decoding accuracy in AC neurons between responses in multisensory vs. corresponding unisensory trials. Black line indicates shuffled control. (<bold>j</bold>) Positive relationship between the change in MSI (A<sub>10k</sub>V<sub>vt</sub> - A<sub>3k</sub>V<sub>hz</sub>) and the change in selectivity (multisensory selectivity - auditory selectivity). (<bold>k</bold>) Probability density functions of predicted mean multisensory responses (predicted AV) based on 0.83 times the sum of auditory (A) and visual (V) responses (same neuron as in <xref ref-type="fig" rid="fig2">Figure 2c</xref>). The observed multisensory response matches the predicted mean (Z-score=–0.17). (<bold>l</bold>) Frequency distributions of <italic>Z</italic>-scores. Open bars indicate no significant difference between actual and predicted multisensory responses. Red bars: Z-scores ≥ 1.96; blue bars: Z-scores ≤ −1.96.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Cue selectivity and multisensory integration in the late period (151–300ms) after cue onset.</title><p>(<bold>a–c</bold>) Auditory (<bold>a</bold>), visual (<bold>b</bold>), and multisensory (<bold>c</bold>) selectivity during the late period (151–300ms) after cue onset. Filled bars indicate neurons with a selectivity index significantly different from 0 (permutation test, p&lt;0.05, bootstrap n=5000). The dashed line represents zero. (<bold>d</bold>) Comparison of mean selectivity indices for auditory (blue), visual (green), and multisensory (red) conditions. Error bars represent standard deviations (SD). (<bold>e</bold>) MSI for A<sub>3k</sub>-V<sub>hz</sub> (x-axis) and A<sub>10k</sub>-V<sub>vt</sub> (y-axis) conditions. (<bold>f</bold>) Mean MSI for A<sub>3k</sub>-V<sub>hz</sub> (unfilled bars) and A<sub>10k</sub>-V<sub>vt</sub> (filled bars) conditions. Error bar represent SD of then mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>The observed versus the predicted multisensory response.</title><p>(<bold>a</bold>) Probability density functions of the predicted mean multisensory responses (predicted AV) based on summing the modality-specific visual (V) and auditory (A) responses (see Materials and methods for details). Black arrow, the mean of the predicted distribution. The same neuron is shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref> and only responses in correct contralateral choice trials are involved. The predicted mean, actual mean observed, and their difference expressed in SDs (<italic>Z</italic>-score) are shown. (<bold>b</bold>) Frequency distributions of <italic>Z</italic>-scores. Open bars represent Z-scores between –1.96 and 1.96, indicating that the actual observed multisensory response is not significantly different from the predicted multisensory response (additive integration). Red bars, <italic>Z</italic>-score ≥ 1.96 (superadditive integration); blue bars, <italic>Z</italic>-score ≤ −1.96 (subadditive integration). (<bold>c</bold>) Comparison between the mean predicted multisensory response and the actual observed multisensory response for all audiovisual neurons. Note that the predicted multisensory response is greater than the observed in most cases. Red squares and blue diamonds represent neurons with observed multisensory responses that are significantly greater (red squares) or less (blue diamonds) than the predicted mean multisensory responses.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig3-figsupp2-v1.tif"/></fig></fig-group><p>Interestingly, AC neurons adopted distinct integration strategies depending on the specific auditory-visual pairing presented. Neurons often displayed multisensory enhancement for one pairing but not another (<xref ref-type="fig" rid="fig3">Figure 3b</xref>), or even exhibited multisensory inhibition (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). This enhancement was mainly specific to the A<sub>10k</sub>-V<sub>vt</sub> pairing, as shown by population averages (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Within this pairing, significantly more neurons exhibited enhancement than inhibition (114 vs 36) (<xref ref-type="fig" rid="fig3">Figure 3e</xref>). In contrast, the A<sub>3k</sub>-V<sub>hz</sub> pairing showed a balanced distribution of enhancement and inhibition (35 vs 33; <xref ref-type="fig" rid="fig3">Figure 3e</xref>). This resulted in a significantly higher mean MSI for the A<sub>10k</sub>-V<sub>vt</sub> pairing compared to the A<sub>3k</sub>-V<sub>hz</sub> pairing (0.047±0.124 vs 0.003±0.096; paired t-test, p&lt;0.001). Among audiovisual neurons, this biasing is even more pronounced (enhanced vs. inhibited: 62 vs 2 in A<sub>10k</sub>-V<sub>vt</sub> pairing, 6 vs 13 in A<sub>3k</sub>-V<sub>hz</sub> pairing; mean MSI: 0.119±0.105 in A<sub>10k</sub>-V<sub>vt</sub> pairing vs. 0.020±0.083 A<sub>3k</sub>-V<sub>hz</sub> pairing, paired t-test, p&lt;0.00001; <xref ref-type="fig" rid="fig3">Figure 3f</xref>). Unlike the early period (0–150ms after cue onset), no significant differences in multisensory integration were observed during the late period (151–300ms after cue onset; <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>A similar pattern, albeit less strongly expressed, was observed under anesthesia (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), suggesting that multisensory perceptual learning may induce plastic changes in AC. In contrast, AC neurons in the free-choice group did not exhibit differential integration patterns (<xref ref-type="fig" rid="fig3">Figure 3g and h</xref>), indicating that multisensory discrimination and subsequent behavioral reporting are necessary for biased enhancement.</p><p>To understand how these distinct integration strategies influence multisensory discrimination, we compared the difference in MSI between A<sub>10k</sub>-V<sub>vt</sub> and A<sub>3k</sub>-V<sub>hz</sub> pairings, and the change in neuronal multisensory versus auditory selectivity (<xref ref-type="fig" rid="fig3">Figure 3j</xref>). We found a significant correlation between these measures (<italic>R</italic>=0.65, p&lt;0.001, Pearson correlation test). The greater the difference in MSI, the more pronounced the increase in cue discrimination during multisensory trials compared to auditory trials. This highlights how distinct integration models enhance cue selectivity in multisensory conditions. Additionally, a subset of neurons exhibited multisensory enhancement for each pairing (<xref ref-type="fig" rid="fig3">Figure 3e</xref>), potentially aiding in differentiating between multisensory and unisensory responses. SVM decoding analysis confirmed that population neurons could effectively discriminate between multisensory and unisensory stimuli (<xref ref-type="fig" rid="fig3">Figure 3i</xref>).</p><p>We further explored the integrative mechanisms—whether subadditive, additive, or superadditive—used by AC neurons to combine auditory and visual inputs. Using the bootstrap method, we generated a distribution of predicted multisensory responses by summing mean visual and auditory responses from randomly sampled trials where each stimulus was presented alone. Robust auditory and visual responses were primarily observed in correct contralateral choice trials, so our calculations focused on the A<sub>10k</sub>-V<sub>vt</sub> pairing. We found that, for most neurons, the observed multisensory response in contralateral choice trials was below the anticipated sum of the corresponding visual and auditory responses (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>). Specifically, as exemplified in <xref ref-type="fig" rid="fig3">Figure 3k</xref>, the observed multisensory response approximated 83% of the sum of the auditory and visual responses in most cases, as quantified in <xref ref-type="fig" rid="fig3">Figure 3l</xref>.</p></sec><sec id="s2-4"><title>Impact of incorrect choices on audiovisual integration</title><p>To investigate how incorrect choices affected audiovisual integration, we compared multisensory integration for correct and incorrect choices within each auditory-visual pairing, focusing on neurons with a minimum of nine trials per choice per cue. Our findings demonstrated a significant reduction in the magnitude of multisensory enhancement during incorrect choice trials in the A<sub>10k</sub>-V<sub>vt</sub> pairing. <xref ref-type="fig" rid="fig4">Figure 4a</xref> illustrates a representative case. The mean MSI for incorrect choices was significantly lower than that for correct choices (correct vs. incorrect: 0.059±0.137 vs 0.006±0.207; p=0.005, paired t-test, <xref ref-type="fig" rid="fig4">Figure 4b and c</xref>). In contrast, the A<sub>3k</sub>-V<sub>hz</sub> pairing showed no difference in MSI between correct and incorrect trials (correct vs. incorrect: 0.011±0.081 vs 0.003±0.199; p=0.542, paired t-test, <xref ref-type="fig" rid="fig4">Figure 4d and e</xref>). Interestingly, correct choices here correspond to ipsilateral behavioral selection, while incorrect choices correspond to contralateral behavioral selection. This indicates that contralateral behavioral choice alone does not guarantee stronger multisensory enhancement. Overall, these findings suggest that the multisensory perception reflected by behavioral choices (correct vs. incorrect) might be shaped by the underlying integration strength. Furthermore, our analysis revealed that incorrect choices were associated with a decline in cue selectivity, as shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Impact of choice selection on audiovisual Integration.</title><p>(<bold>a</bold>) PSTHs show mean responses of an exemplar neuron for different cue and choice trials. (<bold>b</bold>) Mean MSI across neurons for correct (orange) and incorrect (blue) choices in the A<sub>10k</sub>-V<sub>vt</sub> pairing. (<bold>c</bold>) Comparison of MSI between correct and incorrect choices for the A<sub>10k</sub>-V<sub>vt</sub> pairing. Error bars, SEM. (<bold>d, e</bold>) Similar comparisons of MSI for correct and incorrect choices in the A<sub>3k</sub>-V<sub>hz</sub> pairing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Comparison of cue selectivity between correct and incorrect choice conditions.</title><p>(<bold>a</bold>) Comparison of auditory selectivity between incorrect and correct choice trials for neurons with at least nine trials in the incorrect condition. The solid line indicates equality. (<bold>b</bold>) Mean auditory selectivity across neurons between incorrect and correct choice trials. Error bars represent SEM. Statistical significance: ***, p=0.0004. (<bold>c</bold>) Scatterplot comparing multisensory selectivity between correct and incorrect choice trials, limited to neurons with a minimum of nine trials in the incorrect condition. (<bold>d</bold>) Mean multisensory selectivity across neurons between incorrect and correct choice trials. Error bars represent SEM. Statistical significance: *, p=0.02.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig4-figsupp1-v1.tif"/></fig></fig-group></sec><sec id="s2-5"><title>Impact of informational match on audiovisual integration</title><p>A pivotal factor influencing multisensory integration is the precise alignment of informational content conveyed by distinct sensory cues. To explore the impact of the association status between auditory and visual cues on multisensory integration, we introduced two new multisensory cues (A<sub>10k</sub>V<sub>hz</sub> and A<sub>3k</sub>V<sub>vt</sub>) into the target cue pool during task engagement. These cues were termed 'unmatched multisensory cues' as their auditory and visual components indicated different behavioral choices. Rats received water rewards with a 50% chance in either port when an unmatched multisensory cue was triggered. Behavioral analysis revealed that Rats displayed notable confusion in response to unmatched multisensory cues, as evidenced by their inconsistent choice patterns (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>We recorded from 280 AC neurons in well-trained rats performing a multisensory discrimination task with matched and unmatched audiovisual cues. We analyzed the integrative and discriminative characteristics of these neurons for matched and unmatched auditory-visual pairings. The results revealed distinct integrative patterns in AC neurons when an auditory target cue was paired with the matched visual cue as opposed to the unmatched visual cue. Neurons typically showed a robust response to A<sub>10k</sub>, which was enhanced by the matched visual cue but was either unaffected (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) or inhibited (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) by the unmatched visual cue. In some neurons, both matched and unmatched visual cues enhanced the auditory response but matched cues provided a greater enhancement (<xref ref-type="fig" rid="fig5">Figure 5c</xref>). We compared the MSI for different auditory-visual pairings (<xref ref-type="fig" rid="fig5">Figure 5d</xref>). Unlike V<sub>vt</sub>, the unmatched visual cue, V<sub>hz</sub>, generally failed to significantly enhance the response to the preferred sound (A<sub>10k</sub>) in most cases (mean MSI: 0.052±0.097 for A<sub>10k</sub>-V<sub>vt</sub> pairing vs. 0.016±0.123 for A<sub>10k</sub>-V<sub>hz</sub> pairing, p&lt;0.0001, paired t-test). This suggests that consistent information across modalities strengthens multisensory integration. In contrast, neither associative nor non-associative visual cue could boost neurons’ response to the nonpreferred sound (A<sub>3k</sub>) overall (mean MSI: 0.003±0.008 for A<sub>3k</sub>-V<sub>hz</sub> pairing vs. 0.003±0.006 for A<sub>3k</sub>-V<sub>vt</sub> pairing, p=0.19, paired t-test).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Information-dependent integration and discrimination of audiovisual cues.</title><p>(<bold>a–c</bold>) Responses of three example neurons to six target stimuli and two unmatched multisensory cues (A<sub>3k</sub>V<sub>vt</sub> and A<sub>10k</sub>V<sub>hz</sub>) presented during task engagement. (<bold>d</bold>) Lower panel: MSI for different auditory-visual pairings. Neurons are sorted by their MSI for the A<sub>10k</sub>V<sub>vt</sub> condition. Upper panel: Mean MSI across the population for each pairing. Error bar, SEM. ***, p&lt;0.001. (<bold>e</bold>) Population decoding accuracy for all stimulus pairings (8×8) within a 150ms window after stimulus onset. Green, purple, and red squares denote the decoding accuracy for discriminating two target multisensory cues (green), discriminating two unmatched auditory-visual pairings (red) and matched versus unmatched audiovisual pairings (purple), respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Behavioral choice in incongruent and congruent audiovisual trials.</title><p>(<bold>a</bold>) Proportions of choices (filled bars for left choice, unfilled bars for right choice) for incongruent A<sub>3k</sub>V<sub>vt</sub> (red) and A<sub>10k</sub>V<sub>hz</sub> (blue) trials. Circles connected by a dashed line represent data from one individual rat. (<bold>b</bold>) Behavioral performance in congruent A<sub>3k</sub>V<sub>hz</sub> (white) and A<sub>10k</sub>V<sub>vt</sub> (gray) conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig5-figsupp1-v1.tif"/></fig></fig-group><p>Although distinct AC neurons exhibited varying integration profiles for different auditory-visual pairings, we explored whether, as a population, they could distinguish these pairings. We trained a linear classifier to identify each pair of stimuli and employed the classifier to decode stimulus information from the population activity of grouped neurons. The resulting decoding accuracy matrix for discriminating pairs of stimuli was visualized (<xref ref-type="fig" rid="fig5">Figure 5e</xref>). We found that the population of neurons could effectively discriminate two target multisensory cues. While matched and unmatched visual cues failed to differentially modulate the response to the nonpreferred sound (A<sub>3k</sub>) at the single-neuron level, grouped neurons still could discriminate them with a decoding accuracy of 0.79 (<xref ref-type="fig" rid="fig5">Figure 5e</xref>), close to the accuracy of 0.81 for discriminating between A<sub>10k</sub>-V<sub>vt</sub> and A<sub>10k</sub>-V<sub>hz</sub> pairings. This suggests that associative learning experiences enabled AC neurons to develop multisensory discrimination abilities. However, the accuracy for discriminating matched vs. unmatched cues was lower compared to other pairings (<xref ref-type="fig" rid="fig5">Figure 5e</xref>).</p></sec><sec id="s2-6"><title>Cue preference and multisensory integration in left AC</title><p>Our data showed that most neurons in the right AC preferred the cue directing the contralateral choice, regardless of whether it was auditory or visual. However, this could simply be because these neurons were naturally more responsive to those specific cues, not necessarily because they learned an association between the cues and the choice. To address this, we trained another four rats to perform the same discrimination task but recorded neuronal activity in left AC (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). We analyzed 193 neurons, of which about a third (31%, 60/193) responded to both auditory and visual cues (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). Similar to the right AC, the average response across neurons in the left AC preferred cues guiding the contralateral choice (<xref ref-type="fig" rid="fig6">Figure 6c</xref>). However, in this case, the preferred cues are A<sub>3k</sub>, V<sub>hz</sub> and the audiovisual pairing A<sub>3k</sub>V<sub>hz</sub>. As shown in <xref ref-type="fig" rid="fig6">Figure 6d</xref>, more auditory-responsive neurons favored the sound denoting the contralateral choice. The same was true for visual selectivity in visually responsive neurons (<xref ref-type="fig" rid="fig6">Figure 6e</xref>). This strongly suggests that the preference was not simply due to a general bias toward a specific cue, but rather reflected the specific associations the animals learned during the training.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Multisensory integration and cue preferences of neurons recorded in left AC.</title><p>(<bold>a</bold>) The behavioral task and the recording site within the brain (the left AC). (<bold>b</bold>) The proportion of neurons responsive to auditory-only (A), visual-only (V), both auditory and visual (A, V), and audiovisual-only (VA) stimuli based on their spiking activity. (<bold>c</bold>) The mean PSTHs of normalized responses across neurons for different cue conditions. (<bold>d</bold>) A histogram of auditory selectivity index. Filled bars represent neurons with a significant selectivity index (permutation test, p&lt;0.05, bootstrap n=5000). (<bold>e</bold>) Similar to panel (<bold>d</bold>), this panel shows a histogram of the visual selectivity index. (<bold>f</bold>) Comparison of MSI between A<sub>3k</sub>-V<sub>hz</sub> and A<sub>10k</sub>-V<sub>vt</sub> pairings. (<bold>g</bold>) A boxplot shows the comparison of MSI for audiovisual neurons. ***, p&lt;0.001. The conventions used are consistent with those in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig6-v1.tif"/></fig><p>Consistent with the cue biasing, differential multisensory integration was observed, with multisensory enhancement biased toward the A<sub>3k</sub>-V<sub>hz</sub> pairing (<xref ref-type="fig" rid="fig6">Figure 6f</xref>). This mirrors the finding in the right AC, where multisensory enhancement was biased toward the auditory-visual pairing guiding the contralateral choice. In audiovisual neurons, mean MSI in the A<sub>3k</sub>-V<sub>hz</sub> pairing is substantially higher than in the A<sub>10k</sub>-V<sub>vt</sub> pairing (0.135±0.126 to -0.039±0.138; p&lt;0.0001, paired t-test; <xref ref-type="fig" rid="fig6">Figure 6g</xref>). These findings suggest that AC neurons exhibit cue preference and biased multisensory enhancement based on the learned association between cues and behavioral choice. This mechanism could enable the brain to integrate cues of different modalities into a common behavioral response.</p></sec><sec id="s2-7"><title>Unisensory training does not replicate multisensory training effects</title><p>Our data suggest that most AC audiovisual neurons exhibited a visual preference consistent with their auditory preference following consistent multisensory discrimination training. Additionally, these neurons developed selective multisensory enhancement for a specific audiovisual stimulus pairing. To investigate whether these properties stemmed solely from long-term multisensory discrimination training, we trained a new group of animals (n=3) first on auditory and then on visual discriminations (<xref ref-type="fig" rid="fig7">Figure 7a</xref>). These animals did not receive task-related multisensory associations during the training period. We then examined the response properties of neurons recorded in the right AC when well-trained animals performed auditory, visual and audiovisual discrimination tasks.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Multisensory integration and cue preferences in right AC neurons after unisensory training.</title><p>(<bold>a</bold>) This panel depicts the training stages for the rats: auditory discrimination training followed by visual discrimination training. (<bold>b</bold>) The proportion of neurons of different types. (<bold>c</bold>) Mean normalized responses across neurons for different cue conditions. (<bold>d, e</bold>) Histograms of visual (<bold>d</bold>) and auditory (<bold>e</bold>) selectivity. (<bold>f</bold>) Comparison of neuronal MSI between A<sub>3k</sub>-V<sub>hz</sub> and A<sub>10k</sub>-V<sub>vt</sub> pairings. (<bold>g</bold>) MSI comparison in audiovisual neurons. N.S., no significant difference. The conventions used are consistent with those in <xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-fig7-v1.tif"/></fig><p>Among recorded AC neurons, 28% (49/174) responded to visual target cues (<xref ref-type="fig" rid="fig7">Figure 7b</xref>). Unlike the multisensory training group, most visually responsive neurons in the unisensory training group lacked a visual preference, regardless of whether they were visually-only responsive or audiovisual (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). Interestingly, similar to the multisensory training group, nearly half of the recorded neurons (47%, 75/159) demonstrated clear auditory discrimination, with most (80%, 60 out of 75) favoring the sound guiding the contralateral choice (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). Furthermore, the unisensory training group did not exhibit population-level multisensory enhancement (n=174, <xref ref-type="fig" rid="fig7">Figure 7f</xref>). The mean MSI for the A<sub>3k</sub>-V<sub>Hz</sub> and A<sub>10k</sub>-V<sub>Hz</sub> pairs showed no significant difference (p=0.327, paired t-test), with values of –0.02±0.12 and –0.01±0.13, respectively (<xref ref-type="fig" rid="fig7">Figure 7g</xref>). Even among audiovisual neurons (n=37), multisensory integration did not differ significantly between the two pairings (A<sub>3k</sub>-V<sub>Hz</sub> vs A<sub>10k</sub>-V<sub>Hz</sub>: 0.002±0.126 vs 0.017±0.147; p=0.63; <xref ref-type="fig" rid="fig7">Figure 7g</xref>). These findings suggest that the development of multisensory enhancement for specific audiovisual cues and the alignment of auditory and visual preferences likely depend on the association of auditory and visual stimuli with the corresponding behavioral choice during multisensory training. Unisensory training alone cannot replicate these effects.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we investigated how AC neurons integrate auditory and visual inputs to discriminate multisensory objects and whether this integration reflects the congruence between auditory and visual cues. Our findings reveal that most AC neurons exhibited distinct integrative patterns for different auditory-visual pairings, with multisensory enhancement observed primarily for favored pairings. This indicates that AC neurons show significant selectivity in their integrative processing. Furthermore, AC neurons effectively discriminated between matched and mismatched auditory-visual pairings, highlighting the crucial role of the AC in multisensory object recognition and discrimination. Interestingly, a subset of auditory neurons not only developed visual responses but also exhibited congruence between auditory and visual selectivity. These findings suggest that multisensory perceptual training establishes a memory trace of the trained audiovisual experiences within the AC and enhances the preferential linking of auditory and visual inputs. Sensory cortices, like AC, may act as a vital bridge for communicating sensory information across different modalities.</p><p>Numerous studies have explored the cross-modal interaction of sensory cortices (<xref ref-type="bibr" rid="bib38">Perrodin et al., 2015</xref>; <xref ref-type="bibr" rid="bib23">Kayser et al., 2008</xref>; <xref ref-type="bibr" rid="bib5">Bizley et al., 2007</xref>; <xref ref-type="bibr" rid="bib31">Meijer et al., 2017</xref>; <xref ref-type="bibr" rid="bib13">Deneux et al., 2019</xref>; <xref ref-type="bibr" rid="bib18">Ghazanfar and Schroeder, 2006</xref>). Recent research has highlighted the critical role of cross-modal modulation in shaping the stimulus characteristics encoded by sensory cortical neurons. For instance, sound has been shown to refine orientation tuning in layer 2/3 neurons of the primary visual cortex (<xref ref-type="bibr" rid="bib21">Ibrahim et al., 2016</xref>), and a congruent visual stimulus can enhance sound representation in the AC (<xref ref-type="bibr" rid="bib2">Atilgan et al., 2018</xref>). Meijer et al. reported that congruent audiovisual stimuli evoke balanced enhancement and suppression in V1, while incongruent stimuli predominantly lead to suppression (<xref ref-type="bibr" rid="bib31">Meijer et al., 2017</xref>), mirroring our findings in AC, where multisensory integration was dependent on stimulus feature. Despite these findings, the functional role and patterns of cross-modal interaction during perceptual discrimination remain unclear. In this study, we made a noteworthy discovery that AC neurons employed a nonuniform mechanism to integrate visual and auditory stimuli while well-trained rats performed a multisensory discrimination task. This differentially integrative pattern improved multisensory discrimination. Notably, this integrative pattern did not manifest during a free-choice task. These findings indicate that depending on the task, different sensory elements need to be combined to guide adaptive behavior. We propose that task-related differentially integrative patterns may not be exclusive to sensory cortices but could represent a general model in the brain.</p><p>Consistent with prior research (<xref ref-type="bibr" rid="bib10">Chang et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Zheng et al., 2021</xref>), most AC neurons exhibited a selective preference for cues associated with contralateral choices, regardless of the sensory modality. This suggests that AC neurons may contribute to linking sensory inputs with decision-making, although their causal role remains to be examined. Associative learning may drive the formation of new connections between sensory and motor areas of the brain, such as cortico-cortical pathways (<xref ref-type="bibr" rid="bib30">Makino et al., 2016</xref>). Notably, this cue-preference biasing was absent in the free-choice group. A similar bias was also reported in a previous study, where auditory discrimination learning selectively potentiated corticostriatal synapses from neurons representing either high or low frequencies associated with contralateral choices (<xref ref-type="bibr" rid="bib54">Xiong et al., 2015</xref>). These findings highlight the significant role of associative learning in cue encoding, emphasizing the integration of stimulus discrimination and behavioral choice.</p><p>Prior work has investigated how neurons in sensory cortices discriminate unisensory cues in perceptual tasks (<xref ref-type="bibr" rid="bib41">Romo and Rossi-Pool, 2020</xref>; <xref ref-type="bibr" rid="bib22">Juavinett et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Schoups et al., 2001</xref>). It is well-established that when animals learn the association of sensory features with corresponding behavioral choices, cue representations in early sensory cortices undergo significant changes (<xref ref-type="bibr" rid="bib36">Otazu et al., 2009</xref>; <xref ref-type="bibr" rid="bib57">Yan et al., 2014</xref>; <xref ref-type="bibr" rid="bib53">Xin et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Makino and Komiyama, 2015</xref>). For instance, visual learning shapes cue-evoked neural responses and increases visual selectivity through changes in the interactions and correlations between visual cortical neurons (<xref ref-type="bibr" rid="bib9">Chadwick et al., 2023</xref>; <xref ref-type="bibr" rid="bib24">Khan et al., 2018</xref>). Consistent with these previous studies, we found that auditory discrimination in the AC of well-trained rats substantially improved during the task.</p><p>In this study, we extended our investigation to include multisensory discrimination. We discovered that when rats performed the multisensory discrimination task, AC neurons exhibited robust multisensory selectivity, responding strongly to one auditory-visual pairing and showing weak or negligible responses to the other, similar to their behavior in auditory discrimination. Audiovisual neurons demonstrated higher selectivity in multisensory trials compared to auditory trials, driven by the differential integration pattern. Additionally, more AC neurons were involved in auditory-visual discrimination compared to auditory discrimination alone, suggesting that the recruitment of additional neurons may partially explain the higher behavioral performance observed in multisensory trials. A previous study indicates that cross-modal recruitment of more cortical neurons also enhances perceptual discrimination (<xref ref-type="bibr" rid="bib28">Lomber et al., 2010</xref>).</p><p>Our study explored how multisensory discrimination training influences visual processing in the AC. We observed well-trained rats exhibited a higher number of AC neurons responding to visual cues compared to untrained rats. This finding aligns with growing evidence that multisensory perceptual learning can effectively drive plastic change in both sensory and association cortices (<xref ref-type="bibr" rid="bib44">Shams and Seitz, 2008</xref>; <xref ref-type="bibr" rid="bib39">Proulx et al., 2014</xref>). Our results extend prior findings (<xref ref-type="bibr" rid="bib5">Bizley et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Morrill and Hasenstaub, 2018</xref>), showing that visual input not only reaches the AC but can also drive discriminative responses, particularly during task engagement. This task-specific plasticity enhances cross-modal integration, as demonstrated in other sensory systems. For example, calcium imaging studies in mice showed that a subset of multimodal neurons in visual cortex develops enhanced auditory responses to the paired auditory stimulus following coincident auditory–visual experience (<xref ref-type="bibr" rid="bib25">Knöpfel et al., 2019</xref>). A study conducted on the gustatory cortex of alert rats has shown that cross-modal associative learning was linked to a dramatic increase in the prevalence of neurons responding to nongustatory stimuli (<xref ref-type="bibr" rid="bib50">Vincis and Fontanini, 2016</xref>). Moreover, in the primary visual cortex, experience-dependent interactions can arise from learned sequential associations between auditory and visual stimuli, mediated by corticocortical connections rather than simultaneous audiovisual presentations (<xref ref-type="bibr" rid="bib16">Garner and Keller, 2022</xref>).</p><p>Among neurons responding to both auditory and visual stimuli, a congruent visual and auditory preference emerged during multisensory discrimination training, as opposed to unisensory discrimination training. These neurons primarily favored visual cues that matched their preferred sound. Interestingly, this preference was not observed in neurons solely responsive to visual targets. The strength of a visual response seems to be contingent upon the paired auditory input received by the same neuron. This aligns with known mechanisms in other brain regions, where learning strengthens or weakens connections based on experience (<xref ref-type="bibr" rid="bib25">Knöpfel et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Komiyama et al., 2010</xref>). This synchronized auditory-visual selectivity may be a way for AC to bind corresponding auditory and visual features, potentially forming memory traces for learned multisensory objects. These results indicate that multisensory training could drive the formation of specialized neural circuits within the auditory cortex, facilitating integrated processing of related auditory and visual information. However, further causal studies are required to confirm this hypothesis and to determine whether the auditory cortex is the primary site of these circuit modifications.</p><p>There is ongoing debate about whether cross-sensory responses in sensory cortices predominantly reflect sensory inputs or are influenced by behavioral factors, such as cue-induced body movements. A recent study shows that sound-clip evoked activity in visual cortex have a behavioral rather than sensory origin and is related to stereotyped movements (<xref ref-type="bibr" rid="bib4">Bimbard et al., 2023</xref>). Several studies have demonstrated sensory neurons can encode signals associated with whisking (<xref ref-type="bibr" rid="bib49">Stringer et al., 2019</xref>), running (<xref ref-type="bibr" rid="bib35">Niell and Stryker, 2010</xref>), pupil dilation (<xref ref-type="bibr" rid="bib51">Vinck et al., 2015</xref>) and other movements (<xref ref-type="bibr" rid="bib34">Musall et al., 2019</xref>). In our study, the responses to visual stimuli in the auditory cortex occurred primarily within a 100ms window following cue onset, suggesting that visual information reaches the AC through rapid pathways. Potential candidates include direct or fast cross-modal inputs, such as pulvinar-mediated pathways (<xref ref-type="bibr" rid="bib11">Chou et al., 2020</xref>) or corticocortical connections (<xref ref-type="bibr" rid="bib2">Atilgan et al., 2018</xref>; <xref ref-type="bibr" rid="bib42">Schmehl and Groh, 2021</xref>), rather than slower associative mechanisms. This early timing indicates that the observed responses were less likely modulated by visually evoked body or orofacial movements, which typically occur with a delay relative to sensory cue onset (<xref ref-type="bibr" rid="bib37">Oude Lohuis et al., 2024</xref>).</p><p>A recent study by <xref ref-type="bibr" rid="bib12">Clayton et al., 2024</xref> demonstrated that sensory stimuli can evoke rapid motor responses, such as facial twitches, within 50ms, mediated by subcortical pathways and modulated by descending corticofugal input (<xref ref-type="bibr" rid="bib12">Clayton et al., 2024</xref>). These motor responses provide a sensitive behavioral index of auditory processing. Although Clayton et al. did not observe visually evoked facial movements, it is plausible that visually driven motor activity occurs more frequently in freely moving animals compared to head-fixed conditions. In goal-directed tasks, such rapid motor responses might contribute to the contralaterally tuned responses observed in our study, potentially reflecting preparatory motor behaviors associated with learned responses. Consequently, some of the audiovisual integration observed in the auditory cortex may represent a combination of multisensory processing and preparatory motor activity. Comprehensive investigation of these motor influences would require high-speed tracking of orofacial and body movements. Therefore, our findings should be interpreted with this consideration in mind. Future studies should aim to systematically monitor and control eye, orofacial, and body movements to disentangle sensory-driven responses from motor-related contributions, enhancing our understanding of motor planning’s role in multisensory integration.</p><p>Additionally, our study sheds light on the role of semantic-like information in multisensory integration. During training, we created an association between specific auditory and visual stimuli, as both signaled the same behavioral choice. This setup mimics real-world scenarios where visual and auditory cues possess semantic coherence, such as an image of a cat paired with the sound of a ‘meow’. Previous research has shown that semantically congruent multisensory stimuli enhance behavioral performance, while semantically incongruent stimuli either show no enhancement or result in decreased performance (<xref ref-type="bibr" rid="bib27">Laurienti et al., 2004</xref>). Intriguingly, our findings revealed a more nuanced role for semantic information. While AC neurons displayed multisensory enhancement for the preferred congruent audiovisual pairing, this wasn't for enhanced multisensory integration. However, the strength of multisensory enhancement itself served as a key indicator in differentiating between matched and mismatched cues. These findings provide compelling evidence that the nature of the semantic-like information plays a vital role in modifying multisensory integration at the neuronal level.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Animals</title><p>The animal procedures conducted in this study (Protocol number: m+R20211004) were ethically approved by the Local Ethical Review Committee of East China Normal University and followed the guidelines outlined in the Guide for the Care and Use of Laboratory Animals of East China Normal University. Twenty-five adult male Long-Evans rats (age: 10–12 weeks), weighing approximately 250 g, were obtained from the Shanghai Laboratory Animal Center (Shanghai, China) and used as subjects for the experiments. Of these rats, 14 were assigned to the multisensory discrimination task experiments, 3 to the unisensory discrimination task, and 8 to the control free-choice task experiments. The rats were group-housed with no more than four rats per cage and maintained on a regular light-dark cycle. They underwent water deprivation for 2 days before the start of behavioral training, with water provided exclusively inside the training box on training days. Training sessions were conducted 6 days per week, each lasting between 50 and 80 min, and were held at approximately the same time each day to minimize potential circadian variations in performance. The body weight of the rats was carefully monitored throughout the study, and supplementary water was provided to those unable to maintain a stable body weight from task-related water rewards.</p></sec><sec id="s4-2"><title>Behavioral apparatus</title><p>All experiments were conducted in a custom-designed operant chamber, measuring 50×30 × 40 cm (length ×width × height), with an open-top design. The chamber was placed in a sound-insulated double-walled room, and the inside walls and ceiling were covered with 3 inches of sound-absorbing foam to minimize external noise interference. One sidewall of the operant chamber was equipped with three snout ports, each monitored by a photoelectric switch (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>).</p><p>Automated training procedures were controlled using a real-time control program developed in MATLAB (Mathworks, Natick, MA, USA). The auditory signals generated by the program were sent to an analog-digital multifunction card (NI USB 6363, National Instruments, Austin, TX, USA), amplified by a power amplifier (ST-601, SAST, Zhejiang, China), and delivered through a speaker (FS Audio, Zhejiang, China). The auditory stimuli consisted of 300ms-long (15ms ramp-decay) pure tones with a frequency of 3 kHz or 10 kHz. The sound intensity was set at 60 dB sound pressure level (SPL) against an ambient background of 35–40 dB SPL. SPL measurements were taken at the position of the central port, which served as the starting position for the rats.</p><p>The visual cue was generated using two custom-made devices located on each side in front of the central port. Each device consisted of two arrays of closely aligned light-emitting diodes arranged in a cross pattern. The light emitted by the vertical or horizontal LED array passed through a ground glass, resulting in the formation of the corresponding vertical or horizontal light bar, each measuring 6×0.8 cm (<xref ref-type="fig" rid="fig1">Figure 1</xref>). As stimuli, the light bars were illuminated for 300ms at an intensity of 2~3 cd/m<sup>2</sup>. The audiovisual cue (multisensory cue) was the simultaneous presentation of both auditory and visual stimuli.</p></sec><sec id="s4-3"><title>Multisensory discrimination task</title><p>The rats were trained to perform a cue-guided two-alternative forced-choice task, modified from previously published protocols (<xref ref-type="bibr" rid="bib10">Chang et al., 2022</xref>; <xref ref-type="bibr" rid="bib58">Zheng et al., 2021</xref>). Each trial began with the rat placing its nose into the center port. Following a short variable delay period (500–700ms) after nose entry, a randomly selected stimulus signal was presented. Upon cue presentation, the rats were allowed to initiate their behavioral choice by moving to either the left or right port (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). The training consisted of two stages. In the first stage, which typically lasted 3–5 weeks, the rats were trained to discriminate between two audiovisual cues. In the second stage, an additional four unisensory cues were introduced, training the rats to discriminate a total of six cues (two auditory, two visual, and two audiovisual). This stage also lasted approximately 3–5 weeks.</p><p>During the task, the rats were rewarded with a drop (15–20 μl) of water when they moved to the left reward port following the presentation of a 10 kHz pure tone sound, a vertical light bar, or their combination. For trials involving a 3 kHz tone sound, a horizontal light bar, or their combination, the rats were rewarded when they moved to the right reward port. Incorrect choices or failures to choose within 3 s after cue onset resulted in a timeout punishment of 5–6 s. Typically, the rats completed between 300 and 500 trials per day. They were trained to achieve a competency level of more than 80% correct overall and &gt;70% correct in each cue condition in three consecutive sessions before the surgical implantation of recording electrodes.</p><p>The correct rate was calculated as follows:</p><p>Correct rate (%)=100*(the number of correct trials) / (total number of trials).</p><p>The reaction time was defined as the temporal gap between the cue onset and the time when the rat withdrew its nose from the infrared beam in the cue port.</p></sec><sec id="s4-4"><title>Unisensory discrimination task</title><p>The rats first learned to discriminate between two auditory cues. Once their performance in the auditory discrimination task exceeded 75% correct, the rats then learned to discriminate between two visual cues. The auditory and visual cues used were the same as those in the multisensory discrimination task.</p></sec><sec id="s4-5"><title>No cue discrimination free-choice task</title><p>In this task, the rats were not required to discriminate the cues presented. They received a water reward at either port following the onset of the cue, and their port choice was spontaneous. The six cues used were the same as those in the multisensory discrimination task. One week of training was sufficient for the rats to learn this no cue discrimination free-choice task.</p></sec><sec id="s4-6"><title>Assembly of tetrodes</title><p>The tetrodes were constructed using Formvar-Insulated Nichrome Wire (bare diameter: 17.78 μm, A-M systems, WA, USA) twisted in groups of four. Two 20-cm-long wires were folded in half over a horizontal bar to facilitate twisting. The ends of the wires were clamped together and manually twisted in a clockwise direction. The insulation coating of the twisted wires was then fused using a heat gun at the desired twist level. Subsequently, the twisted wire was cut in the middle to produce two tetrodes. To enhance the longitudinal stability of each tetrode, it was inserted into polymide tubing (inner diameter: 0.045 inches; wall: 0.005 inches; A-M systems, WA, USA) and secured in place using cyanoacrylate glue. An array of 2×4 tetrodes was assembled, with an inter-tetrode gap of 0.4–0.5 mm. After assembly, the insulation coating at the tip of each wire was gently removed, and the exposed wire was soldered to a connector pin. For the reference electrode, a Ni-Chrome wire with a diameter of 50.8 μm (A-M systems, WA, USA) was used, with its tip exposed. A piece of copper wire with a diameter of 0.1 mm served as the ground electrode. Each of these electrodes was also soldered to a corresponding pin on the connector. The tetrodes, reference electrode, and ground electrode, along with their respective pins, were carefully arranged and secured using silicone gel. Immediately before implantation, the tetrodes were trimmed to an appropriate length.</p></sec><sec id="s4-7"><title>Electrode implantation</title><p>Prior to surgery, the animal received subcutaneous injections of atropine sulfate (0.01 mg/kg) to reduce bronchial secretions. The animal was then anesthetized with an intraperitoneal (i.p.) injection of sodium pentobarbital (40–50 mg/kg) and securely positioned on a stereotaxic apparatus (RWD, Shenzhen, China). An incision was made in the scalp, and the temporal muscle was carefully recessed. Subsequently, a craniotomy and durotomy were performed to expose the target brain region. Using a micromanipulator (RWD, Shenzhen, China), the tetrode array was precisely positioned at stereotaxic coordinates 3.5–5.5 mm posterior to bregma and 6.4 mm lateral to the midline, and advanced to a depth of approximately 2–2.8 mm from the brain surface, corresponding to the primary auditory cortex. The craniotomy was then sealed with tissue gel (3 M, Maplewood, MN, USA). The tetrode array was secured to the skull using stainless steel screws and dental acrylic. Following the surgery, animals received a 4-day prophylactic course of antibiotics (Baytril, 5 mg/kg, body weight, Bayer, Whippany, NJ, USA). They were allowed a recovery period of at least 7 days (typically 9–12 days) with free access to food and water.</p></sec><sec id="s4-8"><title>Neural recordings and analysis</title><p>After rats had sufficiently recovered from the surgery, they resumed performing the same behavioral task they were trained to accomplish before surgery. Recording sessions were initiated once the animals' behavioral performance had returned to the level achieved before the surgery (typically within 2–3 days). Wideband neural signals in the range of 300–6000 Hz were recorded using the AlphaOmega system (AlphaOmega Instruments, Nazareth Illit, Israel). The amplified signals (×20) were digitized at a sampling rate of 25 kHz. These neural signals, along with trace signals representing the stimuli and session performance information, were transmitted to a PC for online observation and data storage. Neural responses were analyzed within a 0–150ms temporal window after cue onset, as this period was identified as containing the main cue-evoked responses for most neurons. This time window was selected based on the consistent and robust neural activity observed during this period.</p><p>Additionally, we recorded neural responses from well-trained rats under anesthesia. Anesthesia was induced with an intraperitoneal injection of sodium pentobarbital (40 mg/kg body weight) and maintained throughout the experiment by continuous intraperitoneal infusion of sodium pentobarbital (0.004 ~ 0.008 g/kg/hr) using an automatic microinfusion pump (WZ-50C6, Smiths Medical, Norwell, MA, USA). The anesthetized rats were placed in the behavioral training chamber, and their heads were positioned in the cue port to mimic the cue-triggering as observed during task engagement. To maintain body temperature, a heating blanket was used to maintain a temperature of 37.5 °C. The same auditory, visual, and audiovisual stimuli used during task engagement were randomly presented to the anesthetized rats. For each cue condition, we recorded 40–60 trials of neural responses.</p></sec><sec id="s4-9"><title>Analysis of electrophysiological data</title><p>The raw neural signals were recorded and saved for subsequent offline analysis. Spike sorting was performed using Spike 2 software (CED version 8, Cambridge, UK). Initially, the recorded raw neural signals were band-pass filtered in the range of 300–6000 Hz to eliminate field potentials. A threshold criterion, set at no less than three times the standard deviation (SD) above the background noise, was applied to automatically identify spike peaks. The detected spike waveforms were then subjected to clustering using template-matching and built-in principal component analysis tool in a three-dimensional feature space. Manual curation was conducted to refine the sorting process. Each putative single unit was evaluated based on its waveform and firing patterns over time. Waveforms with inter-spike intervals of less than 2.0ms were excluded from further analysis. Spike trains corresponding to an individual unit were aligned to the onset of the stimulus and grouped based on different cue and choice conditions. Units were included in further analysis only if their presence was stable throughout the session, and their mean firing rate exceeded 2 Hz. The reliability of auditory and visual responses for each unit was assessed, with well-isolated units typically showing the highest response reliability. To generate peristimulus time histograms (PSTHs), the spike trains were binned at a resolution of 10ms, and the average firing rate in each bin was calculated. The resulting firing rate profile was then smoothed using a Gaussian kernel with a standard deviation (σ) of 50ms. In order to normalize the firing rate, the mean firing rate and SD during a baseline period (a 400 ms window preceding cue onset) were used to convert the averaged firing rate of each time bin into a Z-score.</p></sec><sec id="s4-10"><title>Population decoding</title><p>To evaluate population discrimination for paired stimuli (cue_A vs. cue_B), we trained an SVM classifier with a linear kernel to predict cue selectivity. The SVM classifier was implemented using the ‘fitcsvm’ function in Matlab. In this analysis, spike counts for each neuron in correct trials were grouped based on the triggered cues and binned into a 100ms window with a 10ms resolution. To minimize overfitting, only neurons with more than 30 trials for each cue were included. All these neurons were combined to form a pseudo population. The responses of the population neurons were organized into an M × N × T matrix, where M is the number of trials, N is the number of neurons, and T is the number of bins. For each iteration, 30 trials were randomly selected for each cue from each neuron. During cross-validation, 90% of the trials were randomly sampled as the training set, while the remaining 10% were used as the test set. The training set was used to compute the linear hyperplane that optimally separated the population response vectors corresponding to cue_A vs cue_B trials. The performance of the classifier was calculated as the fraction of correctly classified test trials, using 10-fold cross-validation procedures. To ensure robustness, we repeated the resampling process 100 times and computed the mean and standard deviation of the decoding accuracy across the 100 resampling iterations. Decoders were trained and tested independently for each bin. To assess the significance of decoding accuracy exceeding the chance level, shuffled decoding procedures were conducted by randomly shuffling the trial labels for 1000 iterations.</p></sec><sec id="s4-11"><title>Cue selectivity</title><p>To quantify cue (auditory, visual and multisensory) selectivity between two different cue conditions (e.g. low tone trials vs. high tone trials), we employed a ROC-based analysis, following the method described in a previous study (<xref ref-type="bibr" rid="bib6">Britten et al., 1992</xref>). This approach allowed us to assess the difference between responses in cue_A and cue_B trials. Firstly, we established 12 threshold levels of neural activity that covered the range of firing rates obtained in both cue_A and cue_B trials. For each threshold criterion, we plotted the proportion of cue_A trials where the neural response exceeded the criterion against the proportion of cue_B trials exceeding the same criterion. This process generated an ROC curve, from which we calculated the area under the ROC curve (auROC). The cue selectivity value was then defined as 2 * (auROC - 0.5). A cue selectivity value of 0 indicated no difference in the distribution of neural responses between cue_A and cue_B, signifying similar responsiveness to both cues. Conversely, a value of 1 or –1 represented the highest selectivity, indicating that responses triggered by cue_A were consistently higher or lower than those evoked by cue_B, respectively. To determine the statistical significance of the observed cue selectivity, we conducted a two-tailed permutation test with 2000 permutations. We randomly reassigned the neural responses to cue_A and cue_B trials and recalculated a cue selectivity value for each permutation. This generated a distribution of values from which we calculated the probability of our observed result. If the observed ROC value exceeds the top 2.5% of the distribution or falls below the bottom 2.5%, it was deemed significant (i.e. p&lt;0.05).</p></sec><sec id="s4-12"><title>Comparison of actual and predicted multisensory responses</title><p>We conducted a comprehensive analysis to compare the observed multisensory responses with predicted values. The predicted multisensory response is calculated as either the sum of visual and auditory responses or as a coefficient multiplied by this sum. To achieve this, we first computed the mean observed multisensory response by averaging across audiovisual trials. We then created a benchmark distribution of predicted multisensory responses by iteratively calculating all possible predictions. In each iteration, we randomly selected (without replacement) the same number of trials as used in the actual experiment for both auditory and visual conditions. The responses from these selected auditory and visual trials were then averaged to obtain mean predicted auditory and visual responses, which were used to create a predicted multisensory response. This process was repeated 5000 times to generate a comprehensive reference distribution of predicted multisensory responses. By comparing the actual mean multisensory response to this distribution, we expressed their relationship as a Z-score. This method allowed us to quantitatively assess multisensory interactions, providing valuable insights into neural processing and enhancing our understanding of the mechanisms underlying multisensory integration.</p></sec><sec id="s4-13"><title>Histology</title><p>Following the final data recording session, the precise tip position of the recording electrode was marked by creating a small DC lesion (–30 μA for 15 s). Subsequently, the rats were deeply anesthetized with sodium pentobarbital (100 mg/kg) and underwent transcardial perfusion with saline for several minutes, followed immediately by PBS containing 4% paraformaldehyde (PFA). The brains were carefully extracted and immersed in the 4% PFA solution overnight. To ensure optimal tissue preservation, the fixed brain tissue underwent cryoprotection in PBS with a 20% sucrose solution for at least 3 days. Afterward, the brain tissue was coronally sectioned using a freezing microtome (Leica, Wetzlar, Germany) with a slice thickness of 50 μm. The resulting sections, which contained the auditory cortex, were stained with methyl violet to verify the lesion sites and/or the trace of electrode insertion within the primary auditory cortex.</p></sec><sec id="s4-14"><title>Statistical analysis</title><p>We also use ROC analysis to calculate the MSI to denote the difference between multisensory and the corresponding stronger unisensory responses. All statistical analyses were conducted in MATLAB, and statistical significance was defined as a p value of&lt;0.05. To determine the responsiveness of AC neurons to sensory stimuli, neurons exhibiting evoked responses greater than 2 spikes/s within a 0.3 s window after stimulus onset, and significantly higher than the baseline response (p &lt; 0.05, Wilcoxon signed-rank test), were included in the subsequent analysis. For behavioral data, such as mean reaction time differences between unisensory and multisensory trials, cue selectivity and mean MSI across different auditory-visual conditions, comparisons were performed using either the paired t-test or the Wilcoxon signed-rank test. The Shapiro-Wilk test was conducted to assess normality, with the paired t-test used for normally distributed data and the Wilcoxon signed-rank test for non-normal data. We performed a Chi-square test to analyze the difference in the proportions of neurons responding to visual stimuli between the multisensory discrimination and free-choice groups. Correlation values were computed using Pearson’s correlation. All data are presented as mean ± SD for the respective groups. The data were not collected and analyzed in a blinded fashion.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Resources, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Project administration</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Validation</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Formal analysis, Funding acquisition, Investigation, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>The animal procedures conducted in this study were ethically approved by the Local Ethical Review Committee of East China Normal University (Protocol number: m+R20211004) and followed the guidelines outlined in the Guide for the Care and Use of Laboratory Animals of East China Normal University. All surgery was performed under sodium pentobarbital anesthesia, and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-102926-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="sdata1"><label>Source data 1.</label><caption><title>Source data contain most of data shown in <xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig7">7</xref>.</title><p>However, some exemplar neuron data were not properly saved in the Excel files. These datasets have been deposited separately in <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17632/xm2gfng69b/1">Mendeley Data</ext-link> for accessibility.</p></caption><media xlink:href="elife-102926-data1-v1.xlsx" mimetype="application" mime-subtype="xlsx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All experimental data and matlab custom codes have been deposited in <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.17632/xm2gfng69b.1">Mendeley Data</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name></person-group><source>Mendeley Data</source><year iso-8601-date="2025">2025</year><data-title>Auditory Cortex Learns to Discriminate Audiovisual Cues through Selective Multisensory Enhancement</data-title><pub-id pub-id-type="doi">10.17632/xm2gfng69b.1</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by grants from the “STI2030-major projects” (2021ZD0202600), Natural Science Foundation of China (32371046, 31970925, 32271057).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Angelaki</surname><given-names>DE</given-names></name><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Multisensory integration: psychophysics, neurophysiology, and computation</article-title><source>Current Opinion in Neurobiology</source><volume>19</volume><fpage>452</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2009.06.008</pub-id><pub-id pub-id-type="pmid">19616425</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atilgan</surname><given-names>H</given-names></name><name><surname>Town</surname><given-names>SM</given-names></name><name><surname>Wood</surname><given-names>KC</given-names></name><name><surname>Jones</surname><given-names>GP</given-names></name><name><surname>Maddox</surname><given-names>RK</given-names></name><name><surname>Lee</surname><given-names>AKC</given-names></name><name><surname>Bizley</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Integration of visual information in auditory cortex promotes auditory scene analysis through multisensory binding</article-title><source>Neuron</source><volume>97</volume><fpage>640</fpage><lpage>655</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.034</pub-id><pub-id pub-id-type="pmid">29395914</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banks</surname><given-names>MI</given-names></name><name><surname>Uhlrich</surname><given-names>DJ</given-names></name><name><surname>Smith</surname><given-names>PH</given-names></name><name><surname>Krause</surname><given-names>BM</given-names></name><name><surname>Manning</surname><given-names>KA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Descending projections from extrastriate visual cortex modulate responses of cells in primary auditory cortex</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>2620</fpage><lpage>2638</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr048</pub-id><pub-id pub-id-type="pmid">21471557</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bimbard</surname><given-names>C</given-names></name><name><surname>Sit</surname><given-names>TPH</given-names></name><name><surname>Lebedeva</surname><given-names>A</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Behavioral origin of sound-evoked activity in mouse visual cortex</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>251</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01227-x</pub-id><pub-id pub-id-type="pmid">36624279</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bizley</surname><given-names>JK</given-names></name><name><surname>Nodal</surname><given-names>FR</given-names></name><name><surname>Bajo</surname><given-names>VM</given-names></name><name><surname>Nelken</surname><given-names>I</given-names></name><name><surname>King</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Physiological and anatomical evidence for multisensory interactions in auditory cortex</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>2172</fpage><lpage>2189</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhl128</pub-id><pub-id pub-id-type="pmid">17135481</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name><name><surname>Movshon</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The analysis of visual motion: a comparison of neuronal and psychophysical performance</article-title><source>The Journal of Neuroscience</source><volume>12</volume><fpage>4745</fpage><lpage>4765</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.12-12-04745.1992</pub-id><pub-id pub-id-type="pmid">1464765</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campi</surname><given-names>KL</given-names></name><name><surname>Bales</surname><given-names>KL</given-names></name><name><surname>Grunewald</surname><given-names>R</given-names></name><name><surname>Krubitzer</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Connections of auditory and visual cortex in the prairie vole (Microtus ochrogaster): evidence for multisensory processing in primary sensory areas</article-title><source>Cerebral Cortex</source><volume>20</volume><fpage>89</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhp082</pub-id><pub-id pub-id-type="pmid">19395525</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cappe</surname><given-names>C</given-names></name><name><surname>Barone</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Heteromodal connections supporting multisensory integration at low levels of cortical processing in the monkey</article-title><source>The European Journal of Neuroscience</source><volume>22</volume><fpage>2886</fpage><lpage>2902</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2005.04462.x</pub-id><pub-id pub-id-type="pmid">16324124</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chadwick</surname><given-names>A</given-names></name><name><surname>Khan</surname><given-names>AG</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Blot</surname><given-names>A</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Learning shapes cortical dynamics to enhance integration of relevant sensory input</article-title><source>Neuron</source><volume>111</volume><fpage>106</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.10.001</pub-id><pub-id pub-id-type="pmid">36283408</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Zheng</surname><given-names>M</given-names></name><name><surname>Keniston</surname><given-names>L</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Integrating visual information into the auditory cortex promotes sound discrimination through choice-related multisensory integration</article-title><source>The Journal of Neuroscience</source><volume>42</volume><fpage>8556</fpage><lpage>8568</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0793-22.2022</pub-id><pub-id pub-id-type="pmid">36150889</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chou</surname><given-names>XL</given-names></name><name><surname>Fang</surname><given-names>Q</given-names></name><name><surname>Yan</surname><given-names>L</given-names></name><name><surname>Zhong</surname><given-names>W</given-names></name><name><surname>Peng</surname><given-names>B</given-names></name><name><surname>Li</surname><given-names>H</given-names></name><name><surname>Wei</surname><given-names>J</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Contextual and cross-modality modulation of auditory cortical processing through pulvinar mediated suppression</article-title><source>eLife</source><volume>9</volume><elocation-id>e54157</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.54157</pub-id><pub-id pub-id-type="pmid">32142411</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clayton</surname><given-names>KK</given-names></name><name><surname>Stecyk</surname><given-names>KS</given-names></name><name><surname>Guo</surname><given-names>AA</given-names></name><name><surname>Chambers</surname><given-names>AR</given-names></name><name><surname>Chen</surname><given-names>K</given-names></name><name><surname>Hancock</surname><given-names>KE</given-names></name><name><surname>Polley</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Sound elicits stereotyped facial movements that provide a sensitive index of hearing abilities in mice</article-title><source>Current Biology</source><volume>34</volume><fpage>1605</fpage><lpage>1620</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2024.02.057</pub-id><pub-id pub-id-type="pmid">38492568</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneux</surname><given-names>T</given-names></name><name><surname>Harrell</surname><given-names>ER</given-names></name><name><surname>Kempf</surname><given-names>A</given-names></name><name><surname>Ceballo</surname><given-names>S</given-names></name><name><surname>Filipchuk</surname><given-names>A</given-names></name><name><surname>Bathellier</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Context-dependent signaling of coincident auditory and visual events in primary visual cortex</article-title><source>eLife</source><volume>8</volume><elocation-id>e44006</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.44006</pub-id><pub-id pub-id-type="pmid">31115334</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elgueda</surname><given-names>D</given-names></name><name><surname>Duque</surname><given-names>D</given-names></name><name><surname>Radtke-Schuller</surname><given-names>S</given-names></name><name><surname>Yin</surname><given-names>P</given-names></name><name><surname>David</surname><given-names>SV</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name><name><surname>Fritz</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>State-dependent encoding of sound and behavioral meaning in a tertiary region of the ferret auditory cortex</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>447</fpage><lpage>459</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0317-8</pub-id><pub-id pub-id-type="pmid">30692690</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Falchier</surname><given-names>A</given-names></name><name><surname>Clavagnier</surname><given-names>S</given-names></name><name><surname>Barone</surname><given-names>P</given-names></name><name><surname>Kennedy</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Anatomical evidence of multimodal integration in primate striate cortex</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>5749</fpage><lpage>5759</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-13-05749.2002</pub-id><pub-id pub-id-type="pmid">12097528</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garner</surname><given-names>AR</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A cortical circuit for audio-visual predictions</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>98</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00974-7</pub-id><pub-id pub-id-type="pmid">34857950</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Maier</surname><given-names>JX</given-names></name><name><surname>Hoffman</surname><given-names>KL</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Multisensory integration of dynamic faces and voices in rhesus monkey auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>5004</fpage><lpage>5012</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0799-05.2005</pub-id><pub-id pub-id-type="pmid">15901781</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Schroeder</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Is neocortex essentially multisensory?</article-title><source>Trends in Cognitive Sciences</source><volume>10</volume><fpage>278</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2006.04.008</pub-id><pub-id pub-id-type="pmid">16713325</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Keniston</surname><given-names>L</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multisensory-guided associative learning enhances multisensory representation in primary auditory cortex</article-title><source>Cerebral Cortex</source><volume>32</volume><fpage>1040</fpage><lpage>1054</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhab264</pub-id><pub-id pub-id-type="pmid">34378017</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hirokawa</surname><given-names>J</given-names></name><name><surname>Sadakane</surname><given-names>O</given-names></name><name><surname>Sakata</surname><given-names>S</given-names></name><name><surname>Bosch</surname><given-names>M</given-names></name><name><surname>Sakurai</surname><given-names>Y</given-names></name><name><surname>Yamamori</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multisensory information facilitates reaction speed by enlarging activity difference between superior colliculus hemispheres in rats</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e25283</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0025283</pub-id><pub-id pub-id-type="pmid">21966481</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ibrahim</surname><given-names>LA</given-names></name><name><surname>Mesik</surname><given-names>L</given-names></name><name><surname>Ji</surname><given-names>X-Y</given-names></name><name><surname>Fang</surname><given-names>Q</given-names></name><name><surname>Li</surname><given-names>H-F</given-names></name><name><surname>Li</surname><given-names>Y-T</given-names></name><name><surname>Zingg</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>LI</given-names></name><name><surname>Tao</surname><given-names>HW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Cross-modality sharpening of visual cortical processing through layer-1-mediated inhibition and disinhibition</article-title><source>Neuron</source><volume>89</volume><fpage>1031</fpage><lpage>1045</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.027</pub-id><pub-id pub-id-type="pmid">26898778</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Decision-making behaviors: weighing ethology, complexity, and sensorimotor compatibility</article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>42</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.11.001</pub-id><pub-id pub-id-type="pmid">29179005</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visual modulation of neurons in auditory cortex</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>1560</fpage><lpage>1574</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm187</pub-id><pub-id pub-id-type="pmid">18180245</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khan</surname><given-names>AG</given-names></name><name><surname>Poort</surname><given-names>J</given-names></name><name><surname>Chadwick</surname><given-names>A</given-names></name><name><surname>Blot</surname><given-names>A</given-names></name><name><surname>Sahani</surname><given-names>M</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct learning-induced changes in stimulus selectivity and interactions of GABAergic interneuron classes in visual cortex</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>851</fpage><lpage>859</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0143-z</pub-id><pub-id pub-id-type="pmid">29786081</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knöpfel</surname><given-names>T</given-names></name><name><surname>Sweeney</surname><given-names>Y</given-names></name><name><surname>Radulescu</surname><given-names>CI</given-names></name><name><surname>Zabouri</surname><given-names>N</given-names></name><name><surname>Doostdar</surname><given-names>N</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Barnes</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Audio-visual experience strengthens multisensory assemblies in adult mouse visual cortex</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>5684</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-13607-2</pub-id><pub-id pub-id-type="pmid">31831751</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komiyama</surname><given-names>T</given-names></name><name><surname>Sato</surname><given-names>TR</given-names></name><name><surname>O’Connor</surname><given-names>DH</given-names></name><name><surname>Zhang</surname><given-names>Y-X</given-names></name><name><surname>Huber</surname><given-names>D</given-names></name><name><surname>Hooks</surname><given-names>BM</given-names></name><name><surname>Gabitto</surname><given-names>M</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Learning-related fine-scale specificity imaged in motor cortex circuits of behaving mice</article-title><source>Nature</source><volume>464</volume><fpage>1182</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.1038/nature08897</pub-id><pub-id pub-id-type="pmid">20376005</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laurienti</surname><given-names>PJ</given-names></name><name><surname>Kraft</surname><given-names>RA</given-names></name><name><surname>Maldjian</surname><given-names>JA</given-names></name><name><surname>Burdette</surname><given-names>JH</given-names></name><name><surname>Wallace</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Semantic congruence is a critical factor in multisensory behavioral performance</article-title><source>Experimental Brain Research</source><volume>158</volume><fpage>405</fpage><lpage>414</lpage><pub-id pub-id-type="doi">10.1007/s00221-004-1913-2</pub-id><pub-id pub-id-type="pmid">15221173</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lomber</surname><given-names>SG</given-names></name><name><surname>Meredith</surname><given-names>MA</given-names></name><name><surname>Kral</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Cross-modal plasticity in specific auditory cortices underlies visual compensations in the deaf</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1421</fpage><lpage>1427</lpage><pub-id pub-id-type="doi">10.1038/nn.2653</pub-id><pub-id pub-id-type="pmid">20935644</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makino</surname><given-names>H</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Learning enhances the relative impact of top-down processing in the visual cortex</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1116</fpage><lpage>1122</lpage><pub-id pub-id-type="doi">10.1038/nn.4061</pub-id><pub-id pub-id-type="pmid">26167904</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makino</surname><given-names>H</given-names></name><name><surname>Hwang</surname><given-names>EJ</given-names></name><name><surname>Hedrick</surname><given-names>NG</given-names></name><name><surname>Komiyama</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Circuit mechanisms of sensorimotor learning</article-title><source>Neuron</source><volume>92</volume><fpage>705</fpage><lpage>721</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.029</pub-id><pub-id pub-id-type="pmid">27883902</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meijer</surname><given-names>GT</given-names></name><name><surname>Montijn</surname><given-names>JS</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name><name><surname>Lansink</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Audiovisual modulation in mouse primary visual cortex depends on cross-modal stimulus configuration and congruency</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>8783</fpage><lpage>8796</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0468-17.2017</pub-id><pub-id pub-id-type="pmid">28821672</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrill</surname><given-names>RJ</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Visual information present in infragranular layers of mouse auditory cortex</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2854</fpage><lpage>2862</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3102-17.2018</pub-id><pub-id pub-id-type="pmid">29440554</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrill</surname><given-names>RJ</given-names></name><name><surname>Bigelow</surname><given-names>J</given-names></name><name><surname>DeKloe</surname><given-names>J</given-names></name><name><surname>Hasenstaub</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Audiovisual task switching rapidly modulates sound encoding in mouse auditory cortex</article-title><source>eLife</source><volume>11</volume><elocation-id>e75839</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.75839</pub-id><pub-id pub-id-type="pmid">35980027</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musall</surname><given-names>S</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Juavinett</surname><given-names>AL</given-names></name><name><surname>Gluf</surname><given-names>S</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Single-trial neural dynamics are dominated by richly varied movements</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1677</fpage><lpage>1686</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0502-4</pub-id><pub-id pub-id-type="pmid">31551604</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niell</surname><given-names>CM</given-names></name><name><surname>Stryker</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Modulation of visual responses by behavioral state in mouse visual cortex</article-title><source>Neuron</source><volume>65</volume><fpage>472</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.01.033</pub-id><pub-id pub-id-type="pmid">20188652</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Otazu</surname><given-names>GH</given-names></name><name><surname>Tai</surname><given-names>LH</given-names></name><name><surname>Yang</surname><given-names>Y</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Engaging in an auditory task suppresses responses in auditory cortex</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>646</fpage><lpage>654</lpage><pub-id pub-id-type="doi">10.1038/nn.2306</pub-id><pub-id pub-id-type="pmid">19363491</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oude Lohuis</surname><given-names>MN</given-names></name><name><surname>Marchesi</surname><given-names>P</given-names></name><name><surname>Olcese</surname><given-names>U</given-names></name><name><surname>Pennartz</surname><given-names>CMA</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Triple dissociation of visual, auditory and motor processing in mouse primary visual cortex</article-title><source>Nature Neuroscience</source><volume>27</volume><fpage>758</fpage><lpage>771</lpage><pub-id pub-id-type="doi">10.1038/s41593-023-01564-5</pub-id><pub-id pub-id-type="pmid">38307971</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrodin</surname><given-names>C</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Petkov</surname><given-names>CI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Natural asynchronies in audiovisual communication signals regulate neuronal multisensory interactions in voice-sensitive cortex</article-title><source>PNAS</source><volume>112</volume><fpage>273</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1073/pnas.1412817112</pub-id><pub-id pub-id-type="pmid">25535356</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Proulx</surname><given-names>MJ</given-names></name><name><surname>Brown</surname><given-names>DJ</given-names></name><name><surname>Pasqualotto</surname><given-names>A</given-names></name><name><surname>Meijer</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multisensory perceptual learning and sensory substitution</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>41</volume><fpage>16</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2012.11.017</pub-id><pub-id pub-id-type="pmid">23220697</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raposo</surname><given-names>D</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A category-free neural population supports evolving demands during decision-making</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1784</fpage><lpage>1792</lpage><pub-id pub-id-type="doi">10.1038/nn.3865</pub-id><pub-id pub-id-type="pmid">25383902</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romo</surname><given-names>R</given-names></name><name><surname>Rossi-Pool</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Turning touch into perception</article-title><source>Neuron</source><volume>105</volume><fpage>16</fpage><lpage>33</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.11.033</pub-id><pub-id pub-id-type="pmid">31917952</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmehl</surname><given-names>MN</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Visual signals in the mammalian auditory system</article-title><source>Annual Review of Vision Science</source><volume>7</volume><fpage>201</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-091517-034003</pub-id><pub-id pub-id-type="pmid">34242053</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoups</surname><given-names>A</given-names></name><name><surname>Vogels</surname><given-names>R</given-names></name><name><surname>Qian</surname><given-names>N</given-names></name><name><surname>Orban</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Practising orientation identification improves orientation coding in V1 neurons</article-title><source>Nature</source><volume>412</volume><fpage>549</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1038/35087601</pub-id><pub-id pub-id-type="pmid">11484056</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Seitz</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Benefits of multisensory learning</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>411</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.07.006</pub-id><pub-id pub-id-type="pmid">18805039</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smyre</surname><given-names>SA</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Rowland</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Multisensory enhancement of overt behavior requires multisensory experience</article-title><source>The European Journal of Neuroscience</source><volume>54</volume><fpage>4514</fpage><lpage>4527</lpage><pub-id pub-id-type="doi">10.1111/ejn.15315</pub-id><pub-id pub-id-type="pmid">34013578</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stehberg</surname><given-names>J</given-names></name><name><surname>Dang</surname><given-names>PT</given-names></name><name><surname>Frostig</surname><given-names>RD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Unimodal primary sensory cortices are directly connected by long-range horizontal projections in the rat sensory cortex</article-title><source>Frontiers in Neuroanatomy</source><volume>8</volume><elocation-id>93</elocation-id><pub-id pub-id-type="doi">10.3389/fnana.2014.00093</pub-id><pub-id pub-id-type="pmid">25309339</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Meredith</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="1993">1993</year><source>The Merging of the Senses</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name><name><surname>Rowland</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Development of multisensory integration from the perspective of the individual neuron</article-title><source>Nature Reviews. Neuroscience</source><volume>15</volume><fpage>520</fpage><lpage>535</lpage><pub-id pub-id-type="doi">10.1038/nrn3742</pub-id><pub-id pub-id-type="pmid">25158358</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>C</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>N</given-names></name><name><surname>Reddy</surname><given-names>CB</given-names></name><name><surname>Carandini</surname><given-names>M</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Spontaneous behaviors drive multidimensional, brainwide activity</article-title><source>Science</source><volume>364</volume><elocation-id>255</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav7893</pub-id><pub-id pub-id-type="pmid">31000656</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vincis</surname><given-names>R</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Associative learning changes cross-modal representations in the gustatory cortex</article-title><source>eLife</source><volume>5</volume><elocation-id>e16420</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.16420</pub-id><pub-id pub-id-type="pmid">27572258</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vinck</surname><given-names>M</given-names></name><name><surname>Batista-Brito</surname><given-names>R</given-names></name><name><surname>Knoblich</surname><given-names>U</given-names></name><name><surname>Cardin</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Arousal and locomotion make distinct contributions to cortical activity patterns and visual encoding</article-title><source>Neuron</source><volume>86</volume><fpage>740</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.03.028</pub-id><pub-id pub-id-type="pmid">25892300</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>MT</given-names></name><name><surname>Woynaroski</surname><given-names>TG</given-names></name><name><surname>Stevenson</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multisensory integration as a window into orderly and disrupted cognition and communication</article-title><source>Annual Review of Psychology</source><volume>71</volume><fpage>193</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-010419-051112</pub-id><pub-id pub-id-type="pmid">31518523</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xin</surname><given-names>Y</given-names></name><name><surname>Zhong</surname><given-names>L</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>T</given-names></name><name><surname>Pan</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>N-L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sensory-to-category transformation via dynamic reorganization of ensemble structures in mouse auditory cortex</article-title><source>Neuron</source><volume>103</volume><fpage>909</fpage><lpage>921</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.06.004</pub-id><pub-id pub-id-type="pmid">31296412</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>Q</given-names></name><name><surname>Znamenskiy</surname><given-names>P</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Selective corticostriatal plasticity during acquisition of an auditory discrimination task</article-title><source>Nature</source><volume>521</volume><fpage>348</fpage><lpage>351</lpage><pub-id pub-id-type="doi">10.1038/nature14225</pub-id><pub-id pub-id-type="pmid">25731173</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Sun</surname><given-names>X</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The cortical distribution of multisensory neurons was modulated by multisensory experience</article-title><source>Neuroscience</source><volume>272</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2014.04.068</pub-id><pub-id pub-id-type="pmid">24813435</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Bi</surname><given-names>T</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Meng</surname><given-names>F</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Hu</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>X</given-names></name><name><surname>Zhang</surname><given-names>J</given-names></name><name><surname>Zhou</surname><given-names>X</given-names></name><name><surname>Keniston</surname><given-names>L</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Spatial receptive field shift by preceding cross-modal stimulation in the cat superior colliculus</article-title><source>The Journal of Physiology</source><volume>596</volume><fpage>5033</fpage><lpage>5050</lpage><pub-id pub-id-type="doi">10.1113/JP275427</pub-id><pub-id pub-id-type="pmid">30144059</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yan</surname><given-names>Y</given-names></name><name><surname>Rasch</surname><given-names>MJ</given-names></name><name><surname>Chen</surname><given-names>M</given-names></name><name><surname>Xiang</surname><given-names>X</given-names></name><name><surname>Huang</surname><given-names>M</given-names></name><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Perceptual training continuously refines neuronal population codes in primary visual cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>1380</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1038/nn.3805</pub-id><pub-id pub-id-type="pmid">25195103</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Keniston</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>J</given-names></name><name><surname>Chang</surname><given-names>S</given-names></name><name><surname>Yu</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Choice-dependent cross-modal interaction in the medial prefrontal cortex of rats</article-title><source>Molecular Brain</source><volume>14</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.1186/s13041-021-00732-7</pub-id><pub-id pub-id-type="pmid">33446258</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102926.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>King</surname><given-names>Andrew J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This is an <bold>important</bold> study that aims to investigate the behavioral relevance of multisensory responses recorded in the auditory cortex. The experiments are elegant and well-designed and are supported by appropriate analyses of the data. Although <bold>solid</bold> evidence is presented that is consistent with learning-dependent encoding of visual information in auditory cortex, further work is needed to establish the origin and nature of these non-auditory signals and to definitively rule out any effects of movement-related activity.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102926.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Chang and colleagues use tetrode recordings in behaving rats to study how learning an audiovisual discrimination task shapes multisensory interactions in auditory cortex. They find that a significant fraction of neurons in auditory cortex responded to visual (crossmodal) and audiovisual stimuli. Both auditory-responsive and visually-responsive neurons preferentially responded to the cue signaling the contralateral choice in the two-alternative forced choice task. Importantly, multisensory interactions were similarly specific for the congruent audiovisual pairing for the contralateral side.</p><p>Strengths:</p><p>The experiments are conducted in a rigorous manner. Particularly thorough are the comparisons across cohorts of rats trained in a control task, in a unisensory auditory discrimination task and the multisensory task, while also varying the recording hemisphere and behavioral state (engaged vs. anesthesia). The resulting contrasts strengthen the authors' findings and rule out important alternative explanations regarding the effect of experience. Through the comparisons, they show that the enhancements of activity in multisensory trials in auditory cortex are specific to the paired audiovisual stimulus and specific to contralateral choices in correct trials and thus dependent on learned associations in a task engaged state.</p><p>Weaknesses:</p><p>The main result that multisensory interactions are specific for contralateral paired audiovisual stimuli is consistent across experiments and interpretable as a learned task-dependent effect. However, the alternative interpretation of behavioral signals is crucial to rule out, which would also be specific to contralateral, correct trials in trained animals. Although the authors focus on the first 150 ms after cue onset, some of the temporal profiles of activity suggest that choice-related activity could confound some of the results.</p><p>The main concern (noted by all reviewers) is the interpretation of the evoked activity in visual trials. In the revised manuscript, the authors have not provided much data to disentangle movement related activity from sensory related activity. The only new data is on the visual response dynamics in supplementary figure 2, which is unconvincing both in terms of visual response latency and response dynamics. Therefore, the response of the authors has been insufficient to prove the visual nature of the evoked responses.</p><p>In this supplemental figure 2 the same example neuron as in the original manuscript is shown again as well as the average z-scored visual response. First, the visual response latency is inconsistent with literature. The first evoked activity in mouse V1 (!) is routinely reported around 50 ms (for example, 45 ms in Niell Stryker 2008, 52 ms, Schnabel et al. 2018, 54 ms in Oude Lohuis et al. 2024). According to the authors the potential route of crossmodal modulation of AC can occur through either corticocortical connections (which will impose further polysynaptic delays - monosynaptic projection from dLGN or V1 incredibly sparse), or through pulvinar (but pulvinar visual responses are much later (they find 170 vs 80 ms in dLGN, Roth et al. 2019) as expected from a higher order thalamic nucleus). One can also critique the estimation of the response latency which depends on the signal strength (visual response is smaller) and thus choice of threshold. With a different arbitrary threshold one would come to different conclusions.</p><p>Second, the temporal response dynamics to visual input are the same as the auditory response. It can be observed that if the data were normalized by the max response the dynamics are very similar, with the response back to near baseline levels at 100 ms post stimulus. I am not aware of publications that have observed response dynamics that are similar between A and V stimuli, nor such short-lasting visual response. In the visual system, mean activity typically drops again around 150-200ms.</p><p>With the nature of the observed activity unclear, careful interpretation is warranted about audiovisual interactions in auditory cortex.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102926.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In this revision the authors have made a solid effort to address each of the points raised by all three reviewers. Due to the fact that animals in this study were freely moving, and there has not been any high-speed video recordings to measure whisker movements or other possible stimulus-induced motor effects it is still not possible to rule out motor effects completely. However, the fact that the multisensory enhancements are stimulus specific, much stronger in the multisensory case than the visual only condition, and short in latency it does seem the most parsimonious explanation is likely that these responses are visual in nature.</p><p>The delayed auditory stimulus offers some explanation for the very small latency difference between audio and visual stimulus elements. Studies using LED flashes in rat V2 report latencies around ~50 ms (e.g. 2017 paper from Brian Allman's group). The response latencies for visual stimuli in this manuscript are of this order of magnitude, albeit still shorter than that (which presumably means they don't originate from V2).</p><p>There are still parts of the manuscript that are inappropriately causal - e.g. line 283 &quot;this suggests that strong multisensory integration is critical for behavior&quot; - it could just as well be the case that high attention / motivation / arousal leads to both strong integration and good behavior.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102926.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The manuscript by Chang et al. aims to investigate how the behavioral relevance of auditory and visual stimuli influences the way in which the primary auditory cortex encodes auditory, visual and audiovisual information. The main results is that behavioral training induces an increase in the encoding of auditory and visual information and in multisensory enhancement that is mainly related to the choice located contralaterally with respect to the recorded hemisphere.</p><p>Strengths:</p><p>The manuscript reports the results of an elegant and well planned experiment meant to investigate if auditory cortex encodes visual information and how learning shapes visual responsiveness in auditory cortex. Analyses are typically well done and properly address the questions raised</p><p>Weaknesses:</p><p>The authors have addressed most of my comments satisfactorily. However, I am still not convinced by the authors' claim that the use of LED should lead to visually-evoked responses with faster dynamics compared to the use of normal screens. In fact, previous studies using screen-emitted flashed did not report such faster dynamics. Visually-evoked responses in V1 (which are expected to occur earlier than A1) typically do not show onset latencies faster than 40 ms, and have a peak latency of about 100-120 ms. The dynamics shown in the new supplementary Fig. 2 are still faster than this, and thus should be explained. The authors' claim is in fact not supported by cited literature. The authors should at least provide evidence that a similar effect has been observed previously, or otherwise collect evidence themselves. In the absence of such evidence, I remain dubious about the visual nature of the observed activity, especially since, in contrast with what the authors say elsewhere in the rebuttal, involuntary motor reaction to (at least auditory) stimuli can be extremely fast (&lt;40 ms; Clayton et al. 2024) and might thus potentially, at least partially, explain the observed &quot;visual&quot; response.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.102926.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chang</surname><given-names>Song</given-names></name><role specific-use="author">Author</role><aff><institution>East China Normal University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Zheng</surname><given-names>Beilin</given-names></name><role specific-use="author">Author</role><aff><institution>Hangzhou Vocational and Technical College</institution><addr-line><named-content content-type="city">Hangzhou</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Keniston</surname><given-names>Les</given-names></name><role specific-use="author">Author</role><aff><institution>University of Pikeville</institution><addr-line><named-content content-type="city">Pikeville</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Xu</surname><given-names>Jinghong</given-names></name><role specific-use="author">Author</role><aff><institution>East China Normal University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Yu</surname><given-names>Liping</given-names></name><role specific-use="author">Author</role><aff><institution>East China Normal University</institution><addr-line><named-content content-type="city">Shanghai</named-content></addr-line><country>China</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>Chang and colleagues used tetrode recordings in behaving rats to study how learning an audiovisual discrimination task shapes multisensory interactions in the auditory cortex. They found that a significant fraction of neurons in the auditory cortex responded to visual (crossmodal) and audiovisual stimuli. Both auditory-responsive and visually-responsive neurons preferentially responded to the cue signaling the contralateral choice in the two-alternative forced choice task. Importantly, multisensory interactions were similarly specific for the congruent audiovisual pairing for the contralateral side.</p><p>Strengths:</p><p>The experiments were conducted in a rigorous manner. Particularly thorough are the comparisons across cohorts of rats trained in a control task, in a unisensory auditory discrimination task, and the multisensory task, while also varying the recording hemisphere and behavioral state (engaged vs. anesthesia). The resulting contrasts strengthen the authors' findings and rule out important alternative explanations. Through the comparisons, they show that the enhancements of multisensory responses in the auditory cortex are specific to the paired audiovisual stimulus and specific to contralateral choices in correct trials and thus dependent on learned associations in a task-engaged state.</p></disp-quote><p>We thank Reviewer #1 for the thorough review and valuable feedback.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>The main result is that multisensory interactions are specific for contralateral paired audiovisual stimuli, which is consistent across experiments and interpretable as a learned task-dependent effect. However, the alternative interpretation of behavioral signals is crucial to rule out, which would also be specific to contralateral, correct trials in trained animals. Although the authors focus on the first 150 ms after cue onset, some of the temporal profiles of activity suggest that choice-related activity could confound some of the results.</p></disp-quote><p>We thank the reviewer for raising this important point regarding the potential influence of choice-related activity on our results. In our experimental setup, it is challenging to completely disentangle the effects of behavioral choice from multisensory interaction. However, we conducted relevant analyses to examine the influence of choice-related components on multisensory interaction.</p><p>First, we analyzed neural responses during incorrect trials and found a significant reduction in multisensory enhancement for the A<sup>10k</sup>-V<sup>vt</sup> pairing (Fig. 4). In contrast, for the A<sup>3k</sup>-V<sup>hz</sup> pairing, there was no strong multisensory interaction during either correct (right direction) or incorrect (left direction) choices. This finding suggests that the observed multisensory interactions are strongly associated with specific cue combinations during correct task performance.</p><p>Second, we conducted experiments with unisensory training, in which animals were trained separately on auditory and visual discriminations without explicit multisensory associations. The results demonstrated that unisensory training did not lead to the development of selective multisensory enhancement or congruent auditory-visual preferences, as observed in the multisensory training group. This indicates that the observed multisensory interactions in the auditory cortex are specific to multisensory training and cannot be attributed solely to behavioral signals or choice-related effects.</p><p>Finally, we specifically focused on the early 0-150 ms time window after cue onset in our main analyses to minimize contributions from motor-related or decision-related activity, which typically emerge later. This time window allowed us to capture early sensory processing while reducing potential confounds.</p><p>Together, these findings strongly suggest that the observed choice-dependent multisensory enhancement is a learned, task-dependent phenomenon that is specific to multisensory training.</p><disp-quote content-type="editor-comment"><p>The auditory stimuli appear to be encoded by short transient activity (in line with much of what we know about the auditory system), likely with onset latencies (not reported) of 15-30 ms. Stimulus identity can be decoded (Figure 2j) apparently with an onset latency around 50-75 ms (only the difference between A and AV groups is reported) and can be decoded near perfectly for an extended time window, without a dip in decoding performance that is observed in the mean activity Figure 2e. The dynamics of the response of the example neurons presented in Figures 2c and d and the average in 2e therefore do not entirely match the population decoding profile in 2j. Population decoding uses the population activity distribution, rather than the mean, so this is not inherently problematic. It suggests however that the stimulus identity can be decoded from later (choice-related?) activity. The dynamics of the population decoding accuracy are in line with the dynamics one could expect based on choice-related activity. Also the results in Figures S2e,f suggest differences between the two learned stimuli can be in the late phase of the response, not in the early phase.</p></disp-quote><p>We appreciate the reviewer’s detailed observations and questions regarding the dynamics of auditory responses and decoding profiles in our study. In our experiment, primary auditory cortex (A1) neurons exhibited short response latencies that meet the established criteria for auditory responses in A1, consistent with findings from many other studies conducted in both anesthetized and task-engaged animals. While the major responses typically occurred during the early period (0-150ms) after cue onset (see population response in Fig. 2e), individual neuronal responses in the whole population were generally dynamic, as illustrated in Figures 2c, 2d, and 3a–c. As the reviewer correctly noted, population decoding leverages the distribution of activity across neurons rather than the mean activity, which explains why the dynamics of population decoding accuracy align well with choice-related activity. This also accounts for the extended decoding window observed in Figure 2j, which does not entirely match the early population response profiles in Figure 2e.</p><p>To address the reviewer’s suggestion that differences between the two learned stimuli might arise in the late phase of the response, we conducted a cue selectivity analysis during the 151–300 ms period after cue onset. The results, shown below, indicate that neurons maintained cue selectivity in this late phase for each modality (Supplementary Fig. 5), though the selectivity was lower than in the early phase. However, interpreting this late-phase activity remains challenging. Since A<sup>3k</sup>, V<sup>hz</sup>, and A<sup>3k</sup>-V<sup>hz</sup> were associated with the right choice, and A<sup>10k</sup>, V<sup>vt</sup>, and A<sup>10k</sup>-V<sup>vt</sup> with the left choice, it is difficult to disentangle whether the responses reflect choice, sensory features, or a combination of both.</p><p>To further investigate, we examined multisensory interactions during the late phase, controlling for choice effects by calculating unisensory and multisensory responses within the same choice context. Our analysis revealed no evident multisensory enhancement for any auditory-visual pairing, nor significant differences between pairings—unlike the robust effects observed in the early phase (Supplementary Fig. 5). We hypothesize that early responses are predominantly sensory-driven and exhibit strong multisensory integration, whereas late responses likely reflect task-related, choice-related, or combined sensory-choice activity, where sensory-driven multisensory enhancement is less prominent. As the focus of this manuscript is on multisensory integration and cue selectivity, we prioritized a detailed analysis of the early phase, where these effects are most prominent. However, the complexity of interpreting late-phase activity remains a challenge and warrants further investigation. We cited Supplementary Fig. 5 in revised manuscript as the following:</p><p>“This resulted in a significantly higher mean MSI for the A<sup>10k</sup>-V<sup>vt</sup> pairing compared to the A<sup>3k</sup>-V<sup>hz</sup> pairing (0.047 ± 0.124 vs. 0.003 ± 0.096; paired t-test, p &lt; 0.001). Among audiovisual neurons, this biasing is even more pronounced (enhanced vs. inhibited: 62 vs. 2 in A<sup>10k</sup>-V<sup>vt</sup> pairing, 6 vs. 13 in A<sup>3k</sup>-V<sup>hz</sup> pairing; mean MSI: 0.119±0.105 in A<sup>10k</sup>-V<sup>vt</sup> pairing vs. 0.020±0.083 A<sup>3k</sup>-V<sup>hz</sup> pairing, paired t-test, p&lt;0.00001) (Fig. 3f). Unlike the early period (0-150ms after cue onset), no significant differences in multisensory integration were observed during the late period (151-300ms after cue onset) (Supplementary Fig. 5).”</p><disp-quote content-type="editor-comment"><p>First, it would help to have the same time axis across panels 2,c,d,e,j,k. Second, a careful temporal dissociation of when the central result of multisensory enhancements occurs in time would discriminate better early sensory processing-related effects versus later decision-related modulations.</p></disp-quote><p>Thank you for this valuable feedback. Regarding the first point, we used a shorter time axis in Fig. 2j-k to highlight how the presence of visual cues accelerates the decoding process. This visualization choice was intended to emphasize the early differences in processing speed. For the second point, we have carefully analyzed multisensory integration across different temporal windows. The results presented in the Supplementary Fig. 5 (also see above) already address the late phase, where our data show no evidence of multisensory enhancement for any auditory-visual pairings. This distinction helps clarify that the observed multisensory effects are primarily related to early sensory processing rather than later decision-related modulations. We hope this addresses the concerns raised and appreciate the opportunity to clarify these points.</p><disp-quote content-type="editor-comment"><p>In the abstract, the authors mention &quot;a unique integration model&quot;, &quot;selective multisensory enhancement for specific auditory-visual pairings&quot;, and &quot;using this distinct integrative mechanisms&quot;. I would strongly recommend that the authors try to phrase their results more concretely, which I believe would benefit many readers, i.e. selective how (which neurons) and specific for which pairings?</p></disp-quote><p>We appreciate the reviewer’s suggestion to clarify our phrasing for better accessibility. To address this, we have revised the relevant sentence in the abstract as follows:</p><p>&quot;This model employed selective multisensory enhancement for the auditory-visual pairing guiding the contralateral choice, which correlated with improved multisensory discrimination.&quot;</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary</p><p>In this study, rats were trained to discriminate auditory frequency and visual form/orientation for both unisensory and coherently presented AV stimuli. Recordings were made in the auditory cortex during behaviour and compared to those obtained in various control animals/conditions. The central finding is that AC neurons preferentially represent the contralateral-conditioned stimulus - for the main animal cohort this was a 10k tone and a vertically oriented bar. Over 1/3rd of neurons in AC were either AV/V/A+V and while a variety of multisensory neurons were recorded, the dominant response was excitation by the correctly oriented visual stimulus (interestingly this preference was absent in the visual-only neurons). Animals performing a simple version of the task in which responses were contingent on the presence of a stimulus rather than its identity showed a smaller proportion of AV stimuli and did not exhibit a preference for contralateral conditioned stimuli. The contralateral conditioned dominance was substantially less under anesthesia in the trained animals and was present in a cohort of animals trained with the reverse left/right contingency. Population decoding showed that visual cues did not increase the performance of the decoder but accelerated the rate at which it saturated. Rats trained on auditory and then visual stimuli (rather than simultaneously with A/V/AV) showed many fewer integrative neurons.</p><p>Strengths</p><p>There is a lot that I like about this paper - the study is well-powered with multiple groups (free choice, reversed contingency, unisensory trained, anesthesia) which provides a lot of strength to their conclusions and there are many interesting details within the paper itself. Surprisingly few studies have attempted to address whether multisensory responses in the unisensory cortex contribute to behaviour - and the main one that attempted to address this question (Lemus et al., 2010, uncited by this study) showed that while present in AC, somatosensory responses did not appear to contribute to perception. The present manuscript suggests otherwise and critically does so in the context of a task in which animals exhibit a multisensory advantage (this was lacking in Lemus et al.,). The behaviour is robust, with AV stimuli eliciting superior performance to either auditory or visual unisensory stimuli (visual were slightly worse than auditory but both were well above chance).</p></disp-quote><p>We thank the reviewer for their positive evaluation of our study.</p><disp-quote content-type="editor-comment"><p>Weaknesses</p><p>I have a number of points that in my opinion require clarification and I have suggestions for ways in which the paper could be strengthened. In addition to these points, I admit to being slightly baffled by the response latencies; while I am not an expert in the rat, usually in the early sensory cortex auditory responses are significantly faster than visual ones (mirroring the relative first spike latencies of A1 and V1 and the different transduction mechanisms in the cochlea and retina). Yet here, the latencies look identical - if I draw a line down the pdf on the population level responses the peak of the visual and auditory is indistinguishable. This makes me wonder whether these are not sensory responses - yet, they look sensory (very tightly stimulus-locked). Are these latencies a consequence of this being AuD and not A1, or ... ? Have the authors performed movement-triggered analysis to illustrate that these responses are not related to movement out of the central port, or is it possible that both sounds and visual stimuli elicit characteristic whisking movements? Lastly, has the latency of the signals been measured i.e. you generate and play them out synchronously, but is it possible that there is a delay on the audio channel introduced by the amp, which in turn makes it appear as if the neural signals are synchronous? If the latter were the case I wouldn't see it as a problem as many studies use a temporal offset in order to give the best chance of aligning signals in the brain, but this is such an obvious difference from what we would expect in other species that it requires some sort of explanation.</p></disp-quote><p>Thank you for your insightful comments. I appreciate the opportunity to clarify these points and strengthen our manuscript. Below, I address your concerns in detail:</p><p>We agree that auditory responses are typically faster than visual responses due to the distinct transduction mechanisms. However, in our experiment, we intentionally designed the stimulus setup to elicit auditory and visual responses within a similar time window to maximize the potential for multisensory integration. Specifically, we used pure tone sounds with a 15 ms ramp and visual stimuli generated by an LED array, which produce faster responses compared to mostly used light bars shown on a screen (see Supplementary Fig. 2a). The long ramp of the auditory stimulus slightly delayed auditory response onset, while the LED-generated bar (compared to the bar shown on the screen) elicited visual responses more quickly. This alignment likely facilitated the observed overlap in response latencies.</p><p>Neurons’ strong spontaneous activity in freely moving animals complicates the measurement of first spike latencies. Despite that, we still can infer the latency from robust cue-evoked responses. Supplementary Fig. 2b illustrates responses from an exemplar neuron (the same neuron as shown in Fig. 2c), where the auditory response begins 9 ms earlier than the visual response. Given the 28 ms auditory response latency observed here using 15 ms-ramp auditory stimulus, this value is consistent with prior studies in the primary auditory cortex usually using 5 ms ramp pure tones, where latencies typically range from 7 to 28 ms. Across the population (n=559), auditory responses consistently reached 0.5 of the mean Z-scored response 15 ms earlier than visual responses (Supplementary Fig. 2c). The use of Gaussian smoothing in PSTHs supports the reliability of using the 0.5 threshold as an onset latency marker. We cited Supplementary Fig. 2 in the revised manuscript within the Results section (also see the following):</p><p>“This suggests multisensory discrimination training enhances visual representation in the auditory cortex. To optimize the alignment of auditory and visual responses and reveal the greatest potential for multisensory integration, we used long-ramp pure tone auditory stimuli and quick LED-array-elicited visual stimuli (Supplementary Fig. 2). While auditory responses were still slightly earlier than visual responses, the temporal alignment was sufficient to support robust integration.”</p><p>We measured the time at which rats left the central port and confirmed that these times occur significantly later than the neuronal responses analyzed (see Fig. 1c-d). While we acknowledge the potential influence of movements such as whiskering, facial movements, head direction changes, or body movements on neuronal responses, precise monitoring of these behaviors in freely moving animals remains a technical challenge. However, given the tightly stimulus-locked nature of the neuronal responses observed, we believe they primarily reflect sensory processing rather than movement-related activity.</p><p>To ensure accurate synchronization of auditory and visual stimuli, we verified the latencies of our signals. The auditory and visual stimuli were generated and played out synchronously with no intentional delay introduced. The auditory amplifier used in our setup introduces minimal latency, and any such delay would have been accounted for during calibration. Importantly, even if a small delay existed, it would not undermine our findings, as many studies intentionally use temporal offsets to facilitate alignment of neural signals. Nonetheless, the temporal overlap observed here is primarily a result of our experimental design aimed at promoting multisensory integration.</p><p>We hope these clarifications address your concerns and highlight the robustness of our findings.</p><disp-quote content-type="editor-comment"><p>Reaction times were faster in the AV condition - it would be of interest to know whether this acceleration is sufficient to violate a race model, given the arbitrary pairing of these stimuli. This would give some insight into whether the animals are really integrating the sensory information. It would also be good to clarify whether the reaction time is the time taken to leave the center port or respond at the peripheral one.</p></disp-quote><p>We appreciate your request for clarification. In our analysis, reaction time (RT) is defined as the time taken for the animal to leave the center port after cue onset. This measure was chosen because it reflects the initial decision-making process and the integration of sensory information leading to action initiation. The time taken to respond at the peripheral port, commonly referred to as movement time, was not included in our RT measure. However, movement time data is available in our dataset, and we are open to further analysis if deemed necessary.</p><p>To determine whether the observed acceleration in RTs in the audiovisual (AV) condition reflects true multisensory integration rather than statistical facilitation, we tested for violations of the race model inequality (Miller, 1982). This approach establishes a bound for the probability of a response occurring within a given time interval under the assumption that the auditory (A) and visual (V) modalities operate independently. Specifically, we calculated cumulative distribution functions (CDFs) for the RTs in the A, V, and AV conditions (please see Author response image 1). In some rats, the AV_RTs exceeded the race model prediction at multiple time points, suggesting that the observed acceleration is not merely due to statistical facilitation but reflects true multisensory integration. Examples of these violations are shown in Panels a-b of the following figure. However, in other rats, the AV_RTs did not exceed the race model prediction, as illustrated in Author response image 1c-d.</p><p>This variability may be attributed to task-specific factors in our experimental design. For instance, the rats were not under time pressure to respond immediately after cue onset, as the task emphasized accuracy over speed. This lack of urgency may have influenced their behavioral responses and movement patterns. The race model is typically applied to assess multisensory integration in tasks where rapid responses are critical, often under conditions that incentivize speed (e.g., time-restricted tasks). In our study, the absence of strict temporal constraints may have reduced the likelihood of observing consistent violations of the race model. Furthermore, In our multisensory discrimination task, animals should discriminate multiple cues and make a behavioral choice have introduced additional variability in the degree of integration observed across individual animals. Additionally, factors such as a decline in thirst levels and physical performance as the task progressed may have significantly contributed to the variability in our results. These considerations are important for contextualizing the race model findings and interpreting the data within the framework of our experimental design.</p><fig id="sa4fig1" position="float"><label>Author response image 1.</label><caption><title>Reaction time cumulative distribution functions (CDFs) and race model evaluation.</title><p>(a) CDFs of reaction times (RTs) for auditory (blue), visual (green), and audiovisual stimuli (red) during the multisensory discrimination task. The summed CDF of the auditory and visual conditions (dashed purple, CDF_Miller) represents the race model prediction under independent sensory processing. The dashed yellow line represents the CDF of reaction times predicted by the race model. According to the race model inequality, the CDF for audiovisual stimuli (CDF_AV) should always lie below or to the right of the sum of CDF_A and CDF_V. In this example, the inequality is violated at nearly t = 200 ms, where CDF_AV is above CDF_Miller. (b) Data from another animal, showing similar results. (c, d) CDFs of reaction times for two other animals. In these cases, the CDFs follow the race model inequality, with CDF_AV consistently lying below or to the right of CDF_A + CDF_V.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-sa4-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>The manuscript is very vague about the origin or responses - are these in AuD, A1, AuV... ? Some attempts to separate out responses if possible by laminar depth and certainly by field are necessary. It is known from other species that multisensory responses are more numerous, and show greater behavioural modulation in non-primary areas (e.g. Atilgan et al., 2018).</p></disp-quote><p>Thank you for highlighting the importance of specifying the origin of the recorded responses. In the manuscript, we have detailed the implantation process in both the Methods and Results sections, indicating that the tetrode array was targeted to the primary auditory cortex. Using a micromanipulator (RWD, Shenzhen, China), the tetrode array was precisely positioned at stereotaxic coordinates 3.5–5.5 mm posterior to bregma and 6.4 mm lateral to the midline, and advanced to a depth of approximately 2–2.8 mm from the brain surface, corresponding to the primary auditory cortex. Although our recordings were aimed at A1, it is likely that some neurons from AuD and/or AuV were also included due to the anatomical proximity.</p><p>In fact, in our unpublished data collected from AuD, we observed that over 50% of neurons responded to or were modulated by visual cues, consistent with findings from many other studies. This suggests that visual representations are more pronounced in AuD compared to A1. However, as noted in the manuscript, our primary focus was on A1, where we observed relatively fewer visual or audiovisual modulations in untrained rats.</p><p>Regarding laminar depth, we regret that we were unable to determine the specific laminar layers of the recorded neurons in this study, a limitation primarily due to the constraints of our recording setup.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>The manuscript by Chang et al. aims to investigate how the behavioral relevance of auditory and visual stimuli influences the way in which the primary auditory cortex encodes auditory, visual, and audiovisual information. The main result is that behavioral training induces an increase in the encoding of auditory and visual information and in multisensory enhancement that is mainly related to the choice located contralaterally with respect to the recorded hemisphere.</p><p>Strengths:</p><p>The manuscript reports the results of an elegant and well-planned experiment meant to investigate if the auditory cortex encodes visual information and how learning shapes visual responsiveness in the auditory cortex. Analyses are typically well done and properly address the questions raised.</p></disp-quote><p>We sincerely thank the reviewer for their thoughtful and positive evaluation of our study.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>Major</p><p>(1) The authors apparently primarily focus their analyses of sensory-evoked responses in approximately the first 100 ms following stimulus onset. Even if I could not find an indication of which precise temporal range the authors used for analysis in the manuscript, this is the range where sensory-evoked responses are shown to occur in the manuscript figures. While this is a reasonable range for auditory evoked responses, the same cannot be said for visual responses, which commonly peak around 100-120 ms, in V1. In fact, the latency and overall shape of visual responses are quite different from typical visual responses, that are commonly shown to display a delay of up to 100 ms with respect to auditory responses. All traces that the authors show, instead, display visual responses strikingly overlapping with auditory ones, which is not in line with what one would expect based on our physiological understanding of cortical visually-evoked responses. Similarly, the fact that the onset of decoding accuracy (Figure 2j) anticipates during multisensory compared to auditory-only trials is hard to reconcile with the fact that visual responses have a later onset latency compared to auditory ones. The authors thus need to provide unequivocal evidence that the results they observe are truly visual in origin. This is especially important in view of the ever-growing literature showing that sensory cortices encode signals representing spontaneous motor actions, but also other forms of non-sensory information that can be taken prima facie to be of sensory origin. This is a problem that only now we realize has affected a lot of early literature, especially - but not only - in the field of multisensory processing. It is thus imperative that the authors provide evidence supporting the true visual nature of the activity reported during auditory and multisensory conditions, in both trained, free-choice, and anesthetized conditions. This could for example be achieved causally (e.g. via optogenetics) to provide the strongest evidence about the visual nature of the reported results, but it's up to the authors to identify a viable solution. This also applies to the enhancement of matched stimuli, that could potentially be explained in terms of spontaneous motor activity and/or pre-motor influences. In the absence of this evidence, I would discourage the author from drawing any conclusion about the visual nature of the observed activity in the auditory cortex.</p></disp-quote><p>We thank the reviewers for highlighting the critical issue of validating the sensory origin of the reported responses, particularly regarding the timing of visual responses and the potential confound of motor-related activity.</p><p>We analyzed neural responses within the first 150 ms following cue onset, as stated in the manuscript. This temporal window encompasses the peak of visual responses. The responses to visual stimuli occur predominantly within the first 100 ms after cue onset, preceding the initiation of body movements in behavioral tasks. This temporal dissociation aligns with previous studies, which demonstrate that motor-related activity in sensory cortices generally emerges later and is often associated with auditory rather than visual stimuli</p><p>We acknowledge that auditory responses are typically faster than visual responses due to distinct transduction mechanisms. However, in our experiment, we intentionally designed the stimulus setup to elicit auditory and visual responses within a similar time window to maximize the potential for multisensory integration. Specifically, we used pure tone sounds with a 15 ms ramp and visual stimuli generated by an LED array, which produce faster responses compared to commonly used light bars shown on a screen. The long ramp of the auditory stimulus slightly delayed auditory response onset, while the LED-generated bar elicited visual responses more quickly (Supplementary Fig. 2). This alignment facilitated the observed overlap in response latencies. As we measured in neurons with robust visual response, first spike latencies is approximately 40 ms, as exemplified by a neuron with a low spontaneous firing rate and a strong, stimulus-evoked response (Supplementary Fig. 4). Across the population (n = 559 neurons), auditory responses reached 0.5 of the mean Z-scored response 15 ms earlier than visual responses on average (Supplementary Fig. 2). We cited Supplementary Fig. 4 in the Results section as follows:</p><p>“Regarding the visual modality, 41% (80/196) of visually-responsive neurons showed a significant visual preference (Fig. 2f). The visual responses observed within the 0–150 ms window after cue onset were consistent and unlikely to result from visually evoked movement-related activity. This conclusion is supported by the early timing of the response (Fig. 2e) and exemplified by a neuron with a low spontaneous firing rate and a robust, stimulus-evoked response (Supplementary Fig. 4).”</p><p>We acknowledge the growing body of literature suggesting that sensory cortices can encode signals related to motor actions or non-sensory factors. To address this concern, we emphasize that visual responses were present not only during behavioral tasks but also in anesthetized conditions, where motor-related signals are absent. Additionally, movement-evoked responses tend to be stereotyped and non-discriminative. In contrast, the visual responses observed in our study were highly consistent and selective to visual cue properties, further supporting their sensory origin.</p><p>In summary, the combination of anesthetized and behavioral recordings, the temporal profile of responses, and their discriminative nature strongly support the sensory (visual) origin of the observed activity within the early response period. While the current study provides strong temporal and experimental evidence for the sensory origin of the visual responses, we agree that causal approaches, such as optogenetic silencing of visual input, could provide even stronger validation. Future work will explore these methods to further dissect the visual contributions to auditory cortical activity.</p><disp-quote content-type="editor-comment"><p>(2) The finding that AC neurons in trained mice preferentially respond - and enhance - auditory and visual responses pertaining to the contralateral choice is interesting, but the study does not show evidence for the functional relevance of this phenomenon. As has become more and more evident over the past few years (see e.g. the literature on mouse PPC), correlated neural activity is not an indication of functional role. Therefore, in the absence of causal evidence, the functional role of the reported AC correlates should not be overstated by the authors. My opinion is that, starting from the title, the authors need to much more carefully discuss the implications of their findings.</p></disp-quote><p>We fully agree that correlational data alone cannot establish causality. In light of your suggestion, we will revise the manuscript to more carefully discuss the implications of our findings, acknowledging that the preferred responses observed in AC neurons, particularly in relation to the contralateral choice, are correlational. We have updated several sentences in the manuscript to avoid overstating the functional relevance of these observations. Below are the revisions we have made:</p><p>Abstract section</p><p>&quot;Importantly, many audiovisual neurons in the AC exhibited experience-dependent associations between their visual and auditory preferences, displaying a unique integration model. This model employed selective multisensory enhancement for the auditory-visual pairing guiding the contralateral choice, which correlated with improved multisensory discrimination.&quot;</p><p>(Page 8, fourth paragraph in Results Section)</p><p>&quot;This aligns with findings that neurons in the AC and medial prefrontal cortex selectively preferred the tone associated with the behavioral choice contralateral to the recorded cortices during sound discrimination tasks, potentially reflecting the formation of sound-to-action associations. However, this preference represents a neural correlate, and further work is required to establish its causal link to behavioral choices.&quot;</p><p>(rewrite 3rd paragraph in Discussion Section)</p><p>&quot;Consistent with prior research(10,31), most AC neurons exhibited a selective preference for cues associated with contralateral choices, regardless of the sensory modality. This suggests that AC neurons may contribute to linking sensory inputs with decision-making, although their causal role remains to be examined. &quot;</p><p>&quot;These results indicate that multisensory training could drive the formation of specialized neural circuits within the auditory cortex, facilitating integrated processing of related auditory and visual information. However, further causal studies are required to confirm this hypothesis and to determine whether the auditory cortex is the primary site of these circuit modifications.&quot;</p><disp-quote content-type="editor-comment"><p>MINOR:</p><p>(1) The manuscript is lacking what pertains to the revised interpretation of most studies about audiovisual interactions in primary sensory cortices following the recent studies revealing that most of what was considered to be crossmodal actually reflects motor aspects. In particular, recent evidence suggests that sensory-induced spontaneous motor responses may have a surprisingly fast latency (within 40 ms; Clayton et al. 2024). Such responses might also underlie the contralaterally-tuned responses observed by the authors if one assumes that mice learn a stereotypical response that is primed by the upcoming goal-directed, learned response. Given that a full exploration of this issue would require high-speed tracking of orofacial and body motions, the authors should at least revise the discussion and the possible interpretation of their results not just on the basis of the literature, but after carefully revising the literature in view of the most recent findings, that challenge earlier interpretations of experimental results.</p></disp-quote><p>Thank you for pointing out this important consideration. We have revised the discussion (paragraph 8-9) as follows:</p><p>“There is ongoing debate about whether cross-sensory responses in sensory cortices predominantly reflect sensory inputs or are influenced by behavioral factors, such as cue-induced body movements. A recent study shows that sound-clip evoked activity in visual cortex have a behavioral rather than sensory origin and is related to stereotyped movements(48). Several studies have demonstrated sensory neurons can encode signals associated with whisking(49), running(50), pupil dilation 510 and other movements(52). In our study, the responses to visual stimuli in the auditory cortex occurred primarily within a 100 ms window following cue onset. This early timing suggests that the observed responses likely reflect direct sensory inputs, rather than being modulated by visually-evoked body or orofacial movements, which typically occur with a delay relative to sensory cue onset(53).</p><p>A recent study by Clayton et al. (2024) demonstrated that sensory stimuli can evoke rapid motor responses, such as facial twitches, within 50 ms, mediated by subcortical pathways and modulated by descending corticofugal input(56). These motor responses provide a sensitive behavioral index of auditory processing. Although Clayton et al. did not observe visually evoked facial movements, it is plausible that visually driven motor activity occurs more frequently in freely moving animals compared to head-fixed conditions. In goal-directed tasks, such rapid motor responses might contribute to the contralaterally tuned responses observed in our study, potentially reflecting preparatory motor behaviors associated with learned responses. Consequently, some of the audiovisual integration observed in the auditory cortex may represent a combination of multisensory processing and preparatory motor activity. Comprehensive investigation of these motor influences would require high-speed tracking of orofacial and body movements. Therefore, our findings should be interpreted with this consideration in mind. Future studies should aim to systematically monitor and control eye, orofacial, and body movements to disentangle sensory-driven responses from motor-related contributions, enhancing our understanding of motor planning’s role in multisensory integration.”</p><disp-quote content-type="editor-comment"><p>(2) The methods section is a bit lacking in details. For instance, information about the temporal window of analysis for sensory-evoked responses is lacking. Another example: for the spike sorting procedure, limited details are given about inclusion/exclusion criteria. This makes it hard to navigate the manuscript and fully understand the experimental paradigm. I would recommend critically revising and expanding the methods section.</p></disp-quote><p>Thank you for raising this point. We clarified the temporal window by including additional details in the methods section, even though this information was already mentioned in the results section. Specifically, we now state:</p><p>(Neural recordings and Analysis in methods section)</p><p>“...These neural signals, along with trace signals representing the stimuli and session performance information, were transmitted to a PC for online observation and data storage. Neural responses were analyzed within a 0-150ms temporal window after cue onset, as this period was identified as containing the main cue-evoked responses for most neurons. This time window was selected based on the consistent and robust neural activity observed during this period.”</p><p>We appreciate your concern regarding spike sorting procedure. To address this, we have expanded the methods section to provide more detailed information about the quality of our single-unit recordings. we have added detailed information in the text, as shown below (Analysis of electrophysiological data in methods section):</p><p>“Initially, the recorded raw neural signals were band-pass filtered in the range of 300-6000 Hz to eliminate field potentials. A threshold criterion, set at no less than three times the standard deviation (SD) above the background noise, was applied to automatically identify spike peaks. The detected spike waveforms were then subjected to clustering using template-matching and built-in principal component analysis tool in a three-dimensional feature space. Manual curation was conducted to refine the sorting process. Each putative single unit was evaluated based on its waveform and firing patterns over time. Waveforms with inter-spike intervals of less than 2.0 ms were excluded from further analysis. Spike trains corresponding to an individual unit were aligned to the onset of the stimulus and grouped based on different cue and choice conditions. Units were included in further analysis only if their presence was stable throughout the session, and their mean firing rate exceeded 2 Hz. The reliability of auditory and visual responses for each unit was assessed, with well-isolated units typically showing the highest response reliability.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>(1) Some of the ordering of content in the introduction could be improved. E.g. line 49 reflects statements about the importance of sensory experience, which is the topic of the subsequent paragraph. In the discussion, line 436, there is a discussion of the same findings as line 442. These two paragraphs in general appear to discuss similar content. Similarly, the paragraph starting at line 424 and at line 451 both discuss the plasticity of multisensory responses through audiovisual experience, as well as the paragraph starting at line 475 (but now audiovisual pairing is dubbed semantic). In the discussion of how congruency/experience shapes multisensory interactions, the authors should relate their findings to those of Meijer et al. 2017 and Garner and Keller 2022 (visual cortex) about enhanced and suppressed responses and their potential role (as well as other literature such as Banks et al. 2011 in AC).</p></disp-quote><p>We thank the reviewer for their detailed observations and valuable recommendations to improve the manuscript's organization. Below, we address each point:</p><p>We deleted the sentence, &quot;Sensory experience has been shown to shape cross-modal presentations in sensory cortices&quot; (Line 49), as the subsequent paragraph discusses sensory experience in detail.</p><p>To avoid repetition, we removed the sentence, &quot;This suggests that multisensory training enhances AC's ability to process visual information&quot; (Lines 442–443).</p><p>Regarding the paragraph starting at Line 475, we believe its current form is appropriate, as it focuses on the influence of semantic congruence on multisensory integration, which differs from the topics discussed in the other paragraphs.</p><p>We have cited the three papers suggested by the reviewer in the appropriate sections of the manuscript.</p><p>(Paragraph 6 in discussion section)</p><p>“…A study conducted on the gustatory cortex of alert rats has shown that cross-modal associative learning was linked to a dramatic increase in the prevalence of neurons responding to nongustatory stimuli (24). Moreover, in the primary visual cortex, experience-dependent interactions can arise from learned sequential associations between auditory and visual stimuli, mediated by corticocortical connections rather than simultaneous audiovisual presentations (26).”</p><p>(Paragraph 2 in discussion section)</p><p>“...Meijer et al. reported that congruent audiovisual stimuli evoke balanced enhancement and suppression in V1, while incongruent stimuli predominantly lead to suppression(6), mirroring our findings in AC, where multisensory integration was dependent on stimulus feature…”</p><p>(Paragraph 2 in introduction section)</p><p>“...Anatomical investigations reveal reciprocal nerve projections between auditory and visual cortices(4,11-15), highlighting the interconnected nature of these sensory systems. Moreover, two-photon calcium imaging in awake mice has shown that audiovisual encoding in the primary visual cortex depends on the temporal congruency of stimuli, with temporally congruent audiovisual stimuli eliciting balanced enhancement and suppression, whereas incongruent stimuli predominantly result in suppression(6).”</p><disp-quote content-type="editor-comment"><p>(2) The finding of purely visually responsive neurons in the auditory cortex that moreover discriminate the stimuli is surprising given previous results (Iurilli et al. 2012, Morrill and Hasenstaub 2018 (only L6), Oude Lohuis et al. 2024, Atilgan et al. 2018, Chou et al. 2020). Reporting the latency of this response is interesting information about the potential pathways by which this information could reach the auditory system. Furthermore, spike isolation quality and histological verification are described in little detail. It is crucial for statements about the auditory, visual, or audiovisual response of individual neurons to substantiate the confidence level about the quality of single-unit recordings and where they were recorded. Do the authors have data to support that visual and audiovisual responses were not restricted to posteromedial tetrodes or clusters with poor quality? A discussion of finding V-responsive units in AC with respect to literature is warranted. Furthermore, the finding that also in visual trials behaviorally relevant information about the visual cue (with a bias for the contralateral choice cue) is sent to the AC is pivotal in the interpretation of the results, which as far as I note not really considered that much.</p></disp-quote><p>We appreciate the reviewer’s thoughtful comments and have addressed them as follows:</p><p>Discussion of finding choice-related V-responsive units in AC with respect to literature and potential pathways</p><p>3rd paragraph in the Discussion section</p><p>“Consistent with prior research(10,31), most AC neurons exhibited a selective preference for cues associated with contralateral choices, regardless of the sensory modality. This suggests that AC neurons may contribute to linking sensory inputs with decision-making, although their causal role remains to be examined. Associative learning may drive the formation of new connections between sensory and motor areas of the brain, such as cortico-cortical pathways(35). Notably, this cue-preference biasing was absent in the free-choice group. A similar bias was also reported in a previous study, where auditory discrimination learning selectively potentiated corticostriatal synapses from neurons representing either high or low frequencies associated with contralateral choices(32)…”</p><p>6th paragraph in the Discussion section</p><p>“Our results extend prior finding(4,47), showing that visual input not only reaches the AC but can also drive discriminative responses, particularly during task engagement. This task-specific plasticity enhances cross-modal integration, as demonstrated in other sensory systems. For example, calcium imaging studies in mice showed that a subset of multimodal neurons in visual cortex develops enhanced auditory responses to the paired auditory stimulus following coincident auditory–visual experience(25)…”</p><p>8th paragraph in the Discussion section</p><p>“…In our study, the responses to visual stimuli in the auditory cortex occurred primarily within a 100 ms window following cue onset, suggesting that visual information reaches the AC through rapid pathways. Potential candidates include direct or fast cross-modal inputs, such as pulvinar-mediated pathways(8) or corticocortical connections(5，54), rather than slower associative mechanisms. This early timing indicates that the observed responses were less likely modulated by visually-evoked body or orofacial movements, which typically occur with a delay relative to sensory cue onset(55).”</p><p>Response Latency</p><p>Regarding the latency of visually driven responses, we have included this information in our response to the second reviewer’s first weakness (please see the above). Briefly, we analyzed neural responses within a 0-150ms temporal window after cue onset, as this period captures the most consistent and robust cue-evoked responses across neurons.</p><p>Purely Visually Responsive Neurons in A1</p><p>We agree that the finding of visually responsive neurons in the auditory cortex may initially seem surprising. However, these neurons might not have been sensitive to target auditory cues in our task but could still respond to other sound types. Cortical neurons are known to exhibit significant plasticity during the cue discrimination tasks, as well as during passive sensory exposure. Thus, the presence of visually responsive neurons is not inconsistent with prior findings but highlights task-specific sensory tuning. We confirm that responses were not restricted to posteromedial tetrodes or low-quality clusters (see an example of a robust visually responsive neuron in supplementary Fig. 4). Histological analysis verified electrode placements across the auditory cortex.</p><p>For spike sorting, we have added detailed information in the text, as shown below:</p><p>“Initially, the recorded raw neural signals were band-pass filtered in the range of 300-6000 Hz to eliminate field potentials. A threshold criterion, set at no less than three times the standard deviation (SD) above the background noise, was applied to automatically identify spike peaks. The detected spike waveforms were then subjected to clustering using template-matching and built-in principal component analysis tool in a three-dimensional feature space. Manual curation was conducted to refine the sorting process. Each putative single unit was evaluated based on its waveform and firing patterns over time. Waveforms with inter-spike intervals of less than 2.0 ms were excluded from further analysis. Spike trains corresponding to an individual unit were aligned to the onset of the stimulus and grouped based on different cue and choice conditions. Units were included in further analysis only if their presence was stable throughout the session, and their mean firing rate exceeded 2 Hz. The reliability of auditory and visual responses for each unit was assessed, with well-isolated units typically showing the highest response reliability.”</p><disp-quote content-type="editor-comment"><p>(3) In the abstract it seems that in &quot;Additionally, AC neurons...&quot; the connective word 'additionally' is misleading as it is mainly a rephrasing of the previous statement.</p></disp-quote><p>Replaced &quot;Additionally&quot; with &quot;Furthermore&quot; to better signal elaboration and continuity.</p><disp-quote content-type="editor-comment"><p>(4) The experiments included multisensory conflict trials - incongruent audiovisual stimuli. What was the behavior for these trials given multiple interesting studies on the neural correlates of sensory dominance (Song et al. 2017, Coen et al. 2023, Oude Lohuis et al. 2024).</p></disp-quote><p>We appreciate your feedback and have addressed it by including a new figure (supplemental Fig. 8) that illustrates choice selection during incongruent audiovisual stimuli. Panel (a) shows that rats displayed confusion when exposed to mismatched stimuli, resulting in choice patterns that differed from those observed in panel (b), where consistent audiovisual stimuli were presented. To provide clarity and integrate this new figure effectively into the manuscript, we updated the results section as follows:</p><p>“...Rats received water rewards with a 50% chance in either port when an unmatched multisensory cue was triggered. Behavioral analysis revealed that Rats displayed notable confusion in response to unmatched multisensory cues, as evidenced by their inconsistent choice patterns (supplementary Fig. 8).”</p><disp-quote content-type="editor-comment"><p>(5) Line 47: The AC does not 'perceive' sound frequency, individual brain regions are not thought to perceive.</p></disp-quote><p>e appreciate the reviewer’s observation and have revised the sentence to ensure scientific accuracy. The updated sentence in the second paragraph of the Introduction now reads:</p><p>“Even irrelevant visual cues can affect sound discrimination in AC<sup>10</sup>.”</p><disp-quote content-type="editor-comment"><p>(6) Line 59-63: The three questions are not completely clear to me. Both what they mean exactly and how they are different. E.g. Line 60: without specification, it is hard to understand which 'strategies' are meant by the &quot;same or different strategies&quot;? And Line 61: What is meant by the quotation marks for match and mismatch? I assume this is referring to learned congruency and incongruency, which appears almost the same question as number 3 (how learning affects the cortical representation).</p></disp-quote><p>We have revised the three questions for improved clarity and distinction as follows:</p><p>“This limits our understanding of multisensory integration in sensory cortices, particularly regarding: (1) Do neurons in sensory cortices adopt consistent integration strategies across different audiovisual pairings, or do these strategies vary depending on the pairing? (2) How does multisensory perceptual learning reshape cortical representations of audiovisual objects? (3) How does the congruence between auditory and visual features—whether they &quot;match&quot; or &quot;mismatch&quot; based on learned associations—impact neural integration?”</p><disp-quote content-type="editor-comment"><p>(7) Is the data in Figures 1c and d only hits?</p></disp-quote><p>Only correct trials are included. We add this information in the figure legend. Please see Fig. 1 legend. Also, please see below</p><p>“c Cumulative frequency distribution of reaction time (time from cue onset to leaving the central port) for one representative rat in auditory, visual and multisensory trials (correct only). d Comparison of average reaction times across rats in auditory, visual, and multisensory trials (correct only).”</p><disp-quote content-type="editor-comment"><p>(8) Figure S1b: Preferred frequency is binned in non-equidistant bins, neither linear nor logarithmic. It is unclear what the reason is.</p></disp-quote><p>The edges of the bins for the preferred frequency were determined based on a 0.5-octave increment, starting from the smallest boundary of 8 kHz. Specifically, the bin edges were calculated as follows:</p><p>8×2<sup>0.5</sup>=11.3 kHz;</p><p>8×2<sup>1</sup>=16 kHz;</p><p>8×2<sup>1.5</sup>=22.6 kHz;</p><p>8×2<sup>2</sup>=32 kHz;</p><p>This approach reflects the common practice of using changes in octaves to define differences between pure tone frequencies, as it aligns with the logarithmic perception of sound frequency in auditory neuroscience.</p><disp-quote content-type="editor-comment"><p>(9) Figure S1d: why are the responses all most neurons very strongly correlated given the frequency tuning of A1 neurons? Further, the mean normalized response presented in Figure S2e does seem to indicate a stronger response for 10kHz tones than 3kHz, in conflict with the data from anesthetized rats presented in Figure S2e.</p></disp-quote><p>There is no discrepancy in the data. In Figure S1d, we compared neuronal responses to 10 kHz and 3 kHz tones, demonstrating that most neurons responded well to both frequencies. This panel does not aim to illustrate frequency selectivity but rather the overall responsiveness of neurons to these tones. For detailed information on sound selectivity, readers can refer to Figures S3a-b, which show that while more neurons preferred 10 kHz tones, the proportion is lower than in neurons recorded during the multisensory discrimination task. This distinction explains the observed differences and aligns with the results presented.</p><disp-quote content-type="editor-comment"><p>(10) Line 79: For clarity, it can be added that the multisensory trials presented are congruent trials (jointly indicated rewarded port), and perhaps that incongruent trials are discussed later in the paper.</p></disp-quote><p>We believe additional clarification is unnecessary, as the designations &quot;A<sup>3k</sup>V<sup>hz</sup>&quot; and &quot;A<sup>10k</sup>V<sup>vt</sup>&quot; clearly indicate the specific combinations of auditory and visual cues presented during congruent trials. Additionally, the discussion of incongruent trials is provided later in the manuscript, as noted by the reviewer.</p><disp-quote content-type="editor-comment"><p>(11) Line 111: the description leaves unclear that the 35% reflects the combination of units responsive to visual only and responsive to auditory or visual.</p></disp-quote><p>The information is clearly presented in Figure 2b, which shows the proportions of neurons responding to auditory-only (A), visual-only (V), both auditory and visual (A, V), and audiovisual-only (VA) stimuli in a pie chart. Readers can refer to this figure for a detailed breakdown of the neuronal response categories.</p><disp-quote content-type="editor-comment"><p>(12) Figure 2h: consider a colormap with diverging palette and equal positive and negative maximum (e.g. -0.6 to 0.6) and perhaps reiterate in the color bar legend which stimulus is preferred for which selectivity index.</p></disp-quote><p>We appreciate the suggestion; however, we believe that the current colormap effectively conveys the data and the intended interpretation. The existing color bar legend already provides clear information about the selectivity index, and the stimulus preference is adequately explained in the figure caption. As such, further adjustments are not necessary.</p><disp-quote content-type="editor-comment"><p>(13) Line 160: &quot;a ratio of 60:20 for V<sup>vt</sup> 160 preferred vs. V<sup>hz</sup> preferred neurons.&quot; Is this supposed to add up to 100, or is this a ratio of 3:1?</p></disp-quote><p>We rewrite the sentence. Please see below:</p><p>“Similar to the auditory selectivity observed, a greater proportion of neurons favored the visual stimulus (V<sup>vt</sup>) associated with the contralateral choice, with a 3:1 ratio of V<sup>vt</sup>-preferred to V<sup>hz</sup>-preferred neurons.”</p><disp-quote content-type="editor-comment"><p>(14) The statement in Figure 2g and line 166/167 could be supported by a statistical test (chi-square?).</p></disp-quote><p>Thank you for the suggestion. However, we believe that a statistical test is not required in this case, as the patterns observed are clearly represented in Figure 2g. The qualitative differences between the groups are evident and sufficiently supported by the data.</p><disp-quote content-type="editor-comment"><p>(15) Line 168, it is unclear in what sense 'dominant' is meant. Is audition perceived as a dominant sensory modality in a behavioral sense (e.g. Song et al. 2017), or are auditory signals the dominant sensory signal locally in the auditory cortex?</p></disp-quote><p>Thank you for the clarification. To address your question, by &quot;dominant,&quot; we are referring to the fact that auditory inputs are the most prominent and influential among the sensory signals feeding into the auditory cortex. This reflects the local dominance of auditory signals within the auditory cortex, rather than a behavioral dominance of auditory perception. We have revised the sentence as follows:</p><p>“We propose that the auditory input, which dominates within the auditory cortex, acts as a 'teaching signal' that shapes visual processing through the selective reinforcement of specific visual pathways during associative learning.”</p><disp-quote content-type="editor-comment"><p>(16) Line 180: &quot;we discriminated between auditory, visual, and multisensory cues.&quot; This phrasing indicated that the SVMs were trained to discriminate sensory modalities (as is done later in the manuscript), rather than what was done: discriminate stimuli within different categories of trials.</p></disp-quote><p>Thank you for your comment. We have revised the sentence for clarity. Please see the updated version below:</p><p>“Using cross-validated support vector machine (SVM) classifiers, we examined how this pseudo-population discriminates stimulus identity within the same modality (e.g., A<sup>3k</sup> vs. A<sup>10k</sup> for auditory stimuli, V<sup>hz</sup> vs. V<sup>vt</sup> for visual stimuli, A<sup>3k</sup>V<sup>hz</sup> vs. A<sup>10k</sup>V<sup>vt</sup> for multisensory stimuli).”</p><disp-quote content-type="editor-comment"><p>(17) Line 185: &quot;a deeply accurate incorporation of visual processing in the auditory cortex.&quot; the phrasing is a bit excessive for a binary classification performance.</p></disp-quote><p>Thank you for pointing this out. We have revised the sentence to better reflect the findings without overstating them:</p><p>“Interestingly, AC neurons could discriminate between two visual targets with around 80% accuracy (Fig. 2j), demonstrating a meaningful incorporation of visual information into auditory cortical processing.”</p><disp-quote content-type="editor-comment"><p>(18) Figure 3, title. An article is missing (a,an/the).</p></disp-quote><p>Done. Please see below:</p><p><bold>“</bold>Fig. 3 Auditory and visual integration in the multisensory discrimination task<bold>”</bold></p><disp-quote content-type="editor-comment"><p>(19) Line 209, typo pvalue: p&lt;-0.00001.</p></disp-quote><p>Done (p&lt;0.00001).</p><disp-quote content-type="editor-comment"><p>(20) Line 209, the pattern is not weaker. The pattern is the same, but more weakly expressed.</p></disp-quote><p>Thank you for your valuable feedback. We appreciate your clarification and agree that our phrasing could be improved for accuracy. The observed pattern under anesthesia is indeed the same but less strongly expressed compared to the task engagement. We have revised the sentence to better reflect this distinction:</p><p>“A similar pattern, albeit less strongly expressed, was observed under anesthesia (Supplementary Fig. 3c-3f), suggesting that multisensory perceptual learning may induce plastic changes in AC.”</p><disp-quote content-type="editor-comment"><p>(21) Line 211: choice-free group → free-choice group.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(22) Line 261: wrong → incorrect (to maintain consistent terminology).</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(23) Line 265: why 'likely'? Are incorrect choices on the A<sup>3k</sup>-V<sup>hz</sup> trials not by definition contralateral and vice versa? Or are there other ways to have incorrect trials?</p></disp-quote><p>We deleted the word of ‘likely’. Please see below:</p><p>“…, correct choices here correspond to ipsilateral behavioral selection, while incorrect choices correspond to contralateral behavioral selection.”</p><disp-quote content-type="editor-comment"><p>(24) Typo legend Fig 3a-c (tasks → task). (only one task performed).</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(25) Line 400: typo: Like → like.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(26) Line 405: What is meant by a cohesive visual stimulus? Congruent? Rephrase.</p></disp-quote><p>Done. Please see the below:</p><p>“…layer 2/3 neurons of the primary visual cortex(7), and a <bold>congruent</bold> visual stimulus can enhance sound representation…”</p><p>(27) Line 412: Very general statement and obviously true: depending on the task, different sensory elements need to be combined to guide adaptive behavior.</p><p>We really appreciate the reviewer and used this sentence (see second paragraph in discussion section).</p><disp-quote content-type="editor-comment"><p>(28) Line 428: within → between (?).</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>(29) Figure 3L is not referenced in the main text. By going through the figures and legends my understanding is that this shows that most neurons have a multisensory response that lies between 2 z-scores of the predicted response in the case of 83% of the sum of the auditory and the visual response. However, how was the 0.83 found? Empirically? Figure S3 shows a neuron that does follow a 100% summation. Perhaps the authors could quantitatively support their estimate of 83% of the A + V sum, by varying the fraction of the sum (80%, 90%, 100% etc.) and showing the distribution of the preferred fraction of the sum across neurons, or by showing the percentage of neurons that fall within 2 z-scores for each of the fractions of the sum.</p></disp-quote><p>Thank you for your detailed feedback and suggestions regarding Figure 3L and the 83% multiplier.</p><p>(1) Referencing Figure 3L:</p><p>Figure 3L is referenced in the text. To enhance clarity, we have revised the text to explicitly highlight its relevance:</p><p>“Specifically, as illustrated in Fig. 3k, the observed multisensory response approximated 83% of the sum of the auditory and visual responses in most cases, as quantified in Fig. 3L.”</p><p>(2) Determination of the 0.83 Multiplier:</p><p>The 0.83 multiplier was determined empirically by comparing observed audiovisual responses with the predicted additive responses (i.e., the sum of auditory and visual responses). For each neuron, we calculated the auditory, visual, and audiovisual responses. We then compared the observed audiovisual response with scaled sums of auditory and visual responses (Fig. 3k), expressed as fractions of the additive prediction (e.g., 0.8, 0.83, 0.9, etc.). We found that when the scaling factor was 0.83, the population-wide difference between predicted and observed multisensory responses, expressed as z-scores, was minimized. Specifically, at this value, the mean z-score across the population was approximately zero (-0.0001±1.617), indicating the smallest deviation between predicted and observed responses.</p><p>(30) Figure 5e: how come the diagonal has 0.5 decoding accuracy within a category? Shouldn't this be high within-category accuracy? If these conditions were untested and it is an issue of the image display it would be informative to test the cross-validated performance within the category as well as a benchmark to compare the across-category performance to. Aside, it is unclear which conventions from Figure 2 are meant by the statement that conventions were the same.</p><p>The diagonal values (~0.5 decoding accuracy) within each category reflect chance-level performance. This occurs because the decoder was trained and tested on the same category conditions in a cross-validated manner, and within-category stimulus discrimination was not the primary focus of our analysis. Specifically, the stimuli within a category shared overlapping features, leading to reduced discriminability for the decoder when distinguishing between them. Our primary objective was to assess cross-category performance rather than within-category accuracy, which may explain the observed pattern in the diagonal values.</p><p>Regarding the reference to Figure 2, we appreciate the reviewer pointing out the ambiguity. To avoid any confusion, we have removed the sentence referencing &quot;conventions from Figure 2&quot; in the legend for Figure 5e, as it does not contribute meaningfully to the understanding of the results.</p><disp-quote content-type="editor-comment"><p>(31) Line 473: &quot;movement evoked response&quot;, what is meant by this?</p></disp-quote><p>Thank the reviewer for highlighting this point. To clarify, by &quot;movement-evoked response,&quot; we are referring to neural activity that is driven by the animal's movements, rather than by sensory inputs. This type of response is typically stereotyped, meaning that it has a consistent, repetitive pattern associated with specific movements, such as whisking, running, or other body or facial movements.</p><p>In our study, we propose that the visually-evoked responses observed within the 150 ms time window after cue onset primarily reflect sensory inputs from the visual stimulus rather than movement-related activity. This interpretation is supported by the response timing: visual-evoked activity occurs within 100 ms of the light flash onset, a timeframe too rapid to be attributed to body or orofacial movements. Additionally, unlike stereotyped movement-evoked responses, the visual responses we observed are discriminative, varying based on specific visual features—a hallmark of sensory processing rather than motor-driven activity.</p><p>We have revised the manuscript as follows (eighth paragraph in discussion section):</p><p>“There is ongoing debate about whether cross-sensory responses in sensory cortices predominantly reflect sensory inputs or are influenced by behavioral factors, such as cue-induced body movements. A recent study shows that sound-clip evoked activity in visual cortex have a behavioral rather than sensory origin and is related to stereotyped movements(49). Several studies have demonstrated sensory neurons can encode signals associated with whisking(50), running(51), pupil dilation(52) and other movements(53). In our study, the responses to visual stimuli in the auditory cortex occurred primarily within a 100 ms window following cue onset. suggests that visual information reaches the AC through rapid pathways. Potential candidates include direct or fast cross-modal inputs, such as pulvinar-mediated pathways(8) or corticocortical connections(5，54), rather than slower associative mechanisms. This early timing suggests that the observed responses were less likely modulated by visually-evoked body or orofacial movements, which typically occur with a delay relative to sensory cue onset(55). ”</p><disp-quote content-type="editor-comment"><p>(32) Line 638-642: It is stated that a two-tailed permutation test is done. The cue selectivity can be significantly positive and negative, relative to a shuffle distribution. This is excellent. But then it is stated that if the observed ROC value exceeds the top 5% of the distribution it is deemed significant, which corresponds to a one-tailed test. How were significantly negative ROC values detected with p&lt;0.05?</p></disp-quote><p>Thank you for pointing this out. We confirm that a two-tailed permutation test was indeed used to evaluate cue selectivity. In this approach, significance is determined by comparing the observed ROC value to both tails of the shuffle distribution. Specifically, if the observed ROC value exceeds the top 2.5% or falls below the bottom 2.5% of the distribution, it is considered significant at p&lt; 0.05. This two-tailed test ensures that both significantly positive and significantly negative cue selectivity values are identified.</p><p>To clarify this in the manuscript, we have revised the text as follows:</p><p>“This generated a distribution of values from which we calculated the probability of our observed result. If the observed ROC value exceeds the top 2.5% of the distribution or falls below the bottom 2.5%, it was deemed significant (i.e., p &lt; 0.05).”</p><disp-quote content-type="editor-comment"><p>(33) Line 472: the cited paper (reference 52) actually claims that motor-related activity in the visual cortex has an onset before 100ms and thus does not support your claim that the time window precludes any confound of behaviorally mediated activity. Furthermore, that study and reference 47 show that sensory stimuli could be discriminated based on the cue-evoked body movements and are discriminative. A stronger counterargument would be that both studies show very fast auditory-evoked body movements, but only later visually-evoked body movements.</p></disp-quote><p>We appreciate the reviewer’s comments. As Lohuis et al. (reference 55) demonstrated, activity in the visual cortex (V1) can reflect distinct visual, auditory, and motor-related responses, with the latter often dissociable in timing. In their findings, visually-evoked movement-related activity arises substantially later than the sensory visual response, generally beginning around 200 ms post-stimulus onset. In contrast, auditory-evoked activity in A1 occurs relatively early.</p><p>We have revised the manuscript as follows (eighth paragraph in discussion section):</p><p>“A recent study shows that sound-clip evoked activity in visual cortex have a behavioral rather than sensory origin and is related to stereotyped movements(49). ...This early timing suggests that the observed responses were less likely modulated by visually-evoked body or orofacial movements, which typically occur with a delay relative to sensory cue onset(55). ”</p><disp-quote content-type="editor-comment"><p>(34) The training order (multisensory cue first) is important to briefly mention in the main text.</p></disp-quote><p>We appreciate the reviewer’s suggestion and have added this information to the main text. The revised text now reads:</p><p>“The training proceeded in two stages. In the first stage, which typically lasted 3-5 weeks, rats were trained to discriminate between two audiovisual cues. In the second stage, an additional four unisensory cues were introduced, training the rats to discriminate a total of six cues.”</p><disp-quote content-type="editor-comment"><p>(35) Line 542: As I understand the multisensory rats were trained using the multisensory cue first, so different from the training procedure in the unisensory task rats where auditory trials were learned first.</p></disp-quote><p>Thank you for pointing this out. You are correct that, in the unisensory task, rats were first trained to discriminate auditory cues, followed by visual cues. To improve clarity and avoid any confusion, we have removed the sentence &quot;Similar to the multisensory discrimination task&quot; from the revised text.</p><disp-quote content-type="editor-comment"><p>(36) Line 546: Can you note on how the rats were motivated to choose both ports, or whether they did so spontaneously?</p></disp-quote><p>Thank you for your insightful comment. The rats' port choice was spontaneous in this task, as there was no explicit motivation required for choosing between the ports. We have clarified this point in the text to address your concern. The revised sentence now reads:</p><p>“They received a water reward at either port following the onset of the cue, and their port choice was spontaneous.”</p><disp-quote content-type="editor-comment"><p>(37) It is important to mention in the main text that the population decoding is actually pseudopopulation decoding. The interpretation is sufficiently important for interpreting the results.</p></disp-quote><p>Thank you for this valuable suggestion. We have revised the text to specify &quot;pseudo-population&quot; instead of &quot;population&quot; to clarify the nature of our decoding analysis. The revised text now reads:</p><p>“Our multichannel recordings enabled us to decode sensory information from a pseudo-population of AC neurons on a single-trial basis. Using cross-validated support vector machine (SVM) classifiers, we examined how this pseudo-population discriminates between stimuli.”</p><disp-quote content-type="editor-comment"><p>(38) The term modality selectivity for the description of the multisensory interaction is somewhat confusing. Modality selectivity suggests different responses to the visual or auditory trials. The authors could consider a different terminology emphasizing the multisensory interaction effect.</p></disp-quote><p>Thank you for your insightful comment. We have replaced &quot; modality selectivity &quot; with &quot; multisensory interactive index &quot; (MSI). This term more accurately conveys a tendency for neurons to favor multisensory stimuli over individual sensory modalities (visual or auditory alone).</p><disp-quote content-type="editor-comment"><p>(39) In Figures 3 e and g the color code is different from adjacent panels b and c and is to be deciphered from the legend. Consider changing the color coding, or highlight to the reader that the coloring in Figures 3b and c is different from the color code in panels 3 e and g.</p></disp-quote><p>We appreciate the reviewer’s observation. However, we believe that a change in the color coding is not necessary. Figures 3e and 3g differentiate symbols by both shape and color, ensuring accessibility and clarity. This is clearly explained in the figure legend to guide readers effectively.</p><disp-quote content-type="editor-comment"><p>(40) Figure S2b: was significance tested here?</p></disp-quote><p>Yes, we did it.</p><disp-quote content-type="editor-comment"><p>(41) Figure S2d: test used?</p></disp-quote><p>Yes, test used.</p><disp-quote content-type="editor-comment"><p>(42) Line 676: &quot;as appropriate&quot;, was a normality test performed prior to statistical test selection?</p></disp-quote><p>In our analysis, we assessed normality before choosing between parametric (paired t-test) and non-parametric (Wilcoxon signed-rank test) methods. We used the Shapiro-Wilk test to evaluate the normality of the data distributions. When data met the assumption of normality, we applied the paired t-test; otherwise, we used the Wilcoxon signed-rank test.</p><p>Thank you for pointing this out. We confirm that a normality test was performed prior to the selection of the statistical test. Specifically, we used the Shapiro-Wilk test to assess whether the data distributions met the assumption of normality. Based on this assessment, we applied the paired t-test for normally distributed data and the Wilcoxon signed-rank test for non-normal data.</p><p>To ensure clarity, we update the &quot;Statistical Analysis&quot; section of the manuscript with the following revised text:</p><p>“For behavioral data, such as mean reaction time differences between unisensory and multisensory trials, cue selectivity and mean modality selectivity across different auditory-visual conditions, comparisons were performed using either the paired t-test or the Wilcoxon signed-rank test. The Shapiro-Wilk test was conducted to assess normality, with the paired t-test used for normally distributed data and the Wilcoxon signed-rank test for non-normal data.”</p><disp-quote content-type="editor-comment"><p>(43) Line 679: incorrect, most data is actually represented as mean +- SEM.</p></disp-quote><p>Thank you for pointing this out. In the Results section, we report data as mean ± SD for descriptive statistics, while in the figures, the error bars typically represent the standard error of the mean (SEM) to visually indicate variability around the mean. We have specified in each figure legend whether the error bars represent SD or SEM.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>(1) Line 182 - here it sounds like you mean your classifier was trained to decode the modality of the stimulus, when I think what you mean is that you decoded the stimulus contingencies using A/V/AV cues?</p></disp-quote><p>Thank you for pointing out this potential misunderstanding. We would like to clarify that the classifier was trained to decode the stimulus identity (e.g., A<sup>3k</sup> vs. A<sup>10k</sup> for auditory stimuli, V<sup>hz</sup> vs. V<sup>vt</sup> for visual stimuli, and A<sup>3k</sup>V<sup>hz</sup> vs. A<sup>10k</sup>V<sup>vt</sup> for multisensory stimuli) rather than the modality of the stimulus. The goal of the analysis was to determine how well the pseudo-population of AC neurons could distinguish between individual stimuli within the same modality. We have revised the relevant text in the revised manuscript to ensure this distinction is clear. Please see the following:</p><p>“Our multichannel recordings enabled us to decode sensory information from a pseudo-population of AC neurons on a single-trial basis. Using cross-validated support vector machine (SVM) classifiers, we examined how this pseudo-population discriminates stimulus identity (e.g., A<sup>3k</sup> vs. A<sup>10k</sup> for auditory stimuli, V<sup>hz</sup> vs. V<sup>vt</sup> for visual stimuli, A<sup>3k</sup>V<sup>hz</sup> vs. A<sup>10k</sup>V<sup>vt</sup> for multisensory stimuli).”</p><disp-quote content-type="editor-comment"><p>(2) Lines 256 - here the authors look to see whether incorrect trials diminish audiovisual integration. I would probably seek to turn the causal direction around and ask are AV neurons critical for behaviour - nevertheless, since this is only correlational the causal direction cannot be unpicked. However, the finding that contralateral responses per se do not result in enhancement is a key control. Showing that multisensory enhancement is less on error trials is a good first step to linking neural activity and perception, but I wonder if the authors could take this further however by seeking to decode choice probabilities as well as stimulus features in an attempt to get a little closer to addressing the question of whether the animals are using these responses for behaviour.</p></disp-quote><p>Thank you for your comment and for highlighting the importance of understanding whether audiovisual (AV) neurons are critical for behavior. As you noted, the causal relationship between AV neural activity and behavioral outcomes cannot be directly determined in our current study due to its correlational nature. We agree that this is an important topic for future exploration. In our study, we examined how incorrect trials influence multisensory enhancement. Our findings show that multisensory enhancement is less pronounced during error trials, providing an initial link between neural activity and behavioral performance. To address your suggestion, we conducted an additional analysis comparing auditory and multisensory selectivity between correct and incorrect choice trials. As shown in Supplementary Fig. 7, both auditory and multisensory selectivity were significantly lower during incorrect trials. This result highlights the potential role of these neural responses in decision-making, suggesting they may extend beyond sensory processing to influence choice selection. We have cited this figure in the Results section as follows: (the paragraph regarding Impact of incorrect choices on audiovisual integration):</p><p>“Overall, these findings suggest that the multisensory perception reflected by behavioral choices (correct vs. incorrect) might be shaped by the underlying integration strength. Furthermore, our analysis revealed that incorrect choices were associated with a decline in cue selectivity, as shown in Supplementary Fig. 7.”</p><p>We acknowledge your suggestion to decode choice probabilities alongside stimulus features as a more direct approach to exploring whether animals actively use these neural responses for behavior. Unfortunately, in the current study, the low number of incorrect trials limited our ability to perform such analyses reliably. Nonetheless, we are committed to pursuing this direction in subsequent work. We plan to use techniques such as optogenetics in future studies to causally test the role of AV neurons in driving behavior.</p><disp-quote content-type="editor-comment"><p>(3) Figure 5E - the purple and red are indistinguishable - could you make one a solid line and keep one dashed?</p></disp-quote><p>We thank the reviewer for pointing out that the purple and red lines in Figure 5E were difficult to distinguish. To address this concern, we modified the figure by making two lines solid and changing the color of one square, as suggested. These adjustments enhance visual clarity and improve the distinction between them.</p><disp-quote content-type="editor-comment"><p>(4) The unisensory control training is a really nice addition. I'm interested to know whether behaviourally these animals experienced an advantage for audiovisual stimuli in the testing phase? This is important information to include as if they don't it is one step closer to linking audiovisual responses in AC to improved behavioural performance (and if they do, we must be suitably cautious in interpretation!).</p></disp-quote><p>Thank you for raising this important point. To address this, we have plotted the behavioral results for each animal (see Author response image 2). The data indicate that performance with multisensory cues is slightly better than with the corresponding unisensory cues. However, given the small sample size (n=3) and the considerable variation in behavioral performance across individuals, we remain cautious about drawing definitive conclusions on this matter. We recognize the need for further investigation to establish a robust link between audiovisual responses in the auditory cortex and improved behavioral performance. In future studies, we plan to include a larger number of animals and more thoroughly explore this relationship to provide a comprehensive understanding.</p><fig id="sa4fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-102926-sa4-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(5) Line 339 - I don't think you can say this leads to binding with your current behaviour or neural responses. I would agree there is a memory trace established and a preferential linking in AC neurons.</p></disp-quote><p>We thank the reviewer for raising this important point. In the revised manuscript, we have clarified that our data suggest the formation of a memory trace and preferential linking in AC neurons. The text has been updated to emphasize this distinction. Please see the revised section below (first paragraph in Discussion section).</p><p>“Interestingly, a subset of auditory neurons not only developed visual responses but also exhibited congruence between auditory and visual selectivity. These findings suggest that multisensory perceptual training establishes a memory trace of the trained audiovisual experiences within the AC and enhances the preferential linking of auditory and visual inputs. Sensory cortices, like AC, may act as a vital bridge for communicating sensory information across different modalities.”</p></body></sub-article></article>