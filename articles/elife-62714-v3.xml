<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">62714</article-id><article-id pub-id-type="doi">10.7554/eLife.62714</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Structural Biology and Molecular Biophysics</subject></subj-group></article-categories><title-group><article-title>Bayesian inference of kinetic schemes for ion channels by Kalman filtering</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-205200"><name><surname>Münch</surname><given-names>Jan L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9177-6466</contrib-id><email>jan.muench@med.uni-jena.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-207108"><name><surname>Paul</surname><given-names>Fabian</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-207109"><name><surname>Schmauder</surname><given-names>Ralf</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-85698"><name><surname>Benndorf</surname><given-names>Klaus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0707-4083</contrib-id><email>KLAUS.BENNDORF@med.uni-jena.de</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qpz1x62</institution-id><institution>Institut für Physiologie II, Universitätsklinikum Jena, Friedrich Schiller University Jena</institution></institution-wrap><addr-line><named-content content-type="city">Jena</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>Department of Biochemistry and Molecular Biology, University of Chicago</institution></institution-wrap><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goldschen-Ohm</surname><given-names>Marcel P</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Aldrich</surname><given-names>Richard W</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>04</day><month>05</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e62714</elocation-id><history><date date-type="received" iso-8601-date="2020-09-02"><day>02</day><month>09</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2022-04-22"><day>22</day><month>04</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2020-04-28"><day>28</day><month>04</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.04.27.029207"/></event></pub-history><permissions><copyright-statement>© 2022, Münch et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Münch et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-62714-v3.pdf"/><abstract><p>Inferring adequate kinetic schemes for ion channel gating from ensemble currents is a daunting task due to limited information in the data. We address this problem by using a parallelized Bayesian filter to specify hidden Markov models for current and fluorescence data. We demonstrate the flexibility of this algorithm by including different noise distributions. Our generalized Kalman filter outperforms both a classical Kalman filter and a rate equation approach when applied to patch-clamp data exhibiting realistic open-channel noise. The derived generalization also enables inclusion of orthogonal fluorescence data, making unidentifiable parameters identifiable and increasing the accuracy of the parameter estimates by an order of magnitude. By using Bayesian highest credibility volumes, we found that our approach, in contrast to the rate equation approach, yields a realistic uncertainty quantification. Furthermore, the Bayesian filter delivers negligibly biased estimates for a wider range of data quality. For some data sets, it identifies more parameters than the rate equation approach. These results also demonstrate the power of assessing the validity of algorithms by Bayesian credibility volumes in general. Finally, we show that our Bayesian filter is more robust against errors induced by either analog filtering before analog-to-digital conversion or by limited time resolution of fluorescence data than a rate equation approach.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>ligand-gated ion channel</kwd><kwd>patch-clamp</kwd><kwd>Bayesian Filter</kwd><kwd>hidden Markov</kwd><kwd>Bayesian statistics</kwd><kwd>patch-clamp fluorometry</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>TRR 166 ReceptorLight (Project A5) of the</award-id><principal-award-recipient><name><surname>Benndorf</surname><given-names>Klaus</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>Research Unit 2518 DynIon (Project P2)</award-id><principal-award-recipient><name><surname>Benndorf</surname><given-names>Klaus</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>For analyzing time-dependent patch-clamp or patch-clamp fluorometry data of ion channels in terms of Markovian models, the superiority of Bayesian filtering with respect to traditional deterministic approaches is demonstrated enabling more reliable quantification of the parameters.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Ion channels are essential proteins for the homeostasis of an organism. Disturbance of their function by mutations often causes severe diseases, such as epilepsy (<xref ref-type="bibr" rid="bib76">Oyrer et al., 2018</xref>; <xref ref-type="bibr" rid="bib41">Goldschen-Ohm et al., 2010</xref>), sudden cardiac death (<xref ref-type="bibr" rid="bib16">Clancy and Rudy, 2001</xref>), or sick sinus syndrome (<xref ref-type="bibr" rid="bib100">Verkerk and Wilders, 2014</xref>) indicating a medical need (<xref ref-type="bibr" rid="bib41">Goldschen-Ohm et al., 2010</xref>) to gain further insight into the biophysics of ion channels. The gating of ion channels is usually interpreted by kinetic schemes which are inferred either from macroscopic currents with rate equations (REs) (<xref ref-type="bibr" rid="bib18">Colquhoun and Hawkes, 1995b</xref>; <xref ref-type="bibr" rid="bib12">Celentano and Hawkes, 2004</xref>; <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>; <xref ref-type="bibr" rid="bib92">Stepanyuk et al., 2011</xref>; <xref ref-type="bibr" rid="bib103">Wang et al., 2012</xref>) or from single-channel currents using dwell time distributions (<xref ref-type="bibr" rid="bib72">Neher and Sakmann, 1976</xref>; <xref ref-type="bibr" rid="bib19">Colquhoun et al., 1997a</xref>; <xref ref-type="bibr" rid="bib49">Horn and Lange, 1983</xref>; <xref ref-type="bibr" rid="bib77">Qin et al., 1996</xref>; <xref ref-type="bibr" rid="bib25">Epstein et al., 2016</xref>; <xref ref-type="bibr" rid="bib89">Siekmann et al., 2016</xref>) or hidden Markov models (HMMs) (<xref ref-type="bibr" rid="bib15">Chung et al., 1990</xref>; <xref ref-type="bibr" rid="bib30">Fredkin and Rice, 1992</xref>; <xref ref-type="bibr" rid="bib78">Qin et al., 2000</xref>; <xref ref-type="bibr" rid="bib99">Venkataramanan and Sigworth, 2002</xref>). A HMM consists of a discrete set of metastable states. Changes of their occupation occur as random events over time. Each state is characterized by transition probabilities, related to transition rates, and a probability distribution of the observed signal (<xref ref-type="bibr" rid="bib79">Rabiner, 1989</xref>). It is becoming increasingly clear that the use of Bayesian statistics in HMM estimation constitutes a major advantage (<xref ref-type="bibr" rid="bib3">Ball et al., 1999</xref>; <xref ref-type="bibr" rid="bib23">De Gunst et al., 2001</xref>; <xref ref-type="bibr" rid="bib80">Rosales et al., 2001</xref>; <xref ref-type="bibr" rid="bib81">Rosales, 2004</xref>; <xref ref-type="bibr" rid="bib40">Gin et al., 2009</xref>; <xref ref-type="bibr" rid="bib88">Siekmann et al., 2012</xref>; <xref ref-type="bibr" rid="bib87">Siekmann et al., 2011</xref>; <xref ref-type="bibr" rid="bib47">Hines et al., 2015</xref>; <xref ref-type="bibr" rid="bib86">Sgouralis and Pressé, 2017b</xref>; <xref ref-type="bibr" rid="bib85">Sgouralis and Pressé, 2017a</xref>; <xref ref-type="bibr" rid="bib58">Kinz-Thompson and Gonzalez, 2018</xref>). In ensemble patches, simultaneous orthogonal fluorescence measurement of either conformational changes (<xref ref-type="bibr" rid="bib108">Zheng and Zagotta, 2000</xref>; <xref ref-type="bibr" rid="bib94">Taraska and Zagotta, 2007</xref>; <xref ref-type="bibr" rid="bib95">Taraska et al., 2009</xref>; <xref ref-type="bibr" rid="bib8">Bruening-Wright et al., 2007</xref>; <xref ref-type="bibr" rid="bib56">Kalstrup and Blunck, 2013</xref>; <xref ref-type="bibr" rid="bib57">Kalstrup and Blunck, 2018</xref>; <xref ref-type="bibr" rid="bib107">Wulf and Pless, 2018</xref>) or ligand binding itself (<xref ref-type="bibr" rid="bib6">Biskup et al., 2007</xref>; <xref ref-type="bibr" rid="bib61">Kusch et al., 2010</xref>; <xref ref-type="bibr" rid="bib62">Kusch et al., 2011</xref>; <xref ref-type="bibr" rid="bib106">Wu et al., 2011</xref>) has increased insight into the complexity of channel activation.</p><p>Currently, a Bayesian estimator that can collect information from cross-correlations and time correlations inherent in multi-dimensional signals of ensembles of ion channels is still missing. Traditionally, macroscopic currents are analyzed with solutions of REs which yield a point estimate of the rate matrix or its eigenvalues (<xref ref-type="bibr" rid="bib19">Colquhoun et al., 1997a</xref>; <xref ref-type="bibr" rid="bib83">Sakmann and Neher, 2013</xref>; <xref ref-type="bibr" rid="bib22">d’Alcantara et al., 2002</xref>; <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>; <xref ref-type="bibr" rid="bib103">Wang et al., 2012</xref>) if they are fitted to the data. The RE approach is based on a deterministic differential equation derived by averaging the chemical master equation (CME) for the underlying kinetic scheme (<xref ref-type="bibr" rid="bib60">Kurtz, 1972</xref>; <xref ref-type="bibr" rid="bib96">Van Kampen, 1992</xref>; <xref ref-type="bibr" rid="bib51">Jahnke and Huisinga, 2007</xref>). Its accuracy can be improved by processing the information contained in the intrinsic noise (stochastic gating and binding) (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>; <xref ref-type="bibr" rid="bib69">Munsky et al., 2009</xref>). Nevertheless, all deterministic approaches do not use the information of the time- and cross-correlations of the intrinsic noise. These deterministic approaches are asymptotically valid for an infinite number of channels. Thus, a time trace with a finite number of channels contains, strictly speaking, only one independent data point. Previous rigorous attempts to incorporate the autocorrelation of the intrinsic noise of current data into the estimation (<xref ref-type="bibr" rid="bib12">Celentano and Hawkes, 2004</xref>) suffer from cubic computational complexity (<xref ref-type="bibr" rid="bib92">Stepanyuk et al., 2011</xref>) in the amount of data points, rendering the algorithm non-optimal or even impractical for a Bayesian analysis of larger data set. To understand this, note, that a maximum likelihood optimization (ML) usually takes several orders of magnitude fewer likelihood evaluations to converge compared to the number of posterior evaluations when one samples the posterior. One Monte Carlo iteration (<xref ref-type="bibr" rid="bib5">Betancourt, 2017</xref>) evaluates the posterior distribution and its derivatives many times to propose one sample from the posterior. Stepanyuk suggested an algorithm (<xref ref-type="bibr" rid="bib92">Stepanyuk et al., 2011</xref>; <xref ref-type="bibr" rid="bib93">Stepanyuk et al., 2014</xref>) which derives from the algorithm of <xref ref-type="bibr" rid="bib12">Celentano and Hawkes, 2004</xref> but evaluates the likelihood quicker. Under certain conditions, Stepanyuk’s algorithm can be faster than the Kalman filter (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>). The algorithm by <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> achieves its superior computation time efficiency at the cost of ignoring the time correlations of the fluctuations. A further argument for our approach, independent of the Bayesian context, is investigated in this paper: The KF is the minimal variance filter (<xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>). Instead of strong analog filtering of currents to reduce the noise, but with the inevitable signal distortions (<xref ref-type="bibr" rid="bib90">Silberberg and Magleby, 1993</xref>), we suggest to apply the KF with higher analyzing frequency on minimally filtered data.</p><boxed-text id="box1"><label>Box 1.</label><caption><title>Phenomenological difference between an RE approach and our Bayesian filter</title></caption><fig position="float" id="box1fig1"><label>Box 1—figure 1.</label><caption><title>The First-order Markov property of the data requires a recursive prediction of the signal (by the model) and correction (by the data) scheme of the algorithm.</title><p>(<bold>a</bold>) Idealized patch-clamp (PC) data in the absence of instrumental noise for either ten (colored) or an infinite number of channels generating the mean time trace (black). The fluctuations with respect to the mean time trace (black) reveal autocorrelation (<bold>b</bold>) Conceptual idea of the Kalman Filter (KF): the stochastic evolution of the ensemble signal is predicted and the prediction model updated recursively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-box1-fig1-v3.tif"/></fig><p>Two major problems for parameter inference for the dynamics of the ion channel ensemble <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are: (I) that currents are only low-dimensional observations (e.g. one dimension for patch-clamp or two for cPCF) of a high-dimensional process (dimension being the number of model states) blurred by noise and (II) the fluctuations due to the stochastic gating and binding process cause autocorrelation in the signal. Traditional analyses for macroscopic PC data (and also for related fluorescence data) by the RE approach, e.g. <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> ignores the long-lasting autocorrelations of the deviations (<xref ref-type="fig" rid="box1fig1">Box 1—figure 1a</xref>) blue and orange curves from the mean time trace (black) that occur in real data measured from a <italic>finite</italic> ensemble. To account for the autocorrelation in the signal, an optimal prediction (<xref ref-type="fig" rid="box1fig1">Box 1—figure 1b</xref>) of the future signal distribution <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> should use the measurement <inline-formula><mml:math id="inf3"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> from the current time step <italic>t</italic><sub>1</sub> to update the belief about the underlying <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Based on stochastic modelling of the time evolution of the channel ensemble, it then predicts <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><fig position="float" id="box1fig2"><label>Box 1—figure 2.</label><caption><title>The residuals between model prediction and data reveal long autocorrelations if the analysis algorithm ignores the first-order Markov property.</title><p>(<bold>a</bold>) Autocorrelation of the residuals <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of two ligand concentrations of currents (blue) and of the fluorescence (red) after the data have been analyzed with the KF approach. (<bold>b</bold>) autocorrelation of <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> after analysing with he RE approach.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-box1-fig2-v3.tif"/></fig><p>To demonstrate the difference how the two algorithms analyze the data, we compute the autocorrelation of the residuals of the data. After the analysis with either the RE approach or the KF, we can construct from the model with the mean predicted signal <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Eq. 4 for the definition of <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the predicted standard deviation <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> the normalized residual time trace of the data which are defined as<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:msqrt></mml:mfrac><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>Filtering (fitting) with the KF (given the true kinetic scheme) one expects to find a white-noise process for the residuals. Plots of the autocorrelation function of both signal components (<xref ref-type="fig" rid="box1fig2">Box 1—figure 2a</xref>) confirms our expectation. The estimated autocorrelation vanishes after one multiple of the lag time (the interval between sampling points), which means that the residuals are indeed a white-noise process. In contrast, the residuals derived from the RE approach (<xref ref-type="fig" rid="box1fig2">Box 1—figure 2b</xref>) display long lasting periodic autocorrelations.</p></boxed-text><p>On the one hand, a complete HMM analysis (forward algorithm) would deliver the most exact likelihood of macroscopic data. On the other hand, the computational complexity of the forward algorithm limits this type of analysis in ensemble patches to no more than a few hundred channels per time trace (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>). To tame the computational complexity (<xref ref-type="bibr" rid="bib51">Jahnke and Huisinga, 2007</xref>), we approximate the solution of the CME with a Kalman filter (KF), thereby remaining in a stochastic framework <xref ref-type="bibr" rid="bib55">Kalman, 1960</xref>. This allows us to explicitly model the time evolution of the first two moments (mean value and covariance matrix) of the probability distribution of the hidden channel states. Notably, for linear (first or pseudo) Gaussian system dynamics, the KF is optimal in producing a minimal prediction error for the mean state. KFs have been used previously in several protein expression studies which also demonstrate the connection of the KF to the linear noise approximation (<xref ref-type="bibr" rid="bib59">Komorowski et al., 2009</xref>; <xref ref-type="bibr" rid="bib27">Finkenstädt et al., 2013</xref>; <xref ref-type="bibr" rid="bib26">Fearnhead et al., 2014</xref>; <xref ref-type="bibr" rid="bib28">Folia and Rattray, 2018</xref>; <xref ref-type="bibr" rid="bib9">Calderazzo et al., 2019</xref>; <xref ref-type="bibr" rid="bib43">Gopalakrishnan et al., 2011</xref>).</p><p>Our approach generalizes the work of <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> by including state-dependent fluctuations such as open-channel noise and Poisson noise in additional fluorescence data. A central technical difficulty which we solved is that due to the state-dependent noise the central Bayesian update equation loses its analytical solution. We derived an approximation which is correct for the first two moments of the probability distributions. Stochastic rather than deterministic modeling is generally preferable for small systems or non-linear dynamics (<xref ref-type="bibr" rid="bib96">Van Kampen, 1992</xref>; <xref ref-type="bibr" rid="bib39">Gillespie and Golightly, 2012</xref>). However, even with simulated data of unrealistic high numbers of channels per patch (more than several thousands within one patch), the KF outperforms the deterministic approach in estimating the model parameters. <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> already demonstrated the advantage of the KF to learn absolute rates from time traces at equilibrium. Like all algorithms that estimate the variance and the mean (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>) the KF can infer the number of channels <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> for each time trace, the single-channel current <inline-formula><mml:math id="inf12"><mml:mi>i</mml:mi></mml:math></inline-formula> and analogous in optical recordings the mean number <inline-formula><mml:math id="inf13"><mml:msub><mml:mi>λ</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:math></inline-formula> of photons from bound ligands per recorded frame. To select models and to identify parameters, stochastic models are formulated within the framework of Bayesian statistics where parameters are assigned uncertainties by treating them as random variables (<xref ref-type="bibr" rid="bib46">Hines, 2015</xref>; <xref ref-type="bibr" rid="bib4">Ball, 2016</xref>). In contrast, previous work on ensemble currents combined the KF only with ML estimation (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>). Difficulties in treating simple stochastic models by ML approaches in combination with the KF (<xref ref-type="bibr" rid="bib2">Auger-Méthé et al., 2016</xref>), especially with non-observable dynamics, justify the computational burden of Bayesian statistics. Bayesian statistics has an intuitive way to incorporate soft or hard constrains from diverse sources of prior information. Those sources include mathematical prerequisites, other experiments, simulations or theoretical assumptions. They are applied as additional model assumptions by a prior probability distribution over the possible parameter space. Hence, knowledge of the model parameters prior to the experiment are correctly accounted for in the analyzes of the new data. Alternatively, some of these benefits of prior knowledge can be incorporated by penalized maximum likelihood (<xref ref-type="bibr" rid="bib84">Salari et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Navarro et al., 2018</xref>). Bayesian inference provides outmatching tools for modeling over point estimates: First, the Bayesian approach is still applicable in situations where parameters are not identifiable (<xref ref-type="bibr" rid="bib45">Hines et al., 2014</xref>; <xref ref-type="bibr" rid="bib66">Middendorf and Aldrich, 2017b</xref>) or posteriors are non-Gaussian, whereas ML fitting ceases to be valid (<xref ref-type="bibr" rid="bib10">Calderhead et al., 2013</xref>; <xref ref-type="bibr" rid="bib104">Watanabe, 2007</xref>). Second, a Bayesian approach provides superior model selection tools for singular models such as HMMs or KFs Gelman et al. (2014). Third, Bayesian statistics has a correct uncertainty quantification (<xref ref-type="bibr" rid="bib39">Gillespie and Golightly, 2012</xref>) based on the data and the prior for the statistical problem. In contrast, ML or maximum posterior approaches lack uncertainty quantification based on one data set (<xref ref-type="bibr" rid="bib54">Joshi et al., 2006</xref>). Only under optimal conditions their uncertainty quantification becomes equivalent to Bayesian credibility volumes (<xref ref-type="bibr" rid="bib52">Jaynes and Kempthorne, 1976</xref>). This study focuses on the effects on the posterior due to formulating the likelihood via a KF instead of an RE approach and the benefits of adding a second dimension of observation. We consider the performance of our algorithm against the gold standards in four different aspects: (I) The relative distance of the posterior to the true values, (II) the uncertainty quantification, here in the form of the shape of the posterior, (III) parameter identifiability, and (IV) robustness against typical misspecifications of the likelihood (such as ignoring that currents are filtered or that the integration time of fluorescence data points is finite) of real experimental data.</p></sec><sec id="s2" sec-type="results|discussion"><title>Results and discussion</title><sec id="s2-1"><title>Simulation of ligand-gated ion-channel data</title><p>Here we treat an exemplary ligand-gated channel with two ligand binding steps and one open-closed isomerization described by an HMM (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>). For this model, confocal patch-clamp fluorometry (cPCF) data were simulated: time courses of ligand binding and channel current upon concentration jumps were generated (see Appendix 5 and Materials and methods section). Idealized example data with added white noise are shown in <xref ref-type="fig" rid="fig1">Figure 1b–d</xref>. We added realistic instrumental noise to the simulated data (see Appendix 5). A qualitative description of the statistical problem that needs to be addressed when modeling time series data such as the simulated is outlined in Box. 1.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Kinetic scheme and simulated data.</title><p>(<bold>a</bold>) The Markov state model (kinetic scheme) consists of two binding steps and one opening step. The rate matrix <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is parametrized by the absolute rates <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the ratios <inline-formula><mml:math id="inf16"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> between on and off rates (i.e. equilibrium constants) and <inline-formula><mml:math id="inf17"><mml:mi>L</mml:mi></mml:math></inline-formula>, the ligand concentration in the solution. The units of the rates are <inline-formula><mml:math id="inf18"><mml:msup><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">M</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">s</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, respectively. The liganded states are <italic>C</italic><sub>2</sub>, <italic>C</italic><sub>3</sub>, <italic>O</italic><sub>4</sub>. The open state <italic>O</italic><sub>4</sub> conducts a mean single-channel current <inline-formula><mml:math id="inf20"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Note, that absolute magnitude of the single channel current is irrelevant regarding this study what matters is its relative magnitude compared with <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>σ</mml:mi><mml:mi>ex</mml:mi></mml:msub></mml:math></inline-formula>. Simulations were performed with 10 kHz or 100 kHz (for <xref ref-type="fig" rid="fig11">Figures 11</xref> and <xref ref-type="fig" rid="fig12">12</xref>) sampling, KF analysis frequency <italic>f</italic><sub><italic>ana</italic></sub> for cPCF data is in the range of (200-500) Hz while pure current data is analyzed at 2-5 kHz. (<bold>b-c</bold>) Normalized time traces of simulated relaxation experiments of ligand concentration jumps with <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> channels, <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.375</mml:mn></mml:mrow></mml:math></inline-formula> mean photons per bound ligand per frame and single-channel current <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, open-channel noise with <inline-formula><mml:math id="inf26"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>op</mml:mtext><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and an instrumental noise with the variance <inline-formula><mml:math id="inf27"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. The current <italic>y</italic><sub><italic>curr</italic></sub> and fluorescence <italic>y</italic><sub><italic>flu</italic></sub> time courses are calculated from the same simulation. For visualization, the signals are normalized by the respective median estimates of the KF. The black lines are the theoretical open probabilities <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the average binding per channel <inline-formula><mml:math id="inf29"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>ch</mml:mtext></mml:msub><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> of the used model. Typically, we used 10 ligand concentrations which are (0.0625, 0.125, 0.25, 0.5, 1, 2,4, 8, 16, 64) <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:math></inline-formula>. d, Equilibrium binding and open probability as function of the ligand concentration <inline-formula><mml:math id="inf32"><mml:mi>L</mml:mi></mml:math></inline-formula>.</p><p><supplementary-material id="fig1sdata1"><label>Figure 1—source data 1.</label><caption><title>The example data is provided.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig1-data1-v3.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig1-v3.tif"/></fig></sec><sec id="s2-2"><title>Kalman filter derived from a Bayesian filter</title><p>Here and in the Materials and methods section, we derive the mathematical tools to account correctly for the stochastic Markov dynamics of single molecules in the fluctuations of macroscopic signals. The KF is a Bayesian filter (see Materials and methods), that is a continuous state HMM with a multivariate normal transition probability <xref ref-type="bibr" rid="bib36">Ghahramani, 1997</xref> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). We define the hidden ensemble state vector<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>which counts the number of channels in each state <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Methods). To make use of the KF, we assume the following general form of the dynamic model: The evolution of <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is determined by a linear model that is parametrized by the state evolution matrix <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>where ∼ means <italic>sampled from</italic> and <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a shorthand for the multivariate normal distribution, with the mean μ and the variance-covariance matrix <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. The state evolution matrix (transition matrix) is related to the rate matrix <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> by the matrix exponential <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The mean of the hidden state evolves according to the equation <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. It is perturbed by normally distributed white process noise <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with the following properties: The mean value of the noise fulfills <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and the variance-covariance matrix of the noise is <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ω</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Materials and methods <xref ref-type="disp-formula" rid="equ43">Equation 38d</xref>, <xref ref-type="bibr" rid="bib4">Ball, 2016</xref>). In short, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> defines a Gaussian Markov process.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The first-order hidden Markov structure is explicitly used by the Bayesian filter.</title><p>The filter can be seen as continuous state analog of the forward algorithm. (<bold>a</bold>) Graphical model of the conditional dependencies of the stochastic process. Horizontal black arrows represent the conditional multivariate normal transition probability <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of a continuous state Markov process. Notably, it is <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> which is treated as the Markov state by the KF. The transition matrix <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the time-dependent covariance <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> characterise the single-channel dynamics. The vertical black arrows represent the conditional observation distribution <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The observation distribution summarizes the noise of the experiment, which in the KF is assumed to be multivariate normal. Given a set of model parameters and a data point <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the Bayesian theorem allows to calculate in the correction step <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (red arrow). The posterior is propagated linearly in time by the model, predicting a state distribution <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (orange arrow). The propagated posterior predicts together with the observation distribution the mean and covariance of the next observation. Thus, it creates a multivariate normal likelihood for each data point in the observation space. (<bold>b</bold>) Observation space trajectories of the predictions and data of the binding per channel vs. open probability for different ligand concentrations. The curves are normalized by the median estimates of <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the ratio of open-channels <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mi>i</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> which approximates the open probability <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The black crosses represent the predicted mean signal <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is calculated by multiplying the observational matrix <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with the mean predicted state <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For clarity, we used the mean value of the posterior of the KF. The green and blue trajectories represent the part of the time traces with after the jump to non-zero ligand concentration and after jumping backt to zero ligand concentration in the bulk, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig2-v3.tif"/></fig><p>The observations <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> depend linearly on the hidden state <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The linear map is determined by an observation matrix <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The noise of the measurement setup (Appendix 5 and <xref ref-type="disp-formula" rid="equ48">Equation 43</xref>) is modeled as a random perturbation of the mean observation vector. The noise fulfills <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> defines the state-conditioned observation distribution <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). The set of all measurements up to time <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is defined by <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. If the system strictly obeys <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> then the KF is optimal in the sense that it is the minimum variance filter of that system <xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>. If the distributions of ν and ω are not normal, the KF is still the minimum variance filter in the class of all linear filters but there might be better non-linear filters. In case of colored noise ν and ω the filtering equations (see Materials and methods) can be reformulated by state augmentation or measurement-time-difference approach techniques <xref ref-type="bibr" rid="bib13">Chang, 2014</xref>. For each element in a sequence of hidden states <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>t</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>T</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and for a fixed set of parameters <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, an algorithm based on a Bayesian filter (<xref ref-type="fig" rid="fig2">Figure 2a</xref>), explicitly exploits the conditional dependencies of the assumed Markov process. A Bayesian filter recursively predicts prior distributions for the next <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>given what is known about <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> due to <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The KF as a special Bayesian filter assumes that the transition probability is multivariate normal according to <xref ref-type="disp-formula" rid="equ3">Equation 3</xref><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>Note, that <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> is a central approximation of the KF. While the exact transition distribution of an ensemble of ion channels is the generalized-multinomial distribution (Methods <xref ref-type="disp-formula" rid="equ34">Equation 32</xref>), the quality of normal approximations to multinomial <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> or generalized-multinomial <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> distributions depends on the number of ion channels <inline-formula><mml:math id="inf73"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> in the patch and on the position of the probability vector in the simplex space. The difference between the log-likelihoods of the true generalized-multinomial dynamics and <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> type approximation scales as <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>. As a rule of thumb one should be careful with both algorithms for time traces with <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Below or even inside this interval there are more qualified concepts such as the forward algorithm or even particle filters (<xref ref-type="bibr" rid="bib42">Golightly and Wilkinson, 2011</xref>; <xref ref-type="bibr" rid="bib39">Gillespie and Golightly, 2012</xref>) which avoid the normal approximation.</p><p>Each prediction of <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) is followed by a correction step,<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>that allows to incorporate the current data point into the estimate, based on the Bayesian theorem (<xref ref-type="bibr" rid="bib14">Chen, 2003</xref>). Additionally, the KF assumes (<xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>; <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>) a multivariate normal observation distribution<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>If the initial prior distribution is multivariate normal then due to the mathematical properties of the normal distributions the prior and posterior <inline-formula><mml:math id="inf77"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> become multivariate normal <xref ref-type="bibr" rid="bib14">Chen, 2003</xref> for each time step. In this case, one can derive algebraic equations for the prediction (Materials and methods <xref ref-type="disp-formula" rid="equ39">Equation 37</xref>, <xref ref-type="disp-formula" rid="equ43">Equation 38d</xref>) and correction (Materials and methods <xref ref-type="disp-formula" rid="equ69">Equation 58</xref> and <xref ref-type="disp-formula" rid="equ69">Equation 58</xref>) of the mean and covariance. The algebraic equations originate from the fact that a normal prior is the conjugated prior for the mean value of a normal likelihood. Due to the recursiveness of its equations, the KF has a time complexity that is linear in the number of data points, allowing a fast algorithm. The denominator of <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> is the normal distributed marginal likelihood <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for each data point, which constructs by<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mspace linebreak="newline"/><mml:mo>=</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>a product marginal likelihood of normal distributions of the whole time trace <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">T</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of length <inline-formula><mml:math id="inf80"><mml:msub><mml:mi>N</mml:mi><mml:mi mathvariant="normal">T</mml:mi></mml:msub></mml:math></inline-formula> for the KF. For the derivation of <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> see Materials and methods (<xref ref-type="disp-formula" rid="equ43">Equation 38d</xref>) and <xref ref-type="disp-formula" rid="equ48">Equation 43</xref>. <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the covariance of the prior distribution over <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> before the KF took <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> into account. The likelihood for the data allows to ascribe a probability to the parameters <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, given the observed data (Methods <xref ref-type="disp-formula" rid="equ21">Equation 20</xref>). An illustration for the operation of the KF on the observation space is given in <xref ref-type="fig" rid="fig2">Figure 2b</xref>. The predicted mean signal <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to binding degree <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> and open probability <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>O</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>. These values are plotted as vector trajectories.</p><p>The standard KF (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>; <xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>; <xref ref-type="bibr" rid="bib14">Chen, 2003</xref>) has additive constant noise <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the observation model. Thus, in this case a constant variance term <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is added, in <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> to the aleatory variance <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> which, as mentioned above, originates (<xref ref-type="disp-formula" rid="equ43">Equation 38d</xref>) from the the fact that we do not know the true system state <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. For signals with Poisson-distributed photon counting or open-channel noise, we need to generalize the noise model to account for additional white-noise fluctuations with <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>-dependent <italic>variance</italic>. For instance, in single-channel currents additional noise is often observed whose variance is referred to by <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. In macroscopic currents this additional noise can be modeled by a term <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, causing state-dependency of our noise model.<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">⇔</mml:mo><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The second noise term <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub></mml:math></inline-formula> is defined in terms of the first two moments <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. To the best of our knowledge such a state-dependent noise makes the integration of the denominator of <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> (which is also the incremental likelihood) intractable<disp-formula id="equ11"><label>(11a)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thickmathspace"/><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><label>(11b)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>∫</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This is because the state distribution <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> as the prior also influences the variance parameter of the likelihood which means that the conjugacy property is lost. While a normal distribution is the conjugated prior of the mean of a normal likelihood, it is not the conjugated prior for the variance. However, by applying the theorem of total variance decomposition <xref ref-type="disp-formula" rid="equ51">Equation 46a</xref> we deduce a normal approximation to <xref ref-type="disp-formula" rid="equ8">Equation 8</xref> and to the related problem of Poisson-distributed noise in fluorescence <xref ref-type="disp-formula" rid="equ68">Equation 57</xref>, <xref ref-type="disp-formula" rid="equ62">Equation 55a</xref> data. By computing the mean and the variance or covariance matrix of the signal, we can reformulate the noise model to fit the form of the traditional KF framework. Note, that the derived equations for the covariance matrix are still exact for the more general noise model. Mean and covariance just do not form a set of sufficient statistics anymore.</p><p>Our derivation is not limited to ligand-gated ion channels. For example, when investigating voltage-gated channels, the corresponding noise model can be easily adapted. This holds also when using the P/n protocol for which the noise model resembles that of the additional variance in the fluorescence signal. The additional variance is induced because the mean signal from the ligands swimming in the bulk (Materials amd methods <xref ref-type="disp-formula" rid="equ48">Equation 43</xref> Appendix 5) is eliminated by subtracting scaled mean reference signal which itself has an error. This manipulation adds additional variance to the resulting signal comparable to P/n protocol. Other experimental challenges, as for example series resistance compensation promoting oscillatory behavior of the amplifier, deserve certainly advanced treatment. Nevertheless, for voltage-clamp experiments with a rate equation approach it also becomes clear (<xref ref-type="bibr" rid="bib63">Lei et al., 2020</xref>) that modeling of the actual experimental limitations, including series resistance, membrane and pipette capacitance, voltage offsets, imperfect compensations by the amplifier, and leak currents are necessary for consistent kinetic scheme inference.</p><p>The Bayesian posterior distribution<disp-formula id="equ13"><label>(12)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>encodes all information from model assumptions and experimental data used during model training (see Materials and methods). A full Bayesian inference is usually not an optimization (finding the global maximum or mode of the posterior or likelihood) but calculates all sorts of quantities derived from the posterior distribution such as mean values of any function <inline-formula><mml:math id="inf101"><mml:mi>f</mml:mi></mml:math></inline-formula> including the mean value or covariance matrix of the parameters themselves or even the likelihood of the data.<disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>Besides the covariance matrix of the parameter to express parameter uncertainty, the posterior allows to calculate a credibility volume. The smallest volume <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> that encloses a probability mass <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> of<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mo>∫</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>is called the Highest Density Credibility Volume/Interval (HDCV/HDCI). Those credibility volumes should not be confused with confidence volumes although under certain conditions they can become equivalent. Given that our model sufficiently captures the true process, the true values <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> will be inside that volume with a probability <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Unfortunately, typically there is no analytical solution to <xref ref-type="disp-formula" rid="equ13">Equation 12</xref> . However, it can be solved numerically with Monte Carlo techniques, enabling to calculate all quantities related to <xref ref-type="disp-formula" rid="equ14">Equation 13</xref> and <xref ref-type="disp-formula" rid="equ15">Equation 14</xref> . Our algorithm uses automatic differentiation of the statistical model to sample from the posterior (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1a</xref>) via Hamiltonian Monte Carlo (HMC) (<xref ref-type="bibr" rid="bib5">Betancourt, 2017</xref>), see Appendix 7 , as provided by the Stan software (<xref ref-type="bibr" rid="bib48">Hoffman and Gelman, 2014</xref>; <xref ref-type="bibr" rid="bib35">Gelman et al., 2015</xref>).</p></sec><sec id="s2-3"><title>Benchmark for PC data against the gold standard algorithms</title><p>We compare the posterior distribution (<xref ref-type="fig" rid="fig3">Figure 3</xref>) of our algorithm against Bayesian versions of the deterministic (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>) and stochastic (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>) algorithms, which we consider as the gold standard algorithms for macroscopic patch-clamp data. Simulated currents of a patch with <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> are shown in (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). The resulting posteriors (<xref ref-type="fig" rid="fig3">Figure 3a</xref>) show that both former algorithms are further away from the true parameter values with their maxima or mean values (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). E.g., the relative error of the maximum of the posterior are <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>200</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> and <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mn>240</mml:mn><mml:mi mathvariant="normal">%</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> . The four other parameters including the three equilibrium constants behave less problematic as judged by their relative error. Additionally, if one does not only judge the performance by the relative distance of maximum (or some other significant point) of the posterior but considers the spread of the posterior as well, it becomes apparent, that the marginal posterior of both former algorithms fail to cover the true values within at least the reasonable parts of their tails. Accordingly, for maximum likelihood inferences the true value would be far outside the estimated confidence interval. For the RE approach only the marginal posterior of <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>21</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is nicely centered over the true values and the marginal of <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>32</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> could be considered to cover within a reasonable part of the distribution the true value. Uncertainty quantification is investigated in more detail further down (<xref ref-type="fig" rid="fig4">Figures 4</xref>—<xref ref-type="fig" rid="fig9">9</xref>). Note that in <xref ref-type="fig" rid="fig3">Figure 3a</xref>, parameter unidentifiability by heavy tails/ multiple maxima of the posterior distribution or (anti-) correlation is easily visible as non axial symmetric patterns.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The Bayesian filter overcomes the sensitivity to varying (open-channel) noise of the classic Kalman filter and does not show the overconfidence of the RE-approach.</title><p>Overall it shows the highest accuracy and the posterior covers the true values. The classical deterministic RE (blue), 2007 Kalman filter (red) and our Bayesian filter (green) are implemented as a full Bayesian version and the obtained posterior distributions are compared. For all PC data sets in the figure the analysing frequencies <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mtext>ana</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> ranges within 2-5. (<bold>a</bold>) Posterior of the parameters for the 3 algorithms for the data set displayed in panel d. The blue crosses indicate the true values. All samples are normalized by their true values which is indicated by the ∼ above the parameters. For clarity, we only show a fraction of the samples of the posterior for blue and red. b, Effect of open channel noise: The Euclidean error for all three approaches is plotted vs. <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>op</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (low axis).The upper axis displays the ratio of the ‘typical’ standard deviation of the open channel excess noise of the ensemble of channels <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi>N</mml:mi><mml:mn>0.5</mml:mn><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> to the standard deviation of instrumental noise. c, Influence of patch size: Scaling of the Euclidean error vs. <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> follows <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∼</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> indicated by the dashed lines for <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>ch</mml:mtext></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for the RE and the Bayesian filter approach. The data indicates a constant error ratio (orange) for large <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>ch</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> samples of the posteriors for many data sets suggest an improper posterior. An instrumental noise of <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> was used. (<bold>d</bold>) The time traces on which the posteriors of panel a are based (for the ligand concentrations see <xref ref-type="fig" rid="fig1">Figure 1</xref>). Panel b used the same data too, but σ and <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> were varied.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>The data folder includes all 15 sets of time traces.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig3-data1-v3.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig3-v3.tif"/></fig><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>For multidimensional data (cPCF) the RE approach almost approaches the accuracy (Euclidean error) of the Bayesian Filter.</title><p>However, only the Bayesian filter covers the true value in a reasonable HDCV while RE based posteriors are too narrow. All samples are normalized by their true values which is indicated by the ∼ above the parameters. (<bold>a</bold>) Euclidean errors of the maximum for the rate <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and equilibrium constants <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> obtained by the KF (green) and from the REs (blue) are plotted against <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> for <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Both algorithms scale like <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> (dashed lines) for larger <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> which is the expected scaling For smaller <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (gray range) the error is roughly the same indicating that limitations of the normal approximation to the multinomial distribution dominate the overall error in this regime. The combination of fluorescence and current data(cPCF) decreases the eucleadian error for both approaches compared to current data alone(PC). (<bold>b</bold>), HDCI and the mode of the 3 <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and 3 <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> plotted vs. <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> revealing that the maximum is a consistent estimator (converges in distribution to the true value with increasing data quality). While the KF (green) 0.95-HDCI includes usually the true value, the RE HDCI (blue) is too narrow and, thus, the real values are frequently not included. (<bold>c</bold>) Bayesian estimation of true success probability for the event that all 6 0.95-HDCI include the respective true values at the same time by a binomial likelihood. Since the data sets have different <inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and the model approximations become better with increasing <inline-formula><mml:math id="inf135"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>, we use a cut-off for <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. d, Comparison of 1-D and combinations of 2-D marginal posteriors of the parameters of interest for both algorithms calculated from a <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> simulation. Blue lines indicate the true value. We depict that in two dimensions the disproportion of the deviation of the mode and the spread of RE (blue) approach is worsened while KF (green) posterior includes the true values with more reasonable probability mass.</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>6µMol.</title><p>Each of the source data folders contains for a specific ligand concentration the time traces of cPCF data for all <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>.</p></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data1-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata2"><label>Figure 4—source data 2.</label><caption><title>64µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data2-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata3"><label>Figure 4—source data 3.</label><caption><title>32µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data3-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata4"><label>Figure 4—source data 4.</label><caption><title>8µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data4-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata5"><label>Figure 4—source data 5.</label><caption><title>4µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data5-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata6"><label>Figure 4—source data 6.</label><caption><title>1µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data6-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata7"><label>Figure 4—source data 7.</label><caption><title>2µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data7-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata8"><label>Figure 4—source data 8.</label><caption><title>025µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data8-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata9"><label>Figure 4—source data 9.</label><caption><title>05µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data9-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata10"><label>Figure 4—source data 10.</label><caption><title>00625µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data10-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata11"><label>Figure 4—source data 11.</label><caption><title>003125µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data11-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig4sdata12"><label>Figure 4—source data 12.</label><caption><title>0125µMol.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig4-data12-v3.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig4-v3.tif"/></fig><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The HDCV of the posterior of the KF follows the requested binomial statistics while the HDCVs of the RE approach are too narrow.</title><p>(<bold>a</bold>) Cumulative χ-square distribution vs. the Mahalanobis distance <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub></mml:math></inline-formula>. The y axis denotes the probability mass which is counted by moving away from the maximum before an ellipsoid with distance <inline-formula><mml:math id="inf140"><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub></mml:math></inline-formula> is reached. The different colours represent the changes of the cdf with an increasing number of rate parameters. The blue cdf at <inline-formula><mml:math id="inf141"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> represents how much probability mass can be found from <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:msubsup><mml:mi>normal</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, see inset. In one dimension, we can expect to find the true value within <inline-formula><mml:math id="inf143"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> around the mean with the usual probability of <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.682</mml:mn></mml:mrow></mml:math></inline-formula> for univariate normally distributed random variables. The six parameters (brown) of the full rate matrix will almost certainly be beyond <inline-formula><mml:math id="inf145"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.0</mml:mn></mml:mrow></mml:math></inline-formula>. The higher the dimensions of the space the less important becomes the maximum of the probability density distribution for the typical set which is by definition the region where the probability mass resides. The mathematical reason for this is that the probability mass <inline-formula><mml:math id="inf146"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi>V</mml:mi></mml:msub><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>V</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the integrated product of volume and probability density. b, The two methods to count volume in units of probability mass for the KF (green) and the RE (blue). The gray area indicates which data sets are considered a success if one chooses to evaluate a proababilty mass of 0.4 of each posterior around its mode. All data sets in the white area are considered a failure. For the optimistic noise assumptions <inline-formula><mml:math id="inf147"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ex</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf148"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.05</mml:mn><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and a mean photon count per bound ligand per frame <inline-formula><mml:math id="inf149"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> the RE approach (blue) distributes the probability mass such that the HDCV never includes the true rate matrix. From <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> both HDCV estimates of the KF posterior (green curves) include the true value within a reasonable volume and show a similar behaviour. c, Binomial success statistics of HDCV to cover the true value vs. the expected probability constructed from the data of (<bold>b</bold>). Calculated for <inline-formula><mml:math id="inf151"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.25</mml:mn><mml:mo>⁢</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf152"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.025</mml:mn><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf153"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> and minimal background noise.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig5-v3.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Even for the highest tested experimental noise the RE approach does not follow the required binomial statistics, generating an underestimated uncertainty.</title><p>(<bold>a</bold>) Binomial success statistics of HDCV to cover the true value vs. the expected probability. Calculated for <inline-formula><mml:math id="inf154"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf155"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf156"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.375</mml:mn></mml:mrow></mml:math></inline-formula> and a strong background noise. (<bold>b</bold>) Binomial success statistics of HDCV to cover the true value vs. the expected probability. For <inline-formula><mml:math id="inf157"><mml:mrow><mml:mrow><mml:mn>10</mml:mn><mml:mo>⋅</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf158"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf159"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.375</mml:mn></mml:mrow></mml:math></inline-formula> and a strong background noise. For both algorithms, the adaptation of the sampler of the posterior was more fragile for small <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>, leading to differences in the posterior if the posterior is constructed from different independent sampling chains. Those data sets were then excluded. We assume that these instabilities are induced in both algorithms by the shortcomings of the multivariate normal assumptions. (<bold>c</bold>) Comparison of the Euclidean error vs. <inline-formula><mml:math id="inf161"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> for the pessimistic noise case (solid lines) with Euclidean error for the optimistic noise case (dotted lines).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig6-v3.tif"/></fig><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Higher model complexity drastically increases the minimal requirements of the data.</title><p>With PC data the RE approach is frequently incapable to identify all parameters while the Bayesian filter is more robust. cPCF data alleviate the parameter unidentifiabilities for patch sizes for which PC data are insufficient. Each panel column corresponds to a particular true process with increasing complexity from left to right, as indicated by the model schemes on top. Within all kinetic schemes, each transition to the right adds one bound ligand. Each transition to left is an unbinding step. Vertical transitions are either conformational or opening transitions. Plots in each row share the same y-axis respectively. Each column shares the same abscissa. (<bold>a-c</bold>) Error ratio for PC data (blue) and cPCF data (red). The dashed lines indicate the mean error ratio under the simplifying assumption that the error ratio does not depend on <inline-formula><mml:math id="inf162"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:math></inline-formula> The vertical bars are the standard deviations of the mean values. Theses values were calculated from the Euclidean errors shown in <xref ref-type="fig" rid="fig3">Figures 3c</xref> and <xref ref-type="fig" rid="fig4">4a</xref> for a, and panels (<bold>d-e</bold>), for (<bold>b-c</bold>), respectively. Results from the KF algorithm (green) and the RE algorithm (blue) are compared for PC (lighter shades) and cPCF (strong lines). The diagonal gray areas indicate a <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> proportionality. For simulating the underlying PC data, we used standard deviations of <inline-formula><mml:math id="inf164"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf165"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> and for the cPCF data additionally a ligand of brightness <inline-formula><mml:math id="inf166"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>. To facilitate the inference for the two more complex models, we assumed that the experimental noise and the single channel current are well characterized, meaning <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf169"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>gamma</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In the models containing loops (last 2 columns), a prior was used to enforce microscopic-reversibility and set to <inline-formula><mml:math id="inf170"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mn>25</mml:mn><mml:mo>⋆</mml:mo></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mi>beta</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>100</mml:mn><mml:mo>,</mml:mo><mml:mn>100</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> multiplied by <inline-formula><mml:math id="inf171"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>8</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>⋅</mml:mo><mml:mn>0.995</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mo>⋆</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p><supplementary-material id="fig7sdata1"><label>Figure 7—source data 1.</label><caption><title>The folder of the five-state model includes 15 sets of time traces in the interval <inline-formula><mml:math id="inf172"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</title><p>Each of them has 10 ligand concentrations. The number in the file name reports the amount of ion channels in the patch.</p></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig7-data1-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig7sdata2"><label>Figure 7—source data 2.</label><caption><title>The folder of the six-state model includes 14 sets of time traces in the interval <inline-formula><mml:math id="inf173"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</title><p>Each of them has 10 ligand concentrations. The number in the file name reports the amount of ion channels in the patch.</p></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig7-data2-v3.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig7-v3.tif"/></fig><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Revisiting the PC data obtained by the 4-state-1-open-state model shows that the KF succeeds to produce realistic uncertainty quantification, while the overconfidence problem (unreliable uncertainty quantification) of the RE approach remains.</title><p>Comparison of a series of HDCIs shown as functions of <inline-formula><mml:math id="inf174"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> for each parameter of the rate matrix obtained by the KF (green) and the RE algorithm (blue). The differing shades of green and blue indicate the set of <inline-formula><mml:math id="inf175"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.95</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-HDCIs. Only the interval <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in which all parameters are identified is displayed. The data are taken from the KF vs. RE benchmark of <xref ref-type="fig" rid="fig3">Figures 3c</xref> and <xref ref-type="fig" rid="fig7">7a</xref> . The first row corresponds to three rates <inline-formula><mml:math id="inf177"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the second row to the equilibrium constants <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. All parameters are normalized by their true value. The insets show the error ratios of the respective single parameter estimates. Note that the error ratios on the single-parameter level can be even of the order of magnitude of 10<sup>2</sup>. Thus, they can be much larger than the error ratios calculated from the Euclidean error if the errors of the respective parameters are small compared to other error terms in the Euclidean error <xref ref-type="disp-formula" rid="equ16">Equation 15</xref> .The lowest Euclidean error for this kinetic scheme has cPCF data analyzed with the KF. (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). A 6-state-1-open-states model with cPCF data has again an error ratio of the the usual scale (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). As expected, the Euclidean error continuously increases with model complexity (<xref ref-type="fig" rid="fig7">Figure 7d and e</xref>). For PC data of the 6-state-1-open-states model even the likelihood of the KF is that weak (<xref ref-type="fig" rid="fig7">Figure 7e</xref>) that it delivers unidentified parameters even for <inline-formula><mml:math id="inf179"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> and we can detect heavy tailed distributions up until <inline-formula><mml:math id="inf180"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Using RE on PC data alone does not lead to parameter identification, thus no error ratio can be calculated.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig8-v3.tif"/></fig><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>The HDCIs for PC data for a 5-state-2-open-states model show negligible bias for the KF with the true value being included.</title><p>In contrast, the HDCE for RE approach frequently does not include the true value and in general appears biased and frequently leaves certain parameters unidentified. Comparison of a series of <inline-formula><mml:math id="inf181"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.95</mml:mn><mml:mo>,</mml:mo><mml:mn>0.6</mml:mn><mml:mo>,</mml:mo><mml:mn>0.2</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>-HDCIs as functions of <inline-formula><mml:math id="inf182"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> for each parameter of the rate matrix obtained by the KF (green) and the RE algorithm (blue). The HDCIs correspond to the PC data displayed in <xref ref-type="fig" rid="fig7">Figure 7b and d</xref> . The first row corresponds to three rates <inline-formula><mml:math id="inf183"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the second row to the equilibrium constants <inline-formula><mml:math id="inf184"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. All parameters are normalized by their true value. <inline-formula><mml:math id="inf185"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>25</mml:mn></mml:msub></mml:math></inline-formula> is because of the microscopic-reversibility prior a parameter which is strongly dependent on the other rates and ratios. Refer to the caption of <xref ref-type="fig" rid="fig7">Figure 7</xref> for details about the prior that enforces microscopic-reversibility. Thus, the deviations of <inline-formula><mml:math id="inf186"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>25</mml:mn></mml:msub></mml:math></inline-formula> are inherited from the other parameters. The rate <inline-formula><mml:math id="inf187"><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>54</mml:mn></mml:msub></mml:math></inline-formula> is frequently not identified by the RE approach and only the limits of the sampling box confines he posterior.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig9-v3.tif"/></fig><p>To assess the location of the posterior conditioned on <inline-formula><mml:math id="inf188"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>, we select the median vector <inline-formula><mml:math id="inf189"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> of the marginal posteriors and calculate its Euclidean distance to the true values by:<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mrow><mml:mtext>Euclidean Error</mml:mtext><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:munder><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>true</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>This defines a single value to judge the overall accuracy of the posterior. Varying <inline-formula><mml:math id="inf190"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mtext>op</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> reveals the range of the validity (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) of the algorithm (red) from <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> . While both stochastic approaches are nearly equivalent for low open-channel noise, the RE (blue) performs consistently poorer. It may seem surprising that even for <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> the two stochastic algorithms start to produce different results. But considering the scaling (Materials and methods <xref ref-type="disp-formula" rid="equ51">Equation 46a</xref>) of the total open-channel noise (top axis) from currents of an ensemble patch <inline-formula><mml:math id="inf192"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>open</mml:mi><mml:mo>,</mml:mo><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>0.5</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>open</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> one sees that if <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>open</mml:mi><mml:mo>,</mml:mo><mml:mi>max</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>0.5</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>open</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> approaches σ the traditional KF suffers from ignoring state dependent noise contributions. The lower scale changes with experiments (e.g. <inline-formula><mml:math id="inf194"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf195"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>). In contrast, the upper scale is largely independent of the particular measurements. The two different normalizations indicate an experimental intuition: “ Why should I consider the extra noise from the open state of the single channel if only <inline-formula><mml:math id="inf196"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow><mml:mo>≈</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>” is misleading. The small advantage of our algorithm for small <inline-formula><mml:math id="inf197"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula> over <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> is due to the fact that we could apply an informative prior in the formulation of the inference problem on <inline-formula><mml:math id="inf198"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>exp</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>normal</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>exp</mml:mi><mml:mo>,</mml:mo><mml:mi>true</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>exp</mml:mi><mml:mo>,</mml:mo><mml:mi>true</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⋅</mml:mo><mml:mn>0.01</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> by taking advantage of our generalization (<xref ref-type="disp-formula" rid="equ51">Equation 46a</xref>) Bayesian filter. Further, <xref ref-type="fig" rid="fig3">Figure 3b</xref> indicates the importance that the functional form of the likelihood is flexible enough to capture the second order statistics of the noise of the data sufficiently.</p><p>For an increasing data quality, which in our benchmark is an increasing <inline-formula><mml:math id="inf199"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> per trace, we show (<xref ref-type="fig" rid="fig3">Figure 3c</xref>) that the deterministic RE and our Bayesian filter are consistent estimators, that is they converge in distribution to the true parameter values with their posterior maxima or median for increasing data quality. The scaling of the RE approach (blue) and our Bayesian filter (green) vs. <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> shows that for large <inline-formula><mml:math id="inf201"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> both algorithms seem to have a constant error ratio relative to each other. They are both well described by <inline-formula><mml:math id="inf202"><mml:mrow><mml:mrow><mml:mi>error</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∝</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> with an error ratio computed from the fit of 4.4. Thus, although our statistical model is singular (meaning that the fisher information matrix is singular <xref ref-type="bibr" rid="bib104">Watanabe, 2007</xref>), its asymptotic learning behaviour is similar to a regular model (<xref ref-type="fig" rid="fig4">Figure 4c</xref>) which, however, means that the euclidean error from both algorithms stays different also for large <inline-formula><mml:math id="inf203"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. For data with <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> the samples from the posterior typically indicate that the posterior is improper which is defined as<disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mrow><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></disp-formula></p><p>We consider this as the case of unidentified parameters. This data-driven definition is in so far different from structural and practical identifiability definitions (<xref ref-type="bibr" rid="bib65">Middendorf and Aldrich, 2017a</xref>; <xref ref-type="bibr" rid="bib66">Middendorf and Aldrich, 2017b</xref>) as the two latter cases are not distinguished. Still the practical consequence of structural or practical unidentifiability, which is usually an improper posterior, is captured. Cases of structural or practical unidentifiability which lead to a confined region of constant posterior density will be considered identified as the posterior is still normalizable thus the uncertainty quantification will still be correct, even when this finding is not sufficient to answer the research question at hand.</p></sec><sec id="s2-4"><title>Benchmarking for cPCF data against the gold standard algorithm</title><p>For the simulated time traces with an optimistically high signal-to-noise assumption, the posterior of the KF (from hereon KF denotes our Bayesian Filter) and a RE (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>) approach are compared for cPCF data (<xref ref-type="fig" rid="fig4">Figure 4a–d</xref>). For a brief introduction of the RE approach, see Appendix 8 . The failure to analyze PC data with moderate open-channel noise (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>; <xref ref-type="fig" rid="fig3">Figure 3a</xref>) disqualifies the classical KF with its constant noise variance also as a useful algorithm for fluorescence data, because here the Poisson distribution of the signal generates an even stronger state dependency of the signal variance.</p><p>By “high signal-to-noise assumption” , we refer to an experimental situation with a standard deviation of the current recordings <inline-formula><mml:math id="inf205"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>ex</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>, a low additional <inline-formula><mml:math id="inf206"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>, and a high mean photon rate per bound ligand and frame <inline-formula><mml:math id="inf207"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>. Additionally, we assume vanishing fluorescence background noise generated by the ligands in the bulk. The benefit of the high signal-to-noise is that the limitations of the two different approximations to the stochastic process of binding and gating can be investigated without running into the risk of being compensated or obscured by the noise from the experimental setup. For these experimental settings (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), we calculate the Euclidean distance of the median (<xref ref-type="disp-formula" rid="equ1">Equation 15</xref>) for different <inline-formula><mml:math id="inf208"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. For <inline-formula><mml:math id="inf209"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (gray shaded area in <xref ref-type="fig" rid="fig4">Figure 4a</xref>), the Euclidean error of both algorithms is roughly the same. On the single parameter level (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), this can be seen as an onset of correlated deviations from the true value for both algorithms. Each marginal posterior has for each <inline-formula><mml:math id="inf210"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> a similar deviation in magnitude and direction. That is in particular true for <inline-formula><mml:math id="inf211"><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>32</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf212"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>32</mml:mn></mml:msub></mml:math></inline-formula> which dominate <xref ref-type="disp-formula" rid="equ16">Equation 15</xref> . In spite of the correlation in direction of the errors of <inline-formula><mml:math id="inf213"><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>21</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf214"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>21</mml:mn></mml:msub></mml:math></inline-formula> their magnitude is still smaller for the KF. In summary, this indicates that in this regime the approximations to the involved multinomial distributions fail in a similar manner for both algorithms. That implies that treating the autocorrelation of the gating and binding becomes similar important compared to the error induced by normal approximations (which are used by the KF and the RE approach). For larger <inline-formula><mml:math id="inf215"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>, the Euclidean error of the RE is on average 1.6 times larger than the corresponding error of the posterior mode of the KF, which we deduce by fitting the function <inline-formula><mml:math id="inf216"><mml:mrow><mml:mrow><mml:mi>error</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt></mml:mfrac></mml:mrow></mml:math></inline-formula>. On the one hand, both algorithms are better in approaching the true values than with patch-clamp data alone. On the other hand, the smaller error ratio means, that adding a second observable constrains the posterior, such that much of the overfitting is prevented for the RE approach. By overfitting, we define the adaptation of any inference algorithm to the specific details of the used data set due to experimental and intrinsic noise which is aggravated if too complex kinetic schemes are used. Similarly, (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>) showed that the over fitting tendency of the RE can be reduced if the autocorrelation of the data is eliminated. The dotted green curve derives from PC data. The Euclidean error is roughly an order of magnitude larger for <inline-formula><mml:math id="inf217"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, in this regime the cPCF data set is equivalent to 10<sup>2</sup> fold more time traces or 10<sup>2</sup> more <inline-formula><mml:math id="inf218"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> in a similar PC data set. For <inline-formula><mml:math id="inf219"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>2000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> only cPCF establishes parameter identifiability (given a data set of 10 ligand concentrations and no other prior information). In <xref ref-type="fig" rid="fig4">Figure 4b(1-6)</xref>, we demonstrate the 0.95-HDCI (<xref ref-type="disp-formula" rid="equ15">Equation 14</xref>) of all parameters and their modes vs. <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. Even though the Bayesian filter and the RE approach are both consistent estimators, the RE approach covers the true values with its 0.95-HDCI only occasionally. The modeling assumption of the RE approach of treating each data point as if it does not come from a Markov process but from an individual draw from a multinomial distribution with deterministically evolving mean and variance makes the parameter estimates overly confident (<xref ref-type="fig" rid="fig4">Figure 4b(1-6)</xref>) . A likely explanation can be found by analyzing the extreme case where data points are sampled at high frequency relative to the time scales of the channel dynamics. The RE approach treats each data point as a new draw from <xref ref-type="disp-formula" rid="equ81">Equation 67</xref> while in reality the ion channel ensemble had no time to evolve into a new state. In contrast, the KF updates its information about the ensemble state after incorporating the current data point and then predicts from this updated information the generalised multinomial distribution of the next data point. For <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the marginal posterior of the KF usually contains the true value. Nevertheless, one might depict a bias in both algorithms, in particular (<xref ref-type="fig" rid="fig4">Figure 4b</xref> 2,4) for <inline-formula><mml:math id="inf222"><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>32</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf223"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>32</mml:mn></mml:msub></mml:math></inline-formula> for <inline-formula><mml:math id="inf224"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>ch</mml:mtext></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, similar to the findings of <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> . A proper investigation of bias can be found in Figure 11 and 12 and in the Appendix. Notably, with the more realistic higher experimental noise level, in those tests the bias is hardly or not all detectable (consider the unfiltered or infinitely fast integrated data). A plausible explanation is that the bias only occurs (<xref ref-type="fig" rid="fig4">Figure 4</xref> 2,4) because the data are that perfect that the discrete nature of the ensemble dynamics is almost visually detectable, thus deviating from to the modeling assumption of multi-variate normal distributions.</p><p>To investigate the six one-dimensional 0.95-HDCIs simultaneously, we declare the analysis of a data set as successful if all 0.95-HDCIs include the true values. Otherwise we define it as a failure. This enables to determine the true probability at which the probability mass of the KF and the RE approach covers the true values in a binomial setting. The left blue vertical line in <xref ref-type="fig" rid="fig4">Figure 4c</xref> indicates <inline-formula><mml:math id="inf225"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>0.95</mml:mn><mml:mn>6</mml:mn></mml:msup><mml:mo>≈</mml:mo><mml:mn>0.735</mml:mn></mml:mrow></mml:math></inline-formula> which is the lower limit and which would be the true success probability for an ideal model whose six 0.95-HDCIs are drawn from <inline-formula><mml:math id="inf226"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>binomial</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.95</mml:mn><mml:mo>,</mml:mo><mml:mn>6</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This is the probability of getting 6 successes in 6 trials. The right blue vertical line equals <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn></mml:mrow></mml:math></inline-formula>, signifying the upper limit obtained by treating the six <inline-formula><mml:math id="inf228"><mml:mrow><mml:mn>0.95</mml:mn><mml:mo>-</mml:mo></mml:mrow></mml:math></inline-formula> HDCIs as being drawn from <inline-formula><mml:math id="inf229"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>binomial</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.95</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> each, which is a rather loose approximation. All marginal distributions are computed from the same high-dimensional posterior which is formed by one data set for each trial. Thus, the six <inline-formula><mml:math id="inf230"><mml:mrow><mml:mn>0.95</mml:mn><mml:mo>-</mml:mo></mml:mrow></mml:math></inline-formula> HDCIs <inline-formula><mml:math id="inf231"><mml:mrow><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>binomial</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.95</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> must have success rates between those two extremes if the algorithm creates an accurate posterior. We next combine the binomial likelihood with the conjugated beta prior (<xref ref-type="bibr" rid="bib45">Hines et al., 2014</xref>) for mathematical convenience. On this occasion, for the sake of the argument, <inline-formula><mml:math id="inf232"><mml:mrow><mml:mi>beta</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> seems sufficient. A <inline-formula><mml:math id="inf233"><mml:mrow><mml:mi>beta</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> prior is a uniform prior on the open interval <inline-formula><mml:math id="inf234"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>. The estimated true success rate of the RE approach (blue) is <inline-formula><mml:math id="inf235"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula> and therefore far away from the success probability an algorithm should have when it is based on an exact likelihood of the data. In contrast, the posterior (green) of the true success probability of the KF resides with a large probability mass between the lower and upper limit of the success probability of an optimal algorithm (given the correct kinetic scheme). As both algorithms use the same prior distribution, the different performance is not induced by the prior.</p><p>Exploiting six one-dimensional posterior distributions does not necessarily answer whether the posterior is accurate in 6 dimensions but we can refine the used binomial setting. In <xref ref-type="fig" rid="fig4">Figure 4d</xref> <inline-formula><mml:math id="inf236"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>32</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>43</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we see that 2-D marginal distributions can, due to their additional degree of freedom, twist around the true value without covering it with HDCV (<xref ref-type="disp-formula" rid="equ15">Equation 14</xref>) of reasonable size while simultaneously the two 1–D marginal distribution do cover it with a reasonable HDCI. In general, the KF posterior distribution has its mode much closer to the true value for various parameter combinations and it seems that the posterior is approximately multivariate normal. Further, we recognize that the probability mass of the reasonably sized HDCV of the KF posterior includes the true values whereas the HDCV from the RE does not. In 6 dimensions we lack visual representations of the posterior. Since we showed that both algorithms are consistent for a given identifiable model, we are looking for a way to ask whether the posterior is accurate (has the posterior distribution the right shape). We can answer that question by asking, how much probability mass around the mode (or around multiple modes) needs to be counted to construct a HDCV <xref ref-type="disp-formula" rid="equ15">Equation 14</xref> which includes the true values. Then we can ask for <inline-formula><mml:math id="inf237"><mml:msub><mml:mi>N</mml:mi><mml:mi>set</mml:mi></mml:msub></mml:math></inline-formula> data sets how often did we find the true values inside a volume <inline-formula><mml:math id="inf238"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of a specific probability mass <inline-formula><mml:math id="inf239"><mml:mi>P</mml:mi></mml:math></inline-formula> of the posterior distribution<disp-formula id="equ18"><label>(17)</label><mml:math id="m18"><mml:mrow><mml:mi>success</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi>binomial</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>set</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>.</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>An algorithm which estimates the parameters of the true process should fulfill this property simultaneously to being consistent. Otherwise credibility volumes or confidence volumes are meaningless. Noteworthy, that this is a empirical test of how sufficient the Bayesian filter and the RE approach hold frequentist coverage property of their HDCVs (<xref ref-type="bibr" rid="bib82">Rubin and Schenker, 1986</xref>). We explain (Appendix 8) in detail how to quantify the overall shape and <inline-formula><mml:math id="inf240"><mml:mi>n</mml:mi></mml:math></inline-formula>-dimensional posterior and comment on its geometrical meaning. One way is to use an analytical approximation via the cumulative Chi-squared distribution (<xref ref-type="fig" rid="fig5">Figure 5a and b</xref>), The other way is to count the probability mass of <inline-formula><mml:math id="inf241"><mml:mi>n</mml:mi></mml:math></inline-formula>-dimensional histogram bins starting with the highest value until the first bin includes the true values (<xref ref-type="fig" rid="fig5">Figure 5b</xref>).</p><p>Knowing how much volume/probability mass is needed to include the true rate matrix allows us to test whether all HDCVs constructed from the two probability distributions match the binomial distributions of the ideal model. For each data set and for each HDCV of a fixed probability mass, there are two possible outcomes: The true rate matrix is inside or outside of that volume. For a chosen HDCV with a fixed probability volume, as indicated by a gray space in <xref ref-type="fig" rid="fig5">Figure 5b</xref> , we count how many times the true matrix is included in the volume of that probability mass for each trail in a fixed amount of trials. Since the success is binomially distributed, we plot the expected mean of a perfect model <inline-formula><mml:math id="inf242"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>trials</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>true</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and binomial quantiles and compare them with the success rate found in our test runs (<xref ref-type="fig" rid="fig5">Figure 5c</xref>) for both algorithms with both methods to determine the posterior shape. The posterior of the KF distributes the probability mass in a consistent manner such that each volume includes the true rate matrix within the quantile range. In contrast, the RE approach fails for all data sets for all HDCVs (from 0 – 0.95 probability mass) and does not include the true values in one single case. Note, that all the binomial trials for each HDCV are made from the same set of data sets which explains the correlated deviation from the mean. For lower but realistic signal to noise ratios, where the fit quality decreases, for example by producing larger errors/wider posterior distributions (<xref ref-type="fig" rid="fig6">Figure 6a</xref>), the statistics of the HDCV from the RE approach improve but are still outperformed by the KF. In particular, in our tested case of realistic experimental noise we never find the true values within a 0.65-HDCV if the data are analyzed with a RE approach. Even for the highest noise level (<xref ref-type="fig" rid="fig6">Figure 6b</xref>), the probability mass of the KF posterior needed to include the true rate matrix remains almost always smaller then the posterior mass of the RE approach. That means that the posterior mass of the KF is much closer to the true value distributed than the posterior mass of the RE. With the KF we find the true rate matrix for one data set in small volume <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> around the mode. To achieve the same with the RE approach we need at least a probability mass of 0.3.</p><p>In the inset of <xref ref-type="fig" rid="fig6">Figure 6a and b</xref> we do not observe a trend, thus no indication that the RE approach has a better performance for large values <inline-formula><mml:math id="inf244"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> in this regard. This challenges the common argument that the RE approach should be equivalent to the KF for large <inline-formula><mml:math id="inf245"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> because the ratio of mean signal vs. the intrinsic binding and gating noise is so large. Thus, including the autocorrelation into the analysis is important even for unrealistic large <inline-formula><mml:math id="inf246"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. One possible explanation is model a signal-to-noise ratio which scales <inline-formula><mml:math id="inf247"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. From the multinomial distribution both algorithms inherit mean signals which scale <inline-formula><mml:math id="inf248"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and variances which scale in the terms dominating for large <inline-formula><mml:math id="inf249"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> similarly with <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Thus, identical to the real signal, both algorithms model the scaling of the signal-to-noise ratio <inline-formula><mml:math id="inf251"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:msqrt><mml:mi>N</mml:mi></mml:msqrt></mml:mrow></mml:math></inline-formula>. It is plausible, that both algorithms remain sensitive for the occurrence of autocorrelation of the noise even for largest signal-to-noise ratios. In <xref ref-type="fig" rid="fig5">Figure 5c</xref> we compare the Euclidean error of the pessimistic high white noise case with an over-optimistic low noise case. We see, that when increasing <inline-formula><mml:math id="inf252"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> there is a regime where the Euclidean error increases faster than <inline-formula><mml:math id="inf253"><mml:msup><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> which we indicate with a coarse approximate fit <inline-formula><mml:math id="inf254"><mml:mrow><mml:mi/><mml:mo>∝</mml:mo><mml:msubsup><mml:mi>N</mml:mi><mml:mi>ch</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:math></inline-formula>. In that regime two effects happen simultaneously. First, the mean and the intrinsic fluctuations of the signal become more and more dominant over the experimental noise. Second, the standard deviation of intrinsic fluctuations becomes smaller relative to the mean signal. We speculate, that this produces together a learning rate which is faster than the usual asymptotic learning rate <inline-formula><mml:math id="inf255"><mml:msup><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> of a regular model but relaxes asymptotically towards <inline-formula><mml:math id="inf256"><mml:msup><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p></sec><sec id="s2-5"><title>Statistical properties of both algorithms for more complex models</title><p>We have seen in <xref ref-type="fig" rid="fig3">Figure 3c</xref> and <xref ref-type="fig" rid="fig4">Figure 4a</xref> that the RE and the KF algorithm are consistent estimators, while their error ratio (<xref ref-type="fig" rid="fig7">Figure 7a</xref>) seems to have no trend to approach 1 with increasing <inline-formula><mml:math id="inf257"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. Adding a second observable increases parameter accuracy and adds identifiability for both algorithms since less aspects of the dynamics need to be statistically inferred (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). Furthermore, the second observable takes away much of the tendency (compare <xref ref-type="fig" rid="fig4">Figure 4b</xref> 1 – 6 with 8) of the RE approach to overinterpret (overfit) which leads to a shrinking of the error ratio <inline-formula><mml:math id="inf258"><mml:mrow><mml:mn>5.6</mml:mn><mml:mo>±</mml:mo><mml:mn>1.4</mml:mn></mml:mrow></mml:math></inline-formula> for PC data to smaller values for cPCF data (<xref ref-type="fig" rid="fig7">Figure 7a</xref>) (red) which are on average still bigger than one, while the Euclidean error is reduced (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). If we then keep the amount and quality of the PC/cPCF data but increase the complexity of the model which produced the data (<xref ref-type="fig" rid="fig7">Figure 7b and d</xref>) from a four-state to a five-state model (see kinetic schemes above <xref ref-type="fig" rid="fig7">Figure 7a–c</xref>), we see that for cPCF data the error ratio stays roughly the same (difference between <xref ref-type="fig" rid="fig7">Figure 7a and b</xref>). For PC data instead both algorithms deliver an unidentified <italic>k</italic><sub>21</sub> for <inline-formula><mml:math id="inf259"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>≦</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (defined as an improper posterior). For larger <inline-formula><mml:math id="inf260"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> the KF always identifies all parameters while the RE fails at <inline-formula><mml:math id="inf261"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>7000</mml:mn><mml:mo>,</mml:mo><mml:mn>2000</mml:mn><mml:mo>,</mml:mo><mml:mn>75000</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to identify <italic>k</italic><sub>54</sub>. Thus, the KF reduces the risk of unidentified parameters. To calculate the mean error ratio, we exclude the values were some of the parameters are unidentified in total that still amounts to <inline-formula><mml:math id="inf262"><mml:mrow><mml:mn>6.8</mml:mn><mml:mo>±</mml:mo><mml:mn>2.7</mml:mn></mml:mrow></mml:math></inline-formula> thus the advantage of the KF (given all parameters are identified) might increase with model complexity for PC data. The lowest Euclidean error for this kinetic scheme has cPCF data analyzed with the KF. (<xref ref-type="fig" rid="fig7">Figure 7d</xref>). A 6-state-1-open-states model with cPCF data has again an error ratio of the the usual scale (<xref ref-type="fig" rid="fig7">Figure 7c</xref>). As expected, the Euclidean error continuously increases with model complexity (<xref ref-type="fig" rid="fig7">Figure 7d and e</xref>). For PC data of the 6-state-1-open-states model even the likelihood of the KF is that weak (<xref ref-type="fig" rid="fig7">Figure 7e</xref>) that it delivers unidentified parameters even for and we can detect heavy tailed distributions up until . Using RE on PC data alone does not lead to parameter identification, thus no error ratio can be calculated.</p><p>Consistent with our findings, fluorescence data itself, should lower the advantage of the KF compared to PC data simply by signal-to-noise arguments. The stochastic aspect of the ligand binding is usually more dominated by the noise of Photon counting and background noise than the stochastic gating is dominated in current data by experimental noise. In terms of uncertainty quantification the advantage of the KF with cPCF varies with the model complexity (see, Appendix 9).</p><p>Besides analyzing what causes the changes in the Euclidean error (<xref ref-type="fig" rid="fig7">Figure 7a and b</xref>) at the single parameter, we now investigate whether the posterior is a proper representation of uncertainty. Thus, we look back at the HDCIs. The HDCIs of the 4-state-1-open-state (<xref ref-type="fig" rid="fig8">Figure 8</xref>) of the PC data from <xref ref-type="fig" rid="fig3">Figure 3</xref> reveal an exacerbated over-confidence problem of the RE approach (blue) compared to cPCF-data (<xref ref-type="fig" rid="fig4">Figures 4b1</xref>—<xref ref-type="fig" rid="fig6">6</xref>). This, underlines our conclusion of <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref> that the Bayesian posterior sampled by the RE approach is misshaped. As a consequence a confidence volume derived from the curvature at the ML estimate of the RE algorithm understates parameter uncertainty. A possible way for ML methods to derive correct uncertainty quantification is by using bootstrapping data methods (<xref ref-type="bibr" rid="bib54">Joshi et al., 2006</xref>). Furthermore, the error ratios of each single parameter from its true value in the last column <inline-formula><mml:math id="inf263"><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>43</mml:mn></mml:msub></mml:math></inline-formula><inline-formula><mml:math id="inf264"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>43</mml:mn></mml:msub></mml:math></inline-formula> strongly increased their magnitudes (insets <xref ref-type="fig" rid="fig8">Figure 8</xref>). Even error ratios of <inline-formula><mml:math id="inf265"><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> are possible. Note, that the way we defined <xref ref-type="disp-formula" rid="equ16">Equation 15</xref> suppresses the influence of the smaller parameter errors in the overall error ratio. Thus the advantage (error ratio) of the KF over RE approach for a single parameter can be much larger or lower compared to the error ratio derived from the Euclidean error if the respective parameter is contributing less to the Euclidean error. The posterior of the KF (green) seems to be unbiased after the transition into the regime <inline-formula><mml:math id="inf266"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> where all parameters are identified. Similarly, for the RE algorithm there is no obvious bias in the inference. If we use the RE algorithm and change from the four-state to the five-state model (PC data from <xref ref-type="fig" rid="fig7">Figure 7b</xref>), bias occurs (<xref ref-type="fig" rid="fig9">Figure 9</xref>) in many inferred parameters, even for the highest <inline-formula><mml:math id="inf267"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> investigated. <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> showed that one or the reason of the biased inference of the RE approach is its ignorance of autocorrelation of the intrinsic noise. We add here that the bias problem clearly aggravates with an increased model complexity. It is even present in unrealistically large patches which in principle could be generated by summing up 10<sup>2</sup> time traces with <inline-formula><mml:math id="inf268"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. In contrast, the KF algorithm reveals that its parameter inference is either unbiased or at least much less biased in the displayed <inline-formula><mml:math id="inf269"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> regime. Furthermore, for both algorithms the position of the HDCI relative to the true value is for some parameters highly correlated, which corresponds to the correlation between optima of the ML method of <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> ; <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>.</p><p>As a side note, unbiased parameter estimates are a highly desirable feature of an inference algorithm. For example, with a bias in the inference, repeated experiments do not lead to the true value if the arithmetic mean of the parameter inferences is taken. With bias even bootstrapping methods fail to produce reliable uncertainty quantification. Due to the variation of the data the <italic>k</italic><sub>54</sub> parameter is either identified in some neighbourhood of the true value or complete unidentified (<xref ref-type="fig" rid="fig9">Figure 9</xref>), if the RE algorithm is used. The unidentified <italic>k</italic><sub>54</sub> occurs even at high-quality data such as <inline-formula><mml:math id="inf270"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>7.5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Only because of the nonphysical prior (<xref ref-type="fig" rid="fig9">Figure 9</xref>) of <italic>k</italic><sub>54</sub> induced by the limits of the sampling box of the sampling algorithm the posterior appears to be proper but is in fact either unidentified or or more than two orders of magnitude away from the true value. For the same data using the KF did not result in any unidentified parameters. Note, that comparable inference pathologies such as multimodal distributions of inferred parameter were also reported for the maximum likelihood RE algorithm for low quality PC data or too simple stimulation protocols (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>).</p><p>In conclusion, the two different perspectives on parameter uncertainty: On the one hand distributions of ML estimates due to the random data (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> ; <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>) and the Bayesian posterior distribution loose their tightly linked (and necessary) connection if the RE algorithm is used. Thus, KF robustifies also ML inferences of the rate matrix. Our findings are consistent with the findings for gene regulatory networks (<xref ref-type="bibr" rid="bib39">Gillespie and Golightly, 2012</xref>) which show that RE approaches deliver a too narrow posterior in contrast to stochastic approximations which deliver an acceptable posterior compared to the true posterior (defined by a particle filter algorithm). On the data side of the inference problem adding cPCF data eliminates the bias, reduces the variance of the position of the HDCI and eliminates unidentified parameters (<xref ref-type="fig" rid="app9fig1">Appendix 9—figures 1</xref> and <xref ref-type="fig" rid="app9fig2">2</xref>) for both investigated algorithms. This advantage increases with model-complexity.</p><p>For the five-state and six-state model, we applied microscopic-reversibility (<xref ref-type="bibr" rid="bib21">Colquhoun et al., 2004</xref>). We enforced it by hierarchical prior distribution (Materials and methods <xref ref-type="disp-formula" rid="equ71">Equation 60</xref>) whose parameters can be chosen such that they allow only arbitrarily small violations of microscopic-reversibility. But the prior distribution can also be used to enforce some softer regularization around microscopic-reversibility. Thus, we can transfer the usually strictly applied algebraic constraint (<xref ref-type="bibr" rid="bib84">Salari et al., 2018</xref>) of microscopic-reversibility to a constraint with scalable softness. In that way we can model the lack of information if microscopic-reversibility is exactly fulfilled (<xref ref-type="bibr" rid="bib21">Colquhoun et al., 2004</xref>) by the given ion channel instead of enforcing the strict constraint upon the model.</p></sec><sec id="s2-6"><title>Prior critique and model complexity</title><p>In the Bayesian framework, the likelihood of the data and the prior generate the posterior. Thus, the performance of both algorithms can be influenced by appropriate prior distributions. We used a uniform prior over the rate matrix which is not optimal. Note, that uniform priors are widely used by several reasons. They appear to be unbiased, and are assumed to be a ‘no prior’ option (which they are not). This is true for location parameters like mean values. In contrast, for other parameters, such as scaling parameters like rates or variances, a uniform prior adds bias to the inference towards faster rates (<xref ref-type="bibr" rid="bib109">Zwickl and Holder, 2004</xref>). We suspect, that for the PC data even in the simplest model discussed here the lower data quality limit below which we detected unidentified parameters (improper posteriors) is caused by the uniform prior. This lower limit for the KF also increases with the complexity of the model from <inline-formula><mml:math id="inf271"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> for the foue-state model till <inline-formula><mml:math id="inf272"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>≦</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for 6-state-1-open-state model. Note, that it is hardly possible to fit the 6-state-1-open-state model with the RE approach for the same amount of PC data. We observe cPCF data eases this problem because the likelihood becomes more concentrated for all parameters. The likelihood dominates the uniform prior. Nevertheless, for most parts of the paper we used a uniform prior over the rates and equilibrium constants to be comparable with the usual default method: a plain ML which influences our results in data regimes in which the data is not strong enough to dominate the bias from the uniform prior. Thus, both algorithms perform better with smarter informative or at least unbiased prior choices for the rate matrix.</p><p>In principle, to rule out an influence of the prior, unbiased priors should be used for the rates. The standard concept for unbiased least informative priors is to construct a Jeffreys prior <xref ref-type="bibr" rid="bib53">Jeffreys, 1946</xref> for the rate matrix which is, however, beyond the scope of the paper.</p></sec><sec id="s2-7"><title>The influence of the brightness of the ligands of cPCF data on the inference</title><p>To evaluate the advantage of cPCF data <xref ref-type="bibr" rid="bib6">Biskup et al., 2007</xref> with respect to PC data only (<xref ref-type="fig" rid="fig10">Figure 10</xref>), we compare different types of ligands: Idealized ligands with brightness <inline-formula><mml:math id="inf273"><mml:msub><mml:mi>λ</mml:mi><mml:mtext>b</mml:mtext></mml:msub></mml:math></inline-formula>, emitting light only when bound to the channels, ‘real’ ligands which also produce background fluorescence when diffusing in the bath solution (Appendix 5) and current data alone. For datasets including fluorescence, the increased precision for the dissociation rate of the first ligand, <inline-formula><mml:math id="inf274"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, is that strong that the variance of the posterior <inline-formula><mml:math id="inf275"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> nearly vanishes in the combined plot with the current data (nearly all probability mass is concentrated in a single point in <xref ref-type="fig" rid="fig10">Figure 10a</xref>). The effect on the error of the equilibrium constants <inline-formula><mml:math id="inf276"><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is less strong. Additionally, the bias is reduced and even the estimation of <inline-formula><mml:math id="inf277"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> is improved. The brighter the ligands are, the more the posterior of the rates decorrelates, in particular <inline-formula><mml:math id="inf278"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig10">Figure 10a</xref>). All median estimates of nine different cPCF data sets (<xref ref-type="fig" rid="fig10">Figure 10b</xref>) differ by less than a factor 1.1 from the true parameter except <inline-formula><mml:math id="inf279"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, which does not profit as much from the fluorescence data as <inline-formula><mml:math id="inf280"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig10">Figure 10c</xref>). The 95th percentiles, <italic>l</italic><sub>95</sub> of <inline-formula><mml:math id="inf281"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>K</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> follow <inline-formula><mml:math id="inf283"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>95</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus, with increasing magnitude of ligand brightness λ, the estimation of <inline-formula><mml:math id="inf284"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> becomes increasingly better compared to that of <inline-formula><mml:math id="inf285"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="fig" rid="fig10">Figure 10c</xref>). The posterior of the binding and unbinding rates of the first ligand contracts with increasing <inline-formula><mml:math id="inf286"><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:math></inline-formula>. The <italic>l</italic><sub>95</sub> percentiles of other parameters exhibit a weaker dependency on the brightness (<inline-formula><mml:math id="inf287"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>95</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:msup><mml:mi>λ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>). For <inline-formula><mml:math id="inf288"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> photons per bound ligand and frame, which corresponds to a maximum mean signal of 20 photons per frame, the normal approximation to the Poisson noise hardly captures the asymmetry of photon counting noise included in the time traces. Nevertheless, <italic>l</italic><sub>95</sub> decreases about ten times when cPCF data are used (<xref ref-type="fig" rid="fig10">Figure 10c</xref>). The estimated variance of<disp-formula id="equ19"><label>(18)</label><mml:math id="m19"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mfrac><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>with the mean predicted signal <inline-formula><mml:math id="inf289"><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, for PC or cPCF data is <inline-formula><mml:math id="inf290"><mml:mrow><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig10">Figure 10d</xref>) which means that the modeling predicts the stochastic process correctly up to the variance of the signal. Note that the mean value and covariance of the signal and the state form sufficient statistics of the process, since all involved distributions are approximately multivariate normal. The fat tails and skewness of <inline-formula><mml:math id="inf291"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>21</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>12</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> arises because the true model is too flexible for current data without further prior information. The KF allows to determine the variance (<xref ref-type="fig" rid="fig10">Figure 10e</xref>) of the open-channel current noise for <inline-formula><mml:math id="inf293"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Adding fluorescence data has roughly the same effect on the estimation of <inline-formula><mml:math id="inf294"><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub></mml:math></inline-formula> like using five times more ion channels to estimate <inline-formula><mml:math id="inf295"><mml:msubsup><mml:mi>σ</mml:mi><mml:mtext>op</mml:mtext><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>.</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>The Benchmark of the KF for PC versus cPCF data with different bright ligands shows that even adding a weak fluorescence binding signal can add enough information to identify before unidentified parameters.</title><p>(<bold>a</bold>) Posteriors of PC data (blue), cPCF data with <inline-formula><mml:math id="inf296"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.00375</mml:mn></mml:mrow></mml:math></inline-formula> (orange) and cPCF data with <inline-formula><mml:math id="inf297"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.375</mml:mn></mml:mrow></mml:math></inline-formula> (green). For the data set with <inline-formula><mml:math id="inf298"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.375</mml:mn></mml:mrow></mml:math></inline-formula>, we additionally accounted for the superimposing fluorescence of unbound ligands in solution. In all cases <inline-formula><mml:math id="inf299"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. The black lines represent the true values of the simulated data. The posteriors for cPCF <inline-formula><mml:math id="inf300"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are centered around the true values that are hardly visible on the scale of the posterior for the PC data. The solid lines on the diagonal are kernel estimates of the probability density. (<bold>b</bold>) Accuracy and precision of the median estimates visualized by a violin plot for the parameters of the rate matrix for 5different data sets. Four of the five data sets are used a second time with different instrumental noise, with <inline-formula><mml:math id="inf301"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.375</mml:mn></mml:mrow></mml:math></inline-formula> and superimposing bulk signal. The blue lines represent the median, mean and the maximal and minimal extreme value. (<bold>c</bold>) The 95th percentile of the marginalized posteriors vs. <inline-formula><mml:math id="inf302"><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:math></inline-formula> normalized by the true value of each parameter. A regime with <inline-formula><mml:math id="inf303"><mml:mrow><mml:msub><mml:mi>l</mml:mi><mml:mn>95</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:mi>λ</mml:mi></mml:msqrt></mml:mrow></mml:mrow></mml:math></inline-formula> is shown for <inline-formula><mml:math id="inf304"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <italic>K</italic><sub>1</sub>, while other parameters show a weaker dependency on the ligand brightness. (<bold>d</bold>) Histograms of the residuals <inline-formula><mml:math id="inf305"><mml:mi>r</mml:mi></mml:math></inline-formula> of cPCF with <inline-formula><mml:math id="inf306"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2.5</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> data and PC data. The randomness of the normalized residuals of the cPCF or PC data is well described by <inline-formula><mml:math id="inf307"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mtext>normal</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>res</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The estimated variance is <inline-formula><mml:math id="inf308"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>res</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.98</mml:mn><mml:mo>+</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Note that the fluorescence signal per frame is very low such that the normal approximation to Poisson counting statistics does not hold. e, Posterior of the open-channel noise <inline-formula><mml:math id="inf309"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>/</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>op</mml:mi><mml:mo>,</mml:mo><mml:mi>true</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for PC data with <inline-formula><mml:math id="inf310"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (green) and <inline-formula><mml:math id="inf311"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (blue) as well as for cPCF data with <inline-formula><mml:math id="inf312"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> (red) with <inline-formula><mml:math id="inf313"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mtext>b</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.375</mml:mn></mml:mrow></mml:math></inline-formula>. We assumed as prior for the instrumental variance <inline-formula><mml:math id="inf314"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p><supplementary-material id="fig10sdata1"><label>Figure 10—source data 1.</label><caption><title>Five different sets of time traces of panel b.</title><p>All instrumental noise is already added.</p></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig10-data1-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig10sdata2"><label>Figure 10—source data 2.</label><caption><title>Eighteen sets of time traces of panel c.</title><p>The number in the file name indicates the brightness.</p></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig10-data2-v3.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig10-v3.tif"/></fig></sec><sec id="s2-8"><title>Sensitivity towards filtering before the analog-to-digital conversion of the signal</title><p>On the one side, every analog signal to be digitized needs analog filtering for antialiasing according to the Nyquist theorem. On the other side, every analog filter does not only suppress unwanted white noise but also distorts the dynamics (<xref ref-type="fig" rid="fig11">Figure 11a</xref>) of the signal of interest (<xref ref-type="bibr" rid="bib90">Silberberg and Magleby, 1993</xref>). Therefore, (<xref ref-type="bibr" rid="bib78">Qin et al., 2000</xref>) recommend to avoid analog filtering as much as possible in single-channel analysis and let the HMM analyze the data in the rawest available form, even with simultaneous drift correction (<xref ref-type="bibr" rid="bib85">Sgouralis and Pressé, 2017a</xref>). One can also expect that analog filtering of a macroscopic signal is harmful for the inference of the KF and the RE approach. For the CCCO model considered herein we investigated the mean behavior (accuracy and precision) of the posterior of both algorithms with seven data sets (simulated at 100 kHz to mimic an analog signal). A digital fourth-order Bessel filter (<xref ref-type="bibr" rid="bib101">Virtanen et al., 2020</xref>) was then applied. The maximum analysing frequency <italic>f</italic><sub><italic>ana</italic></sub> of the KF used is <inline-formula><mml:math id="inf315"><mml:mrow><mml:mn>100</mml:mn><mml:mo>-</mml:mo><mml:mn>400</mml:mn></mml:mrow></mml:math></inline-formula> Hz to be comparable to cPCF setups. The slower frequency at which the Bayesian filter analyzes the data is necessary because the applied Bessel filter has caused additional time correlations in the originally white noise of the signal. Thus, an all-data-points fit would immediately violate the white noise assumption of <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> which we restore by analyzing at a much lower frequency. We then let the time scales of the induced time correlations become larger and larger by decreasing <italic>f</italic><sub><italic>cut</italic></sub>. Physically, the absolute cut-off frequency <italic>f</italic><sub><italic>cut</italic></sub> is irrelevant; what matters is the magnitude of <italic>f</italic><sub><italic>cut</italic></sub> relative to <italic>f</italic><sub><italic>ana</italic></sub> and to the eigenvalues <inline-formula><mml:math id="inf316"><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the ensemble (see, Appendix 3), since the eigenvalues determine the time evolution of the mean ensemble state, the autocorrelation, and Fourier spectrum of the fluctuations around the equilibrium distribution (<xref ref-type="bibr" rid="bib20">Colquhoun et al., 1997b</xref>). The eigenvalues depend on the ligand concentration such that for a four-state model for each ligand concentration there are three relevant time scales <inline-formula><mml:math id="inf317"><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf318"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>) plus the equilibrium solution which satisfies <inline-formula><mml:math id="inf319"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. For 10 different time series <inline-formula><mml:math id="inf320"><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:mn>10</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> the outcome is to have different values of <inline-formula><mml:math id="inf321"><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>.Each eigenvalue is the inverse of the time constant of an exponential decay (see, Appendix 3). For this reason, we normalize in the following (<xref ref-type="fig" rid="fig11">Figure 11</xref>) the cut-off frequencies by <inline-formula><mml:math id="inf322"><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> at the highest ligand concentration. We analyze the arithmetic mean from 7 different data sets of the median of the posterior of the rate matrix. The mean Euclidean error of the median (<xref ref-type="fig" rid="fig11">Figure 11b</xref>) and a series of quantiles demonstrate that overall the error of the mean median of the posterior KF (green) is smaller than that obtained by the RE. For unfiltered data, the accuracy of the mean median of the KF is increased by <inline-formula><mml:math id="inf323"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mn>1.6</mml:mn></mml:mrow></mml:math></inline-formula>. Based on the Euclidean error both algorithms benefit slightly from careful analog filtering for <inline-formula><mml:math id="inf324"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> while the offset remains rather constant. A strong negative effect of analog filtering starts for both algorithms around <inline-formula><mml:math id="inf325"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mi>kHZ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. This is induced by <inline-formula><mml:math id="inf326"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (see, Appendix 10). In contrast, based on the level of each individual parameter of the rate matrix (<xref ref-type="fig" rid="fig11">Figure 11c</xref> 1–6) the bias induced by analog filtering immediately starts with <inline-formula><mml:math id="inf327"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>70</mml:mn><mml:mo>⁢</mml:mo><mml:mi>kHz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig11">Figure 11c</xref> 1–3). Note, that visual inspection of the signal (<xref ref-type="fig" rid="fig11">Figure 11a</xref>) does not reveal signal distortions <inline-formula><mml:math id="inf328"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi>kHz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> though they are detected by both algorithms. For unfiltered data, the maximum of the posterior for the RE approach is a biased estimate <inline-formula><mml:math id="inf329"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ME</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≠</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>true</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> for at least the parameters <inline-formula><mml:math id="inf330"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>21</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>21</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>32</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> of the true value <inline-formula><mml:math id="inf331"><mml:msub><mml:mi>θ</mml:mi><mml:mi>true</mml:mi></mml:msub></mml:math></inline-formula>, which is explained (<xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref>) by the fact that RE approaches ignore the autocorrelation of the intrinsic noise. Additionally, the data indicate that for <inline-formula><mml:math id="inf332"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>43</mml:mn></mml:msub></mml:math></inline-formula> the maximum of the posterior is even for the KF a biased estimate which we interpret as limitations induced by the fact that the mean vector and covariance-matrix do not constitute sufficient statistics as soon as Poisson distributed photon counting or open-channel noise blurs the signal. For the RE approach, the additional bias induced by the analog filter on the mean maximum of all parameters of the posterior starts with <inline-formula><mml:math id="inf333"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>70</mml:mn></mml:mrow></mml:math></inline-formula> kHz or, in other words, at the fastest time scale in the whole data set. The total bias in the estimate is reduced for <italic>k</italic><sub>21</sub> with the additional bias from the analog filtering but increased for <italic>k</italic><sub>32</sub> which for the Euclidean error leads at first to a small increase in accuracy. The KF is more robust towards analog filtering, as the results alter less with <italic>f</italic><sub><italic>cut</italic></sub> (given a reasonable <italic>f</italic><sub><italic>cut</italic></sub>), and less biased for unfiltered data in the estimates of these parameters. On the one hand, the Euclidean error shrinks for <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> kHz (<xref ref-type="fig" rid="fig11">Figure 11b</xref>). On the other hand, on the single-parameter level (<xref ref-type="fig" rid="fig11">Figure 11c</xref> 1–6), the parameter estimates pick up bias due the analog filtering even for high filter frequencies, in particular for the RE approach. Only for <italic>k</italic><sub>43</sub> the KF is more biased than the RE approach.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>The KF is robust against moderate analog filtering of the current signal.</title><p>High (Bayesian) sampling frequencies and minimal analog filtering does minimize bias which otherwise deteriorates parameter identification. In order to mimic an analog signal before the analog-to-digital conversion we simulated seven different 100 kHz signals which were then filtered by a digital fourth-order (4 pole) Bessel filter. The activation curves were then analyzed with the Bayesian filter at 125 Hz and the deactivation curves at sampling rates between 166-500 Hz. We chose for the analog signal <inline-formula><mml:math id="inf335"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>exp</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf336"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, thus a stronger background noise, and we set the mean photon count per bound ligand as <inline-formula><mml:math id="inf337"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>. For the ensemble size we choose <inline-formula><mml:math id="inf338"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. (<bold>a</bold>) Current time trace filtered with different <italic>f</italic><sub><italic>cut</italic></sub>. Except for 100 Hz (red) the signal distortion is visually undetectable. Nevertheless, the invisible signal distortions from analog filtering are problematic for both algorithms. (<bold>b</bold>) Estimate of the distribution mean Euclidean error of the median of the posterior vs. the cut-off frequency of a 4 pole Bessel filter (upper scale is in units of kHz) or scaled to the channel time scale (lower scale, see text). The fastest two eigenvalues <inline-formula><mml:math id="inf339"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for the highest ligand concentration are indicated by the black vertical lines. The fastest ratios <inline-formula><mml:math id="inf340"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for the next smaller ligand concentration are indicated by the red vertical lines. The slowest eigenvalue ratio <inline-formula><mml:math id="inf341"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> at the highest ligand concentration is beyond the left limit of the x-axis. The solid line is the mean median of five data sets of the respective RE posterior (blue) and KF posterior (green). The green shaded area indicates the 0.6 quantile (ranging from the 20th percentile till the 80th percentile), demonstrating the distribution of the error of the posterior median due to the randomness of the data. (<bold>c</bold>) 1–3, Accuracy (bias) and precision of the maxima of the posterior <inline-formula><mml:math id="inf342"><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>max</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> of the posterior maxima of the rates vs. the cut-off frequency of a Bessel filter. The shaded areas indicate the 0.6 quantiles (ranging from the 20th percentile till the 80th percentile) due the variability among data sets while the error bars show the standard error of the mean. The deviation of the mean from the true value is an estimate of the accuracy of the algorithm while the quantile indicates the precision. (<bold>c</bold>) 4–6, Accuracy and precision of the maxima of the posterior <inline-formula><mml:math id="inf343"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mi>max</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> of the posterior maxima of the corresponding equilibria vs. the cut-off frequency of a Bessel filter.</p><p><supplementary-material id="fig11sdata1"><label>Figure 11—source data 1.</label><caption><title>Representative data set of cPCF data whose current has been filtered with decreasing <italic>f</italic><sub><italic>cut</italic></sub>.</title><p>In the name of each file is <italic>f</italic><sub><italic>cut</italic></sub> encoded in units of 100 kHz. The time axis is identical for all time traces and saved in the file Time.txt. The folder is similar to six other provided source data folders which have in total been used in this figure.</p></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig11-data1-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig11sdata2"><label>Figure 11—source data 2.</label><caption><title>Representative data set of cPCF data whose current has been filtered with decreasing <italic>f</italic><sub><italic>cut</italic></sub>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig11-data2-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig11sdata3"><label>Figure 11—source data 3.</label><caption><title>Representative data set of cPCF data whose current has been filtered with decreasing <italic>f</italic><sub><italic>cut</italic></sub>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig11-data3-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig11sdata4"><label>Figure 11—source data 4.</label><caption><title>Representative data set of cPCF data whose current has been filtered with decreasing <italic>f</italic><sub><italic>cut</italic></sub>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig11-data4-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig11sdata5"><label>Figure 11—source data 5.</label><caption><title>Representative data set of cPCF data whose current has been filtered with decreasing <italic>f</italic><sub><italic>cut</italic></sub>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig11-data5-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig11sdata6"><label>Figure 11—source data 6.</label><caption><title>Representative data set of cPCF data whose current has been filtered with decreasing <italic>f</italic><sub><italic>cut</italic></sub>.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig11-data6-v3.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig11-v3.tif"/></fig><p>The KF is the unique minimal variance Bayesian filter for a linear Gaussian process (<xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>) which means, given that the assumptions of the KF are fulfilled by the true process of interest, the KF is mathematically proven the best model-based filter to apply. Consequently, analog filtering does not provide an advantage unless it removes specific high frequency external noise sources (colored noise). We demonstrate (Appendix 10) this for PC data and varied <italic>f</italic><sub><italic>cut</italic></sub> and <italic>f</italic><sub><italic>ana</italic></sub>. On the downside, increasing <italic>f</italic><sub><italic>ana</italic></sub> makes the results of both algorithms more fragile if <inline-formula><mml:math id="inf344"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> does not hold. Thus, the critical edge in <xref ref-type="fig" rid="fig11">Figure 11b</xref> is indeed induced by <italic>f</italic><sub><italic>cut</italic></sub> approaching <italic>f</italic><sub><italic>ana</italic></sub>. This suggests that the white noise assumption of both algorithms is violated. On the upside, if <inline-formula><mml:math id="inf345"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is given, the KF with an order of magnitude higher <italic>f</italic><sub><italic>ana</italic></sub> has a reduced bias of up to 20% for <inline-formula><mml:math id="inf346"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula> for individual parameters compared to the KF with lower <italic>f</italic><sub><italic>ana</italic></sub>. Additionally, a higher <italic>f</italic><sub><italic>ana</italic></sub> reduces the variance. To reduce the bias of parameter estimates to a minimum, the experimental design offers two remedies, either doing cPCF experiments with additional discussed advantages or using the KF at a high <italic>f</italic><sub><italic>ana</italic></sub> with even much higher <italic>f</italic><sub><italic>cut</italic></sub>.</p><p>By theoretical grounds a further argument for doing less analog filtering is that this benchmark analyzes data of a finite state Markov process, which is a coarse proxy for the true process. In reality, relaxation of a protein is a high-dimensional continuous-state Markov process with infinitely many relaxation time scales (eigenvalues) (<xref ref-type="bibr" rid="bib29">Frauenfelder et al., 1991</xref>) which, however, might be grouped in slower experimentally accessible and non-accessible faster time scales (<xref ref-type="bibr" rid="bib75">Noé et al., 2013</xref>). With larger data sets of higher quality from better experiments, the faster time scales might become accessible if they are not distorted by analog filtering. In conclusion, deciding on a specific kinetic scheme and inferring its parameters means finding a model which accommodates in the best way to the set of observed eigenvalues. Analog filtering hampers the RE, KF or HMM forward-backward algorithm (<xref ref-type="bibr" rid="bib78">Qin et al., 2000</xref>) to correctly describe the faster time scales.</p></sec><sec id="s2-9"><title>Error due to finite integration time of fluorescence data</title><p>So far, we idealized the fluorescence data integration time as being instantaneously relative to the time scales of ensemble dynamics. In real experiments, the fluorescence signal of cPCF data has orders of magnitude longer minimal integration time <inline-formula><mml:math id="inf347"><mml:msub><mml:mi>T</mml:mi><mml:mi>int</mml:mi></mml:msub></mml:math></inline-formula> (time to record all voxels of a frame) or maximal integration frequency <inline-formula><mml:math id="inf348"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>int</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, than the possible sampling frequency of current recordings. We mimic the finite integration time<disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>digital</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>int</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>analog</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>start</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>int</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>by summing with a sliding window the 100 kHz signal including the white noise to obtain data at an effectively lower sampling frequency (<xref ref-type="fig" rid="fig12">Figure 12a</xref>). Additionally we set the Bessel filter for the current data to <inline-formula><mml:math id="inf349"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>4.59</mml:mn></mml:mrow></mml:math></inline-formula> or <inline-formula><mml:math id="inf350"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mpadded width="+1.7pt"><mml:mn>90</mml:mn></mml:mpadded></mml:mrow></mml:math></inline-formula> kHz. The fastest used analysing frequency is <inline-formula><mml:math id="inf351"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>500</mml:mn><mml:mo>⁢</mml:mo><mml:mi>Hz</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. We scale mean photo brightness <inline-formula><mml:math id="inf352"><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:math></inline-formula> and background noise down such that the signal-to-noise ratio of the lower integration frequency data is the same as of the high-frequency data <inline-formula><mml:math id="inf353"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>int</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> . We do that in order to separate the bias from the finite integration time from other effects such as a better signal to noise ratios for each integrated point. Note that we only analyzed the plot until <inline-formula><mml:math id="inf354"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Both algorithms incur very similar bias due to the finite integration time (<xref ref-type="fig" rid="fig12">Figure 12b</xref>). The KF (green) is more precise for high integration frequencies <inline-formula><mml:math id="inf355"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> until <inline-formula><mml:math id="inf356"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>≈</mml:mo><mml:mn>0.08</mml:mn></mml:mrow></mml:math></inline-formula> then the RE approach becomes more robust. Similar to Bessel-filtered current data (<xref ref-type="fig" rid="fig11">Figure 11b</xref>) on the single parameter level the systematic deviations start early for example <inline-formula><mml:math id="inf357"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> kHz for <italic>K</italic><sub>21</sub> (<xref ref-type="fig" rid="fig12">Figure 12c4</xref>). Possibly the systematic deviations start (<xref ref-type="fig" rid="fig12">Figure 12c2</xref>) already at <inline-formula><mml:math id="inf358"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula> kHz for <italic>k</italic><sub>32</sub>. The sudden increase of the Euclidean error (<xref ref-type="fig" rid="fig12">Figure 12b</xref>) of the mean median at <inline-formula><mml:math id="inf359"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>≈</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> occurs in this case not due to <italic>f</italic><sub><italic>int</italic></sub> approaching <italic>f</italic><sub><italic>cut</italic></sub> but due to <inline-formula><mml:math id="inf360"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>⪅</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> for many ligand concentrations. To show this we plot the results of the fitting of five different data sets without including the highest 4 ligand concentrations (red) which means the largest eigenvalues are much smaller (<xref ref-type="fig" rid="fig12">Figure 12b,C1-6</xref>). Additionally, we keep <inline-formula><mml:math id="inf361"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Although fluctuations of the posterior medians are higher, the KF becomes more robust. Note, that the fastest eigenvalues of these reduced data sets are indicated by the blue bars (<xref ref-type="fig" rid="fig12">Figure 12b and c4</xref>). Based on the Euclidean error (<xref ref-type="fig" rid="fig11">Figures 11a</xref> and <xref ref-type="fig" rid="fig12">12a</xref>) the robustness of both algorithms against the cut-off frequency is compared with the robustness against the integration frequency found to be about an order of magnitude higher. That is related to a specific detail of the model used: the binding reaction, corresponds to the fastest time scales of the overall dynamic (difference between <xref ref-type="fig" rid="fig1">Figure 1b and c</xref>), which is exposed by the fluorescence signal. Thus, kinetic analysis of any data should make sure that the corresponding frequency of the most dominant timescales of the time series are much slower than the respective <italic>f</italic><sub><italic>int</italic></sub><italic>f</italic><sub><italic>cut</italic></sub> independently of the investigated algorithms.</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Finite integration time of fluorescence recordings acts also as a filter.</title><p>Thus the sampling should be faster than the fastest eigenvalues to avoid biased results. We simulated five different 100 kHz cPCF signals. All forms of noise were added and then the fluorescence signal was summed up using a sliding window to account for the integration time to produce one digital data point. The activation curves were then analyzed with the Bayesian filter at 125 Hz and the deactivation curves at <inline-formula><mml:math id="inf362"><mml:mrow><mml:mn>166</mml:mn><mml:mo>-</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> kHz, see caption of <xref ref-type="fig" rid="fig8">Figure 8</xref>. We plot the 0.6-quantile (interval between the 20th and the 80th percentile) to mimic ±one standard deviation from the mean as well as the mean of the distribution of the maxima of the posterior for different data sets. (Note, this is not equivalent to the mean and quantiles of the posterior of a single data set.). The quantiles represent the randomness of the data while the error bars indicate the standard error of the mean maximum of the posterior. Blue (RE) and green (KF) indicate the two algorithms with the standard data set while red (KF) shows examples that use only the six smallest ligand concentrations for the analysis in order to limit the highest eigenvalues. a, Instantaneous probing of the ligand binding (blue) compared with a probing signal which runs at <inline-formula><mml:math id="inf363"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> kHz. The integrated brightness of the bound ligand is <inline-formula><mml:math id="inf364"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> photons per frame. Although the red curves seem like decent measurements of the process except for the highest two shown ligand concentrations, the mean error is roughly an order of magnitude worse than for <inline-formula><mml:math id="inf365"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> kHz. Note, that for visualization we plot at a higher frequency than the Kalman filter analyzed the data. b, Estimate of the distribution of the (Euclidean error of the mean median of the posterior) vs. the scaled integration frequency <inline-formula><mml:math id="inf366"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>int</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>integration</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We use integration frequency instead of the integration time to make the plot comparable to the Bessel filter plot. The solid line is the mean median of five data sets of the respective KF posterior (green, red) and RE posterior (blue). The shaded areas indicate the 0.6-quantile which visualizes the spread of the distribution of point estimates. The two fastest time scales (eigenvalues) at the highest ligand concentration are indicated by the vertical black lines, the time scales of the next lower ligand concentrations with the red vertical lines. c 1–3, Accuracy (bias) and precision of the maxima of the posterior <inline-formula><mml:math id="inf367"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> rates vs. the integration frequency. c 4–6, Accuracy and precision of the maxima of the posterior <inline-formula><mml:math id="inf368"><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> of the corresponding equilibria vs. the cut-off frequency of a Bessel filter.</p><p><supplementary-material id="fig12sdata1"><label>Figure 12—source data 1.</label><caption><title>The data folder contains one data set of cPCF data.</title><p>The fluorescence has been integrated with decreasing <italic>f</italic><sub><italic>int</italic></sub>. The number in the file name encodes the amount of <inline-formula><mml:math id="inf369"><mml:mrow><mml:mn>100</mml:mn><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:math></inline-formula> data points which have been summed up to mimic the integration time. The time axis is identical for all time traces and saved in the file Time.txt. The folder is similar to six other provided source data folders which have in total been used in this figure.</p></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig12-data1-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig12sdata2"><label>Figure 12—source data 2.</label><caption><title>The data folder contains one data set of cPCF data.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig12-data2-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig12sdata3"><label>Figure 12—source data 3.</label><caption><title>The data folder contains one data set of cPCF data.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig12-data3-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig12sdata4"><label>Figure 12—source data 4.</label><caption><title>The data folder contains one data set of cPCF data.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig12-data4-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig12sdata5"><label>Figure 12—source data 5.</label><caption><title>The data folder contains one data set of cPCF data.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig12-data5-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig12sdata6"><label>Figure 12—source data 6.</label><caption><title>The data folder contains one data set of cPCF data.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig12-data6-v3.zip"/></supplementary-material></p><p><supplementary-material id="fig12sdata7"><label>Figure 12—source data 7.</label><caption><title>The data folder contains one data set of cPCF data.</title></caption><media mimetype="application" mime-subtype="zip" xlink:href="elife-62714-fig12-data7-v3.zip"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-fig12-v3.tif"/></fig></sec><sec id="s2-10"><title>Conclusions</title><p>We generalized the filter equations (Methods <xref ref-type="disp-formula" rid="equ39">Equation 37</xref>, 38d, 57, 58 and 59) of the KF for analyzing the gating and binding dynamics of ligand-gated ion channels with a realistic signal-generating model for isolated patch-clamp (PC) and confocal patch-clamp fluorometry (cPCF) data including open-channel noise, photon-counting noise and background noise. Any other type of linear kinetic scheme (e.g. for voltage-dependent channels) and signal can be applied as long as the characteristics of the signal are sufficiently described by normal distributions. Our approach is derived by approximating the chemical master equation of a first order chemical reaction network (which ion channel experiments usually are) which is exact up to the second statistical moment. For first-order chemical reaction networks, the linear noise approximation (<xref ref-type="bibr" rid="bib102">Wallace et al., 2012</xref>) are exact up to the second moment too (<xref ref-type="bibr" rid="bib44">Grima, 2015</xref>). Thus, we can conclude that our Bayesian filter uses a time integrated version of the linear noise approximation. To our understanding of <xref ref-type="bibr" rid="bib102">Wallace et al., 2012</xref> our approach is thus equivalent to approaches based on the chemical Langevin or Fokker-Planck equations (<xref ref-type="bibr" rid="bib38">Gillespie, 2002</xref>). Consequently, this also makes the considerations of the quality of the chemical Langevin equation as an approximation (<xref ref-type="bibr" rid="bib37">Gillespie, 2000</xref>) of the chemical master equation valid for our approach. Compared to previous attempts <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>, this mathematical generalization is necessary (<xref ref-type="fig" rid="fig3">Figure 3b</xref>) in order to use Bayesian filters on macroscopic PC or cPCF data. With our algorithm, we demonstrate (<xref ref-type="fig" rid="fig3">Figures 3c</xref> and <xref ref-type="fig" rid="fig7">7</xref>) that the common assumption that for large ensembles of ion channels simpler deterministic modeling by RE approaches is on par with stochastic modeling, such as a KF, is wrong in terms of Euclidean error and uncertainty quantification (<xref ref-type="fig" rid="fig5">Figures 5a–c ,</xref>–<xref ref-type="fig" rid="fig6">6a–b</xref>).</p><p>Enriching the data by fluorescence-based ligand binding reveals two regimes. In one regime, the two-dimensional data increase the accuracy of the parameter estimates up to <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi/><mml:mo rspace="0.8pt">≈</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>-fold (<xref ref-type="fig" rid="fig4">Figure 4a and c</xref>). In the other regime of lower channel expression, enriching the data by the second observable, makes non-identified parameters to identified parameters. The second observable in cPCF data decreases the overfitting tendency (<xref ref-type="fig" rid="fig4">Figure 4a, b and d</xref>) of the RE approach on the true process. Thus, in this regard the advantage of the KF becomes smaller. However, by exploiting Bayesian HDCV we gain a second perspective: We show for various signal-to-noise ratios (<xref ref-type="fig" rid="fig5">Figures 5a–c ,</xref>–<xref ref-type="fig" rid="fig6">6a–b</xref>) that the posterior sampled by a RE approach never covers the true values within a reasonable HDCV. Thus, the central feature of Bayesian statistics, exact uncertainty quantification by having the full posterior, is meaningless in combination with an RE approach (considering the type of data and set of signal-to-noise ratios that we tested). This even holds true for very pessimistic signal-to-noise assumptions <xref ref-type="fig" rid="fig6">Figure 6b</xref>. If HDCVs based on an RE approach cannot be trusted, the same applies to confidence volumes based on the curvature of the likelihood. This is not the case for the KF which delivers properly shaped posteriors (<xref ref-type="fig" rid="fig6">Figures 6a–c ,</xref>–<xref ref-type="fig" rid="fig5">5a–c</xref>). Increasing the model complexity, at unchanged PC data quality (<xref ref-type="fig" rid="fig7">Figure 7</xref>) shows that the RE approach displays unidentified rates even for large ion channel ensembles while our approach identified all parameters for the same data. We also investigated the robustness of both algorithms against the cut-off frequency of a Bessel filter (<xref ref-type="fig" rid="fig11">Figure 11</xref>) and showed the overall superior robustness of the KF against errors of analog filtering compared to the RE approach. Analog filtering has its limitations due to distorting the higher frequencies of the Fourier spectrum of the signal. Thus, one should let the KF sample as fast as possible, with a cut-off frequency of at least one order of magnitude higher than the sampling frequency of the KF.</p><p>Similar to the Bessel filter, the KF is more robust than the RE approach against errors due to the finite integration time. Nevertheless, it is crucial for both algorithms (<xref ref-type="fig" rid="fig12">Figure 12</xref>), that the intrinsic time scales (1/eigenvalues) of the process to be analyzed are slower than the integration time of the data points. Otherwise the accuracy of the inference deteriorates.</p><p>Altogether, we demonstrated the performance of the generalized Kalman filter on ion channel data for inference of kinetic schemes. Nevertheless, our approach can approximate any other stochastic system and signal distributions of linear (pseudo-first-order) kinetics (<xref ref-type="bibr" rid="bib91">Sorenson and Alspach, 1971</xref>). Prospective extensions of the Bayesian filter, for example by Bayesian Gaussian sum filters or similar numerically brute force concepts such as particle filters (<xref ref-type="bibr" rid="bib42">Golightly and Wilkinson, 2011</xref>; <xref ref-type="bibr" rid="bib39">Gillespie and Golightly, 2012</xref>), can overcome modeling errors at low ion channel numbers or low photon fluxes.</p></sec></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><p>We simulated state evolution <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with either the software QuB (<xref ref-type="bibr" rid="bib73">Nicolai and Sachs, 2014</xref>) for PC data or an inhouse Matlab routine (The code will be shared on request.) for cPCF data. The inhouse Matlab routine is an implementation of the Gillespie algorithm Gillespie Daniel T. (1977). Traces were summed up, defining the ensemble state vector <inline-formula><mml:math id="inf372"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, which counts the number of channels in each state. At first we used a 10 kHz sampling frequency for the Gillespie algorithm but for investigating the errors induced by analog filtering the current signal and the finite integration time for each fluorescence data point the Gillespie algorithm sampled at 100 kHz. The KF, RE, and Bayesian filter routines were implemented in Stan (<xref ref-type="bibr" rid="bib11">Carpenter et al., 2017</xref>) with the interface package PyStan and ran on a high performance computing cluster with O(100) Broadwell and SkyLake nodes. A Tutorial for Patch clamp data can be found on the git hub page <ext-link ext-link-type="uri" xlink:href="https://github.com/JanMuench/Tutorial_Patch-clamp_data">https://github.com/JanMuench/Tutorial_Patch-clamp_data</ext-link> and for cPCF data, <ext-link ext-link-type="uri" xlink:href="https://github.com/JanMuench/Tutorial_Bayesian_Filter_cPCF_data">https://github.com/JanMuench/Tutorial_Bayesian_Filter_cPCF_data</ext-link>. The cPCF data simulation code can be found here: <ext-link ext-link-type="uri" xlink:href="https://cloudhsm.it-dlz.de/s/QB2pQQ7ycMXEitE">https://cloudhsm.it-dlz.de/s/QB2pQQ7ycMXEitE</ext-link> (<xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>).</p><sec id="s3-1"><title>Methods</title><p>Hereinafter, we derive the equations for our Bayesian filter for time series analysis of hidden linear chemical reaction networks (kinetic schemes). A detailed description of the experimental noise is provided in the Appendix 5.</p></sec><sec id="s3-2"><title>The relation of Bayesian statistics to the Kalman filter</title><p>The following conventions are generally used: Bold symbols are used for multi-dimensional objects such as vectors or matrices. Calligraphic letters are used for (some) vectorial time series and double-strike letters are used for probabilities and probability densities. Within the Bayesian paradigm (<xref ref-type="bibr" rid="bib46">Hines, 2015</xref>; <xref ref-type="bibr" rid="bib4">Ball, 2016</xref>), each unknown quantity, including model parameters <inline-formula><mml:math id="inf373"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> and time series of occupancies of hidden states <inline-formula><mml:math id="inf374"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:msubsup></mml:mrow></mml:math></inline-formula>, are treated as random variables conditioned on observed time series data <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The prior <inline-formula><mml:math id="inf376"><mml:mrow><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mi>j</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>par</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> or posterior distribution <inline-formula><mml:math id="inf377"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> encodes the available information about the parameter values before and after analysing the data, respectively. According to the Bayesian theorem, the posterior distribution<disp-formula id="equ21"><label>(20)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>is a probability distribution of a parameter set <inline-formula><mml:math id="inf378"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> conditioned on <inline-formula><mml:math id="inf379"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The likelihood <inline-formula><mml:math id="inf380"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> encodes the distribution of the data by modelling the intrinsic fluctuations of the protein as well as noise coming from the experimental devices. The prior provides either assumptions before measuring data or what has been learnt from previous experiments about <inline-formula><mml:math id="inf381"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>. The normalization constant<disp-formula id="equ22"><label>(21)</label><mml:math id="m22"><mml:mrow><mml:mi>Z</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>∫</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>d</mml:mi><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>ensures that the posterior is a normalized distribution. The KF is a special class of models in the family of Bayesian filters (<xref ref-type="bibr" rid="bib36">Ghahramani, 1997</xref>), which is a generalisation of the classical KF. Due to its linear time evolution (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), the KF is particularly useful for modeling time series data of ensembles dynamics of first order chemical networks. It delivers a set of recursive algebraic equations (Materials and methods <xref ref-type="disp-formula" rid="equ30">Equation 28</xref> and <xref ref-type="disp-formula" rid="equ34">Equation 32</xref>) for each time point, which allows to express the prior <inline-formula><mml:math id="inf382"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and (after incorporating <inline-formula><mml:math id="inf383"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) the posterior <inline-formula><mml:math id="inf384"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> occupancies of hidden states <inline-formula><mml:math id="inf385"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf386"><mml:mi>t</mml:mi></mml:math></inline-formula> given a set of parameters <inline-formula><mml:math id="inf387"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula>. This means the KF solves the filtering problem (inference of <inline-formula><mml:math id="inf388"><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula>) by explicitly modeling the time evolution of <inline-formula><mml:math id="inf389"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by multivariate normal distributions. This allows us to replace <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> of <xref ref-type="disp-formula" rid="equ21">Equation 20</xref> by the expression of <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>.</p><p>The Bayesian framework (as demonstrated in this article) has various properties which makes it superior to ML estimation (MLE) (<xref ref-type="bibr" rid="bib64">McElreath, 2018</xref>). Those properties are in particular useful for the analysis of biophysical data since very often the dynamics of interest are hidden or latent in the data. Models with a hidden structure are called singular. For regular (non-singular) statistical models, maxima <inline-formula><mml:math id="inf391"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>ML</mml:mi></mml:msub></mml:math></inline-formula> of the posterior or likelihood converge in distribution<disp-formula id="equ23"><label>(22)</label><mml:math id="m23"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">lim</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:munder><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">L</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">F</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>to the true value <inline-formula><mml:math id="inf392"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>true</mml:mi></mml:msub></mml:math></inline-formula>,where <inline-formula><mml:math id="inf393"><mml:mrow><mml:msup><mml:mi mathvariant="bold">F</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>true</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the inverse Fisher information matrix. Under those conditions it is justified to derive from the curvature of the likelihood at <inline-formula><mml:math id="inf394"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>ML</mml:mi></mml:msub></mml:math></inline-formula> via the Cramer-Rao-bound theorem<disp-formula id="equ24"><label>(23)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mi>covar</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ML</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>F</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>ML</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>a confidence volume for the inferred parameters. In contrast, consider for example the type of data investigated in this study which probes the protein dynamics by current and light. Singularity means that the Fisher information matrix of a model is not invertible leading to the breakdown of the Cramer-Rao Bound theorem. Due to the breakdown, it cannot be guaranteed that even in the asymptotic limit the log-likelihood function can be approximated by a quadratic form <xref ref-type="bibr" rid="bib104">Watanabe, 2007</xref>. Thus, usually the MLE does not obey <xref ref-type="disp-formula" rid="equ23">Equation 22</xref>. Consequently, the posterior distribution is usually not a normal distribution either (<xref ref-type="bibr" rid="bib104">Watanabe, 2007</xref>). Using the full posterior distribution without further approximations detects the resulting problems such as deviation from normality or non-identifiability of parameters, related to the singularity. In conclusion, the posterior is still a valid representation of parameter plausibility while ML fails.</p></sec><sec id="s3-3"><title>Time evolution of a Markov Model for a single channel</title><p>In the following, we write the time <inline-formula><mml:math id="inf395"><mml:mi>t</mml:mi></mml:math></inline-formula> as function argument rather than a subscript. Following standard approaches, we attribute to each state of the Markov model an element of a vector space with dimension <inline-formula><mml:math id="inf396"><mml:mi>M</mml:mi></mml:math></inline-formula>. At a time, a channel can only be in a single state. This implies that the set of possible states is S:=<inline-formula><mml:math id="inf397"><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo rspace="4.2pt">,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⊂</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn> 1</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. In the following, Greek subscripts refer to different states while Latin subscripts refer to different channels. By <inline-formula><mml:math id="inf398"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> we specify that the channel is in state α at time <inline-formula><mml:math id="inf399"><mml:mi>t</mml:mi></mml:math></inline-formula>. Mathematically, <inline-formula><mml:math id="inf400"><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:math></inline-formula> stands for the α-th canonical unit Cartesian vector (<xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Important symbols.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Symbol</th><th align="left" valign="bottom">Meaning</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf401"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Set of all unknown model parameters for which the posterior distribution is sampled</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf402"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">Hidden ensemble occupancy vector of channel states in a specific patch at time <inline-formula><mml:math id="inf403"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> which is a continuous Markov state vector <inline-formula><mml:math id="inf404"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf405"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">Variance-covariance matrix of a hidden ensemble state <inline-formula><mml:math id="inf406"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> n a specific patch at time <inline-formula><mml:math id="inf407"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> which contains the dispersion of the ensemble and the lacking knowledge of the algorithm about the true <inline-formula><mml:math id="inf408"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf409"><mml:mi mathvariant="bold">T</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Transition matrix of a single channel</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf410"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Rate matrix which is the logarithm of the transition matrix</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf411"><mml:mi mathvariant="bold">H</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Observation matrix which projects the hidden ensemble state vector onto its mean signal.</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf412"><mml:mi mathvariant="bold">s</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Single-molecule Markov state vector</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf413"><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Specific transition rate from state <italic>j</italic> to state <italic>i</italic>, <inline-formula><mml:math id="inf414"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> ,</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf415"><mml:msub><mml:mi>K</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Ratio of two transition rates i.e. an equilibrium constant</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf416"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">Data point at time</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf417"><mml:mi>T</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Number of observations in a time series</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf418"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Time series of <inline-formula><mml:math id="inf419"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> data points, <inline-formula><mml:math id="inf420"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf421"><mml:msub><mml:mi>N</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Time series of <inline-formula><mml:math id="inf422"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> hidden ensemble states, <inline-formula><mml:math id="inf423"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="fraktur">N</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf424"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ch</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Number of channels in patch number</td></tr><tr><td align="left" valign="top"><inline-formula><mml:math id="inf425"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">Mean electrical current through a single-channel</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf426"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf427"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Variance of the current including all noise from the patch and the recording system</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf428"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">Variance of the current noise generated by a single open-channel</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf429"><mml:msub><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">b</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Mean brightness of a bound ligand</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf430"><mml:msub><mml:mi>λ</mml:mi><mml:mi>Fl</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">Mean brightness of the fluorescence signal from bulk and bound ligands</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf431"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Variance of the fluorescence generated by unbound ligands after subtraction of the image obtained for the reference dye</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf432"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">Number of single-channel states which is the dimension of <inline-formula><mml:math id="inf433"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∈</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">N</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> in the KF algorithm</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf434"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Dimensions of the observational space</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf435"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">F</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">True probability density of <inline-formula><mml:math id="inf436"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, i.e. the true data-generating process</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf437"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Likelihood function of the model parameters</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf438"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Posterior distribution of the model parameters</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf439"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">d</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Predictive distribution of the new data points</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf440"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Distribution of observables for a single time step</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf441"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Normal distribution with mean μ and variance-covariance matrix <inline-formula><mml:math id="inf442"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∑</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Mean value</td></tr></tbody></table></table-wrap><p>Assuming that the state transitions can be modeled by a first order Markov process, the path probability can be decomposed as the product of conditional probabilities as follows:<disp-formula id="equ25"><label>(24)</label><mml:math id="m25"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>path)</mml:mtext><mml:mo>=</mml:mo><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi mathvariant="normal">⋯</mml:mi><mml:mi>ℙ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>∣</mml:mo><mml:mi>s</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>T</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mtext>.</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Markov models (MMs) and rate models are widely used for modeling molecular kinetics (Appendix 2). They provide an interpretation of the data in terms of a set of conformational states and the transition rates between these states. For exactness it remains indispensable to model the dynamics with a HMMs (<xref ref-type="bibr" rid="bib75">Noé et al., 2013</xref>). The core of a hidden Markov model is a conventional Markov model, which is supplemented with a an additional observation model. We will therefore first focus on a conventional Markov model. State-to-state transitions can be equivalently described with a transition matrix <inline-formula><mml:math id="inf444"><mml:mi mathvariant="bold">T</mml:mi></mml:math></inline-formula> in discrete time or with a rate matrix <inline-formula><mml:math id="inf445"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula> in continuous time, as follows:<disp-formula id="equ26"><label>(25)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>e</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>β</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>K</mml:mi><mml:mo>⋅</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf446"><mml:mi>exp</mml:mi></mml:math></inline-formula> is the matrix exponential. We aim to infer the elements of the rate matrix <inline-formula><mml:math id="inf447"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>, constituting a kinetic model or reaction network of the channel. Realizations of sequences of states can be produced by the Doob-Gillespie algorithm Gillespie Daniel T. (1977). To derive succinct equations for the stochastic dynamics of a system, it is beneficial to consider the time propagation of an ensemble of virtual system copies. This allows to ascribe a probability vector <inline-formula><mml:math id="inf448"><mml:mrow><mml:mi mathvariant="bold">p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to the system, in which each element <inline-formula><mml:math id="inf449"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability to find the system at <inline-formula><mml:math id="inf450"><mml:mi>t</mml:mi></mml:math></inline-formula> in state α. One can interpret the probability vector <inline-formula><mml:math id="inf451"><mml:mi mathvariant="bold">p</mml:mi></mml:math></inline-formula> as the instantaneous expectation value of the state vector <inline-formula><mml:math id="inf452"><mml:mi mathvariant="bold">s</mml:mi></mml:math></inline-formula>.<disp-formula id="equ27"><label>(26)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">p</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The probability vector obeys the discrete-time Master equation<disp-formula id="equ28"><label>(27a)</label><mml:math id="m28"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi>Tp</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ29"><label>(27b)</label><mml:math id="m29"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></sec><sec id="s3-4"><title>Time evolution of an ensemble of identical non-interacting channels</title><p>We model the experimentally observed system as a collection of non-interacting channels. A single channel can be modeled with a first-order MM. The same applies to the ensemble of non-interacting channels. We focus on modeling the time course of extensive macroscopic observables such as the mean current and fluorescence signals as well as their fluctuations. A central quantity is the vector <inline-formula><mml:math id="inf453"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which is the occupancy of the channel states at time <inline-formula><mml:math id="inf454"><mml:mi>t</mml:mi></mml:math></inline-formula>:<disp-formula id="equ30"><label>(28)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>This quantity, like <inline-formula><mml:math id="inf455"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, is a random variate. Unlike <inline-formula><mml:math id="inf456"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, its domain is not confined to canonical unit vectors but to <inline-formula><mml:math id="inf457"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℕ</mml:mi><mml:mi>M</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. From the linearity of <xref ref-type="disp-formula" rid="equ30">Equation 28</xref> in the channel dimension and from the single-channel CME <xref ref-type="disp-formula" rid="equ29">Equation 27b</xref> one can immediately derive the equation for the time evolution of the mean occupancy <inline-formula><mml:math id="inf458"><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ31"><label>(29)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>β</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>β</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with the transition matrix <inline-formula><mml:math id="inf459"><mml:mi mathvariant="bold">T</mml:mi></mml:math></inline-formula>. The full distribution <inline-formula><mml:math id="inf460"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a generalized multinomial distribution. To understand the generalized multinomial distribution and how it can be constructed from the (conventional) multinomial distribution, consider the simplified case where all channels are assumed to be in the same state α. Already after one time step, the channels will have spread out over the state space. The channel distribution after one time step is parametrized by the transition probabilities in row number α of the single-channel transition matrix <inline-formula><mml:math id="inf461"><mml:mi mathvariant="bold">T</mml:mi></mml:math></inline-formula>. According to the theory of Markov models, the final distribution of channels originating from state α is the multinomial distribution<disp-formula id="equ32"><label>(30)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>!</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>!</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>M</mml:mi></mml:msub><mml:mo>!</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">⋯</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>M</mml:mi></mml:msub></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In general, the initial ensemble will not have only one but multiple occupied channel states. Because of the independence of the channels, one can imagine each initial sub-population spreading out over the state space independently. Each sub-population with initial state α gives rise to its own final multinomial distribution that contributes <inline-formula><mml:math id="inf462"><mml:msubsup><mml:mi>n</mml:mi><mml:mi>β</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> transitions into state β to the total final distribution. The total number of channels at <inline-formula><mml:math id="inf463"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> in each state can then be simply found by adding the number of channels transitioning out of the different states α.<disp-formula id="equ33"><label>(31)</label><mml:math id="m33"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:mrow><mml:msup><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Evidently, the total number of channels is conserved during propagation. The distribution of <inline-formula><mml:math id="inf464"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, defined by <xref ref-type="disp-formula" rid="equ32 equ33">Equations 30; 31</xref>, is called the <italic>generalized multinomial distribution</italic>:<disp-formula id="equ34"><label>(32)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi>general</mml:mi><mml:mo>-</mml:mo><mml:mi>multinomial</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>While no simple expression exists for the generalized multinomial distribution, closed form expressions for its moments can be readily derived. For large <inline-formula><mml:math id="inf465"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> each <inline-formula><mml:math id="inf466"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi mathvariant="bold">n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mi>α</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> can be approximated by a multivariate-normal distribution such that also <inline-formula><mml:math id="inf467"><mml:mrow><mml:mrow><mml:mi>general</mml:mi><mml:mo>-</mml:mo><mml:mi>multinomial</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> has a multivariate-normal approximation. In the next section, we combine the kinetics of channel ensembles with the KF by a moment expansion of the governing equations for the ensemble probability evolution.</p></sec><sec id="s3-5"><title>Moment expansion of ensemble probability evolution</title><p>The multinomial distribution (<xref ref-type="bibr" rid="bib30">Fredkin and Rice, 1992</xref>) has the following mean and covariance matrix<disp-formula id="equ35"><label>(33)</label><mml:math id="m35"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ36"><label>(34)</label><mml:math id="m36"><mml:mrow><mml:mrow><mml:msup><mml:mi>Σ</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>diag</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf468"><mml:msub><mml:mi mathvariant="bold">T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the column number α of the transition matrix and <inline-formula><mml:math id="inf469"><mml:mrow><mml:mi>diag</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold">T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> describes the diagonal matrix with <inline-formula><mml:math id="inf470"><mml:msub><mml:mi mathvariant="bold">T</mml:mi><mml:mrow><mml:mo>;</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> on its diagonal. Combining <xref ref-type="disp-formula" rid="equ33">Equation 31</xref> with <xref ref-type="disp-formula" rid="equ35 equ36">Equations 33; 34</xref> we deduce the mean and variance of the generalized multinomial distribution:<disp-formula id="equ37"><label>(35)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>Tn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ38"><label>(36)</label><mml:math id="m38"><mml:mrow><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt">∣</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>α</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>α</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>diag</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>T</mml:mi><mml:mrow><mml:mo>:</mml:mo><mml:mo>,</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>diag</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Tn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mo>⁢</mml:mo><mml:mi>diag</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>T</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note that <xref ref-type="disp-formula" rid="equ37 equ38">Equations 35; 36</xref> are conditional expectations that depend on the random state <inline-formula><mml:math id="inf471"><mml:mi mathvariant="bold">n</mml:mi></mml:math></inline-formula> at the previous time <inline-formula><mml:math id="inf472"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and not only on the previous mean <inline-formula><mml:math id="inf473"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula>. To find the absolute mean, the law of total expectation is applied to <xref ref-type="disp-formula" rid="equ37">Equation 35</xref>, giving<disp-formula id="equ39"><label>(37)</label><mml:math id="m39"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold">n</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>in agreement with the simple derivation of <xref ref-type="disp-formula" rid="equ31">Equation 29</xref>. We introduce a shorthand <inline-formula><mml:math id="inf474"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>:=</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> for the absolute covariance matrix of <inline-formula><mml:math id="inf475"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Similarly, <inline-formula><mml:math id="inf476"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> can be found by applying the law of total variance decomposition (<xref ref-type="bibr" rid="bib105">Weiss, 2005</xref> to <xref ref-type="disp-formula" rid="equ38 equ37">Equations 35; 36</xref>), giving<disp-formula id="equ40"><label>(38a)</label><mml:math id="m40"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>∣</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ41"><label>(38b)</label><mml:math id="m41"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ42"><label>(38c)</label><mml:math id="m42"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula><disp-formula id="equ43"><label>(38d)</label><mml:math id="m43"><mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ39 equ43">Equations 37, 38d</xref> dare compact analytical expressions for the mean and the covariance matrix of the occupancy vector <inline-formula><mml:math id="inf477"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf478"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> that depend on the mean <inline-formula><mml:math id="inf479"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula> and covariance matrix <inline-formula><mml:math id="inf480"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> at the previous time step <inline-formula><mml:math id="inf481"><mml:mi>t</mml:mi></mml:math></inline-formula>. Chaining these equations for different time steps <inline-formula><mml:math id="inf482"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>T</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> allows to model the whole evolution of a channel ensemble. Moreover, these two equations together with the output statistics of <inline-formula><mml:math id="inf483"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> are sufficient to formulate correction equations <xref ref-type="disp-formula" rid="equ70">Equation 59</xref> of the KF (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>; <xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>). These equations will be used in a Bayesian context to sample the posterior distribution of the model parameters. The sampling entails repeated numerical evaluation of the model likelihood. Therefore, analytical equations for the ensemble evolution that can be quickly evaluated on a computer millions of times are indispensable. This was achieved by deriving <xref ref-type="disp-formula" rid="equ39">Equation 37</xref>, <xref ref-type="disp-formula" rid="equ43">Equation 38d</xref>. Comparing <xref ref-type="disp-formula" rid="equ43">Equation 38d</xref> with the KF prediction equation (<xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>) for <inline-formula><mml:math id="inf484"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, we obtain the state-dependent covariance matrix of <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> as<disp-formula id="equ44"><label>(39)</label><mml:math id="m44"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Q</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">T</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:math></disp-formula></p><p>In the following section on properties of measured data and the KF, we no longer need to refer to the random variate <inline-formula><mml:math id="inf485"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. All subsequent equations can be formulated by only using the mean hidden state <inline-formula><mml:math id="inf486"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and the variance-covariance matrix of the hidden state <inline-formula><mml:math id="inf487"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We therefore drop the overbar in <inline-formula><mml:math id="inf488"><mml:mrow><mml:mover accent="true"><mml:mi mathvariant="bold">n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> so that the symbol <inline-formula><mml:math id="inf489"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> refers from now on to the mean hidden state.</p></sec><sec id="s3-6"><title>Modeling simultaneous measurement of current and fluorescence</title><p>In the following, we develop a model for the conditional observation distribution <inline-formula><mml:math id="inf490"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (Appendix 5 for experimental details). Together with the hidden ensemble dynamics this will enable us to derive the output statistics of the KF (see, below). Let <inline-formula><mml:math id="inf491"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> be the vector of all observations at <inline-formula><mml:math id="inf492"><mml:mi>t</mml:mi></mml:math></inline-formula>. Components of the vector are the ion current and fluorescence intensity.<disp-formula id="equ45"><label>(40)</label><mml:math id="m45"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mtext>fluorescence intensity</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>ion current</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As outlined in the introduction part, in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> we model the observation by using a conditional probability distribution <inline-formula><mml:math id="inf493"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> that only depends on the mean hidden state <inline-formula><mml:math id="inf494"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, as well as on fixed channel and other measurement parameters. <inline-formula><mml:math id="inf495"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is modeled as a multivariate normal distribution with mean <inline-formula><mml:math id="inf496"><mml:mrow><mml:mi mathvariant="bold">Hn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and variance-covariance matrix <inline-formula><mml:math id="inf497"><mml:mrow><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, that can in general depend on the mean state vector <inline-formula><mml:math id="inf498"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (much like the covariance matrix of the kinetics in (<xref ref-type="disp-formula" rid="equ43">Equation 38d</xref>) ). The observation matrix <inline-formula><mml:math id="inf499"><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>obs</mml:mi></mml:msub><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> projects the hidden state vector <inline-formula><mml:math id="inf500"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> onto <inline-formula><mml:math id="inf501"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">Hn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mi>obs</mml:mi></mml:msub></mml:msup></mml:mrow></mml:math></inline-formula>, the observation space. The observation distribution is<disp-formula id="equ46"><label>(41)</label><mml:math id="m46"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">⇔</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mo mathvariant="bold" stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mo mathvariant="bold" stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This measurement model is very flexible and allows to include different types of signals and error sources arising from both the molecules and the instruments. A summary of the signals and sources of measurement error and their contributions to the parameters of <inline-formula><mml:math id="inf502"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is provided by <xref ref-type="table" rid="table2">Table 2</xref>. Below we address the two types of signals and four noise sources one by one. For this, we decompose the observation matrix and the observation noise covariance matrix into the individual terms:<disp-formula id="equ47"><label>(42)</label><mml:math id="m47"><mml:mrow><mml:mi>H</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>binding</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ48"><label>(43)</label><mml:math id="m48"><mml:mrow><mml:mrow><mml:mi>Σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>open</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>meas</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>binding</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mi>back</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Summary of signals and noise sources for the exemplary CCCO model with the closed states <inline-formula><mml:math id="inf503"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> and the open state <inline-formula><mml:math id="inf504"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula>.</title><p>The observed space is two-dimensional with <inline-formula><mml:math id="inf505"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>fluorescence</mml:mtext></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf506"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mtext>ion current</mml:mtext></mml:mrow></mml:math></inline-formula>. The fluorescence signal is assumed to be derived from the difference of two spectrally different Poisson distributed fluorescent signals. That procedure results in a scaled Skellam distribution of the noise.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="2">ion current</th><th align="left" valign="bottom" colspan="2">fluorescence</th></tr><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">current signal</th><th align="left" valign="bottom">measurement noise</th><th align="left" valign="bottom">fluorescence signal</th><th align="left" valign="bottom">background fluorescence</th></tr></thead><tbody><tr><td align="left" valign="bottom">Signaling states</td><td align="left" valign="bottom">Open state</td><td align="left" valign="bottom">-</td><td align="left" valign="bottom">Ligand-bound states</td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">Error term</td><td align="left" valign="bottom">Open-channel noise</td><td align="left" valign="bottom">Measurement noise</td><td align="left" valign="bottom">Photon counts</td><td align="left" valign="bottom">Bulk noise</td></tr><tr><td align="left" valign="bottom">Affected signal</td><td align="left" valign="bottom">Current</td><td align="left" valign="bottom">Current</td><td align="left" valign="bottom">Fluorescence</td><td align="left" valign="bottom">Fluorescence</td></tr><tr><td align="left" valign="bottom">Distribution</td><td align="left" valign="bottom">Normal <inline-formula><mml:math id="inf507"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Normal <inline-formula><mml:math id="inf508"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">Poisson <inline-formula><mml:math id="inf509"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">Scaled Skellam</td></tr><tr><td align="left" valign="bottom">Contribution to <inline-formula><mml:math id="inf510"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf511"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">-</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf512"><mml:mrow><mml:msub><mml:mi mathvariant="bold">H</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">-</td></tr><tr><td align="left" valign="bottom">Contribution to <inline-formula><mml:math id="inf513"><mml:mi mathvariant="bold">Σ</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf514"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf515"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf516"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf517"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>back</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula></td></tr></tbody></table></table-wrap><p>In the following, we report the individual matrices for the exemplary CCCO model with one open state <inline-formula><mml:math id="inf518"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> and three closed states <inline-formula><mml:math id="inf519"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. Matrices can be constructed analogously for the other models. For the definition of <inline-formula><mml:math id="inf520"><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>back</mml:mi></mml:msub></mml:math></inline-formula> refer to (Appendix 5).</p></sec><sec id="s3-7"><title>Macroscopic current and open-channel noise</title><p>We model the current and the intrinsic fluctuations of the open-channel state <inline-formula><mml:math id="inf521"><mml:mrow><mml:mi mathvariant="bold">s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi mathvariant="bold">e</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> (the <italic>open channel noise</italic>) by a state-dependent normal distribution with mean <inline-formula><mml:math id="inf522"><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf523"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the number of channels in the open state at <inline-formula><mml:math id="inf524"><mml:mi>t</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf525"><mml:mi>i</mml:mi></mml:math></inline-formula> is the single-channel current. The additional variance of the single-channel current is described by <inline-formula><mml:math id="inf526"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>open</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. The sum of the instrumental noise of the experimental setup and the <italic>open channel noise</italic> is modeled as uncorrelated (white) normally distributed noise with the mean <inline-formula><mml:math id="inf527"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf528"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">I</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. By making the open-channel noise dependent on the hidden state population <inline-formula><mml:math id="inf529"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we fully take advantage of the flexibility of Bayesian filters which admits an (explicitly or implicitly) time-dependent observation model. By tabulating the parameters of the two normal distributions into <inline-formula><mml:math id="inf530"><mml:mi mathvariant="bold">H</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf531"><mml:mi mathvariant="bold">Σ</mml:mi></mml:math></inline-formula>, we obtain<disp-formula id="equ49"><label>(44)</label><mml:math id="m49"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi mathvariant="normal">I</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mi>i</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ50"><label>(45)</label><mml:math id="m50"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>open</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>Σ</mml:mi><mml:mrow><mml:mi>meas</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:msub></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>One can now ask for the variance of a data point <inline-formula><mml:math id="inf532"><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> given the epistemic and aleatory uncertainty of <inline-formula><mml:math id="inf533"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> encoded by <inline-formula><mml:math id="inf534"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ43">Equation 38d</xref>. By using the law of total variance the signal variance follows as:<disp-formula id="equ51"><label>(46a)</label><mml:math id="m51"><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ52"><label>(46b)</label><mml:math id="m52"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mtext>I</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ53"><label>(46c)</label><mml:math id="m53"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mtext>I</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mtext>I</mml:mtext><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>See, Appendix 6 for further details.</p></sec><sec id="s3-8"><title>Fluorescence and photon-counting noise</title><p>The statistics of photon counts in the fluorescence signal are described by a Poisson distribution with emission rate <inline-formula><mml:math id="inf535"><mml:msub><mml:mi>λ</mml:mi><mml:mi>Fl</mml:mi></mml:msub></mml:math></inline-formula><disp-formula id="equ54"><label>(47)</label><mml:math id="m54"><mml:mrow><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>Fl</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi>Pois</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>Fl</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>.</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The total emission rate <inline-formula><mml:math id="inf536"><mml:msub><mml:mi>λ</mml:mi><mml:mi>Fl</mml:mi></mml:msub></mml:math></inline-formula> can be modeled as a weighted sum of the specific emission rates <inline-formula><mml:math id="inf537"><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula> of each ligand class <inline-formula><mml:math id="inf538"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>. The weights are given by the stoichiometric factors which reflect the number of bound ligands. In order to cast the Poisson distribution into the functional form of the observation model (<xref ref-type="disp-formula" rid="equ46">Equation 41</xref>), we invoke the central limit theorem to approximate<disp-formula id="equ55"><label>(48)</label><mml:math id="m55"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The larger <inline-formula><mml:math id="inf539"><mml:msub><mml:mi>λ</mml:mi><mml:mi>Fl</mml:mi></mml:msub></mml:math></inline-formula> the better is the approximation. We assume, that the confocal volume is equally illuminated. For our model of ligand fluorescence, we assume for a moment that there is no signal coming from ligands in the bulk. We will drop this assumption in the next section. With these assumptions, we arrive at the following observation matrix<disp-formula id="equ56"><label>(49)</label><mml:math id="m56"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>binding</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="center"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The matrix <inline-formula><mml:math id="inf540"><mml:mi mathvariant="bold">H</mml:mi></mml:math></inline-formula> aggregates the states into two conductivity classes: non-conducting and conducting and three different fluorescence classes. The first element <inline-formula><mml:math id="inf541"><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold">Hn</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is the mean fluorescence <inline-formula><mml:math id="inf542"><mml:mrow><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>Fl</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The variance-covariance matrix <inline-formula><mml:math id="inf543"><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>binding</mml:mi></mml:msub></mml:math></inline-formula> can be derived along the same lines using <xref ref-type="disp-formula" rid="equ55">Equation 48</xref>. We find<disp-formula id="equ57"><label>(50)</label><mml:math id="m57"><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>binding</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>Hn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>1</mml:mn></mml:msub></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="center"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Under these assumptions, the observation matrix can be written as follows<disp-formula id="equ58"><label>(51)</label><mml:math id="m58"><mml:mrow><mml:mi>H</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mtd><mml:mtd columnalign="right"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mi>i</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s3-9"><title>Output statistics of a Kalman Filter</title><p>with two-dimensional state-dependent noiseNow simultaneously measured current and fluorescence data <inline-formula><mml:math id="inf544"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, obtained by cPCF, are modeled. Thus, the observation matrix fulfills <inline-formula><mml:math id="inf545"><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mo>∈</mml:mo><mml:msup><mml:mi>ℝ</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>. One can formulate the observation distribution as<disp-formula id="equ59"><label>(52)</label><mml:math id="m59"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">⇔</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mtext>.</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>The vector <inline-formula><mml:math id="inf546"><mml:msub><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:math></inline-formula> denotes the experimental noise, with <inline-formula><mml:math id="inf547"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">ν</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and variance given by the diagonal matrix <inline-formula><mml:math id="inf548"><mml:mrow><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>meas</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>back</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The second noise term arises from Poisson-distributed photon counting statistics and the open-channel noise. It has the properties<disp-formula id="equ60"><label>(53)</label><mml:math id="m60"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>pois</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>and<disp-formula id="equ61"><label>(54)</label><mml:math id="m61"><mml:mrow><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>pois</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>pois</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="center"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>open</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>Σ</mml:mi><mml:mi>binding</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>.</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The matrix <inline-formula><mml:math id="inf549"><mml:mi mathvariant="bold">Σ</mml:mi></mml:math></inline-formula> is a diagonal matrix. To derive the covariance matrix <inline-formula><mml:math id="inf550"><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> we need to additionally calculate <inline-formula><mml:math id="inf551"><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>fluo</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf552"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">v</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. By the same arguments as above we get<disp-formula id="equ62"><label>(55a)</label><mml:math id="m62"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ63"><label>(55b)</label><mml:math id="m63"><mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula><disp-formula id="equ64"><label>(55c)</label><mml:math id="m64"><mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">k</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="sans-serif">T</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>The cross terms can be calculated by using the law of total covariance<disp-formula id="equ65"><label>(56a)</label><mml:math id="m65"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo>+</mml:mo><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">o</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ66"><label>(56b)</label><mml:math id="m66"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ67"><label>(56c)</label><mml:math id="m67"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>H</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>:</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>yielding the matrix<disp-formula id="equ68"><label>(57)</label><mml:math id="m68"><mml:mrow><mml:mrow><mml:mi>cov</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>HP</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>Σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We assumed that the Poisson distribution is well captured by the normal approximation. In cPCF data, the ligand binding to only a sub-ensemble of the channels is monitored, which we assume to represent the conducting ensemble such that <inline-formula><mml:math id="inf553"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ch</mml:mi><mml:mo>,</mml:mo><mml:mi>FL</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>ch</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. For real data, further refinement might be necessary to model the randomness of the sub-ensemble in the summed voxels. With the time evolution equations for the mean (<xref ref-type="disp-formula" rid="equ37">Equation 35</xref>) and for the covariance matrix <xref ref-type="disp-formula" rid="equ43">Equation 38d</xref> as well as with the expressions for the signal variance, we possess all parameters that are needed in the correction equation of the (<xref ref-type="bibr" rid="bib55">Kalman, 1960</xref>; <xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>).</p></sec><sec id="s3-10"><title>The correction step</title><p>For completeness we write down the correction step (Bayesian update) of the KF, although its derivation can be found in <xref ref-type="bibr" rid="bib14">Chen, 2003</xref>; <xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>; <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>. The mean ensemble state <inline-formula><mml:math id="inf554"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is corrected by the current data point<disp-formula id="equ69"><label>(58)</label><mml:math id="m69"><mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>posterior</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>Kal</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>Hn</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where Kalman gain matrix <inline-formula><mml:math id="inf555"><mml:mrow><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>Kal</mml:mi></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>prior</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">H</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> evaluates the intrinsic noise against the experimental noise. How precise are my model predictions about <inline-formula><mml:math id="inf556"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> compared with the information gained about <inline-formula><mml:math id="inf557"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by measuring <inline-formula><mml:math id="inf558"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The covariance <inline-formula><mml:math id="inf559"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the ensemble state <inline-formula><mml:math id="inf560"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is corrected by<disp-formula id="equ70"><label>(59)</label><mml:math id="m70"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>posterior</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mi>Kal</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>HP</mml:mi><mml:mi>prior</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>Σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>K</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ69 equ70 equ39 equ43">Equation 58,59, 37 and 38d</xref> form the filtering equations which summarize the algorithm. One initialises the first <inline-formula><mml:math id="inf561"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf562"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and with an equilibrium assumption.</p></sec><sec id="s3-11"><title>Microscopic-reversibility as a hierarchical prior</title><p>We applied microscopic-reversibility (<xref ref-type="bibr" rid="bib21">Colquhoun et al., 2004</xref>) by a hierarchical prior distribution. Usually, micro-reversibility is strictly enforced by setting the product of the rates of the clockwise loop <inline-formula><mml:math id="inf563"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> equal to the anti-clockwise loop <inline-formula><mml:math id="inf564"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> and then solving for the desired rate parameter to be replaced. This means that the classical approach can be described by drawing the resulting rate from a Dirac delta distribution prior with<disp-formula id="equ71"><label>(60)</label><mml:math id="m71"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Following <xref ref-type="disp-formula" rid="equ71">Equation 60</xref>, we can model microscopic-reversibility with any hierarchical prior distribution whose limit for a vanishing variance is <xref ref-type="disp-formula" rid="equ71">Equation 60</xref>. For mathematical convenience, we defined the hierarchical prior by a sharply peaking beta distribution<disp-formula id="equ72"><label>(61)</label><mml:math id="m72"><mml:mrow><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mo>⋆</mml:mo></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mi>beta</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>100.01</mml:mn><mml:mo>,</mml:mo><mml:mn>100.01</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and by rescaling and adding an offset<disp-formula id="equ73"><label>(62)</label><mml:math id="m73"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>5</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>6</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>7</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>8</mml:mn></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>3</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mn>4</mml:mn></mml:msub></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:mn>0.995</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>0.01</mml:mn><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>k</mml:mi><mml:mn>1</mml:mn><mml:mo>⋆</mml:mo></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>we derived a conditional prior which allows at maximum a ±0.005 relative deviation from the strict microscopic-reversibility. The ±0.005 micro-reversibility constraint is applied in (<xref ref-type="fig" rid="fig7">Figure 7b–d</xref>). In this way, one could model or even test possible small violation of microscopic-reversibility if smaller beta parameters such as <inline-formula><mml:math id="inf565"><mml:mrow><mml:mi>beta</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> would be chosen.</p></sec></sec></body><back><sec sec-type="additional-information" id="s4"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Validation, Visualization, Writing – original draft</p></fn><fn fn-type="con" id="con2"><p>Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Data curation, Methodology, Supervision, Writing – original draft</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Project administration, Supervision, Writing – original draft</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s5"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-62714-transrepform1-v3.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>cPCF data simulation code.</title></caption><media xlink:href="elife-62714-code1-v3.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><sec sec-type="data-availability" id="s6"><title>Data availability</title><p>We included the simulated data time traces into supporting files and we uploaded the source code on <ext-link ext-link-type="uri" xlink:href="https://github.com/JanMuench/Tutorial_Patch-clamp_data">https://github.com/JanMuench/Tutorial_Patch-clamp_data</ext-link> and <ext-link ext-link-type="uri" xlink:href="https://github.com/JanMuench/Tutorial_Bayesian_Filter_cPCF_data">https://github.com/JanMuench/Tutorial_Bayesian_Filter_cPCF_data</ext-link>.</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors are grateful to E Schulz for designing a software to simulate channel activity in ensemble patches and to Th Eick for performing the simulations. FP acknowledges funding from the Yen Post-Doctoral Fellowship in Interdisciplinary Research and from the National Cancer Institute of the National Institutes of Health (NIH) through Grant CAO93577. The authors are also indebted to M Habeck and I Schroeder for comments on the manuscript, to M Bücker for help with the computer cluster at the Friedrich Schiller University Jena, and to F Noé, R Blunck, G Mirams and S Pressé for helpful discussions. This work was supported by the Research Unit 2,518 DynIon (Project P2) and the TRR 166 ReceptorLight (Project A5) of the Deutsche Forschungsgemeinschaft to KB. Model parameterization, prior distributions, and the general time-reversible model in bayesian phylogenetics. <italic>Systematic Biology</italic>, 53(6):877 – 888.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Anderson</surname><given-names>BD</given-names></name><name><surname>Moore</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Optimal Filtering</source><publisher-loc>Massachusetts, United States</publisher-loc><publisher-name>Courier Corporation</publisher-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auger-Méthé</surname><given-names>M</given-names></name><name><surname>Field</surname><given-names>C</given-names></name><name><surname>Albertsen</surname><given-names>CM</given-names></name><name><surname>Derocher</surname><given-names>AE</given-names></name><name><surname>Lewis</surname><given-names>MA</given-names></name><name><surname>Jonsen</surname><given-names>ID</given-names></name><name><surname>Mills Flemming</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>State-space models’ dirty little secrets: even simple linear Gaussian models can have estimation problems</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>26677</elocation-id><pub-id pub-id-type="doi">10.1038/srep26677</pub-id><pub-id pub-id-type="pmid">27220686</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>FG</given-names></name><name><surname>Cai</surname><given-names>Y</given-names></name><name><surname>Kadane</surname><given-names>JB</given-names></name><name><surname>O’Hagan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Bayesian inference for ion–channel gating mechanisms directly from single–channel recordings, using Markov chain Monte Carlo</article-title><source>Proceedings of the Royal Society of London. Series A</source><volume>455</volume><fpage>2879</fpage><lpage>2932</lpage><pub-id pub-id-type="doi">10.1098/rspa.1999.0432</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ball</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>MCMC for Ion-Channel Sojourn-Time Data: A Good Proposal</article-title><source>Biophysical Journal</source><volume>111</volume><fpage>267</fpage><lpage>268</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2016.02.042</pub-id><pub-id pub-id-type="pmid">27463127</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Betancourt</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A Conceptual Introduction to Hamiltonian Monte Carlo</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.02434">https://arxiv.org/abs/1701.02434</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Biskup</surname><given-names>C</given-names></name><name><surname>Kusch</surname><given-names>J</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Nache</surname><given-names>V</given-names></name><name><surname>Schwede</surname><given-names>F</given-names></name><name><surname>Lehmann</surname><given-names>F</given-names></name><name><surname>Hagen</surname><given-names>V</given-names></name><name><surname>Benndorf</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Relating ligand binding to activation gating in CNGA2 channels</article-title><source>Nature</source><volume>446</volume><fpage>440</fpage><lpage>443</lpage><pub-id pub-id-type="doi">10.1038/nature05596</pub-id><pub-id pub-id-type="pmid">17322905</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname><given-names>CM</given-names></name><name><surname>Dalal</surname><given-names>RB</given-names></name><name><surname>Hebert</surname><given-names>B</given-names></name><name><surname>Digman</surname><given-names>MA</given-names></name><name><surname>Horwitz</surname><given-names>AR</given-names></name><name><surname>Gratton</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Raster image correlation spectroscopy (RICS) for measuring fast protein dynamics and concentrations with a commercial laser scanning confocal microscope</article-title><source>Journal of Microscopy</source><volume>229</volume><fpage>78</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2818.2007.01871.x</pub-id><pub-id pub-id-type="pmid">18173647</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bruening-Wright</surname><given-names>A</given-names></name><name><surname>Elinder</surname><given-names>F</given-names></name><name><surname>Larsson</surname><given-names>HP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Kinetic relationship between the voltage sensor and the activation gate in spHCN channels</article-title><source>The Journal of General Physiology</source><volume>130</volume><fpage>71</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1085/jgp.200709769</pub-id><pub-id pub-id-type="pmid">17591986</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calderazzo</surname><given-names>S</given-names></name><name><surname>Brancaccio</surname><given-names>M</given-names></name><name><surname>Finkenstädt</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Filtering and inference for stochastic oscillators with distributed delays</article-title><source>Bioinformatics (Oxford, England)</source><volume>35</volume><fpage>1380</fpage><lpage>1387</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty782</pub-id><pub-id pub-id-type="pmid">30202930</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Calderhead</surname><given-names>B</given-names></name><name><surname>Sivilotti</surname><given-names>L</given-names></name><name><surname>Girolami</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>In Silico Systems Biology</source><publisher-loc>Totowa, NJ</publisher-loc><publisher-name>Humana Press</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-62703-450-0</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>B</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Goodrich</surname><given-names>B</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Brubaker</surname><given-names>M</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>P</given-names></name><name><surname>Riddell</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stan: A probabilistic programming language</article-title><source>Journal of Statistical Software</source><volume>76</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Celentano</surname><given-names>JJ</given-names></name><name><surname>Hawkes</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Use of the covariance matrix in directly fitting kinetic parameters: application to GABAA receptors</article-title><source>Biophysical Journal</source><volume>87</volume><fpage>276</fpage><lpage>294</lpage><pub-id pub-id-type="doi">10.1529/biophysj.103.036632</pub-id><pub-id pub-id-type="pmid">15240464</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On kalman filter for linear system with colored measurement noise</article-title><source>Journal of Geodesy</source><volume>88</volume><fpage>1163</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1007/s00190-014-0751-7</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Bayesian filtering: From kalman filters to particle filters, and beyond</article-title><source>Statistics</source><volume>182</volume><elocation-id>257</elocation-id><pub-id pub-id-type="doi">10.1080/02331880309257</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>SH</given-names></name><name><surname>Moore</surname><given-names>JB</given-names></name><name><surname>Xia</surname><given-names>LG</given-names></name><name><surname>Premkumar</surname><given-names>LS</given-names></name><name><surname>Gage</surname><given-names>PW</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Characterization of single channel currents using digital signal processing techniques based on Hidden Markov Models</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>329</volume><fpage>265</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1098/rstb.1990.0170</pub-id><pub-id pub-id-type="pmid">1702543</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clancy</surname><given-names>CE</given-names></name><name><surname>Rudy</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Cellular consequences of HERG mutations in the long QT syndrome: precursors to sudden cardiac death</article-title><source>Cardiovascular Research</source><volume>50</volume><fpage>301</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1016/s0008-6363(00)00293-5</pub-id><pub-id pub-id-type="pmid">11334834</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Colquhoun</surname><given-names>D</given-names></name><name><surname>Hawkes</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1995">1995a</year><chapter-title>A Q-Matrix Cookbook</chapter-title><person-group person-group-type="editor"><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Neher</surname><given-names>E</given-names></name></person-group><source>Single-Channel Recording</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer</publisher-name><fpage>589</fpage><lpage>633</lpage><pub-id pub-id-type="doi">10.1007/978-1-4419-1229-9_20</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Colquhoun</surname><given-names>D</given-names></name><name><surname>Hawkes</surname><given-names>GA</given-names></name></person-group><year iso-8601-date="1995">1995b</year><chapter-title>The Principles of the Stochastic Interpretation of Ion-Channel Mechanisms</chapter-title><person-group person-group-type="editor"><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Neher</surname><given-names>E</given-names></name></person-group><source>Single-Channel Recording</source><publisher-loc>Boston, MA</publisher-loc><publisher-name>Springer</publisher-name><fpage>397</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1007/978-1-4419-1229-9_18</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colquhoun</surname><given-names>D</given-names></name><name><surname>Hawkes</surname><given-names>GA</given-names></name><name><surname>Bernard</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997a</year><article-title>On the stochastic properties of single ion channels</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>211</volume><fpage>205</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1098/rspb.1981.0003</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colquhoun</surname><given-names>D</given-names></name><name><surname>Hawkes</surname><given-names>GA</given-names></name><name><surname>Bernard</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997b</year><article-title>Relaxation and fluctuations of membrane currents that flow through drug-operated channels</article-title><source>Proc. R. Soc. Lond</source><volume>199</volume><fpage>231</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1098/rspb.1977.0137</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colquhoun</surname><given-names>D</given-names></name><name><surname>Dowsland</surname><given-names>KA</given-names></name><name><surname>Beato</surname><given-names>M</given-names></name><name><surname>Plested</surname><given-names>AJR</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>How to impose microscopic reversibility in complex reaction mechanisms</article-title><source>Biophysical Journal</source><volume>86</volume><fpage>3510</fpage><lpage>3518</lpage><pub-id pub-id-type="doi">10.1529/biophysj.103.038679</pub-id><pub-id pub-id-type="pmid">15189850</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>d’Alcantara</surname><given-names>P</given-names></name><name><surname>Cardenas</surname><given-names>LM</given-names></name><name><surname>Swillens</surname><given-names>S</given-names></name><name><surname>Scroggs</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Reduced transition between open and inactivated channel states underlies 5HT increased I(Na+) in rat nociceptors</article-title><source>Biophysical Journal</source><volume>83</volume><fpage>5</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(02)75146-1</pub-id><pub-id pub-id-type="pmid">12080097</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Gunst</surname><given-names>MCM</given-names></name><name><surname>Künsch</surname><given-names>HR</given-names></name><name><surname>Schouten</surname><given-names>JG</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Statistical Analysis of Ion Channel Data Using Hidden Markov Models With Correlated State-Dependent Noise and Filtering</article-title><source>Journal of the American Statistical Association</source><volume>96</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1198/016214501753208519</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deuflhard</surname><given-names>P</given-names></name><name><surname>Weber</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Robust Perron cluster analysis in conformation dynamics</article-title><source>Linear Algebra and Its Applications</source><volume>398</volume><fpage>161</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.laa.2004.10.026</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>M</given-names></name><name><surname>Calderhead</surname><given-names>B</given-names></name><name><surname>Girolami</surname><given-names>MA</given-names></name><name><surname>Sivilotti</surname><given-names>LG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Bayesian Statistical Inference in Ion-Channel Models with Exact Missed Event Correction</article-title><source>Biophysical Journal</source><volume>111</volume><fpage>333</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2016.04.053</pub-id><pub-id pub-id-type="pmid">27463136</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fearnhead</surname><given-names>P</given-names></name><name><surname>Giagos</surname><given-names>V</given-names></name><name><surname>Sherlock</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Inference for reaction networks using the linear noise approximation</article-title><source>Biometrics</source><volume>70</volume><fpage>457</fpage><lpage>466</lpage><pub-id pub-id-type="doi">10.1111/biom.12152</pub-id><pub-id pub-id-type="pmid">24467590</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finkenstädt</surname><given-names>B</given-names></name><name><surname>Woodcock</surname><given-names>DJ</given-names></name><name><surname>Komorowski</surname><given-names>M</given-names></name><name><surname>Harper</surname><given-names>CV</given-names></name><name><surname>Davis</surname><given-names>JRE</given-names></name><name><surname>White</surname><given-names>MRH</given-names></name><name><surname>Rand</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Quantifying intrinsic and extrinsic noise in gene transcription using the linear noise approximation: An application to single cell data</article-title><source>The Annals of Applied Statistics</source><volume>7</volume><pub-id pub-id-type="doi">10.1214/13-AOAS669</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Folia</surname><given-names>MM</given-names></name><name><surname>Rattray</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Trajectory inference and parameter estimation in stochastic models with temporally aggregated data</article-title><source>Statistics and Computing</source><volume>28</volume><fpage>1053</fpage><lpage>1072</lpage><pub-id pub-id-type="doi">10.1007/s11222-017-9779-x</pub-id><pub-id pub-id-type="pmid">30147250</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frauenfelder</surname><given-names>H</given-names></name><name><surname>Sligar</surname><given-names>SG</given-names></name><name><surname>Wolynes</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>The energy landscapes and motions of proteins</article-title><source>Science (New York, N.Y.)</source><volume>254</volume><fpage>1598</fpage><lpage>1603</lpage><pub-id pub-id-type="doi">10.1126/science.1749933</pub-id><pub-id pub-id-type="pmid">1749933</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fredkin</surname><given-names>DR</given-names></name><name><surname>Rice</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Maximum likelihood estimation and identification directly from single-channel recordings</article-title><source>Proceedings. Biological Sciences</source><volume>249</volume><fpage>125</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1098/rspb.1992.0094</pub-id><pub-id pub-id-type="pmid">1280834</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gabry</surname><given-names>J</given-names></name><name><surname>Simpson</surname><given-names>D</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Betancourt</surname><given-names>M</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Visualization in Bayesian workflow</article-title><source>Journal of the Royal Statistical Society</source><volume>182</volume><fpage>389</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.1111/rssa.12378</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1992">1992a</year><article-title>A single series from the gibbs sampler provides A false sense of security</article-title><source>Bayesian Statistics</source><volume>4</volume><elocation-id>75</elocation-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name></person-group><year iso-8601-date="1992">1992b</year><article-title>Inference from Iterative Simulation Using Multiple Sequences</article-title><source>Statistical Science</source><volume>7</volume><elocation-id>1136</elocation-id><pub-id pub-id-type="doi">10.1214/ss/1177011136</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Hwang</surname><given-names>J</given-names></name><name><surname>Vehtari</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Understanding predictive information criteria for Bayesian models</article-title><source>Statistics and Computing</source><volume>24</volume><fpage>997</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1007/s11222-013-9416-2</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Guo</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Stan: A probabilistic programming language for bayesian inference and optimization</article-title><source>J. of Educational and Behavioral Statistics</source><volume>40</volume><elocation-id>113</elocation-id><pub-id pub-id-type="doi">10.3102/1076998615606113</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ghahramani</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="1997">1997</year><source>Learning Dynamic Bayesian Networks</source><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>The chemical Langevin equation</article-title><source>The Journal of Chemical Physics</source><volume>113</volume><fpage>297</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1063/1.481811</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Exact stochastic simulation of coupled chemical reactions</article-title><source>The Journal of Physical Chemistry</source><volume>81</volume><fpage>2340</fpage><lpage>2361</lpage><pub-id pub-id-type="doi">10.1021/j100540a008</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gillespie</surname><given-names>CS</given-names></name><name><surname>Golightly</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Bayesian Inference for the Chemical Master Equation Using Approximate Models</source><publisher-loc>Ulm, Germany</publisher-loc><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gin</surname><given-names>E</given-names></name><name><surname>Falcke</surname><given-names>M</given-names></name><name><surname>Wagner</surname><given-names>LE</given-names></name><name><surname>Yule</surname><given-names>DI</given-names></name><name><surname>Sneyd</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Markov chain Monte Carlo fitting of single-channel data from inositol trisphosphate receptors</article-title><source>Journal of Theoretical Biology</source><volume>257</volume><fpage>460</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1016/j.jtbi.2008.12.020</pub-id><pub-id pub-id-type="pmid">19168073</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldschen-Ohm</surname><given-names>MP</given-names></name><name><surname>Wagner</surname><given-names>DA</given-names></name><name><surname>Petrou</surname><given-names>S</given-names></name><name><surname>Jones</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An epilepsy-related region in the GABA(A) receptor mediates long-distance effects on GABA and benzodiazepine binding sites</article-title><source>Molecular Pharmacology</source><volume>77</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1124/mol.109.058289</pub-id><pub-id pub-id-type="pmid">19846749</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Golightly</surname><given-names>A</given-names></name><name><surname>Wilkinson</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Bayesian parameter inference for stochastic biochemical network models using particle Markov chain Monte Carlo</article-title><source>Interface Focus</source><volume>1</volume><fpage>807</fpage><lpage>820</lpage><pub-id pub-id-type="doi">10.1098/rsfs.2011.0047</pub-id><pub-id pub-id-type="pmid">23226583</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gopalakrishnan</surname><given-names>A</given-names></name><name><surname>Kaisare</surname><given-names>NS</given-names></name><name><surname>Narasimhan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Incorporating delayed and infrequent measurements in Extended Kalman Filter based nonlinear state estimation</article-title><source>Journal of Process Control</source><volume>21</volume><fpage>119</fpage><lpage>129</lpage><pub-id pub-id-type="doi">10.1016/j.jprocont.2010.10.013</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grima</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Linear-noise approximation and the chemical master equation agree up to second-order moments for a class of chemical systems</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>92</volume><elocation-id>042124</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.92.042124</pub-id><pub-id pub-id-type="pmid">26565185</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>KE</given-names></name><name><surname>Middendorf</surname><given-names>TR</given-names></name><name><surname>Aldrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Determination of parameter identifiability in nonlinear biophysical models: A Bayesian approach</article-title><source>The Journal of General Physiology</source><volume>143</volume><fpage>401</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1085/jgp.201311116</pub-id><pub-id pub-id-type="pmid">24516188</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>KE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A primer on Bayesian inference for biophysical systems</article-title><source>Biophysical Journal</source><volume>108</volume><fpage>2103</fpage><lpage>2113</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2015.03.042</pub-id><pub-id pub-id-type="pmid">25954869</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hines</surname><given-names>KE</given-names></name><name><surname>Bankston</surname><given-names>JR</given-names></name><name><surname>Aldrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Analyzing single-molecule time series via nonparametric Bayesian inference</article-title><source>Biophysical Journal</source><volume>108</volume><fpage>540</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2014.12.016</pub-id><pub-id pub-id-type="pmid">25650922</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffman</surname><given-names>MD</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo</article-title><source>Journal of Machine Learning Research</source><volume>15</volume><fpage>1593</fpage><lpage>1623</lpage></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horn</surname><given-names>R</given-names></name><name><surname>Lange</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Estimating kinetic constants from single channel data</article-title><source>Biophysical Journal</source><volume>43</volume><fpage>207</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(83)84341-0</pub-id><pub-id pub-id-type="pmid">6311301</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hwang</surname><given-names>Y</given-names></name><name><surname>Kim</surname><given-names>JS</given-names></name><name><surname>Kweon</surname><given-names>IS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sensor noise modeling using the Skellam distribution: Application to the color edge detection</article-title><conf-name>IEEE conference on computer vision and pattern recognition</conf-name><pub-id pub-id-type="doi">10.1109/CVPR.2007.383004</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jahnke</surname><given-names>T</given-names></name><name><surname>Huisinga</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Solving the chemical master equation for monomolecular reaction systems analytically</article-title><source>Journal of Mathematical Biology</source><volume>54</volume><fpage>1</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1007/s00285-006-0034-x</pub-id><pub-id pub-id-type="pmid">16953443</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Jaynes</surname><given-names>ET</given-names></name><name><surname>Kempthorne</surname><given-names>O</given-names></name></person-group><year iso-8601-date="1976">1976</year><chapter-title>Confidence intervals vs bayesian intervals</chapter-title><person-group person-group-type="editor"><name><surname>Harper</surname><given-names>WL</given-names></name><name><surname>Hooker</surname><given-names>CA</given-names></name></person-group><source>Foundations of Probability Theory, Statistical Inference, and Statistical Theories of Science</source><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer</publisher-name><fpage>175</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1007/978-94-010-1436-6_6</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jeffreys</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1946">1946</year><article-title>An invariant form for the prior probability in estimation problems</article-title><source>Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences</source><volume>186</volume><fpage>453</fpage><lpage>461</lpage><pub-id pub-id-type="doi">10.1098/rspa.1946.0056</pub-id><pub-id pub-id-type="pmid">20998741</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joshi</surname><given-names>M</given-names></name><name><surname>Seidel-Morgenstern</surname><given-names>A</given-names></name><name><surname>Kremling</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Exploiting the bootstrap method for quantifying parameter confidence intervals in dynamical systems</article-title><source>Metabolic Engineering</source><volume>8</volume><fpage>447</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1016/j.ymben.2006.04.003</pub-id><pub-id pub-id-type="pmid">16793301</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalman</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A New Approach to Linear Filtering and Prediction Problems</article-title><source>Journal of Basic Engineering</source><volume>82</volume><fpage>35</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1115/1.3662552</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalstrup</surname><given-names>T</given-names></name><name><surname>Blunck</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamics of internal pore opening in KV channels probed by a fluorescent unnatural amino acid</article-title><source>PNAS</source><volume>110</volume><fpage>8272</fpage><lpage>8277</lpage><pub-id pub-id-type="doi">10.1073/pnas.1220398110</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalstrup</surname><given-names>T</given-names></name><name><surname>Blunck</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>S4–S5 linker movement during activation and inactivation in voltage-gated K <sup>+</sup> channels</article-title><source>PNAS</source><volume>115</volume><elocation-id>29</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.1719105115</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kinz-Thompson</surname><given-names>CD</given-names></name><name><surname>Gonzalez</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Increasing the Time Resolution of Single-Molecule Experiments with Bayesian Inference</article-title><source>Biophysical Journal</source><volume>114</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2017.11.3741</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Komorowski</surname><given-names>M</given-names></name><name><surname>Finkenstädt</surname><given-names>B</given-names></name><name><surname>Harper</surname><given-names>CV</given-names></name><name><surname>Rand</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Bayesian inference of biochemical kinetic parameters using the linear noise approximation</article-title><source>BMC Bioinformatics</source><volume>10</volume><elocation-id>343</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2105-10-343</pub-id><pub-id pub-id-type="pmid">19840370</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kurtz</surname><given-names>TG</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>The Relationship between Stochastic and Deterministic Models for Chemical Reactions</article-title><source>The Journal of Chemical Physics</source><volume>57</volume><fpage>2976</fpage><lpage>2978</lpage><pub-id pub-id-type="doi">10.1063/1.1678692</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusch</surname><given-names>J</given-names></name><name><surname>Biskup</surname><given-names>C</given-names></name><name><surname>Thon</surname><given-names>S</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Nache</surname><given-names>V</given-names></name><name><surname>Zimmer</surname><given-names>T</given-names></name><name><surname>Schwede</surname><given-names>F</given-names></name><name><surname>Benndorf</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Interdependence of receptor activation and ligand binding in HCN2 pacemaker channels</article-title><source>Neuron</source><volume>67</volume><fpage>75</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.05.022</pub-id><pub-id pub-id-type="pmid">20624593</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kusch</surname><given-names>J</given-names></name><name><surname>Thon</surname><given-names>S</given-names></name><name><surname>Schulz</surname><given-names>E</given-names></name><name><surname>Biskup</surname><given-names>C</given-names></name><name><surname>Nache</surname><given-names>V</given-names></name><name><surname>Zimmer</surname><given-names>T</given-names></name><name><surname>Seifert</surname><given-names>R</given-names></name><name><surname>Schwede</surname><given-names>F</given-names></name><name><surname>Benndorf</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>How subunits cooperate in cAMP-induced activation of homotetrameric HCN2 channels</article-title><source>Nature Chemical Biology</source><volume>8</volume><fpage>162</fpage><lpage>169</lpage><pub-id pub-id-type="doi">10.1038/nchembio.747</pub-id><pub-id pub-id-type="pmid">22179066</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lei</surname><given-names>CL</given-names></name><name><surname>Clerx</surname><given-names>M</given-names></name><name><surname>Whittaker</surname><given-names>DG</given-names></name><name><surname>Gavaghan</surname><given-names>DJ</given-names></name><name><surname>de Boer</surname><given-names>TP</given-names></name><name><surname>Mirams</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Accounting for variability in ion current recordings using a mathematical model of artefacts in voltage-clamp experiments</article-title><source>Philosophical Transactions of the Royal Society A</source><volume>378</volume><elocation-id>20190348</elocation-id><pub-id pub-id-type="doi">10.1098/rsta.2019.0348</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McElreath</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Statistical Rethinking</source><publisher-name>Chapman and Hall/CRC</publisher-name><pub-id pub-id-type="doi">10.1201/9781315372495</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Middendorf</surname><given-names>TR</given-names></name><name><surname>Aldrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Structural identifiability of equilibrium ligand-binding parameters</article-title><source>The Journal of General Physiology</source><volume>149</volume><fpage>105</fpage><lpage>119</lpage><pub-id pub-id-type="doi">10.1085/jgp.201611702</pub-id><pub-id pub-id-type="pmid">27993952</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Middendorf</surname><given-names>TR</given-names></name><name><surname>Aldrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>The structure of binding curves and practical identifiability of equilibrium ligand-binding parameters</article-title><source>The Journal of General Physiology</source><volume>149</volume><fpage>121</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1085/jgp.201611703</pub-id><pub-id pub-id-type="pmid">27993951</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milescu</surname><given-names>LS</given-names></name><name><surname>Akk</surname><given-names>G</given-names></name><name><surname>Sachs</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Maximum likelihood estimation of ion channel kinetics from macroscopic currents</article-title><source>Biophysical Journal</source><volume>88</volume><fpage>2494</fpage><lpage>2515</lpage><pub-id pub-id-type="doi">10.1529/biophysj.104.053256</pub-id><pub-id pub-id-type="pmid">15681642</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moffatt</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Estimation of ion channel kinetics from fluctuations of macroscopic currents</article-title><source>Biophysical Journal</source><volume>93</volume><fpage>74</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1529/biophysj.106.101212</pub-id><pub-id pub-id-type="pmid">17416622</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Munsky</surname><given-names>B</given-names></name><name><surname>Trinh</surname><given-names>B</given-names></name><name><surname>Khammash</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Listening to the noise: random fluctuations reveal gene network parameters</article-title><source>Molecular Systems Biology</source><volume>5</volume><elocation-id>318</elocation-id><pub-id pub-id-type="doi">10.1038/msb.2009.75</pub-id><pub-id pub-id-type="pmid">19888213</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navarro</surname><given-names>MA</given-names></name><name><surname>Salari</surname><given-names>A</given-names></name><name><surname>Milescu</surname><given-names>M</given-names></name><name><surname>Milescu</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Estimating kinetic mechanisms with prior knowledge II: Behavioral constraints and numerical tests</article-title><source>Journal of General Physiology</source><volume>150</volume><fpage>339</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1085/jgp.201711912</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Neal</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>MCMC Using Hamiltonian Dynamics</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1206.1901">https://arxiv.org/abs/1206.1901</ext-link></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neher</surname><given-names>E</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Single-channel currents recorded from membrane of denervated frog muscle fibres</article-title><source>Nature</source><volume>260</volume><fpage>799</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1038/260799a0</pub-id><pub-id pub-id-type="pmid">1083489</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nicolai</surname><given-names>C</given-names></name><name><surname>Sachs</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Solving ion channel kinetics with the qub software</article-title><source>Biophysical Reviews and Letters</source><volume>08</volume><fpage>191</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1142/S1793048013300053</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noé</surname><given-names>F</given-names></name><name><surname>Doose</surname><given-names>S</given-names></name><name><surname>Daidone</surname><given-names>I</given-names></name><name><surname>Löllmann</surname><given-names>M</given-names></name><name><surname>Sauer</surname><given-names>M</given-names></name><name><surname>Chodera</surname><given-names>JD</given-names></name><name><surname>Smith</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Dynamical fingerprints for probing individual relaxation processes in biomolecular dynamics with simulations and kinetic experiments</article-title><source>PNAS</source><volume>108</volume><fpage>4822</fpage><lpage>4827</lpage><pub-id pub-id-type="doi">10.1073/pnas.1004646108</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noé</surname><given-names>F</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Prinz</surname><given-names>JH</given-names></name><name><surname>Plattner</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Projected and hidden Markov models for calculating kinetics and metastable states of complex molecules</article-title><source>The Journal of Chemical Physics</source><volume>139</volume><elocation-id>184114</elocation-id><pub-id pub-id-type="doi">10.1063/1.4828816</pub-id><pub-id pub-id-type="pmid">24320261</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oyrer</surname><given-names>J</given-names></name><name><surname>Maljevic</surname><given-names>S</given-names></name><name><surname>Scheffer</surname><given-names>IE</given-names></name><name><surname>Berkovic</surname><given-names>SF</given-names></name><name><surname>Petrou</surname><given-names>S</given-names></name><name><surname>Reid</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Ion Channels in Genetic Epilepsy: From Genes and Mechanisms to Disease-Targeted Therapies</article-title><source>Pharmacological Reviews</source><volume>70</volume><fpage>142</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1124/pr.117.014456</pub-id><pub-id pub-id-type="pmid">29263209</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>F</given-names></name><name><surname>Auerbach</surname><given-names>A</given-names></name><name><surname>Sachs</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Estimating single-channel kinetic parameters from idealized patch-clamp data containing missed events</article-title><source>Biophysical Journal</source><volume>70</volume><fpage>264</fpage><lpage>280</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(96)79568-1</pub-id><pub-id pub-id-type="pmid">8770203</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qin</surname><given-names>F</given-names></name><name><surname>Auerbach</surname><given-names>A</given-names></name><name><surname>Sachs</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Hidden Markov modeling for single channel kinetics with filtering and correlated noise</article-title><source>Biophysical Journal</source><volume>79</volume><fpage>1928</fpage><lpage>1944</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(00)76442-3</pub-id><pub-id pub-id-type="pmid">11023898</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rabiner</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>A tutorial on hidden Markov models and selected applications in speech recognition</article-title><source>Proceedings of the IEEE</source><volume>77</volume><fpage>257</fpage><lpage>286</lpage><pub-id pub-id-type="doi">10.1109/5.18626</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosales</surname><given-names>R</given-names></name><name><surname>Stark</surname><given-names>JA</given-names></name><name><surname>Fitzgerald</surname><given-names>WJ</given-names></name><name><surname>Hladky</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Bayesian restoration of ion channel records using hidden Markov models</article-title><source>Biophysical Journal</source><volume>80</volume><fpage>1088</fpage><lpage>1103</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(01)76087-0</pub-id><pub-id pub-id-type="pmid">11222275</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosales</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>MCMC for hidden Markov models incorporating aggregation of states and filtering</article-title><source>Bulletin of Mathematical Biology</source><volume>66</volume><fpage>1173</fpage><lpage>1199</lpage><pub-id pub-id-type="doi">10.1016/j.bulm.2003.12.001</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Schenker</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Efficiently Simulating the Coverage Properties of Interval Estimates</article-title><source>Applied Statistics</source><volume>35</volume><elocation-id>159</elocation-id><pub-id pub-id-type="doi">10.2307/2347266</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sakmann</surname><given-names>B</given-names></name><name><surname>Neher</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Single-Channel Recording</source><publisher-loc>Berlin, Germany</publisher-loc><publisher-name>Springer Science &amp; Business Media</publisher-name><pub-id pub-id-type="doi">10.1007/978-1-4419-1229-9</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salari</surname><given-names>A</given-names></name><name><surname>Navarro</surname><given-names>MA</given-names></name><name><surname>Milescu</surname><given-names>M</given-names></name><name><surname>Milescu</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Estimating kinetic mechanisms with prior knowledge I: Linear parameter constraints</article-title><source>The Journal of General Physiology</source><volume>150</volume><fpage>323</fpage><lpage>338</lpage><pub-id pub-id-type="doi">10.1085/jgp.201711911</pub-id><pub-id pub-id-type="pmid">29321264</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sgouralis</surname><given-names>I</given-names></name><name><surname>Pressé</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>An Introduction to Infinite HMMs for Single-Molecule Data Analysis</article-title><source>Biophysical Journal</source><volume>112</volume><fpage>2021</fpage><lpage>2029</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2017.04.027</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sgouralis</surname><given-names>I</given-names></name><name><surname>Pressé</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>ICON: An Adaptation of Infinite HMMs for Time Traces with Drift</article-title><source>Biophysical Journal</source><volume>112</volume><fpage>2117</fpage><lpage>2126</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2017.04.009</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siekmann</surname><given-names>I</given-names></name><name><surname>Wagner</surname><given-names>LE</given-names></name><name><surname>Yule</surname><given-names>D</given-names></name><name><surname>Fox</surname><given-names>C</given-names></name><name><surname>Bryant</surname><given-names>D</given-names></name><name><surname>Crampin</surname><given-names>EJ</given-names></name><name><surname>Sneyd</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>MCMC estimation of Markov models for ion channels</article-title><source>Biophysical Journal</source><volume>100</volume><fpage>1919</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2011.02.059</pub-id><pub-id pub-id-type="pmid">21504728</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siekmann</surname><given-names>I</given-names></name><name><surname>Sneyd</surname><given-names>J</given-names></name><name><surname>Crampin</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>MCMC Can Detect Nonidentifiable Models</article-title><source>Biophysical Journal</source><volume>103</volume><fpage>2275</fpage><lpage>2286</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2012.10.024</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siekmann</surname><given-names>I</given-names></name><name><surname>Fackrell</surname><given-names>M</given-names></name><name><surname>Crampin</surname><given-names>EJ</given-names></name><name><surname>Taylor</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Modelling modal gating of ion channels with hierarchical Markov models</article-title><source>Proceedings of the Royal Society A</source><volume>472</volume><elocation-id>20160122</elocation-id><pub-id pub-id-type="doi">10.1098/rspa.2016.0122</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silberberg</surname><given-names>SD</given-names></name><name><surname>Magleby</surname><given-names>KL</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Preventing errors when estimating single channel properties from the analysis of current fluctuations</article-title><source>Biophysical Journal</source><volume>65</volume><fpage>1570</fpage><lpage>1584</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(93)81196-2</pub-id><pub-id pub-id-type="pmid">7506065</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorenson</surname><given-names>HW</given-names></name><name><surname>Alspach</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Recursive bayesian estimation using gaussian sums</article-title><source>Automatica</source><volume>7</volume><fpage>465</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1016/0005-1098(71)90097-5</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stepanyuk</surname><given-names>AR</given-names></name><name><surname>Borisyuk</surname><given-names>AL</given-names></name><name><surname>Belan</surname><given-names>PV</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Efficient maximum likelihood estimation of kinetic rate constants from macroscopic currents</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e29731</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0029731</pub-id><pub-id pub-id-type="pmid">22242142</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stepanyuk</surname><given-names>A</given-names></name><name><surname>Borisyuk</surname><given-names>A</given-names></name><name><surname>Belan</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Maximum likelihood estimation of biophysical parameters of synaptic receptors from macroscopic currents</article-title><source>Frontiers in Cellular Neuroscience</source><volume>8</volume><elocation-id>303</elocation-id><pub-id pub-id-type="doi">10.3389/fncel.2014.00303</pub-id><pub-id pub-id-type="pmid">25324721</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taraska</surname><given-names>JW</given-names></name><name><surname>Zagotta</surname><given-names>WN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Structural dynamics in the gating ring of cyclic nucleotide-gated ion channels</article-title><source>Nature Structural &amp; Molecular Biology</source><volume>14</volume><fpage>854</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1038/nsmb1281</pub-id><pub-id pub-id-type="pmid">17694071</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taraska</surname><given-names>JW</given-names></name><name><surname>Puljung</surname><given-names>MC</given-names></name><name><surname>Olivier</surname><given-names>NB</given-names></name><name><surname>Flynn</surname><given-names>GE</given-names></name><name><surname>Zagotta</surname><given-names>WN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Mapping the structure and conformational movements of proteins with transition metal ion FRET</article-title><source>Nature Methods</source><volume>6</volume><fpage>532</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1038/nmeth.1341</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Van Kampen</surname><given-names>NG</given-names></name></person-group><year iso-8601-date="1992">1992</year><source>Stochastic Processes in Physics and Chemistry</source><publisher-loc>Amsterdam, Netherlands</publisher-loc><publisher-name>Elsevier</publisher-name><pub-id pub-id-type="doi">10.1016/B978-0-444-52965-7.X5000-4</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vats</surname><given-names>D</given-names></name><name><surname>Knudson</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Revisiting the Gelman-Rubin Diagnostic</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1812.09384">https://arxiv.org/abs/1812.09384</ext-link></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vehtari</surname><given-names>A</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Simpson</surname><given-names>D</given-names></name><name><surname>Carpenter</surname><given-names>B</given-names></name><name><surname>Bürkner</surname><given-names>PC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rank-Normalization, Folding, and Localization: An Improved Rˆ for Assessing Convergence of MCMC with Discussion</article-title><source>Bayesian Analysis</source><volume>16</volume><elocation-id>121</elocation-id><pub-id pub-id-type="doi">10.1214/20-BA1221</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venkataramanan</surname><given-names>L</given-names></name><name><surname>Sigworth</surname><given-names>FJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Applying hidden Markov models to the analysis of single ion channel activity</article-title><source>Biophysical Journal</source><volume>82</volume><fpage>1930</fpage><lpage>1942</lpage><pub-id pub-id-type="doi">10.1016/S0006-3495(02)75542-2</pub-id><pub-id pub-id-type="pmid">11916851</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verkerk</surname><given-names>AO</given-names></name><name><surname>Wilders</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pacemaker activity of the human sinoatrial node: effects of HCN4 mutations on the hyperpolarization-activated current</article-title><source>Journal of the Working Groups on Cardiac Pacing</source><volume>16</volume><fpage>384</fpage><lpage>395</lpage><pub-id pub-id-type="doi">10.1093/europace/eut348</pub-id><pub-id pub-id-type="pmid">24569893</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Author Correction: SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><elocation-id>352</elocation-id><pub-id pub-id-type="doi">10.1038/s41592-020-0772-5</pub-id><pub-id pub-id-type="pmid">32094914</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wallace</surname><given-names>EWJ</given-names></name><name><surname>Gillespie</surname><given-names>DT</given-names></name><name><surname>Sanft</surname><given-names>KR</given-names></name><name><surname>Petzold</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Linear noise approximation is valid over limited times for any chemical system that is sufficiently large</article-title><source>IET Systems Biology</source><volume>6</volume><fpage>102</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1049/iet-syb.2011.0038</pub-id><pub-id pub-id-type="pmid">23039691</pub-id></element-citation></ref><ref id="bib103"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Xiao</surname><given-names>F</given-names></name><name><surname>Zeng</surname><given-names>X</given-names></name><name><surname>Yao</surname><given-names>J</given-names></name><name><surname>Yuchi</surname><given-names>M</given-names></name><name><surname>Ding</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Optimal estimation of ion-channel kinetics from macroscopic currents</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e35208</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0035208</pub-id><pub-id pub-id-type="pmid">22536358</pub-id></element-citation></ref><ref id="bib104"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Watanabe</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Almost All Learning Machines are Singular</article-title><conf-name>2007 IEEE Symposium on Foundations of Computational Intelligence</conf-name><fpage>383</fpage><lpage>388</lpage><pub-id pub-id-type="doi">10.1109/FOCI.2007.371500</pub-id></element-citation></ref><ref id="bib105"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>NA</given-names></name></person-group><year iso-8601-date="2005">2005</year><source>A Course in Probability</source><publisher-loc>Boston, U.S.A</publisher-loc><publisher-name>Addison-Wesley</publisher-name></element-citation></ref><ref id="bib106"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>S</given-names></name><name><surname>Vysotskaya</surname><given-names>ZV</given-names></name><name><surname>Xu</surname><given-names>X</given-names></name><name><surname>Xie</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>Q</given-names></name><name><surname>Zhou</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>State-dependent cAMP binding to functioning HCN channels studied by patch-clamp fluorometry</article-title><source>Biophysical Journal</source><volume>100</volume><fpage>1226</fpage><lpage>1232</lpage><pub-id pub-id-type="doi">10.1016/j.bpj.2011.01.034</pub-id><pub-id pub-id-type="pmid">21354395</pub-id></element-citation></ref><ref id="bib107"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wulf</surname><given-names>M</given-names></name><name><surname>Pless</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>High-Sensitivity Fluorometry to Resolve Ion Channel Conformational Dynamics</article-title><source>Cell Reports</source><volume>22</volume><fpage>1615</fpage><lpage>1626</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2018.01.029</pub-id><pub-id pub-id-type="pmid">29425514</pub-id></element-citation></ref><ref id="bib108"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>J</given-names></name><name><surname>Zagotta</surname><given-names>WN</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Gating rearrangements in cyclic nucleotide-gated channels revealed by patch-clamp fluorometry</article-title><source>Neuron</source><volume>28</volume><fpage>369</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(00)00117-3</pub-id><pub-id pub-id-type="pmid">11144348</pub-id></element-citation></ref><ref id="bib109"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zwickl</surname><given-names>DJ</given-names></name><name><surname>Holder</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Model parameterization, prior distributions, and the general time-reversible model in Bayesian phylogenetics</article-title><source>Systematic Biology</source><volume>53</volume><fpage>877</fpage><lpage>888</lpage><pub-id pub-id-type="doi">10.1080/10635150490522584</pub-id><pub-id pub-id-type="pmid">15764557</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s7"><title>Efficiency of the Hamiltonian Monte Carlo sampler</title><p>Bayesian posterior sampling requires a fast and efficient sampler which requires in turn a fast evaluation of the likelihood and prior distribution. Optimally, the calculation time should increase as little as possible with the amount of data. This is why we chose the KF framework and parallelized the processing of a time trace across many CPUs. Compared to the maximization of the likelihood, sampling from the posterior requires approximately one to multiple orders of magnitude times more evaluations of the likelihood function to have a decent representation of the posterior. For example, in <xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref> both algorithms need roughly 10<sup>2</sup> to 10<sup>3</sup> evaluations of the likelihood function to converge. This is only a small fraction of the number of evaluations needed here to obtain a converged estimate of the posterior (in fact the typical &quot;warm-up&quot; phase of the sampler which we used is barely half completed with such a small number of evaluations).</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>The posterior samples display autocorrelation which lowers the effective sample size.</title><p>(<bold>a</bold>) Sample traces from the 6-dimensional posterior of the rate matrix recorded after the warm-up of the sampler. (<bold>b</bold>) These traces are anti-correlated (in time) such that the effective sample size with which all quantities of relevance are estimated is smaller than the actual number of samples. Note that even at one iteration the absolute value of the auto-correlation is never larger than 0.05. The standard error of the bootstrapped mean auto-correlation is in the order of <inline-formula><mml:math id="inf566"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app1-fig1-v3.tif"/></fig><p>The ratio of the number of used samples to the number of likelihood evaluations can serve as a simple efficiency measure. However, this ratio is a too optimistic measure as revealed by a look into the internal dynamics of Hamiltonian Monte Carlo sampler (HMC). HMC treats the current parameter sample as part of a phase space point of a (dynamic) Hamiltonian system. The force acting on the system is the derivative of the negative logarithm of the posterior density <xref ref-type="bibr" rid="bib5">Betancourt, 2017</xref>. Each suggested sample is derived from a ballistic movement with random kinetic energy in the augmented parameter space. Calculating this movement is done by integration of the corresponding Hamiltonian equations. Therefore, one suggested sample requires many evaluations of the gradient of the log posterior distribution. How many evaluations were needed in turn depends on the shape of the posterior and the length of the ballistic movement.</p><fig id="app1fig2" position="float"><label>Appendix 1—figure 2.</label><caption><title>Computational time in units of 1000 samples per day vs <inline-formula><mml:math id="inf567"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>data</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>CPU</mml:mi></mml:mrow></mml:math></inline-formula><inline-formula><mml:math id="inf568"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>.</title><p>The two colors represent two different implementations. The difference between the two implementations is that the derived quantities which are the predicted mean signal and the predicted signal variance for each data point in the less efficient sampling algorithm are saved. In the more efficient algorithm these derived quantities are discarded.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app1-fig2-v3.tif"/></fig><p>Strong changes of the gradient of the posterior (high curvature of the posterior) require more integration steps. Setting all those sampler parameters requires either expert knowledge about the statistical model and sampler or a sampler which automatically learns optimal sampler parameter settings (warm-up) before it starts the actual sampling of the parameters of interest. Nevertheless, HMC generates samples (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1a</xref>) from high-dimensional, correlated posteriors more efficiently than many other classical Markov chain Monte Carlo methods (<xref ref-type="bibr" rid="bib5">Betancourt, 2017</xref>). By more efficient we mean the product of the speed of drawing one correlated sample with the amount of iterations needed for the autocorrelation function of those samples to vanish sufficiently. By construction, the samples from any Markov chain Monte Carlo method are correlated (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1b</xref>). The quality of the adaptive NUTS HMC sampler provided by Stan can by seen in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1b</xref>. The sample traces show a small anticorrelation whose absolute value is hardly larger than 0.05. This is achieved by letting the sampler adapt to the geometry of the posterior in the warm-up phase. The samples collected during the warm-up phase are discarded.</p><p>Hereinafter, we show that likelihood and sampler considerations are one aspect of computational efficiency. The other is the implementation of a parallelized algorithm. The blue curve (<xref ref-type="fig" rid="app1fig2">Appendix 1—figure 2</xref>) represents the scaling of the KF with an implementation of the algorithm which saves besides the sampled parameters the predicted mean and the covariance of the signal. This means for cPCF data that for each parameter sample <inline-formula><mml:math id="inf569"><mml:mrow><mml:mrow><mml:mn>5</mml:mn><mml:mo>⋅</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>traces</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>data</mml:mi></mml:msub></mml:mrow><mml:mo>/</mml:mo><mml:mi>CPU</mml:mi></mml:mrow></mml:math></inline-formula> more memory operations are needed on the cluster node. Note, since we assigned each ligand concentration jump and the following data points to its own CPU the following relation holds: <inline-formula><mml:math id="inf570"><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>data</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>CPU</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>data</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>trace</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Skipping saving those derived quantities (orange curve) creates a speedup of roughly 2 orders of magnitude. The derived quantities are redundant quantities which we suggest to calculate by feeding a small subsample of the posterior samples to the KF filter which then reanalyzes the traces by the given draws but does not redraw from the posterior. See Git-Hub: <ext-link ext-link-type="uri" xlink:href="https://github.com/JanMuench/Tutorial_Patch-clamp_data">https://github.com/JanMuench/Tutorial_Patch-clamp_data</ext-link>.</p><p>We compare the computation time for the KF (green) and the RE approach (blue) for the 4-state model (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3a</xref>) versus the data quality by <inline-formula><mml:math id="inf571"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. The calculation time <italic>t</italic><sub><italic>calc</italic></sub> rises for both algorithms roughly like <inline-formula><mml:math id="inf572"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:math></inline-formula>. For each curve the likelihood evaluations stay constant for the whole plot. Thus this scaling relates to the integration time of the HMC sampler. Taking the average of the last 5 calculation times shows that the KF is about <inline-formula><mml:math id="inf573"><mml:mrow><mml:mn>2.7</mml:mn><mml:mo>±</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> times slower than the RE approach. The computation time for cPCF data of the 5-state-2-open states model (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3b</xref>) shows a similar <inline-formula><mml:math id="inf574"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:math></inline-formula>-regime until it seems to become independent of <inline-formula><mml:math id="inf575"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. Note, the different <inline-formula><mml:math id="inf576"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> interval which is displayed.</p><fig id="app1fig3" position="float"><label>Appendix 1—figure 3.</label><caption><title>Computational time for cPCF data.</title><p>Computational time in units of 1000 samples per day vs. the amount of ion channels per trace <inline-formula><mml:math id="inf577"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. The green curve belongs to the results from the KF, the blue curve to the results from the RE approach. The brighter blue curve corresponds to the ratio of computational time. a, four-state model with cPCF data. The posterior of the RE approach was sampled from three independent chains. We drew 18,000 samples per chain which included a large fraction of 6000 warm-up samples per chain. The posterior of the KF was sampled from 4independent chains. We drew 26,000 samples per chain which included a large fraction of 4000 warm-up samples per chain. b Computational time of the 5-state-2-open-states model with cPCF data. The KF algorithm drew 9000 samples which included a large fraction of 6,000 warm-up samples per chain. The RE algorithm drew 10,000 samples which included a large fraction of 5,000 warm-up samples per chain. In total, we used four independent chains. c, Computational time of the 6-state-1-open-state model with cPCF data. The KF algorithm drew 10,000 samples which included a large fraction of 6000 warm-up samples per chain. The RE algorithm drew 12,000 samples which included a large fraction of 7000 warm-up samples per chain. In total, we used four independent sampling chains.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app1-fig3-v3.tif"/></fig><p>Taking the average of the last 4 computation times shows that the KF is about <inline-formula><mml:math id="inf578"><mml:mrow><mml:mn>2.24</mml:mn><mml:mo>±</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> times slower than the RE approach. Surprisingly, with increasing model complexity the ratio of the computation time becomes not necessarily larger. The computation time for cPCF data of the 6-state-1-open state model (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3c</xref>) shows a similar <inline-formula><mml:math id="inf579"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:msqrt></mml:mrow></mml:math></inline-formula>-regime until it becomes independent of <inline-formula><mml:math id="inf580"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. Taking the average of the last 4 calculation times reveals that the KF is about <inline-formula><mml:math id="inf581"><mml:mrow><mml:mn>3.3</mml:mn><mml:mo>±</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> times slower than the RE approach. With increasing model complexity, the ratio of the computation times becomes larger. Note, that there is little variation of the absolute calculation time of the algorithm across model complexity which is a good prospect for more complex kinetic schemes. Nevertheless, to achieve this computational speed we used in total 72 CPUs on one node, 20 CPUs per chain for 20 time traces. Thus, nodes with more then 80 CPUs would enable a better performance. Overall, the more complex likelihood calculations of the KF require 2–3 times more calculation for the tested models and cPCF data. Surprisingly, for most part of the displayed interval the KF is faster than the RE approach (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4a</xref>) for PC data. Considering the more expensive likelihood evaluation (prediction and correction) of the KF, this result reminds that this computational time benchmark is a downstream test that integrates all aspects of the sampling. One possible explanation could be the following: We describe in the main text (<xref ref-type="fig" rid="fig4">Figures 4b1</xref>—<xref ref-type="fig" rid="fig6">6</xref>) for the RE approach, that due to the treatment of every data point as an individual draw from a multinomial distribution, HDCVs (<xref ref-type="fig" rid="fig5">Figures 5</xref>–<xref ref-type="fig" rid="fig6">6</xref>) are too narrow. This problem gets exacerbated (<xref ref-type="fig" rid="fig8">Figure 8</xref>) as we used 10 times more data points which we interpret as the cause of the KF being faster than the RE approach (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4a</xref>). The narrower the posterior is the more integration steps from the sampler are needed to calculate the proposed trajectory with sufficient precision. Another example displayed (<xref ref-type="fig" rid="app1fig4">Appendix 1—figure 4b</xref>), has an increased kinetic scheme complexity. Note that we reduced the analyzing frequency 10 times to be comparable with the analyzing frequency of cPCF data. Under this condition, the intuition from likelihood calculation complexity is confirmed that the KF is slower. Taking the average of the last 4 calculation times shows that the KF is about <inline-formula><mml:math id="inf582"><mml:mrow><mml:mn>1.8</mml:mn><mml:mo>±</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> slower than the RE algorithm for PC data.</p><fig id="app1fig4" position="float"><label>Appendix 1—figure 4.</label><caption><title>Computational time for PC data.</title><p>The computational time in units of 1,000 samples per day are plotted vs the amount of ion channels per trace <inline-formula><mml:math id="inf583"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. The green curve belongs to the results from the KF, the blue curve to the results from the RE approach. a, Four-state model with PC data. In contrast to the cPCF data (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>), <italic>f</italic><sub><italic>ana</italic></sub> is increased by an order of magnitude. Thus, 10 times more data points per time trace and CPU are evaluated. These results were obtained by four independent chains which drew 9000 samples per chain, including a large fraction of 7000 warm-up samples per chain.b, Five-state model with PC data. Note that here <italic>f</italic><sub><italic>ana</italic></sub> equals the analyzing frequency of cPCF data (<xref ref-type="fig" rid="app1fig3">Appendix 1—figure 3</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app1-fig4-v3.tif"/></fig></sec><sec sec-type="appendix" id="s8"><title>Convergence diagnostics</title><p>In the limit of infinitely many samples drawn from a posterior by any MCMC method, given that posterior space is simply connected, the sampling is guaranteed to converge to the typical set <xref ref-type="bibr" rid="bib5">Betancourt, 2017</xref>. For a finite amount of samples from the posterior it is not guaranteed that the samples are representative for the typical set of the posterior. To ensure that the trace has converged to the typical set is an active field of statistical research. A discussion of these important consistency checks (<xref ref-type="bibr" rid="bib32">Gelman and Rubin, 1992a</xref>; <xref ref-type="bibr" rid="bib34">Gelman et al., 2013</xref>; <xref ref-type="bibr" rid="bib97">Vats and Knudson, 2018</xref>; <xref ref-type="bibr" rid="bib31">Gabry et al., 2019</xref>; <xref ref-type="bibr" rid="bib98">Vehtari et al., 2021</xref>) ensuring that the sampled traces represent the typical set, is beyond the scope of this paper. In short, we usually worked with 4 independent sampling chains of the posterior (<xref ref-type="bibr" rid="bib33">Gelman and Rubin, 1992b</xref>). On the one hand, we visually checked that all 4 chains do report the same typical set to verify that the sampler is fine. This convergence can be monitored by the <inline-formula><mml:math id="inf584"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> statistics (<xref ref-type="bibr" rid="bib34">Gelman et al., 2013</xref>; <xref ref-type="bibr" rid="bib98">Vehtari et al., 2021</xref>). <inline-formula><mml:math id="inf585"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> evaluates differences between the within- and between-chain parameter estimates. A bad mixing is indicated by large values of <inline-formula><mml:math id="inf586"><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula>. Perfectly converged chains would be reported with <inline-formula><mml:math id="inf587"><mml:mrow><mml:mover accent="true"><mml:mi>R</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Additionally, the effective sample size <inline-formula><mml:math id="inf588"><mml:msub><mml:mi>N</mml:mi><mml:mi>eff</mml:mi></mml:msub></mml:math></inline-formula> of each object of interest needs to be monitored. Both of them are reported by the “summary” method from the “fit” object in PyStan. We show (<xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>) an edited version of the output from this method. The last two columns would indicate signs of missing convergence. The second column reports the Markov sampling error telling if we drew enough samples in total. Note that any quantity far out in the tails would have a much larger error. There were cases when the sampler was not fine for all chains, meaning that 3 chains showed the same posterior but one showed something different. In such cases we simply discarded that chain. This occurred in particular more often with the RE approach. As we are benchmarking methods and know the true parameter values, it was rather obvious which chain did not work well. In particular, the successful binomial test of the HDCV is a downstream test that confirms besides the assumptions of the algorithm also whether the sampler worked sufficiently. For the Bayesian filter, sampler problems occurred at very low ion channel counts below <inline-formula><mml:math id="inf589"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula>.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Typical summary statistics of the posterior as printed by the “summary” method from the “fit” object in Pystan.</title><p>We deleted the 25% and 75% percentiles to decrease the size of the table and changed some of the names with respect to the used name in the algorithm to the symbol used in this article. Note that we assumed to measure 2 ligand concentrations (which includes 4 jumps) from one patch. Thus, we assumed for 10 ligand concentrations that they are coming from 5 patches which leads to a five-dimensional channels per patch vector <inline-formula><mml:math id="inf590"><mml:msub><mml:mi mathvariant="bold">N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. The bias of <inline-formula><mml:math id="inf591"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula><inline-formula><mml:math id="inf592"><mml:mi>i</mml:mi></mml:math></inline-formula> discussed in the main article, is also observed here.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">mean</th><th align="left" valign="bottom"><italic>se</italic><sub><italic>mean</italic></sub></th><th align="left" valign="bottom">sd</th><th align="left" valign="bottom">2.5%</th><th align="left" valign="bottom">50%</th><th align="left" valign="bottom">97.5%</th><th align="left" valign="bottom"><italic>n</italic><sub><italic>eff</italic></sub></th><th align="left" valign="bottom">Rhat</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>k</italic><sub>21</sub></td><td align="left" valign="bottom">507.65</td><td align="left" valign="bottom">0.06</td><td align="left" valign="bottom">11.91</td><td align="left" valign="bottom">484.74</td><td align="left" valign="bottom">507.46</td><td align="left" valign="bottom">531.62</td><td align="left" valign="bottom">41262</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><italic>k</italic><sub>32</sub></td><td align="left" valign="bottom">481.15</td><td align="left" valign="bottom">0.16</td><td align="left" valign="bottom">28.63</td><td align="left" valign="bottom">428.34</td><td align="left" valign="bottom">480.19</td><td align="left" valign="bottom">540.32</td><td align="left" valign="bottom">33610</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><italic>k</italic><sub>43</sub></td><td align="left" valign="bottom">509.95</td><td align="left" valign="bottom">0.04</td><td align="left" valign="bottom">6.99</td><td align="left" valign="bottom">496.39</td><td align="left" valign="bottom">509.91</td><td align="left" valign="bottom">523.77</td><td align="left" valign="bottom">36006</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><italic>K</italic><sub>1</sub></td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">2.6e-5</td><td align="left" valign="bottom">5.2e-3</td><td align="left" valign="bottom">0.49</td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">0.51</td><td align="left" valign="bottom">40548</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><italic>K</italic><sub>2</sub></td><td align="left" valign="bottom">0.51</td><td align="left" valign="bottom">6.7e-5</td><td align="left" valign="bottom">0.01</td><td align="left" valign="bottom">0.49</td><td align="left" valign="bottom">0.51</td><td align="left" valign="bottom">0.54</td><td align="left" valign="bottom">33228</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><italic>K</italic><sub>3</sub></td><td align="left" valign="bottom">0.49</td><td align="left" valign="bottom">4.2e-5</td><td align="left" valign="bottom">7.5e-3</td><td align="left" valign="bottom">0.47</td><td align="left" valign="bottom">0.49</td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">31445</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">N<sub>ch</sub>[1]</td><td align="left" valign="bottom">1007.1</td><td align="left" valign="bottom">0.14</td><td align="left" valign="bottom">20.05</td><td align="left" valign="bottom">968.84</td><td align="left" valign="bottom">1006.8</td><td align="left" valign="bottom">1047.2</td><td align="left" valign="bottom">19838</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">N<sub>ch</sub>[2]</td><td align="left" valign="bottom">1006.1</td><td align="left" valign="bottom">0.14</td><td align="left" valign="bottom">20.04</td><td align="left" valign="bottom">967.83</td><td align="left" valign="bottom">1005.8</td><td align="left" valign="bottom">1046.0</td><td align="left" valign="bottom">19874</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">N<sub>ch</sub>[3]</td><td align="left" valign="bottom">1003.9</td><td align="left" valign="bottom">0.14</td><td align="left" valign="bottom">20.0</td><td align="left" valign="bottom">965.66</td><td align="left" valign="bottom">1003.6</td><td align="left" valign="bottom">1043.8</td><td align="left" valign="bottom">19877</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">Nch[4]</td><td align="left" valign="bottom">1007.0</td><td align="left" valign="bottom">0.14</td><td align="left" valign="bottom">20.09</td><td align="left" valign="bottom">968.67</td><td align="left" valign="bottom">1006.6</td><td align="left" valign="bottom">1047.1</td><td align="left" valign="bottom">19843</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">N<sub>ch</sub>[5]</td><td align="left" valign="bottom">1005.9</td><td align="left" valign="bottom">0.14</td><td align="left" valign="bottom">20.09</td><td align="left" valign="bottom">967.61</td><td align="left" valign="bottom">1005.6</td><td align="left" valign="bottom">1045.9</td><td align="left" valign="bottom">19874</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">i</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">7.0e-5</td><td align="left" valign="bottom">9.8e-3</td><td align="left" valign="bottom">0.98</td><td align="left" valign="bottom">1.0</td><td align="left" valign="bottom">1.02</td><td align="left" valign="bottom">19971</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf593"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>exp</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">0.25</td><td align="left" valign="bottom">1.2e-5</td><td align="left" valign="bottom">2.5e-3</td><td align="left" valign="bottom">0.25</td><td align="left" valign="bottom">0.25</td><td align="left" valign="bottom">0.26</td><td align="left" valign="bottom">43142</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf594"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">2.5e-3</td><td align="left" valign="bottom">1.2e-7</td><td align="left" valign="bottom">2.5e-5</td><td align="left" valign="bottom">2.5e-3</td><td align="left" valign="bottom">2.5e-3</td><td align="left" valign="bottom">2.5e-3</td><td align="left" valign="bottom">42707</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf595"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>fluo</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">1.3e-5</td><td align="left" valign="bottom">2.3e-8</td><td align="left" valign="bottom">4.6e-6</td><td align="left" valign="bottom">1.0e-5</td><td align="left" valign="bottom">1.2e-5</td><td align="left" valign="bottom">2.5e-5</td><td align="left" valign="bottom">41306</td><td align="left" valign="bottom">1.0</td></tr><tr><td align="left" valign="bottom">λ</td><td align="left" valign="bottom">9.98</td><td align="left" valign="bottom">1.1e-4</td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">9.94</td><td align="left" valign="bottom">9.98</td><td align="left" valign="bottom">10.02</td><td align="left" valign="bottom">35109</td><td align="left" valign="bottom">1.0</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s9"><title>HMC parameter settings</title><p>The sampling efficiency of an HMC algorithm is sensitive to three parameters (<xref ref-type="bibr" rid="bib71">Neal, 2011</xref>, <xref ref-type="bibr" rid="bib48">Hoffman and Gelman, 2014</xref>). The three parameters, discretization time <inline-formula><mml:math id="inf596"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>, the metric <inline-formula><mml:math id="inf597"><mml:mi mathvariant="bold">M</mml:mi></mml:math></inline-formula> and the number of steps taken <inline-formula><mml:math id="inf598"><mml:mi>L</mml:mi></mml:math></inline-formula> can be predefined or/and are adapted in the warm-up phase of the sampler. A typical number of iterations <inline-formula><mml:math id="inf599"><mml:msub><mml:mi>N</mml:mi><mml:mi>warm</mml:mi></mml:msub></mml:math></inline-formula> in the warm-up phase for each chain is between <inline-formula><mml:math id="inf600"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>warm</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mn>3</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>⁢</mml:mo><mml:mn>7</mml:mn></mml:mrow><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Since a sufficient <inline-formula><mml:math id="inf601"><mml:msub><mml:mi>N</mml:mi><mml:mi>warm</mml:mi></mml:msub></mml:math></inline-formula> depends on the correlation structure of the posterior, which depends on the quality and quantity of the data, we did not engage to optimize the number of warm-up steps. We rather checked whether for a set of different data (for example with different <inline-formula><mml:math id="inf602"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>) all chains are fine.</p></sec><sec sec-type="appendix" id="s10"><title>Stan</title><p>Stan is a probabilistic programming language that provides the research community with easy access to HMC sampling, variational Bayes, and optimization procedures like Maximum posterior/ML approaches. Stan uses automatic analytical differentiation of the statistical model such that no analytical treatment by hand has to be done for any tested model. The only requirement from the user is to define a statistical model of the data. The Stan compiler calculates the analytical derivatives of the model and then translates the statistical model to C++code and after that into an executable program. Furthermore, Stan uses adaptive HMC such that the length of the ballistic trajectory as well as the variance of the random kinetic energy to create the ballistic trajectory and the number of integration steps are optimized automatically. Then the program can be started from any high-level data processing language of the user’s choice such as Python, Matlab, Julia, R or from the command line.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s11"><title>Markov models for a single ion channel</title><p>Markov models and rate models are widely used for modeling molecular kinetics. They provide an interpretation of the data in terms of a set of functional states and the transition rates between these states. Markov models can be estimated from experimentally recorded data as well as from computer simulation data. The use of Markov models with one-step memory is supported by the concept of the molecular free energy landscape. Molecular energy landscapes are typically characterized by conformationally well-defined free-energy minima that are separated by free-energy barriers. State transitions in molecules are thermally activated barrier-crossing events on this landscape (<xref ref-type="bibr" rid="bib29">Frauenfelder et al., 1991</xref>) leading to a rapid equilibration of the system in the vicinity of this new minimum. Memory of other minima that have been visited in the past is not required. Regarding the wide spectrum of time scales at which processes in a protein take place, one has to be aware that there is typically a small number of relaxation modes with excessively long autocorrelation times and many relaxation modes with much faster autocorrelation times. To model the slow, experimentally accessible processes, it is sufficient to retain the small number of slow modes (<xref ref-type="bibr" rid="bib74">Noé et al., 2011</xref>). It has been shown rigorously that working with the set of slow modes is equivalent to model the state dynamics with a small number of fuzzily defined metastable states in the full conformational space (<xref ref-type="bibr" rid="bib24">Deuflhard and Weber, 2005</xref>) Later it has been shown that the set of slow modes can be well approximated with a hidden Markov model (<xref ref-type="bibr" rid="bib75">Noé et al., 2013</xref>).</p></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s12"><title>Eigenvalues and fitting of sums exponential decays: Mean time evolution of the spectral components of the time evolution</title><p>From the experimental perspective we are used to fit sums of exponential decays to time series data of any signal such as currents or fluorescence.<disp-formula id="equ74"><label>(63)</label><mml:math id="m74"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi mathvariant="normal">∞</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>M</mml:mi></mml:munderover><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The mathematical background for this procedure is the spectral decomposition of the solution of the corresponding RE equation<disp-formula id="equ75"><label>(64)</label><mml:math id="m75"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>Kp</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which is a vector differential equation whose evolution is governed by the rate matrix <inline-formula><mml:math id="inf603"><mml:mi mathvariant="bold">K</mml:mi></mml:math></inline-formula>.The general solution to the mean time evolution of the system is<disp-formula id="equ76"><mml:math id="m76"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>K</mml:mi><mml:mo>⋅</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A Markov state model has as many real valued eigenvalues as it has states <inline-formula><mml:math id="inf604"><mml:mi>M</mml:mi></mml:math></inline-formula>. With those eigenvalues one can decompose the solution into its spectral components<disp-formula id="equ77"><mml:math id="m77"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>All of the eigenvalues obey <inline-formula><mml:math id="inf605"><mml:mrow><mml:mi>α</mml:mi><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. One of them always fulfills <inline-formula><mml:math id="inf606"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> which belongs to the equilibrium probability distribution of the ion channel. With <inline-formula><mml:math id="inf607"><mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> we can write the sum as<disp-formula id="equ78"><label>(65)</label><mml:math id="m78"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The eigenvalue <inline-formula><mml:math id="inf608"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> determines the equilibrium solution. Due to <inline-formula><mml:math id="inf609"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> the solution is stable and converges to<disp-formula id="equ79"><mml:math id="m79"><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The time evolution of the mean signal in its spectral components can then be derived<disp-formula id="equ80"><label>(66)</label><mml:math id="m80"><mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>p</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>by multiplying with the observation matrix <inline-formula><mml:math id="inf610"><mml:mi mathvariant="bold">H</mml:mi></mml:math></inline-formula> and setting <inline-formula><mml:math id="inf611"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>τ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> we derived <xref ref-type="disp-formula" rid="equ74">Equation 63</xref>, <xref ref-type="bibr" rid="bib17">Colquhoun and Hawkes, 1995a</xref>. To derive the likelihood based on this deterministic approximation <xref ref-type="bibr" rid="bib67">Milescu et al., 2005</xref> one assumes for each data point that the ensemble state <inline-formula><mml:math id="inf612"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is drawn from<disp-formula id="equ81"><label>(67)</label><mml:math id="m81"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi>multinomial</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with the additional approximation of the multinomial distribution by a multivariate normal distribution. Subsequently, the equation<disp-formula id="equ82"><label>(68)</label><mml:math id="m82"><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mi>normal</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>Σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>provides the contribution of the recording system to the signal of the ensemble state. In contrast, the KF approximates a<disp-formula id="equ83"><label>(69)</label><mml:math id="m83"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mi>generalized</mml:mi><mml:mo>-</mml:mo><mml:mi>multinomial</mml:mi></mml:mrow><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>distribution with multivariate normal distributions (see Methods) and then the details of the experimental setup are adjoined</p></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s13"><title>A note about Kalman filtering within other frameworks such as variational Bayes, maximum a posteriori and ML</title><p>We like to emphasize that the key property of the Kalman filter likelihood for an autocorrelated time series (see <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>) to factorize over time enables us to treat each data point as if it were independent. The alternative is to treat the whole time series as one high-dimensional data point as done in <xref ref-type="bibr" rid="bib12">Celentano and Hawkes, 2004</xref>. A ML Kalman filter (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>) would try to optimize <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, which is usually done in the log space to improve numerical stability. Taking the logarithm of <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> gives also an insight for the reader, who might be more experienced in using residual sums of squares for fitting.<disp-formula id="equ84"><label>(70)</label><mml:math id="m84"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>where we used the logarithmic algebra rule <inline-formula><mml:math id="inf613"><mml:mrow><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∏</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>i</mml:mi></mml:msub><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> Using the functional form of the multi-variate normal distribution, we arrive at<disp-formula id="equ85"><label>(71)</label><mml:math id="m85"><mml:mrow><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>det</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mtext>,</mml:mtext></mml:mrow></mml:math></disp-formula></p><p>which is a vectorial residual sum of squares whose summands are weighted by <inline-formula><mml:math id="inf614"><mml:mrow><mml:mrow><mml:msub><mml:mi mathvariant="bold">HP</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">H</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="bold">Σ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The past observations of time series are encoded in <inline-formula><mml:math id="inf615"><mml:msub><mml:mi mathvariant="bold">P</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf616"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which are calculated with the Kalman filtering equations before the likelihood can be evaluated.</p><p>Generally, the user of our algorithm is not restricted to the full Bayesian inference. He/she can also stick to optimizing <xref ref-type="disp-formula" rid="equ85">Equation 71</xref>. This is because in the probabilistic programming language Stan one only formulates the statistical model. The method to evaluate the statistical model and the data is provided by the Stan compiler and the high level programming language interface. Together they allow to switch quickly between HMC sampling, variational Bayesian posterior estimation, maximum a posteriori and ML. In particular, the variational Bayesian posterior might be a way to ease the computational burden due HMC sampling while keeping many of the benefits of knowing the full posterior distribution.</p><p>In the previous versions before PyStan 3.0 all three approaches were supported. Currently, with PyStan 3.0 only HMC sampling of the posterior is supported but, if desired, any other interface can be used (Matlab, Julia, R, CmdStan, Pystan 2.9, CmdStanPy,…) to perform optimization or variational Bayesian approaches. No actual changes in the Stan code are required, eventually with the exception that the CPU based parallelization needs to be eliminated. Given that one chooses optimization, the differences between maximum a posteriori/penalized maximum likelihood or only ML are just a matter of prior choice which need to be coded into the statistical model. Apart from those minor points, there is no need of extensive re-implementation of the code.</p></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s14"><title>The fluorescence signal of cPCF experiments</title><sec sec-type="appendix" id="s14-1"><title>First four moments of a photomultiplier signal</title><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Benchmark of the signal variance vs its mean for experimental solution data recorded under cPCF conditions: The concentrations of the fluorescent ligand were 0.25, 3, and 15 <inline-formula><mml:math id="inf617"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:math></inline-formula> and a reference dye was present. The laser intensities covered 1.6 orders of magnitude at constant detection settings. The data points were obtained from <inline-formula><mml:math id="inf618"><mml:mrow><mml:mrow><mml:mn>1.4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>6</mml:mn></mml:msup></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>pixel</mml:mi></mml:mrow></mml:math></inline-formula>. The red and blue lines indicate the theoretical prediction for a Poisson and Gamma distribution, respectively, assuming <inline-formula><mml:math id="inf619"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The linear relation allows to relate the measured a.u. (top, right axis) to photons (bottom, left axis). The inset provides the corresponding log-log plot. Important for the KF algorithm is that skewness and excess kurtosis is small.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app5-fig1-v3.tif"/></fig><p>In this work, the KF analysis assumes Poisson statistics for the fluorescence signal in cPCF. Many commercial microscopes are not equipped with photon counting detectors or detectors are not operated in photon-counting mode, often to due to ease of use or limitation in dynamic range. Therefore, it is important to verify that the fluorescence signal follows, at least approximately, Poisson counting statistics. In particular, for the KF it is assumed that higher order statistics, such as skewness and excess kurtosis, vanish. The central assumption of the derivation of our Bayesian filter is <inline-formula><mml:math id="inf620"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula></p><p>Here we show that this assumption for the detectors used in our previously used LSM 710 (Carl Zeiss, Jena) system (<xref ref-type="bibr" rid="bib6">Biskup et al., 2007</xref>; <xref ref-type="bibr" rid="bib61">Kusch et al., 2010</xref>) is fulfilled by re-scaling to photon-numbers. The measured variance obeys <inline-formula><mml:math id="inf621"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">l</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. It depends linearly on the mean signals (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1</xref>).<disp-formula id="equ86"><label>(72)</label><mml:math id="m86"><mml:mrow><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ87"><label>(73)</label><mml:math id="m87"><mml:mrow><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ88"><label>(74)</label><mml:math id="m88"><mml:mrow><mml:mrow><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi mathvariant="normal">E</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For the scaled signal <inline-formula><mml:math id="inf622"><mml:mi>x</mml:mi></mml:math></inline-formula> being Poisson distributed follows <inline-formula><mml:math id="inf623"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula>. Re-scaling of the signal by <inline-formula><mml:math id="inf624"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> provides approximately Poisson distributed values. A linear fit yields <inline-formula><mml:math id="inf625"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>205</mml:mn><mml:mo>⁢</mml:mo><mml:mtext>a.u.</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>16</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>bit</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:mtext>photon</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> (for 680 V PMT voltage, 3.26 <inline-formula><mml:math id="inf626"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:math></inline-formula> pixel dwell time). (<xref ref-type="fig" rid="app5fig2">Appendix 5—figures 2</xref> and <xref ref-type="fig" rid="app5fig3">3</xref>) show that excess kurtosis and skewness remain small at all levels of photons/pixel but are somewhat higher than theoretically predicted for Poisson-distributed data. The proportionalities are correctly described by the Poisson distribution assumption but the skewness and the kurtosis are too small by a constant factor of <inline-formula><mml:math id="inf627"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> and 4, respectively. This finding has to be verified for different experimental conditions, because at lower concentration/particle densities and higher count rates, particle number fluctuations can dominate statistics (<xref ref-type="bibr" rid="bib7">Brown et al., 2008</xref>). For comparison another option would be a Gamma distribution which has the mean and the variance of <inline-formula><mml:math id="inf628"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf629"><mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. Thus, the applied scaling requires that <inline-formula><mml:math id="inf630"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The Gamma distribution has a higher skewness by factor two (independently of θ) than a Poisson distribution and overscores the skewness and excess kurtosis of the detector. For simplicity only the Poisson distribution is considered in this work. In conclusion: Typical cPCF fluorescence signal detection rates are well approximated by a Gamma or Poisson distribution which in turn have the desired property that can be approximated by a normal distribution.</p><fig id="app5fig2" position="float"><label>Appendix 5—figure 2.</label><caption><title>Benchmark of the signal Skewness vs its the mean for the identical experimental solution data under cPCF conditions.</title><p>The Skewness is small but the values are slightly larger than theoretically predicted. The inset provides the corresponding log-log plot. Important for the KF algorithm is that skewness and excess kurtosis is small.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app5-fig2-v3.tif"/></fig><fig id="app5fig3" position="float"><label>Appendix 5—figure 3.</label><caption><title>Benchmark of the signal Kurtosis vs its mean for the identical experimental solution data under cPCF conditions.</title><p>The Kurtosis is small but the values are slightly larger than theoretically predicted. The inset provides the corresponding log-log plot. Important for the KF algorithm is that skewness and excess kurtosis is small.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app5-fig3-v3.tif"/></fig></sec><sec sec-type="appendix" id="s14-2"><title>Background noise statistics</title><p>In cPCF measurements with fluorescence-labeled ligands, the signals of the ligands bound to the receptors overlap with the signals from freely diffusing fluorescence-labelled ligands in the bulk. This bulk signal is subtracted from the total signal (<xref ref-type="bibr" rid="bib6">Biskup et al., 2007</xref>). While the mean difference signal <inline-formula><mml:math id="inf631"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>fl</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the confocal voxel <inline-formula><mml:math id="inf632"><mml:mi>k</mml:mi></mml:math></inline-formula> represents the bound ligands in that voxel, its noise <inline-formula><mml:math id="inf633"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> originates from both bound and bulk ligands. The additional bulk signal, e.g. the fraction of bulk solution inside that voxel, varies from voxel to voxel and can hardly be described theoretically. Nevertheless, it can be determined experimentally (<xref ref-type="bibr" rid="bib6">Biskup et al., 2007</xref>). At low expression levels or at ligand concentrations above low nano-molar levels, this background signal is not negligible. It scales linearly with the ligand concentration, while the signal from bound receptors depends on the affinity, as estimated by the concentration of half maximum binding <inline-formula><mml:math id="inf634"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>, and the number of ion channels in the membrane of the observed volume. The binding signal saturates at high concentrations (<xref ref-type="fig" rid="app5fig4">Appendix 5—figure 4</xref>). Thus, both high affinity (low <inline-formula><mml:math id="inf635"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula>) and high expression reduce the relative contribution of the background to the overall signal, improving the signal to noise ratio.</p><fig id="app5fig4" position="float"><label>Appendix 5—figure 4.</label><caption><title>Simulated binding signals.</title><p>a, Comparison of binding of a labeled ligand at two concentrations. A simple two-ligand binding process was simulated with the Hill equation for the two expression levels of 1000 or 10,000 binding sites per patch and a BC<sub>50</sub> of 1000 (BC<sub>50</sub>a) or 10,000(BC<sub>50</sub>b), respectively, given in molecules per observation unit. The observed signal is the sum of the signal from ligands free in solution and bound to the receptors. The solution signal scales linearly with the concentration, while the binding signal saturates.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app5-fig4-v3.tif"/></fig><fig id="app5fig5" position="float"><label>Appendix 5—figure 5.</label><caption><title>Simulated binding signals.</title><p>Relative contribution of the binding signal to the total signal. Note that the contribution of the binding signal scales linearly with the expression level and inversely with the.<inline-formula><mml:math id="inf636"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mn>50</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app5-fig5-v3.tif"/></fig><p>Practically, the bulk signal is estimated by counter-staining the solution with a spectrally distinct reference dye (<xref ref-type="bibr" rid="bib6">Biskup et al., 2007</xref>). The spatial distribution of this dye mimics the spatial distribution of the freely diffusing ligands. The bulk absolute concentration as well as the molecular brightness of the reference dye and the labeled ligand differ. Hence, the binding signal is calculated as the average pixel intensity of the scaled difference image between the signal of labeled ligand and reference dye according to<disp-formula id="equ89"><label>(75)</label><mml:math id="m89"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>fl</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>lig</mml:mi><mml:mo>,</mml:mo><mml:mi>total</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>lig</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>fl</mml:mi><mml:mo>,</mml:mo><mml:mi>ref</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>ref</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>lig</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>lig</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>ref</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>ref</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mtext>,</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf637"><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>lig</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf638"><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>ref</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the arithmetic mean background signals of the ligand and reference dye recorded beyond the membrane where no signal should be recorded. They represent a signal offset which needs to be subtracted. The mean intensities in the bulk, <inline-formula><mml:math id="inf639"><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>bulk</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf640"><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>ref</mml:mi></mml:msub></mml:math></inline-formula>, are estimated outside the pipette. In order to get the correct scaling, the mean intensities need to be corrected by the respective mean background signals. If <inline-formula><mml:math id="inf641"><mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>lig</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>lig</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>ref</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>λ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mrow><mml:mi>ref</mml:mi><mml:mo>,</mml:mo><mml:mi>back</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> holds then <inline-formula><mml:math id="inf642"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>fl</mml:mi><mml:mo>,</mml:mo><mml:mi>bin</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> would be Skellam distributed (<xref ref-type="bibr" rid="bib50">Hwang et al., 2007</xref>). The total signal is then <inline-formula><mml:math id="inf643"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>fl</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>k</mml:mi></mml:msub><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>fl</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>. This procedure creates <inline-formula><mml:math id="inf644"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>ζ</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> but adds an additional noise term <inline-formula><mml:math id="inf645"><mml:mrow><mml:mi mathvariant="bold-italic">ζ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. For the general case of different intensities, we name the distribution ’scaled Skellam distributed’. The scaling variance of the background noise in each voxel of the difference image<disp-formula id="equ90"><label>(76)</label><mml:math id="m90"><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ζ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is derived from simulated data further below. <inline-formula><mml:math id="inf646"><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf647"><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:math></inline-formula> are the fluorescence intensity from the freely diffusing ligands and reference dye molecules per voxel, respectively. <inline-formula><mml:math id="inf648"><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf649"><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:math></inline-formula> are proportional to the volume fraction of the voxel, which is occupied by the bulk, and to the respective concentrations. To achieve a symmetric <inline-formula><mml:math id="inf650"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ζ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, one can set <inline-formula><mml:math id="inf651"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The summed variance of all selected voxels can be tabulated according to<disp-formula id="equ91"><label>(77)</label><mml:math id="m91"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="right"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ζ</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd><mml:mtd columnalign="right"><mml:mn>0</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To mimic an experiment which creates time series data <inline-formula><mml:math id="inf652"><mml:mrow><mml:mi>ζ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, we draw Poisson numbers for the signal from the membrane <inline-formula><mml:math id="inf653"><mml:mrow><mml:mi>Poisson</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">Hn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and for the signal from the bulk we draw numbers from the two respective Poisson distributions. Then subtraction of the two background signals is performed according to<disp-formula id="equ92"><label>(78)</label><mml:math id="m92"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>bulk</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>lig</mml:mi><mml:mo>,</mml:mo><mml:mi>bulk</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>ref</mml:mi><mml:mo>,</mml:mo><mml:mi>bulk</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>lig</mml:mi><mml:mo>,</mml:mo><mml:mi>bulk</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>ref</mml:mi><mml:mo>,</mml:mo><mml:mi>bulk</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>assuming that the dark count signal has been correctly subtracted. Then we add the bulk signal to the bound ligand signal. In this way we produce a time trace with colored noise by the Gillespie algorithm and add white noise to time traces as it is observed in real experiments.</p></sec></sec><sec sec-type="appendix" id="s15"><title>Deriving the moments of the background noise for the difference signal</title><fig id="app5fig6" position="float"><label>Appendix 5—figure 6.</label><caption><title>Master curves of 2nd till 4th centralized moment of photon counting noise ζ arising from the difference signal of fluorescent ligands and the dye in the bulk.</title><p>The curves are created from <inline-formula><mml:math id="inf654"><mml:mrow><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> draws from Poisson distributions with different combinations of intensities for the reference dye <inline-formula><mml:math id="inf655"><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:math></inline-formula> and of the intensity of the confocal voxel fraction.<inline-formula><mml:math id="inf656"><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub></mml:math></inline-formula></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app5-fig6-v3.tif"/></fig><p>For the KF the variance, skewness and kurtosis arising from the background noise has to be calculated. Skewness and excess kurtosis of the distribution have to be small compared to the total variance of the signal including all noise sources because only in this case the KF algorithm can be considered as the optimal solution for the filtering and inference problem (<xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>). In the following the 2nd to 4th moment of ζ are derived. The noise intensity parameter of the reference dye <inline-formula><mml:math id="inf657"><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:math></inline-formula> is proportional to <inline-formula><mml:math id="inf658"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ref</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>bulk</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with <inline-formula><mml:math id="inf659"><mml:mi>V</mml:mi></mml:math></inline-formula> being the confocal volume fraction containing fluorophores and <inline-formula><mml:math id="inf660"><mml:msub><mml:mi>ρ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:math></inline-formula> the density of the fluorophores in this volume. In <xref ref-type="fig" rid="app5fig6">Appendix 5—figure 6</xref> we deduce master curves for the variance skewness and excess kurtosis of the white noise by drawing <inline-formula><mml:math id="inf661"><mml:mrow><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>5</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> Poisson numbers from the respective Poisson distribution and subtract them from each other according to Appendix 5 <xref ref-type="disp-formula" rid="equ92">Equation 78</xref>. The variance is derived empirically to be<disp-formula id="equ93"><label>(79)</label><mml:math id="m93"><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>ζ</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:msubsup><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mtext>.</mml:mtext></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="fig" rid="app5fig6">Appendix 5—figure 6a</xref>, we confirm the intuition <inline-formula><mml:math id="inf662"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi><mml:mo>⇒</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ζ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. Optimally, the skewness should be zero to avoid a biased estimate of <inline-formula><mml:math id="inf663"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> when the data are analyzed by the KF. Empirically, for <inline-formula><mml:math id="inf664"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>≪</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> the skewness holds<disp-formula id="equ94"><label>(80)</label><mml:math id="m94"><mml:mrow><mml:mrow><mml:mi>skew</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ζ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:msqrt></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub></mml:mfrac></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>Additionally for <inline-formula><mml:math id="inf665"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> the skewness holds<disp-formula id="equ95"><label>(81)</label><mml:math id="m95"><mml:mrow><mml:mrow><mml:mi>skew</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ζ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msqrt><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:msqrt></mml:mrow><mml:mo>≤</mml:mo><mml:msqrt><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub></mml:mfrac></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>It is zero when <inline-formula><mml:math id="inf666"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. The KF is optimal if the kurtosis excess approaches zero, in other words if ζ is distributed normally. Empirically the kurtosis holds this<disp-formula id="equ96"><label>(82)</label><mml:math id="m96"><mml:mrow><mml:mrow><mml:mi>kur</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ζ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub></mml:mrow><mml:mo>≤</mml:mo><mml:mfrac><mml:msub><mml:mi>λ</mml:mi><mml:mi>ref</mml:mi></mml:msub><mml:msub><mml:mi>λ</mml:mi><mml:mi>lig</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>for <inline-formula><mml:math id="inf667"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The relative intensity <inline-formula><mml:math id="inf668"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> of the voxel fraction compared to the intensity <inline-formula><mml:math id="inf669"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> depends on the affinity of the ligand to the receptor, the number of receptors in the patch, and the density of the fluorophores <inline-formula><mml:math id="inf670"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> at the patch. For larger concentrations the ratio should be <inline-formula><mml:math id="inf671"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>λ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec></app><app id="appendix-6"><title>Appendix 6</title><sec sec-type="appendix" id="s16"><title>Output statistics of Bayesian filters</title><sec sec-type="appendix" id="s16-1"><title>Classical Kalman Filter without open-channel noise</title><p>Assuming that current measurements are only compromised by additive technical white noise ν but do not contain open-channel noise <inline-formula><mml:math id="inf672"><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub></mml:math></inline-formula>, then our noise model reduces to<disp-formula id="equ97"><label>(83)</label><mml:math id="m97"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>ν</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">⇔</mml:mo><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">O</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>normal</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The noise term <inline-formula><mml:math id="inf673"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> has a mean of <inline-formula><mml:math id="inf674"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and variance <inline-formula><mml:math id="inf675"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. One has to keep in mind that we have to add an extra variance term originating from the dispersion of channels over the state space, as encoded by <inline-formula><mml:math id="inf676"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf677"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The uncertainty <inline-formula><mml:math id="inf678"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is calculated using Methods <xref ref-type="disp-formula" rid="equ32">Equation 30</xref>. The variance of the total output is<disp-formula id="equ98"><label>(84a)</label><mml:math id="m98"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">v</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ99"><label>(84b)</label><mml:math id="m99"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ100"><label>(84c)</label><mml:math id="m100"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>Hn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>Hn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ101"><label>(84d)</label><mml:math id="m101"><mml:mrow><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mi>E</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:msup><mml:mi>H</mml:mi><mml:mo>⊤</mml:mo></mml:msup><mml:mo>+</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>ν</mml:mi><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ102"><label>(84e)</label><mml:math id="m102"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>HP</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The two cross terms <inline-formula><mml:math id="inf679"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">H</mml:mi><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf680"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold">n</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>ν</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are zero since ν is independent of <inline-formula><mml:math id="inf681"><mml:mi mathvariant="bold">n</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf682"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi mathvariant="normal">m</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Our derivation is equivalent to marginalization over the predicted normal prior of the ensemble state <inline-formula><mml:math id="inf683"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="double-struck">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">Y</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> at the time of the measurement except that the prior distribution could be any probability distribution with some mean and variance. <xref ref-type="disp-formula" rid="equ98">Equation 84a</xref> is the classical KF variance prediction of a signal. The first term in <xref ref-type="disp-formula" rid="equ98">Equation 84a</xref>, describes the variance from stochastic gating and that the ensemble state is hidden. Notably, by Methods <xref ref-type="disp-formula" rid="equ32">Equation 30</xref> we realize that <inline-formula><mml:math id="inf684"><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> contains information about <inline-formula><mml:math id="inf685"><mml:mi mathvariant="bold">T</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf686"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which we can exploit with the KF framework.</p></sec><sec sec-type="appendix" id="s16-2"><title>A generalized Kalman filter with state-dependent open-channel noise</title><p>In addition to the standard KF with only additive noise (<xref ref-type="bibr" rid="bib68">Moffatt, 2007</xref>; <xref ref-type="bibr" rid="bib1">Anderson and Moore, 2012</xref>; <xref ref-type="bibr" rid="bib14">Chen, 2003</xref>), fluctuations arising from the single-channel gating lead to a second white-noise term <inline-formula><mml:math id="inf687"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, causing state-dependency of our noise model. The output model is then<disp-formula id="equ103"><label>(85)</label><mml:math id="m103"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">⇔</mml:mo><mml:mi>y</mml:mi><mml:mo>∼</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mtext>normal</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The second noise term <inline-formula><mml:math id="inf688"><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub></mml:math></inline-formula> is defined in terms of the first two moments <inline-formula><mml:math id="inf689"><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and therefore <inline-formula><mml:math id="inf690"><mml:mrow><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi mathvariant="bold">n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. To the best of our knowledge such a state-dependent noise makes the following integration intractable<disp-formula id="equ104"><label>(86a)</label><mml:math id="m104"><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mi>normal</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>Hn</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="5.3pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>normal</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ105"><label>(86b)</label><mml:math id="m105"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true"><mml:mrow><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>-</mml:mo><mml:mi>Hn</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>P</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mover accent="true"><mml:mi>n</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo rspace="4.2pt">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>When assuming that the relative fluctuations of <inline-formula><mml:math id="inf691"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are on average small then <italic>n</italic><sub>4</sub> in the denominator is close to <inline-formula><mml:math id="inf692"><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the state. Thus, the incremental likelihood can be written as in the standard KF, with the only difference that the measurement noise is the sum of two components.<disp-formula id="equ106"><label>(87)</label><mml:math id="m106"><mml:mrow><mml:mi>y</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mi>normal</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mover><mml:mrow><mml:mi mathvariant="bold">n</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">m</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">p</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:msub><mml:mover><mml:mi>n</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">P</mml:mi></mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="bold">H</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">⊤</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To see that this approximation of the variance is correct, we apply the law of total variance decomposition <xref ref-type="bibr" rid="bib105">Weiss, 2005</xref>.<disp-formula id="equ107"><label>(88a)</label><mml:math id="m107"><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ108"><label>(88b)</label><mml:math id="m108"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>var</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>Hn</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ109"><label>(88c)</label><mml:math id="m109"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>op</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mn>4</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>HP</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>H</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The terms <inline-formula><mml:math id="inf693"><mml:mrow><mml:mrow><mml:mi mathvariant="bold">HP</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="bold">H</mml:mi><mml:mo>⊤</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>m</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:math></inline-formula> are the standard output covariance matrix. Again <inline-formula><mml:math id="inf694"><mml:mrow><mml:mi mathvariant="bold">P</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> contains information about <inline-formula><mml:math id="inf695"><mml:mi mathvariant="bold">T</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf696"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> while the additional variance term includes information about the current <inline-formula><mml:math id="inf697"><mml:mrow><mml:mi mathvariant="bold">n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The information contained in the noise influences likelihood in two ways. By the variance or covariance of the current <inline-formula><mml:math id="inf698"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> but also for <inline-formula><mml:math id="inf699"><mml:mrow><mml:mi mathvariant="bold">y</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in correction step by the Kalman gain <inline-formula><mml:math id="inf700"><mml:msub><mml:mi mathvariant="bold">K</mml:mi><mml:mi>Kal</mml:mi></mml:msub></mml:math></inline-formula> matrix defined in the next section.</p></sec></sec></app><app id="appendix-7"><title>Appendix 7</title><sec sec-type="appendix" id="s17"><title>Error induced for the RE or KF approach by analog filtering of PC data</title><fig id="app7fig1" position="float"><label>Appendix 7—figure 1.</label><caption><title>Influence of analog filtering on the accuracy and precision of the maximum of the posterior from PC data We chose for the analog signal <inline-formula><mml:math id="inf701"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>exp</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf702"><mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>op</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>, thus a strong background noise.</title><p>For the ensemble size, we chose <inline-formula><mml:math id="inf703"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>4</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> such that the likelihood dominates the uniform prior. (<bold>a</bold>) Estimate of the distribution of the accuracy (mean Euclidean error of the median of the posterior) vs. the cut-off frequency of a 4th-order Bessel filter scaled to the channel time scale. The solid line is the mean median of 5 data sets of the respective RE posterior (blue) and KF posterior (green). The green shaded area indicates the 0.6 quantile (ranging from 0.2 till 0.8) demonstrating the distribution of the error of the median of the posterior due to the randomness of data.(<bold>b</bold> 1–3) Accuracy and precision of the maxima of the posterior <inline-formula><mml:math id="inf704"><mml:msub><mml:mover accent="true"><mml:mi>k</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> of the maxima of the posterior rates vs. the cut-off frequency of a Bessel filter. The shaded areas indicate the 0.6 quantiles (ranging from 0.2 till 0.8) due to the variability from data set do data set while the error bars show the standard error of the mean. (<bold>b</bold> 4–6) Accuracy and precision of the maxima of the posterior <inline-formula><mml:math id="inf705"><mml:msub><mml:mover accent="true"><mml:mi>K</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:math></inline-formula> of the maxima of the posterior of the corresponding equilibria vs. the cut-off frequency of a Bessel filter.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app7-fig1-v3.tif"/></fig><p>In order to mimic an analog signal before the analog-to-digital conversion we simulated 5 different data sets of 100 kHz signals which were then filtered by a digital fourth-order Bessel filter. The activation curves were then analyzed with the Bayesian filter at 125 Hz and the deactivation curves at sampling rates between <inline-formula><mml:math id="inf706"><mml:mrow><mml:mn>166</mml:mn><mml:mo>-</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math></inline-formula> Hz. Operating the Bayesian filter at a lower frequency is necessary because due to applying the Bessel filter the former white noise of the signal obtained additional time correlations. Thus, an all data points fit would immediately violate the white noise assumption of <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> which we restore by analyzing at a much lower frequency. We then let the time scales of the induced time correlations become larger and larger by decreasing <italic>f</italic><sub><italic>cut</italic></sub>. We show (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1a</xref>) the results of the Euclidean Error for 3 different cases for PC data. The RE approach (blue) and the Bayesian filter (red) share the same analyzing frequency. In contrast, an increased <inline-formula><mml:math id="inf707"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1660</mml:mn><mml:mo>-</mml:mo><mml:mn>5000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> Hz (green) is used for the Bayesian filter. Both algorithms (blue, red) with the same <italic>f</italic><sub><italic>ana</italic></sub> show a similar rather constant region separated by an offset. Similar, to cPCF data the KF is more robust. The Bayesian filter with <inline-formula><mml:math id="inf708"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1660</mml:mn><mml:mo>-</mml:mo><mml:mn>5000</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> Hz (green) does not show this constant region but outperforms the Bayesian filter with the lower <italic>f</italic><sub><italic>ana</italic></sub> if the recording system is operated with minimal analog filtering. This becomes even more apparent when we consider the single parameter deviation vs. <italic>f</italic><sub><italic>cut</italic></sub> (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1b(1-6)</xref>). Note, the strong dependence of the critical <italic>f</italic><sub><italic>cut</italic></sub> at which the performance of all algorithms becomes suddenly worse scales with <italic>f</italic><sub><italic>ana</italic></sub>. Additionally, it is crucial, independently of the algorithm, that <inline-formula><mml:math id="inf709"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>cut</mml:mi></mml:msub><mml:mo>≫</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>ana</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. In (<xref ref-type="fig" rid="app7fig1">Appendix 7—figure 1b(1-6)</xref>) we demonstrate the distribution of the errors on the single parameter level and compare only the KF for different <italic>f</italic><sub><italic>ana</italic></sub>. Similar to the findings in the main text, on the one hand, the Euclidean error (red) shows some robustness against <italic>f</italic><sub><italic>cut</italic></sub>. On the other hand, the influence of <italic>f</italic><sub><italic>cut</italic></sub> on the individual parameter level sets in immediately and is complex. The best results are achieved with minimal analog filtering and a high <italic>f</italic><sub><italic>ana</italic></sub>.</p></sec></app><app id="appendix-8"><title>Appendix 8</title><sec sec-type="appendix" id="s18"><title>Details of the specified two methods to count the probability mass needed to include the true value</title><p>In <xref ref-type="fig" rid="fig4">Figure 4c</xref> we compare the true against the expected success probabilities of finding the complete true rate matrix within an <inline-formula><mml:math id="inf710"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional HDCV for different expected success probabilities of a perfect model. By ‘expected’ and ‘perfect’ we refer to a fictitious ideal algorithm which exactly models exhaustively all details of the true process, including all measurement details. The first way assumes a multivariate normal posterior distribution with <inline-formula><mml:math id="inf711"><mml:mrow><mml:mrow><mml:mi>ℙ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>y</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>post</mml:mi></mml:msub></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>normal</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">Σ</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Then the ellipsoid of a constant probability density is exactly the surface of a HDCV of given probability mass <inline-formula><mml:math id="inf712"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For a two-dimensional representation see <xref ref-type="fig" rid="fig4">Figure 4d</xref> below the diagonal. In one dimension, the ellipsoid consists of the two points with a distance <inline-formula><mml:math id="inf713"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msqrt><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:msqrt><mml:mo>/</mml:mo><mml:mi>σ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> from the mean (inset <xref ref-type="fig" rid="fig5">Figure 5a</xref>). In general, the <inline-formula><mml:math id="inf714"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>-dimensional ellipsoids around the mean of a multivariate normal distribution can be described by points which have the same Mahalanobis distance <inline-formula><mml:math id="inf715"><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub></mml:math></inline-formula> from the mean.<disp-formula id="equ110"><label>(89)</label><mml:math id="m110"><mml:mrow><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>For a multivariate standard normal distribution without correlation, the Mahalanobis distance becomes the Euclidean distance <inline-formula><mml:math id="inf716"><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:math></inline-formula>. Rewriting the multivariate normal distribution in terms of the Mahalanobis distance<disp-formula id="equ111"><label>(90)</label><mml:math id="m111"><mml:mrow><mml:mrow><mml:mi>normal</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mrow><mml:mo>det</mml:mo><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>Mah</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>reveals that all points <inline-formula><mml:math id="inf717"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> with <inline-formula><mml:math id="inf718"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> have the same probability density. The random variable <inline-formula><mml:math id="inf719"><mml:msubsup><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is χ-square distributed. Thus the χ-square distribution (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) is the probability density of drawing an <inline-formula><mml:math id="inf720"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> in <inline-formula><mml:math id="inf721"><mml:mi>n</mml:mi></mml:math></inline-formula> dimensions and finding it on the ellipsoid’s <inline-formula><mml:math id="inf722"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>-dimensional surface, at a distance <inline-formula><mml:math id="inf723"><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub></mml:math></inline-formula> from the mean. This allows us to use the cumulative <inline-formula><mml:math id="inf724"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>-square distribution function to calculate the probability mass inside the ellipsoid which is the desired HDCV. By evaluating <inline-formula><mml:math id="inf725"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>true</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi mathvariant="normal">Σ</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>true</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf726"><mml:mrow><mml:msubsup><mml:mi>χ</mml:mi><mml:mi>cdf</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>d</mml:mi><mml:mi>Mah</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> we specify how much volume, in units of probability mass, has to be counted until the volume includes the true value. Note, that with increasing dimensionality (<xref ref-type="fig" rid="fig5">Figure 5a</xref>) the probability mass is shifted away from the mode of the probability density. For general probability densities, this is also true in higher dimensions where one only rarely finds the true value within a small distance to the maximum of the probability density. Note, that there is no mathematical theorem for singular models (<xref ref-type="bibr" rid="bib104">Watanabe, 2007</xref>) saying that the posterior approaches asymptotically a multivariate normal with increasing data quality/quantity. Consequently, the underlying assumption that the posterior distribution is multivariate normal, is situation dependent valid or highly questionable. Thus the displayed method should be validated additionally in an independent way. To determine the probability mass needed to include the true value into the HDCV for such a posterior, we can also use a histogram-based method. One starts by constructing an <inline-formula><mml:math id="inf727"><mml:mi>n</mml:mi></mml:math></inline-formula>-dimensional histogram from the samples of the posterior and by initializing a global variable with zero. Then we start counting from the bin with most counts and check whether the true value falls inside this bin. If not, we add the probability mass inside that bin to the global variable. Repeating this for next lower bins eventually leads to the detection of the bin with the true rate matrix inside. On detection, the global variable contains the sought-after value <inline-formula><mml:math id="inf728"><mml:mi>P</mml:mi></mml:math></inline-formula>. While this procedure does not depend on a multivariate normal assumption, it is prone to errors due to the discrete bins and the finite samples from the posterior.</p><p>Nevertheless, both procedures show (<xref ref-type="fig" rid="fig5">Figure 5b</xref>) good agreement when plotting the volume/ probability mass needed to include the true parameter value vs. <inline-formula><mml:math id="inf729"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula>. Again <inline-formula><mml:math id="inf730"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub><mml:mo>≈</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> seems to be a reasonable data quality to trust the statistics of the posterior using the KF. In contrast, the posterior of the RE approach never includes the true values with a reasonable HDCV. Note, that a probability mass of <inline-formula><mml:math id="inf731"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> to reach the true value means that qualitatively speaking the estimate of the RE approach is infinitely far away from the true value in terms of the Mahalanobis distance. Further, due to the finite sampling the method of histogram counting is not qualified for the largest HDCVs approaching <inline-formula><mml:math id="inf732"><mml:mrow><mml:mi>P</mml:mi><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The two developed methods contrast the Euclidean distance from the true values to the maximum mode, median or even the mean of the distribution against all higher moments of the distribution. Thus it tests the overall shape of the posterior.</p></sec></app><app id="appendix-9"><title>Appendix 9</title><sec sec-type="appendix" id="s19"><title>Uncertainty quantification for the 5-state and 6-state model with cPCF data</title><p>The effect of the second observable on the single parameter level for the 5-state model can be seen in <xref ref-type="fig" rid="app9fig1">Appendix 9—figure 1</xref>. The biased estimates and the unidentified parameters of the RE approach for PC data (<xref ref-type="fig" rid="fig9">Figure 9</xref>) are eliminated by the fluorescence data.</p><fig id="app9fig1" position="float"><label>Appendix 9—figure 1.</label><caption><title>HDCIs vs <inline-formula><mml:math id="inf733"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> for the 5-state-2-open-states model for cPCF data.</title><p>The single parameter level corresponds to the Euclidean error of <xref ref-type="fig" rid="fig7">Figure 7d</xref>. Compared to the main text we switched rows with columns of the panels, the first column represents now the rates and the second column the equilibrium constants. The prior that enforces microscopic-reversibility is identical to the prior mentioned in the caption of <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app9-fig1-v3.tif"/></fig><p>Even the over-confidence problem seems to be decreased (compare with <xref ref-type="fig" rid="fig4">Figures 4b1</xref>—<xref ref-type="fig" rid="fig6">6</xref>). In contrast, the HDCIs of the 6-states-1-open-state model for cPCF data (<xref ref-type="fig" rid="app9fig2">Appendix 9—figure 2</xref>) display again a much increased over-confidence problem of the rate equation approach. This indicates that not simply counting of states but the actual topology of the kinetic scheme and the structure of the observation matrix <inline-formula><mml:math id="inf734"><mml:mi mathvariant="bold">H</mml:mi></mml:math></inline-formula> determines the scale of the over-confidence problem. Apparently, the more information comes from the fluorescence data relative to the information from the current data the less grave is the over-confidence problem and the less different are the Euclidean errors.</p><fig id="app9fig2" position="float"><label>Appendix 9—figure 2.</label><caption><title>HDCIs vs <inline-formula><mml:math id="inf735"><mml:msub><mml:mi>N</mml:mi><mml:mi>ch</mml:mi></mml:msub></mml:math></inline-formula> for the 6-states-1-open-state model for cPCF data.</title><p>The single parameter level corresponds to the Euclidean error of <xref ref-type="fig" rid="fig7">Figure 7e</xref>. Compared to the main text we switched rows with columns of the panels, the first column represents now the rates and the second column the equilibrium constants. The prior that enforces microscopic-reversibility is identical to the prior mentioned in the <xref ref-type="fig" rid="fig7">Figure 7</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-62714-app9-fig2-v3.tif"/></fig></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62714.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Goldschen-Ohm</surname><given-names>Marcel P</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The authors develop a Bayesian approach to modeling signals arising from ensembles of ion channels that can incorporate multiple simultaneously recorded signals such as fluorescence and ionic current. For simulated data from a simple ion channel model where ligand binding drives pore opening, they show that their approach enhances parameter identifiability and/or estimates of parameter uncertainty over more traditional approaches. The developed approach provides a valuable tool for modeling macroscopic time series data including data with multiple observation channels.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62714.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Goldschen-Ohm</surname><given-names>Marcel P</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Goldschen-Ohm</surname><given-names>Marcel P</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Milescu</surname><given-names>Lorin S</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04rq5mt64</institution-id><institution>University of Maryland, Baltimore</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Kinz-Thompson</surname><given-names>Colin</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05vt9qd57</institution-id><institution>Rutgers</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Bayesian selection of Hidden Markov models for multi-dimensional ion channel data&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Marcel P Goldschen-Ohm as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Richard Aldrich as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Lorin S Milescu (Reviewer #2); Colin Kinz-Thompson (Reviewer #3).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, when editors judge that a submitted work as a whole belongs in <italic>eLife</italic> but that some conclusions require additional discussion or a modest amount of additional analysis, as they do with your paper, we are asking that the manuscript be revised to address the points raised by the reviewers.</p><p>Our expectation is that the authors will eventually carry out the additional analyses and report on how they affect the relevant conclusions either in a preprint on bioRxiv or medRxiv, or if appropriate, as a Research Advance in <italic>eLife</italic>, either of which would be linked to the original paper.</p><p>Summary:</p><p>Extracting ion channel kinetic models from experimental data is an important and perennial problem. Much work has been done over the years by different groups, with theoretical frameworks and computational algorithms developed for specific combinations of data and experimental paradigms, from single channels to real-time approaches in live neurons. At one extreme of the data spectrum, single channel currents are traditionally analyzed by maximum likelihood fitting of dwell time probability distributions; at the other extreme, macroscopic currents are typically analyzed by fitting the average current and other extracted features, such as activation curves. Robust analysis packages exist (e.g., HJCFIT, QuB), and they have been put to good use in the literature.</p><p>Münch et al. focus here on several areas that need improvement: dealing with macroscopic recordings containing relatively low numbers of channels (i.e., hundreds to tens of thousands), combining multiple types of data (e.g., electrical and optical signals), incorporating prior information, and selecting models. The main idea is to approach the data with a predictor-corrector type of algorithm, implemented via a Kalman filter that approximates the discrete-state process (a meta-Markov model of the ensemble of active channels in the preparation) with a continuous-state process that can be handled efficiently within a Bayesian estimation framework, which is also used for parameter estimation and model selection.</p><p>With this approach, one doesn't fit the macroscopic current against a predicted deterministic curve, but rather infers – point by point – the ensemble state trajectory given the data and a set of parameters, themselves treated as random variables. This approach, which originated in the signal processing literature as the Forward-Backward procedure (and the related Baum-Welch algorithm), has been applied since the early 90s to single channel recordings (e.g., Chung et al., 1990), and later has been extended to macroscopic data, in a breakthrough study by Moffatt (2007). In this respect, the study by Münch et al. is not necessarily a conceptual leap forward. However, their work strengthens the existing mathematical formalism of state inference for macroscopic ion channel data, and embeds it very nicely in a rigorous Bayesian estimation framework.</p><p>The main results are very convincing: basically, model parameters can be estimated with greater precision – as much as an order of magnitude better – relative to the traditional approach where the macroscopic data are treated as noisy but deterministic (but see comments below). Estimated uncertainty can be further improved by incorporating prior information on parameters (e.g., diffusion limits), and by including other types of data, such as fluorescence.</p><p>The manuscript is well written and overall clear, and the mathematical treatment is a rigorous tour-de-force. However, the reviewers raised a number points that need further clarification, better discussion or amendment. These concerns are likely to be addressable largely by changes to the main text and software documentation along with some additional analyses. Once addressed, a revised version of this manuscript would likely be suitable for publication in <italic>eLife</italic>. That said, the study is very nice and ambitious, but clarity is a bit impaired by dealing with perhaps too many issues. The state inference and the bayesian model selection are very important but completely different issues. The authors should consider whether they may be better treated separately, or even perhaps in a more specialized journal where they can be developed in more detail.</p><p>Essential revisions:</p><p>1. Tutorial-style computational examples must be provided, along with well commented/documented code. The interested readers should be able to implement the method described here in their own code/program. The supplied code is not well documented, and it is unclear whether it is applicable beyond the specific models examined in the paper. It was supplied as.txt files, but looks like C code, and is unlikely to be easily adaptable by others to their own models.</p><p>2. The authors should clearly discuss the types of data and experimental paradigms that can be optimally handled by this approach, and they must explain when and where it fails or cannot be applied or becomes inefficient in comparison with other methods. One must be aware that ion channel data are very often subject to noise and artifacts that alter the structure of microscopic fluctuations. It seems that the state inference algorithm would work optimally with low noise, stable, patch-clamp recordings (and matching fluorescence recordings) in heterologous expression systems (e.g., HEK293 cells), where the currents are relatively small, and only the channel of interest is expressed (macropatches?). In contrast, it may not be effective with large currents that are recorded with low gain, are subject to finite series resistance, limited rise time, restricted bandwidth, colored noise, contaminated by other currents that are (partially) eliminated with the P/n protocol with the side effect of altering the noise structure, power line 50/60 Hz noise, baseline fluctuations, etc. This basically excludes some types of experimental data and experimental paradigms, such as recordings from neurons in brain slices or in vivo, oocytes, etc. Of course, artifacts can affect all estimation algorithms, but approaches based on fitting the predicted average current have the obvious benefit of averaging out some of these artifacts.</p><p>The discussion in the manuscript is insufficient in this regard and must be expanded. The method should be tested under non-ideal but commonly occurring conditions, such as limited bandwidth and in the presence of contaminating noise. For example, compare estimates obtained without filtering with estimates obtained with 2, 3 times over-filtering, with and without large measurement noise added (whole cell recordings with low-gain feedback resistors and series resistance compensation are quite noisy), with and without 50/60 Hz interference. How does the algorithm deal with limited bandwidth that distorts the noise spectrum? How are the estimated parameters affected? The reader will have to get sense of how sensitive is this method to artifacts. Also, fluorescence data in particular is usually of much lower temporal resolution than current measurements. How does this impact its benefit to parameter estimation?</p><p>3. Even more emphasis on the approximation of n(t) as being distributed according to a multivariate normal, and thus being continuous, should be placed in the main text. It seems that this may limit the applicability of the method to data with &gt; ~100s of channels; although the point is not investigated. In Figure 3, the method is only benchmarked to a lower limit of ~500 channels. As shown in Milescu et al., 2005, fitting macroscopic currents is asymptotically unbiased. In other words, the estimates are accurate, unless the number of channels is small (tens or hundreds), in which case the multinomial distribution is not very well approximated by a Gaussian. What about the predictor-corrector method? How accurate are the estimates, particularly at low channel counts (10 or 100)? Since the Kalman filter also uses a Gaussian to approximate the multinomial distribution of state fluctuations, I would also expect asymptotic accuracy. Parameter accuracy should be tested, not just precision.</p><p>4. Achieving the ability to rigorously perform model selection is very impressive aspect of this work and a large contribution to the field. However, the manuscript offers too many solutions to performing that model selection itself along with a long discussion of the field (for instance, line 376-395 could be completely cut). Since probabilistic model selection is an entire area of study by itself, the authors do not need to present underdeveloped investigations of each of them in a paper on modeling channel data (e.g., of course WAIC outperforms AIC. Why not cover BIC and WBIC?). The authors should pick one, and maybe write a second paper on the others instead of presenting non-rigorous comparisons (e.g., one kinetic scheme and set of parameters). As a side note, it is strange that the authors did not consider obtaining evidence or Bayes factors to directly perform Bayesian model selection – for instance, they could have used thermodynamic integration since they used MC to obtain posteriors anyway (c.f., Computing Bayes Factors Using Thermodynamic Integration by Lartillot and Philippe, Systematic Biology, 2006, 55(2), 195-207. DOI: 10.1080/10635150500433722)</p><p>5. Regarding model selection for the PC data in Figure 7, why does the RE model with BC appear to provide a visually better identification of the true model than the KF model with BC if the KF model does a better job at estimating the parameters and setting non true rates to zero? Doesn't this suggest that RE with cross validation is better than the proposed KF approach in regards to model selection? In terms of parameter estimates (i.e. as shown in Figure 3), how does RE + BC stack up?</p><p>6. A better comparison with alternative parameter estimation approaches is necessary. First of all, explain more clearly what is different from the predictor-corrector formalism originally proposed by Moffatt (2007). The manuscript mentions that it expands on that, but exactly how? If it is only an incremental improvement, of interest to a very limited audience, a specialized, technical journal, is more appropriate.</p><p>Second, the method proposed by Celentano and Hawkes, 2004, is not a predictor-corrector type but it utilizes the full covariance matrix between data values at different time points. It seems that the covariance matrix approach uses all the information contained in the macroscopic data and should be on par with the state inference approach. However, this method is only briefly mentioned here and then it's quickly dismissed as &quot;impractical&quot;. We all agree that it's a slower computation than, say, fitting exponentials, but so is the Kalman filter. Where do we draw the line of impracticability? Computational speed should be balanced with computational simplicity, estimation accuracy, and parameter and model identifiability. Moreover, that method was published in 2004, and the computational costs reported there should be projected to present day computational power. We are not saying that the authors should code the C&amp;H procedure and run it here, but should at least give it credit and discuss its potential against the KF method.</p><p>The only comparison provided in the manuscript is with the &quot;rate equation&quot; approach, by which the authors understand the family of methods that fit the data against a predicted average trajectory. In principle, this comparison is sufficient, but there are some issues with the way it's done.</p><p>Table 3 compares different features of their state inference algorithm and the &quot;rate equation fitting&quot;, referencing Milescu et al., 2005. However, there seems to be a misunderstanding: the algorithm presented in that paper does in fact predict and use not only the average but also – optionally – the variance of the current, as contributed by stochastic state fluctuations and measurement noise. These quantities are predicted at any point in time as a function of the initial state, which is calculated from the experimental conditions. In contrast, the KF calculates the average and variance at one point in time as a projection of the average and variance at the previous point. However, both methods (can) compare the data value against a predicted probability distribution. The Kalman filter can produce more precise estimates but presumably with the cost of more complex and slower computation, and increased sensitivity to data artifacts.</p><p>Figure 3 is very informative in this sense, showing that estimates obtained with the state inference (KF) algorithm are about 10 times more precise that those obtained with the &quot;rate equation&quot; approach. However, for this test, the &quot;rate equation&quot; method was allowed to use only the average, not the variance.</p><p>Considering this, the comparison made in Figure 3 should be redone against a &quot;rate equation&quot; method that utilizes not only the expected average but also the expected variance to fit the data, as in Milescu et al., 2005. Calculating this variance is trivial and the authors should be able to implement it easily (reviewers are happy to provide feedback). The comparison should include calculation times, as well as convergence.</p><p>7. The manuscript nicely points out that a &quot;rate equation&quot; approach would need 10 times more channels (N) to attain the same parameter precision as with the Kalman filter, when the number of channels is in the approximate range of 10^2 … 10^4. With larger N, the two methods become comparable in this respect.</p><p>This is very important, because it means that estimate precision increases with N, regardless of the method, which also means that one should try to optimize the experimental approach to maximize the number of channels in the preparation. However, it should be pointed out that one could simply repeat the recording protocol 10 times (in the same cell or across cells) to accumulate 10 times more channels, and then use a &quot;rate equation&quot; algorithm to obtain estimates that are just as good. Presumably, the &quot;rate equation&quot; calculation is significantly faster than the Kalman filter (particularly when one fits &quot;features&quot;, such as activation curves), and repeating a recording may only add seconds or minutes of experiment time, compared to a comprehensive data analysis that likely involves hours and perhaps days. Although obvious, this point can be easily missed by the casual reader and so it would be useful to be mentioned in the manuscript.</p><p>8. A misunderstanding is that a current normalization is mandatory with &quot;rate equation&quot; algorithms. This is really not the case, as shown in Milescu et al., 2005, where it is demonstrated clearly that one can explicitly use channel count and unitary current to predict the observed macroscopic data. Consequently, these quantities can also be estimated, but state variance must be included in the calculation. Without variance, one can only estimate the product i x N, where i is unitary current and N is channel count. This should be clarified in the manuscript: any method that uses variance can be used to estimate i and N, not just the Kalman filter. In fact, the non-stationary noise analysis does exactly that: a model-blind estimation of N and i from non-equilibrium data. Also, one should be realistic here: in some circumstances it is far more efficient to fit data &quot;features&quot;, such as the activation curve, in which case the current needs to be normalized.</p><p>9. It's great that the authors develop a rigorous Bayesian formalism here, but it would be a good idea to explain – even briefly – how to implement a (presumably simpler) maximum likelihood version that uses the Kalman filter. This should satisfy those readers who are less interested in the Bayesian approach and will also be suitable for situations when no prior information is available.</p><p>10. The Bayesian formalism is not the only way of incorporating prior knowledge into an estimation algorithm. In fact, the reader may have more practical and pressing problems than guessing what the parameter prior distribution should be, whether uniform or Gaussian or other. More likely one would want to enforce a certain KD, microscopic (i)reversibility, an (in)equality relationship between parameters, a minimum or maximum rate constant value, or complex model properties and behaviors, such as maximum Popen or half-activation voltage. A comprehensive framework for handling these situations via parameter constraints (linear or non-linear) and cost function penalty has been recently published (Salari et al/Navarro et al., 2018). Obviously, the Bayesian approach has merit, but the authors should discuss how it can better handle the types of practical problems presented in those papers, if it is to be considered an advance in the field, or at least a usable alternative.</p><p>11. The methods section should include information concerning the parameter initialization choices, HMC parameters (e.g. number of steps) and any burn-in period used in the analyses used in Figures 3-6. For example, how is convergence established? How many iterations does it take to reach convergence? How long does it take to run? How does it scale with the data length, channel count, and model state count? How long does it take to optimize a large model (e.g., 10 or 20 states)? Provide some comparison with the &quot;rate equation method&quot;.</p><p>12. In the section on priors, the entire part concerning the use of a β distribution should be removed or replaced, because it is a probabilistic misrepresentation of the actual prior information that the authors claim to have in the manuscript text. The max-entropy prior derived for the situation described in the text (i.e., an unknown magnitude where you don't know any moments but do have upper and lower bounds; the latter could be from the length from the experiment) is actually P(x) = (ln(x_{max}) – ln(x_{min}))^{-1} * x^{-1}. Reviewers are happy to discuss more with the authors.</p><p>13. Here and there, the manuscript somehow gives the impression that existing algorithms that extract kinetic parameters by fitting the average macroscopic current (&quot;fitting rate equations&quot;) are less &quot;correct&quot;, or ignorant of the true mathematical description of the data. This is not the case. Published algorithms often clearly state what data they apply to, what their limitations are, and what approximations were made, and thus they are correct within that defined context and are meant to be more effective than alternatives. Some quick editing throughout the manuscript should eliminate this impression.</p><p>14. The manuscript refers to the method where the data are fitted against a predicted current as &quot;rate equations&quot;. However, it is not completely clear what that means. The rate equation is something intrinsic to the model, not a feature of any algorithm. An alternative terminology must be found. Perhaps different algorithms could be classified based on what statistical properties are used and how. E.g., average (+variance) predicted from the starting probabilities (Milescu et al., 2005), full covariance (Celentano and Hawkes, 2004), point-by-point predictor-corrector (Moffatt, 2007).</p><p>15. The manuscript needs line editing and proofreading (e.g., on line 494, &quot;Roa&quot; should be &quot;Rao&quot;; missing an equals sign in equation 13). Additionally, in many paragraphs, several of the sentences are tangential and distract from communicating the message of the paper (e.g., line 55). Removing them will help to streamline the text, which is quite long.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Bayesian inference of kinetic schemes for ion channels by Kalman filtering&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Marcel P Goldschen-Ohm as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Richard Aldrich as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Colin Kinz-Thompson (Reviewer #3); Lorin Milescu (Reviewer #4).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>The authors develop a Bayesian approach to modeling macroscopic signals arising from ensembles of individual units described by a Markov process, such as a collection of ion channels. Their approach utilizes a Kalman filter to account for temporal correlations in the bulk signal. For simulated data from a simple ion channel model where ligand binding drives pore opening, they show that their approach enhances parameter identifiability and/or estimates of parameter uncertainty over more traditional approaches. Furthermore, simultaneous measurement of both binding and pore gating signals further increases parameter identifiability. The developed approach provides a valuable tool for modeling macroscopic time series data with multiple observation channels.</p><p>The authors have spent considerable effort to address the previous reviewer comments, and I applaud them for the breadth and depth of their current analysis.</p><p>1. The figure caption titles tend to say what analysis or comparison is being presented, but not what the take home message of the figure is. I suggest changing them to emphasize the latter. This will especially help non-experts to understand what the figures are trying to convey to them.</p><p>2. I very much appreciate the GitHub code and examples for running your software. However, I feel that a hand-holding step-by-step explicit example of running a new model on new data is likely necessary for many to be able to utilize your software. Much more hand-holding than the current instructions on the GitHub repo.</p><p>3. Figure captions sometimes do not explain enough of what is shown. I appreciate that many of these details are in the main text, but data that is displayed in the figure and not concretely described in the caption can makes the figures hard to follow. e.g. Figure 4a – &quot;With lighter green we indicate the Euclidean error of patch-clamp data set.&quot; But what data set do the other colors reflect? It is not stated in the caption. Again, I realize this is described in the main text, but it also needs to be defined in the caption where the data is presented. Figure 4d – Please spell out what &quot;both algorithms&quot; intends. Also, a suggestion: instead of having to refer to the caption for the color codes (i.e. RE vs. KF, etc.) it would speed figure interpretation to have a legend in the figures themselves. Few other examples: Box 1. Figure 2. – Please define the solid vs. dashed lines in the caption. Figure 3c – Please define the solid vs. dashed lines in the caption. Figure 12 – &quot;We simulated 5 different 100kHz signals.&quot; What kind of signals? Fluorescence I assume, but this needs to be explicitly defined. I'd check all figures for similar.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>In this revised manuscript, the Münch et al. have addressed all of my original concerns. It is significantly revised, though, and includes many new investigations of the algorithm's performance. Overall, the narrative of this manuscript is now to introduce an approximation to the solution for a Bayesian Kalman Filter, and then spend time demonstrating that this approximation is reasonable and even better than previous methods. In my opinion, they successfully do this, although, as they mention in their comments, their manuscript is very long.</p><p>I am not 100% certain, but the approximation that the author's make seems to be equivalent (or at least similar to) an approximation of the chemical master equation using just the 1st and 2nd moments, which is just the Fokker-Planck equation. The authors should discuss any connection to this approximation, as there is a great deal of literature on this topic (e.g., see van Kampen's book).</p><p>In Figures 3A and 4D, it is unclear to me what is plotted for the RE and classical Kalman filter (i.e., how is there a posterior if they are not Bayesian methods)? Perhaps it is buried in the methods or appendices, but, if so, it needs to at least be clarified in the figure captions.</p><p>The Bayesian statistical tests devised to determine success (e.g., on pgs. 12-13) seem a little ad hoc, but are technically acceptable. I do not see a need for additional metrics.</p><p>Line 939: Equation 61 is absolutely not &quot;close to uniform distribution&quot;. The α and β parameters of 100.01 are much larger than 1. It is incredibly peaked around 0.5. Perhaps this is a typo?</p><p>Line 942: The allowed small breaking of microscopic reversibility in the prior is an interesting idea that I wish the authors would expound upon more.</p><p>Line 712: The authors state that the simulation 'code will be shared up-on request'. They should include it with their github pages tutorials for running the examples in case others wish to check their work and/or use it. There is no reason to withhold it.</p><p>Line 707: 'Perspectively' is not a commonly used word.</p><p><italic>Reviewer #4 (Recommendations for the authors):</italic></p><p>The authors have addressed all my comments and suggestions. The manuscript is nice and extremely comprehensive, and should advance the field.</p><p>Nevertheless, the manuscript is also very long (but justifiably so), and certain statements could be a little clearer. Most of these statements refer to the comparison with the so-called rate equation (RE) methods, with which I'm more familiar. For example:</p><p>Abstract: &quot;Furthermore, the Bayesian filter delivers unbiased estimates for a wider range of data quality and identifies parameters which the rate equation approach does not identify.&quot;</p><p>The first part of this statement is not quite true, as Figure 4 shows clearly that the Bayesian estimates are biased (and the authors acknowledge this elsewhere in the manuscript). If they are biased in one &quot;range of data quality&quot;, that probably means they are always biased, just to different degrees. This is not surprising, because the Kalman filter is a continuous state approximation to a discrete state process, and the overall estimation algorithm makes a number of approximations to intractable probability distributions. It would definitely be correct to say that the estimates are very good, but not unbiased.</p><p>Second, this statement is also ambiguous. Are you referring to the theoretical non-identifiability caused by having more parameters in the model than what the combination of estimator+experimental protocol can extract from data? In this case, it's not a matter of certain parameters that cannot be identified, but a matter of how many can be identified uniquely. The more information is extracted from the data, the more unique parameters can be identified, so the Kalman filter should do better. Or, alternatively, are you referring to specific parameters that are poorly identified because, for example, they refer to transitions that are rarely undertaken by the channel? In this case, it would be a matter of obtaining looser or tighter estimates with one method or another, but the parameters should be intrinsically identifiable, I imagine, regardless of the algorithm. In any case, it's not clear that the better identifiability is the result of the Bayesian side, or of the predictor-corrector state inference filter. I would guess it is the Kalman filter, but I'm not sure.</p><p>Perhaps it would be clearer if you said that the KF method produces good estimates and generally improves parameter identifiability compared to other methods, as it extracts more information from the data?</p><p>Introduction:</p><p>32: I'm not sure, but if the intention here is to cite mathematical treatments for estimation, you may add references to the &quot;macroscopic&quot; papers by Celentano, Milescu, Stepaniuk and perhaps a few others that use &quot;rate equations&quot;. Also, you may cite Qin et al., 1996, as a single channel paper describing a method used in hundreds of studies.</p><p>Pg. 3:</p><p>51: I remain skeptical that it is a good idea to use &quot;rate equations&quot; (RE) as a term to refer to those methods that are fundamentally different from the approach described here (also see my comment to the first submission). The rate equations must always be used to predict a future state from a past or current state, by all methods, explicitly or implicitly, because REs simply describe the channel dynamics. In this very manuscript, Equation 3, central to the Kalman filter formalism, is nothing but a deterministic rate equation with a stochastic term added to approximate the stochastic evolution of an ensemble of channels. In fact, there are some old papers by Fox and some more recent by Osorio (I'm not exactly sure of the name and I don't remember the years) that discuss that approximation and its shortcomings – perhaps that is the source of bias?</p><p>Whether that prediction is then corrected, as in the Kalman filter approach, with a corrector step is irrelevant, as far as the rate equations. Furthermore, it's all a matter of degree. For example, the Milescu et al. approach, which is classified here under the RE umbrella, predicts future states as a mean + variance (or as a mean only), but only using the initial state. There is no correction at each point, as with the Kalman filter method, but there is a correction of the initial state from one iteration to the next (by the way, it's not clear if you implemented that feature here, which would make a big difference). Then, because it considers the stochastical aspect of the data, the Milescu et al. approach should also be considered a stochastic method, just one that doesn't use all the information contained in the data (and so it is fast). Imagine a situation where you ran the same stimulation protocol multiple times, but each time you record only one data point, further and further away from the beginning of the stimulation protocol. A &quot;stochastic&quot; algorithm applied to this type of data would be exactly as described in Milescu et al. Of course, in reality all points are recorded in sequence, but that doesn't mean that the approach is not stochastic, just that it 's simplifying and discarding some information to gain speed. All methods (such as the Kalman filter described here) make some compromises to reduce complexity and increase speed.</p><p>The bottom line is that there is the most basic approach of solving the rate equations deterministically, without considering any variance whatsoever, and then there is everything else.</p><p>58: &quot;Thus, a time trace with a finite number of channels contains, strictly speaking, only one independent data point.&quot;</p><p>I don't understand this sentence. Could you please clarify?</p><p>74: &quot;The KF is the minimal variance filter Anderson and Moore (2012). Thus, instead of excessive analog filtering of currents with the inevitable signal distortions Silberberg and Magleby (1993) one can apply the KF with higher analysing frequency on less filtered data.&quot;</p><p>Yes, but I'm not sure why should we use excessive filtering? Where is this idea coming from?</p><p>131: &quot;However, even with simulated data of unrealistic high numbers of channels per patch, the KF outperforms the deterministic approach in estimating the model parameters.&quot;</p><p>It is clearly true from your figures, but please give a number as to what is unrealistic (10,000? 100,000?). Also, outperforms, but by how much? As I commented above and below, all methods tested here seem to produce good estimates under the conditions they were designed for (and even outside), and one might argue that it's not worth adding the additional computational complexity and running time for a possibly small increase in accuracy. How is one to judge that increase in accuracy?</p><p>276: Has the RE method been used with the mean + variance or mean only? Also, how was the initial probability vector calculated with the RE method? If you used the mean + variance, then (as mentioned above), you can't call it deterministic. If you used only the mean, then it is indeed a deterministic method, but it's not the approach described in Milescu et al. Please explain.</p><p>229: &quot;added, in Equation 9 to the variance … which originates (Equation 38d) from the fact that we do not know the true system state&quot;</p><p>Which noise are you referring to? The state noise or the measurement noise?</p><p>326: Which are the two algorithms?</p><p>278: &quot;Both former algorithms are far away from the true parameter values with their maxima and also fail to cover the true values within reasonable tails of their posterior.&quot;</p><p>I think the authors might have gotten a little carried away when they made this statement. There is no doubt that the Kalman/Bayesian method produces more accurate estimates, but the other two methods (KF and RE) are very reasonable as well. I see estimates that are within 5, 10, or 20% from the true values, for most parameters. This is not &quot;failure&quot; by any stretch of the imagination. Most people would call this quite good, given the nature of the data.</p><p>294: &quot;The small advantage of our algorithm for small … over Moffatt (2007) is due to the fact that we could apply an informative prior in the formulation of the inference problem … by taking advantage of our Bayesian filter. &quot;</p><p>This is an interesting and important statement. I interpret it as saying that the Bayesian aspect itself makes only a small contribution to the quality of the estimates, when comparing it with the Moffatt method, which is a Kalman filter as well. The only issue with the Moffatt method is the lack of an explicit formalism for the excess state noise (which could presumably be added). It also suggests to me that any method that tries to use the noise may run into difficulties when the noise model is unknown (typical real-life scenario). The Moffatt method is confronted with unknown noise and it fails. What about when the Bayesian method is confronted with unknown noise? There are some comments in the manuscript, but nothing tangible. Could you please comment?</p><p>Figure 3: There are no data sets in the figure. What data were analyzed here?</p><p>Is there a typo regarding the colors in a? What are the red, blue, black, green symbols? Please verify.</p><p>Assuming the red is KF, there is something very curious about its estimates, which are very different in distribution from the Bayesian estimates. Could there be a coding problem? If red is actually RE, then it would make more sense. Could you explain? Is this the effect of the &quot;excessive&quot; open state noise? If so, I find this situation a bit unfair, because then the 2007 KF algorithm is tested with data that it is not meant to work with.</p><p>Also, I think it would be more interesting to have a comparison between the original KF and the newer Bayesian approach, so we can understand what Bayesian estimation does. In any case, I can't really interpret the data until you clarify the colors.</p><p>The legend is unclear overall.</p><p>303: What means &quot;singular&quot;?</p><p>324: What shaded area in Figure 1a? Perhaps you mean Figure 4a?</p><p>Figure 4: I don't see any light green.</p><p>368: &quot;The estimated true success rate by the RE approach (blue) is ≈ 0.15 and therefore far away from the true success probability. In contrast, the posterior (green) of the true success probability of the KF resides with a large probability mass between the lower and upper limit of the success probability of an optimal algorithm (given the correct kinetic scheme).&quot;</p><p>I'm really a bit puzzled here: yes, the KF is more accurate (given the correct kinetic scheme), but what I see in this figure is that both algorithms are biased, yet both are generally within 10% (and quite a bit better for KF) of the true values. If one would plot the log of the rates, to transform into δ Gs, the differences would be even smaller. I would bet that experimental artifacts would contort the estimates to a greater degree anyway.</p><p>It's very nice that the RE approach was embedded and tested within a Bayesian framework, but it would still be interesting to know what is the contribution of the Bayesian aspect (unless I missed this point in the manuscript).</p><p>436: &quot;…while their error ratio (Figure 7a) seems to have no trend to diminish with increasing NCh&quot;.</p><p>This would be unexpected, because the Kalman filter will always use more information than RE. Why would the KF approach become relatively more erroneous at higher Nc? I don't see any reason. To me, an important question is when do the overall errors become dominated by external factors, such as recording artifacts, using the wrong model, etc.</p><p>511: &quot;Thus, transferring the unusually strictly applied algebraic constraint Salari et al. (2018) of microscopic-reversibility to constraint with scalable softness we can model the lack information if microscopic-reversibility is exactly fulfilled Colquhoun et al. (2004) by the given ion channel instead of forcing it upon the model.&quot;</p><p>I'm not sure I understand what you mean here. I think it is very usual to constrain microscopic reversibility EXACTLY, whether through the SVD method (Qin et al., Plested et al., Milescu et al., Salari et al), or through some other algebraic method (Plested et al). It's not &quot;forcing&quot; it on the channel, but simply testing the data for that condition. Of course, one could test the condition where there is no reversibility enforced at all (i.e., it's &quot;given by the channel&quot;), or anything in between.</p><p>675: &quot;With our algorithm we demonstrate (Figure 3c and 7) that the common assumption that for large ensembles of ion channels simpler deterministic modeling by RE approaches is on par with stochastic modeling, such as a KF, is wrong in terms of Euclidean error and uncertainty quantification (Figure 5a-c and Figure 6a-b)&quot;.</p><p>I find this statement a little subjective. I think anyone who cares about stochastic vs. deterministic modeling knows enough that any method that uses more information from data should produce better estimates, regardless of the number of channels. I would say that the more likely assumption is that deterministic estimators produce poor estimates with small numbers of channels, perfect estimates with infinitely many channels, and anything in between. In fact, the KF method behaves exactly the same way, just with overall higher accuracy. Looking at the tests in this manuscript, I would say that all the previous studies that modeled macroscopic data using deterministic methods are safe. They don't need to be redone. The future, of course, is a different matter.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62714.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1. Tutorial-style computational examples must be provided, along with well commented/documented code. The interested readers should be able to implement the method described here in their own code/program. The supplied code is not well documented, and it is unclear whether it is applicable beyond the specific models examined in the paper. It was supplied as.txt files, but looks like C code, and is unlikely to be easily adaptable by others to their own models.</p></disp-quote><p>An extended tutorial-style git-hub page for PC data can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/JanMuench/Tutorial_Patch-clamp_data">https://github.com/JanMuench/Tutorial_Patch-clamp_data</ext-link> and for confocal patch-clamp fluorometry data, <ext-link ext-link-type="uri" xlink:href="https://github.com/JanMuench/Tutorial_Bayesian_Filter_cPCF_data">https://github.com/JanMuench/Tutorial_Bayesian_Filter_cPCF_data</ext-link>.</p><disp-quote content-type="editor-comment"><p>2. The authors should clearly discuss the types of data and experimental paradigms that can be optimally handled by this approach, and they must explain when and where it fails or cannot be applied or becomes inefficient in comparison with other methods. One must be aware that ion channel data are very often subject to noise and artifacts that alter the structure of microscopic fluctuations. It seems that the state inference algorithm would work optimally with low noise, stable, patch-clamp recordings (and matching fluorescence recordings) in heterologous expression systems (e.g., HEK293 cells), where the currents are relatively small, and only the channel of interest is expressed (macropatches?). In contrast, it may not be effective with large currents that are recorded with low gain, are subject to finite series resistance, limited rise time, restricted bandwidth, colored noise, contaminated by other currents that are (partially) eliminated with the P/n protocol with the side effect of altering the noise structure, power line 50/60 Hz noise, baseline fluctuations, etc. This basically excludes some types of experimental data and experimental paradigms, such as recordings from neurons in brain slices or in vivo, oocytes, etc. Of course, artifacts can affect all estimation algorithms, but approaches based on fitting the predicted average current have the obvious benefit of averaging out some of these artifacts.</p><p>The discussion in the manuscript is insufficient in this regard and must be expanded. The method should be tested under non-ideal but commonly occurring conditions, such as limited bandwidth and in the presence of contaminating noise. For example, compare estimates obtained without filtering with estimates obtained with 2, 3 times over-filtering, with and without large measurement noise added (whole cell recordings with low-gain feedback resistors and series resistance compensation are quite noisy), with and without 50/60 Hz interference. How does the algorithm deal with limited bandwidth that distorts the noise spectrum? How are the estimated parameters affected? The reader will have to get sense of how sensitive is this method to artifacts. Also, fluorescence data in particular is usually of much lower temporal resolution than current measurements. How does this impact its benefit to parameter estimation?</p></disp-quote><p>We developed the algorithm for analysing patch-clamp or patch-clamp fluorometry data though we are convinced that other experimental settings are applicable, as long as normal distributions can be reasonably applied and the white noise assumption for the measurement process holds. We show that it is advantageous to use the data in the ”rawest” form possible, e.g. without additional filtering (Figure 11 and App. 7 for patch-clamp data). Instead of smoothing, averaging, or subtraction, optimal statistical description of noise sources is needed in our approach. For example, in uncompensated whole-cell measurements the RC-circuit of the membrane is a low-pass filter with known characteristics. It should be noted that any active compensation of the series resistance certainly affects the noise characteristics in a hardly predictable way. It should therefore be used with particular care. In contrast, traces corrected by P/n protocols (given sufficiently low open-probability of the inactive channel) should be treatable with our approach when including the additional variance of the noise introduced by the P/n trace adequately. In fact, the assumed fluorescence signal in a confocal patch-clamp fluorometry experiment is a difference signal just as the current signal obtained with the P/n technique. Here, the amount of mean signal of swimming bulk ligands is subtracted from the original signal. Thus, in the current signal in Equation 4 we simply need to add a second noise term whose variance needs to be characterized. Non-stochastic powerline or slow baseline fluctuations are not described by our approach and should be minimized by experimental means. Although, adding a drift term in Equation 4, governed by a Gaussian process with an appropriate kernel, is worth to try. In the similar way, with a periodic kernel powerline errors could be tackled. In the “Kalman filter derived from a Bayesian filter” section (line 244-255) we specify those considerations.</p><p>Moreover, we selected now two non-ideal aspects induced by limitations of real experiments to investigate the robustness of the idealizations of our algorithm versus the approach by Milescu et al., 2005. First, we analyzed the bias induced by the analog filtering of confocal patch-clamp fluorometry data before the analog-to-digital conversion (Figure 11). For PC only data see App. 7. Second, we investigated finite integration times (Figure 12) of fluorescence data (as explicitly asked by Reviewer 1). We chose to combine those two aspects because of their similar physical origin. Both of them induce an intrinsic time scale of the recording system which is not represented in the algorithms.</p><disp-quote content-type="editor-comment"><p>3. Even more emphasis on the approximation of n(t) as being distributed according to a multivariate normal, and thus being continuous, should be placed in the main text. It seems that this may limit the applicability of the method to data with &gt; ~100s of channels; although the point is not investigated. In Figure 3, the method is only benchmarked to a lower limit of ~500 channels.</p></disp-quote><p>We expanded for confocal patch-clamp fluorometry data (Figure 4a, 7d,e) the benchmark to channel numbers of 5 · 10<sup>1</sup> to 10<sup>6</sup> per patch. At the lower limit below <italic>N</italic><sub>ch</sub> <italic>&lt;</italic> 200 we see that highest density volumina (HDCV) do not report the correct success probability of the true parameters to be inside this volume. Below this limit both algorithms have very similar Euclidean errors and the deviations of the highest density intervals for both algorithms for each <italic>N</italic><sub>ch</sub> become similar. Therefore, we suspect that the error due to the approximations of the multinomial distribution becomes more relevant than the autocorrelation of the intrinsic noise. In principle, the almost equal errors could also result from a bias induced by the uniform prior over the rate matrix. Therefore, we tested the Jeffreys Prior (loguniform on the dwell times) and dirichilet distribution on the probabilities which transition is chosen and did not detect differences for confocal patch-clamp fluorometry data. For PC data (Figure 3c), we see that below <italic>N</italic><sub>ch</sub> <italic>&lt;</italic> 2000 the posterior becomes improper for some parameters (unidentified parameters) such that here a comparison is problematic to interpret. Below <italic>N</italic><sub>ch</sub> <italic>&lt;</italic> 2000 the mentioned Jeffreys prior has an enormous effect but to avoid prolongation of the text we save this aspect for a separate paper about priors in kinetic scheme estimation. We added in the main text a reference to Moffat, 2007 (line 201-207). He applied his algorithm even for ion channel numbers as low as <italic>N</italic><sub>ch</sub> = 1 and found in Figure 4 an inverse relationship <inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∝</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> for the difference between the true log likelihood value and the loglikelihood of his Kalman filter approximation which establishes around <italic>N</italic><sub>ch</sub> = 20. Of course, the differences also depend on which data points of the time traces are used for the inference because the quality of the used approximations also depend on where on the probability simplex it is applied. Thus, in this regard our results should be seen as a rule of thumb for the lower limit. Nevertheless, we expect a similar behaviour.</p><disp-quote content-type="editor-comment"><p>As shown in Milescu et al., 2005, fitting macroscopic currents is asymptotically unbiased. In other words, the estimates are accurate, unless the number of channels is small (tens or hundreds), in which case the multinomial distribution is not very well approximated by a Gaussian. What about the predictor-corrector method? How accurate are the estimates, particularly at low channel counts (10 or 100)?</p></disp-quote><p>We expanded the benchmarking for confocal patch-clamp fluorometry data (Figure 4) to low channel counts of 50. We observe that both algorithms have similar Euclidean errors if the channel numbers for confocal patch-clamp fluorometry data for the simple model drops below <italic>N</italic><sub>ch</sub> <italic>&lt;</italic> 200. For those small ion channel ensembles, the uncertainty quantification by Bayesian credibility intervals/volume (Figure 4. b1-6) becomes unreliable even for the Kalman filter. Note, we showed that uncertainty quantification by Bayesian credibility volume in combination with the rate equation approach is generally unreliable because the posterior is to narrow (over-confident) leading to bad coverage properties of its Highest Density Credibility Volume/Interval (Figure 5 and 6). Thus, in that regime both algorithms fail in a similar way.</p><p>On the one hand, these problems can originate from the used approximations of both algorithms to derive the likelihood. One the other hand, as information from the data gets weaker in the lower regime it could also be a bias from the uniform prior of the rate matrix. This is because a uniform prior is not an unbiased minimum information prior for rates, as indicated by Reviewer 3. Tests with a Jeffreys prior for confocal patch-clamp fluorometry data with the 4-state model did not reveal a significant difference such that we consider in the article only the first option and for clarity took out prior discussions.</p><p>We also like to note that the difference between the exact likelihood and a Bayesian/Kalman Filter approximation is investigated for patch-clamp data of a 3-state model in (Moffatt, 2007) (Figure 2, 3 and 4). He shows that the difference of the log likelihood of data between an algorithm with the exact likelihood and the Kalman filter implementation scales with ∝ 1<italic>/N</italic><sub>ch</sub> if <italic>N</italic><sub>ch</sub> <italic>&gt;</italic> 20.</p><disp-quote content-type="editor-comment"><p>Since the Kalman filter also uses a Gaussian to approximate the multinomial distribution of state fluctuations, I would also expect asymptotic accuracy.</p></disp-quote><p>For a 5-state-2-open-states model for PC data (Figure 9), we demonstrate that the advantage of the KF is a bias reduction compared to the RE approach. Thus, a high accuracy is reached by the KF already at <italic>N</italic><sub>ch</sub> = 10<sup>3</sup>. In contrast, we detect bias and inaccurate inferences of the RE approach even for ensemble sizes as large as <italic>N</italic><sub>ch</sub> = 5·10<sup>5</sup>. Due to the small number of test data sets a small undetected amount of inaccuracy in parameter inferences of the KF is possible. Additionally, for some data sets of high quality <italic>N</italic><sub>ch</sub> = 7<italic>.</italic>5·10<sup>4</sup> the RE algorithm fails to identify parameters, whereas the KF algorithm has no identification problems.</p><p>In contrast, with confocal patch-clamp fluorometry data for all tested models bias and inaccuracy is a smaller but still present issue (Figure 11 and 12). The over-confidence problem of the RE algorithm remains for all tested models.</p><disp-quote content-type="editor-comment"><p>Parameter accuracy should be tested, not just precision.</p></disp-quote><p>consistent estimators (given an identifiable model) as their Euclidean error (distance from the true value) vanishes in distributions with increasing data quality. If any estimator is consistent it has to be asymptotically accurate and precise.</p><p>Accurate means</p><p>accurate = <italic>E</italic>[<italic>θ<sub>i</sub></italic> − <italic>θ<sub>i,</sub></italic><sub>true</sub>] = 0 (1)</p><p>with <italic>i</italic> indicating the <italic>i</italic>-th parameter and <italic>E</italic> means the arithmetic mean of many inferences with different data sets. The common mathematical definition of precision is<disp-formula id="sa2equ1"><label>(2)</label><mml:math id="sa2m2"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>inference</mml:mtext></mml:mrow></mml:msub><mml:msqrt><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>inference</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>θ</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>For a data set of finite quality or amount, the estimators could still be inaccurate (biased) and/or imprecise but it could also be that inferences are perfectly accurate (unbiased) and only imprecise. To see that the last aspect is true, take for example a perfectly accurate inference: Thus, the mean equals exactly the true value <italic>E</italic>[<italic>θ</italic>] − <italic>θ</italic><sub>true</sub> = 0 and assume its imprecision is Σ = 1. In this case the Euclidean error <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msqrt><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>prameters</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula> , is a random variable that is <italic>χ</italic>-distributed. Thus, the mean Euclidean distance equates to <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mtext>error</mml:mtext><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>parameter</mml:mtext></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>parameter</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>≠</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, with Γ() being the γ function. In conclusion, the Euclidean error is not a direct estimator of precision but has good summarizing properties.</p><p>We display now in Figure 4b1-6, 8 and 9 and App. 9 accuracy and precision implicitly by the narrowest highest density interval. By implicitly, we mean that both aspects are demonstrated by the local trend (accuracy) and the spread (precision) around the trend vs. <italic>N</italic><sub>ch</sub>. In this representation, perfect accuracy means <italic>E</italic>[<italic>θ</italic><sup>˜</sup><italic><sub>i</sub></italic>] = 1 as the parameters are normalized to their true value. The advantage of the single parameter level is that systematically inaccurate (biased) estimates <italic>E</italic>[<italic>θ</italic><sup>˜</sup><italic><sub>i</sub></italic>] 6 = 1 for a finite data quality can be detected Figure 9 for which the more summarizing Euclidean error is blind.</p><p>Conceptually, we prefer the Bayesian highest density credibility interval. In the new version of the article we argue that Euclidean error and Bayesian highest density volumes together are a very effective way to probe the performance of an algorithm (Figure 5. and Figure 6). Highest density credibility volumes (HDCV) allow uncertainty quantification exactly in the way the researcher desires in most cases. HDCVs allow to determine the probability mass of the true parameters to be in a certain volume for a given data set. This guided us to the binomial testing of the overall shape of the posterior. This interpretation is not possible for the usual confidence intervals/volumes of maximum likelihood, Janes, et al., 1976. In the benchmark situation of this article, we used the HDCV to show that the RE approach is generally too confident in the Bayesian parameter uncertainty quantification, which also leads to too confident (narrow) confidence volumes or errorbars in a maximum likelihood context. Note, that this shortcoming had remained undetected if we would have explored only accuracy and precision. Later in the article we worked explicitly with the classical definition of accuracy and precision because we want to understand the mean bias (for all possible random data sets) induced by analog Bessel-filtering (Figure 11 and App. 7 Figure 1) the data before analysing them. The same applies to the mean bias due to limited time resolution of fluorescence data (Figure 12).</p><disp-quote content-type="editor-comment"><p>4. Achieving the ability to rigorously perform model selection is very impressive aspect of this work and a large contribution to the field. However, the manuscript offers too many solutions to performing that model selection itself along with a long discussion of the field (for instance, line 376-395 could be completely cut). Since probabilistic model selection is an entire area of study by itself, the authors do not need to present underdeveloped investigations of each of them in a paper on modeling channel data (e.g., of course WAIC outperforms AIC. Why not cover BIC and WBIC?). The authors should pick one, and maybe write a second paper on the others instead of presenting non-rigorous comparisons (e.g., one kinetic scheme and set of parameters). As a side note, it is strange that the authors did not consider obtaining evidence or Bayes factors to directly perform Bayesian model selection – for instance, they could have used thermodynamic integration since they used MC to obtain posteriors anyway (c.f., Computing Bayes Factors Using Thermodynamic Integration by Lartillot and Philippe, Systematic Biology, 2006, 55(2), 195-207. DOI: 10.1080/10635150500433722)</p></disp-quote><p>We took out the whole model selection part and saved it for a successor publication.</p><disp-quote content-type="editor-comment"><p>5. Regarding model selection for the PC data in Figure 7, why does the RE model with BC appear to provide a visually better identification of the true model than the KF model with BC if the KF model does a better job at estimating the parameters and setting non true rates to zero? Doesn't this suggest that RE with cross validation is better than the proposed KF approach in regards to model selection? In terms of parameter estimates (i.e. as shown in Figure 3), how does RE + BC stack up?</p></disp-quote><p>We are pleased to clarify this point though the model selection will be placed in a second article. In the previous version of the article, we presented in Figure 6 and Figure 7 two related aspects.</p><p>First, we define overfitting in the usual way as a performance difference of a trained model when it is used to predict the training data and prediction data. A trained model, no matter if under complex or overly complex performs on average better to predict the already used trainings data than unseen new data. When we choose a too complex kinetic scheme in which the true process is nested we observe, that the RE approach, when applied to a more complex model, has a higher tendency to overfit. We concluded that from the observation that the RE algorithm concentrates less the posterior mass around zero for rates which do not exist in the true process (Figure 6 of the previous article) than the KF. The RE algorithm places during the training more statistical weight on values of rates to explain fluctuations which are specific for the training data set. This happens no matter which model is used just because the amount of data is finite. That leads due to the higher flexibility of a too complex model to even more ways to overfit the data set. This means for a kinetic scheme trained by an RE algorithm, that if the trained model is now used to predict new data, it does not perform (generalize) as good as if the simpler model would have been trained and then used to predict new data (cross validation). In mathematical terms, more statistical weight on structures which are not present in the real process reduces, of course, the value of Equation 16 of the previous version of the article. This tendency to overfit more of the RE approach can be exploited if the model inference is done by Bayesian cross validation (Figure 7a, see Equation 16 of the previous version of the article). Because cross-validation penalizes more the bigger tendency of RE approach on the too complex model to overfit to the trainings data.</p><p>Thus, the conclusion of Reviewer 1 is the same as we drew in lines 406 -410 of the previous version of the article “This means, ignoring intrinsic fluctuations of macroscopic data, such as RE approaches do, leads to the inference of more states than present in the true model if the model performance is evaluated by the training data. One can exploit this weakness by choosing the kinetic scheme on cross-validated data, since too complex models derived from the RE approach do not generalize as good to new data as a parsimonious model”. We were intending to state that if one uses a RE approach one is limited to cross validation for the kinetic scheme selection. To the best of our knowledge, cross-validation is not the default strategy how models are selected. But the data (Figure 7a in the previous version of the article) show that it should be the method of choice if an RE approach is used.</p><p>The Bayesian filter is more flexible than a RE approach. It is more reliable to detect structures that are over-fitted by asking for which rates is the probability mass close to zero (continuous model expansion). But it can also use estimators of the minimum Kullback-Leibler entropy (Bayesian cross validation or information criteria such as WAIC). Because, Bayesian cross validation is data intensive, it is an advantage to have an algorithm which is able to perform this task on the training data by WAIC.</p><disp-quote content-type="editor-comment"><p>6. A better comparison with alternative parameter estimation approaches is necessary. First of all, explain more clearly what is different from the predictor-corrector formalism originally proposed by Moffatt (2007). The manuscript mentions that it expands on that, but exactly how? If it is only an incremental improvement, of interest to a very limited audience, a specialized, technical journal, is more appropriate.</p></disp-quote><p>We thank the Reviewer for mentioning these concerns. First, we inserted the new Figure 3 to address these concerns. In particular, Figure 3b shows even for tiny standard deviations of the open-channel noise, measured in units of single channel current <italic>σ<sub>op</sub>/i</italic> ≈ 0<italic>.</italic>01, that the previous Kalman filter, Moffat, 2007, produces a larger Euclidean error than our algorithm. The magnitude of the Euclidean error of the parameters of the kinetic scheme inferred by Moffat, 2007, becomes quickly even worse than the Euclidean Error of Milescu et al., 2005 while our algorithm and the one used by, Milescu et al., 2005 is hardly affected. The reason for this is revealed if one considers the scaling of the total amount of open-channel noise in the macroscopic signal. In fact, we interpret our findings such that for most ion channels this generalization from pure additive to additional multiplicative noise (from adding instrumental noise of a constant variance to adding instrumental noise whose variance depends on the ensemble state) is a necessary step to apply the Kalman filter. Second, we discuss the mathematical difficulties now in more detail in the main text in the subsection “Kalman filter derived from a Bayesian filter”.</p><disp-quote content-type="editor-comment"><p>Second, the method proposed by Celentano and Hawkes, 2004, is not a predictor-corrector type but it utilizes the full covariance matrix between data values at different time points. It seems that the covariance matrix approach uses all the information contained in the macroscopic data and should be on par with the state inference approach. However, this method is only briefly mentioned here and then it's quickly dismissed as &quot;impractical&quot;.</p></disp-quote><p>Thanks for pointing out that our concerns with the algorithm by Celentano and Hawkes, 2004, were not convincing. We agree that the likelihood of the Celentano and Hawkes algorithm seems to be a mathematical equivalent. But our major concern is computational speed. We agree that there is no sharp borderline between possible and impossible with regard to a Bayesian adaptation of the Celentano and Hawkes, 2004, algorithm and our algorithm. Thus, we softened the tone by writing “non-optimal or even impractical” (line 62-63). Nevertheless, we clarified our reasons to call it impractical here after the next quote and in the main article (line 60-72).</p><disp-quote content-type="editor-comment"><p>We all agree that it's a slower computation than, say, fitting exponentials, but so is the Kalman filter. Where do we draw the line of impracticability? Computational speed should be balanced with computational simplicity, estimation accuracy, and parameter and model identifiability. Moreover, that method was published in 2004, and the computational costs reported there should be projected to present day computational power.</p></disp-quote><p>To clarify our concerns, note that a maximum likelihood optimization usually takes some orders of magnitude fewer likelihood evaluations compared to the number of posterior evaluations when one samples the posterior (App. 1). The reason is not simply the larger number of samples by some orders of magnitude compared to the iteration steps of a maximum likelihood optimizer: For each excepted sample, the Hamiltonian Monte Carlo sampler (Betancourt, 2017) evaluates the derivative of the likelihood and prior distribution many times. This is necessary for numerically integrating the Hamiltonian equations via a leapfrog integrator. The evaluations of the likelihood by Celentano and Hawkes, 2004, have a computational complexity due to matrix inversion from <italic>N<sub>data</sub></italic><sup>2<italic>.</italic>3</sup> to <italic>N<sub>data</sub></italic><sup>3</sup> while the Kalman filter has a linear computational complexity in <italic>N<sub>data</sub></italic> (App. 1).</p><disp-quote content-type="editor-comment"><p>We are not saying that the authors should code the C&amp;H procedure and run it here, but should at least give it credit and discuss its potential against the KF method.</p></disp-quote><p>We expanded our considerations on the Celentano and Hawkes algorithm, and its offsprings, in the introduction (line 60-72). Based on the Celentano and Hawkes algorithm, Stepanyuk and Borisyuk, 2011, and Stepanyuk, 2014, published an algorithm which evaluates the likelihood quicker. Under certain conditions they claim that their algorithm can be faster than the Kalman filter from Moffatt, 2007.</p><p>A second point relates again to the computational complexity. Analog filtering before the analog-to-digital conversion distorts the dynamics of interest. Instead of a extensive analog filtering, it is beneficial to use the Kalman filter with a high analysing frequency because it is mathematically proven to be the minimal variance filter, Anderson et all, 2012, given the assumptions mentioned in the article are fulfilled. Under these conditions the linear computational complexity relative to <italic>N</italic><sub>data</sub> becomes even more important.</p><p>That said it is not sufficient to just care about the scaling of the computational time. Memory operations (App. 1 Figure 2) of the code between the CPUs become quickly the bottleneck of the total computational time. For example, if the algorithm is asked for each data point to save the covariance matrix and mean vector of the signal, the computational time increases roughly by 5 · 10 − 10<sup>2</sup> times (App. 1 Figure 2).</p><p>A third reason for choosing the KF, is the implementation of a second dimension of the signal which is straightforward for the Bayesian filter as all of the Kalman filter equations are vector and matrix equations.</p><disp-quote content-type="editor-comment"><p>The only comparison provided in the manuscript is with the &quot;rate equation&quot; approach, by which the authors understand the family of methods that fit the data against a predicted average trajectory. In principle, this comparison is sufficient, but there are some issues with the way it's done.</p><p>Table 3 compares different features of their state inference algorithm and the &quot;rate equation fitting&quot;, referencing Milescu et al., 2005. However, there seems to be a misunderstanding: the algorithm presented in that paper does in fact predict and use not only the average but also – optionally – the variance of the current, as contributed by stochastic state fluctuations and measurement noise. These quantities are predicted at any point in time as a function of the initial state, which is calculated from the experimental conditions. In contrast, the KF calculates the average and variance at one point in time as a projection of the average and variance at the previous point. However, both methods (can) compare the data value against a predicted probability distribution. The Kalman filter can produce more precise estimates but presumably with the cost of more complex and slower computation, and increased sensitivity to data artifacts.</p></disp-quote><p>We thank the Reviewer for this criticism because it allows us to point out what we liked to say with Table 3 in the previous version of the article. We used the terminology of the Kalman filter which separates the prediction equations for the mean and covariances of the system from the correction equations. The prediction equation evolves the mean and the covariance in time. The prediction of the mean value of the RE approach is identical to the one of the Kalman filter but there is no prediction equation for the covariance in the terminology sense of the Kalman filter. This does not mean that we intended to say that the RE approach cannot predict a covariance of the signal and state. The RE approach uses the multinomial assumption and the mean values for the prediction of the covariance. But this distinction obviously leads to unnecessary confusion and is not necessary. We therefore deleted the table.</p><p>As a rule of thumb at moderate analysing frequencies, the KF is usually 2 to 3 times slower than the RE algorithm (App. 1 Figure 3) for cPCF data. Using higher analysing frequencies for the patch-clamp data (App. 1 Figure 4a) the computational time of the Kalman filter can be even faster. This effect is not present at a lower analysing frequency. (App. 1 Figure 4b). For the two investigated data artifacts (Figure 11, 12 and App. 7) we could show that the Kalman filter performs better.</p><disp-quote content-type="editor-comment"><p>Figure 3 is very informative in this sense, showing that estimates obtained with the state inference (KF) algorithm are about 10 times more precise that those obtained with the &quot;rate equation&quot; approach. However, for this test, the &quot;rate equation&quot; method was allowed to use only the average, not the variance.</p><p>Considering this, the comparison made in Figure 3 should be redone against a &quot;rate equation&quot; method that utilizes not only the expected average but also the expected variance to fit the data, as in Milescu et al., 2005. Calculating this variance is trivial and the authors should be able to implement it easily (reviewers are happy to provide feedback). The comparison should include calculation times, as well as convergence.</p></disp-quote><p>We benchmarked against an in-house algorithm, but of course, Milescu et al., 2005, and Moffat, 2007, should be considered the gold standard. Therefore, we now challenge (Figure 3) our algorithm for patch-clamp data by the Bayesian version of Milescu et al., 2005 and Moffat, 2007. The error ratio between both algorithms amounts to 5<italic>.</italic>6 ± 1<italic>.</italic>4 for 4-state model and to 6<italic>.</italic>8 ± 2<italic>.</italic>7 for 5-state model (Figure 7).</p><p>In particular, the algorithm used by Moffat, 2007 suffers from state-depended noise (Figure 3b). Consequently, his approach is not applicable for Poissonian distributed fluorometry data. Thus, we benchmark only against Milescu et al., 2005 for confocal patch-clamp fluorometry data on a 4-state model (Figure 4), and on two more complex models (Figure 7,8 and 9) with 5 and 6 states. We show that the error ratio between the KF and the RE algorithm gets smaller when the second observable is added. For all tested models (Figure 7) the error ratio is about 1.5-2.0. Thus, adding the variance information, as asked by the Reviewer, made a large difference, similar to patch-clamp data, Milescu et al., 2005. That said, we show in Figure 4,5,6 that even if the error ratios are smaller the posterior sampled by an RE algorithm is meaningless as it is over-confident, which has also consequences for the confidence volume of maximum likelihood estimation (see discussion around Figure 4b,5,6,8,9).</p><p>In Figure 7b and d, we show that the advantage of the KF for patch-clamp data depends on the complexity of the process to be inferred. The KF is unbiased and identifies all parameters while the RE algorithm generates on the same data biased estimates and sometimes unidentified parameters.</p><p>For patch-clamp data of the most complex tested 6-state model (Figure 7 e) even the KF requires at least <italic>N</italic><sub>ch</sub> <italic>&gt;</italic> 4 · 10<sup>4</sup> channels per patch to identify all parameters. We suspect that this effect is caused by the 12 parameter dimensions of the rate matrix. Probably, the 12 dimensions make it difficult for the likelihood to dominate the bias picked up from the uniform prior.</p><p>We discuss calculation times and convergence in App. 1 Figure 2-4. Normally, the Kalman filter is 2-3 times slower than the RE approach. But against intuition, (App. 1 Figure 4a) we show that the Kalman filter can be even faster than a Rate equation type algorithm if the analysing frequency is high. This is only possible if the integration times of the Hamiltonian dynamics of the sampler are taking much longer for the RE approach. We showed that the RE approach creates too confident posteriors which in turn could lead to more integration steps of the leap-frog integrator of the sampler. Thus, algorithms with lower computational complexity in the likelihood evaluation can still be slower regarding the total computational time.</p><disp-quote content-type="editor-comment"><p>7. The manuscript nicely points out that a &quot;rate equation&quot; approach would need 10 times more channels (N) to attain the same parameter precision as with the Kalman filter, when the number of channels is in the approximate range of 10^2 … 10^4. With larger N, the two methods become comparable in this respect.</p><p>This is very important, because it means that estimate precision increases with N, regardless of the method, which also means that one should try to optimize the experimental approach to maximize the number of channels in the preparation. However, it should be pointed out that one could simply repeat the recording protocol 10 times (in the same cell or across cells) to accumulate 10 times more channels, and then use a &quot;rate equation&quot; algorithm to obtain estimates that are just as good.</p></disp-quote><p>As we were asked to redo the benchmark against the RE approach using the likelihood of Milescu, 2005, instead of least squares, there are many things that changed in our interpretation. First, we do not see anymore, in any of the tested scenarios for patch-clamp data (Figure 3c) our patch-clamp fluorometry data (Figures 4a, 7d,e) that both algorithms become equivalent in terms of the Euclidean error (Figures 3c, 4a, 7d,e) or HDCV (Figures 5c, 6a-b) with increasing channel numbers. They rather have a constant mean error ratio (Figure 7a,b) such that our old interpretation, to repeat the experiment 10 times, does not hold anymore.</p><p>Second, the ratio of the Euclidean error changes, such that with the second observable the KF is less better compared to the rate equation approach, given a constant model complexity (Figure 7a).</p><p>Third, we show that the rate equation approach is incapable to produce reliable credibility volumes. Thus, the Bayesian benefit that a parameter uncertainty quantification based on one data set is possible requires to use an algorithm which mimics the autocorrelation of the intrinsic noise, even if accuracy differences are small (Figures 5a-c). That is true even under pessimistic signal to experimental noise conditions (Figure 6b). This has consequences also for confidence volumes of maximum likelihood which at best are equivalent to the credibility volume if the likelihood dominates the prior, Jaynes, et all, 1976. For a discussion on limitations and work arounds of maximum likelihood error quantification of a dynamical system, see Joshi, et all, 2006.</p><disp-quote content-type="editor-comment"><p>Presumably, the &quot;rate equation&quot; calculation is significantly faster than the Kalman filter (particularly when one fits &quot;features&quot;, such as activation curves), and repeating a recording may only add seconds or minutes of experiment time, compared to a comprehensive data analysis that likely involves hours and perhaps days. Although obvious, this point can be easily missed by the casual reader and so it would be useful to be mentioned in the manuscript.</p></disp-quote><p>Of course, the computational complexity for the calculation of the likelihood in a rate equation approach is smaller than the computational complexity of the Bayesian filter. We confirm the intuition that typically the RE algorithm is 1.8 till 3 times faster (App. 1 Figure 3) with one important exception for which the RE algorithm is slower (App. 1 Figure 4). We hypothesize that this exception is caused by the fact that, the full computational time to gain <italic>N</italic> samples depends not only on the computational complexity and memory operations but also how many integration steps of the Hamiltonian equations need to be applied to suggest a new sample. If an algorithm such as the RE approach generates much narrower posteriors, the curvature of the posterior is higher and, consequently the HMC sampling would require more integration.</p><p>But even if our discussed limits of the RE approach are considered to be unimportant, the bottle neck in the whole scientific process data acquisition or data analysis/modeling depends on the one hand on both the speed of the data acquisition and the complexity of the modeling question. On the other hand, it depends on the resources of the laboratory. Data acquisition of fast voltage-gated channels might take many orders of magnitude less time than a conconfocal patch-clamp fluorometry experiments in slowly gating HCN channels. Taking other limitations into account, such as e.g. photo toxicity when working with fluorescent ligands, it is not even a question of the time scales but also a matter of how many repetitions can be done at all.</p><disp-quote content-type="editor-comment"><p>8. A misunderstanding is that a current normalization is mandatory with &quot;rate equation&quot; algorithms. This is really not the case, as shown in Milescu et al., 2005, where it is demonstrated clearly that one can explicitly use channel count and unitary current to predict the observed macroscopic data. Consequently, these quantities can also be estimated, but state variance must be included in the calculation. Without variance, one can only estimate the product i x N, where i is unitary current and N is channel count. This should be clarified in the manuscript: any method that uses variance can be used to estimate i and N, not just the Kalman filter. In fact, the non-stationary noise analysis does exactly that: a model-blind estimation of N and i from non-equilibrium data. Also, one should be realistic here: in some circumstances it is far more efficient to fit data &quot;features&quot;, such as the activation curve, in which case the current needs to be normalized.</p></disp-quote><p>We apologise for this error and corrected the introduction accordingly. In our old version this sentence referred rather to the in-house algorithm against which we benchmarked originally. The subtle but important differences in RE approaches were carelessly deleted during the text refinement. We indeed intended this article to be about kinetic scheme estimation not about fitting certain aspects of the data. Thus, we assume a situation where the data is qualified enough to try kinetic scheme inference.</p><disp-quote content-type="editor-comment"><p>9. It's great that the authors develop a rigorous Bayesian formalism here, but it would be a good idea to explain – even briefly – how to implement a (presumably simpler) maximum likelihood version that uses the Kalman filter. This should satisfy those readers who are less interested in the Bayesian approach and will also be suitable for situations when no prior information is available.</p></disp-quote><p>The programming language Stan offers to select from three options: Sampling the posterior, variational Bayesian inference and maximising the posterior. The same code provided by us can be used (see, Appendix 4). If one maximises the parameter of a posterior or simply of a likelihood it is then a matter of statements of prior distributions within the code. There is no need for reimplementation of the code.</p><disp-quote content-type="editor-comment"><p>10. The Bayesian formalism is not the only way of incorporating prior knowledge into an estimation algorithm. In fact, the reader may have more practical and pressing problems than guessing what the parameter prior distribution should be, whether uniform or Gaussian or other. More likely one would want to enforce a certain KD, microscopic (i)reversibility,</p></disp-quote><p>We thank the Reviewer for pointing out that our former text was not stating that adding prior information is not unique to Bayesian statistics. Similar approaches with regard to the maximum likelihood method for algebraic constrains (Salari et al., 2018) and behavioral constraints (Navarro et al., 2018) called penalized maximum likelihood, are now cited. We like to note, that the penalized maximum likelihood is synonymously called maximum a posteriori because the penalizing function is usually the logarithm of a prior (Gelman et all 2013). Thus, we do not think that it is helpful to separate simple prior considerations like uniform, Jeffrey or informative priors on subsets of or single parameters from more complex priors incorporating relations (Navarro et al., 2018) between parameters: For example, we demonstrate in the discussion of the methods section related to (Figures 7 and 9) how enforce from strictly to softly micro reversibility by means of a conditional prior on<disp-formula id="sa2equ2"><label>(3)</label><mml:math id="sa2m5"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this way we translated the algebraic constraint (Salari et al., 2018) to a rather behavioral constraint (Navarro et al., 2018) which might be chosen to be strict (thus to be effectively algebraic) or soft to allow testing whether for a channel the thermodynamic equilibrium assumption is fulfilled.</p><p>As this article is already quite long with investigating the behaviour and the robustness of the likelihood, we hardly dwell into prior selecting questions expect that we demonstrate a way to incorporate micro-reversibility. In general, all problems which can be formulated as log-likelihood with an added penalizing function for the parameters can be expressed as a prior multiplied by the likelihood and sampled (given that exp[penalizing function] can be integrated).</p><disp-quote content-type="editor-comment"><p>…an (in)equality relationship between parameters, a minimum or maximum rate constant value, or complex model properties and behaviors, such as maximum Popen or half-activation voltage.</p></disp-quote><p>The programming language Stan employs base parameter types, whose support (the interval between their minimal and maximal values) can be readily defined by the user. The Stan compiler does then the necessary transformation to an unconstrained parameter space where the sampling is takes places. More complex parameter relationships, such as inequalities, are incorporated by other predefined parameter types such as simplexes, ordered vectors etc. Again, the stan compiler does the nasty parameter transformation to an unconstrained space automatically such that these constraints are fulfilled. Thus, testing model assumptions can be implemented fast. A constraint such as <italic>P</italic><sub>open</sub> can quickly be enforced from the fact that</p><p><italic>P</italic><sub>open</sub> = <italic>f</italic>(K<italic>,L</italic>) (4)</p><p>which means for each parameter sample of the rate matrix K we can calculate the open probability for any ligand concentration <italic>L</italic>. Suppose that we have derived from previous data posteriors P(<italic>P</italic><sub>open</sub>(<italic>L</italic>)) of a set of open probabilities <italic>P</italic><sub>open</sub> for a set ligand concentrations. Then we can simply define the prior by</p><p>P(<italic>P</italic><sub>open</sub>) = P(<italic>f</italic>(K<italic>,L</italic>)) (5)</p><p>It is not even necessary that the inverse function <italic>f</italic><sup>−1</sup>(<italic>P</italic><sub>open</sub>) = K exists (which it obviously does not). The only prerequisite for doing this information fusion in a mathematically rigorous attempt is that the previous analysis derived a posterior on <italic>P<sub>open</sub></italic>. Proxy’s such as point estimates and their standard errors of should be used cautiously.</p><p>A hard limit for a derived quantity, which is not naturally fulfilled, can be rather tricky. A possible solution is defining a own prior on this derived quantity in the same manner as Equation 2. The Prerequisite is that it can be differentiated such as a sum of Gaussians. With increasing terms in the sum it is possible to model a hard limit as a soft limit. That in turn alleviates hard limit problems.</p><disp-quote content-type="editor-comment"><p>A comprehensive framework for handling these situations via parameter constraints (linear or non-linear) and cost function penalty has been recently published (Salari et al/Navarro et al., 2018). Obviously, the Bayesian approach has merit, but the authors should discuss how it can better handle the types of practical problems presented in those papers, if it is to be considered an advance in the field, or at least a usable alternative.</p></disp-quote><p>Unless the asymptotic large data limiting case of the Bernstein-von Mises theorem hasn’t taken over, it is crucial to select a reasonable prior distribution in the Bayesian context. Nevertheless, we restricted ourselves mainly to the behaviour of the likelihood of both algorithms (in the asymptotic strong data case) which offers many things to explore. If faced with the situation that a substantial amount of information enters the parameter inference via soft or inequality constraints (Salari et al/Navarro et al., 2018) one risks a waste of information if one uses a point estimator. That is because if one for example reports only the parameter vector of a penalized maximum likelihood inference and its covariance matrix most of the time one does not report the sufficient statistics of the corresponding posterior (Gelman et al., Chapter 4.5, 2013 third Edition). Take for example an inequality constraint that cuts at one standard deviation distance from the mean through the sampling distribution of a maximum likelihood inference. The mentioned Bernstein-von Mises theorem is a Bayesian justification for doing maximum likelihood inferences in the asymptotic limit (Gelman et al., Chapter 4.5, 2013 third Edition) when all prior information becomes irrelevant. Thus, we do not see any relevant differences in which statistical framework prior information for a parametric model can be formulated. But Bayesian statistics will make the most of the prior information unless the data completely dominates the prior information. A rather detailed discussion on priors is beyond the scope of this article.</p><disp-quote content-type="editor-comment"><p>11. The methods section should include information concerning the parameter initialization choices, HMC parameters (e.g. number of steps) and any burn-in period used in the analyses used in Figures 3-6.</p></disp-quote><p>We thank the Reviewer for pointing out the missing information: We included in App. 1 a section about the typically used parameter settings and convergence diagnostics. The Stan compiler generates a HMC sampler which adaptively tunes the sampling parameters. Additionally, the no-U-turn implementation (NUTS) automatically selects an appropriate number of leapfrog steps in each iteration. In that way there is much less knowledge of the user required to operate the program. Typically, we used 4 independent sampling chains with a warm up phase of 3 − 9 · 10<sup>3</sup> iterations per chain followed by the actual iterations which generate the posterior. Convergence is monitored by the <italic>R</italic>ˆ statistics (Appendix 1).</p><disp-quote content-type="editor-comment"><p>For example, how is convergence established? How many iterations does it take to reach convergence? How long does it take to run? How does it scale with the data length, channel count, and model state count? How long does it take to optimize a large model (e.g., 10 or 20 states)? Provide some comparison with the &quot;rate equation method&quot;.</p></disp-quote><p>We dedicated App. 1 to this discussion</p><disp-quote content-type="editor-comment"><p>12. In the section on priors, the entire part concerning the use of a β distribution should be removed or replaced, because it is a probabilistic misrepresentation of the actual prior information that the authors claim to have in the manuscript text. The max-entropy prior derived for the situation described in the text (i.e., an unknown magnitude where you don't know any moments but do have upper and lower bounds; the latter could be from the length from the experiment) is actually P(x) = (ln(x_{max}) – ln(x_{min}))^{-1} * x^{-1}. Reviewers are happy to discuss more with the authors.</p></disp-quote><p>We picked up these suggestions. We deleted this section and continued to employ uniform priors. We do not want to advocate a uniform prior for rate matrices. We use the uniform prior to be comparable to plain maximum likelihood, as it is the default method of most researchers, and to reduce the size of this publication. Unfortunately, the article is already quite long which makes us focus on the likelihood side of the Bayesian formalism. Therefore, we restrict the comparison between the algorithms to the situation where the likelihood dominates the uniform prior. Nevertheless, there are clear indications in Figure 3, 7, 8 and 9 that the uniform prior becomes insufficient for patch-clamp data, in particular for more complex (data generating) models and if the RE approach is used. We will present a detailed prior investigation in the near future in the archive version of another article.</p><disp-quote content-type="editor-comment"><p>13. Here and there, the manuscript somehow gives the impression that existing algorithms that extract kinetic parameters by fitting the average macroscopic current (&quot;fitting rate equations&quot;) are less &quot;correct&quot;, or ignorant of the true mathematical description of the data. This is not the case. Published algorithms often clearly state what data they apply to, what their limitations are, and what approximations were made, and thus they are correct within that defined context and are meant to be more effective than alternatives. Some quick editing throughout the manuscript should eliminate this impression.</p></disp-quote><p>We did not intend to generate this impression. What we liked to communicate was to emphasize both the similarities but in several details also relevant differences in the assumptions adopted by both approaches. We revised the manuscript accordingly.</p><disp-quote content-type="editor-comment"><p>14. The manuscript refers to the method where the data are fitted against a predicted current as &quot;rate equations&quot;. However, it is not completely clear what that means. The rate equation is something intrinsic to the model, not a feature of any algorithm. An alternative terminology must be found. Perhaps different algorithms could be classified based on what statistical properties are used and how. E.g., average (+variance) predicted from the starting probabilities (Milescu et al., 2005), full covariance (Celentano and Hawkes, 2004), point-by-point predictor-corrector (Moffatt, 2007).</p></disp-quote><p>We thank the Reviewer for this criticism which helped us to clarify our terminology. We agree that rate equations are something intrinsic of a kinetic scheme but this does not exclude them to be also a feature of an algorithm: A good Bayesian model is always a forward model. This means, given a proper prior, one can simulate parameters and then (given parameters) sample data at every time point from the likelihood which in turn is calculated either by the deterministic rate equation or by the first order Markov process of the KF. For the inverse problem every Bayesian inference algorithm takes the model assumptions of the forward model and uses them to implement a calculation rule for the likelihood and the prior given the data. In this way, it makes sense to us to speak of an algorithm as a mathematical (non-unique) representation of the set of model assumptions/features which allows to calculate the posterior. The same applies to maximum likelihood models except that parameters are fixed and can not be updated once new information is available. They have to be refitted completely. Our terminology is coherent with the terminology of the statistical research community. They mean by statistical model either a likelihood or the combination of likelihood and prior.</p><disp-quote content-type="editor-comment"><p>15. The manuscript needs line editing and proofreading (e.g., on line 494, &quot;Roa&quot; should be &quot;Rao&quot;; missing an equals sign in equation 13). Additionally, in many paragraphs, several of the sentences are tangential and distract from communicating the message of the paper (e.g., line 55). Removing them will help to streamline the text, which is quite long.</p></disp-quote><p>Thanks. We changed Roa to Rao and also streamlined and divided the text. The changes evoked by the detailed criticism slightly prolonged the article.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Reviewer #1 (Recommendations for the authors):</p><p>The authors develop a Bayesian approach to modeling macroscopic signals arising from ensembles of individual units described by a Markov process, such as a collection of ion channels. Their approach utilizes a Kalman filter to account for temporal correlations in the bulk signal. For simulated data from a simple ion channel model where ligand binding drives pore opening, they show that their approach enhances parameter identifiability and/or estimates of parameter uncertainty over more traditional approaches. Furthermore, simultaneous measurement of both binding and pore gating signals further increases parameter identifiability. The developed approach provides a valuable tool for modeling macroscopic time series data with multiple observation channels.</p><p>The authors have spent considerable effort to address the previous reviewer comments, and I applaud them for the breadth and depth of their current analysis.</p></disp-quote><p>We are glad that our revision could convince the reviewer.</p><disp-quote content-type="editor-comment"><p>1. The figure caption titles tend to say what analysis or comparison is being presented, but not what the take home message of the figure is. I suggest changing them to emphasize the latter. This will especially help non-experts to understand what the figures are trying to convey to them.</p></disp-quote><p>We checked and modified the figure caption titles that all provide now a brief take home message.</p><disp-quote content-type="editor-comment"><p>2. I very much appreciate the GitHub code and examples for running your software. However, I feel that a hand-holding step-by-step explicit example of running a new model on new data is likely necessary for many to be able to utilize your software. Much more hand-holding than the current instructions on the GitHub repo.</p></disp-quote><p>We followed the suggestion and edited the tutorial https://github.com/</p><p>JanMuench/Tutorial_Patch-clamp_data/blob/main/README.md</p><p>and https://github.com/JanMuench/Tutorial_Bayesian_Filter_cPCF_data/blob/main/ README.md to make it more a step-by-step description of how to analyze a new data set with a new model.</p><disp-quote content-type="editor-comment"><p>3. Figure captions sometimes do not explain enough of what is shown. I appreciate that many of these details are in the main text, but data that is displayed in the figure and not concretely described in the caption can makes the figures hard to follow. e.g. Figure 4a – &quot;With lighter green we indicate the Euclidean error of patch-clamp data set.&quot; But what data set do the other colors reflect? It is not stated in the caption. Again, I realize this is described in the main text, but it also needs to be defined in the caption where the data is presented. Figure 4d – Please spell out what &quot;both algorithms&quot; intends. Also, a suggestion: instead of having to refer to the caption for the color codes (i.e. RE vs. KF, etc.) it would speed figure interpretation to have a legend in the figures themselves. Few other examples: Box 1. Figure 2. – Please define the solid vs. dashed lines in the caption. Figure 3c – Please define the solid vs. dashed lines in the caption. Figure 12 – &quot;We simulated 5 different 100kHz signals.&quot; What kind of signals? Fluorescence I assume, but this needs to be explicitly defined. I'd check all figures for similar.</p></disp-quote><p>We edited the Figure 3,4 caption in this regard. Every panel has its legend and the color coding of all panel matches. But in order to keep the figure caption and the figure together small enough that they fit onto one page we took away some of the redundant information from the caption. Additionally, Figure 8, 9 got the information how much probability mass is included in each HDCI. We also edited the caption of Figure 12 and state now that we simulated cPCF data.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>In this revised manuscript, the Münch et al. have addressed all of my original concerns. It is significantly revised, though, and includes many new investigations of the algorithm's performance. Overall, the narrative of this manuscript is now to introduce an approximation to the solution for a Bayesian Kalman Filter, and then spend time demonstrating that this approximation is reasonable and even better than previous methods. In my opinion, they successfully do this, although, as they mention in their comments, their manuscript is very long.</p></disp-quote><p>We thank the reviewer that he appreciates our work.</p><disp-quote content-type="editor-comment"><p>I am not 100% certain, but the approximation that the author's make seems to be equivalent (or at least similar to) an approximation of the chemical master equation using just the 1st and 2nd moments, which is just the Fokker-Planck equation. The authors should discuss any connection to this approximation, as there is a great deal of literature on this topic (e.g., see van Kampen's book).</p></disp-quote><p>We added a respective sentence in the introduction about protein expression studies which approximates the solution of the chemical master equation (CME) with the linear noise approximation to derive a Kalman filter. The linear noise approximation is the van Kampen’s System size expansion “pursued only up to first order”[13], or a linearized Fokker-Planck equation [11].</p><p>We added a paragraph in the conclusion to contextualize our method against the background of the classical approximations of the CME. We conclude due to the first order dynamics of chemical reaction network our approach is actually equivalent to the full chemical Fokker-Planck equation.</p><disp-quote content-type="editor-comment"><p>In Figures 3A and 4D, it is unclear to me what is plotted for the RE and classical Kalman filter (i.e., how is there a posterior if they are not Bayesian methods)? Perhaps it is buried in the methods or appendices, but, if so, it needs to at least be clarified in the figure captions.</p></disp-quote><p>We implemented a full Bayesian Version of Milescu 2005, algorithm and Moffatt 2007, algorithm, thus all results in this paper were obtained by posterior sampling. Since we used uniform priors, the mode of the posterior equals the maximum of the likelihood, i.e. the outcome of the classical implementation as a point estimator. However, our implementation of Milescu 2005 algorithm and Moffatt 2007 algorithms has the advantage that all sorts of priors can be applied. Unfortunately, our wording was somewhat ambiguous: Every Kalman filter is Bayesian filter (Moffatt, 2007) regarding the time varying unknown <bold>n</bold>(<italic>t</italic>) ensemble state. That is independent of how the time constant parameters such as the rate matrix parameters are inferred. The wording “Bayesian Filter” for our algorithm was used to distinguish it from the previous algorithm (Moffatt, 2007), as only our generalized Bayesian filter allows the inclusion of state dependent noise. We added to the manuscript in the caption and the main text that we implemented both algorithms in the full Bayesian framework.</p><disp-quote content-type="editor-comment"><p>The Bayesian statistical tests devised to determine success (e.g., on pgs. 12-13) seem a little ad hoc, but are technically acceptable. I do not see a need for additional metrics.</p></disp-quote><p>Interestingly, we found out is is not that actually not that “ad hoc” as we thought in the beginning. We in fact test the frequentist coverage property of “credibility” intervals or volumes, see [8] Equation 2.1. Usually for parametric Bayesian statistics it is at least asymptotically clear by the Bernstein von-Mises theorem (when the data swamped the prior) that coverage property should hold the frequentist interpretation of probability [9]. Just like us, others interpret the Bayesian uncertainty quantification as “invalid” if this frequentist coverage does not hold. [10]. We add a sentence “Noteworthy, that this is a empirical test of how sufficient the Bayesian filter and the RE approach hold frequentist coverage property of their HDCVs.” to indicate the theoretical foundation of our approach.</p><disp-quote content-type="editor-comment"><p>Line 939: Equation 61 is absolutely not &quot;close to uniform distribution&quot;. The α and β parameters of 100.01 are much larger than 1. It is incredibly peaked around 0.5. Perhaps this is a typo?</p></disp-quote><p>This was indeed a left-over text fragment from the previous version of the article. We changed it to ”sharply peaked β distribution”.</p><disp-quote content-type="editor-comment"><p>Line 942: The allowed small breaking of microscopic reversibility in the prior is an interesting idea that I wish the authors would expound upon more.</p></disp-quote><p>Thanks for this encouragement. Indeed, we plan to go more into detail on this topic in the future.</p><disp-quote content-type="editor-comment"><p>Line 712: The authors state that the simulation 'code will be shared up-on request'. They should include it with their github pages tutorials for running the examples in case others wish to check their work and/or use it. There is no reason to withhold it.</p></disp-quote><p>The simulation code is exemplified here:</p><p>https://cloudhsm.it-dlz.de/s/QB2pQQ7ycMXEitE and now also in the manuscript.</p><disp-quote content-type="editor-comment"><p>Line 707: 'Perspectively' is not a commonly used word.</p></disp-quote><p>Thanks. We changed “Perspectively, extensions …” to “Prospective extensions …”</p><disp-quote content-type="editor-comment"><p>Reviewer #4 (Recommendations for the authors):</p><p>The authors have addressed all my comments and suggestions. The manuscript is nice and extremely comprehensive, and should advance the field.</p><p>Nevertheless, the manuscript is also very long (but justifiably so), and certain statements could be a little clearer. Most of these statements refer to the comparison with the so-called rate equation (RE) methods, with which I'm more familiar. For example:</p></disp-quote><p>We thank the Reviewer for his appreciation of our work.</p><disp-quote content-type="editor-comment"><p>Abstract: &quot;Furthermore, the Bayesian filter delivers unbiased estimates for a wider range of data quality and identifies parameters which the rate equation approach does not identify.&quot;</p><p>The first part of this statement is not quite true, as Figure 4 shows clearly that the Bayesian estimates are biased (and the authors acknowledge this elsewhere in the manuscript). If they are biased in one &quot;range of data quality&quot;, that probably means they are always biased, just to different degrees.</p></disp-quote><p>We agree that it is right to say “If they are biased in one “range of data quality” this probably means they are always biased, just to different degrees”. The approximations lead to all sorts of errors including bias. This argument is also supported by the findings of Moffatt, 2007 (Figure 11) in which he detects bias in the deterministic approach (Milescu, 2005) and his own Kalman filter. It can certainly be stated that Figure 4b indicates a bias. However, even for optimistically high signal-to-noise in the fluorescence signal as in Figure 4, reasonable large HDCIs such as the 0<italic>.</italic>95−HDCIs of Figure 4c are covering the true value. Thus, the bias can be interpreted as negligible relative to the parameter uncertainty quantification. One can actually observe how the size (the probability mass) of the HDCV of the Bayesian filter overcomes the bias in Figure 5c. We toned down our statement by adding the word “negligible” in the context with “biased estimates” in the abstract. Considering the differences in the bias in Figure 9, this seems justified to us. We also mention the bias now in the discussion of Figure 4.</p><disp-quote content-type="editor-comment"><p>This is not surprising, because the Kalman filter is a continuous state approximation to a discrete state process, and the overall estimation algorithm makes a number of approximations to intractable probability distributions. It would definitely be correct to say that the estimates are very good, but not unbiased.</p></disp-quote><p>We agree with the reviewer and added to the discussion of Figure 4 a statement indicating the bias of both algorithms.</p><disp-quote content-type="editor-comment"><p>Second, this statement is also ambiguous. Are you referring to the theoretical non-identifiability caused by having more parameters in the model than what the combination of estimator+experimental protocol can extract from data? In this case, it's not a matter of certain parameters that cannot be identified, but a matter of how many can be identified uniquely. The more information is extracted from the data, the more unique parameters can be identified, so the Kalman filter should do better. Or, alternatively, are you referring to specific parameters that are poorly identified because, for example, they refer to transitions that are rarely undertaken by the channel? In this case, it would be a matter of obtaining looser or tighter estimates with one method or another, but the parameters should be intrinsically identifiable, I imagine, regardless of the algorithm.</p></disp-quote><p>In the context of the whole article this sentence seems to be unambiguous to us. The abstract alone leaves of course room for interpretation but we defined what we mean by unidentified parameters by Equation 16 in the main article. To avoid confusion we changed in the abstract the sentence to “For some data sets it identifies more parameters than the rate equation approach”. By our definition we are able to detect many cases of both practical and structural unidentifiability (Middendorf,Aldrich 2017). But our definition is not equivalent to the definition by (Middendorf,Aldrich 2017). We added an explanatory paragraph after Equation 16.</p><disp-quote content-type="editor-comment"><p>In any case, it's not clear that the better identifiability is the result of the Bayesian side, or of the predictor-corrector state inference filter. I would guess it is the Kalman filter, but I'm not sure.</p></disp-quote><p>Indeed, it is only the “predictor-corrector” side of the algorithm. Unfortunately, our wording was somewhat ambiguous which gives rise to a couple of the Reviewer’s following questions: Every Kalman filter is a Bayesian Filter but not every Bayesian Filter is (the classical) Kalman filter. In fact, Moffatt, 2007 derives his Kalman filter as a Bayesian forward algorithm in continuous space. He thus shows that the forward algorithm and the Kalman filtering performed for discrete and continuous state Markov processes, respectively, are the same. We come to the same conclusion herein, see Equation 5-9. (Nevertheless, he optimizes the rate matrix etc. via maximum likelihood optimization.)</p><p>We additionally describe in Equation 11 the consequences of the open-channel noise, that Equation 11 cannot be derived by the original Kalman filter equations. A more flexible noise modeling is necessary. This leads to our algorithm which is exact up to the second moment. That also includes other state-dependent noise such as photon counting noise. We believe that this explicit noise description is the reason for the performance advantage of our algorithm. We would also expect this advantage if our algorithm would be implemented as point estimator. The full Bayesian implementation results in a fully sampled posterior and, in principle, allows for arbitrary priors. Since we used uniform priors, the mode of the posterior equals the maximum of the likelihood, i.e. the outcome of the classical implementation as point estimator.</p><disp-quote content-type="editor-comment"><p>Perhaps it would be clearer if you said that the KF method produces good estimates and generally improves parameter identifiability compared to other methods, as it extracts more information from the data?</p></disp-quote><p>As mentioned above, we changed the abstract to avoid misunderstandings.</p><disp-quote content-type="editor-comment"><p>Introduction:</p><p>32: I'm not sure, but if the intention here is to cite mathematical treatments for estimation, you may add references to the &quot;macroscopic&quot; papers by Celentano, Milescu, Stepaniuk and perhaps a few others that use &quot;rate equations&quot;. Also, you may cite Qin et al., 1996, as a single channel paper describing a method used in hundreds of studies.</p></disp-quote><p>Thank you for pointing out the Qin paper, 1996. We also refer now to the other proposed papers at that position in the text.</p><disp-quote content-type="editor-comment"><p>Pg. 3:</p><p>51: I remain skeptical that it is a good idea to use &quot;rate equations&quot; (RE) as a term to refer to those methods that are fundamentally different from the approach described here (also see my comment to the first submission).</p></disp-quote><p>We follow with our terminology on stochastic processes that of other groups in the community [6], [4], (Walczak 2011). In this terminology, the rate equation or (reaction-)rate equation is a deterministic ordinary differential equation describing the time evolution of the first moment (the mean value) of the chemical master equation (CME) [3, 4]. To our understanding the rate equation is the basis of the work of Milescu 2005 which justifies the wording.</p><disp-quote content-type="editor-comment"><p>The rate equations must always be used to predict a future state from a past or current state, by all methods, explicitly or implicitly, because REs simply describe the channel dynamics. In this very manuscript, Equation 3, central to the Kalman filter formalism, is nothing but a deterministic rate equation with a stochastic term added to approximate the stochastic evolution of an ensemble of channels.</p></disp-quote><p>Indeed, all approximations (deterministic or stochastic) which make physically sense should have as the large system limit the solution of the RE and thus solve at least implicitly the RE. We hesitate to use today only the celebrated traditional chemical kinetics perspective [5] that the Res describe the (full) channel dynamics. But we acknowledge that some others do. Also, we are aware of its central importance as one aspect of the evolution of a chemical system. However, we prefer the accepted terminology [4, 5, 12] that the CME is the governing equation of a well stirred ensemble of chemically reacting molecules or in other words of a Markov jump process. Our Bayesian filter uses only stochastic approximation Equation 3 of the CME, not the CME itself.</p><p>Strictly speaking, Equation 3 is the solution of the rate equation plus a stochastic perturbation. The perturbation around the mean evolution defines it as a stochastic approach. It is a standard technique to approximate the solution of the CME around the mean evolution (the RE) with a Gaussian perturbation [4],[12]. In this regard our method does not differ from the cited methods but it does differ from the RE approach by its rigorous evaluation of the likelihood. Consequently the solution of the RE without any perturbation does also occur in the Bayesian filter as the time evolution of the mean value of the ensemble vector <italic>E</italic>[n(<italic>t</italic>+∆<italic>t</italic>)|n(<italic>t</italic>)] = Tn(<italic>t</italic>). Everything else would be mathematically surprising.</p><disp-quote content-type="editor-comment"><p>In fact, there are some old papers by Fox and some more recent by Osorio (I'm not exactly sure of the name and I don't remember the years) that discuss that approximation and its shortcomings – perhaps that is the source of bias?</p></disp-quote><p>We would be really interested to discuss the mentioned papers but without further comments we are not able to find them. We agree that the approximations are the origin of the bias.</p><disp-quote content-type="editor-comment"><p>Whether that prediction is then corrected, as in the Kalman filter approach, with a corrector step is irrelevant, as far as the rate equations.</p></disp-quote><p>It is true that in-between the correction steps the solution of the RE is used to predict the future mean state and similarly a matrix expression is used to deterministically predict the future covariance. But the defining stochastic perturbation of Equation 3 allows to go beyond a deterministic mean evolution (RE approach) of the system. This is exactly induced by the “correction step”. Therefore, we believe it is appropriate to call Milescu 2005 a deterministic RE approach. On a further note, if one would be only interested in solving the dynamic equations without relation to experimental data, we agree that the difference between REs with added variance and Equation 3 would be one of wording; it is, however, the combination of the stochastic dynamic model with the rigorous likelihood computation via the correction steps which makes the Kalman filter perform differently from RE approaches when applied to data.</p><disp-quote content-type="editor-comment"><p>Furthermore, it's all a matter of degree. For example, the Milescu et al. approach, which is classified here under the RE umbrella, predicts future states as a mean + variance (or as a mean only), but only using the initial state. There is no correction at each point, as with the Kalman filter method, but there is a correction of the initial state from one iteration to the next (by the way, it's not clear if you implemented that feature here, which would make a big difference). Then, because it considers the stochastical aspect of the data, the Milescu et al. approach should also be considered a stochastic method, just one that doesn't use all the information contained in the data (and so it is fast).</p></disp-quote><p>To our understanding we are consistent with the common terminology when distinguishing deterministic (Milescu, 2005) and stochastic modelling (Moffat, 2007) in that way: An illustration of this distinction can be seen in Figure 2. panel A and B in Moffatt, 2007: The RE approach (panel A in Moffatt, 2007) creates a mean signal which is continuous and derivatives can be taken. Panel B and C are only continuous but one cannot take the derivative of the mean signal (at least not in the strict sense of a mathematical limit), which is one central mathematical problem of stochastic methods. In particular that is true for stochastic differential equations which therefore are usually represented in an integrated form such as Equation 3. We like to state that we do appreciate the work of (Milescu, 2005) and (Moffat, 2007) because it is the basis for our stochastic extension elaborated herein. We only disagree with the suggested wording. The following refers to the statement in brackets: We like to draw the Reviewer’s attention to two instances of the article where we state how initialisation was done:</p><p>1. Directly after Equation 8 line 224 (which is maybe a bit indirect).</p><p>2. After Equation 59 which is explicit.</p><p>Similar for the RE approach the equilibrium assumption is used as described in (Milescu, 2005).</p><disp-quote content-type="editor-comment"><p>Imagine a situation where you ran the same stimulation protocol multiple times, but each time you record only one data point, further and further away from the beginning of the stimulation protocol. A &quot;stochastic&quot; algorithm applied to this type of data would be exactly as described in Milescu et al.</p></disp-quote><p>Indeed, in this case the deterministic RE approach (Milescu, 2005) derives a solution which is identical to the solution of the master equation of a closed system if the initial conditions are multinomial [6]. We agree that Milescu, 2005 is a sound approximation of a stochastic process, however, it is not stochastic modeling itself (to our understanding of the terminology), even if the derived likelihood is identical in the mentioned example to solving the master equation.</p><disp-quote content-type="editor-comment"><p>Of course, in reality all points are recorded in sequence, but that doesn't mean that the approach is not stochastic, just that it's simplifying and discarding some information to gain speed. All methods (such as the Kalman filter described here) make some compromises to reduce complexity and increase speed.</p><p>The bottom line is that there is the most basic approach of solving the rate equations deterministically, without considering any variance whatsoever, and then there is everything else.</p></disp-quote><p>All methods make compromises and so does the Bayesian filter. Using the word deterministic simply points to the details of the assumptions which are adopted to calculate a likelihood, given the terminology that we chose.</p><disp-quote content-type="editor-comment"><p>58: &quot;Thus, a time trace with a finite number of channels contains, strictly speaking, only one independent data point.&quot; I don't understand this sentence. Could you please clarify?</p></disp-quote><p>This is exactly meant in the way the Reviewer constructed his example. The RE plus the multinomial assumption is exact if one uses only one data point per relaxation because implicitly (Milescu, 2005) models each data point as an independent draw from a multinomial distribution (plus the experimental noise) whose probability vector evolves in time governed by the rate equation. But in the common perception that the dynamics of the ion channel can be described by a first order Markov process, every following data point has some conditional dependency on the first one. Thus, every following data point is statistically not independent.</p><disp-quote content-type="editor-comment"><p>74: &quot;The KF is the minimal variance filter Anderson and Moore (2012). Thus, instead of excessive analog filtering of currents with the inevitable signal distortions Silberberg and Magleby (1993) one can apply the KF with higher analysing frequency on less filtered data.&quot;</p><p>Yes, but I'm not sure why should we use excessive filtering? Where is this idea coming from?</p></disp-quote><p>We thank the Reviewer for his careful reading. We changed the sentence to “The KF is the minimal variance filter [<bold>?</bold>]. Instead of strong analog filtering of currents to reduce the noise, but with the inevitable signal distortions (Silberberg and Magleby (1993)), we suggest to apply the KF with higher analyzing frequency on minimally filtered data.”</p><disp-quote content-type="editor-comment"><p>131: &quot;However, even with simulated data of unrealistic high numbers of channels per patch, the KF outperforms the deterministic approach in estimating the model parameters.&quot;</p><p>It is clearly true from your figures, but please give a number as to what is unrealistic (10,000? 100,000?). Also, outperforms, but by how much? As I commented above and below, all methods tested here seem to produce good estimates under the conditions they were designed for (and even outside), and one might argue that it's not worth adding the additional computational complexity and running time for a possibly small increase in accuracy. How is one to judge that increase in accuracy?</p></disp-quote><p>We added a “more than a couple of thousands within one patch” to specify typical channels numbers among patches. In our opinion, evaluating the performance of an algorithm by one number (in terms of accuracy only) does not give all the relevant information. There is the relative distance to the true values which the Reviewer has his focus on. But there is also the problem with the uncertain quantification of the RE (Figure 4,5,6,8,9) approach and with the fact that the rate equation approach sometimes does not identify all parameters (Figure 9) which can be identified by the Bayesian filter. All of these aspects are central points of this article and are lumped together in the word “outperforms”. We do not see how to break this down to a simple number.</p><p>To be less vague, we added a sentence at the end of the introduction saying which aspects of performance are considered ”We consider the performance of our algorithm against the gold standards in four different aspects: (I) The relative distance of the posterior to the true values, (II) the uncertainty quantification, here in the form of the shape of the posterior, (III) parameter identifiability, and (IV) robustness against typical misspecifications of the likelihood (such as ignoring that currents are filtered or that the integration time of fluorescence data points is finite) of real experimental data.”</p><disp-quote content-type="editor-comment"><p>276: Has the RE method been used with the mean + variance or mean only?</p></disp-quote><p>We used the (Milescu, 2005) algorithm with the variance information.</p><disp-quote content-type="editor-comment"><p>Also, how was the initial probability vector calculated with the RE method? If you used the mean + variance, then (as mentioned above), you can't call it deterministic. If you used only the mean, then it is indeed a deterministic method, but it's not the approach described in Milescu et al. Please explain.</p></disp-quote><p>The publication (Milescu, 2005) describes how to initialize the time evolution by making use of the equilibrium assumption over all states. In the same way, we solve the transition matrix for its equilibrium vector (explained after Equation 59). For the RE approach the probability vector is evolved in time whereas for the Bayesian filter both the mean vector and the covariance matrix of the initial distribution are evolved in time.</p><p>To our understanding the variance is a deterministic parameter describing a stochastic process, but is not stochastic itself. Its definition is based on a mean value<disp-formula id="sa2equ3"><label>(1)</label><mml:math id="sa2m6"><mml:mrow><mml:mtext>var</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo stretchy="false">[</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">]</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To the best of our knowledge the deterministic differential equation for the evolution of the variance is derived by applying the definition above to the CME [15].</p><p>It is the same situation for the initial distribution. The definition<disp-formula id="sa2equ4"><label>(2)</label><mml:math id="sa2m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mi>y</mml:mi><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mi>y</mml:mi><mml:mo>∙</mml:mo><mml:mi>P</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>of the mean value which is a deterministic object enables to express the starting probability vector as<disp-formula id="sa2equ5"><label>(3)</label><mml:math id="sa2m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>ch</mml:mtext></mml:mrow></mml:msub></mml:mfrac><mml:mi>E</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:munder><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>∙</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mtext>equilibrium</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This equation holds independently of the fact that all investigated algorithms calculate their initial probability vector by Equation 14 of (Milescu, 2005).</p><p>We are aware that an estimator of the mean or the variance or the location of the maximum of the likelihood in parameter space are mathematically random variables because they are data-dependent. However, to our understanding of the terminology, that does not necessarily mean that stochastic modeling has been used to define the estimator (of a mean value, a variance or a full likelihood). To exemplify this for the initial probability vector <bold>Ξ</bold>(<italic>t</italic><sub>0</sub>) both algorithms start with the identical multinomial assumption. But given the current parameter sample the Bayesian filter updates from the multinomial distribution by accounting the first data point <italic>y</italic>(<italic>t</italic><sub>0</sub>) the first two moments <italic>E</italic>[n(<italic>t</italic><sub>0</sub>)] and P(<italic>t</italic><sub>0</sub>) before the two moments are evolved deterministically in time. This update takes explicitly the stochastic nature of the process into account but it is not present in the RE approach. In our opinion that is meant with the distinction between deterministic and stochastic in the terminology.</p><disp-quote content-type="editor-comment"><p>229: &quot;added, in Equation 9 to the variance … which originates (Equation 38d) from the fact that we do not know the true system state&quot;</p><p>Which noise are you referring to? The state noise or the measurement noise?</p></disp-quote><p>Both noise aspects are influencing this term: P(<italic>t</italic>) is the covariance matrix of the aleatory uncertainty about n(<italic>t</italic>). Thus, the covariance matrix of the prior distribution over n(<italic>t</italic>) before the information of the current data Equation 58-59 has been taken into account (before the correction step). So it is influenced by the combination of both intrinsic and experimental noise sources Equation 38d. Only at the initial step, where the equilibrium distribution of the channel is used, P(<italic>t</italic>) is the covariance matrix of the intrinsic noise alone. To clarify this in the article, we added the sentence “P<italic><sub>t</sub></italic> is the covariance of the prior distribution over n(<italic>t</italic>) before the KF took y(<italic>t</italic>) into account” and we used the word “aleatory”.</p><disp-quote content-type="editor-comment"><p>326: Which are the two algorithms?</p></disp-quote><p>We changed “the two algorithms” to “KF and the RE approach”.</p><disp-quote content-type="editor-comment"><p>278: &quot;Both former algorithms are far away from the true parameter values with their maxima and also fail to cover the true values within reasonable tails of their posterior.&quot;</p><p>I think the authors might have gotten a little carried away when they made this statement. There is no doubt that the Kalman/Bayesian method produces more accurate estimates, but the other two methods (KF and RE) are very reasonable as well. I see estimates that are within 5, 10, or 20% from the true values, for most parameters. This is not &quot;failure&quot; by any stretch of the imagination. Most people would call this quite good, given the nature of the data.</p></disp-quote><p>We replaced “far” by “further” to avoid any subjective definition about what is close and what is far. We added a quantification of parameter divergence: “[…] E.g., the relative error of the maximum of the posterior are ∆<italic>k</italic><sub>21</sub> ≈ 200% for [1] and ∆<italic>k</italic><sub>32</sub> ≈ 240% for [2]. The 4 other parameters including the three equilibrium constants behave less problematic judged by their relative error […]” We also like to direct the attention of the reader to our statement “[…] also fail to cover the true values within reasonable tails of their posterior”, this means that the uncertainty quantification is indeed failing (the posteriors are too narrow). This points to a central aspect of our work (Figures 3, 4, 5, 6, 8, and 9). With this statement, we did not refer to the relative errors which the Reviewer uses to evaluate the figure. To clarify this, we added: “… Additionally, if one does not only judge the performance by the relative distance of maximum (or some other significant point) of the posterior but considers the spread of the posterior as well, it becomes apparent, that the marginal posterior of both former algorithms fail to cover the true values…”</p><p>To clarify our evaluation perspective, we added a paragraph at the end of the introduction. We thank the reviewer for inspiring these clarifications.</p><disp-quote content-type="editor-comment"><p>294: &quot;The small advantage of our algorithm for small … over Moffatt (2007) is due to the fact that we could apply an informative prior in the formulation of the inference problem … by taking advantage of our Bayesian filter. &quot;</p><p>This is an interesting and important statement. I interpret it as saying that the Bayesian aspect itself makes only a small contribution to the quality of the estimates, when comparing it with the Moffatt method, which is a Kalman filter as well. The only issue with the Moffatt method is the lack of an explicit formalism for the excess state noise (which could presumably be added).</p></disp-quote><p>We suggest that this interpretation is based on a misunderstanding. There are two layers of Bayesian statistics in our algorithm and one in the previous Kalman filter. We added the word “generalizations” to make clear that one of these layers of our algorithm uses a similar Bayesian update (in other words prediction-correction) as in (Moffatt, 2007). As argued above, each KF is a Bayesian filter. The derivation of (Moffatt, 2007) is done as a Bayesian version of the forward algorithm. Nevertheless, his implementation of the Kalman filter infers the rate matrix via maximum likelihood. The statement “The only issue with the Moffatt method is the lack of an explicit formalism for the excess state noise (which could presumably be added)” does not reflect that our article demonstrates exactly how to change the calculation of the likelihood of (Moffatt, 2007) to include “excess state noise” (Equation 44-46c) and Poisson noise (Equation 52 57). The central mathematical point is how to circumvent that Equation 8 cannot be solved in the way it was done by Moffatt, 2007, for state-dependent noise (Equation 10-11). Our solution is independent of which statistical framework (either maximum likelihood or Bayesian statistics) is used for the inference of the rate matrix. (Also, our implementation in Stan can be used as a point estimator or to sample the full posterior.)</p><p>The conclusion “I interpret it as saying that the Bayesian aspect itself makes only a small contribution to the quality of the estimates” additionally skips, over the fact that the (Moffatt, 2007) algorithm can be seen as the limiting algorithm of ours for <italic>σ</italic><sub>op</sub> → 0. The more one goes to the left of Figure 3b the more the two algorithms become identical and thus also their results. In case that the Reviewer meant the Bayesian posterior sampling of the rate matrix etc. we agree. But according to our arguments above, a parameter whose influence becomes less and less important in the algorithm as a whole (going to the left in Figure 3b) can asymptotically be fixed at a maximum likelihood inference, Simply, because also its error becomes more and more irrelevant. But this should not be the case going further to the right in Figure 3b.</p><p>Another point of view is that one can see the (Moffatt, 2007) algorithm as a likelihood misspecification for data with state-dependent noise. The more we go to the left in Figure 3b the smaller the misspecification is.</p><disp-quote content-type="editor-comment"><p>It also suggests to me that any method that tries to use the noise may run into difficulties when the noise model is unknown (typical real-life scenario). The Moffatt method is confronted with unknown noise and it fails. What about when the Bayesian method is confronted with unknown noise? There are some comments in the manuscript, but nothing tangible. Could you please comment?</p></disp-quote><p>It depends on what the Reviewer means with unknown. It is not the issue that in Figure 3b (Moffatt, 2007) <italic>σ</italic> and <italic>σ</italic><sub>op</sub> are unknown. The problem of (Moffatt, 2007) is that his likelihood is not flexible enough and that there is no structure in the algorithm (Moffatt, 2007) to account for <italic>σ</italic><sub>op</sub>. Inspired by the question of the Reviewer we added the sentence “Further, (Figure 3b) indicates the importance that the functional form of likelihood is flexible enough to capture the second order statistics of the noise of the data sufficiently.” We observed also for cPCF data the same effect at the early stages of the project when coding mistakes made the conditional likelihood of the experiment flawed.</p><p>In general, misspecifications of the likelihood will always lead to sometimes small and sometimes huge deviations in the inferred parameters compared to a fictitious inference with an ideal likelihood and prior. But this holds for both maximum likelihood inference and Bayesian statistics.</p><p>The learning of the unknown noise statistics is briefly discussed in Figure 10e. It displays the posterior of <italic>σ</italic><sub>op</sub><sup>2</sup> for PC or cPCF data if only an informative prior on <italic>σ</italic><sup>2</sup> is used. Information about the baseline variance is always possible to be characterized prior to the inference of interest. Also Figure 11 only assumes a pre-characterization of <italic>σ</italic> and then <italic>σ</italic><sub>op</sub> is inferred from the actual data (even though we did not display <italic>σ</italic><sub>op</sub>).</p><disp-quote content-type="editor-comment"><p>Figure 3: There are no data sets in the figure. What data were analyzed here?</p><p>Is there a typo regarding the colors in a? What are the red, blue, black, green symbols? Please verify.</p><p>Assuming the red is KF, there is something very curious about its estimates, which are very different in distribution from the Bayesian estimates. Could there be a coding problem? If red is actually RE, then it would make more sense. Could you explain? Is this the effect of the &quot;excessive&quot; open state noise? If so, I find this situation a bit unfair, because then the 2007 KF algorithm is tested with data that it is not meant to work with.</p></disp-quote><p>We improved figure 3 and its legend and used the colors more consistently. We display in the new Figure 3d the data which were used to generate the posteriors of panel a and b. Yes, red is our own Bayesian implementation of the previous Kalman filter (Moffatt, 2007) and (blue) is our Bayesian implementation of (Milescu, 2005). To increase clarity, we made the blue colors identical which were not identical in the previous version. It is indeed the effect of the open-channel noise which we state in the main text by “Varying <italic>σ</italic><sub>op</sub><italic>/i</italic> reveals the range of the validity (Figure 3b) of the algorithm (red) from Moffatt (2007)”. The figure legend was adapted accordingly.</p><p>We like to state further that it was one of the central questions in the first comment by the reviewer to demonstrate that our algorithm is not only an incremental step forward. If we compare the algorithm in a situation where the standard deviation of the excessive open channel <italic>σ</italic><sub>op</sub> = 0, our algorithm and the previous Kalman filter (Moffatt, 2007) produce an (almost) identical posterior because the likelihoods of both algorithms are essentially identical (see Methods section). The posteriors are only almost identical because our algorithm allows some uncertainty in <italic>σ</italic><sub>op</sub>. Figure 3b shows a limitation of (Moffatt, 2007) which is not a limitation of our algorithm.</p><disp-quote content-type="editor-comment"><p>Also, I think it would be more interesting to have a comparison between the original KF and the newer Bayesian approach, so we can understand what Bayesian estimation does. In any case, I can't really interpret the data until you clarify the colors.</p></disp-quote><p>Back to the misunderstanding above: Both Kalman filters are Bayesian filters. Our one is a more flexible version of the previous one. It is the role of Figure 3b to illustrate this comparison. Thus, Figure 3b explains when our generalization is necessary. The question about what advantage the sampling of the posterior of the rate matrix gives vs. a maximum likelihood inference is not in scope here. The sampling of the posterior, if dominated by data and not the prior itself does not improve parameter identification, however, enables a more detailed uncertainty evaluation. As we are using a uniform prior for all parameters the mode of the posterior corresponds to the maximum likelihood inference.</p><disp-quote content-type="editor-comment"><p>The legend is unclear overall.</p></disp-quote><p>We revised the legend.</p><disp-quote content-type="editor-comment"><p>303: What means &quot;singular&quot;?</p></disp-quote><p>We added now also in the main text “meaning that the fisher information matrix is singular (Watanabe, 2007)”.</p><disp-quote content-type="editor-comment"><p>324: What shaded area in Figure 1a? Perhaps you mean Figure 4a?</p></disp-quote><p>We are not sure what the Reviewer means here because the figure reference where the shaded area is mentioned in the main text indeed points to Figure 4a.</p><disp-quote content-type="editor-comment"><p>Figure 4: I don't see any light green.</p></disp-quote><p>We changed the light green to a dotted green line.</p><disp-quote content-type="editor-comment"><p>368: &quot;The estimated true success rate by the RE approach (blue) is ≈ 0.15 and therefore far away from the true success probability. In contrast, the posterior (green) of the true success probability of the KF resides with a large probability mass between the lower and upper limit of the success probability of an optimal algorithm (given the correct kinetic scheme).&quot;</p><p>I'm really a bit puzzled here: yes, the KF is more accurate (given the correct kinetic scheme), but what I see in this figure is that both algorithms are biased, yet both are generally within 10% (and quite a bit better for KF) of the true values.</p></disp-quote><p>The quoted sentence refers to Figure 4c which is used to introduce a Bayesian way to look at the spread of the probability mass versus the distance to the true value. We disagree with the Reviewer that one can deduce a biased <italic>P</italic><sub>success</sub> from the posterior of the binomial test.</p><p>Possibly, the Reviewer refers to a different aspect than we do. We assume he was wondering why the observed bias of Figure 4b is not visible in Figure 4c. The answer is because we tested in Figure 4c for 0.95 probability mass which, even with the bias included, covers the true values according to the binomial interpretation.</p><p>We plotted Figure 4c to have a gentle introduction to the concept. The final solutions are discussed in Figures 5 and 6. We assume that the Reviewer expected to see that binomial statistics do not match the data that well. But Figure 4c is not the optimal figure to detect this because the probability mass is chosen too big. But one can see this in Figure 5c because here we plot success vs. probability mass. And indeed, one might discover the bias again (if one knows that it is there) in the slight under-performance of the Bayesian filter for probability masses <italic>&lt;</italic> 0<italic>.</italic>5. Note, that these plots are for uncertainty quantification but are not optimally suited for bias detection because the direction of the bias is lost, and all data sets are lumped together in one binomial test. They test if the posterior (its highest density volumes) holds frequentist properties.</p><disp-quote content-type="editor-comment"><p>If one would plot the log of the rates, to transform into δ Gs, the differences would be even smaller. I would bet that experimental artifacts would contort the estimates to a greater degree anyway.</p></disp-quote><p>Indeed, the ratio of the accuracies is small, but it is the essential message of Figure 4 that even if the accuracies are almost the same the uncertainty quantification of the RE approach is defective.</p><p>Of course, depending on the experimental setting, experimental artifacts can mess up everything if they are not correctly accounted for.</p><disp-quote content-type="editor-comment"><p>It's very nice that the RE approach was embedded and tested within a Bayesian framework, but it would still be interesting to know what is the contribution of the Bayesian aspect (unless I missed this point in the manuscript).</p></disp-quote><p>As mentioned above the posterior sampling itself (when using uniform priors) does not influence the accuracy of the parameter estimates. The mode of the posterior equals the maximum of the likelihood. Using more elaborate priors is not in the scope of the paper.</p><disp-quote content-type="editor-comment"><p>436: &quot;…while their error ratio (Figure 7a) seems to have no trend to diminish with increasing NCh&quot;.</p><p>This would be unexpected, because the Kalman filter will always use more information than RE. Why would the KF approach become relatively more erroneous at higher Nc? I don't see any reason.</p></disp-quote><p>Indeed, this was an awkward use of words. What we like to say is that the error ratio does not approximate 1. The error of the RE approach does not seem to converge to the error of the KF. There is obviously no reason to assume that the Bayesian filter becomes more erroneous than the RE. We changed the sentence to “to have no trend to approach 1 with increasing <italic>N</italic><sub>ch</sub>”. In fact, it is exactly our conclusion to say that the Kalman filter will always use more information (see lines 467-773).</p><disp-quote content-type="editor-comment"><p>To me, an important question is when do the overall errors become dominated by external factors, such as recording artifacts, using the wrong model, etc.</p></disp-quote><p>Of course, there might be experimental situations where these problems are not the major concern. So, there might be other opportune choices for algorithms but then there is also the possibility to adapt the algorithm for those unwanted perturbations (likelihood misspecifications) of the signal. In Figures 11 and 12 we show some aspects of typical problems in realistic experimental data. One can actually see the whole article as a discussion of various aspects of likelihood misspecifications.</p><disp-quote content-type="editor-comment"><p>511: &quot;Thus, transferring the unusually strictly applied algebraic constraint Salari et al. (2018) of microscopic-reversibility to constraint with scalable softness we can model the lack information if microscopic-reversibility is exactly fulfilled Colquhoun et al. (2004) by the given ion channel instead of forcing it upon the model.&quot;</p><p>I'm not sure I understand what you mean here. I think it is very usual to constrain microscopic reversibility EXACTLY, whether through the SVD method (Qin et al., Plested et al., Milescu et al., Salari et al), or through some other algebraic method (Plested et al).</p></disp-quote><p>Thanks for the careful reading to detect this typo. It indeed changed the meaning of the sentence. We changed the sentence to “Thus, we can transfer the usually strictly applied algebraic constraint (Salari,2018) of microscopic reversibility to a constraint with scalable softness. In that way we can model the lack of information if microscopic-reversibility is exactly fulfilled (Colquhoun,2004impose) by the given ion channel instead of enforcing the strict constraint upon the model.”</p><disp-quote content-type="editor-comment"><p>It's not &quot;forcing&quot; it on the channel, but simply testing the data for that condition. Of course, one could test the condition where there is no reversibility enforced at all (i.e., it's &quot;given by the channel&quot;), or anything in between.</p></disp-quote><p>To be more clear we changed the part of the sentence to “enforcing the strict constraint”.</p><p>To the best of our knowledge, often microscopic reversibility is used to constrain the parameter space such that the inference algorithm converges to some global optimum rather than having some unidentified or marginally identified parameters (which does not mean that the constraint is actually fulfilled in reality). In this case it makes sense to model the uncertainty as a hierarchical Bayesian model because the data sets will likely be vague about the strict microscopic reversibility hypothesis. The Bayesian framework is a good way to model this uncertainty because the sampler automatically marginalizes over this hierarchical structure in the model which is induced by the uncertainty. Maximum likelihood or maximum posterior methods have to integrate this uncertainty analytically which is not always possible. If the data are ambiguous about microscopic reversibility, the posterior of the model is going to have the ambiguity of the prior and the data, too.</p><disp-quote content-type="editor-comment"><p>675: &quot;With our algorithm we demonstrate (Figure 3c and 7) that the common assumption that for large ensembles of ion channels simpler deterministic modeling by RE approaches is on par with stochastic modeling, such as a KF, is wrong in terms of Euclidean error and uncertainty quantification (Figure 5a-c and Figure 6a-b)&quot;.</p><p>I find this statement a little subjective. I think anyone who cares about stochastic vs. deterministic modeling knows enough that any method that uses more information from data should produce better estimates, regardless of the number of channels. I would say that the more likely assumption is that deterministic estimators produce poor estimates with small numbers of channels, perfect estimates with infinitely many channels, and anything in between. In fact, the KF method behaves exactly the same way, just with overall higher accuracy.</p></disp-quote><p>We think here it is matter of perspective. If one’s concern is accuracy alone than we agree. We do not doubt that the rate equation is accurate for an infinite amount of ion channels.</p><p>But for an inference algorithm we advocate that increasing accuracy is necessary but not sufficient. We meant with line 675 that <italic>“uncertainty quantification”</italic> is flawed for the RE approach independently of the amount of channels. Or at least inside the large tested ranges. This does not seem trivial to us and the summary of the Reviewer “In fact, the KF method behaves exactly the same way, just with overall higher accuracy<italic>”</italic> seems not to reflect this. Sometimes it might be opportune to not care about uncertainty quantification but for a full Bayesian analysis that does not make sense. To the best of our knowledge correct uncertainty quantification is the merit or origin of all merits of Bayesian statistics.</p><disp-quote content-type="editor-comment"><p>Looking at the tests in this manuscript, I would say that all the previous studies that modeled macroscopic data using deterministic methods are safe. They don't need to be redone. The future, of course, is a different matter.</p></disp-quote><p>We agree that previous analysis of macroscopic currents were essential for valuable insight into channel physiology. Still our results suggest that more information can be gained from the data and that possibly some previous error estimates should not be relied on too heavily. We hope that our contribution will move the field forward.</p><p>References</p><list list-type="bullet"><list-item><p>Milescu, Lorin S., Gustav Akk, and Frederick Sachs. (2005) Maximum likelihood estimation of ion channel kinetics from macroscopic currents<italic>.</italic>,</p></list-item><list-item><p>Moffatt, Luciano (2007) Estimation of ion channel kinetics from fluctuations of macroscopic currents<italic>.</italic>,</p></list-item><list-item><p>Daniel T. Gillespie (1979) Approximating the master equation by Fokker-</p></list-item></list><p>Planck-type equations for single variable chemical systems,</p><list list-type="bullet"><list-item><p>Gillespie, Daniel T The chemical Langevin equation<italic>. (2000)</italic>,</p></list-item><list-item><p>Gillespie, Daniel T. Stochastic simulation of chemical kinetics<italic>.</italic> Annu. Rev. Phys. Chem. 58 (2007): 35-55,</p></list-item><list-item><p>Tobias Jahnke·Wilhelm Huisinga (2007) Solving the chemical master equation for monomolecular reaction systems analytically,</p></list-item><list-item><p>Aleksandra M. Walczak Andrew Mugler Chris H. Wiggins Analytic methods for modeling stochastic regulatory networks (2012),</p></list-item><list-item><p>Rubin, Donald B., and Nathaniel Schenker (1986) Efficiently simulating the coverage properties of interval estimates<italic>.</italic>,</p></list-item><list-item><p>Van der Vaart, A. W. (1998) Asymptotics Statistics <italic>(1998)</italic> Cambridge series in statistical and probabilistic mathematics.”,</p></list-item><list-item><p>Martin, Ryan, and Bo Ning. Sankhya A Empirical priors and coverage of posterior credible sets in a sparse normal mean model. ,</p></list-item><list-item><p>Gardiner, Crispin W. Handbook of stochastic methods,</p></list-item><list-item><p>Wallace, E. W. J., et al. Linear noise approximation is valid over limited times for any chemical system that is sufficiently large<italic>.</italic>,</p></list-item><list-item><p>Wallace, Edward WJ. A simplified derivation of the linear noise approximation<italic>.</italic></p></list-item><list-item><p>Grima, Ramon. Linear-noise approximation and the chemical master equation agree up to second-order moments for a class of chemical systems<italic>.</italic> Physical Review E 92.4 (2015): 042124.</p></list-item><list-item><p>Munsky, Brian, Brooke Trinh, and Mustafa Khammash. Listening to the noise: random fluctuations reveal gene network parameters<italic>.</italic></p></list-item></list></body></sub-article></article>