<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">107454</article-id><article-id pub-id-type="doi">10.7554/eLife.107454</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107454.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>OpenSpliceAI provides an efficient modular implementation of SpliceAI enabling easy retraining across nonhuman species</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name><surname>Chao</surname><given-names>Kuan-Hao</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0099-0692</contrib-id><email>kh.chao@cs.jhu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name><surname>Mao</surname><given-names>Alan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2381-0607</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Anqi</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Salzberg</surname><given-names>Steven L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8859-7432</contrib-id><email>salzberg@jhu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Pertea</surname><given-names>Mihaela</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0762-8637</contrib-id><email>mpertea@jhu.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Department of Computer Science, Johns Hopkins University</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Center for Computational Biology, Johns Hopkins University</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Department of Biomedical Engineering, Johns Hopkins University</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Department of Biostatistics, Johns Hopkins University</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Koo</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02qz8b764</institution-id><institution>Cold Spring Harbor Laboratory</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Weigel</surname><given-names>Detlef</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0243gzr89</institution-id><institution>Max Planck Institute for Biology Tübingen</institution></institution-wrap><country>Germany</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>30</day><month>10</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP107454</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-05-06"><day>06</day><month>05</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-03-23"><day>23</day><month>03</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.03.20.644351"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-07-22"><day>22</day><month>07</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107454.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-09-09"><day>09</day><month>09</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107454.2"/></event></pub-history><permissions><copyright-statement>© 2025, Chao, Mao et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Chao, Mao et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-107454-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-107454-figures-v1.pdf"/><abstract><p>The SpliceAI deep learning system is currently one of the most accurate methods for identifying splicing signals directly from DNA sequences. However, its utility is limited by its reliance on older software frameworks and human-centric training data. Here, we introduce OpenSpliceAI, a trainable, open-source version of SpliceAI implemented in PyTorch to address these challenges. OpenSpliceAI supports both training from scratch and transfer learning, enabling seamless retraining on species-specific datasets and mitigating human-centric biases. Our experiments show that it achieves faster processing speeds and lower memory usage than the original SpliceAI code, allowing large-scale analyses of extensive genomic regions on a single GPU. Additionally, OpenSpliceAI’s flexible architecture makes for easier integration with established machine learning ecosystems, simplifying the development of custom splicing models for different species and applications. We demonstrate that OpenSpliceAI’s output is highly concordant with SpliceAI. In silico mutagenesis analyses confirm that both models rely on similar sequence features, and calibration experiments demonstrate similar score probability estimates.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>Splice site prediction</kwd><kwd>SpliceAI</kwd><kwd>PyTorch</kwd><kwd>Transfer learning</kwd><kwd>splice junctions</kwd><kwd>deep learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd><italic>A. thaliana</italic></kwd><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Honeybee</kwd><kwd>Zebrafish</kwd><kwd><italic>Arabidopsis thaliana</italic></kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05h1kgg64</institution-id><institution>U.S. National Institute of Health</institution></institution-wrap></funding-source><award-id>R01-HG006677</award-id><principal-award-recipient><name><surname>Chao</surname><given-names>Kuan-Hao</given-names></name><name><surname>Mao</surname><given-names>Alan</given-names></name><name><surname>Salzberg</surname><given-names>Steven L</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05h1kgg64</institution-id><institution>U.S. National Institute of Health</institution></institution-wrap></funding-source><award-id>R35-GM130151</award-id><principal-award-recipient><name><surname>Chao</surname><given-names>Kuan-Hao</given-names></name><name><surname>Mao</surname><given-names>Alan</given-names></name><name><surname>Salzberg</surname><given-names>Steven L</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05h1kgg64</institution-id><institution>U.S. National Institute of Health</institution></institution-wrap></funding-source><award-id>R35-GM156470</award-id><principal-award-recipient><name><surname>Chao</surname><given-names>Kuan-Hao</given-names></name><name><surname>Mao</surname><given-names>Alan</given-names></name><name><surname>Pertea</surname><given-names>Mihaela</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>DBI 2412449</award-id><principal-award-recipient><name><surname>Chao</surname><given-names>Kuan-Hao</given-names></name><name><surname>Mao</surname><given-names>Alan</given-names></name><name><surname>Pertea</surname><given-names>Mihaela</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/021nxhr62</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>OAC 1920103</award-id><principal-award-recipient><name><surname>Chao</surname><given-names>Kuan-Hao</given-names></name><name><surname>Mao</surname><given-names>Alan</given-names></name><name><surname>Salzberg</surname><given-names>Steven L</given-names></name><name><surname>Pertea</surname><given-names>Mihaela</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>OpenSpliceAI is an open, retrainable framework for splice site prediction that enables rapid, memory-efficient, cross-species analyses at scale with accuracy comparable to SpliceAI.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Predicting splice sites within primary DNA sequences has a wide range of uses, including understanding gene regulation, identifying alternative protein isoforms, and detecting sequence variants that affect splicing (<xref ref-type="bibr" rid="bib8">Black, 2000</xref>; <xref ref-type="bibr" rid="bib11">Braunschweig et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Wagner et al., 2023</xref>; <xref ref-type="bibr" rid="bib63">Xiong et al., 2015</xref>). Splicing is a complex and tightly regulated process that enables the production of multiple protein isoforms from a single gene, contributing to cellular complexity, adaptability, and diversity across different cells and tissues (<xref ref-type="bibr" rid="bib9">Blencowe, 2006</xref>; <xref ref-type="bibr" rid="bib27">Johnson et al., 2003</xref>; <xref ref-type="bibr" rid="bib60">Wang et al., 2008</xref>).</p><p>Aberrant splicing regulation can contribute to a wide range of diseases, including some types of cancer (<xref ref-type="bibr" rid="bib10">Bonnal et al., 2020</xref>; <xref ref-type="bibr" rid="bib28">Jung et al., 2015</xref>; <xref ref-type="bibr" rid="bib33">Lee and Abdel-Wahab, 2016</xref>; <xref ref-type="bibr" rid="bib56">Supek et al., 2014</xref>; <xref ref-type="bibr" rid="bib57">Sveen et al., 2016</xref>), neurodegenerative disorders (<xref ref-type="bibr" rid="bib35">Li et al., 2021</xref>; <xref ref-type="bibr" rid="bib44">Mills and Janitz, 2012</xref>; <xref ref-type="bibr" rid="bib48">Nikom and Zheng, 2023</xref>), cardiovascular diseases (<xref ref-type="bibr" rid="bib22">Gotthardt et al., 2023</xref>; <xref ref-type="bibr" rid="bib41">Martí-Gómez et al., 2022</xref>), metabolic syndromes (<xref ref-type="bibr" rid="bib18">Dlamini et al., 2017</xref>; <xref ref-type="bibr" rid="bib45">Moore et al., 2010</xref>), and various genetic conditions (<xref ref-type="bibr" rid="bib54">Segal and Widom, 2009</xref>; <xref ref-type="bibr" rid="bib63">Xiong et al., 2015</xref>). Notably, Duchenne muscular dystrophy (<xref ref-type="bibr" rid="bib1">Aartsma-Rus et al., 2002</xref>; <xref ref-type="bibr" rid="bib42">McClorey et al., 2005</xref>) and spinal muscular atrophy (<xref ref-type="bibr" rid="bib14">Burnett et al., 2009</xref>; <xref ref-type="bibr" rid="bib37">Lorson et al., 1999</xref>; <xref ref-type="bibr" rid="bib47">Naryshkin et al., 2014</xref>) are well-known examples of disorders arising from splicing defects. It has been estimated that 15–50% of disease-causing mutations in humans influence splice site selection (<xref ref-type="bibr" rid="bib5">Baralle and Giudice, 2017</xref>; <xref ref-type="bibr" rid="bib6">Barash et al., 2010</xref>; <xref ref-type="bibr" rid="bib59">Wang and Cooper, 2007</xref>), underscoring the critical need for precise modeling of splicing regulation at the DNA level and accurate interpretation of model predictions.</p><p>Building on advancements in deep learning, particularly convolutional neural networks (CNNs), genomics researchers have made substantial progress in modeling complex, long-range dependencies in DNA sequences. These models have driven significant improvements in predictive accuracy across diverse applications, including regulatory grammar (<xref ref-type="bibr" rid="bib4">Alipanahi et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Kelley et al., 2018</xref>; <xref ref-type="bibr" rid="bib29">Kelley et al., 2016</xref>; <xref ref-type="bibr" rid="bib67">Zhou et al., 2018</xref>; <xref ref-type="bibr" rid="bib66">Zhou and Troyanskaya, 2015</xref>), 3D genome organization (<xref ref-type="bibr" rid="bib21">Fudenberg et al., 2020</xref>), mRNA stability (<xref ref-type="bibr" rid="bib3">Agarwal and Kelley, 2022</xref>), and notably, splice site prediction (<xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>; <xref ref-type="bibr" rid="bib55">Sokolova et al., 2024</xref>). Among these, SpliceAI (<xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>) stands out as the leading tool for splice site prediction, applying a deep residual CNN architecture to identify patterns dictating splicing mechanisms directly from primary sequences without relying on human-engineered features.</p><p>Despite its success, SpliceAI has limitations that hinder its broader application. The official implementation relies on an outdated version of TensorFlow (<xref ref-type="bibr" rid="bib2">Abadi et al., 2016</xref>) and Keras, which may not function well with newer machine learning frameworks such as PyTorch (<xref ref-type="bibr" rid="bib49">Paszke et al., 2019</xref>), which has been widely adopted in recent years. Additionally, SpliceAI’s use of human training data limits its performance on nonhuman species, suggesting that a retrained module could provide substantial advantages for those wishing to use it on model organisms or other species.</p><p>To address these limitations, we developed OpenSpliceAI, a trainable open-source implementation of SpliceAI in PyTorch. OpenSpliceAI supports both training from scratch and transfer-learning approaches, making it adaptable to species-specific datasets. As we show in our experiments below, OpenSpliceAI offers faster processing speed, reduced memory usage, and efficient GPU utilization, enabling analysis of long sequences and large datasets on a single GPU. In silico mutagenesis (ISM) analyses revealed the features that both SpliceAI and OpenSpliceAI rely on for making predictions. Calibration experiments showed that OpenSpliceAI models are well calibrated, improving the reliability of splice site predictions.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>OpenSpliceAI: an open-source splice site prediction framework in PyTorch</title><p>Our new system, which we call OpenSpliceAI, is a suite of modular Python scripts that provide researchers with a user-friendly computational framework to study RNA splicing. OpenSpliceAI is an open-source version of SpliceAI, a highly accurate splice site prediction method (<xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>). By replacing TensorFlow and Keras with the more-efficient PyTorch framework, OpenSpliceAI offers improved performance, scalability, and compatibility with current machine learning workflows (see Discussion).</p><p>The framework faithfully replicates SpliceAI’s architecture while extending its functionality. It is important to note that the models produced by OpenSpliceAI and the original SpliceAI are not identical. Variations in weight initialization, data shuffling, batch normalization, and optimizer stochasticity introduce subtle differences between the models, as discussed in detail in the Discussion section. Additionally, we provide new modules for training the network, allowing for easy retraining on other species, which we show provides more accurate performance on those species. OpenSpliceAI supports custom model training on long DNA sequences and offers both training-from-scratch and transfer-learning approaches to adapt models to species-specific datasets. We also conducted experiments to analyze the effects of DNA mutations on OpenSpliceAI’s predicted scores for donor and acceptor sites and show how to use it to identify cryptic splicing events, where a mutation can activate a normally dormant splice site.</p><p>To streamline its use, OpenSpliceAI offers six subcommands for data preprocessing, model training, transfer learning, calibration, prediction, and variant analysis (see <xref ref-type="fig" rid="fig1">Figure 1</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplements 1</xref>–<xref ref-type="fig" rid="fig1s3">3</xref>). Detailed functionalities of each module are described in Methods.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Overview of the OpenSpliceAI design.</title><p>This toolkit features six primary subcommands: (<bold>A</bold>) The ‘create-data’ subcommand processes genome annotations in GFF/GTF format and genome sequences in FASTA format to produce one-hot encoded gene sequences (<bold>X</bold>) and corresponding labels (<bold>Y</bold>), both stored in HDF5 format. (<bold>B</bold>) The ‘train’ subcommand utilizes the HDF5 files generated by ‘create-data’ to train the SpliceAI model using PyTorch, resulting in a serialized model in PT format. This process also generates logs for training, testing, and validation. (<bold>C</bold>) The ‘calibrate’ subcommand takes both training and test datasets along with a pre-trained model in PT format. It randomly allocates 10% of the training data as a validation (calibration) set, which is then used to adjust the model’s output probabilities so that they more accurately reflect the observed empirical probabilities during evaluation on the test set. (<bold>D</bold>) The ‘transfer’ subcommand allows for model customization using a dataset from a different species, requiring a pre-trained model in PT format and HDF5 files for transfer learning and testing. (<bold>E</bold>) The ‘predict’ subcommand enables users to predict splice site probabilities for sequences in given FASTA files. (<bold>F</bold>) The ‘variant’ subcommand assesses the impact of potential SNPs and indels on splice sites using VCF format files, providing predicted cryptic splice sites.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Overview of the OpenSpliceAI architectures trained with different flanking sequence lengths.</title><p>(<bold>A</bold>) OpenSpliceAI-10k: schematic of the model configured for 10 kb flanking regions. The input sequence is passed through an initial convolution layer (Conv1D), followed by 16 residual units, each incorporating skip connections. The final output is fed into a softmax function for splice site classification. (<bold>B</bold>) OpenSpliceAI-2k: model variant with 2 kb flanking sequences, using a similar structure with 12 repeated residual units. (<bold>C</bold>) OpenSpliceAI-400: Model variant with 400 bp flanking sequences, using a similar structure with 8 repeated residual units. (<bold>D</bold>) OpenSpliceAI-80: the smallest variant, trained on 80 bp flanking sequences, using a similar structure with 4 repeated residual units. (<bold>E</bold>) Detailed view of the residual unit (RU) structure, highlighting the convolution, batch normalization, and skip connections. (<bold>F</bold>) Training setup: all models were trained using the AdamW optimizer, either a MultiStepLR or CosineAnnealingWarmRestarts learning rate scheduler, and cross-entropy or focal loss functions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Decision-making and workflow of the predict subcommand.</title><p>Required inputs include the FASTA file and PyTorch model, while optional inputs include a GFF annotation file and custom parameters. The outputs are two BED files corresponding to predicted donor and acceptor splice sites. Several intermediate files may be generated and useful to the user, including the HDF5-compressed data file (raw sequences) and dataset (encoded inputs) for training.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Decision-making and workflow of the variant subcommand.</title><p>Required inputs include the VCF file, FASTA file, and annotation file, while optional inputs include the output path, splicing model, and distance, precision, and masking parameters. The output is a VCF file with the OpenSpliceAI delta scores of the variants. Note that both Keras and PyTorch models are supported in this tool, but PyTorch is recommended.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig1-figsupp3-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Training OpenSpliceAI with human MANE annotation</title><p>Using the OpenSpliceAI framework, we trained a new PyTorch version of SpliceAI using protein-coding genes annotated in the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">RefSeq MANE v1.3 database</ext-link>. MANE provides a standardized set of human gene annotations covering nearly all known protein-coding genes, with one transcript per gene, and ensures that the transcripts are represented identically in the RefSeq and Ensembl/GENCODE annotations of the GRCh38 human reference genome (<xref ref-type="bibr" rid="bib46">Morales et al., 2022</xref>).</p><p>Gene sequences and splice site labels from MANE annotations were extracted and one-hot encoded into tensors for OpenSpliceAI training. Models were trained using flanking sequences of 80, 400, 2000, and 10,000 nucleotides, with five models trained for each sequence length (see <xref ref-type="fig" rid="fig2">Figure 2A</xref>). OpenSpliceAI assigns a score to each position that is an estimate of the probability that the position is a donor site, acceptor site, or neither (see <xref ref-type="fig" rid="fig2">Figure 2B</xref> and Methods). In this setup, ‘OpenSpliceAI’ refers to the reimplemented framework, ‘OSAI’ refers to the model, and ‘OSAI<sub>MANE</sub>’ denotes the model trained specifically with the MANE annotation. ‘SpliceAI-Keras’ denotes the original published SpliceAI model, which was trained using the canonical transcripts from GENCODE version V24lift37 from the hg19/GRCh37 reference genome.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Overview of OpenSpliceAI framework, performance benchmarking, and comparison with SpliceAI-Keras.</title><p>(<bold>A</bold>) Schematic overview of OpenSpliceAI’s approach. Gene sequences are first extracted from the genome FASTA file and one-hot encoded (<italic>X</italic>). Splice sites are identified and labeled using the annotation file (<italic>Y</italic>). The resulting paired data (X, <italic>Y</italic>) for each gene is then compiled for model training (80% of the sequences) and testing (20% of the sequences). (<bold>B</bold>) Workflow of the OSAI<sub>MANE</sub> 10,000 model. Input sequences are one-hot encoded and padded with 5000 Ns ([0,0,0,0]) on each side, totaling 10,000 Ns. The model processes the input and outputs, for each position, the probability of that position being a donor site, an acceptor site, or neither. (<bold>C–D</bold>) Performance comparison between OSAI<sub>MANE</sub> and SpliceAI-Keras on splicing donor and acceptor sites, trained with 80 nt, 400 nt, 2000 nt, and 10,000 nt flanking sequences. Evaluation metrics include top-1 accuracy for both donor and acceptor sites. Blue curves represent SpliceAI-Keras, while orange curves represent OSAI<sub>MANE</sub>. Each dot is the mean over five trained model variants, and error bars show ±1 standard error of the mean. Performance is compared across test datasets from humans. (<bold>E</bold>) Benchmarking results for elapsed time, average memory usage, and GPU peak memory for the prediction submodule<italic>.</italic></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Comparison of splice site prediction performance between SpliceAI-Keras (blue) and OSAI<sub>MANE</sub> (orange) across human (<italic>Homo sapiens</italic>) datasets with varying flanking sequence lengths.</title><p>The plots display donor (5′) and acceptor (3′) splice site prediction metrics using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>MANE</sub> was trained using the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">RefSeq MANE v1.3 database</ext-link> and GRCh38.p14 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Comparison of splice site prediction performance between SpliceAI-Keras (blue) and OSAI<sub>MANE</sub> (orange) across house mouse (<italic>Mus musculus</italic>) datasets with varying flanking sequence lengths.</title><p>The plots display donor (5′) and acceptor (3′) splice site prediction metrics using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>MANE</sub> was trained using the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">RefSeq MANE v1.3 database</ext-link> and GRCh38.p14 genome. The house mouse datasets are curated from <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.27_GRCm39/GCF_000001635.27_GRCm39_genomic.gff.gz">RefSeq GRCm39 annotation</ext-link> and GRCm39 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Comparison of splice site prediction performance between SpliceAI-Keras (blue) and OSAI<sub>MANE</sub> (orange) across honeybee (<italic>Apis mellifera</italic>) datasets with varying flanking sequence lengths.</title><p>The plots display donor (5′) and acceptor (3′) splice site prediction metrics using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>MANE</sub> was trained using the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">RefSeq MANE v1.3 database</ext-link> and GRCh38.p14 genome. The honeybee datasets are curated from <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/003/254/395/GCF_003254395.2_Amel_HAv3.1/GCF_003254395.2_Amel_HAv3.1_genomic.gff.gz">RefSeq Amel_HAv3.1 annotation</ext-link> and Amel_HAv3.1 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Comparison of splice site prediction performance between SpliceAI-Keras (blue) and OSAI<sub>MANE</sub> (orange) across zebrafish (<italic>Danio rerio</italic>) datasets with varying flanking sequence lengths.</title><p>The plots display donor (5′) and acceptor (3′) splice site prediction metrics using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>MANE</sub> was trained using the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">RefSeq MANE v1.3 database</ext-link> and GRCh38.p14 genome. The zebrafish datasets are curated from <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/035/GCF_000002035.6_GRCz11/GCF_000002035.6_GRCz11_genomic.gff.gz">RefSeq GRCz11 annotation</ext-link> and GRCz11 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig2-figsupp4-v1.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Comparison of splice site prediction performance between SpliceAI-Keras (blue) and OSAI<sub>MANE</sub> (orange) across <italic>Arabidopsis thaliana</italic> datasets with varying flanking sequence lengths.</title><p>The plots display donor (5′) and acceptor (3′) splice site prediction metrics using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>MANE</sub> was trained using the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">RefSeq MANE v1.3 database</ext-link> and GRCh38.p14 genome. The <italic>A. thaliana</italic> datasets are curated from <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.gff.gz">RefSeq TAIR10.1 annotation</ext-link> and TAIR10.1 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig2-figsupp5-v1.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Comparison of runtime and memory metrics for ‘predict’ (panels A–F) and ‘variant’ (panels G–L) in OSAI<sub>MANE</sub> models with different flanking sequences.</title><p>Each corresponding pair of panels displays the same metric for the two methods as a function of increasing input size. (<bold>A, G</bold>) Overall elapsed time: total elapsed CPU time to complete processing. (<bold>B, H</bold>) Average memory usage: mean CPU memory consumption (in MB) during execution, reflecting the typical memory footprint. (<bold>C, I</bold>) Peak memory usage: maximum CPU memory recorded (in MB) at any point. (<bold>D, J</bold>) Peak GPU memory: maximum GPU memory recorded (in MB) at any point. (<bold>E, K</bold>) Memory growth rate: the average rate of memory increase during runtime, which indicates how the constant of memory usage increases with larger inputs. (<bold>F, L</bold>) CPU utilization profile: percentage of time spent in native C execution (as opposed to interpreted Python code), reflecting the runtime that is being used by compiled, low-level routines.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig2-figsupp6-v1.tif"/></fig></fig-group><p>To compare the performance of OSAI<sub>MANE</sub> and SpliceAI-Keras, both models were evaluated on a held-out test set comprising genes from MANE annotations on GRCh38 chromosomes 1, 3, 5, 7, and 9. Paralogous genes were excluded to prevent data leakage by aligning the test set against the training sets of both models (Methods). OSAI<sub>MANE</sub> (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref>, orange curve) showed performance comparable to SpliceAI-Keras (<xref ref-type="fig" rid="fig2">Figure 2C and D</xref>, blue curve) across metrics, including top-1 accuracy, area under the precision-recall curve (AUPRC), precision, recall, and F1 score for donor and acceptor splice sites (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, Methods). Results are presented as error bar plots showing the mean and ±1 standard error for each metric.</p><p>The best-performing OSAI<sub>MANE</sub> model, trained with 10,000 nt flanking sequences, demonstrated slight yet consistent improvements over SpliceAI-Keras across all metrics (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Specifically, it achieved a top-1 accuracy increase of 1.25% for donor sites and 1.56% for acceptor sites, an F1 score gain of 1.20% for donor sites and 1.04% for acceptor sites, and an AUPRC improvement of 1.90% for donor sites and 1.76% for acceptor sites.</p><p>Performance improved with longer flanking sequences, consistent with previous SpliceAI findings. The largest gains occurred between 80 nt and 400 nt, with accuracy increasing by 62% for donor sites and 74% for acceptor sites. In comparison, improvements were smaller between 400 nt and 2000 nt (3.2% for donors and 2.5% for acceptors) and between 2000 nt and 10,000 nt (3.8% for donors and 4.2% for acceptors). Cross-species evaluations with mouse (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), honeybee (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), zebrafish (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>), and <italic>Arabidopsis</italic> (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>) test sets confirmed comparable performance across species.</p><p>OpenSpliceAI supports both targeted splice site prediction and genome-wide predictions across full chromosomes. Its ‘variant’ submodule enables researchers to assess the splicing impacts of specific variants, such as acceptor and donor site gains or losses, using pre-trained models. Our performance benchmarks of the ‘predict’ (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6A–F</xref>) and ‘variant’ (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6G–L</xref>) submodules demonstrate that OSAI outperforms SpliceAI in processing speed, memory usage, and GPU efficiency (see Discussion).</p></sec><sec id="s2-3"><title>Retraining models with different species using OpenSpliceAI framework</title><p>To assess whether SpliceAI can generalize across different species and to demonstrate the ease of retraining models with OpenSpliceAI, we selected four model organisms representing diverse taxa: a mammal (mouse, <italic>M. musculus</italic>), an insect (honeybee, <italic>A. mellifera</italic>), a freshwater fish (zebrafish, <italic>D. rerio</italic>), and a flowering plant (thale cress, <italic>A. thaliana</italic>). Using OpenSpliceAI with the same training hyperparameters as the human model, we trained species-specific models that we designated OSAI<sub>Mouse</sub>, OSAI<sub>Zebrafish</sub>, OSAI<sub>Honeybee</sub>, and OSAI<sub>Arabidopsis</sub> (see <xref ref-type="table" rid="table1">Table 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Genome assembly and annotation details for species used for OpenSpliceAI training and transfer learning in this study.</title><p>Note: For each species, the table includes the GenBank accession number, assembly name, ftp sites for assembly and annotation downloads, and annotation release dates.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Species</th><th align="left" valign="bottom">Name</th><th align="left" valign="bottom">Genbank accession</th><th align="left" valign="bottom">Download link</th><th align="left" valign="bottom">Annotation release date</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>H. sapiens</italic></td><td align="left" valign="bottom">GRCh38.p14</td><td align="left" valign="bottom">GCA_000001405.29</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/annotation_releases/9606/GCF_000001405.40-RS_2023_03/">https://ftp.ncbi.nlm.nih.gov/genomes/all/annotation_releases/9606/GCF_000001405.40-RS_2023_03/</ext-link></td><td align="left" valign="bottom">March 21, 2023</td></tr><tr><td align="left" valign="bottom"><italic>M. musculus</italic></td><td align="left" valign="bottom">GRCm39</td><td align="left" valign="bottom">GCA_000001635.9</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.27_GRCm39/">https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.27_GRCm39/</ext-link></td><td align="left" valign="bottom">February 8, 2024</td></tr><tr><td align="left" valign="bottom"><italic>A. mellifera</italic></td><td align="left" valign="bottom">Amel_HAv3.1</td><td align="left" valign="bottom">GCA_003254395.2</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/003/254/395/GCF_003254395.2_Amel_HAv3.1/">https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/003/254/395/GCF_003254395.2_Amel_HAv3.1/</ext-link></td><td align="left" valign="bottom">September 30, 2022</td></tr><tr><td align="left" valign="bottom"><italic>A. thaliana</italic></td><td align="left" valign="bottom">TAIR10.1</td><td align="left" valign="bottom">GCA_000001735.2</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/">https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/</ext-link></td><td align="left" valign="bottom">June 16, 2023</td></tr><tr><td align="left" valign="bottom"><italic>D. rerio</italic></td><td align="left" valign="bottom">GRCz11</td><td align="left" valign="bottom">GCA_000002035.4</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/035/GCF_000002035.6_GRCz11/">https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/035/GCF_000002035.6_GRCz11/</ext-link></td><td align="left" valign="bottom">August 15, 2024</td></tr></tbody></table></table-wrap><p>Training and test sets for each species were generated using the ‘create-data’ submodule (Methods). Due to differences in genome sizes across species, we report the number of protein-coding genes used for training and testing in each model (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), along with statistics on the ratio of canonical to noncanonical splice sites (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>) and intron length distributions (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>) for human MANE and the four other selected species. To ensure that the test sets did not contain paralogs of the training sets, OpenSpliceAI aligns them using minimap2 (<xref ref-type="bibr" rid="bib34">Li, 2018</xref>) and excludes test sequences with over 80% similarity and coverage to the training sequences and enforces a 20% sequence diversity threshold (see Methods). In its original paper, SpliceAI was evaluated on a test set containing genes from human chromosomes 1, 3, 5, 7, and 9, which the Ensembl database (<ext-link ext-link-type="uri" xlink:href="http://grch37.ensembl.org/biomart/martview">http://grch37.ensembl.org/biomart/martview</ext-link>) classifies as free of paralogs. However, applying our paralog removal criteria, we found that 0.71% of the MANE transcripts from these chromosomes were paralogous to training set sequences. In other species, the proportion of removed paralogous sequences was 3.86% for mouse, 31.97% for zebrafish, 0.08% for honeybee, and 2.26% for <italic>Arabidopsis</italic> (<xref ref-type="fig" rid="fig3">Figure 3B</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Cross-species dataset composition, sequence filtering, and performance comparison of OpenSpliceAI with SpliceAI-Keras.</title><p>(<bold>A</bold>) The number of protein-coding genes in the training and test sets, along with the count of paralogous genes removed for each species: Human MANE, mouse, zebrafish, honeybee, and <italic>Arabidopsis</italic>. (<bold>B</bold>) Scatter plots of DNA sequence alignments between testing and training sets for human MANE, mouse, honeybee, zebrafish, and <italic>Arabidopsis</italic>. Each dot represents an alignment, with the <italic>x</italic>-axis showing alignment identity and the <italic>y</italic>-axis showing alignment coverage. Alignments exceeding 80% for both identity and coverage are highlighted in the red-shaded region and excluded from the test sets. (<bold>C–F</bold>) Performance comparisons of OSAIs trained on species-specific datasets (mouse, zebrafish, honeybee, and <italic>Arabidopsis</italic>) vs. SpliceAI-Keras, original published SpliceAI models, trained on human data. The orange curves represent OSAI metrics, while the blue curves show SpliceAI-Keras metrics. Each subplot (<bold>C–F</bold>) includes F1 score evaluated separately for donor and acceptor sites. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Splice site motif count and intron length distributions across five species.</title><p>(<bold>A</bold>) Canonical vs. non-canonical donor and acceptor splice sites. Bar plots depict the total number of donor (blue, orange) and acceptor (green, red) sites across human (MANE), mouse, zebrafish, honeybee, and <italic>Arabidopsis</italic> genomes, subdivided into canonical (GT/AG) and noncanonical motifs. Percentages above each bar indicate the proportion of sites using canonical motifs in each species. (<bold>B</bold>) Splice junction (intron) length distributions. Kernel density curves illustrate the distribution of intron lengths in each genome (colored as in the legend). Differences in the breadth and peak of each distribution highlight notable cross-species variation in intron size. (<bold>C</bold>) Scatter plots of DNA sequence alignments between validation and training sets for human MANE, mouse, honeybee, zebrafish, and <italic>Arabidopsis</italic>. Each dot represents an alignment, with the <italic>x</italic>-axis showing alignment identity and the <italic>y</italic>-axis showing alignment coverage. Alignments exceeding 80% for both identity and coverage are highlighted in the red-shaded region and excluded from the test sets.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Splice site prediction metrics for the mouse (<italic>M. musculus</italic>) across varying flanking sequence lengths.</title><p>Plots compare performance of SpliceAI-Keras (blue) and OSAI<sub>Mouse</sub> (orange) on donor (5′) and acceptor (3′) splice site predictions using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>Mouse</sub> was trained with <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/635/GCF_000001635.27_GRCm39/GCF_000001635.27_GRCm39_genomic.gff.gz">RefSeq GRCm39 annotation</ext-link> and GRCm39 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Splice site prediction metrics for the honeybee (<italic>A. mellifera</italic>) across varying flanking sequence lengths.</title><p>Plots compare performance of SpliceAI-Keras (blue) and OSAI<sub>Honeybee</sub> (orange) on donor (5′) and acceptor (3′) splice site predictions using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>Honeybee</sub> was trained with <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/003/254/395/GCF_003254395.2_Amel_HAv3.1/GCF_003254395.2_Amel_HAv3.1_genomic.gff.gz">RefSeq Amel_HAv3.1 annotation</ext-link> and Amel_HAv3.1 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Splice site prediction metrics for the zebrafish (<italic>D. rerio</italic>) across varying flanking sequence lengths.</title><p>Plots compare performance of SpliceAI-Keras (blue) and OSAI<sub>Zebrafish</sub> (orange) on donor (5′) and acceptor (3′) splice site predictions using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>Zebrafish</sub> was trained with <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/035/GCF_000002035.6_GRCz11/GCF_000002035.6_GRCz11_genomic.gff.gz">RefSeq GRCz11 annotation</ext-link> and GRCz11 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Splice site prediction metrics for the <italic>A. thaliana</italic> across varying flanking sequence lengths.</title><p>Plots compare performance of SpliceAI-Keras (blue) and OSAI<sub>Arabidopsis</sub> (orange) on donor (5′) and acceptor (3′) splice site predictions using 80, 400, 2000, and 10,000 nt of flanking context. OSAI<sub>Arabidopsis</sub> was trained with <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/735/GCF_000001735.4_TAIR10.1/GCF_000001735.4_TAIR10.1_genomic.gff.gz">RefSeq TAIR10.1 annotation</ext-link> and TAIR10.1 genome. (<bold>A</bold>) Donor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true donor site label. (<bold>B</bold>) Donor AUPRC: area under the precision-recall curve for donor site predictions. (<bold>C</bold>) Donor accuracy: proportion of correct donor site calls among all predictions. (<bold>D</bold>) Donor precision: the fraction of predicted donor sites that are correct. (<bold>E</bold>) Donor recall: the fraction of true donor sites that are correctly predicted. (<bold>F</bold>) Donor F1: the harmonic mean of precision and recall for donor sites. (<bold>G</bold>) Acceptor top-1: measures the percentage of times the model’s most confident prediction exactly matches the true acceptor site label. (<bold>H</bold>) Acceptor AUPRC: area under the precision-recall curve for acceptor site predictions. (<bold>I</bold>) Acceptor accuracy: proportion of correct acceptor site calls among all predictions. (<bold>J</bold>) Acceptor precision: the fraction of predicted acceptor sites that are correct. (<bold>K</bold>) Acceptor recall: the fraction of true acceptor sites that are correctly predicted. (<bold>L</bold>) Acceptor F1: the harmonic mean of precision and recall for acceptor sites. Each panel illustrates that increasing the flanking sequence length generally enhances model performance, with both SpliceAI-Keras and OpenSpliceAI achieving high accuracy and F1 scores at 10,000 nt. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig3-figsupp5-v1.tif"/></fig></fig-group><p>As we did with OSAI<sub>MANE</sub>, we retrained each species model five times using different random seeds (10–14) and evaluated performance based on top-1 accuracy, F1 scores, precision, recall, and AUPRC for donor and acceptor sites. The results for OSAI<sub>Mouse</sub>, OSAI<sub>Zebrafish</sub>, OSAI<sub>Honeybee</sub>, and OSAI<sub>Arabidopsis</sub> are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplements 2</xref>–<xref ref-type="fig" rid="fig3s5">5</xref>. We calculated the average percentage improvement across all flanking sequence sizes for donor and acceptor sites under four flanking sequence lengths. On average, OSAI outperformed SpliceAI-Keras by 2% in mouse, 54% in honeybee, 19% in zebrafish, and 57% in <italic>Arabidopsis</italic>.</p><p>The human and mouse genomes share a majority of their protein-coding genes (<xref ref-type="bibr" rid="bib62">Waterston et al., 2002</xref>). This conserved evolutionary relationship likely explains the comparable performance of OSAI<sub>Mouse</sub> and SpliceAI-Keras. In contrast, OSAIs that had been retrained substantially outperformed SpliceAI-Keras in more distantly related species, particularly in honeybee, zebrafish, and <italic>Arabidopsis</italic> (see Discussion).</p></sec><sec id="s2-4"><title>Adapting OSAI<sub>MANE</sub> to different species via transfer learning</title><p>Transfer learning can improve model performance by leveraging knowledge from related source domains (<xref ref-type="bibr" rid="bib68">Zhuang et al., 2021</xref>). In the context of splice site prediction, we tested whether OSAI<sub>MANE</sub>, initially trained on human splice annotations, could be effectively adapted to predict splice sites across other species.</p><p>We evaluated four species – <italic>M. musculus</italic>, <italic>A. mellifera</italic>, <italic>D. rerio</italic>, and <italic>A. thaliana</italic> – by fine-tuning five distinct pretrained OSAI<sub>MANE</sub> models for each species. For every species, each pretrained model was fine-tuned using the same training and test datasets, yielding five transfer-trained variants. These variants – collectively referred to as OSAI<sub>Mouse</sub>-transfer, OSAI<sub>Honeybee</sub>-transfer, OSAI<sub>Zebrafish</sub>-transfer, and OSAI<sub>Arabidopsis</sub>-transfer – were directly compared with models trained from scratch (OSAI<sub>Mouse</sub>, OSAI<sub>Honeybee</sub>, OSAI<sub>Zebrafish</sub>, and OSAI<sub>Arabidopsis</sub>) to assess the benefits of transfer learning. For each species, we trained transfer-learned models using flanking sequences of 80 nt, 400 nt, 2000 nt, and 10,000 nt. We then compared the performance of transfer-trained and scratch-trained models by evaluating top-1 accuracy for donor and acceptor splice site predictions across four lengths of flanking sequences: 80, 400, 2000, and 10,000 nt (<xref ref-type="fig" rid="fig4">Figure 4A–D</xref>). Full results, including top-1 accuracy, F1 score, and AUPRC for donor and acceptor splice sites are provided in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s5">5</xref>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Performance comparison of scratch-trained and transfer-trained OSAIs across species and sequence lengths.</title><p>(<bold>A–D</bold>) Top-1 accuracy for donor and acceptor splice sites of 80 nt, 400 nt, 2000 nt, and 10,000 nt models, comparing OSAI<sub>Mouse</sub> (scratch-trained) and OSAI<sub>Mouse</sub>-transferred (transfer-trained) models over epochs 1–10 on the test dataset. (<bold>E–H</bold>) Top-1 accuracy after one epoch of training vs. after 10 epochs for both scratch-trained and transfer-trained models across the same sequence lengths. Each plot represents one species and its corresponding transfer-trained model: (<bold>E</bold>) OSAI<sub>Mouse</sub> vs. OSAI<sub>Mouse</sub>-transferred, (<bold>F</bold>) OSAI<sub>Zebrafish</sub> vs. OSAI<sub>Zebrafish</sub>-transferred, (<bold>G</bold>) OSAI<sub>Arabidopsis</sub> vs. OSAI<sub>Arabidopsis</sub>-transferred, and (<bold>H</bold>) OSAI<sub>Honeybee</sub> vs. OSAI<sub>Honeybee</sub>-transferred. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Transfer learning from OSAI<sub>MANE</sub> was leveraged to evaluate performance metrics for splice site prediction in mouse (<italic>M. musculus</italic>) across four flanking sequence lengths (80, 400, 2000, and 10,000 nt) and two splice site types (acceptor and donor).</title><p>Each row corresponds to a unique flanking length-splice site combination, and each column depicts a distinct evaluation metric (top-1 accuracy, F1, and AUPRC). Blue and red curves represent the mean performance (± standard deviation) of five models trained from scratch and five models fine-tuned from OSAI<sub>MANE</sub>, respectively. The <italic>x</italic>-axis indicates the training epoch, and the <italic>y</italic>-axis denotes the corresponding metric value. Subplot titles specify the flanking length, splice site type, and metric. Overall, fine-tuned models converge more rapidly, reaching stable performance as early as the first epoch, whereas models trained from scratch require additional epochs to stabilize.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Transfer learning from OSAI<sub>MANE</sub> was leveraged to evaluate performance metrics for splice site prediction in honeybee (<italic>A. mellifera</italic>) across four flanking sequence lengths (80, 400, 2000, and 10,000 nt) and two splice site types (acceptor and donor).</title><p>Each row corresponds to a unique flanking length-splice site combination, and each column depicts a distinct evaluation metric (top-1 accuracy, F1, and AUPRC). Blue and red curves represent the mean performance (<italic>±</italic> standard deviation) of five models trained from scratch and five models fine-tuned from OSAI<sub>MANE</sub>, respectively. The <italic>x</italic>-axis indicates the training epoch, and the <italic>y</italic>-axis denotes the corresponding metric value. Subplot titles specify the flanking length, splice site type, and metric. Overall, fine-tuned models converge more rapidly, reaching stable performance as early as the first epoch, whereas models trained from scratch require additional epochs to stabilize.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Transfer learning from OSAI<sub>MANE</sub> was leveraged to evaluate performance metrics for splice site prediction in zebrafish (<italic>D. rerio</italic>) across four flanking sequence lengths (80, 400, 2000, and 10,000 nt) and two splice site types (acceptor and donor).</title><p>Each row corresponds to a unique flanking length-splice site combination, and each column depicts a distinct evaluation metric (top-1 accuracy, F1, and AUPRC). Blue and red curves represent the mean performance (<italic>±</italic> standard deviation) of five models trained from scratch and five models fine-tuned from OSAI<sub>MANE</sub>, respectively. The <italic>x</italic>-axis indicates the training epoch, and the <italic>y</italic>-axis denotes the corresponding metric value. Subplot titles specify the flanking length, splice site type, and metric. Overall, fine-tuned models converge more rapidly, reaching stable performance as early as the first epoch, whereas models trained from scratch require additional epochs to stabilize.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Transfer learning from OSAI<sub>MANE</sub> was leveraged to evaluate performance metrics for splice site prediction in <italic>A. thaliana</italic> across four flanking sequence lengths (80, 400, 2000, and 10,000 nt) and two splice site types (acceptor and donor).</title><p>Each row corresponds to a unique flanking length-splice site combination, and each column depicts a distinct evaluation metric (top-1 accuracy, F1, and AUPRC). Blue and red curves represent the mean performance (<italic>±</italic> standard deviation) of five models trained from scratch and five models fine-tuned from OSAI<sub>MANE</sub>, respectively. The <italic>x</italic>-axis indicates the training epoch, and the <italic>y</italic>-axis denotes the corresponding metric value. Subplot titles specify the flanking length, splice site type, and metric. Overall, fine-tuned models converge more rapidly, reaching stable performance as early as the first epoch, whereas models trained from scratch require additional epochs to stabilize.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig4-figsupp4-v1.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Cross‐species transfer learning performance on human splice‐site prediction.</title><p>Models pre-trained on (<bold>A</bold>) <italic>A. thaliana</italic> (OSAI<sub>Arabidopsis</sub>), (<bold>B</bold>) <italic>A. mellifera</italic> (OSAI<sub>Honeybee</sub>), (<bold>C</bold>) <italic>M. musculus</italic> (OSAI<sub>Mouse</sub>), and (<bold>D</bold>) <italic>D. rerio</italic> (OSAI<sub>Zebrafish</sub>) were fine‐tuned on the human MANE dataset (orange) and compared to a model trained from scratch on MANE (OSAI<sub>MANE</sub>, blue). Within each panel, the top row shows donor top-K (left) and donor F1 (right) scores, and the bottom row shows acceptor top-K (left) and acceptor F1 (right) scores. The <italic>x</italic>-axis indicates models trained with flanking sequence sizes of 80, 400, 2000, and 10,000 bp. Each data point represents the mean across five independently trained models, with error bars indicating the standard error.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig4-figsupp5-v1.tif"/></fig></fig-group><p>Across all configurations, transfer-trained models consistently outperformed scratch-trained models in both accuracy and training stability, as evidenced by higher top-1 accuracies and lower standard errors across the five pretrained models. Notably, transfer-trained models achieved near-optimal performance after just one epoch, while scratch-trained models required 10 epochs to reach comparable results and showed substantial performance gaps between one and ten epochs (<xref ref-type="fig" rid="fig4">Figure 4E–H</xref>). Moreover, transfer learning also solved a convergence issue for one dataset: in the <italic>A. thaliana</italic> scratch-training experiments using 10k flanking sequences, the CosineAnnealingWarmRestarts scheduler led to unstable optimization. Although switching to MultiStepLR with learning rate decay improved stability, reaching convergence was still a lengthy process. Notably, transfer learning did not display this problem (see the Methods section for training parameters).</p><p>After 10 epochs, transfer-trained models slightly outperformed their scratch-trained counterparts for <italic>Arabidopsis</italic> and honeybee, the two species with the smallest genome sizes among those tested. These results suggest that pre-training improves generalization, particularly for compact genomes.</p></sec><sec id="s2-5"><title>Calibrating OpenSpliceAI models</title><p>Model calibration helps align predicted probabilities with the true likelihood of observed outcomes, thereby mitigating the risk of overconfident or underconfident predictions. Here, we applied class-wise temperature scaling, a single-parameter variant of Platt scaling, to adjust each class’s predicted probabilities without altering the model’s classification performance (see Methods). We calibrated OSAI<sub>MANE</sub> models on the validation set and subsequently evaluated them on the test set.</p><p>We then compared OSAI<sub>MANE</sub> models before and after calibration using reliability diagrams (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>), which show reliability curves for non-splice, acceptor, and donor sites of OSAI<sub>MANE</sub> trained with flanking sequence lengths of 80, 400, 2000, and 10,000 nt, with the calibration temperature (<inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$Ts$\end{document}</tex-math></alternatives></inline-formula>) in the legend. Calibration quality was quantified using negative log-likelihood (NLL) loss and expected calibration error (ECE). For each species, metrics were averaged over five calibrated models, and the results indicated slight improvements in both measures following calibration (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplements 2</xref> and <xref ref-type="fig" rid="fig5s3">3</xref>; see Methods). Temperature parameters greater than one indicated overconfidence, whereas values below one indicated underconfidence. After calibration, score distributions for donor and acceptor sites shifted slightly away from extreme values (1 and 0), resulting in smoother probability distributions (<xref ref-type="fig" rid="fig5">Figure 5C</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Calibration of OSAI<sub>MANE</sub> predictions across splice-site classes and flanking sequence lengths.</title><p>(<bold>A</bold>) Calibration results for OSAI<sub>MANE</sub> across non-splice sites, acceptor sites, and donor sites. Models trained with different flanking sequence lengths are represented by color: 80 nt (blue), 400 nt (green), 2000 nt (orange), and 10,000 nt (red). Dotted curves in lighter colors denote pre-calibration results, while solid curves in darker shades show post-calibration results. (<bold>B</bold>) Expected calibration error (ECE) on the validation set (top) and test set (bottom), comparing the OSAI<sub>MANE</sub>’s performance before (blue bars) and after (orange bars) calibration. For each flanking sequence OSAI<sub>MANE</sub>, five calibration experiments were performed, with the mean loss and <italic>±</italic>1 standard error. (<bold>C</bold>) Two-dimensional calibration map for OSAI<sub>MANE</sub>, illustrating how raw predicted probabilities for acceptor (<italic>x</italic>-axis) and donor (<italic>y</italic>-axis) sites are transformed after calibration. Arrows indicate the shift from pre- to post-calibration states in two-dimensional probability space, resulting in a smoother probability distribution.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Calibration results for human MANE splice site classification at four flanking sequence sizes.</title><p>(<bold>A, C, E, G</bold>) Reliability (calibration) curves for flanking sequence sizes of 80, 400, 2000, and 10,000 nucleotides, respectively. Each plot compares predicted probabilities (<italic>x</italic>-axis) to empirical probabilities (<italic>y</italic>-axis) for non-splice sites (left), acceptor sites (middle), and donor sites (right). The blue curves depict the reliability of the original OSAI<sub>MANE</sub> models, while the green curves show reliability after calibration. Shaded regions represent confidence intervals. The diagonal black line indicates perfect calibration, where predicted probabilities match observed frequencies exactly. (<bold>B, D, F, H</bold>) Temperature scaling maps for each corresponding flanking sequence size, illustrating how raw predicted probabilities for acceptor (<italic>x</italic>-axis) and donor (<italic>y</italic>-axis) sites are transformed after calibration. Arrows indicate the shift from pre- to post-calibration states in two-dimensional probability space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Expected calibration error (ECE) on the validation (top) and test (bottom) sets.</title><p>Blue bars indicate model performance before calibration, and orange bars indicate performance after calibration. Results are shown for (<bold>A</bold>) human MANE, (<bold>B</bold>) mouse, (<bold>C</bold>) honeybee, (<bold>D</bold>) <italic>Arabidopsis</italic>, and (<bold>E</bold>) zebrafish.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-figsupp2-v1.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Negative log likelihood (NLL) loss on the validation (top) and test (bottom) sets.</title><p>Blue bars indicate model performance before calibration, and orange bars indicate performance after calibration. Results are shown for (<bold>A</bold>) human MANE, (<bold>B</bold>) mouse, (<bold>C</bold>) honeybee, (<bold>D</bold>) <italic>Arabidopsis</italic>, and (<bold>E</bold>) zebrafish.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-figsupp3-v1.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Calibration results for house mouse (<italic>M. musculus</italic>) splice site classification at four flanking sequence sizes.</title><p>(<bold>A, C, E, G</bold>) Reliability (calibration) curves for flanking sequence sizes of 80, 400, 2000, and 10,000 nt, respectively. Each plot compares predicted probabilities (<italic>x</italic>-axis) to empirical probabilities (<italic>y</italic>-axis) for non-splice sites (left), acceptor sites (middle), and donor sites (right). The blue curves depict the reliability of the original OSAI<sub>Mouse</sub> models, while the green curves show reliability after calibration. Shaded regions represent confidence intervals. The diagonal black line indicates perfect calibration, where predicted probabilities match observed frequencies exactly. (<bold>B, D, F, H</bold>) Temperature scaling maps for each corresponding flanking sequence size, illustrating how raw predicted probabilities for acceptor (<italic>x</italic>-axis) and donor (<italic>y</italic>-axis) sites are transformed after calibration. Arrows indicate the shift from pre- to post-calibration states in two-dimensional probability space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-figsupp4-v1.tif"/></fig><fig id="fig5s5" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 5.</label><caption><title>Calibration results for zebrafish (<italic>D. rerio</italic>) splice site classification at four flanking sequence sizes.</title><p>(<bold>A, C, E, G</bold>) Reliability (calibration) curves for flanking sequence sizes of 80, 400, 2000, and 10,000 nt, respectively. Each plot compares predicted probabilities (<italic>x</italic>-axis) to empirical probabilities (<italic>y</italic>-axis) for non-splice sites (left), acceptor sites (middle), and donor sites (right). The blue curves depict the reliability of the original OSAI<sub>Zebrafish</sub> models, while the green curves show reliability after calibration. Shaded regions represent confidence intervals. The diagonal black line indicates perfect calibration, where predicted probabilities match observed frequencies exactly. (<bold>B, D, F, H</bold>) Temperature scaling maps for each corresponding flanking sequence size, illustrating how raw predicted probabilities for acceptor (<italic>x</italic>-axis) and donor (<italic>y</italic>-axis) sites are transformed after calibration. Arrows indicate the shift from pre- to post-calibration states in two-dimensional probability space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-figsupp5-v1.tif"/></fig><fig id="fig5s6" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 6.</label><caption><title>Calibration results for honeybee (<italic>A. mellifera</italic>) splice site classification at four flanking sequence sizes.</title><p>(<bold>A, C, E, G</bold>) Reliability (calibration) curves for flanking sequence sizes of 80, 400, 2000, and 10,000 nt, respectively. Each plot compares predicted probabilities (<italic>x</italic>-axis) to empirical probabilities (<italic>y</italic>-axis) for non-splice sites (left), acceptor sites (middle), and donor sites (right). The blue curves depict the reliability of the original OSAI<sub>Honeybee</sub> models, while the green curves show reliability after calibration. Shaded regions represent confidence intervals. The diagonal black line indicates perfect calibration, where predicted probabilities match observed frequencies exactly. (<bold>B, D, F, H</bold>) Temperature scaling maps for each corresponding flanking sequence size, illustrating how raw predicted probabilities for acceptor (<italic>x</italic>-axis) and donor (<italic>y</italic>-axis) sites are transformed after calibration. Arrows indicate the shift from pre- to post-calibration states in two-dimensional probability space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-figsupp6-v1.tif"/></fig><fig id="fig5s7" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 7.</label><caption><title>Calibration results for <italic>A. thaliana</italic> splice site classification at four flanking sequence sizes.</title><p>(<bold>A, C, E, G</bold>) Reliability (calibration) curves for flanking sequence sizes of 80, 400, 2000, and 10,000 nt, respectively. Each plot compares predicted probabilities (<italic>x</italic>-axis) to empirical probabilities (<italic>y</italic>-axis) for non-splice sites (left), acceptor sites (middle), and donor sites (right). The blue curves depict the reliability of the original OSAI<sub>Arabidopsis</sub> models, while the green curves show reliability after calibration. Shaded regions represent confidence intervals. The diagonal black line indicates perfect calibration, where predicted probabilities match observed frequencies exactly. (<bold>B, D, F, H</bold>) Temperature scaling maps for each corresponding flanking sequence size, illustrating how raw predicted probabilities for acceptor (<italic>x</italic>-axis) and donor (<italic>y</italic>-axis) sites are transformed after calibration. Arrows indicate the shift from pre- to post-calibration states in two-dimensional probability space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig5-figsupp7-v1.tif"/></fig></fig-group><p>We observed similar outcomes when calibrating OSAIs on mouse (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>), honeybee (<xref ref-type="fig" rid="fig5s5">Figure 5—figure supplement 5</xref>), zebrafish (<xref ref-type="fig" rid="fig5s6">Figure 5—figure supplement 6</xref>), and <italic>Arabidopsis</italic> (<xref ref-type="fig" rid="fig5s7">Figure 5—figure supplement 7</xref>). We observed very small changes after calibration across phylogenetically diverse species, suggesting that OpenSpliceAI’s training regimen yielded well‐calibrated models, although it is possible that a different calibration algorithm might produce further improvements in performance.</p></sec><sec id="s2-6"><title>Comparing OSAI<sub>MANE</sub> and SpliceAI via variant effects of ISM</title><p>A crucial finding of <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>, was that SpliceAI was capable of capturing nonlocal effects of genomic mutations on splice site location and strength. In order to show that OSAI<sub>MANE</sub> has the same capabilities, we recreated several of their studies, as well as a large-scale ISM experiment aimed at elucidating the model’s learned splice site recognition pattern.</p><p>First, we recreated the experiment from Jaganathan et al<italic>.</italic> in which they mutated every base in a window around exon 9 of the U2SURP gene and calculated its impact on the predicted probability of the acceptor site. We repeated this experiment on exon 2 of the DST gene, again using both SpliceAI and OSAI<sub>MANE</sub>. In both cases, we found a strong similarity between the resultant patterns between SpliceAI and OSAI<sub>MANE</sub>, as shown in <xref ref-type="fig" rid="fig6">Figure 6A</xref>. To evaluate concordance more broadly, we randomly selected 100 donor and 100 acceptor sites and performed the same ISM experiment on each site. The Pearson correlation between SpliceAI and OSAI<sub>MANE</sub> yielded an overall median correlation of 0.857 (see Methods; additional DNA logos in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Comparison of SpliceAI and OSAI<sub>MANE</sub> in predicting mutation impacts and cryptic splicing events.</title><p>(<bold>A</bold>) Plot of importance scores for nucleotides near the acceptor site of exon 9 of U2SURP (top) and DST (bottom), for both SpliceAI and OSAI<sub>MANE</sub>. The importance score is calculated by taking the average decrease in acceptor site score across the three possible point mutations at a given base position. (<bold>B</bold>) Plot of the impact of each possible point mutation within 80 bp of a donor (top) site or acceptor (bottom) site, for both SpliceAI and OSAI<sub>MANE</sub>. The impact is the raw decrease in predicted splice site score after mutating a given base to a different one. (<bold>C</bold>) Visualization of cryptic splicing variants being predicted for the MYBPC3 gene (top), with an acceptor site gain and loss event, from SpliceAI’s original analysis, and the OPA1 gene (bottom), where a cryptic exon inclusion event was recently reported (<xref ref-type="bibr" rid="bib52">Qian et al., 2021</xref>). (<bold>D</bold>) Predicted splice sites for the entire CFTR gene, with the corresponding predicted probability distribution by base position plotted below, for both SpliceAI and OSAI<sub>MANE</sub>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Zoomed‐in 160 bp DNA sequence logos derived from in silico mutagenesis (ISM) importance score profiles for representative donor (<bold>A–E</bold>) and acceptor (<bold>F–J</bold>) splice sites.</title><p>Logos were generated by mapping the ISM importance score at each position centered on the splice site to letter heights, with SpliceAI scores shown in the upper logo and OSAI<sub>MANE</sub> scores shown in the lower logo of each panel.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107454-fig6-figsupp1-v1.tif"/></fig></fig-group><p>To characterize the local sequence features that both models focus on, we computed the average decrease in predicted splice site probability resulting from each of the three possible single-nucleotide substitutions at every position within 80 bp for 100 donor and 100 acceptor sites randomly sampled from the test set (chromosomes 1, 3, 5, 7, and 9). <xref ref-type="fig" rid="fig6">Figure 6B</xref> shows the average decrease in splice site strength for each mutation in the format of a DNA logo, for both tools. Pearson correlation analysis of the position weight matrices per base (<xref ref-type="bibr" rid="bib23">Gupta et al., 2007</xref>) yielded a similarity of 0.996 for the donor and 0.997 for the acceptor between OSAI<sub>MANE</sub> and SpliceAI DNA logos, demonstrating high similarity between the two tools, with the strongest signals observed for mutations at the donor/acceptor sites outside the canonical GT/AG dinucleotide motif. The acceptor sites additionally show relatively higher sensitivity to A and G just upstream of the acceptor site, which is expected due to the CT richness of the polypyrimidine tracts common in this region (<xref ref-type="bibr" rid="bib40">Majewski and Ott, 2002</xref>).</p><p>Jaganathan et al. also demonstrated SpliceAI’s ability to predict cryptic splicing mutations – intronic mutations that create alternatively spliced transcripts. We recreated their experiment in which they investigated an MYBPC3 intron mutation associated with cardiomyopathy. Both OSAI<sub>MANE</sub> and SpliceAI predict very similar changes in the location and strength of acceptor site gain and loss events (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). We then extended this experiment by examining an intronic splicing mutation in the OPA1 gene which has been shown to cause alternative splicing of a cryptic pseudoexon upstream (<xref ref-type="bibr" rid="bib52">Qian et al., 2021</xref>). Again, both OSAI<sub>MANE</sub> and SpliceAI correctly predicted this event with similarly high accuracy.</p><p>We then replicated Jaganathan et al.’s experiment on the CFTR gene, in which they showed that SpliceAI predicted all of the splice sites accurately without any false positives. Using the full gene sequence from the GRCh38 assembly and a score threshold of 0.5, we found that OSAI<sub>MANE</sub> and SpliceAI predict the exact same set of donor and acceptor sites, and accurately capture all but the first donor site, using the MANE Select annotation as reference (<xref ref-type="fig" rid="fig6">Figure 6D</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We developed OpenSpliceAI to be a modular Python toolkit designed as an open-source implementation of SpliceAI, to which we added several key enhancements. The framework replicates the core logic of the SpliceAI model while optimizing prediction efficiency and variant effect analysis, such as acceptor and donor gains or losses, using pre-trained models. Our benchmarks show substantial computational advantages over SpliceAI, with faster processing, lower memory usage, and improved GPU efficiency (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). These improvements are driven by our optimized PyTorch implementation that employs dynamic computation graphs and on-demand GPU memory allocation – allowing memory to be allocated and freed as needed – in contrast to SpliceAI’s static, Keras-based TensorFlow approach, which pre-allocates memory for the worst-case input size. In SpliceAI, this rigid memory allocation leads to high memory overhead and frequent out-of-memory errors when handling large datasets through large loop iteration prediction. Additionally, OpenSpliceAI leverages streamlined data handling and enhanced parallelization through batch prediction and multiprocessing, automatically distributing tasks across available threads. Together, these features prevent the memory pitfalls common in SpliceAI and make OpenSpliceAI a more scalable and efficient solution for large-scale genomic analysis.</p><p>It is important to note that even though OpenSpliceAI and SpliceAI share the same model architecture, the released trained models are not identical. The variability observed between our models and the original SpliceAI – and even among successive training runs using the same code and data – can be attributed to several sources of inherent randomness. First, weight initialization is performed randomly for many layers, which means that different initial weights can lead to distinct convergence paths and final model parameters. Second, the process of data shuffling alters the composition of mini-batches during training, impacting both the training dynamics and the statistics computed in batch normalization layers. Although batch normalization is deterministic for a fixed mini-batch, its reliance on batch statistics introduces variability due to the random sampling of data. Finally, OpenSpliceAI employs the AdamW optimizer (<xref ref-type="bibr" rid="bib39">Loshchilov and Hutter, 2019</xref>), which incorporates exponential moving averages of the first and second moments of the gradients. This mechanism serves a momentum-like role, contributing to an adaptive learning process that is inherently stochastic. Moreover, subtle differences in the order of operations or floating-point arithmetic, particularly in distributed computing environments, can further amplify this stochastic behavior. Together, these factors contribute to the observed nondeterministic behavior, resulting in slight discrepancies between our trained models and the original SpliceAI, as well as among successive training sessions under identical conditions.</p><p>OpenSpliceAI empowers researchers to adapt the framework to many other species by including modules that enable easy retraining. For closely related species such as mice, our retrained model demonstrated comparable or slightly better precision than the human-based SpliceAI model. For more distant species such as <italic>A. thaliana</italic>, whose genomic structure differs substantially from humans, retraining OpenSpliceAI yields much greater improvements in accuracy. Our initial release includes models trained on the human MANE genome annotation and four additional species: mouse, zebrafish, honeybee, and <italic>A. thaliana</italic>. We also evaluated pre-training on mouse (OSAI<sub>Mouse</sub>), honeybee (OSAI<sub>Honeybee</sub>), zebrafish (OSAI<sub>Zebrafish</sub>), and <italic>Arabidopsis</italic> (OSAI<sub>Arabidopsis</sub>) followed by fine-tuning on the human MANE dataset. While cross-species pre-training substantially accelerated convergence during fine-tuning, the final human splicing prediction accuracy was comparable to that of a model trained from scratch on human data. This result indicates that our architecture seems to capture all relevant splicing features from human training data alone and thus gains little or no benefit from cross-species transfer learning in this context (see <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>).</p><p>OpenSpliceAI also includes modules for transfer learning, allowing researchers to initialize models with weights learned on other species. In our transfer learning experiments, models transferred from human to other species displayed faster convergence and higher stability, with potential for increased accuracy. We also incorporate model calibration via temperature scaling, providing better alignment between predicted probabilities and empirical distributions.</p><p>The ISM study revealed that OSAI<sub>MANE</sub> and SpliceAI made predictions using very similar sets of motifs (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Across several experiments, we note that SpliceAI exhibits an inherent bias near the starts and ends of transcripts which are padded with flanking Ns (as was done in the original study), predicting donor and acceptor sites in these boundaries with an extremely high signal that disappears when the sequence is padded with the actual genomic sequence. For example, the model correctly predicted the first donor site of the CFTR gene when the gene’s boundaries were flanked with N’s; however, when replaced those N’s with the actual DNA sequence upstream of the gene boundary, the signal all but disappeared, as seen in <xref ref-type="fig" rid="fig6">Figure 6D</xref>. This suggests a bias resulting from the way the model is trained. In our ISM benchmarks, we thus chose not to use flanking N’s unless explicitly recreating a study from the original SpliceAI paper.</p><p>Additionally, we note that both the SpliceAI and OSAI<sub>MANE</sub> ‘models’ are the averaged result of five individual models, each initialized with slightly different weights. During the prediction process, each individual model was found to have discernibly different performance. By averaging their outputs leveraging the deep-ensemble approach (<xref ref-type="bibr" rid="bib19">Fort et al., 2019</xref>; <xref ref-type="bibr" rid="bib32">Lakshminarayanan et al., 2017</xref>), the overall performance of both SpliceAI and OpenSpliceAI improved while reducing sensitivity to local variations. In essence, this method normalizes the inherent randomness of the individual models, resulting in predictions that are more robust and better represent the expected behavior, ultimately yielding improved average performance across large datasets. OpenSpliceAI’s ‘predict’ submodule averages across all five models by default, but it also supports prediction using a single model.</p><p>In summary, OpenSpliceAI is a fully open-source, accessible, and computationally efficient deep learning system for splice site prediction. Its modular architecture, enhanced performance, and adaptability to diverse species make it a powerful tool for advancing research on gene regulation and splicing across diverse species.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><p>OpenSpliceAI is designed with modular subcommands that allow users to preprocess genomic data into training and test sets, train models, perform model calibration, make efficient predictions, and conduct variant analysis. The following sections summarize the usage and technical implementation of each subcommand.</p><sec id="s4-1"><title>OpenSpliceAI create-data subcommand</title><p>The ‘create-data’ subcommand converts standard genomics data formats into a machine-readable form suitable for training machine learning models. It processes genomic sequences (FASTA) and genome annotations (GFF/GTF) to produce gene sequences and splice site labels stored in Hierarchical Data Format version 5 (HDF5).</p><p>In this standard supervised sequence-to-sequence machine learning framework, a dataset comprising input features (<inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>X</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$Xs$\end{document}</tex-math></alternatives></inline-formula>) and corresponding labels (<inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>Y</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$Ys$\end{document}</tex-math></alternatives></inline-formula>) is constructed. Here, <inline-formula><alternatives><mml:math id="inf4"><mml:mi>X</mml:mi></mml:math><tex-math id="inft4">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula> represents the one-hot-encoded pre-mRNA sequences, which serve as the input variables for prediction, while <inline-formula><alternatives><mml:math id="inf5"><mml:mi>Y</mml:mi></mml:math><tex-math id="inft5">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula> denotes the labels, specifically the donor and acceptor splice sites derived from the genome annotations.</p><p>For each gene locus, the longest transcript is selected as the canonical transcript, consistent with the canonical-transcript-labeling approach of SpliceAI. By default, the ‘--biotype’ argument is set to ‘protein-coding’, which means only protein-coding genes are included in the feature set and label set. Users can change this setting to ‘all’ to include both protein-coding and non-coding genes.</p><sec id="s4-1-1"><title>Splitting gene sequences into training and testing sets</title><p>For generating datasets for OSAI<sub>MANE</sub>, OpenSpliceAI adopts SpliceAI’s chromosome‐based partitioning strategy. In human datasets, the training set is defined by chromosomes 2, 4, 6, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, X, and Y, while the testing set is defined by chromosomes 1, 3, 5, 7, and 9. For nonhuman species, OpenSpliceAI defaults to a random splitting method (specified by the ‘--split-method’ parameter). In this approach, the algorithm first computes the total chromosome length, randomly shuffles the chromosomes, and then iteratively assigns them to the training or testing set until the desired split ratio (defaulting to 80% training) is achieved, with any remaining chromosomes allocated to the test set.</p></sec><sec id="s4-1-2"><title>Pseudogenes and paralogous gene sequences removal</title><p>To ensure the integrity and accuracy of model testing, pseudogenes – segments of DNA that resemble functional genes but are incapable of coding for a protein – are removed from the test dataset. This is accomplished by filtering out genes in the GFF file that either have ‘pseudogene’ as the feature type in the third column or specify ‘pseudogene’, ‘transcribed_pseudogene’, or ‘processed_pseudogene’ for the ‘gene_biotype’ or ‘biotype’ fields.</p><p>The removal of paralogous genes is also critical, as sequence similarity between training and test sets can lead to data leakage. OpenSpliceAI performs DNA sequence alignment to detect paralogous sequences. Specifically, OpenSpliceAI uses mappy, a Python wrapper for minimap2 (<xref ref-type="bibr" rid="bib34">Li, 2018</xref>), to align test sequences to the training set, applying the ‘--asm20’ argument to allow a sequence divergence of up to 20%. Following alignment, OpenSpliceAI examines each result and excludes any test sequence that shows more than 80% sequence similarity and 80% coverage compared to any sequence in the training set.</p></sec><sec id="s4-1-3"><title>One-hot encoding scheme</title><p>The one-hot encoding procedure for the input sequence for model training, testing, and prediction uses the following representation: <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$A=[1,0,0,0]$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$C=[0,1,0,0]$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>G</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$G=[0,0,1,0]$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>U</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$T(or\,U)=[0,0,0,1]$\end{document}</tex-math></alternatives></inline-formula>. Any ambiguous nucleotide (denoted as N or other nonstandard symbols) is encoded as <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$[0,0,0,0]$\end{document}</tex-math></alternatives></inline-formula>. The encoding of the labels for model training uses the scheme: none-splice site <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$= [1,0,0]$\end{document}</tex-math></alternatives></inline-formula>, acceptor site <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$= [0,1,0]$\end{document}</tex-math></alternatives></inline-formula>, donor site <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$=[0,0,1]$\end{document}</tex-math></alternatives></inline-formula>, padding <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo>=</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$=[0,0,0]$\end{document}</tex-math></alternatives></inline-formula>. The predictions use the same scheme, where the three output channels sum to one, representing a probability score.</p></sec><sec id="s4-1-4"><title>Gene sequence segmentation for one-hot-encoded features (<italic>X</italic>s) and labels (<italic>Y</italic>s)</title><p>Following the approach used by SpliceAI, OpenSpliceAI divides gene sequences into overlapping segments, each spanning 15,000 nt. Each segment comprises a central region of 5000 nt, flanked on both sides by 5000 nt extensions, thereby providing essential upstream and downstream context. A step size of 5000 nt is used to ensure comprehensive coverage with overlapping windows. For instance, a gene that is 22,000 nt long is partitioned into five segments. Each segment is represented as a tensor with dimensions <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$(15,000,4)$\end{document}</tex-math></alternatives></inline-formula>; when a segment lacks sufficient nucleotides, the remaining positions are padded with ‘N’ bases to maintain uniform tensor dimensions. Notably, the final segment may contain fewer real nucleotides – only 2000 in this example – with the deficit filled by padding. Thus, the one-hot-encoded feature matrix (<inline-formula><alternatives><mml:math id="inf16"><mml:mi>X</mml:mi></mml:math><tex-math id="inft16">\begin{document}$X$\end{document}</tex-math></alternatives></inline-formula>) for the gene has a shape of <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>15</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>4</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$(5,15,000,4)$\end{document}</tex-math></alternatives></inline-formula>, while the corresponding label tensor (<inline-formula><alternatives><mml:math id="inf18"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>Y</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft18">\begin{document}$Y$\end{document}</tex-math></alternatives></inline-formula>), which focuses on the central 5000 nt, has a shape of <inline-formula><alternatives><mml:math id="inf19"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>5</mml:mn><mml:mo>,</mml:mo><mml:mn>000</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft19">\begin{document}$(5,5,000,3)$\end{document}</tex-math></alternatives></inline-formula>. The batch size is set to 100 by default, meaning that OpenSpliceAI concatenates the first dimension of each tensor from 100 genes into a single matrix and performs the same concatenation for the label tensor matrix.</p></sec><sec id="s4-1-5"><title>Selecting splice sites for inclusion in dataset labels (<italic>Y</italic>s)</title><p>Even with curated annotations, some splice sites in the annotation file may still be misannotated. To improve the accuracy of splice site labeling, OpenSpliceAI provides a ‘--canonical-only’ argument that restricts analysis to canonical splice sites. By default, this option is disabled, so all splice sites in the annotation file are evaluated. These include the U2-snRNP-type motifs ‘GT-AG’ and ‘GC-AG’ (<xref ref-type="bibr" rid="bib13">Brow, 2002</xref>) and the U12-snRNP-type motifs ‘GT-AG’ and ‘AT-AC’ (<xref ref-type="bibr" rid="bib20">Frilander and Steitz, 1999</xref>; <xref ref-type="bibr" rid="bib50">Patel and Steitz, 2003</xref>; <xref ref-type="bibr" rid="bib61">Wassarman and Steitz, 1992</xref>).</p></sec></sec><sec id="s4-2"><title>OpenSpliceAI train subcommand</title><p>After the training and test sets are created, this subcommand takes the HDF5 outputs from the ‘create-data’ subcommand and enables users to train their OpenSpliceAI model. Users can train different OpenSpliceAI models with various flanking sequence lengths, including 80 nt, 400 nt, 2000 nt, and 10,000 nt.</p><sec id="s4-2-1"><title>OpenSpliceAI adaptive learning</title><p>OpenSpliceAI uses the AdamW optimizer (<xref ref-type="bibr" rid="bib39">Loshchilov and Hutter, 2019</xref>) with a default learning rate of 0.001. The training dataset is further split into 90:10 for training and validation. By default, OpenSpliceAI trains a model for 10 epochs, with an early stopping patience of 2.</p><p>The ‘--scheduler’ argument enables users to choose between two built-in PyTorch learning rate schedulers – MultiStepLR and CosineAnnealingWarmRestarts (<xref ref-type="bibr" rid="bib38">Loshchilov and Hutter, 2016</xref>) – to dynamically adjust the learning rate during training. By default, OpenSpliceAI employs ‘MultiStepLR’ with a learning rate of 0.001, beginning with a 0.5 decay from the sixth epoch, the same approach used in the SpliceAI model.</p><p>As detailed in the Results section, training the OSAI model on <italic>Arabidopsis</italic> data using the MultiStepLR scheduler resulted in a more stable training process. In contrast, all other OpenSpliceAI models, except OSAI<sub>Arabidopsis</sub>, were trained using the ‘CosineAnnealingWarmRestarts’ scheduler, configured with ‘T_0=5’, ‘T_mult = 1’, ‘eta_min = 1e-5’, and ‘last_epoch=-1’. This scheduler gradually reduces the learning rate from an initial value of 1e-3 to a minimum of 1e-5 in a smooth, wave-like (cosine) pattern over each cycle. The parameter ‘T_0=5’ sets the initial period for the cosine decay, meaning the learning rate completes one full cycle – from the starting rate down to ‘eta_min’ and back – within 5 epochs. After the first cycle, the learning rate ‘restarts’ at its initial value, creating a ‘warm restart’.</p></sec><sec id="s4-2-2"><title>OpenSpliceAI loss function</title><p>By default, OpenSpliceAI uses the categorical cross-entropy loss function (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) to compute the loss at every nucleotide position in the input DNA sequence. This loss function measures the discrepancy between the predicted probability distribution and the true distribution for each position, which is standard practice for multi-class classification tasks. Alternatively, users can opt for the focal loss (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>; <xref ref-type="bibr" rid="bib36">Lin et al., 2018</xref>).</p><p>Focal loss enhances the standard cross-entropy loss by adding a modulating term, <inline-formula><alternatives><mml:math id="inf20"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft20">\begin{document}$(1- P_{class})^{\gamma}$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$P_{class}$\end{document}</tex-math></alternatives></inline-formula> represents the model’s predicted probability for the correct class. This term down-weights the loss assigned to well-classified examples, allowing the model to concentrate more on the misclassified or harder-to-classify cases. For instance, setting <italic>γ</italic> to 2 amplifies the focus on challenging predictions, which is particularly beneficial in scenarios with class imbalance or when the signal in the data is subtle. This dynamic weighting of loss can enhance overall model accuracy.<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>C</mml:mi><mml:mi>E</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle  Loss_{CEL}=\sum _{class\in \left \{donor,acceptor,neither\right \}}I_{class}\times \mathrm {log}\left (P_{class}\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>F</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>w</mml:mi><mml:mi>h</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle  Loss_{FL}=\sum _{class\in \left \{donor,acceptor,neither\right \}}I_{class}\times \left (1- P_{class}\right)^{\gamma} \times log\left (P_{class}\right),where\,\gamma =2$$\end{document}</tex-math></alternatives></disp-formula></p></sec></sec><sec id="s4-3"><title>OpenSpliceAI transfer subcommand</title><p>Instead of training a model entirely from scratch, users can leverage transfer learning to adapt a human-trained model for a target species using the transfer subcommand. This process resembles standard model training but starts from a pre-trained model specified with the ‘--pretrained-model’ argument. We recommend OSAI<sub>MANE</sub> as the pre-trained base model. Once the pre-trained weights are loaded, the transfer subcommand enables flexible fine-tuning. Users can either unfreeze all layers (using the ‘--unfreeze-all’ flag) or selectively train the final layers (with ‘--unfreeze&lt;INT &gt;’) to adapt the model more effectively to the target species data. In addition, similar to the train subcommand, the transfer subcommand integrates adaptive learning rate scheduling and early stopping to optimize convergence and prevent overfitting, all while using the same loss function configuration.</p><p>While transfer learning employs the same underlying OpenSpliceAI architecture, optimizer, scheduler, and loss function as training from scratch, it differs primarily in its initialization step, which is based on a fully trained model. For optimal results, we recommend selecting species with high-quality genome assemblies and comprehensive annotations, such as <italic>H. sapiens</italic>. This approach substantially reduces training time and can improve accuracy on the target species. See <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s4">4</xref> for further details.</p></sec><sec id="s4-4"><title>OpenSpliceAI calibrate subcommand</title><p>One improvement in OpenSpliceAI over SpliceAI is the incorporation of model calibration, which refines model-predicted probabilities to align more closely with actual outcome likelihoods. This is achieved by calibrating the model’s output so that a prediction with a probability of 0.6, e.g., accurately reflects a 60% chance of being correct. The calibrate subcommand evaluates scores around this value, identifies deviations from expected probabilities, and applies nonlinear adjustments to correct the score distribution without altering the model’s performance. After the OpenSpliceAI model was trained, we used the validation dataset to calibrate the model and evaluated the calibrated results using the test dataset. Such calibration is crucial in predictive modeling, particularly for classification, as it ensures that predicted probabilities are consistent with observed outcomes. Uncalibrated models can be overconfident or underconfident, potentially compromising decision-making quality.</p><p>There are various methods for calibrating models, including Platt scaling (<xref ref-type="bibr" rid="bib51">Platt, 1999</xref>), isotonic regression (<xref ref-type="bibr" rid="bib65">Zadrozny and Elkan, 2002</xref>), and histogram binning (<xref ref-type="bibr" rid="bib64">Zadrozny and Elkan, 2001</xref>). Here, we implemented class-wise temperature scaling, a variant of Platt scaling often used in knowledge distillation and statistical mechanics (<xref ref-type="bibr" rid="bib24">Hinton et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Jaynes, 1957</xref>). Temperature scaling is a post hoc adjustment that modifies model output probabilities to more accurately reflect true class likelihoods.</p><sec id="s4-4-1"><title>OpenSpliceAI calibration optimization procedure</title><p>For model calibration in OpenSpliceAI, we froze the trained OpenSpliceAI model weights and augmented the network with a class-specific temperature scaling layer. Instead of using a single scalar temperature parameter, we employ a vector of temperature parameters <inline-formula><alternatives><mml:math id="inf22"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><tex-math id="inft22">\begin{document}$T=\left [T_{0},T_{1},T_{2}\right ]$\end{document}</tex-math></alternatives></inline-formula> corresponding to the non-splice site, acceptor site, and donor site, respectively. This design allows each class’s logit to be scaled individually, thereby addressing the inherent class imbalance and the sparsity of splice site signals. The logits are divided by their corresponding temperature parameters before applying the softmax function (<xref ref-type="bibr" rid="bib12">Bridle, 1989</xref>), thereby aligning the predicted probabilities with the empirical likelihoods.</p><p>The temperature vector <inline-formula><alternatives><mml:math id="inf23"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft23">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> was optimized using the Adam optimizer (<xref ref-type="bibr" rid="bib31">Kingma and Ba, 2014</xref>) with an initial learning rate of 0.01 for its adaptive capabilities. To further enhance convergence, we employ PyTorch’s ReduceLROnPlateau scheduler, which reduces the learning rate by a factor of 0.1 if the validation loss does not improve over two consecutive epochs. In addition, early stopping was implemented with a patience of two epochs and a minimum improvement threshold (delta) of <inline-formula><alternatives><mml:math id="inf24"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft24">\begin{document}$10^{-6}$\end{document}</tex-math></alternatives></inline-formula>. If the validation loss did not decrease by at least <inline-formula><alternatives><mml:math id="inf25"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft25">\begin{document}$10^{-6}$\end{document}</tex-math></alternatives></inline-formula> over two epochs, optimization halts early, ensuring calibration efficiency and preventing overfitting.</p><p>Temperature scaling modifies the logits <inline-formula><alternatives><mml:math id="inf26"><mml:mi>z</mml:mi></mml:math><tex-math id="inft26">\begin{document}$z$\end{document}</tex-math></alternatives></inline-formula> (the raw outputs of the model before the softmax function) by scaling them with <inline-formula><alternatives><mml:math id="inf27"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft27">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). The adjusted logits <inline-formula><alternatives><mml:math id="inf28"><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>`</mml:mi></mml:mrow></mml:msup></mml:math><tex-math id="inft28">\begin{document}$z^{'}$\end{document}</tex-math></alternatives></inline-formula> are computed as:<disp-formula id="equ3"><label>(3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mi>T</mml:mi></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle z^{'}=\frac{z}{T}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf29"><mml:mi>z</mml:mi></mml:math><tex-math id="inft29">\begin{document}$z$\end{document}</tex-math></alternatives></inline-formula> represents the original logits. The calibrated probabilities <inline-formula><alternatives><mml:math id="inf30"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><tex-math id="inft30">\begin{document}$\overset{\hat }{p}$\end{document}</tex-math></alternatives></inline-formula> are then obtained by applying the softmax function (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref>):<disp-formula id="equ4"><label>(4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>z</mml:mi><mml:mrow><mml:mo>′</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle \hat{p} =softmax (z^{'})$$\end{document}</tex-math></alternatives></disp-formula></p><p>A higher temperature <inline-formula><alternatives><mml:math id="inf31"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft31">\begin{document}$T\gt 1$\end{document}</tex-math></alternatives></inline-formula> spreads out the probability distribution, reducing confidence, while a lower temperature <inline-formula><alternatives><mml:math id="inf32"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft32">\begin{document}$T\lt 1$\end{document}</tex-math></alternatives></inline-formula> sharpens it, increasing confidence. To optimize calibration, we use the negative log-likelihood (NLL) loss function defined as (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>):<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">N</mml:mi><mml:mi mathvariant="bold-italic">L</mml:mi><mml:mi mathvariant="bold-italic">L</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi mathvariant="italic">p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi mathvariant="italic">i</mml:mi><mml:mo mathvariant="italic">,</mml:mo><mml:mi mathvariant="italic">c</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle  \boldsymbol {L_{NLL}}=- \sum _{i=1}^{N}\sum _{c=1}^{C}I_{i,c}\rm {log}\left (\hat{\it p}_{\it i,c}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf33"><mml:mi>N</mml:mi></mml:math><tex-math id="inft33">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> is the number of samples, <inline-formula><alternatives><mml:math id="inf34"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>C</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft34">\begin{document}$C$\end{document}</tex-math></alternatives></inline-formula> is the number of classes, in our case, acceptor site, donor site, and non-splice site. <inline-formula><alternatives><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>I</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft35">\begin{document}$I_{i,c}$\end{document}</tex-math></alternatives></inline-formula> is the indicator function, which equals 1 if sample <inline-formula><alternatives><mml:math id="inf36"><mml:mi>i</mml:mi></mml:math><tex-math id="inft36">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> belongs to class <inline-formula><alternatives><mml:math id="inf37"><mml:mi>c</mml:mi></mml:math><tex-math id="inft37">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula> and 0 otherwise. <inline-formula><alternatives><mml:math id="inf38"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft38">\begin{document}$\hat{p}_{i,c}$\end{document}</tex-math></alternatives></inline-formula> is the model’s predicted probability of the sample <inline-formula><alternatives><mml:math id="inf39"><mml:mi>i</mml:mi></mml:math><tex-math id="inft39">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula> belongs to class <inline-formula><alternatives><mml:math id="inf40"><mml:mi>c</mml:mi></mml:math><tex-math id="inft40">\begin{document}$c$\end{document}</tex-math></alternatives></inline-formula>. A lower NLL indicates that the predicted probabilities are more closely aligned with the true labels, reflecting better calibration and overall model performance.</p><p>The optimal temperature <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msup><mml:mi>T</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$T^{*}$\end{document}</tex-math></alternatives></inline-formula> is determined by minimizing the NLL loss over the validation dataset (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>):<disp-formula id="equ6"><label>(6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>T</mml:mi><mml:mo>∗</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mi>arg</mml:mi><mml:mo>⁡</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mi>T</mml:mi></mml:munder><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle T^* = \arg\min_T \mathcal{L}_{NLL}(\hat{p}, y)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf42"><mml:mi>y</mml:mi></mml:math><tex-math id="inft42">\begin{document}$y$\end{document}</tex-math></alternatives></inline-formula> are the true labels.</p><p>The temperature parameter <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>T</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$T$\end{document}</tex-math></alternatives></inline-formula> was initialized to one and constrained between 0.05 and 5.0 to prevent extreme scaling. We employed gradient-based optimization to minimize the cross-entropy loss (nn.CrossEntropyLoss, i.e., NLL loss) on the validation set while keeping the original model weights fixed.</p></sec><sec id="s4-4-2"><title>OpenSpliceAI calibration evaluation</title><p>The first metric, which we used for temperature optimization, is NLL (<xref ref-type="disp-formula" rid="equ5">Equation 5</xref>), which measures the match between predicted probabilities and the true labels; lower NLL values indicate better calibration.</p><p>We also evaluated the temperature-scaled new probabilities using the expected calibration error (ECE) (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>). ECE quantifies the discrepancy between confidence estimates and actual accuracy over a range of probability bins. It does so by partitioning the predictions into <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$M$\end{document}</tex-math></alternatives></inline-formula> bins, by default OpenSpliceAI sets to 30, and computing the weighted average of the absolute differences between the confidence (predicted probability) and accuracy within each bin. The ECE is defined as:<disp-formula id="equ7"><label>(7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi class="mathcal" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mi>C</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow><mml:mi>N</mml:mi></mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  \mathcal{L}_{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{N} \left| acc(B_m) - conf(B_m) \right|$$\end{document}</tex-math></alternatives></disp-formula></p><p><inline-formula><alternatives><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft45">\begin{document}$B_{m}$\end{document}</tex-math></alternatives></inline-formula> is the set of indices of samples whose predicted probabilities fall into the <italic>m</italic>th bin; <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$\left|B_{m}\right| $\end{document}</tex-math></alternatives></inline-formula> is the number of samples in the <italic>m</italic>th bin; <inline-formula><alternatives><mml:math id="inf47"><mml:mi>N</mml:mi></mml:math><tex-math id="inft47">\begin{document}$N$\end{document}</tex-math></alternatives></inline-formula> is the total number of samples; <inline-formula><alternatives><mml:math id="inf48"><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math><tex-math id="inft48">\begin{document}$acc\left (B_{m}\right)$\end{document}</tex-math></alternatives></inline-formula> is the average accuracy in the <italic>m</italic>th bin (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>).<disp-formula id="equ8"><label>(8)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mn mathvariant="bold">1</mml:mn></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle  acc(B_m) = \frac{1}{|B_m|} \sum_{i \in B_m} \mathbf{1}(\hat{y}_i = y_i)$$\end{document}</tex-math></alternatives></disp-formula></p><p><inline-formula><alternatives><mml:math id="inf49"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$conf\left (B_{m}\right)$\end{document}</tex-math></alternatives></inline-formula> is the average confidence in the <italic>m</italic>th bin (<xref ref-type="disp-formula" rid="equ9">Equation 9</xref>), where <inline-formula><alternatives><mml:math id="inf50"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$\hat {y_{i}}$\end{document}</tex-math></alternatives></inline-formula> is the predicted class for sample <inline-formula><alternatives><mml:math id="inf51"><mml:mi>i</mml:mi></mml:math><tex-math id="inft51">\begin{document}$i$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$\hat {p_{i}}$\end{document}</tex-math></alternatives></inline-formula> is the predicted probability associated with the predicted class <inline-formula><alternatives><mml:math id="inf53"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mrow><mml:mover><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$\hat {y_{i}}$\end{document}</tex-math></alternatives></inline-formula>.<disp-formula id="equ9"><label>(9)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mi>i</mml:mi></mml:msub></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  {conf}(B_m) = \frac{1}{|B_m|} \sum_{i \in B_m} \hat{p}_i$$\end{document}</tex-math></alternatives></disp-formula></p><p>An ECE of 0 indicates perfect calibration, where confidence and accuracy are aligned across all bins.</p></sec><sec id="s4-4-3"><title>OpenSpliceAI reliability curve and confidence interval</title><p>We generated calibration curves for each class using the calibration_curve function from scikit-learn, employing 30 bins with a uniform binning strategy. For input, the logits and labels were reshaped into 2D tensors. This function calculates the mean predicted probability and the true frequency of the positive class within each bin. To visualize the uncertainty in these estimates, we computed confidence intervals for each bin using the normal approximation method (<xref ref-type="bibr" rid="bib53">Raschka, 2018</xref>; <xref ref-type="disp-formula" rid="equ1 equ1 equ1">Equations 10, 11, and 12)</xref>:<disp-formula id="equ10"><label>(10)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>n</mml:mi></mml:mfrac></mml:msqrt></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle SE = \sqrt{\frac{\hat{p}(1 - \hat{p})}{n}} $$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ11"><label>(11)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mtext>CI</mml:mtext><mml:mrow><mml:mtext>lower</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo>⋅</mml:mo><mml:mi>S</mml:mi><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \text{CI}_{\text{lower}} = \max(\hat{p} - z \cdot SE, 0)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ12"><label>(12)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">C</mml:mi><mml:msub><mml:mi mathvariant="normal">I</mml:mi><mml:mrow><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="italic">p</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mi mathvariant="italic">z</mml:mi><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="italic">S</mml:mi><mml:mi mathvariant="italic">E</mml:mi></mml:mrow><mml:mo mathvariant="italic">,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle \rm CI_{upper}=min (\hat {\it p}+\it z\cdot {\it SE}, \rm 1)$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf54"><mml:mover accent="true"><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math><tex-math id="inft54">\begin{document}$\overset{\hat }{p}$\end{document}</tex-math></alternatives></inline-formula> is the empirical probability and <inline-formula><alternatives><mml:math id="inf55"><mml:mi>n</mml:mi></mml:math><tex-math id="inft55">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula> is the number of samples in the bin. <xref ref-type="disp-formula" rid="equ1">Equation 10</xref> defines the standard error (SE) of the estimated parameter. For a 95% confidence level, <inline-formula><alternatives><mml:math id="inf56"><mml:mi>z</mml:mi></mml:math><tex-math id="inft56">\begin{document}$z$\end{document}</tex-math></alternatives></inline-formula> is set to 1.96. <xref ref-type="disp-formula" rid="equ1 equ1">Equations 11 and 12</xref> provide the lower and upper bounds of the confidence interval, respectively (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplements 1</xref>–<xref ref-type="fig" rid="fig5s7">7</xref> for all reliability curve results).</p></sec></sec><sec id="s4-5"><title>OpenSpliceAI predict subcommand</title><p>After the OpenSpliceAI model is trained, users can execute this subcommand to predict splice sites in DNA sequences provided in FASTA format. This command also supports limiting predictions to protein-coding genes by using a GFF annotation file for the given genome. It outputs the results in BED format, collecting all probable donor and acceptor site locations into separate files.</p><sec id="s4-5-1"><title>OpenSpliceAI data preprocessing</title><p>Depending on the inputs of the subcommand, OpenSpliceAI will extract the input sequences differently. If only a FASTA file is provided, OpenSpliceAI will collect all sequences within the file for prediction. If a FASTA and GFF file are both provided, OpenSpliceAI will extract all features of type ‘gene’ from the GFF file and use those coordinates to extract sequences from the FASTA file for prediction.</p><p>To aid in memory management, OpenSpliceAI splits any sequence with length greater than the ‘split-threshold’ (default: 1,500,000 bases) into chunks that are no longer than this threshold. This parameter can be adjusted and ensures that each chunk can be loaded entirely into memory during the one-hot encoding process. Additionally, to optimize speed, if the total length of sequences in the FASTA file is below the ‘hdf_threshold’ (default: 5000 bases), the tool bypasses HDF5 compression and processes the input directly as text, achieving a slight performance speedup.</p><p>After all sequences are collected, the tool preprocesses the inputs using a method similar to that employed during training, but without handling any true labels. Each sequence is split into overlapping windows of size 5000 + ‘flanking_size’, where the overlap is equal to half of the ‘flanking_size’. No clipping is allowed for the input, so if the final subsequence is shorter than 5000 + ‘flanking_size’, it is right-padded with N’s. Similarly, the first subsequence is left-padded with ‘flanking_size’/2 N’s. If a sequence is split according to the ‘split_threshold’, it is divided so that adjacent subsequences share an overlap equal to half the ‘flanking_size’, preventing N-padding from interfering with predictions. This ensures that the model predicts every single base of the provided input sequences.</p><p>After window-based splitting, all 5000 + ‘flanking_size’ subsequences generated from the input FASTA entry are grouped together and one-hot encoded in parallel, yielding an entry with dimensions (N, 5000 + ‘flanking_size’, 4), where N is the number of subsequences generated from the given FASTA entry. These entries are further grouped into chunks of size ‘chunk_size’ (default 100), which ensures that they are processed together, reshaping the input to size (&lt;=‘chunk_size’, N, 5000 + ‘flanking_size’, 4). The resultant chunks are saved together as an HDF5-compressed file.</p></sec><sec id="s4-5-2"><title>OpenSpliceAI prediction algorithm design</title><p>To dynamically manage memory and optimize speed, OpenSpliceAI offers two modes of prediction: standard and turbo, which is controlled by the ‘predict_all’ flag. In standard mode, predictions for all bases are stored (which can be memory-intensive) and written to a BED file. In contrast, turbo mode skips storing individual predictions and directly converts them to a BED file without storing them, reducing memory usage. By default, turbo mode is enabled (see <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>To start the prediction process, OpenSpliceAI first loads the appropriate pre-trained models. Like SpliceAI, each pre-trained OpenSpliceAI model can include multiple individual models that are averaged to produce a final, higher-quality prediction. Users can specify either a single model file or a directory containing multiple models, in which case the tool automatically averages predictions from all provided models. Depending on the user’s system, OpenSpliceAI selects the best available computing device for model loading and prediction, prioritizing CUDA, MPS, CPU in that order.</p><p>OpenSpliceAI performs batch prediction with parallelized prediction, significantly reducing prediction time. The ‘batch_size’ parameter is determined based on the ‘flanking_size’ and the computing device used. Chunked sequences are loaded into a PyTorch DataLoader object, which batches the one-hot-encoded chunk into dimension (&lt;=‘batch_size’, 4, 5000 + ‘flanking_size’). Each batch is processed through all provided models, producing averaged, batched predictions, which are then accumulated by chunk and flattened to reconstruct the full input sequence (FASTA entry) of dimension (&lt;=‘split_threshold’, 3). The second dimension represents predictions for whether each base position is a donor site, acceptor site, or neither.</p><p>OpenSpliceAI’s memory handling varies based on the prediction mode. In standard mode, the output tensor is periodically flushed to a file, controlled by the ‘flush_predict_threshold’ parameter, which specifies how many sequences are stored in memory before flushing to an HDF5 file. Predictions are then converted into a BED file that explicitly identifies donor and acceptor sites. In turbo mode, the prediction and BED-file-writing steps are performed simultaneously, and the raw tensor predictions for each base are discarded from memory rather than saved. This substantially reduces memory and processing time.</p></sec><sec id="s4-5-3"><title>OpenSpliceAI prediction outputs</title><p>The prediction step generates two BED files – one for donor sites and one for acceptor sites – containing the coordinates and scores of all splice sites that exceed a specified score threshold (set by the ‘threshold’ parameter, default 0.5). The tool automatically extracts relevant information from input files to determine splice site coordinates in the BED file. If not enough annotation data is provided (e.g. the FASTA header does not have transcript start and end coordinates and no annotation file is provided), the coordinates are reported relative to the FASTA sequence, with position 0 corresponding to the first nucleotide. If an annotation file is provided, the tool extracts protein-coding gene loci and calculates the coordinates of splice sites within each locus.</p></sec></sec><sec id="s4-6"><title>OpenSpliceAI variant subcommand</title><p>The ‘variant’ subcommand reimplements SpliceAI’s publicly available utility (<xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>) to evaluate the effects of genetic variants on the location and strength of splice sites. It does so by comparing predictions made on wild-type and mutant sequences to determine the impact of single nucleotide polymorphisms and insertions or deletions (INDELs) on the resulting mRNA transcript. The tool outputs ‘delta’ scores for four events: donor site gain, donor site loss, acceptor site gain, and acceptor site loss, along with the most probable position of each event relative to the mutation. It accepts a variant call format (VCF) file as input and returns an output VCF file annotated with the delta scores and positions.</p><p>This subcommand supports variant effect prediction using both PyTorch and Keras models to maintain compatibility with upstream workflows. However, PyTorch-based models are strongly recommended for faster prediction and lower memory overhead.</p><sec id="s4-6-1"><title>OpenSpliceAI variant delta score calculation</title><p>The ‘delta’ score is defined similarly to <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>, and refers to the maximum change in splicing score within a fixed window on each side of the mutation. By default, the window size is 50, meaning it will consider the donor and acceptor scores for the 101 positions around the variant. Supposing the array of donor and acceptor scores of the wild-type sequence are <inline-formula><alternatives><mml:math id="inf57"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft57">\begin{document}$d_{ref}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf58"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft58">\begin{document}$a_{ref}$\end{document}</tex-math></alternatives></inline-formula>, and those of the mutated sequence are <inline-formula><alternatives><mml:math id="inf59"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft59">\begin{document}$d_{alt}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf60"><mml:msub><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft60">\begin{document}$a_{alt}$\end{document}</tex-math></alternatives></inline-formula>, then the delta scores (DS) are calculated as follows (<xref ref-type="disp-formula" rid="equ1">Equations 13–16</xref>):<disp-formula id="equ13"><label>(13)</label><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle DS\left (Acceptor\,Gain\right)=\mathrm {max} \left (a_{alt}- a_{ref}\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ14"><label>(14)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>L</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle DS\left (Acceptor\,Loss\right)=\mathrm {max} \left (a_{ref}- a_{alt}\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ15"><label>(15)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle DS\left (Donor\,Gain\right)={\rm max} \left (d_{alt}- d_{ref}\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ16"><label>(16)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>G</mml:mi><mml:mi>a</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle DS\left (Donor\,Gain\right)={\rm max} \left (d_{alt}- d_{ref}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>In <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>, the term ‘delta score’ specifically refers to the maximum value among the four events. However, we do not use this score in our output. Instead, the output VCF file reports separate scores for each of the four events.</p></sec><sec id="s4-6-2"><title>OpenSpliceAI splice site variant scoring process</title><p>The ‘variant’ subcommand takes in a VCF file, a reference genome in FASTA format, the model, the flanking size, and a custom annotation file. It annotates each variant in the provided VCF with four delta scores and four corresponding ‘delta positions’, which represent the relative nucleotide location of each delta score. By default, the delta position is ±50, with negative values indicating positions upstream of the variant and positive values downstream. Variants outside of genes defined by the annotation file, those that are less than ‘flanking_size’ from the ends of the chromosome, and deletions greater than 2x ‘distance’ are excluded from annotation. The tool returns a VCF file with OpenSpliceAI annotations for all valid variants (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p></sec></sec><sec id="s4-7"><title>OSAI<sub>MANE</sub> training</title><p>We generated training and test datasets using the ‘create-data’ subcommand. Following the SpliceAI approach, chromosomes 1, 3, 5, 7, and 9 were held out for testing. Paralogous genes were stringently removed from the test dataset based on sequence alignment results between the training and test sets (see Methods: Pseudogenes and paralogous gene sequences removal). The ‘--canonical-only’ argument was used to label only donor and acceptor sites in U2-snRNP-type and U12-snRNP-type introns.</p><p>Next, we trained OSAI<sub>MANE</sub> with the train subcommand, employing a cosine annealing scheduler (‘--scheduler CosineAnnealingWarmRestarts’) and a categorical cross-entropy loss function (‘--loss cross_entropy_loss’) over 10 epochs (‘--epochs 10’).</p><p>We trained OSAI<sub>MANE</sub> with four different flanking sequence lengths: 80, 400, 2000, and 10,000 nt. For each flanking sequence length, five models were trained with different random seeds to enable ensemble score predictions, following the SpliceAI-Keras approach. Model performance was evaluated on the held-out chromosomes (1, 3, 5, 7, and 9) using top-k accuracy, AUPRC, overall accuracy, precision, recall, and F1 score for both donor and acceptor sites.</p><sec id="s4-7-1"><title>Commands to train OSAI<sub>MANE</sub></title><p>The following commands reproduce the creation of <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/405/GCF_000001405.40_GRCh38.p14/GCF_000001405.40_GRCh38.p14_genomic.fna.gz">OSAIMANE</ext-link> using the OpenSpliceAI framework. To do so, an annotation GFF file, specifically the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">Human RefSeq MANE v1.3 annotation</ext-link>, and a genome FASTA file, the <ext-link ext-link-type="uri" xlink:href="https://ftp.ncbi.nlm.nih.gov/refseq/MANE/MANE_human/release_1.3/MANE.GRCh38.v1.3.refseq_genomic.gff.gz">GRCh38.p14 </ext-link>genome , are required.</p><list list-type="bullet" id="list1"><list-item><p>Creating training and test dataset:</p></list-item></list><p><monospace>openspliceai create-data --genome-fasta GCF_000001405.40_GRCh38.p14_genomic.fna --annotation-gff MANE.GRCh38.v1.3.refseq_genomic.gff --output-dir train_test_dataset_MANE_test/ --remove-paralogs --min-identity 0.8 --min-coverage 0.8 --parse-type canonical --write-fasta --split-method human --canonical-only</monospace></p><list list-type="bullet" id="list2"><list-item><p>Training OSAI<sub>MANE</sub></p></list-item></list><p><monospace>openspliceai train --flanking-size 10000 --train-dataset dataset_train.h5 --test-dataset dataset_test.h5 --output-dir model_train_outdir/ --project-name OSAI-MANE --loss cross_entropy_loss --scheduler CosineAnnealingWarmRestarts --epochs 10</monospace></p><list list-type="bullet" id="list3"><list-item><p>Calibrating OSAI<sub>MANE</sub> (optional)</p></list-item></list><p><monospace>openspliceai calibrate –flanking-size 10000 --train-dataset dataset_train.h5 --test-dataset dataset_test.h5 -–output-dir model_calibrate_outdir/ –-project-name OSAI-MANE-calibrate --pre-trained-model model_best.pt --loss cross_entropy_loss</monospace></p><p>Similar to training OSAI<sub>MANE</sub>, the OpenSpliceAI framework can also be used to train species-specific models using different genomes and genome annotations.</p></sec></sec><sec id="s4-8"><title>Hardware resources for training in this study</title><p>OpenSpliceAI study was conducted on the Rockfish cluster. For data preprocessing, OpenSpliceAI was run with a single thread on a 24-core Intel Xeon Cascade Lake 6248R processor, with a base frequency of 3.0 GHz and a 1 TB NVMe local drive. The five OpenSpliceAI models – OSAI<sub>MANE</sub>, OSAI<sub>Mouse</sub>, OSAI<sub>Zebrafish</sub>, OSAI<sub>Honeybee</sub>, and OSAI<sub>Arabidopsis</sub> – were each trained with a single Nvidia A100 GPU with 40 GB of memory and 192 GB of DDR4 memory. Slurm jobs were submitted with ‘--mem=64 G’.</p></sec><sec id="s4-9"><title>Model architecture and training hyperparameters</title><p>Building on SpliceAI’s model architecture, we re-implemented the deep residual CNN using PyTorch to improve flexibility and extensibility (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>; <ext-link ext-link-type="uri" xlink:href="https://github.com/Kuanhao-Chao/OpenSpliceAI/blob/main/openspliceai/openspliceai.py">https://github.com/Kuanhao-Chao/OpenSpliceAI/blob/main/openspliceai/openspliceai.py</ext-link>). The network processes an input tensor of shape (batch size, input length, 4) that encodes one-hot nucleotide sequences. Four different flanking sequence lengths – 80, 400, 2000, and 10,000 nucleotides – are used to train four separate models, providing flexibility in capturing splicing signals at varying genomic contexts.</p><p>At the core of OpenSpliceAI, an initial 1D convolution (mapping from 4 channels to <inline-formula><alternatives><mml:math id="inf61"><mml:mi>L</mml:mi></mml:math><tex-math id="inft61">\begin{document}$L$\end{document}</tex-math></alternatives></inline-formula> channels) projects the nucleotide embedding into a higher-dimensional feature space. The resulting features pass through a series of ResidualUnit blocks, each consisting of two dilated convolutional layers with LeakyReLU (<inline-formula><alternatives><mml:math id="inf62"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft62">\begin{document}$\alpha =0.1$\end{document}</tex-math></alternatives></inline-formula>) activation and batch normalization. These dilated convolutional layers employ increasing dilation rates (<inline-formula><alternatives><mml:math id="inf63"><mml:mi>A</mml:mi><mml:mi>R</mml:mi></mml:math><tex-math id="inft63">\begin{document}$AR$\end{document}</tex-math></alternatives></inline-formula> vector) and kernel sizes (<inline-formula><alternatives><mml:math id="inf64"><mml:mi>W</mml:mi></mml:math><tex-math id="inft64">\begin{document}$W$\end{document}</tex-math></alternatives></inline-formula> vector) to enlarge the receptive field without requiring extremely deep networks. Every fourth residual block is followed by a Skip layer that merges skip-connection features via a 1D convolution, ensuring better gradient flow and stabilizing training.</p><p>To accommodate the shrinking of the sequence length necessitated by large receptive fields, we employ a Cropping1D layer that removes extra padding introduced by convolutional dilation. Specifically, the amount of cropping <inline-formula><alternatives><mml:math id="inf65"><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:math><tex-math id="inft65">\begin{document}$CL$\end{document}</tex-math></alternatives></inline-formula> is computed as twice the sum of <inline-formula><alternatives><mml:math id="inf66"><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mi>W</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math><tex-math id="inft66">\begin{document}$AR\times \left (W- 1\right)$\end{document}</tex-math></alternatives></inline-formula>. By slicing out <inline-formula><alternatives><mml:math id="inf67"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mi>L</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math><tex-math id="inft67">\begin{document}$CL/2$\end{document}</tex-math></alternatives></inline-formula> nucleotides from each end of the sequence, the Cropping1D layer aligns the network output with the desired prediction length. The final layer is a 1D convolution mapping the output features into three channels – representing probabilities for the donor site, acceptor site, or neither. We apply a softmax activation over these three positions at each nucleotide, yielding position-wise splice site prediction probabilities. See <xref ref-type="table" rid="table2">Table 2</xref> for a summary of the model architectures trained with four different flanking sequence lengths.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Summary of the four OpenSpliceAI model architectures, each trained with a distinct flanking sequence length (80, 400, 2000, and 10,000 nucleotides).</title><p>The table lists the kernel sizes (W), dilation rates (AR), number of residual and skip blocks, and total cropping length (CL).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Parameter</th><th align="left" valign="top">Flanking = 80</th><th align="left" valign="top">Flanking = 400</th><th align="left" valign="top">Flanking = 2000</th><th align="left" valign="top">Flanking = 10,000</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Kernel sizes (W)</bold></td><td align="left" valign="bottom">[11, 11, 11, 11]</td><td align="left" valign="bottom">[11, 11, 11, 11,<break/>11, 11, 11, 11]</td><td align="left" valign="bottom">[11, 11, 11, 11,<break/>11, 11, 11, 11, 21, 21, 21, 21]</td><td align="left" valign="bottom">[11, 11, 11, 11,<break/>11, 11, 11, 11, 21, 21, 21, 21,<break/>41, 41, 41, 41]</td></tr><tr><td align="left" valign="bottom"><bold>Dilated rates (AR)</bold></td><td align="left" valign="bottom">[1, 1, 1, 1]</td><td align="left" valign="bottom">[1, 1, 1, 1,<break/>4, 4,,4,4]</td><td align="left" valign="bottom">[1, 1, 1, 1,<break/>4, 4,,4,4,<break/>10, 10, 10, 10]</td><td align="left" valign="bottom">[1, 1, 1, 1,<break/>4, 4,,4,4,<break/>10, 10, 10, 10,<break/>25, 25, 25, 25]</td></tr><tr><td align="left" valign="bottom"><bold>Residual blocks</bold></td><td align="left" valign="bottom">4</td><td align="left" valign="bottom">8</td><td align="left" valign="bottom">12</td><td align="left" valign="bottom">16</td></tr><tr><td align="left" valign="bottom"><bold>Skip connections</bold></td><td align="left" valign="bottom">1 (inserted after residual block 4)</td><td align="left" valign="bottom">2 (inserted after residual blocks 4 and 8)</td><td align="left" valign="bottom">3 (inserted after residual blocks 4, 8, and 12)</td><td align="left" valign="bottom">4 (inserted after residual blocks 4, 8, 12, and 16)</td></tr><tr><td align="left" valign="bottom"><bold>Cropping length (CL);</bold><break/>(<inline-formula><alternatives><mml:math id="inf68"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mstyle><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:mo>∑</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>R</mml:mi><mml:mo>×</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>W</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft68">\begin{document}$ 2\times \sum \left [AR\times \left (W- 1\right)\right ]$\end{document}</tex-math></alternatives></inline-formula>)</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">400</td><td align="left" valign="bottom">2000</td><td align="left" valign="bottom">10,000</td></tr></tbody></table></table-wrap><p>While model training is performed with fixed-length sequences (e.g. 15,000 nucleotides to produce 5000 prediction positions), the dilated convolutional structure and final Cropping1D layer allow each trained model to process variable-length sequences at inference time. Specifically, any sequence longer than the network’s receptive field can be fed to the network, which will output predictions aligned to the valid region (i.e. the input length minus the cropping region). For flanking regions, users can either pad with N’s or include relevant upstream and downstream genomic context. Memory permitting, this design grants flexibility in analyzing genomic segments of varying lengths in a single pass, without the need to retrain separate models for each new sequence length.</p><p>In total, these architectural components allow OpenSpliceAI to capture sequence contexts spanning up to 10,000 nucleotides. By varying kernel sizes, dilation rates, and cropping, the model can learn both local and long-range patterns important for accurate identification of canonical splice signals.</p><p>For model training, we introduced enhancements to the adaptive learning rate schedule. Specifically, we set a maximum learning rate for the initial 10 epochs and implemented an early stopping criterion to prevent overfitting. The adaptive learning rate decreases by a factor of 0.5 starting from the sixth epoch in the original implementation. Additionally, we incorporated the CosineAnnealingWarmRestarts scheduler to enable periodic learning rate restarts, which can help escape local minima and improve convergence.</p></sec><sec id="s4-10"><title>Evaluation metrics on model performance</title><p>To evaluate both scratch-trained and transfer-trained models, we used outputs from the ‘create-data’ subcommand derived from a test set that included only protein-coding genes and excluded paralogous sequences. For consistency with the SpliceAI study, we held out the same chromosomes for testing. For other species, we reserved approximately 20% of the data for testing and used the remaining 80% for training. These datasets were then used to assess (1) the original SpliceAI models, (2) all OSAI models trained from scratch, and (3) all OSAI models fine-tuned from OSAI<sub>MANE</sub>.</p><sec id="s4-10-1"><title>Top-k accuracy</title><p>In line with the definition used in the SpliceAI paper, we evaluate the model’s performance using the top-k accuracy metric. This metric is computed by examining each dimension of the model’s predictions. For each DNA sequence of length <inline-formula><alternatives><mml:math id="inf69"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>L</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft69">\begin{document}$L$\end{document}</tex-math></alternatives></inline-formula>, the model outputs an <inline-formula><alternatives><mml:math id="inf70"><mml:mi>L</mml:mi><mml:mo>×</mml:mo><mml:mn>3</mml:mn></mml:math><tex-math id="inft70">\begin{document}$L\times 3$\end{document}</tex-math></alternatives></inline-formula> matrix: the first channel indicates non-splice sites, the second indicates acceptor splice sites, and the third indicates donor splice sites.</p><p>Top-k splice site accuracy evaluation is computed as follows: For a gene sequence of length <inline-formula><alternatives><mml:math id="inf71"><mml:mi>L</mml:mi></mml:math><tex-math id="inft71">\begin{document}$L$\end{document}</tex-math></alternatives></inline-formula>, containing 10 true donor splice sites (<inline-formula><alternatives><mml:math id="inf72"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft72">\begin{document}$n_{true}^{donor}=10$\end{document}</tex-math></alternatives></inline-formula>) and 10 true acceptor splice sites (<inline-formula><alternatives><mml:math id="inf73"><mml:msubsup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math><tex-math id="inft73">\begin{document}$n_{true}^{acceptor}=10$\end{document}</tex-math></alternatives></inline-formula>), the model generates probability scores for donor and acceptor sites across the sequence. For top-k accuracy, the <inline-formula><alternatives><mml:math id="inf74"><mml:msubsup><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft74">\begin{document}$k\times n_{true}^{class}$\end{document}</tex-math></alternatives></inline-formula> highest-scoring predictions are extracted per class (donor/acceptor), where <inline-formula><alternatives><mml:math id="inf75"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft75">\begin{document}$n_{true}^{class}=10$\end{document}</tex-math></alternatives></inline-formula> for each splice site type. Class-specific accuracy is calculated as follows (<xref ref-type="disp-formula" rid="equ17">Equation 17</xref>):<disp-formula id="equ17"><label>(17)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mi mathvariant="italic">T</mml:mi><mml:mi mathvariant="italic">o</mml:mi><mml:mi mathvariant="italic">p</mml:mi><mml:mo mathvariant="italic">−</mml:mo><mml:mi mathvariant="italic">k</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munderover><mml:mi>I</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>∈</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>×</mml:mo><mml:msubsup><mml:mi>n</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>p</mml:mi><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle  {\it Top-k}^{class}=\frac{\sum _{i=1}^{k\times n_{true}^{class}}I\left (p_{i}^{class}\in S_{true}^{class}\right)}{k\times n_{true}^{class}},class\in \left \{donor,acceptor\right \}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf76"><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft76">\begin{document}$p_{true}^{class}$\end{document}</tex-math></alternatives></inline-formula> denotes the <italic>i</italic>th ranked prediction for the class, <inline-formula><alternatives><mml:math id="inf77"><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math><tex-math id="inft77">\begin{document}$S_{true}^{class}$\end{document}</tex-math></alternatives></inline-formula> is the set of true splice sites for the class, and <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">I</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$\rm I$\end{document}</tex-math></alternatives></inline-formula> is an indicator function.</p><p>For <inline-formula><alternatives><mml:math id="inf79"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math><tex-math id="inft79">\begin{document}$k=1$\end{document}</tex-math></alternatives></inline-formula>, this evaluates whether true splice sites are present among the top 10 donor and top 10 acceptor predictions (20 total). Accuracy is defined as the proportion of true sites correctly identified within this subset. For <inline-formula><alternatives><mml:math id="inf80"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math><tex-math id="inft80">\begin{document}$k=2$\end{document}</tex-math></alternatives></inline-formula>, the evaluation expands to the top 20 predictions per class (40 total). This metric quantifies the model’s ability to prioritize true splice sites within ranked candidate positions.</p></sec><sec id="s4-10-2"><title>Accuracy, precision, recall, F1-score, and AUPRC</title><p>Unlike top-k accuracy, which requires knowing the number of ground-truth splice sites, we can determine the predicted class for each position based on the highest probability across each dimension and evaluate predictions as true positives (TP), true negatives (TN), false positives (FP), or false negatives (FN) based on a preset threshold. After labeling each site in a given sequence, we calculate accuracy, precision, recall, F1-score, and AUPRC with the threshold 0.5 for the sequence prediction. For example, for donor sites, the metrics are calculated as follows (<xref ref-type="disp-formula" rid="equ18 equ19 equ20">Equations 18–20)</xref>:<disp-formula id="equ18"><label>(18)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>c</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle Accuracy_{donor\,site}=\frac{TP_{donor\,site}+FP_{donor\,site}}{TP_{donor\,site}+FP_{donor\,site}+TN_{donor\,site}+FN_{donor\,site}}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ19"><label>(19)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>F</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle Precision_{donor\,site}=\frac{TP_{donor\,site}}{TP_{donor\,site}+FN_{donor\,site}}$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ20"><label>(20)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>R</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:msub><mml:mi>l</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>T</mml:mi><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>T</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle Recall_{donor\,site}=\frac{TP_{donor\,site}}{TP_{donor\,site}+TN_{donor\,site}}$$\end{document}</tex-math></alternatives></disp-formula></p></sec><sec id="s4-10-3"><title>Benchmarking SpliceAI-Keras and OSAI<sub>MANE</sub></title><p>To compare OSAI<sub>MANE</sub> with SpliceAI (<xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>), we benchmarked the computational efficiency and performance of both tools across various metrics in two tasks: large-scale prediction (using the ‘predict’ module) and variant effect prediction (using the ‘variant’ module). The results are shown in <xref ref-type="fig" rid="fig2">Figure 2G and H</xref>.</p></sec><sec id="s4-10-4"><title>Time and CPU/GPU memory profiling</title><p>The SpliceAI-Keras and OSAI<sub>MANE</sub> were benchmarked using the Scalene profiling tool (<ext-link ext-link-type="uri" xlink:href="https://github.com/plasma-umass/scalene">https://github.com/plasma-umass/scalene</ext-link>; <xref ref-type="bibr" rid="bib7">Berger, 2025</xref>), a Python-specific profiler which handles CPU, GPU, and memory profiling and evaluates code line-by-line. We measured the following metrics: elapsed CPU time, peak CPU memory, peak GPU memory, percentage of CPU time in low-level C code, CPU memory growth rate, and average memory usage. The first three metrics are visualized in <xref ref-type="fig" rid="fig2">Figure 2E</xref>, while the remaining are presented in <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>.</p></sec><sec id="s4-10-5"><title>Predict benchmark design</title><p>The objective of this benchmark was to provide a fair comparison of the computational efficiency of SpliceAI and OSAI<sub>MANE</sub>, with the key difference being that SpliceAI is implemented in Keras (<ext-link ext-link-type="uri" xlink:href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</ext-link>; <xref ref-type="bibr" rid="bib17">Chollet, 2025</xref>) and OSAI<sub>MANE</sub> is implemented in PyTorch (<ext-link ext-link-type="uri" xlink:href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</ext-link>; <xref ref-type="bibr" rid="bib16">Chintala et al., 2025</xref>). While SpliceAI includes a variant effect prediction utility, it lacks a dedicated tool for large-scale predictions. To address this, we extracted the core Keras-wrapped prediction code from SpliceAI’s variant tool and integrated it into our ‘predict’ utility, which has our specific data preprocessing and BED file generation. We call this tool ‘SpliceAI-Keras’.</p><p>For this experiment, we randomly sampled 1000 protein-coding genes from MANE and benchmarked the tools on increasingly large subsets of the genes. The prediction task is to identify all splice sites within the gene locus.</p><p>With the Scalene profiler active, we ran both SpliceAI-Keras and OSAI<sub>MANE</sub> with the ‘predict’ subcommand across all five models (per default usage), extracting the averaged predictions, and repeated the process for a total of 5 trials. We further benchmarked every model size (80, 400, 2000, and 10,000 bp flanking size). The graph visualizations depict the mean metrics as a solid line, while the shaded region represents the variance between trials.</p><p>If the computation encountered an out-of-memory error that caused the prediction to stall, we discarded that trial. Note that running Scalene also requires dedicated memory usage, which was not included in the graphs. Some input sizes yielded no successful trials for SpliceAI-Keras, reflected in missing datapoints.</p></sec><sec id="s4-10-6"><title>Variant benchmark design</title><p>We compared SpliceAI ‘variant’ tool <ext-link ext-link-type="uri" xlink:href="https://github.com/Illumina/SpliceAI">https://github.com/Illumina/SpliceAI</ext-link> (<xref ref-type="bibr" rid="bib43">McRae and Jaganathan, 2025</xref>) with OpenSpliceAI ‘variant’ command (in default mode) in an analogous manner to the ‘predict’ benchmark. This experiment compared every model size across both SpliceAI-Keras and OSAI<sub>MANE</sub> for 5 trials. For the input VCF file, we used the Mills and 1000 Genomes Project gold standard dataset of known indels in <ext-link ext-link-type="uri" xlink:href="https://storage.googleapis.com/genomics-public-data/resources/broad/hg38/v0/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz">GRCh38</ext-link>, provided by the Broad Institute. We randomly sampled 1000 indels and benchmarked on increasingly large subsets of this data. The resultant graph is shown in <xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6G–L</xref><bold>.</bold></p></sec><sec id="s4-10-7"><title>ISM analysis</title><p>The ISM study compares the prediction patterns of OSAI<sub>MANE</sub> with SpliceAI (<xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>) to demonstrate their similarity and biological relevance. The ISM experiments investigate the effect of mutations on predicted splicing patterns, and we replicate several key experiments from <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>, to illustrate this.</p></sec><sec id="s4-10-8"><title>Importance score</title><p>For assessing the impact of a mutation in a given base position on the strength of the splice site, we calculate an ‘importance score’ as follows (<xref ref-type="disp-formula" rid="equ1">Equation 21</xref>):<disp-formula id="equ21"><label>(21)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>I</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>4</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle Importance=S_{ref}- \frac{S_{A}+S_{C}+S_{G}+S_{T}}{4}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf81"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft81">\begin{document}$S_{ref}$\end{document}</tex-math></alternatives></inline-formula> denotes the splice site score of the wild-type sequence, and <inline-formula><alternatives><mml:math id="inf82"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft82">\begin{document}$S_{A}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf83"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft83">\begin{document}$S_{C}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf84"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>G</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft84">\begin{document}$S_{G}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf85"><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft85">\begin{document}$S_{T}$\end{document}</tex-math></alternatives></inline-formula> denote the splice site’s score with each of the corresponding base substitutions. The importance score reflects the decrease in the predicted strength of the splice site when the target nucleotide is mutated and can be regarded as the significance of the target base in contributing toward the activation of the splice site (<xref ref-type="disp-formula" rid="equ20">Equation 20</xref>).</p></sec><sec id="s4-10-9"><title>Single-nucleotide variation in short exons</title><p>Pursuant to the protocol of <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>, we investigate the importance of various base positions around two known short exons in the human genome, U2SURP exon 9 and DST exon 2. We evaluate the scores using both SpliceAI-Keras and OSAI<sub>MANE</sub> to compare the predicted splicing patterns. The findings are summarized in <xref ref-type="fig" rid="fig6">Figure 6A</xref>.</p><p>For each example, we extract the entire exon, as well as part of the intronic region upstream and downstream of the exon to illustrate the full range of base positions. We then mutate each base of the input sequence to the three possible single nucleotide variations at that position. For the reference sequence and each mutated sequence, we collect the acceptor site score and calculate an importance score for that position. We finally visualize the acceptor site importance scores, corresponding to the vertical size of the DNA logo of the reference sequence.</p><p>Note that for the U2SURP investigation, we use the same input sequence as <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>, which is extracted from the hg19 assembly, to ensure reproducibility. The DST exon used is updated with the more recent GRCh38 assembly. DST was selected to ensure representation across the train and test datasets. Additionally, we note that the DST gene is on the reverse strand, but we display the forward strand (the strand that the splicing models use as input) in our visualization for ease of comparison. Lastly, for this experiment and all subsequent ISM studies, we extract an additional 10,000 bp around the input sequence (as opposed to N-padding) and use the SpliceAI-10k and OSAI<sub>MANE</sub>-10k models to perform prediction.</p></sec><sec id="s4-10-10"><title>Concordance evaluation of ISM importance scores between OSAI<sub>MANE</sub> and SpliceAI</title><p>To assess agreement between OSAI<sub>MANE</sub> and SpliceAI across a broad set of splice sites, we applied our ISM procedure to 100 randomly chosen donor sites and 100 randomly chosen acceptor sites. For each site, we extracted a 5001 nt window centered on the annotated splice junction and, at every coordinate within that window, substituted the reference base with each of the three alternative nucleotides. We recorded the change in predicted splice site probability for each mutation and then averaged these Δ-scores at each position to produce a 5001-score ISM importance profile per site.</p><p>Next, for each splice site, we computed the Pearson correlation coefficient between the paired importance profiles from ensembled OSAI<sub>MANE</sub> and ensembled SpliceAI. The median correlation was 0.857 for all splice sites. Ten additional zoom-in representative splice site DNA logo comparisons are provided in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>.</p></sec><sec id="s4-10-11"><title>Batch mutagenesis for donor and acceptor site motif recognition</title><p>To establish a more representative idea of the splicing pattern around splice sites, we scaled up the ISM experiment across multiple donor and acceptor sites. The data consists of 100 randomly sampled donor and 100 randomly sampled acceptor sites from the testing dataset (chromosomes 1, 3, 5, 7, and 9). Each sequence is 400 bp with the donor and acceptor splicing motifs located at the midpoint of the sequence.</p><p>For each position in the sequence, we mutated the base to every other base and measured the decrease in the strength of the central splice site score. Again, the measurement was taken as the average across all five models for both SpliceAI-Keras and OSAI<sub>MANE</sub> (10k model size). We considered each point mutation separately here. We repeated this for every sequence and took the average result across the 100 samples.</p><p>Finally, we displayed the averaged scores in a DNA logo (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), where the vertical size of a base denotes the magnitude of decrease in splice site strength when the original base at that position is mutated to that base. We show the central 80 base positions in our visualization for readability.</p><p>We note that SpliceAI-Keras runs many orders of magnitude slower than OSAI<sub>MANE</sub>. In order to speed up the computation, we split up the input into 10 smaller batches of 10 transcripts each and processed them in parallel on a GPU cluster. The results were aggregated, then averaged.</p></sec><sec id="s4-10-12"><title>Cryptic splicing mutation analysis</title><p>To reproduce another key experiment from <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), we selected a specific G to A point mutation in intron 14 of the MYBPC3 gene which results in a cryptic splicing variant. We further select another example of a cryptic splicing variant that is validated in vitro, from <xref ref-type="bibr" rid="bib52">Qian et al., 2021</xref>. This A to G point mutation occurs deep in intron 16 of OPA1 and was shown to have the highest aberrant-only splicing in minigene assays, with the inclusion of an entire cryptic exon upstream of the mutation.</p><p>For each variant, we scored the wild-type sequence and the mutated sequence and calculated the change in donor and acceptor scores for all bases around the mutation (specifically at the expected locations of cryptic splice site gain), for both SpliceAI-Keras and OSAI<sub>MANE</sub>. We visualized the most significant splice site gain or loss events, along with their raw splice site scores, in <xref ref-type="fig" rid="fig6">Figure 6C</xref>.</p><p>Because the sequence near the acceptor site differs in the MYBPC3 gene between the hg19 assembly (used in <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>) and GRCh38 assembly, we opted to retain the hg19 annotation for this example, for the purpose of reproducing SpliceAI’s results.</p></sec><sec id="s4-10-13"><title>Full prediction of the CFTR gene</title><p>We finally reproduce the full prediction of splice sites in the CFTR gene using both SpliceAI-Keras and OSAI<sub>MANE</sub>. Notably different, however, is the fact that we are using the updated GRCh38 assembly, and that we use a fixed score threshold of 0.5 (default for ‘predict’) to identify splice sites.</p><p>We extracted the full CFTR gene and ran both tools, taking the averaged score across all five models for each tool. The findings are summarized in <xref ref-type="fig" rid="fig6">Figure 6D</xref>. The exon plot displays the reference MANE annotation with the locations of the predicted donor and acceptor sites marked in color. The histogram below visualizes the corresponding donor and acceptor score distributions.</p><p>We further note that we extract the true 10k DNA flanking sequence around the gene (as opposed to N-padding), which is why our results do not appear to exactly replicate the original study from <xref ref-type="bibr" rid="bib25">Jaganathan et al., 2019</xref>.</p></sec></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Formal analysis, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Validation, Investigation, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Validation, Investigation, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-107454-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>OpenSpliceAI is implemented as a Python package, using Pytorch framework. OpenSpliceAI project is freely available on GitHub at: <ext-link ext-link-type="uri" xlink:href="https://github.com/Kuanhao-Chao/OpenSpliceAI">https://github.com/Kuanhao-Chao/OpenSpliceAI</ext-link> (<xref ref-type="bibr" rid="bib15">Chao, 2025</xref>) and is available on PyPi: <ext-link ext-link-type="uri" xlink:href="https://pypi.org/project/openspliceai/">https://pypi.org/project/openspliceai/</ext-link>. The OpenSpliceAI documentation is available at: <ext-link ext-link-type="uri" xlink:href="https://ccb.jhu.edu/openspliceai/">https://ccb.jhu.edu/openspliceai/</ext-link>.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank all the members of the Salzberg and Pertea Labs for their valuable discussions and insights. This research was supported in part by the U.S. National Institutes of Health under grants R01-HG006677, R35-GM130151, and R35-GM156470, and by the U.S. National Science Foundation under DBI 2412449. Computational work was carried out at the Advanced Research Computing at Hopkins (ARCH) core facility, supported in part by NSF grant OAC 1920103.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aartsma-Rus</surname><given-names>A</given-names></name><name><surname>Bremmer-Bout</surname><given-names>M</given-names></name><name><surname>Janson</surname><given-names>AAM</given-names></name><name><surname>den Dunnen</surname><given-names>JT</given-names></name><name><surname>van Ommen</surname><given-names>GJB</given-names></name><name><surname>van Deutekom</surname><given-names>JCT</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Targeted exon skipping as a potential gene correction therapy for Duchenne muscular dystrophy</article-title><source>Neuromuscular Disorders</source><volume>12 Suppl 1</volume><fpage>S71</fpage><lpage>S77</lpage><pub-id pub-id-type="doi">10.1016/s0960-8966(02)00086-x</pub-id><pub-id pub-id-type="pmid">12206800</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Abadi</surname><given-names>M</given-names></name><name><surname>Agarwal</surname><given-names>A</given-names></name><name><surname>Barham</surname><given-names>P</given-names></name><name><surname>Brevdo</surname><given-names>E</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Citro</surname><given-names>C</given-names></name><name><surname>Corrado</surname><given-names>GS</given-names></name><name><surname>Davis</surname><given-names>A</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name><name><surname>Devin</surname><given-names>M</given-names></name><name><surname>Ghemawat</surname><given-names>S</given-names></name><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Harp</surname><given-names>A</given-names></name><name><surname>Irving</surname><given-names>G</given-names></name><name><surname>Isard</surname><given-names>M</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Kaiser</surname><given-names>L</given-names></name><name><surname>Kudlur</surname><given-names>M</given-names></name><name><surname>Levenberg</surname><given-names>J</given-names></name><name><surname>Mane</surname><given-names>D</given-names></name><name><surname>Monga</surname><given-names>R</given-names></name><name><surname>Moore</surname><given-names>S</given-names></name><name><surname>Murray</surname><given-names>D</given-names></name><name><surname>Olah</surname><given-names>C</given-names></name><name><surname>Schuster</surname><given-names>M</given-names></name><name><surname>Shlens</surname><given-names>J</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Talwar</surname><given-names>K</given-names></name><name><surname>Tucker</surname><given-names>P</given-names></name><name><surname>Vanhoucke</surname><given-names>V</given-names></name><name><surname>Vasudevan</surname><given-names>V</given-names></name><name><surname>Viegas</surname><given-names>F</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Warden</surname><given-names>P</given-names></name><name><surname>Wattenberg</surname><given-names>M</given-names></name><name><surname>Wicke</surname><given-names>M</given-names></name><name><surname>Yu</surname><given-names>Y</given-names></name><name><surname>Zheng</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>TensorFlow: Large-scale machine learning on heterogeneous distributed systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1603.04467">https://doi.org/10.48550/arXiv.1603.04467</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agarwal</surname><given-names>V</given-names></name><name><surname>Kelley</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The genetic and biochemical determinants of mRNA degradation rates in mammals</article-title><source>Genome Biology</source><volume>23</volume><elocation-id>245</elocation-id><pub-id pub-id-type="doi">10.1186/s13059-022-02811-x</pub-id><pub-id pub-id-type="pmid">36419176</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alipanahi</surname><given-names>B</given-names></name><name><surname>Delong</surname><given-names>A</given-names></name><name><surname>Weirauch</surname><given-names>MT</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting the sequence specificities of DNA- and RNA-binding proteins by deep learning</article-title><source>Nature Biotechnology</source><volume>33</volume><fpage>831</fpage><lpage>838</lpage><pub-id pub-id-type="doi">10.1038/nbt.3300</pub-id><pub-id pub-id-type="pmid">26213851</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baralle</surname><given-names>FE</given-names></name><name><surname>Giudice</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Alternative splicing as a regulator of development and tissue identity</article-title><source>Nature Reviews. Molecular Cell Biology</source><volume>18</volume><fpage>437</fpage><lpage>451</lpage><pub-id pub-id-type="doi">10.1038/nrm.2017.27</pub-id><pub-id pub-id-type="pmid">28488700</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barash</surname><given-names>Y</given-names></name><name><surname>Calarco</surname><given-names>JA</given-names></name><name><surname>Gao</surname><given-names>W</given-names></name><name><surname>Pan</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>X</given-names></name><name><surname>Shai</surname><given-names>O</given-names></name><name><surname>Blencowe</surname><given-names>BJ</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Deciphering the splicing code</article-title><source>Nature</source><volume>465</volume><fpage>53</fpage><lpage>59</lpage><pub-id pub-id-type="doi">10.1038/nature09000</pub-id><pub-id pub-id-type="pmid">20445623</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Scalene</data-title><version designator="6ff1870">6ff1870</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/plasma-umass/scalene">https://github.com/plasma-umass/scalene</ext-link></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Black</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Protein diversity from alternative splicing</article-title><source>Cell</source><volume>103</volume><fpage>367</fpage><lpage>370</lpage><pub-id pub-id-type="doi">10.1016/S0092-8674(00)00128-8</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blencowe</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Alternative splicing: new insights from global analyses</article-title><source>Cell</source><volume>126</volume><fpage>37</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2006.06.023</pub-id><pub-id pub-id-type="pmid">16839875</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnal</surname><given-names>SC</given-names></name><name><surname>López-Oreja</surname><given-names>I</given-names></name><name><surname>Valcárcel</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Roles and mechanisms of alternative splicing in cancer - implications for care</article-title><source>Nature Reviews. Clinical Oncology</source><volume>17</volume><fpage>457</fpage><lpage>474</lpage><pub-id pub-id-type="doi">10.1038/s41571-020-0350-x</pub-id><pub-id pub-id-type="pmid">32303702</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braunschweig</surname><given-names>U</given-names></name><name><surname>Gueroussov</surname><given-names>S</given-names></name><name><surname>Plocik</surname><given-names>AM</given-names></name><name><surname>Graveley</surname><given-names>BR</given-names></name><name><surname>Blencowe</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic integration of splicing within gene regulatory pathways</article-title><source>Cell</source><volume>152</volume><fpage>1252</fpage><lpage>1269</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2013.02.034</pub-id><pub-id pub-id-type="pmid">23498935</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bridle</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Training stochastic model recognition algorithms as networks can lead to maximum mutual information estimation of parametersProceedings of the 3rd</article-title><conf-name>International Conference on Neural Information Processing Systems, NIPS’89</conf-name><fpage>211</fpage><lpage>217</lpage></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brow</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Allosteric cascade of spliceosome activation</article-title><source>Annual Review of Genetics</source><volume>36</volume><fpage>333</fpage><lpage>360</lpage><pub-id pub-id-type="doi">10.1146/annurev.genet.36.043002.091635</pub-id><pub-id pub-id-type="pmid">12429696</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burnett</surname><given-names>BG</given-names></name><name><surname>Muñoz</surname><given-names>E</given-names></name><name><surname>Tandon</surname><given-names>A</given-names></name><name><surname>Kwon</surname><given-names>DY</given-names></name><name><surname>Sumner</surname><given-names>CJ</given-names></name><name><surname>Fischbeck</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Regulation of SMN protein stability</article-title><source>Molecular and Cellular Biology</source><volume>29</volume><fpage>1107</fpage><lpage>1115</lpage><pub-id pub-id-type="doi">10.1128/MCB.01262-08</pub-id><pub-id pub-id-type="pmid">19103745</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chao</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>OpenSpliceAI</data-title><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Kuanhao-Chao/OpenSpliceAI">https://github.com/Kuanhao-Chao/OpenSpliceAI</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chintala</surname><given-names>S</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Dzhulgakov</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Bialecki</surname><given-names>P</given-names></name><name><surname>Shulga</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Pytorch</data-title><version designator="c5972eb">c5972eb</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/pytorch/pytorch">https://github.com/pytorch/pytorch</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Chollet</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Keras</data-title><version designator="3137cb0">3137cb0</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/keras-team/keras">https://github.com/keras-team/keras</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dlamini</surname><given-names>Z</given-names></name><name><surname>Mokoena</surname><given-names>F</given-names></name><name><surname>Hull</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Abnormalities in alternative splicing in diabetes: therapeutic targets</article-title><source>Journal of Molecular Endocrinology</source><volume>59</volume><fpage>R93</fpage><lpage>R107</lpage><pub-id pub-id-type="doi">10.1530/JME-17-0049</pub-id><pub-id pub-id-type="pmid">28716821</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Fort</surname><given-names>S</given-names></name><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>Lakshminarayanan</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Deep ensembles: A loss landscape perspective</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1912.02757">https://doi.org/10.48550/arXiv.1912.02757</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frilander</surname><given-names>MJ</given-names></name><name><surname>Steitz</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Initial recognition of U12-dependent introns requires both U11/5’ splice-site and U12/branchpoint interactions</article-title><source>Genes &amp; Development</source><volume>13</volume><fpage>851</fpage><lpage>863</lpage><pub-id pub-id-type="doi">10.1101/gad.13.7.851</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fudenberg</surname><given-names>G</given-names></name><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Pollard</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Predicting 3D genome folding from DNA sequence with Akita</article-title><source>Nature Methods</source><volume>17</volume><fpage>1111</fpage><lpage>1117</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-0958-x</pub-id><pub-id pub-id-type="pmid">33046897</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gotthardt</surname><given-names>M</given-names></name><name><surname>Badillo-Lisakowski</surname><given-names>V</given-names></name><name><surname>Parikh</surname><given-names>VN</given-names></name><name><surname>Ashley</surname><given-names>E</given-names></name><name><surname>Furtado</surname><given-names>M</given-names></name><name><surname>Carmo-Fonseca</surname><given-names>M</given-names></name><name><surname>Schudy</surname><given-names>S</given-names></name><name><surname>Meder</surname><given-names>B</given-names></name><name><surname>Grosch</surname><given-names>M</given-names></name><name><surname>Steinmetz</surname><given-names>L</given-names></name><name><surname>Crocini</surname><given-names>C</given-names></name><name><surname>Leinwand</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Cardiac splicing as a diagnostic and therapeutic target</article-title><source>Nature Reviews. Cardiology</source><volume>20</volume><fpage>517</fpage><lpage>530</lpage><pub-id pub-id-type="doi">10.1038/s41569-022-00828-0</pub-id><pub-id pub-id-type="pmid">36653465</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gupta</surname><given-names>S</given-names></name><name><surname>Stamatoyannopoulos</surname><given-names>JA</given-names></name><name><surname>Bailey</surname><given-names>TL</given-names></name><name><surname>Noble</surname><given-names>WS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Quantifying similarity between motifs</article-title><source>Genome Biology</source><volume>8</volume><elocation-id>R24</elocation-id><pub-id pub-id-type="doi">10.1186/gb-2007-8-2-r24</pub-id><pub-id pub-id-type="pmid">17324271</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hinton</surname><given-names>G</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Dean</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distilling the knowledge in a neural network</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1503.02531">https://doi.org/10.48550/arXiv.1503.02531</ext-link></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaganathan</surname><given-names>K</given-names></name><name><surname>Kyriazopoulou Panagiotopoulou</surname><given-names>S</given-names></name><name><surname>McRae</surname><given-names>JF</given-names></name><name><surname>Darbandi</surname><given-names>SF</given-names></name><name><surname>Knowles</surname><given-names>D</given-names></name><name><surname>Li</surname><given-names>YI</given-names></name><name><surname>Kosmicki</surname><given-names>JA</given-names></name><name><surname>Arbelaez</surname><given-names>J</given-names></name><name><surname>Cui</surname><given-names>W</given-names></name><name><surname>Schwartz</surname><given-names>GB</given-names></name><name><surname>Chow</surname><given-names>ED</given-names></name><name><surname>Kanterakis</surname><given-names>E</given-names></name><name><surname>Gao</surname><given-names>H</given-names></name><name><surname>Kia</surname><given-names>A</given-names></name><name><surname>Batzoglou</surname><given-names>S</given-names></name><name><surname>Sanders</surname><given-names>SJ</given-names></name><name><surname>Farh</surname><given-names>KKH</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predicting splicing from primary sequence with deep learning</article-title><source>Cell</source><volume>176</volume><fpage>535</fpage><lpage>548</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2018.12.015</pub-id><pub-id pub-id-type="pmid">30661751</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaynes</surname><given-names>ET</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Information theory and statistical mechanics</article-title><source>Physical Review</source><volume>106</volume><fpage>620</fpage><lpage>630</lpage><pub-id pub-id-type="doi">10.1103/PhysRev.106.620</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>JM</given-names></name><name><surname>Castle</surname><given-names>J</given-names></name><name><surname>Garrett-Engele</surname><given-names>P</given-names></name><name><surname>Kan</surname><given-names>Z</given-names></name><name><surname>Loerch</surname><given-names>PM</given-names></name><name><surname>Armour</surname><given-names>CD</given-names></name><name><surname>Santos</surname><given-names>R</given-names></name><name><surname>Schadt</surname><given-names>EE</given-names></name><name><surname>Stoughton</surname><given-names>R</given-names></name><name><surname>Shoemaker</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Genome-wide survey of human alternative pre-mRNA splicing with exon junction microarrays</article-title><source>Science</source><volume>302</volume><fpage>2141</fpage><lpage>2144</lpage><pub-id pub-id-type="doi">10.1126/science.1090100</pub-id><pub-id pub-id-type="pmid">14684825</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jung</surname><given-names>H</given-names></name><name><surname>Lee</surname><given-names>D</given-names></name><name><surname>Lee</surname><given-names>J</given-names></name><name><surname>Park</surname><given-names>D</given-names></name><name><surname>Kim</surname><given-names>YJ</given-names></name><name><surname>Park</surname><given-names>WY</given-names></name><name><surname>Hong</surname><given-names>D</given-names></name><name><surname>Park</surname><given-names>PJ</given-names></name><name><surname>Lee</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Intron retention is a widespread mechanism of tumor-suppressor inactivation</article-title><source>Nature Genetics</source><volume>47</volume><fpage>1242</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.1038/ng.3414</pub-id><pub-id pub-id-type="pmid">26437032</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Snoek</surname><given-names>J</given-names></name><name><surname>Rinn</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Basset: learning the regulatory code of the accessible genome with deep convolutional neural networks</article-title><source>Genome Research</source><volume>26</volume><fpage>990</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1101/gr.200535.115</pub-id><pub-id pub-id-type="pmid">27197224</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelley</surname><given-names>DR</given-names></name><name><surname>Reshef</surname><given-names>YA</given-names></name><name><surname>Bileschi</surname><given-names>M</given-names></name><name><surname>Belanger</surname><given-names>D</given-names></name><name><surname>McLean</surname><given-names>CY</given-names></name><name><surname>Snoek</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Sequential regulatory activity prediction across chromosomes with convolutional neural networks</article-title><source>Genome Research</source><volume>28</volume><fpage>739</fpage><lpage>750</lpage><pub-id pub-id-type="doi">10.1101/gr.227819.117</pub-id><pub-id pub-id-type="pmid">29588361</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A method for stochastic optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1412.6980">https://doi.org/10.48550/arXiv.1412.6980</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lakshminarayanan</surname><given-names>B</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><chapter-title>Simple and scalable predictive uncertainty estimation using deep ensembles in</chapter-title><person-group person-group-type="editor"><name><surname>Guyon</surname><given-names>I</given-names></name><name><surname>Luxburg</surname><given-names>UV</given-names></name><name><surname>Bengio</surname><given-names>S</given-names></name><name><surname>Wallach</surname><given-names>H</given-names></name><name><surname>Fergus</surname><given-names>R</given-names></name><name><surname>Vishwanathan</surname><given-names>S</given-names></name><name><surname>Garnett</surname><given-names>R</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><publisher-name>Curran Associates, Inc</publisher-name><fpage>6405</fpage><lpage>6416</lpage></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>SCW</given-names></name><name><surname>Abdel-Wahab</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Therapeutic targeting of splicing in cancer</article-title><source>Nature Medicine</source><volume>22</volume><fpage>976</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1038/nm.4165</pub-id><pub-id pub-id-type="pmid">27603132</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Minimap2: pairwise alignment for nucleotide sequences</article-title><source>Bioinformatics</source><volume>34</volume><fpage>3094</fpage><lpage>3100</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/bty191</pub-id><pub-id pub-id-type="pmid">29750242</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>D</given-names></name><name><surname>McIntosh</surname><given-names>CS</given-names></name><name><surname>Mastaglia</surname><given-names>FL</given-names></name><name><surname>Wilton</surname><given-names>SD</given-names></name><name><surname>Aung-Htut</surname><given-names>MT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neurodegenerative diseases: a hotbed for splicing defects and the potential therapies</article-title><source>Translational Neurodegeneration</source><volume>10</volume><elocation-id>16</elocation-id><pub-id pub-id-type="doi">10.1186/s40035-021-00240-7</pub-id><pub-id pub-id-type="pmid">34016162</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Goyal</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Focal loss for dense object detection</article-title><conf-name>IEEE International Conference on Computer Vision</conf-name><pub-id pub-id-type="doi">10.1109/ICCV.2017.324</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lorson</surname><given-names>CL</given-names></name><name><surname>Hahnen</surname><given-names>E</given-names></name><name><surname>Androphy</surname><given-names>EJ</given-names></name><name><surname>Wirth</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A single nucleotide in the SMN gene regulates splicing and is responsible for spinal muscular atrophy</article-title><source>PNAS</source><volume>96</volume><fpage>6307</fpage><lpage>6311</lpage><pub-id pub-id-type="doi">10.1073/pnas.96.11.6307</pub-id><pub-id pub-id-type="pmid">10339583</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>SGDR: Stochastic Gradient Descent with Warm Restarts</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1608.03983">https://doi.org/10.48550/arXiv.1608.03983</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Loshchilov</surname><given-names>I</given-names></name><name><surname>Hutter</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Decoupled weight decay regularization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1711.05101">https://doi.org/10.48550/arXiv.1711.05101</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majewski</surname><given-names>J</given-names></name><name><surname>Ott</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Distribution and characterization of regulatory elements in the human genome</article-title><source>Genome Research</source><volume>12</volume><fpage>1827</fpage><lpage>1836</lpage><pub-id pub-id-type="doi">10.1101/gr.606402</pub-id><pub-id pub-id-type="pmid">12466286</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martí-Gómez</surname><given-names>C</given-names></name><name><surname>Larrasa-Alonso</surname><given-names>J</given-names></name><name><surname>López-Olañeta</surname><given-names>M</given-names></name><name><surname>Villalba-Orero</surname><given-names>M</given-names></name><name><surname>García-Pavía</surname><given-names>P</given-names></name><name><surname>Sánchez-Cabo</surname><given-names>F</given-names></name><name><surname>Lara-Pezzi</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Functional impact and regulation of alternative splicing in mouse heart development and disease</article-title><source>Journal of Cardiovascular Translational Research</source><volume>15</volume><fpage>1239</fpage><lpage>1255</lpage><pub-id pub-id-type="doi">10.1007/s12265-022-10244-x</pub-id><pub-id pub-id-type="pmid">35355220</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClorey</surname><given-names>G</given-names></name><name><surname>Fletcher</surname><given-names>S</given-names></name><name><surname>Wilton</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Splicing intervention for Duchenne muscular dystrophy</article-title><source>Current Opinion in Pharmacology</source><volume>5</volume><fpage>529</fpage><lpage>534</lpage><pub-id pub-id-type="doi">10.1016/j.coph.2005.06.001</pub-id><pub-id pub-id-type="pmid">16085461</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>McRae</surname><given-names>J</given-names></name><name><surname>Jaganathan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>SpliceAI</data-title><version designator="fc33369">fc33369</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/Illumina/SpliceAI">https://github.com/Illumina/SpliceAI</ext-link></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mills</surname><given-names>JD</given-names></name><name><surname>Janitz</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Alternative splicing of mRNA in the molecular pathology of neurodegenerative diseases</article-title><source>Neurobiology of Aging</source><volume>33</volume><elocation-id>1012</elocation-id><pub-id pub-id-type="doi">10.1016/j.neurobiolaging.2011.10.030</pub-id><pub-id pub-id-type="pmid">22118946</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>MJ</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Kennedy</surname><given-names>CJ</given-names></name><name><surname>Silver</surname><given-names>PA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An alternative splicing network links cell-cycle control to apoptosis</article-title><source>Cell</source><volume>142</volume><fpage>625</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2010.07.019</pub-id><pub-id pub-id-type="pmid">20705336</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morales</surname><given-names>J</given-names></name><name><surname>Pujar</surname><given-names>S</given-names></name><name><surname>Loveland</surname><given-names>JE</given-names></name><name><surname>Astashyn</surname><given-names>A</given-names></name><name><surname>Bennett</surname><given-names>R</given-names></name><name><surname>Berry</surname><given-names>A</given-names></name><name><surname>Cox</surname><given-names>E</given-names></name><name><surname>Davidson</surname><given-names>C</given-names></name><name><surname>Ermolaeva</surname><given-names>O</given-names></name><name><surname>Farrell</surname><given-names>CM</given-names></name><name><surname>Fatima</surname><given-names>R</given-names></name><name><surname>Gil</surname><given-names>L</given-names></name><name><surname>Goldfarb</surname><given-names>T</given-names></name><name><surname>Gonzalez</surname><given-names>JM</given-names></name><name><surname>Haddad</surname><given-names>D</given-names></name><name><surname>Hardy</surname><given-names>M</given-names></name><name><surname>Hunt</surname><given-names>T</given-names></name><name><surname>Jackson</surname><given-names>J</given-names></name><name><surname>Joardar</surname><given-names>VS</given-names></name><name><surname>Kay</surname><given-names>M</given-names></name><name><surname>Kodali</surname><given-names>VK</given-names></name><name><surname>McGarvey</surname><given-names>KM</given-names></name><name><surname>McMahon</surname><given-names>A</given-names></name><name><surname>Mudge</surname><given-names>JM</given-names></name><name><surname>Murphy</surname><given-names>DN</given-names></name><name><surname>Murphy</surname><given-names>MR</given-names></name><name><surname>Rajput</surname><given-names>B</given-names></name><name><surname>Rangwala</surname><given-names>SH</given-names></name><name><surname>Riddick</surname><given-names>LD</given-names></name><name><surname>Thibaud-Nissen</surname><given-names>F</given-names></name><name><surname>Threadgold</surname><given-names>G</given-names></name><name><surname>Vatsan</surname><given-names>AR</given-names></name><name><surname>Wallin</surname><given-names>C</given-names></name><name><surname>Webb</surname><given-names>D</given-names></name><name><surname>Flicek</surname><given-names>P</given-names></name><name><surname>Birney</surname><given-names>E</given-names></name><name><surname>Pruitt</surname><given-names>KD</given-names></name><name><surname>Frankish</surname><given-names>A</given-names></name><name><surname>Cunningham</surname><given-names>F</given-names></name><name><surname>Murphy</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A joint NCBI and EMBL-EBI transcript set for clinical genomics and research</article-title><source>Nature</source><volume>604</volume><fpage>310</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1038/s41586-022-04558-8</pub-id><pub-id pub-id-type="pmid">35388217</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naryshkin</surname><given-names>NA</given-names></name><name><surname>Weetall</surname><given-names>M</given-names></name><name><surname>Dakka</surname><given-names>A</given-names></name><name><surname>Narasimhan</surname><given-names>J</given-names></name><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Feng</surname><given-names>Z</given-names></name><name><surname>Ling</surname><given-names>KKY</given-names></name><name><surname>Karp</surname><given-names>GM</given-names></name><name><surname>Qi</surname><given-names>H</given-names></name><name><surname>Woll</surname><given-names>MG</given-names></name><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Zhang</surname><given-names>N</given-names></name><name><surname>Gabbeta</surname><given-names>V</given-names></name><name><surname>Vazirani</surname><given-names>P</given-names></name><name><surname>Bhattacharyya</surname><given-names>A</given-names></name><name><surname>Furia</surname><given-names>B</given-names></name><name><surname>Risher</surname><given-names>N</given-names></name><name><surname>Sheedy</surname><given-names>J</given-names></name><name><surname>Kong</surname><given-names>R</given-names></name><name><surname>Ma</surname><given-names>J</given-names></name><name><surname>Turpoff</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>CS</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Moon</surname><given-names>YC</given-names></name><name><surname>Trifillis</surname><given-names>P</given-names></name><name><surname>Welch</surname><given-names>EM</given-names></name><name><surname>Colacino</surname><given-names>JM</given-names></name><name><surname>Babiak</surname><given-names>J</given-names></name><name><surname>Almstead</surname><given-names>NG</given-names></name><name><surname>Peltz</surname><given-names>SW</given-names></name><name><surname>Eng</surname><given-names>LA</given-names></name><name><surname>Chen</surname><given-names>KS</given-names></name><name><surname>Mull</surname><given-names>JL</given-names></name><name><surname>Lynes</surname><given-names>MS</given-names></name><name><surname>Rubin</surname><given-names>LL</given-names></name><name><surname>Fontoura</surname><given-names>P</given-names></name><name><surname>Santarelli</surname><given-names>L</given-names></name><name><surname>Haehnke</surname><given-names>D</given-names></name><name><surname>McCarthy</surname><given-names>KD</given-names></name><name><surname>Schmucki</surname><given-names>R</given-names></name><name><surname>Ebeling</surname><given-names>M</given-names></name><name><surname>Sivaramakrishnan</surname><given-names>M</given-names></name><name><surname>Ko</surname><given-names>CP</given-names></name><name><surname>Paushkin</surname><given-names>SV</given-names></name><name><surname>Ratni</surname><given-names>H</given-names></name><name><surname>Gerlach</surname><given-names>I</given-names></name><name><surname>Ghosh</surname><given-names>A</given-names></name><name><surname>Metzger</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>SMN2 splicing modifiers improve motor function and longevity in mice with spinal muscular atrophy</article-title><source>Science</source><volume>345</volume><fpage>688</fpage><lpage>693</lpage><pub-id pub-id-type="doi">10.1126/science.1250127</pub-id><pub-id pub-id-type="pmid">25104390</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nikom</surname><given-names>D</given-names></name><name><surname>Zheng</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Alternative splicing in neurodegenerative disease and the promise of RNA therapies</article-title><source>Nature Reviews. Neuroscience</source><volume>24</volume><fpage>457</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1038/s41583-023-00717-6</pub-id><pub-id pub-id-type="pmid">37336982</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Köpf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>PyTorch: An imperative style, high-performance deep learning library</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1912.01703">https://doi.org/10.48550/arXiv.1912.01703</ext-link></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>AA</given-names></name><name><surname>Steitz</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Splicing double: insights from the second spliceosome</article-title><source>Nature Reviews. Molecular Cell Biology</source><volume>4</volume><fpage>960</fpage><lpage>970</lpage><pub-id pub-id-type="doi">10.1038/nrm1259</pub-id><pub-id pub-id-type="pmid">14685174</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Platt</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</article-title><source>Advances in Large Margin Classifiers</source><volume>10</volume><fpage>61</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.7551/mitpress/1113.003.0008</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Qian</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>M</given-names></name><name><surname>Igelman</surname><given-names>AD</given-names></name><name><surname>Jones</surname><given-names>KD</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Goetz</surname><given-names>KE</given-names></name><name><surname>Birch</surname><given-names>DG</given-names></name><name><surname>Yang</surname><given-names>P</given-names></name><name><surname>Pennesi</surname><given-names>ME</given-names></name><name><surname>Chen</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Identification of deep-intronic splice mutations in a large cohort of patients with inherited retinal diseases</article-title><source>Frontiers in Genetics</source><volume>12</volume><elocation-id>647400</elocation-id><pub-id pub-id-type="doi">10.3389/fgene.2021.647400</pub-id><pub-id pub-id-type="pmid">33737949</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Raschka</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Model evaluation, model selection, and algorithm selection in machine learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1811.12808">https://doi.org/10.48550/arXiv.1811.12808</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Segal</surname><given-names>E</given-names></name><name><surname>Widom</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>From DNA sequence to transcriptional behaviour: a quantitative approach</article-title><source>Nature Reviews. Genetics</source><volume>10</volume><fpage>443</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1038/nrg2591</pub-id><pub-id pub-id-type="pmid">19506578</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sokolova</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>KM</given-names></name><name><surname>Hao</surname><given-names>Y</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Deep learning sequence models for transcriptional regulation</article-title><source>Annual Review of Genomics and Human Genetics</source><volume>25</volume><fpage>105</fpage><lpage>122</lpage><pub-id pub-id-type="doi">10.1146/annurev-genom-021623-024727</pub-id><pub-id pub-id-type="pmid">38594933</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Supek</surname><given-names>F</given-names></name><name><surname>Miñana</surname><given-names>B</given-names></name><name><surname>Valcárcel</surname><given-names>J</given-names></name><name><surname>Gabaldón</surname><given-names>T</given-names></name><name><surname>Lehner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Synonymous mutations frequently act as driver mutations in human cancers</article-title><source>Cell</source><volume>156</volume><fpage>1324</fpage><lpage>1335</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2014.01.051</pub-id><pub-id pub-id-type="pmid">24630730</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sveen</surname><given-names>A</given-names></name><name><surname>Kilpinen</surname><given-names>S</given-names></name><name><surname>Ruusulehto</surname><given-names>A</given-names></name><name><surname>Lothe</surname><given-names>RA</given-names></name><name><surname>Skotheim</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Aberrant RNA splicing in cancer; expression changes and driver mutations of splicing factor genes</article-title><source>Oncogene</source><volume>35</volume><fpage>2413</fpage><lpage>2427</lpage><pub-id pub-id-type="doi">10.1038/onc.2015.318</pub-id><pub-id pub-id-type="pmid">26300000</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagner</surname><given-names>N</given-names></name><name><surname>Çelik</surname><given-names>MH</given-names></name><name><surname>Hölzlwimmer</surname><given-names>FR</given-names></name><name><surname>Mertes</surname><given-names>C</given-names></name><name><surname>Prokisch</surname><given-names>H</given-names></name><name><surname>Yépez</surname><given-names>VA</given-names></name><name><surname>Gagneur</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Aberrant splicing prediction across human tissues</article-title><source>Nature Genetics</source><volume>55</volume><fpage>861</fpage><lpage>870</lpage><pub-id pub-id-type="doi">10.1038/s41588-023-01373-3</pub-id><pub-id pub-id-type="pmid">37142848</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>GS</given-names></name><name><surname>Cooper</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Splicing in disease: disruption of the splicing code and the decoding machinery</article-title><source>Nature Reviews. Genetics</source><volume>8</volume><fpage>749</fpage><lpage>761</lpage><pub-id pub-id-type="doi">10.1038/nrg2164</pub-id><pub-id pub-id-type="pmid">17726481</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>ET</given-names></name><name><surname>Sandberg</surname><given-names>R</given-names></name><name><surname>Luo</surname><given-names>S</given-names></name><name><surname>Khrebtukova</surname><given-names>I</given-names></name><name><surname>Zhang</surname><given-names>L</given-names></name><name><surname>Mayr</surname><given-names>C</given-names></name><name><surname>Kingsmore</surname><given-names>SF</given-names></name><name><surname>Schroth</surname><given-names>GP</given-names></name><name><surname>Burge</surname><given-names>CB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Alternative isoform regulation in human tissue transcriptomes</article-title><source>Nature</source><volume>456</volume><fpage>470</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/nature07509</pub-id><pub-id pub-id-type="pmid">18978772</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wassarman</surname><given-names>KM</given-names></name><name><surname>Steitz</surname><given-names>JA</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>The low-abundance U11 and U12 small nuclear ribonucleoproteins (snRNPs) interact to form a two-snRNP complex</article-title><source>Molecular and Cellular Biology</source><volume>12</volume><fpage>1276</fpage><lpage>1285</lpage><pub-id pub-id-type="doi">10.1128/mcb.12.3.1276-1285.1992</pub-id><pub-id pub-id-type="pmid">1372090</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waterston</surname><given-names>RH</given-names></name><name><surname>Lindblad-Toh</surname><given-names>K</given-names></name><name><surname>Birney</surname><given-names>E</given-names></name><name><surname>Rogers</surname><given-names>J</given-names></name><name><surname>Abril</surname><given-names>JF</given-names></name><name><surname>Agarwal</surname><given-names>P</given-names></name><name><surname>Agarwala</surname><given-names>R</given-names></name><name><surname>Ainscough</surname><given-names>R</given-names></name><name><surname>Alexandersson</surname><given-names>M</given-names></name><name><surname>An</surname><given-names>P</given-names></name><name><surname>Antonarakis</surname><given-names>SE</given-names></name><name><surname>Attwood</surname><given-names>J</given-names></name><name><surname>Baertsch</surname><given-names>R</given-names></name><name><surname>Bailey</surname><given-names>J</given-names></name><name><surname>Barlow</surname><given-names>K</given-names></name><name><surname>Beck</surname><given-names>S</given-names></name><name><surname>Berry</surname><given-names>E</given-names></name><name><surname>Birren</surname><given-names>B</given-names></name><name><surname>Bloom</surname><given-names>T</given-names></name><name><surname>Bork</surname><given-names>P</given-names></name><name><surname>Botcherby</surname><given-names>M</given-names></name><name><surname>Bray</surname><given-names>N</given-names></name><name><surname>Brent</surname><given-names>MR</given-names></name><name><surname>Brown</surname><given-names>DG</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Bult</surname><given-names>C</given-names></name><name><surname>Burton</surname><given-names>J</given-names></name><name><surname>Butler</surname><given-names>J</given-names></name><name><surname>Campbell</surname><given-names>RD</given-names></name><name><surname>Carninci</surname><given-names>P</given-names></name><name><surname>Cawley</surname><given-names>S</given-names></name><name><surname>Chiaromonte</surname><given-names>F</given-names></name><name><surname>Chinwalla</surname><given-names>AT</given-names></name><name><surname>Church</surname><given-names>DM</given-names></name><name><surname>Clamp</surname><given-names>M</given-names></name><name><surname>Clee</surname><given-names>C</given-names></name><name><surname>Collins</surname><given-names>FS</given-names></name><name><surname>Cook</surname><given-names>LL</given-names></name><name><surname>Copley</surname><given-names>RR</given-names></name><name><surname>Coulson</surname><given-names>A</given-names></name><name><surname>Couronne</surname><given-names>O</given-names></name><name><surname>Cuff</surname><given-names>J</given-names></name><name><surname>Curwen</surname><given-names>V</given-names></name><name><surname>Cutts</surname><given-names>T</given-names></name><name><surname>Daly</surname><given-names>M</given-names></name><name><surname>David</surname><given-names>R</given-names></name><name><surname>Davies</surname><given-names>J</given-names></name><name><surname>Delehaunty</surname><given-names>KD</given-names></name><name><surname>Deri</surname><given-names>J</given-names></name><name><surname>Dermitzakis</surname><given-names>ET</given-names></name><name><surname>Dewey</surname><given-names>C</given-names></name><name><surname>Dickens</surname><given-names>NJ</given-names></name><name><surname>Diekhans</surname><given-names>M</given-names></name><name><surname>Dodge</surname><given-names>S</given-names></name><name><surname>Dubchak</surname><given-names>I</given-names></name><name><surname>Dunn</surname><given-names>DM</given-names></name><name><surname>Eddy</surname><given-names>SR</given-names></name><name><surname>Elnitski</surname><given-names>L</given-names></name><name><surname>Emes</surname><given-names>RD</given-names></name><name><surname>Eswara</surname><given-names>P</given-names></name><name><surname>Eyras</surname><given-names>E</given-names></name><name><surname>Felsenfeld</surname><given-names>A</given-names></name><name><surname>Fewell</surname><given-names>GA</given-names></name><name><surname>Flicek</surname><given-names>P</given-names></name><name><surname>Foley</surname><given-names>K</given-names></name><name><surname>Frankel</surname><given-names>WN</given-names></name><name><surname>Fulton</surname><given-names>LA</given-names></name><name><surname>Fulton</surname><given-names>RS</given-names></name><name><surname>Furey</surname><given-names>TS</given-names></name><name><surname>Gage</surname><given-names>D</given-names></name><name><surname>Gibbs</surname><given-names>RA</given-names></name><name><surname>Glusman</surname><given-names>G</given-names></name><name><surname>Gnerre</surname><given-names>S</given-names></name><name><surname>Goldman</surname><given-names>N</given-names></name><name><surname>Goodstadt</surname><given-names>L</given-names></name><name><surname>Grafham</surname><given-names>D</given-names></name><name><surname>Graves</surname><given-names>TA</given-names></name><name><surname>Green</surname><given-names>ED</given-names></name><name><surname>Gregory</surname><given-names>S</given-names></name><name><surname>Guigó</surname><given-names>R</given-names></name><name><surname>Guyer</surname><given-names>M</given-names></name><name><surname>Hardison</surname><given-names>RC</given-names></name><name><surname>Haussler</surname><given-names>D</given-names></name><name><surname>Hayashizaki</surname><given-names>Y</given-names></name><name><surname>Hillier</surname><given-names>LW</given-names></name><name><surname>Hinrichs</surname><given-names>A</given-names></name><name><surname>Hlavina</surname><given-names>W</given-names></name><name><surname>Holzer</surname><given-names>T</given-names></name><name><surname>Hsu</surname><given-names>F</given-names></name><name><surname>Hua</surname><given-names>A</given-names></name><name><surname>Hubbard</surname><given-names>T</given-names></name><name><surname>Hunt</surname><given-names>A</given-names></name><name><surname>Jackson</surname><given-names>I</given-names></name><name><surname>Jaffe</surname><given-names>DB</given-names></name><name><surname>Johnson</surname><given-names>LS</given-names></name><name><surname>Jones</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>TA</given-names></name><name><surname>Joy</surname><given-names>A</given-names></name><name><surname>Kamal</surname><given-names>M</given-names></name><name><surname>Karlsson</surname><given-names>EK</given-names></name><name><surname>Karolchik</surname><given-names>D</given-names></name><name><surname>Kasprzyk</surname><given-names>A</given-names></name><name><surname>Kawai</surname><given-names>J</given-names></name><name><surname>Keibler</surname><given-names>E</given-names></name><name><surname>Kells</surname><given-names>C</given-names></name><name><surname>Kent</surname><given-names>WJ</given-names></name><name><surname>Kirby</surname><given-names>A</given-names></name><name><surname>Kolbe</surname><given-names>DL</given-names></name><name><surname>Korf</surname><given-names>I</given-names></name><name><surname>Kucherlapati</surname><given-names>RS</given-names></name><name><surname>Kulbokas</surname><given-names>EJ</given-names></name><name><surname>Kulp</surname><given-names>D</given-names></name><name><surname>Landers</surname><given-names>T</given-names></name><name><surname>Leger</surname><given-names>JP</given-names></name><name><surname>Leonard</surname><given-names>S</given-names></name><name><surname>Letunic</surname><given-names>I</given-names></name><name><surname>Levine</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Li</surname><given-names>M</given-names></name><name><surname>Lloyd</surname><given-names>C</given-names></name><name><surname>Lucas</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>B</given-names></name><name><surname>Maglott</surname><given-names>DR</given-names></name><name><surname>Mardis</surname><given-names>ER</given-names></name><name><surname>Matthews</surname><given-names>L</given-names></name><name><surname>Mauceli</surname><given-names>E</given-names></name><name><surname>Mayer</surname><given-names>JH</given-names></name><name><surname>McCarthy</surname><given-names>M</given-names></name><name><surname>McCombie</surname><given-names>WR</given-names></name><name><surname>McLaren</surname><given-names>S</given-names></name><name><surname>McLay</surname><given-names>K</given-names></name><name><surname>McPherson</surname><given-names>JD</given-names></name><name><surname>Meldrim</surname><given-names>J</given-names></name><name><surname>Meredith</surname><given-names>B</given-names></name><name><surname>Mesirov</surname><given-names>JP</given-names></name><name><surname>Miller</surname><given-names>W</given-names></name><name><surname>Miner</surname><given-names>TL</given-names></name><name><surname>Mongin</surname><given-names>E</given-names></name><name><surname>Montgomery</surname><given-names>KT</given-names></name><name><surname>Morgan</surname><given-names>M</given-names></name><name><surname>Mott</surname><given-names>R</given-names></name><name><surname>Mullikin</surname><given-names>JC</given-names></name><name><surname>Muzny</surname><given-names>DM</given-names></name><name><surname>Nash</surname><given-names>WE</given-names></name><name><surname>Nelson</surname><given-names>JO</given-names></name><name><surname>Nhan</surname><given-names>MN</given-names></name><name><surname>Nicol</surname><given-names>R</given-names></name><name><surname>Ning</surname><given-names>Z</given-names></name><name><surname>Nusbaum</surname><given-names>C</given-names></name><name><surname>O’Connor</surname><given-names>MJ</given-names></name><name><surname>Okazaki</surname><given-names>Y</given-names></name><name><surname>Oliver</surname><given-names>K</given-names></name><name><surname>Overton-Larty</surname><given-names>E</given-names></name><name><surname>Pachter</surname><given-names>L</given-names></name><name><surname>Parra</surname><given-names>G</given-names></name><name><surname>Pepin</surname><given-names>KH</given-names></name><name><surname>Peterson</surname><given-names>J</given-names></name><name><surname>Pevzner</surname><given-names>P</given-names></name><name><surname>Plumb</surname><given-names>R</given-names></name><name><surname>Pohl</surname><given-names>CS</given-names></name><name><surname>Poliakov</surname><given-names>A</given-names></name><name><surname>Ponce</surname><given-names>TC</given-names></name><name><surname>Ponting</surname><given-names>CP</given-names></name><name><surname>Potter</surname><given-names>S</given-names></name><name><surname>Quail</surname><given-names>M</given-names></name><name><surname>Reymond</surname><given-names>A</given-names></name><name><surname>Roe</surname><given-names>BA</given-names></name><name><surname>Roskin</surname><given-names>KM</given-names></name><name><surname>Rubin</surname><given-names>EM</given-names></name><name><surname>Rust</surname><given-names>AG</given-names></name><name><surname>Santos</surname><given-names>R</given-names></name><name><surname>Sapojnikov</surname><given-names>V</given-names></name><name><surname>Schultz</surname><given-names>B</given-names></name><name><surname>Schultz</surname><given-names>J</given-names></name><name><surname>Schwartz</surname><given-names>MS</given-names></name><name><surname>Schwartz</surname><given-names>S</given-names></name><name><surname>Scott</surname><given-names>C</given-names></name><name><surname>Seaman</surname><given-names>S</given-names></name><name><surname>Searle</surname><given-names>S</given-names></name><name><surname>Sharpe</surname><given-names>T</given-names></name><name><surname>Sheridan</surname><given-names>A</given-names></name><name><surname>Shownkeen</surname><given-names>R</given-names></name><name><surname>Sims</surname><given-names>S</given-names></name><name><surname>Singer</surname><given-names>JB</given-names></name><name><surname>Slater</surname><given-names>G</given-names></name><name><surname>Smit</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>DR</given-names></name><name><surname>Spencer</surname><given-names>B</given-names></name><name><surname>Stabenau</surname><given-names>A</given-names></name><name><surname>Stange-Thomann</surname><given-names>N</given-names></name><name><surname>Sugnet</surname><given-names>C</given-names></name><name><surname>Suyama</surname><given-names>M</given-names></name><name><surname>Tesler</surname><given-names>G</given-names></name><name><surname>Thompson</surname><given-names>J</given-names></name><name><surname>Torrents</surname><given-names>D</given-names></name><name><surname>Trevaskis</surname><given-names>E</given-names></name><name><surname>Tromp</surname><given-names>J</given-names></name><name><surname>Ucla</surname><given-names>C</given-names></name><name><surname>Ureta-Vidal</surname><given-names>A</given-names></name><name><surname>Vinson</surname><given-names>JP</given-names></name><name><surname>Von Niederhausern</surname><given-names>AC</given-names></name><name><surname>Wade</surname><given-names>CM</given-names></name><name><surname>Wall</surname><given-names>M</given-names></name><name><surname>Weber</surname><given-names>RJ</given-names></name><name><surname>Weiss</surname><given-names>RB</given-names></name><name><surname>Wendl</surname><given-names>MC</given-names></name><name><surname>West</surname><given-names>AP</given-names></name><name><surname>Wetterstrand</surname><given-names>K</given-names></name><name><surname>Wheeler</surname><given-names>R</given-names></name><name><surname>Whelan</surname><given-names>S</given-names></name><name><surname>Wierzbowski</surname><given-names>J</given-names></name><name><surname>Willey</surname><given-names>D</given-names></name><name><surname>Williams</surname><given-names>S</given-names></name><name><surname>Wilson</surname><given-names>RK</given-names></name><name><surname>Winter</surname><given-names>E</given-names></name><name><surname>Worley</surname><given-names>KC</given-names></name><name><surname>Wyman</surname><given-names>D</given-names></name><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Yang</surname><given-names>S-P</given-names></name><name><surname>Zdobnov</surname><given-names>EM</given-names></name><name><surname>Zody</surname><given-names>MC</given-names></name><name><surname>Lander</surname><given-names>ES</given-names></name><collab>Mouse Genome Sequencing Consortium</collab></person-group><year iso-8601-date="2002">2002</year><article-title>Initial sequencing and comparative analysis of the mouse genome</article-title><source>Nature</source><volume>420</volume><fpage>520</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1038/nature01262</pub-id><pub-id pub-id-type="pmid">12466850</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xiong</surname><given-names>HY</given-names></name><name><surname>Alipanahi</surname><given-names>B</given-names></name><name><surname>Lee</surname><given-names>LJ</given-names></name><name><surname>Bretschneider</surname><given-names>H</given-names></name><name><surname>Merico</surname><given-names>D</given-names></name><name><surname>Yuen</surname><given-names>RKC</given-names></name><name><surname>Hua</surname><given-names>Y</given-names></name><name><surname>Gueroussov</surname><given-names>S</given-names></name><name><surname>Najafabadi</surname><given-names>HS</given-names></name><name><surname>Hughes</surname><given-names>TR</given-names></name><name><surname>Morris</surname><given-names>Q</given-names></name><name><surname>Barash</surname><given-names>Y</given-names></name><name><surname>Krainer</surname><given-names>AR</given-names></name><name><surname>Jojic</surname><given-names>N</given-names></name><name><surname>Scherer</surname><given-names>SW</given-names></name><name><surname>Blencowe</surname><given-names>BJ</given-names></name><name><surname>Frey</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The human splicing code reveals new insights into the genetic determinants of disease</article-title><source>Science</source><volume>347</volume><elocation-id>1254806</elocation-id><pub-id pub-id-type="doi">10.1126/science.1254806</pub-id><pub-id pub-id-type="pmid">25525159</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zadrozny</surname><given-names>B</given-names></name><name><surname>Elkan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers</article-title><conf-name>ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning</conf-name><fpage>609</fpage><lpage>616</lpage></element-citation></ref><ref id="bib65"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zadrozny</surname><given-names>B</given-names></name><name><surname>Elkan</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Transforming classifier scores into accurate multiclass probability estimates</article-title><conf-name>KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining</conf-name><fpage>694</fpage><lpage>699</lpage><pub-id pub-id-type="doi">10.1145/775047.775151</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Predicting effects of noncoding variants with deep learning-based sequence model</article-title><source>Nature Methods</source><volume>12</volume><fpage>931</fpage><lpage>934</lpage><pub-id pub-id-type="doi">10.1038/nmeth.3547</pub-id><pub-id pub-id-type="pmid">26301843</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Theesfeld</surname><given-names>CL</given-names></name><name><surname>Yao</surname><given-names>K</given-names></name><name><surname>Chen</surname><given-names>KM</given-names></name><name><surname>Wong</surname><given-names>AK</given-names></name><name><surname>Troyanskaya</surname><given-names>OG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep learning sequence-based ab initio prediction of variant effects on expression and disease risk</article-title><source>Nature Genetics</source><volume>50</volume><fpage>1171</fpage><lpage>1179</lpage><pub-id pub-id-type="doi">10.1038/s41588-018-0160-6</pub-id><pub-id pub-id-type="pmid">30013180</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>F</given-names></name><name><surname>Qi</surname><given-names>Z</given-names></name><name><surname>Duan</surname><given-names>K</given-names></name><name><surname>Xi</surname><given-names>D</given-names></name><name><surname>Zhu</surname><given-names>Y</given-names></name><name><surname>Zhu</surname><given-names>H</given-names></name><name><surname>Xiong</surname><given-names>H</given-names></name><name><surname>He</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A comprehensive survey on transfer learning</article-title><source>Proceedings of the IEEE</source><volume>109</volume><fpage>43</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1109/JPROC.2020.3004555</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107454.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Koo</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Cold Spring Harbor Laboratory</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> study introduces a modern and accessible PyTorch reimplementation of the widely used SpliceAI model for splice site prediction. The authors provide <bold>convincing</bold> evidence that their OpenSpliceAI implementation matches the performance of the original while improving usability and enabling flexible retraining across species. These advances are likely to be of broad interest to the computational genomics community.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107454.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Chao et al. produced an updated version of the SpliceAI package using modern deep learning frameworks. This includes data preprocessing, model training, direct prediction, and variant effect prediction scripts. They also added functionality for model fine-tuning and model calibration. They convincingly evaluate their newly trained models against those from the original SpliceAI package and investigate how to extend SpliceAI to make predictions in new species. Their comparisons to the original SpliceAI models are convincing on the grounds of model performance and their evaluation of how well the new models match the original's understanding of non-local mutation effects. However, their evaluation of the new calibration functionality would benefit from a more nuanced discussion of the limitations of calibration.</p><p>Strengths</p><p>(1) They provide convincing evidence that their new implementation of SpliceAI matches the performance and mutation effect estimation capabilities of the original model on a similar dataset while benefiting from improved computational efficiencies. This will enable faster prediction and retraining of splicing models for new species as well as easier integration with other modern deep learning tools.</p><p>(2) They produce models with strong performance on non-human model species and a simple well well-documented pipeline for producing models tuned for any species of interest. This will be a boon for researchers working on splicing in these species and make it easy for researchers working on new species to generate their own models.</p><p>(3) Their documentation is clear and abundant. This will greatly aid the ability of others to work with their code base.</p><p>Weaknesses</p><p>(1) Their discussion of their package's calibration functionality does not adequately acknowledge the limitations of model calibration. This is problematic as this is a package intended for general use and users who are not experienced in modeling broadly and the subfield of model calibration specifically may not already understand these limitations. This could lead to serious errors and misunderstandings down the road. A model is not calibrated or uncalibrated in and of itself, only with respect to a specific dataset. In this case they calibrated with respect to the training dataset, a set of canonical transcript annotations. This is a perfectly valid and reasonable dataset to calibrate against. However, this is unlikely to be the dataset the model is applied to in any downstream use case, and this calibration is not guaranteed or expected to hold for any shift in the dataset distribution. For example, in the next section they use ISM based approaches to evaluate which sequence elements the model is sensitive to and their calibration would not be expected to hold for this set of predictions. This issue is particularly worrying in the case of their model because annotation of canonical transcript splice sites is a task that it is unlikely their model will be applied to after training. Much more likely tasks will be things such as predicting the effects of mutations, identification of splice sites that may be used across isoforms beyond just the canonical one, identification of regulatory sequences through ISM, or evaluation of human created sequences for design or evaluation purposes (such as in the context of an MPSA or designing a gene to splice a particular way), we would not expect their calibration to hold in any of these contexts. To resolve this issue, the authors should clarify and discuss this limitation in their paper (and in the relevant sections of the package documentation) to avoid confusing downstream users.</p><p>(2) The clarity of their analysis of mutation effects could be improved with some minor adjustments. While they report median ISM importance correlation it would be helpful to see a histogram of the correlations they observed. Instead of displaying (and calculating correlations using) importance scores of only the reference sequence, showing the importance scores for each nucleotide at each position provides a more informative representation. This would also likely make the plots in 6B clearer.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107454.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The paper by Chao et al offers a reimplantation of the SpliceAI algorithm in PyTorch so that the model can more easily/efficiently be retrained. They apply their new implementation of the SpliceAI algorithm, which they call OpenSpliceAI, to several species and compare it against the original model, showing that the results are very similar and that in some small species pre-training on other species helps improve performance.</p><p>Strengths:</p><p>On the upside, the code runs fine and it is well documented.</p><p>Weaknesses:</p><p>The paper itself does not offer much beyond reimplementing SpliceAI. There is no new algorithm, new analysis, new data, or new insights into RNA splicing. There is not even any comparison to many of the alternative methods that have since been published to surpass SpliceAI. Given that some of the authors are well known with a long history of important contributions, our expectations were admittedly different. Still, we hope some readers will find the new implementation useful.</p><p>Update for the revised version:</p><p>The update includes mostly clarifications for tech questions/comments raised by the other two reviewers. There is no additional analysis/results that changes our above initial assessment of this paper's contribution.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107454.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Chao</surname><given-names>Kuan-Hao</given-names></name><role specific-use="author">Author</role><aff><institution>Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Mao</surname><given-names>Alan</given-names></name><role specific-use="author">Author</role><aff><institution>Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Anqi</given-names></name><role specific-use="author">Author</role><aff><institution>Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Salzberg</surname><given-names>Steven L</given-names></name><role specific-use="author">Author</role><aff><institution>Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Pertea</surname><given-names>Mihaela</given-names></name><role specific-use="author">Author</role><aff><institution>Johns Hopkins University</institution><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>Chao et al. produced an updated version of the SpliceAI package using modern deep learning frameworks. This includes data preprocessing, model training, direct prediction, and variant effect prediction scripts. They also added functionality for model fine-tuning and model calibration. They convincingly evaluate their newly trained models against those from the original SpliceAI package and investigate how to extend SpliceAI to make predictions in new species. While their comparisons to the original SpliceAI models are convincing on the grounds of model performance, their evaluation of how well the new models match the original's understanding of non-local mutation effects is incomplete. Further, their evaluation of the new calibration functionality would benefit from a more nuanced discussion of what set of splice sites their calibration is expected to hold for, and tests in a context for which calibration is needed.</p><p>Strengths:</p><p>(1) They provide convincing evidence that their new implementation of SpliceAI matches the performance of the original model on a similar dataset while benefiting from improved computational efficiencies. This will enable faster prediction and retraining of splicing models for new species as well as easier integration with other modern deep learning tools.</p><p>(2) They produce models with strong performance on non-human model species and a simple, well-documented pipeline for producing models tuned for any species of interest. This will be a boon for researchers working on splicing in these species and make it easy for researchers working on new species to generate their own models.</p><p>(3) Their documentation is clear and abundant. This will greatly aid the ability of others to work with their code base.</p></disp-quote><p>We thank the reviewer for these positive comments.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) The authors' assessment of how much their model retains SpliceAI's understanding of &quot;nonlocal effects of genomic mutations on splice site location and strength&quot; (Figure 6) is not sufficiently supported. Demonstrating this would require showing that for a large number of (non-local) mutations, their model shows the same change in predictions as SpliceAI or that attribution maps for their model and SpliceAI are concordant even at distances from the splice site. Figure 6A comes close to demonstrating this, but only provides anecdotal evidence as it is limited to 2 loci. This could be overcome by summarizing the concordance between ISM maps for the two models and then comparing across many loci. Figure 6B also comes close, but falls short because instead of comparing splicing prediction differences between the models as a function of variants, it compares the average prediction difference as a function of the distance from the splice site. This limits it to only detecting differences in the model's understanding of the local splice site motif sequences. This could be overcome by looking at comparisons between differences in predictions with mutants directly and considering non-local mutants that cause differences in splicing predictions.</p></disp-quote><p>We agree that two loci are insufficient to demonstrate preservation of non-local effects. To address this, we have extended our analysis to a larger set of sites: we randomly sampled 100 donor and 100 acceptor sites, applied our ISM procedure over a 5,001 nt window centered at each site for both models, and computed the ISM map as before. We then calculated the Pearson correlation between the collection of OSAI<sub>MANE</sub> and SpliceAI ISM importance scores. We also created 10 additional ISM maps similar to those in Figure 6A, which are now provided in Figure S23.</p><p>Follow is the revised paragraph in the manuscript’s Results section:</p><p>First, we recreated the experiment from Jaganathan et al. in which they mutated every base in a window around exon 9 of the U2SURP gene and calculated its impact on the predicted probability of the acceptor site. We repeated this experiment on exon 2 of the DST gene, again using both SpliceAI and OSAI<sub>MANE</sub> . In both cases, we found a strong similarity between the resultant patterns between SpliceAI and OSAI<sub>MANE</sub>, as shown in Figure 6A. To evaluate concordance more broadly, we randomly selected 100 donor and 100 acceptor sites and performed the same ISM experiment on each site. The Pearson correlation between SpliceAI and OSAI<sub>MANE</sub> yielded an overall median correlation of 0.857 (see Methods; additional DNA logos in Figure S23).</p><p>To characterize the local sequence features that both models focus on, we computed the average decrease in predicted splice-site probability resulting from each of the three possible singlenucleotide substitutions at every position within 80bp for 100 donor and 100 acceptor sites randomly sampled from the test set (Chromosomes 1, 3, 5, 7, and 9). Figure 6B shows the average decrease in splice site strength for each mutation in the format of a DNA logo, for both tools.</p><p>We added the following text to the Methods section:</p><p>Concordance evaluation of ISM importance scores between OSAI<sub>MANE</sub> and SpliceAI</p><p>To assess agreement between OSAI<sub>MANE</sub> and SpliceAI across a broad set of splice sites, we applied our ISM procedure to 100 randomly chosen donor sites and 100 randomly chosen acceptor sites. For each site, we extracted a 5,001 nt window centered on the annotated splice junction and, at every coordinate within that window, substituted the reference base with each of the three alternative nucleotides. We recorded the change in predicted splice-site probability for each mutation and then averaged these Δ-scores at each position to produce a 5,001-score ISM importance profile per site.</p><p>Next, for each splice site we computed the Pearson correlation coefficient between the paired importance profiles from ensembled OSAI<sub>MANE</sub> and ensembled SpliceAI. The median correlation was 0.857 for all splice sites. Ten additional zoom-in representative splice site DNA logo comparisons are provided in Supplementary Figure S23.</p><disp-quote content-type="editor-comment"><p>(2) The utility of the calibration method described is unclear. When thinking about a calibrated model for splicing, the expectation would be that the models' predicted splicing probabilities would match the true probabilities that positions with that level of prediction confidence are splice sites. However, the actual calibration that they perform only considers positions as splice sites if they are splice sites in the longest isoform of the gene included in the MANE annotation. In other words, they calibrate the model such that the model's predicted splicing probabilities match the probability that a position with that level of confidence is a splice site in one particular isoform for each gene, not the probability that it is a splice site more broadly. Their level of calibration on this set of splice sites may very well not hold to broader sets of splice sites, such as sites from all annotated isoforms, sites that are commonly used in cryptic splicing, or poised sites that can be activated by a variant. This is a particularly important point as much of the utility of SpliceAI comes from its ability to issue variant effect predictions, and they have not demonstrated that this calibration holds in the context of variants. This section could be improved by expanding and clarifying the discussion of what set of splice sites they have demonstrated calibration on, what it means to calibrate against this set of splice sites, and how this calibration is expected to hold or not for other interesting sets of splice sites. Alternatively, or in addition, they could demonstrate how well their calibration holds on different sets of splice sites or show the effect of calibrating their models against different potentially interesting sets of splice sites and discuss how the results do or do not differ.</p></disp-quote><p>We thank the reviewer for highlighting the need to clarify our calibration procedure. Both SpliceAI and OpenSpliceAI are trained on a single “canonical” transcript per gene: SpliceAI on the hg 19 Ensembl/Gencode canonical set and OpenSpliceAI on the MANE transcript set. To calibrate each model, we applied post-hoc temperature scaling, i.e. a single learnable parameter that rescales the logits before the softmax. This adjustment does not alter the model’s ranking or discrimination (AUC/precision–recall) but simply aligns the predicted probabilities for donor, acceptor, and non-splice classes with their observed frequencies. As shown in our reliability diagrams (Fig. S16-S22), temperature scaling yields negligible changes in performance, confirming that both SpliceAI and OpenSpliceAI were already well-calibrated. However, we acknowledge that we didn’t measure how calibration might affect predictions on non-canonical splice sites or on cryptic splicing. It is possible that calibration might have a detrimental effect on those, but because this is not a key claim of our paper, we decided not to do further experiments. We have updated the manuscript to acknowledge this potential shortcoming; please see the revised paragraph in our next response.</p><disp-quote content-type="editor-comment"><p>(3) It is difficult to assess how well their calibration method works in general because their original models are already well calibrated, so their calibration method finds temperatures very close to 1 and only produces very small and hard to assess changes in calibration metrics. This makes it very hard to distinguish if the calibration method works, as it doesn't really produce any changes. It would be helpful to demonstrate the calibration method on a model that requires calibration or on a dataset for which the current model is not well calibrated, so that the impact of the calibration method could be observed.</p></disp-quote><p>It’s true that the models we calibrated didn’t need many changes. It is possible that the calibration methods we used (which were not ours, but which were described in earlier publications) can’t improve the models much. We toned down our comments about this procedure, as follows.</p><p>Original:</p><p>“Collectively, these results demonstrate that OSAIs were already well-calibrated, and this consistency across species underscores the robustness of OpenSpliceAI’s training approach in diverse genomic contexts.”</p><p>Revised:</p><p>“We observed very small changes after calibration across phylogenetically diverse species, suggesting that OpenSpliceAI’s training regimen yielded well‐calibrated models, although it is possible that a different calibration algorithm might produce further improvements in performance.”</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>The paper by Chao et al offers a reimplementation of the SpliceAI algorithm in PyTorch so that the model can more easily/efficiently be retrained. They apply their new implementation of the SpliceAI algorithm, which they call OpenSpliceAI, to several species and compare it against the original model, showing that the results are very similar and that in some small species, pretraining on other species helps improve performance.</p><p>Strengths:</p><p>On the upside, the code runs fine, and it is well documented.</p><p>Weaknesses:</p><p>The paper itself does not offer much beyond reimplementing SpliceAI. There is no new algorithm, new analysis, new data, or new insights into RNA splicing. There is no comparison to many of the alternative methods that have since been published to surpass SpliceAI. Given that some of the authors are well-known with a long history of important contributions, our expectations were admittedly different. Still, we hope some readers will find the new implementation useful.</p></disp-quote><p>We thank the reviewer for the feedback. We have clarified that OpenSpliceAI is an open-source PyTorch reimplementation optimized for efficient retraining and transfer learning, designed to analyze cross-species performance gains, and supported by a thorough benchmark and the release of several pretrained models to clearly position our contribution.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public review):</bold></p><p>Summary:</p><p>The authors present OpenSpliceAI, a PyTorch-based reimplementation of the well-known SpliceAI deep learning model for splicing prediction. The core architecture remains unchanged, but the reimplementation demonstrates convincing improvements in usability, runtime performance, and potential for cross-species application.</p><p>Strengths:</p><p>The improvements are well-supported by comparative benchmarks, and the work is valuable given its strong potential to broaden the adoption of splicing prediction tools across computational and experimental biology communities.</p><p>Major comments:</p><p>Can fine-tuning also be used to improve prediction for human splicing? Specifically, are models trained on other species and then fine-tuned with human data able to perform better on human splicing prediction? This would enhance the model's utility for more users, and ideally, such fine-tuned models should be made available.</p></disp-quote><p>We evaluated transfer learning by fine-tuning models pretrained on mouse (OSAI<sub>Mouse</sub>), honeybee (OSAI<sub>Honeybee</sub>), Arabidopsis (OSAI<sub>Arabidopsis</sub>), and zebrafish (OSAI<sub>Zebrafish</sub>) on human data. While transfer learning accelerated convergence compared to training from scratch, the final human splicing prediction accuracy was comparable between fine-tuned and scratch-trained models, suggesting that performance on our current human dataset is nearing saturation under this architecture.</p><p>We added the following paragraph to the Discussion section:</p><p>We also evaluated pretraining on mouse (OSAI<sub>Mouse</sub>), honeybee (OSAI<sub>Honeybee</sub>), zebrafish (OSAI<sub>Zebrafish</sub>), and Arabidopsis (OSAI<sub>Arabidopsis</sub>) followed by fine-tuning on the human MANE dataset. While cross-species pretraining substantially accelerated convergence during fine-tuning, the final human splicing-prediction accuracy was comparable to that of a model trained from scratch on human data. This result indicates that our architecture seems to capture all relevant splicing features from human training data alone, and thus gains little or no benefit from crossspecies transfer learning in this context (see Figure S24).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p></disp-quote><p>We thank the editor for summarizing the points raised by each reviewer. Below is our point-bypoint response to each comment:</p><disp-quote content-type="editor-comment"><p>(1) In Figure 3 (and generally in the other figures) OpenSpliceAI should be replaced with OSAI_{Training dataset} because otherwise it is hard to tell which precise model is being compared. And in Figure 3 it is especially important to emphasize that you are comparing a SpliceAI model trained on Human data to an OSAI model trained and evaluated on a different species.</p></disp-quote><p>We have updated the labels in Figures 3, replacing “OpenSpliceAI” with “OSAI_{training dataset}” to more clearly specify which model is being compared.</p><disp-quote content-type="editor-comment"><p>(2) Are genes paralogous to training set genes removed from the validation set as well as the test set? If you are worried about data leakage in the test set, it makes sense to also consider validation set leakage.</p></disp-quote><p>Thank you for this helpful suggestion. We fully agree, and to avoid any data leakage we implemented the identical filtering pipeline for both validation and test sets: we excluded all sequences paralogous or homologous to sequences in the training set, and further removed any sequence sharing &gt; 80 % length overlap and &gt; 80 % sequence identity with training sequences. The effect of this filtering on the validation set is summarized in Supplementary Figure S7C.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations for the authors):</bold></p><p>(1) The legend in Figure 3 is somewhat confusing. The labels like &quot;SpliceAI-Keras (species name)&quot; may imply that the model was retrained using data from that species, but that's not the case, correct?</p></disp-quote><p>Yes, “SpliceAI-Keras (species name)” was not retrained; it refers to the released SpliceAI model evaluated on the specified species dataset. We have revised the Figure 3 legends, changing “SpliceAI-Keras (species name)” to “SpliceAI-Keras” to clarify this.</p><disp-quote content-type="editor-comment"><p>(2) Please address the minor issues with the code, including ensuring the conda install works across various systems.</p></disp-quote><p>We have addressed the issues you mentioned. OpenSpliceAI is now available on Conda and can be installed with: conda install openspliceai.</p><p>The conda package homepage is at: <ext-link ext-link-type="uri" xlink:href="https://anaconda.org/khchao/openspliceai">https://anaconda.org/khchao/openspliceai</ext-link> We’ve also corrected all broken links in the documentation.</p><disp-quote content-type="editor-comment"><p>(3) Utility:</p><p>I followed all the steps in the Quick Start Guide, and aside from the issues mentioned below, everything worked as expected.</p><p>I attempted installation using conda as described in the instructions, but it was unsuccessful. I assume this method is not yet supported.</p><p>In Quick Start Guide: predict, the link labeled &quot;GitHub (models/spliceai-mane/10000nt/)&quot; appears to be incorrect. The correct path is likely &quot;GitHub (models/openspliceaimane/10000nt/)&quot;.</p><p>In Quick Start Guide: variant (<ext-link ext-link-type="uri" xlink:href="https://ccb.jhu.edu/openspliceai/content/quick_start_guide/quickstart_variant.html#quick-startvariant">https://ccb.jhu.edu/openspliceai/content/quick_start_guide/quickstart_variant.html#quick-startvariant</ext-link>), some of the download links for input files were broken. While I was able to find some files in the GitHub repository, I think the -A option should point to data/grch37.txt, not examples/data/input.vcf, and the -I option should be examples/data/input.vcf, not data/vcf/input.vcf.</p></disp-quote><p>Thank you for catching these issues. We’ve now addressed all issues concerning Conda installation and file links. We thank the editor for thoroughly testing our code and reviewing the documentation.</p></body></sub-article></article>