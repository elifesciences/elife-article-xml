<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">84296</article-id><article-id pub-id-type="doi">10.7554/eLife.84296</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Using adversarial networks to extend brain computer interface decoding accuracy over time</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-158494"><name><surname>Ma</surname><given-names>Xuan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3352-1905</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-299104"><name><surname>Rizzoglio</surname><given-names>Fabio</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6744-4605</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-317815"><name><surname>Bodkin</surname><given-names>Kevin L</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-194164"><name><surname>Perreault</surname><given-names>Eric</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-33304"><name><surname>Miller</surname><given-names>Lee E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8675-7140</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-209883"><name><surname>Kennedy</surname><given-names>Ann</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3782-0518</contrib-id><email>ann.kennedy@northwestern.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Department of Neuroscience, Northwestern University</institution></institution-wrap><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Department of Biomedical Engineering, Northwestern University</institution></institution-wrap><addr-line><named-content content-type="city">Evanston</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Department of Physical Medicine and Rehabilitation, Northwestern University</institution></institution-wrap><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Shirley Ryan AbilityLab</institution><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>08</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e84296</elocation-id><history><date date-type="received" iso-8601-date="2022-10-18"><day>18</day><month>10</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-08-01"><day>01</day><month>08</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-08-26"><day>26</day><month>08</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.26.504777"/></event></pub-history><permissions><copyright-statement>© 2023, Ma, Rizzoglio et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Ma, Rizzoglio et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-84296-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-84296-figures-v1.pdf"/><abstract><p>Existing intracortical brain computer interfaces (iBCIs) transform neural activity into control signals capable of restoring movement to persons with paralysis. However, the accuracy of the ‘decoder’ at the heart of the iBCI typically degrades over time due to turnover of recorded neurons. To compensate, decoders can be recalibrated, but this requires the user to spend extra time and effort to provide the necessary data, then learn the new dynamics. As the recorded neurons change, one can think of the underlying movement intent signal being expressed in changing coordinates. If a mapping can be computed between the different coordinate systems, it may be possible to stabilize the original decoder’s mapping from brain to behavior without recalibration. We previously proposed a method based on Generalized Adversarial Networks (GANs), called ‘Adversarial Domain Adaptation Network’ (ADAN), which aligns the distributions of latent signals within underlying low-dimensional neural manifolds. However, we tested ADAN on only a very limited dataset. Here we propose a method based on Cycle-Consistent Adversarial Networks (Cycle-GAN), which aligns the distributions of the full-dimensional neural recordings. We tested both Cycle-GAN and ADAN on data from multiple monkeys and behaviors and compared them to a third, quite different method based on Procrustes alignment of axes provided by Factor Analysis. All three methods are unsupervised and require little data, making them practical in real life. Overall, Cycle-GAN had the best performance and was easier to train and more robust than ADAN, making it ideal for stabilizing iBCI systems over time.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>brain-computer interface</kwd><kwd>motor control</kwd><kwd>EMG</kwd><kwd>unsupervised learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01 NS053603</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Lee E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>R01 NS074044</award-id><principal-award-recipient><name><surname>Miller</surname><given-names>Lee E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Incorporating cycle-consistency loss into a generative adversarial network creates a high-performance and robust 'aligner' of neural population activity, permitting a fixed intracortical brain computer interface to be used for months without recalibration and without requiring inference of a latent manifold.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Intracortical brain-computer interfaces (iBCIs) aim to restore motor function in people with paralysis by transforming neural activity recorded from motor areas of the brain into an estimate of the user’s movement intent. This transformation is accomplished using a neural ‘decoder’, an algorithm that translates the moment-to-moment activity of a population of neurons into a signal used to control intended movements. There has been substantial improvement in our ability to record and decode from large populations of neurons in the past decade, which allows more information to be extracted from the brain and conveyed to the external effectors of the iBCI. However, the long-term stability of iBCIs is still far from satisfactory due in part to the instabilities in neural recordings. The relative micromotion between the electrode tip and the brain tissue (<xref ref-type="bibr" rid="bib49">Sussillo et al., 2016b</xref>), the changes of regional extracellular environment (<xref ref-type="bibr" rid="bib44">Perge et al., 2013</xref>), or even the active and inactive state shifts of neurons (<xref ref-type="bibr" rid="bib52">Volgushev et al., 2006</xref>) could contribute to such instabilities, resulting in the turnover of signals picked by the chronically implanted electrodes on a time scale of days or even a few hours (<xref ref-type="bibr" rid="bib9">Downey et al., 2018</xref>). Given these changes, a decoder could produce inaccurate predictions of the user’s intent leading to the degraded iBCI performance.</p><p>To counteract these effects, a neural decoder might be recalibrated with newly acquired data. A disadvantage of this strategy is that during recalibration, normal use would be interrupted. Furthermore, the recalibration process likely means the user would need to learn the dynamics of the new decoder, imposing additional time and cognitive burden. For persons with paralysis to live more independently, an ideal iBCI would accommodate the gradual drift in neural recordings without supervision, thereby minimizing the need to periodically learn new decoders. For the performance of the initial ‘day-0’ decoder to be maintained, an additional component, an “input stabilizer”, would need to be added to transform the neural recordings made on a later day (‘day-k’) such that they take on the statistics of the day-0 recordings.</p><p>Recently there has been a great deal of interest in the concept of a low-dimensional neural manifold embedded within the neural space that is defined by the full set of recorded neurons, and the ‘latent signals’ that can be computed in it (<xref ref-type="bibr" rid="bib14">Gallego et al., 2017</xref>). A previous paper from our group demonstrated that by aligning the day-k and day-0 latent signals using canonical correlation analysis (CCA), the performance of a fixed day-0 decoder could be maintained over months and even years, despite turnover of the neural recordings.</p><p>Unfortunately, CCA has a couple significant limitations. For one, it is a linear process, not able to account for the nonlinear mappings that have been demonstrated between high-dimensional neural recordings and their low-dimensional manifolds (<xref ref-type="bibr" rid="bib2">Altan et al., 2021</xref>; <xref ref-type="bibr" rid="bib38">Naufel et al., 2019</xref>). Also, its use in a real-life scenario would be cumbersome. This application of CCA can be thought of as rotating two sets of neural signals ‘spatially’ to achieve optimal overlap (and thus temporal correlation). To do so requires cropping or resampling the single-trial data of behaviors on day-0 and day-k such that the paired trials correspond to the same behavior and contain the same number of timepoints, start condition, and end condition. Without trial-alignment, no amount of spatial rotation will achieve a correlation between the neural signals. However, motor behaviors in daily life are typically not well structured, with well-defined onsets and offsets, making trial alignment difficult, if not impossible. Where this method has been used successfully, it has been with highly stereotypic behaviors with distinct trial structure.</p><p>Another recently published linear method for decoder stabilization uses a Procrustes-based (<xref ref-type="bibr" rid="bib19">Gower and Dijksterhuis, 2004</xref>) alignment on low-dimensional manifolds obtained from the neural activity using Factor Analysis (<xref ref-type="bibr" rid="bib8">Degenhart et al., 2020</xref>). This approach, which we will refer to as ‘Procrustes Alignment of Factors’ (PAF), successfully stabilized online iBCI cursor control with a fixed decoder. Trial alignment is not needed for PAF, as it aligns the coordinate axes for the manifolds directly. However, it does require a subset of the coordinate axes in which the manifold is embedded (the neural recording channels) to be unchanged between days 0 and k. Furthermore, the use of a Procrustes-based transformation means that this strategy cannot correct for nonlinear changes in the neural manifold across days.</p><p>In another approach to decoder stabilization, we view changes in neural recordings as arbitrary shifts in the distribution of population firing rates. From this perspective, the reason for poor cross-day performance of decoders is clear: a decoder that is trained only on observations from a given distribution (e.g. those of ‘day-0’) won’t perform well on data from other distributions (i.e. ‘day-k’). A machine learning approach termed ‘domain adaptation’ has been used to cope with such distribution mismatches by learning a transformation that minimizes the difference between the transformed distributions; this permits a model trained on one distribution to generalize to another (<xref ref-type="bibr" rid="bib11">Farahani et al., 2021</xref>; <xref ref-type="bibr" rid="bib40">Pan et al., 2011</xref>). For example, if we have a classifier trained to distinguish photos of objects, domain adaptation could be used to transform drawings of those objects into ‘photo-like’ equivalents, so that the existing photo-based classifier could be used to distinguish the drawn objects.</p><p>Domain adaptation can be implemented with Generative Adversarial Networks (GANs; <xref ref-type="bibr" rid="bib18">Goodfellow et al., 2014</xref>). GANs use two networks – a generator trained to transform a source distribution into a target distribution, and a discriminator trained to do the opposite: determine whether a given distribution is real or synthesized by the generator. The adversarial nature of the generator and discriminator enables the model to be trained in an unsupervised manner (<xref ref-type="bibr" rid="bib16">Ganin and Lempitsky, 2015</xref>; <xref ref-type="bibr" rid="bib50">Tzeng et al., 2017</xref>). GAN-based domain adaptation has been applied to computer vision problems, like adapting a classifier trained to recognize the digits of one style for use in recognizing those of another style (<xref ref-type="bibr" rid="bib50">Tzeng et al., 2017</xref>), or translating images in the style of one domain to another (e.g. colorizing black-and-white photos, <xref ref-type="bibr" rid="bib25">Isola et al., 2017</xref>).</p><p>We recently developed an approach we named Adversarial Domain Adaptation Network (ADAN; <xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>), that used a GAN to perform domain adaptation to enable a fixed day-0 iBCI decoder to work accurately on input signals recorded on day-k. ADAN finds low-dimensional manifolds using a nonlinear autoencoder, and aligns the empirical distribution of the day-k recordings (the source domain) to those of day-0 (the target domain) by aligning the distributions of residuals (as in <xref ref-type="bibr" rid="bib54">Zhao et al., 2016</xref>) between neural firing rates and their nonlinear autoencoder reconstructions (that is, the portion of neurons’ activity not predicted from the manifold). Note that, compared to PAF, ADAN performs the alignment in the high-dimensional space of reconstructed firing rates, but requires the computation of a low-dimensional manifold to do so. In the earlier study we found that ADAN outperforms both CCA and an alignment process that minimized the KL divergence between the distributions of the day-k and day-0 latent spaces (Kullback-Leibler Divergence Minimization, KLDM; <xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>). However, ADAN was only tested on data from a single monkey and a single task, for just 2 weeks. Our subsequent exploration into applying ADAN to other datasets suggests that, while it can work in other settings, its performance is quite sensitive to model hyperparameter settings. This is consistent with previous reports that GANs can be highly dependent on choice of architecture and a variety of hyperparameter settings (<xref ref-type="bibr" rid="bib12">Farnia and Ozdaglar, 2020</xref>). We therefore sought alternative GAN-based approaches that might offer more robust performance.</p><p>Recently, <xref ref-type="bibr" rid="bib55">Zhu et al., 2017</xref> developed a novel GAN architecture named Cycle-Consistent Adversarial Networks (Cycle-GAN) in the context of image domain adaptation. Cycle-GAN introduced a mechanism termed ‘cycle-consistency’, which helps to regularize model performance. Specifically, Cycle-GAN implements both forward and inverse mappings between a pair of domains: the forward mapping translates data in the source domain to the target domain, while the inverse mapping brings the translated data back to the source domain. This regularization mechanism forces the learned transformation between the source and the target distributions to be a bijection, thereby reducing the search space of possible transformations (<xref ref-type="bibr" rid="bib1">Almahairi et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">Zhu et al., 2017</xref>).</p><p>In addition to its promise of greater robustness, Cycle-GAN is to our knowledge unique among neural alignment methods in that it does not rely on projection of neural population activity to a low-dimensional manifold: rather, it aligns the full-dimensional distributions of the day-0 and day-k recordings directly. Other alignment methods that we have explored (CCA, PAF, KLDM, and ADAN) all work with low-dimensional latent signals. Aligning on full-dimensional data leads to the advantage that the (small) information loss caused by dimensionality reduction can be avoided. Furthermore, as most existing iBCI decoders are computed directly from the full-dimensional neural recordings, no extra transformation of neural recordings is required between alignment and decoding.</p><p>In this study, we compare Cycle-GAN, ADAN, and PAF using datasets from several monkeys, spanning a broad variety of motor behaviors, and spanning several months. We chose not to test CCA, as it requires trial alignment of the data, and it (as well as KLDM) was outperformed by ADAN in our earlier study (<xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>). We found that both GAN-based methods outperformed PAF. We also demonstrated that the addition of cycle-consistency improved the alignment and made training much less dependent on hyperparameters.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Performance of a well-calibrated iBCI decoder declines over time</title><p>We trained six monkeys to perform five tasks: power and key grasping, center-out target reaching using isometric wrist torque, and center-out and random-target reaching movements (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). After training, each monkey was implanted with a 96-channel microelectrode array in either the hand or arm area of M1. Four animals (monkeys J, S, G, P) were also implanted with intramuscular leads in forearm and hand muscles contralateral to the cortical implant; these were used to record electromyograms (EMGs). We recorded multi-unit activity on each M1 electrode together with motor output (EMGs and/or hand trajectories) for many sessions across multiple days. All recording sessions for a specific task and an individual monkey were taken together to form a dataset. We collected a total of seven datasets, and the recording sessions in each of them spanned from ~30 to~100 days (See Materials and methods; <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>).</p><p>As in previous studies (<xref ref-type="bibr" rid="bib15">Gallego et al., 2020</xref>; <xref ref-type="bibr" rid="bib49">Sussillo et al., 2016b</xref>), we found substantial instability in the M1 neurons we recorded over time, even though the motor outputs and task performance were generally stable (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplements 2</xref> and <xref ref-type="fig" rid="fig2s3">3</xref>). We first asked how this instability affected the performance of an iBCI decoder. We fit a Wiener filter decoder with data recorded on a reference day (designated ‘day-0’; <xref ref-type="fig" rid="fig1">Figure 1A</xref>). We then used this decoder to predict the motor outputs from M1 neural recordings on later days (‘day-k’) and computed the coefficient of determination (R<sup>2</sup>) between the predictions and the actual data (see Materials and methods). <xref ref-type="fig" rid="fig2">Figure 2</xref> shows example predictions from each task. In all cases, both EMG (top row) and kinematic (bottom row) decoders could reconstruct movement trajectories with high accuracy on held-out trials from the day of training (‘day-0’). However, the calibrated day-0 decoders consistently failed to predict EMGs or hand trajectories accurately on day-k. The degradation of the performance across time occurred for all behavioral tasks and monkeys, and could be substantial even a few days after decoder training (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Setup for stabilizing an intracortical brain computer interface (iBCI) with adversarial domain adaptation.</title><p>(<bold>A</bold>) Initial iBCI decoder training on day-0. The decoder is computed to predict the motor outputs from neural signals, using either the full-dimensional neural recordings or the low-dimensional latent signals obtained through dimensionality reduction. This decoder will remain fixed over time after training. (<bold>B</bold>) A general framework for adversarial domain adaptation training on a subsequent day-k. The ‘Generator’ (G) is a feedforward neural network that takes day-k neural signals as the inputs and aims to transform them into a form similar to day-0 signals; we also refer to G as the ‘aligner’. The ‘Discriminator’ (D) is another feedforward neural network that takes both the outputs of G (aligned day-k neural signals) and day-0 neural signals as the inputs and aims to discriminate between them. (<bold>C</bold>) A trained aligner and the fixed day-0 decoder are used for iBCI decoding on day-k. The aligned signals generated by G are fed to the day-0 decoder to produce the predicted motor outputs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig1-v1.tif"/></fig><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The performance of well-calibrated decoders declines over time.</title><p>(<bold>A</bold>) Actual EMGs (black) and predicted EMGs (orange) using the day-0 decoder for flexor carpi ulnaris (FCU) and extensor carpi radialis longus (ECRl) during the isometric wrist task. (<bold>B</bold>) Actual and predicted EMGs using the day-0 decoder for flexor digitorum profundus (FDP) and first dorsal interosseous (1DI) during the power grasp task. (<bold>C</bold>) Actual hand trajectories and predictions using the day-0 decoder during the center-out (CO) reach task. Colors represent different reaching directions. (<bold>D</bold>) Actual and predicted hand trajectories using the day-0 decoder during the random-target (RT) reach task. Colors represent different reaching directions.</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Table summarizing the datasets analyzed in this paper, including cortical implant site and date, number of recording sessions, number of days between recording start and end, recording days relative to time of array implantation, and motor outputs (EMG or hand velocities) recorded.</title></caption><media mimetype="application" mime-subtype="docx" xlink:href="elife-84296-fig2-data1-v1.docx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Behavior tasks.</title><p>(<bold>A</bold>) The structure of the isometric wrist task. Each trial started with the appearance of a center target requiring the monkeys to hold for a random time (0.2–1.0 s), after which one of eight possible outer targets selected in a block-randomized fashion appeared, accompanied with an auditory go cue. The monkey was allowed to move the cursor to the target within 2.0 s and hold for 0.8 s to receive a liquid reward. (<bold>B</bold>) The structure of the grasping tasks. At the beginning of each trial the monkey was required to keep the hand resting on a touch pad for a random time (0.5–1.0 s). A successful holding triggered the onset of one of three possible rectangular targets on the screen and an auditory go cue. The monkey was required to place the cursor into the target and hold for 0.6 s by increasing and maintaining the grasping force applied on the gadget. (<bold>C</bold>) The structure of the center-out (CO) reach task. At the beginning of each trial, the monkey needed to move the hand to the center of the workspace. One of eight possible outer targets equally spaced in a circle was presented to the monkey after a random waiting period. The monkey needed to keep holding for a variable delay period until receiving an auditory go cue. To receive a liquid reward, the monkey was required to reach the outer target within 1.0 s and hold within the target for 0.5 s. (<bold>D</bold>) The structure of the random-target (RT) reach task. At the beginning of each trial the monkey also needed to move the hand to the center of the workspace. Three targets were then presented to the monkey sequentially, and the monkey was required to move the cursor into each of them within 2.0 s after viewing each target. The positions of these targets were randomly selected, thus the cursor trajectory for each trial presented a ‘random-target’ manner.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Unstable neural recordings underlying stable motor outputs.</title><p>Data from monkey J, who was trained to perform the isometric wrist task. (<bold>A</bold>) Peri-event time histograms (PETHs) for the multiunit activity from three cortical electrodes (E35, E73, E60) and the EMGs from two forearm muscles (flexor carpi ulnaris, FCU; extensor carpi radialis longus, ECRl) on day 0 and day 95. Each column corresponds to a target direction indicated by the arrows on the top. For each direction, 15 trials were averaged to get the mean values (solid lines) and the standard errors (shaded area). The dashed vertical line in each subplot indicates the timing of force onset. While the neural activity picked by the implanted electrodes may change dramatically (E35, E73) or remain largely consistent (E95), the EMG patterns from two muscles which are critical to the task remain stable. (<bold>B</bold>) The distributions of the neural firing rates from E35, E73 and E60 and the EMGs from FCU and ECRl. The order of the subplots is consistent with (<bold>A</bold>). Note that for E35 the distribution of day-95 neural firing rates was omitted, since all values are close to 0. (<bold>C</bold>) The within-session and between-session maximum mean discrepancy (MMD) values for M1 signals (top panel) and EMGs (bottom panel). MMD provides a measure of distance between two multivariate distributions, and was used here to quantify the similarity of the distributions of neural activity or motor outputs between pairs of separate recording sessions in the dataset. In each panel the solid orange line shows a linear fit for all between-session MMDs, the dashed purple line indicates the mean of all within-session MMDs. The histograms for within-session and between-session MMDs are plotted on the right side of each panel, and the mean (solid dots) and standard deviation (solid lines) are shown. The between-session MMDs for M1 signals were an order of magnitude larger than for EMGs, and at least 10 times larger than the corresponding within-session values, indicating that instabilities in neural recordings are greater than in the motor output (note that the monkey was already well trained and proficient with the tasks before the data collection process began). However, factors such as monkey’s daily condition, noise levels of recordings, and drifts of the sensors on the behavioral apparatus could have altered the measured motor outputs across time and led to the reported gradual increase of the between-session MMDs for EMGs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Evaluation of the stability of M1 neural signals and motor outputs over time for monkeys / tasks (besides monkey J).</title><p>Stability is characterized by the discrepancy in the distributions of signals between pairs of recording sessions in each dataset, which are measured by maximum mean discrepancy (MMD). Each subplot corresponds to a dataset: isometric wrist task of monkey S (<bold>A</bold>), power grasp of monkey P (<bold>B</bold>), key grasp of monkey G (<bold>C</bold>), center-out reach of monkey C (<bold>D</bold>) and monkey M (<bold>E</bold>), and random-target reach of monkey M (<bold>F</bold>). In each subplot, we showed the between-session MMD (orange) for M1 signals (top panel) and motor outputs (either EMG or hand velocity, bottom panel), and indicated the mean value of the within-session MMDs using a dashed purple line. The histograms for within-session and between-session MMDs are plotted on the right side of each panel, and the mean (solid dots) and standard deviation (solid lines) are shown.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>The accuracy of a well-calibrated iBCI decoder degrades over time for different behavioral tasks.</title><p>We fit an iBCI decoder (Wiener filter) using the data collected on a specific day (day-0), and used this decoder to predict the motor outputs from M1 signals for all remaining days in a dataset (day-k). The performance of the decoder was evaluated by the R² value between the actual signals and the predictions. We used all available days in a dataset as the day-0 and repeated the same analysis for them. Each subplot corresponds to a behavioral task, and may contain the data from multiple monkeys: isometric wrist task of monkeys J and S (<bold>A</bold>), power and key grasp of monkeys P and G (<bold>B</bold>), center-out reach of monkeys M and C (<bold>C</bold>), random-target reach of monkey M (<bold>D</bold>). In each subplot, the R² values when using decoders to predict the motor outputs on the same day they were fit are shown (same-day decoders, purple). The x-axis on the top shows the number of the day which the recording session is on, where “0” corresponds to the earliest date in a dataset. The R² values when using decoders to predict the motor outputs on day-k are also shown (day-0 decoders, orange). The x-axis on the bottom shows the days since decoder training. The solid lines show linear fits for the R²s of the same-day decoders (purple) and day-0 decoders on day-k (orange).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig2-figsupp4-v1.tif"/></fig></fig-group></sec><sec id="s2-2"><title>Adversarial networks mitigate the performance declines of day-0 decoders</title><p>We proposed to use generative adversarial network (GAN) based domain adaptation (<xref ref-type="fig" rid="fig1">Figure 1B</xref>) to address the problem described above. We tested two different architectures: Adversarial Domain Adaptation Network (ADAN) (<xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>), and Cycle-Consistent Adversarial Networks (Cycle-GAN) (<xref ref-type="bibr" rid="bib55">Zhu et al., 2017</xref>). As both ADAN and Cycle-GAN were trained to reduce the discrepancy between the neural recordings on day-0 and those on day-k by aligning their probability density functions (PDFs), we call them ‘aligners’. Importantly, both ADAN and Cycle-GAN are static methods, trained only on instantaneous neural activity datapoints with no knowledge of dynamics. Both methods are causal and can be used in real time. We used the dataset with the longest recording timespan (monkey J, isometric wrist task, spanning 95 days) to determine appropriate choices of the hyperparameters for neural network training, which are presented in detail in a later section. We used the resulting hyperparameter values for the tests of all other monkeys and tasks. For comparison, we also used all datasets to test another type of ‘aligner’ that aimed to align the low-dimensional neural manifolds between day-0 and day-k (<xref ref-type="bibr" rid="bib8">Degenhart et al., 2020</xref>), which we termed ‘Procrustes Alignment of Factors’ (PAF).</p><p>The tests were conducted with the procedures presented by <xref ref-type="fig" rid="fig1">Figure 1</xref>. First, we picked a given day as day-0, and used the data recorded on that day to fit a Wiener filter as the ‘day-0 decoder’ (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Then, we trained the three types of aligners (ADAN, Cycle-GAN, and PAF) to align the neural recordings on a different day (day-k) to those on day-0 (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Each day in a dataset other than the designated day-0 was treated as a day-k, whether it occurred before or after day-0. Finally, we processed the neural recordings on day-k with the trained aligners, fed the aligned signals to the fixed day-0 decoder, and evaluated the accuracy of the predictions this decoder could obtain (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). For each of the seven datasets being tested, we repeated these three procedures for multiple instantiations using different day-0s (see <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>). To characterize the performance of the day-0 decoder after alignment, we represent the decoder accuracy as the ‘performance drop’ with respect to a daily recalibrated decoder (R<sup>2</sup><sub>aligned</sub> – R<sup>2</sup><sub>same-day</sub>). If an aligner works perfectly, we expect the performance drop of day-0 decoders to be close to 0, which means the decoder achieves accuracy equal to a within-day decoder after the alignment.</p><p>Unlike ADAN and PAF, Cycle-GAN alignment does not require computation of a latent representation from neural recordings. As a result, Cycle-GAN is naturally suited to a decoder trained on the full-dimensional neural firing rate signals. It is theoretically possible to use a full-dimensional decoder with ADAN and PAF as well, by training on firing rates reconstructed from the latent spaces of the ADAN autoencoder and PAF factors respectively. However, we found that the performance of these full-dimensional decoders was inferior to that of a decoder trained on the inferred latent signals (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). For completeness, we also tested a decoder trained on Cycle-GAN-generated firing rates projected into a low-dimensional manifold obtained using Factor Analysis; as expected, its performance was slightly worse than that of a full-dimensional decoder, but still better than ADAN and PAF with a low-dimensional decoder (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>).</p><p>In light of the analysis above, we here compare the better-performing of the two potential decoder input formats for each alignment method: full-dimensional for Cycle-GAN, and low-dimensional for ADAN and PAF (<xref ref-type="fig" rid="fig3">Figure 3</xref>, see Materials and methods for details). Aside from this difference of input dimensionality, the architecture of the day-0 decoder (a Wiener filter) was the same for all aligners. The within-day accuracy of the day-0 decoders of the three aligners was modestly but significantly different across tasks (<xref ref-type="fig" rid="fig3">Figure 3A</xref>): ADAN: R<sup>2</sup>=0.73 ± 0.009 (mean ± s.e.); Cycle-GAN: R<sup>2</sup>=0.72 ± 0.009; PAF: R<sup>2</sup>=0.71 ± 0.009 (p=0.008, linear mixed-effect model with the type of aligner as fixed and the type of task as random factor, n=204 samples, where each sample is one aligner/task/day-0/day-k combination).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The proposed GANs-based domain adaptation methods outperform Procrustes Alignment of Factors in diverse experimental settings.</title><p>(<bold>A</bold>) Prediction accuracy over time using the fixed decoder trained on day-0 data is shown for all experimental conditions (single dots: R² as a function of days after decoder training, lines: locally weighted scatterplot smoothing fits). We compared the performance of the day-0 decoder after domain adaptation alignment with Cycle-GAN (green), ADAN (red) and PAF (blue). (<bold>B</bold>) We computed the prediction performance drop with respect to a daily-retrained decoder (single dots: R² drop (R²<sub>aligned</sub> - R²<sub>same-day</sub>) for days after decoder training, lines: linear fits). Cycle-GAN and ADAN both outperformed PAF, with Cycle-GAN degrading most slowly for all the experimental conditions. (<bold>C</bold>) We compared the performance of each pair of aligners by plotting the prediction performance drop of one aligner versus that of another. Each dot represents the R² drop after decoder training relative to the within-day decoding. Marker colors indicate the task. Both proposed domain adaptation techniques outperformed PAF (left and center panels), with Cycle-GAN providing the best domain adaptation for most experimental conditions (right panel).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Cycle-GAN outperforms ADAN and Procrustes Alignment of Factors (PAF) with both full-dimensional and low-dimensional day-0 decoder.</title><p>We trained the day-0 decoders for each alignment method with either the full-dimensional firing rates (<bold>A</bold>) or the corresponding projections in a low-dimensional space (<bold>B</bold>). For the full-D decoder of ADAN and PAF, we used the reconstructed firing rates obtained from their nonlinear and linear latent space respectively. For ADAN, we used the decoder sub-network of the day-0 AE and for PAF we reversed the day-0 FA parameters to reconstruct the full-D firing rates. (<bold>C</bold>) Cycle-GAN outperforms ADAN and PAF with both a full-D (olive) and low-D (magenta) day-0 decoder. ADAN and PAF work better with a day-0 decoder trained on the latent signals. Note that PAF fails with a full-D decoder. For each alignment method, we computed the decoder performance drop with respect to a daily-retrained decoder (single dots: R² drop (R²<sub>aligned</sub> - R²<sub>same-day)</sub> for days after decoder training).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Cycle-GAN and ADAN consistently outperform Procrustes Alignment of Factors (PAF) for all experimental conditions.</title><p>(<bold>A</bold>) ADAN vs. PAF. (<bold>B</bold>) Cycle-GAN vs. PAF. (<bold>C</bold>) Cycle-GAN vs. ADAN. Figure shows the prediction performance drop with respect to a daily recalibrated decoder (R²<sub>aligned</sub> – R²<sub>same-day</sub>). Each dot represents the R² drop of a given day-k. Marker colors indicate the task. Points above the unity line indicate that the aligner on the y-axis outperformed that on the x-axis. ADAN and Cycle-GAN outperform PAF for both EMG (isometric wrist, 1st row and key/power grasping, 2nd row) and kinematic (center out reaching, 3rd row and random target reaching, 4th row) decoding. Cycle-GAN performances are slightly superior to those of ADAN for the tasks where we decoded EMG (1st and 2nd row). This difference was more remarkable for the tasks where we decoded hand velocity (3rd and 4th row).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Cycle-GAN outperforms ADAN and Procrustes Alignment of Factors (PAF) when aligning continuous neural recordings.</title><p>We compared the performance of the day-0 decoders, trained without excluding the inter-trial data, after domain adaptation alignment with Cycle-GAN (green), ADAN (red) and PAF (blue). We computed the prediction performance drop with respect to a daily-retrained decoder (single dots: R² drop (R²<sub>aligned</sub> - R²<sub>same-day</sub>) for days after decoder training, lines: linear fits). The accuracy of the fixed decoder on day-1 was significantly different across the aligners, with Cycle-GAN showing the least decoding drop, followed by ADAN and PAF. The performance degradation for periods greater than one day was mitigated in a similar way by all three alignment methods.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig3-figsupp3-v1.tif"/></fig></fig-group><p>To test for a significant performance difference between aligners, we fit a linear mixed-effect model with type of aligner and days as fixed factors and type of task as random factor for a quantitative evaluation of the performance of the three aligners (n=2361 samples). The performance drop of the day-0 decoder on data collected on the day immediately following day-0 (i.e. day-1) after alignment was significantly different across the aligners (Cycle-GAN: –0.02±0.004 (mean ± s.e.); ADAN: –0.06±0.005; PAF: –0.11±0.005; p~0). Cycle-GAN significantly outperformed both ADAN (p~0) and PAF (p~0). ADAN also significantly outperformed PAF (p~0).</p><p>The performance degradation of day-0 decoders for periods greater than one day (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>) was also mitigated by all three alignment methods, although to different extents. Nonetheless, there remained a significant and increasing performance drop over time (<xref ref-type="fig" rid="fig3">Figure 3A and B</xref>). We found a significant interaction between time and alignment method (p=0.026), indicating that there was a difference between methods in performance drop over time, and a post-hoc comparison showed that Cycle-GAN had the least overall performance degradation, significantly better than PAF, and better, but not significantly so, than ADAN (p=0.008 vs PAF; p=0.328 vs ADAN). ADAN was better, but not significantly, than PAF (p=0.091). Taken together, this analysis shows that Cycle-GAN moderately outperforms both ADAN and PAF (see also <xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B, C</xref>), and furthermore that the two nonlinear alignment methods tend to be more stable over time than PAF (see also <xref ref-type="fig" rid="fig3">Figure 3C</xref>; <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A, B</xref>).</p><p>While CCA-style trial alignment is not required by Cycle-GAN, ADAN, or PAF, we did preprocess the data to exclude behaviors not related to the investigated task (inter-trial data) and used data only from the beginning to the end of each trial (see Materials and methods). Among other advantages, this helped to unify behavior across monkeys and behavioral tasks. However, in a true iBCI setting, the user has uninterrupted control, so it would be ideal to train the aligner on that data, without the need to classify and exclude portions of a recording session that are not task-related. Therefore, we also tested aligners on the continuous neural recordings on the isometric wrist task data of monkey J (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>). Under this condition, Cycle-GAN was clearly superior to ADAN and PAF. We fit a linear mixed-effect model with type of aligner and days as fixed factors (n=531 samples) and found that the accuracy of the day-0 decoder on day-1 after alignment was significantly different across the aligners (Cycle-GAN: –0.05±0.015 (mean ±s.e.); ADAN: –0.14±0.023; PAF: –0.18±0.019; p~0). Cycle-GAN significantly outperformed both ADAN (p~0) and PAF (p~0), while ADAN outperformed PAF, but not significantly (p=0.134). On the other hand, we did not find a significant interaction between time and alignment method (p=0.56), indicating that the performance degradation over time was mitigated in a similar way by all three methods.</p></sec><sec id="s2-3"><title>Cycle-GAN is robust to hyperparameter settings</title><p>While they can be powerful, GANs can present a training challenge: choosing suitable hyperparameters is important, for example, to balance the learning process and prevent either of the two networks (the generator or discriminator) from dominating the loss function. High sensitivity of model performance to hyperparameter values would pose a potential barrier to the adoption of either ADAN or Cycle-GAN as a tool for cross-day alignment. As in <xref ref-type="bibr" rid="bib17">Ghosh et al., 2020</xref>, we assessed sensitivity to hyperparameters by testing the impact of batch size and learning rates on alignment performance. Because these hyperparameter sweeps are very computationally expensive, we evaluated them using only the single dataset with the greatest span of time.</p><p>We trained both ADAN and Cycle-GAN aligners on day-k data relative to four selected day-0 reference days. We kept the learning rates for the generator (LR<sub>G</sub>) and the discriminator (LR<sub>D</sub>) fixed (for ADAN, LR<sub>G</sub> = 0.0001, LR<sub>D</sub>/LR<sub>G</sub> = 0.5; for Cycle-GAN, LR<sub>G</sub> = 0.0001, LR<sub>D</sub>/LR<sub>G</sub> = 10). As in the previous section, we evaluated the drops in aligned day-0 decoder accuracy. We found that ADAN maintained good performance when batch size was small, but that performance started to drop significantly for larger batch sizes (64: –0.13±0.0096 (mean ± s.e.); 256: –0.17±0.013; p~0, Wilcoxon’s signed rank test, n=76; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). In contrast, Cycle-GAN based aligners performed consistently at all tested batch sizes. These results suggest that ADAN may need a small batch size, while Cycle-GAN-based aligners have no strong requirement.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Cycle-GAN is more robust to hyperparameter tuning than ADAN.</title><p>Effect of different batch sizes during training of Cycle-GAN (green) and ADAN (red) with mini-batch gradient descent on (<bold>A</bold>) the day-k performance of 4 selected day-0 decoders and (<bold>B</bold>) the execution time of 200 training epochs. The much faster execution time of PAF (blue) is also shown for reference. Compared to ADAN, Cycle-GAN did not require a small batch size, resulting in faster training (Cycle-GAN: 98 s with batch size 256; ADAN: 129 s with batch size 8; FA aligner: 11.5 s). Effect of training each domain adaptation method with different generator (<bold>C</bold>) and discriminator (<bold>D</bold>) learning rate. The generator and the discriminator learning rate were denoted as LR<sub>G</sub> and LR<sub>D</sub>, respectively. For LR<sub>D</sub> testing, we kept LR<sub>G</sub> fixed (LR<sub>G</sub> = 1e-4 for both ADAN and Cycle-GAN), and changed the ratio between LR<sub>D</sub> and LR<sub>G</sub> (LR<sub>D</sub>/LR<sub>G</sub>). ADAN-based aligners did not perform well for large LR<sub>G</sub> or LR<sub>D</sub>/LR<sub>G</sub> values, while Cycle-GAN-based aligners remained stable for all the testing conditions. In (<bold>A</bold>), (<bold>C</bold>) and (<bold>D</bold>) single dots show the prediction performance drop on each day-k relative to the 4 selected day-0s with respect to the R² of a daily-retrained decoder (R²<sub>aligned</sub> - R²<sub>same-day</sub>). Boxplots show 25th, 50th and 75th percentiles of the R² drop with the whiskers extending to the entire data spread, not including outliers.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig4-v1.tif"/></fig><p>Neural network training time is inversely proportional to batch size - therefore given two batch size options that give comparable model performance, the larger of the two will yield faster training. We found that Cycle-GAN was slower than ADAN for smaller batch sizes, although neither method required more than a few minutes when operating within their optimal batch size range (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Thus, we set the ADAN batch size for subsequent analyses to 8 and for Cycle-GAN to 256. Although we could have increased the batch size for ADAN, we decided instead to use a conservative value further from its region of decreased performance at the expense of slower training. For reference, we also computed the execution time of PAF, which was much faster than both ADAN and Cycle-GAN (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, dashed blue line) as it has a closed form solution (<xref ref-type="bibr" rid="bib47">Schönemann, 1966</xref>). We also note that the inference time (i.e. the time it takes to transform data once the aligner is trained) for both Cycle-GAN and ADAN is well under 1 ms per 50 ms sample of neural firing rates– this is because the forward map in both models consists simply of a fully connected network with only two hidden layers.</p><p>We next examined the effect of learning rates for each aligner. We first tested different values for the LR<sub>G</sub>, while fixing the ratio between LR<sub>D</sub> and LR<sub>G</sub> (for ADAN, LR<sub>D</sub>/LR<sub>G</sub> = 0.5; for Cycle-GAN, LR<sub>D</sub>/LR<sub>G</sub> = 10). As shown in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, ADAN achieved good performance when LR<sub>G</sub> was set to 1e-5 and 1e-4 but did not work well if LR<sub>G</sub> was set to 1e-3. Cycle-GAN maintained stable performance when LR<sub>G</sub> was set to 1e-3 and 1e-4, and had a significant performance drop when LR<sub>G</sub> was 1e-5 (1e-4: –0.064±0.0062 (mean ± s.e.); 1e-5: –0.095±0.0068; p~0, Wilcoxon’s signed rank test, n=76), but still significantly better than ADAN with the same LR<sub>G</sub> (Cycle-GAN: –0.095±0.0068 (mean ± s.e.); ADAN: –0.15±0.011; p~0, Wilcoxon’s signed rank test, n=76). We then tested different ratios between LR<sub>D</sub> and LR<sub>G</sub> with LR<sub>G</sub> fixed (LR<sub>G</sub> = 1e-4 for both types of aligners). As <xref ref-type="fig" rid="fig4">Figure 4D</xref> shows, ADAN could only be trained well when LR<sub>D</sub> was equal to or smaller than LR<sub>G</sub>. On the other hand, the performance of a Cycle-GAN based aligner remained stable for all tested LR<sub>D</sub>/LR<sub>G</sub> values.</p></sec><sec id="s2-4"><title>GAN-based methods require very little training data for alignment</title><p>Aligners in practical iBCI applications must be fast to train and perhaps more importantly, require little training data. Here we investigated the aligner performance with limited training data. We trained ADAN, Cycle-GAN, and PAF to align the data on each day-k to four selected day-0s using randomly selected subsets of the full 120-trial training set from Monkey J. We then decoded EMGs from the aligned M1 signals on a fixed 40-trial held-out testing set using the day-0 decoder. As <xref ref-type="fig" rid="fig5">Figure 5A</xref> shows, all three aligners improved the performance of day-0 decoders with 20 or fewer training trials. Performance increased as more training trials were included but started to plateau near 40 trials. When using only 10 trials, both ADAN and Cycle-GAN significantly outperformed PAF (Cycle-GAN: –0.19±0.0076 (mean ± s.e.); ADAN: –0.21±0.011; PAF: –0.26±0.011; p~0, Wilcoxon’s signed rank test, n=76), with Cycle-GAN significantly outperforming ADAN (p=0.003, Wilcoxon’s signed rank test, n=76). It is also worth noting that ADAN and Cycle-GAN trained with only 20 trials significantly outperformed PAF trained with the full training set of 120 trials (Cycle-GAN trained with 20 trials: –0.10±0.0083 (mean ±s.e.); ADAN trained with 20 trials: –0.16±0.0096; PAF trained with 120 trials: –0.20±0.011; p~0, Wilcoxon’s signed rank test, n=76) (<xref ref-type="fig" rid="fig5">Figure 5B</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Cycle-GAN and ADAN need only a limited amount of data for training.</title><p>(<bold>A</bold>) Effect of the number of trials used for training Cycle-GAN (green), ADAN (red) and PAF (blue) on the day-k decoding accuracy using 4 selected day-0 fixed decoders. All the aligners needed 20–40 trials to achieve a satisfactory performance, before reaching a plateau. The average prediction performance drop with respect to a daily-retrained decoder (R²<sub>aligned</sub> - R²<sub>same-day</sub>) on all day-ks is shown for each tested value of training trials (x-axis is in log scale). When using 10 trials, both Cycle-GAN and ADAN significantly outperformed PAF (<bold>B</bold>, left boxplots). Moreover, both Cycle-GAN-based and ADAN aligners trained with 20 trials had significantly better performance than the PAF trained on all 120 trials (<bold>B</bold>, center and right boxplots). Single dots show the prediction performance drop on each day-k to the 4 selected day-0s with respect to a daily-retrained decoder. Boxplots show 25th, 50th and 75th percentiles of the R² drop with the whiskers extending to the entire data spread, not including outliers. Asterisks indicate significance levels: *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig5-v1.tif"/></fig></sec><sec id="s2-5"><title>Recovery of single-electrode activity patterns through alignment</title><p>Both ADAN and Cycle-GAN generate reconstructed versions of the aligned day-k single neuron signals, agnostic to downstream use. However, our objective of decoder stabilization does not require that the full distribution of day-0 responses be recovered: we need only recover signals that are relevant to the decoding dimension. Decoder performance alone therefore does not provide a complete picture of the quality of neural alignment. To more thoroughly investigate the extent to which distribution alignment introduces biases or artifacts in predicted neural responses, we first compared aligner predictions of single-neuron with those of their recorded day-0 analogs.</p><p>Because PAF operates directly on the low-dimensional neural manifold, it can only generate single-neuron responses in the aligned representation by projecting back out from the manifold. We found that a stabilized day-0 decoder that uses these reconstructed firing rates from the latent space of the PAF factors performs poorly (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). In contrast to PAF, Cycle-GAN and ADAN each generate synthetic firing rates for the full neural population (although ADAN still relies on a low-dimensional manifold as an intermediate step). Therefore, we restricted our analysis of single-neuron properties on the outputs of ADAN and Cycle-GAN.</p><p>Specifically, we asked how response properties of the day-k ‘aligned neurons’ differed from those of the neurons recorded on the same electrode on day-0. To do so, we examined the aligned neural representations generated by Cycle-GAN and ADAN, again using the 95-day isometric wrist task dataset of monkey J. We first compared the peri-event time histograms (PETHs) of firing rates before and after alignment, to determine how the aligners altered day-k neural activity at the level of single electrodes. The PETHs in <xref ref-type="fig" rid="fig6">Figure 6A</xref> show three examples of the ways in which single-electrode signals may differ across days, and the change produced by alignment. Electrode E35 is an example of neuron drop-out, in which the activity captured on day-0 was not observed on day-95. The PETHs of aligned day-95 data matched those of day-0 for all force directions, demonstrating that on day 95 both ADAN and Cycle-GAN aligners synthesized appropriate neural activity (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). Second, E73 is an example of activity not present on day-0, but recorded on day-95. In this case, the day-95 activity was suppressed to match that on day-0. Finally, E60 is an example of consistent neural activity over the two days, which the aligners left unchanged.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The changes of single-electrode and coordinated neural activity patterns after alignment.</title><p>(<bold>A</bold>) The PETHs of the multiunit activity from three cortical electrodes (E35, E73, E60) before and after alignment. Each column corresponds to a target direction indicated by the arrows on the top. For each direction, mean (solid lines) and standard errors (shaded areas) are shown for 15 trials. The dashed vertical line in each subplot indicates the time of force onset. (<bold>B</bold>) Between-session MMDs for M1 signals before and after alignment, as well as the within-session MMDs. The main panel plots the between-session MMDs before (orange) and after alignment (red: by ADAN, green: by Cycle-GAN) for all pairs of sessions with different days apart, and the dashed purple line indicates the mean of the within-session MMD values. The side panel plots the histogram for each type of data. Note y-axis is in log scale.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig6-v1.tif"/></fig><p>We also examined the distributions of the recovered single-electrode activity by computing the Maximum Mean Discrepancy (MMD <xref ref-type="bibr" rid="bib21">Gretton et al., 2012a</xref>, see Materials and methods) between all pairs of sessions (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Before alignment, the between-day MMDs were significantly larger than the within-day MMDs (orange, between-day MMD: 1.42±0.029 (mean ± s.e.); purple, within-day MMD: 0.059±0.0054; p~0, Wilcoxon’s rank sum test, n=171). After alignment, the between-day MMDs were substantially reduced by both Cycle-GAN and ADAN, becoming comparable to the within-day MMDs (ADAN: red, 0.19±0.0065 (mean ±s.e.); Cycle-GAN: green, 0.091±0.0024; within-day: purple, 0.059±0.0054). Cycle-GAN based aligners generally achieved a significantly lower between-day MMD than ADAN across the entire timespan (p~0, Wilcoxon’s rank sum test, n=171).</p></sec><sec id="s2-6"><title>Recovery of neural manifolds from aligned representations</title><p>While Cycle-GAN works only with the full-dimensional neural recordings, ADAN, whose discriminator is essentially an autoencoder, computes a low-dimensional neural manifold from which it reconstructs the high-level signals it needs to align the high-level residuals. Consequently, we wanted to explore to what extent each method also altered the low-dimensional representations. We applied Principal Component Analysis (PCA) to the firing rates recorded for the 95-day isometric wrist task of monkey J on four selected day-0s and examined the trajectories of M1 neural activity within the neural subspaces defined by the principal components (PCs, see Materials and methods). We then projected the firing rates of the remaining day-k’s onto the neural subspace defined by the corresponding day-0 PCs.</p><p>Generally, the day-k neural trajectories projected onto the top two day-0 PCs did not match those of day-0 (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). However, after alignment (3rd and 4th columns), the day-k trajectories closely resemble those of day-0.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Neural manifold is stable over time after domain adaptation based neural alignment.</title><p>(<bold>A</bold>) Representative latent trajectories when projecting unaligned / aligned neural activity onto the first two principal components (PCs) for the day-0 neural activity of monkey J during isometric wrist task. Top left corner: latent trajectories for day-0 firing rates, as the reference. 2nd column: latent trajectories for unaligned firing rates on day-7 (top row), day-37 (center row) and day-95 (bottom row). 3rd column and 4th column: latent trajectories for firing rates aligned by ADAN (3rd column) and Cycle-GAN (4th column) on day-7, day-37, and day-95. Data were averaged over the first 16 trials for each target location and aligned to movement onset for visualization purposes. (<bold>B</bold>) First ten principal angles between the neural manifolds of day-0 and a given day-k for unaligned (black), aligned by ADAN (red) and aligned by Cycle-GAN (green). Upper bound was found by computing principal angles between surrogate subspaces with preserved statistics of day-0 and day-95 (0.1st percentile is shown). Within-day angles were found between subspaces relative to even-numbered and odd-numbered trials of day-0 neural recordings. Principal angle values were averaged across four different time intervals (relative to initial decoder training) indicated by the transparency of the line (lighter for days closer to day-0, darker for days further away from day-0).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-fig7-v1.tif"/></fig><p>Finally, to directly quantify the similarity between the neural manifolds of day-0 and an aligned day-k, we calculated the principal angles (<xref ref-type="bibr" rid="bib30">Knyazev and Argentati, 2002</xref>) between the neural subspaces for all sessions relative to the selected day-0 (see Materials and methods). To interpret the magnitude of the overlap between a given pair of days, we compared the observed angle with an upper bound provided by the principal angles across random subspaces that preserved the covariance of the day-0 and day-95 neural data, using the method described in <xref ref-type="bibr" rid="bib10">Elsayed et al., 2016</xref>. We also found a ‘within-day’ bound by computing the angles between the day-0 neural recordings of even-numbered trials and odd-numbered trials (this was done to reduce the effect of any within-day drift). We found that alignment with either Cycle-GAN or ADAN made the neural manifolds of any day-k substantially more similar to those of day-0. In particular, after applying Cycle-GAN-based aligners, the population subspaces highly overlapped (<xref ref-type="fig" rid="fig7">Figure 7B</xref>).</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We previously demonstrated the utility of a GAN-based method, ADAN, to ‘align’ M1 data across time, thereby allowing a fixed iBCI decoder to be used for weeks without re-calibration, despite a gradual change in the neurons recorded over the same period (<xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>). However, we had tested ADAN on a very limited dataset. Because GANs are notoriously sensitive to hyperparameter settings (<xref ref-type="bibr" rid="bib12">Farnia and Ozdaglar, 2020</xref>; <xref ref-type="bibr" rid="bib17">Ghosh et al., 2020</xref>; <xref ref-type="bibr" rid="bib31">Kurach et al., 2018</xref>), it was unclear how robust ADAN would be in practice. Another promising method, PAF, had been tested primarily in terms of two monkeys’ online iBCI performance (<xref ref-type="bibr" rid="bib8">Degenhart et al., 2020</xref>). We wished to compare both approaches directly, using a very diverse dataset including recordings from six monkeys and five tasks. We also compared a third approach based on a more recent GAN architecture, Cycle-GAN (<xref ref-type="bibr" rid="bib55">Zhu et al., 2017</xref>). Cycle-GAN has the potential advantage over ADAN that it reduces the search space of aligners by encouraging the learned transformation to be a bijection, which might help stabilize its performance. Moreover, unlike ADAN and PAF, the Cycle-GAN architecture does not require computation of a low-dimensional manifold underlying the neural population activity, allowing its straightforward use with spike-rate based decoders.</p><p>Both ADAN and Cycle-GAN achieved higher performance than PAF, but each method had tradeoffs. Although ADAN needed less time to train than Cycle-GAN, PAF was much faster to train than both GAN methods. But while slower, Cycle-GAN was easier to train than ADAN, in the sense that it was less sensitive to hyperparameter values and therefore likely to be more effective ‘out-of-the-box’, and when working with different data binning and sampling rates. Importantly, Cycle-GAN also had clearly superior performance compared to both ADAN and PAF when tested with continuously recorded data (with no trial segmentation). Overall, our work suggests that GAN-based alignment, and Cycle-GAN in particular, is a promising method for improving the stability of an iBCI over time.</p><sec id="s3-1"><title>Comparison of GANs to other methods for iBCI stabilization</title><p>Other approaches to address iBCI decoder instability include supervised techniques that aim at stabilizing iBCI performance by recalibrating the decoder during ongoing iBCI control by relying on access to the task output variables (<xref ref-type="bibr" rid="bib7">Dangi et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Jarosiewicz et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Orsborn et al., 2012</xref>), as well as unsupervised methods that do not require to re-estimate decoder parameters and only need neural data, with no provided task output variables or task labels (<xref ref-type="bibr" rid="bib8">Degenhart et al., 2020</xref>; <xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Gallego et al., 2020</xref>; <xref ref-type="bibr" rid="bib27">Karpowicz et al., 2022</xref>; <xref ref-type="bibr" rid="bib53">Willett et al., 2021</xref>). We restricted our comparison to GAN-based aligners and PAF for several reasons. First, both GANs and PAF are unsupervised methods. We argue that unsupervised methods are ideal for iBCI stabilization: because they do not require data labels, they should be simpler to implement in eventual clinical applications. Second, neither GANs nor PAF require trial alignment of the data, which CCA does require. This flexibility allowed us to align the neural data for more complicated behaviors. For example, one task in this study was a random-target reaching task in which monkeys moved a cursor between targets as they appeared on screen; this task structure produces movements of random length and direction, with varied speed and duration. Despite this complexity, all three of the tested aligners could still achieve good performance. Importantly, though, we previously demonstrated that ADAN still achieves higher performance than both CCA (<xref ref-type="bibr" rid="bib15">Gallego et al., 2020</xref>) and KLDM (<xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>) for the stereotyped isometric wrist task (<xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>).</p><p>Although earlier attempts to achieve alignment via KLDM achieved only moderate success, a recent approach using KLD to align neural latent dynamics identified using Latent Factor Analysis through Dynamical Systems (LFADS) (<xref ref-type="bibr" rid="bib42">Pandarinath et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Sussillo et al., 2016a</xref>) was more successful (<xref ref-type="bibr" rid="bib27">Karpowicz et al., 2022</xref>). Comparing this approach (called Nonlinear Manifold Alignment with Dynamics, or NoMAD) with Cycle-GAN turns out to be problematic because they are solving overlapping but different problems. A stable iBCI device has several interacting components: data preprocessing, an aligner that registers neural representations across days, and a decoder that translates neural activity to a predicted motor command. Higher iBCI performance could arise from an improvement to any of these processes. NoMAD includes the first two steps, performing both alignment of the neural representations via KLDM and data preprocessing via LFADS-based smoothing. Because Karpowicz et al., contrast NoMAD (alignment +powerful dynamics-based smoothing) to two methods that perform alignment with only very simple linear smoothing (ADAN and PAF), it is not possible to tell from their manuscript the extent to which NoMAD’s higher performance arises from better alignment vs their use of LFADS for data smoothing. Nevertheless, the effects of the preprocessing can be inferred from their results: because of its more powerful dynamics preprocessing, NoMAD outperforms ADAN (and PAF) not only at day-k, but also on day-0 where neural alignment is not involved. The day-0 performance makes it clear that a substantial portion of NoMAD’s higher performance comes not from its KLD-based alignment but from how the neural recordings are pre-processed with LFADS.</p><p>We can also draw conclusions purely from the method NoMAD uses for alignment, namely by minimizing the KLD between the distributions of day-0 and day-k states that come out of a day-0 LFADS Generator network. This alignment strategy is very similar to the KLDM method tested in <xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>, where KLDM between neural states (obtained via an autoencoder) had inferior performance compared even to ADAN. This suggests that the apparent performance improvement of NoMAD over ADAN is a consequence of its embedded LFADS model rather than an indicator of KLD being a better alignment strategy. Theoretically, one could therefore replace the KLD-based alignment in NoMAD with a Cycle-GAN-based aligner and achieve even better performance. Going forward, it will be important for the field to establish consensus benchmark datasets and evaluation methods to disentangle the contributions of new methods in data preprocessing, neural alignment, and decoding, within each of these three areas.</p><p>A very different approach to iBCI stabilization was proposed by Sussillo et al., who trained a decoder with a large dataset spanning many months, under the hypothesis that neural turnover allows neurons not only to disappear, but potentially also to reappear later (<xref ref-type="bibr" rid="bib49">Sussillo et al., 2016b</xref>). Although making the decoder robust to changes in the recorded neural populations, this approach has the inherent disadvantage of requiring the accumulation of a long stretch of historical data, which might be impractical for clinical use. In contrast to this approach, neither Cycle-GAN nor ADAN has a special requirement for the robustness of the day-0 decoder, and effective performance can be achieved with remarkably little data (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p></sec><sec id="s3-2"><title>iBCI stabilization without manifolds</title><p>CCA, KLDM, PAF, and ADAN all rely on dimensionality reduction of the recorded neural population prior to alignment. As a result, a portion of the variance of recorded neural activity is always lost in the alignment process. In contrast, Cycle-GAN allows alignment to be performed on the full-dimensional neural recording, and achieves a superior performance compared to ADAN and PAF (<xref ref-type="fig" rid="fig3">Figure 3</xref>). This also means that Cycle-GAN can be used directly with any previously trained spike-rate based decoder. This is in contrast to ADAN and PAF, which only align the neural latent space and therefore require either a new, latent space decoder to be trained, or an additional post-alignment, backwards-projection step to convert the latent representation into a predicted set of spikes. The backwards-projection step leads to lower decoding performance for ADAN, and complete failure for PAF, as shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p><p>Because Cycle-GAN operates in the higher-dimensional space of the recorded neurons, it also recovers the response properties of individual neurons following alignment, providing the means to infer their response properties across many days of recording, even when those neurons are not actually observed. While single-neuron signals can in principle be generated by manifold-based alignment methods, we show here that these more indirectly reconstructed firing rates are less accurate (<xref ref-type="fig" rid="fig6">Figure 6</xref>). The potential applications of this ability to synthesize neural data from population recordings are yet undeveloped but intriguing. One possibility is that this strategy could be used to synthesize a “null distribution” of neural responses, to better detect effects of learning or behavioral changes that alter the response distribution of cells.</p></sec><sec id="s3-3"><title>Sources of decoding error following cross-day alignment</title><p>In this study, we relied on offline estimates of decoder accuracy, as they allowed us to examine large amounts of previously collected data across many monkeys and tasks. Also, by literally taking the monkey out of the loop, we were able to examine the accuracy of the alignment and decoding processes without the added complication of the monkeys’ unknown and variable adaptation to the decoder. Although alignment by either ADAN or Cycle-GAN significantly improved the performance of a day-0 decoder on a given day-k, in most cases it did not attain the performance of a re-calibrated decoder, especially at long time offsets between day-0 and day-k (<xref ref-type="fig" rid="fig3">Figure 3B</xref>). One interesting potential cause of aligner performance drop is a change in the animal’s behavioral strategy across days. Because the limb is kinematically redundant, the same hand position can be achieved with different limb postures (e.g. wrist angle) and muscle activation patterns. Similarly, differing strategies might be adopted to grasp the power or pinch force transducers. Even within a single experimental session, an M1 decoder trained on one behavior often fails to perform well when tested on a different behavior. Similarly, unsupervised M1 alignment will not be able to compensate for changes in strategy if they shift EMG (or kinematic) signals outside the space of values observed during training of the original decoder. We find some evidence for such drift in some tasks (predominantly the key grasp, <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3C</xref>), as indicated by differences between within- and across-day MMD of the motor outputs. Such differences were small, but could not be neglected (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplements 2C</xref> and <xref ref-type="fig" rid="fig2s3">3</xref>).</p></sec><sec id="s3-4"><title>Network training challenges</title><p>Training GANs is a challenging task, in part because the learning rates of generator and discriminator networks must be carefully balanced to allow the networks to be trained in tandem (<xref ref-type="bibr" rid="bib12">Farnia and Ozdaglar, 2020</xref>; <xref ref-type="bibr" rid="bib45">Salimans et al., 2016</xref>). Many strategies have been proposed to improve the stability of learning and facilitate the convergence of GANs (<xref ref-type="bibr" rid="bib3">Arjovsky and Bottou, 2017</xref>; <xref ref-type="bibr" rid="bib5">Brock et al., 2019</xref>; <xref ref-type="bibr" rid="bib12">Farnia and Ozdaglar, 2020</xref>; <xref ref-type="bibr" rid="bib37">Nagarajan and Kolter, 2017</xref>; <xref ref-type="bibr" rid="bib41">Pan et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Salimans et al., 2016</xref>). ADAN and Cycle-GAN incorporate several of those strategies. First, both networks include an L1 loss term in their objective function, a modification that has been found in practice to improve the stabilization of model training by encouraging sparseness of model weights (<xref ref-type="bibr" rid="bib3">Arjovsky and Bottou, 2017</xref>). The networks also use a two-timescale update rule for generator and discriminator learning rates, which facilitates convergence of generator and discriminator to a balanced solution (<xref ref-type="bibr" rid="bib23">Heusel et al., 2017</xref>).</p><p>Correct optimization of GANs is also directly linked to proper tuning of the dynamics of learning during training (<xref ref-type="bibr" rid="bib31">Kurach et al., 2018</xref>; <xref ref-type="bibr" rid="bib46">Saxena and Cao, 2021</xref>), which we investigated here in depth. Given the many GAN variants, there are still no comprehensive guidelines for a particular architecture (<xref ref-type="bibr" rid="bib17">Ghosh et al., 2020</xref>). Consistent with this, we found that ADAN and Cycle-GAN differ substantially in their sensitivity to learning rate and batch size hyperparameters. Notably, ADAN exhibited poor generalization with larger batch sizes (like <xref ref-type="bibr" rid="bib28">Keskar et al., 2016</xref>), while Cycle-GAN worked well across all tested values (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). The ability to work with larger batch sizes gave Cycle-GAN several advantages over ADAN: its training was faster than ADAN (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) and it also enabled Cycle-GAN to maintain stable performance with higher learning rates (<xref ref-type="fig" rid="fig4">Figure 4C and D</xref>, similar to the observations of <xref ref-type="bibr" rid="bib20">Goyal et al., 2017</xref>).</p></sec><sec id="s3-5"><title>Conclusions</title><p>In summary, we demonstrated the successful use of GANs for the stabilization of an iBCI, thereby overcoming the need for daily supervised re-calibration. Both approaches we tested (ADAN and Cycle-GAN) require remarkably little training data, making them practical for long-term iBCI clinical applications. Between the two approaches, Cycle-GAN achieved better performance which was less affected by inaccurate hyperparameter tuning; it is therefore our recommended method for future use. Notably, Cycle-GAN works directly with the unstable full-dimensional neural recordings, which further increases its performance and simplifies its implementation.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects and behavior tasks</title><p>Six 9–10 kg adult male rhesus monkeys (Macaca mulatta) were used in this study. They were trained to sit in a primate chair and control a cursor on a screen in front of them using different behavioral apparatuses (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>Monkeys J and S were trained to perform an isometric wrist task, which required them to control the cursor on the screen by exerting forces on a small box placed around one of the hands. The box was padded to comfortably constrain the monkey’s hand and minimize its movement within the box, and the forces were measured by a 6 DOF load cell (JR3 Inc, CA) aligned to the wrist joint. During the task, flexion/extension force moved the cursor right and left respectively, while force along the radial/ulnar deviation axis moved the cursor up and down. Each trial started with the appearance of a center target requiring the monkeys to hold for a random time (0.2–1.0 s), after which one of eight possible outer targets selected in a block-randomized fashion appeared, accompanied with an auditory go cue. The monkey was allowed to move the cursor to the target within 2.0 s and hold for 0.8 s to receive a liquid reward. For both decoding and alignment analyses, we only used the data within each single trial (from ‘trial start’ to ‘trial end’, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). We did not do any temporal alignment with the trials, so the lengths of the trials were different from each other.</p><p>Monkeys P and G were trained to perform a grasping task, which required them to reach and grasp a gadget placed under the screen with one hand. The gadget was a cylinder for monkey P facilitating a power grasp with the palm and the fingers, while a small rectangular cuboid for monkey G facilitating a key grasp with the thumb and the index finger. A pair of force sensitive resistors (FSRs) were attached on the sides of the gadgets to measure the grasping forces the monkeys applied. The sum and the difference of the FSR outputs were used to determine the position of the cursor on the vertical axis and the horizontal axis respectively. At the beginning of each trial the monkey was required to keep the hand resting on a touch pad for a random time (0.5–1.0 s). A successful holding triggered the onset of one of three possible rectangular targets on the screen and an auditory go cue. The monkey was required to place the cursor into the target and hold for 0.6 s by increasing and maintaining the grasping force applied on the gadget (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). For this task we extracted trials from ‘go cue time’ to ‘trial end’, as the monkeys’ movements were quite random before the go cue.</p><p>Monkeys C and M were trained to perform a center-out (CO) reaching task while grasping the upright handle of a planar manipulandum, operated with the upper arm in a parasagittal plane. Monkey C performed the task with the right hand, monkey M with the left. At the beginning of each trial the monkey needed to move the hand to the center of the workspace. One of eight possible outer targets equally spaced in a circle was presented to the monkey after a random waiting period. The monkey needed to keep holding for a variable delay period until receiving an auditory go cue. To receive a liquid reward, the monkey was required to reach the outer target within 1.0 s and hold within the target for 0.5 s (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). For this task we extracted trials from ‘go cue time’ to ‘trial end’, since the monkeys kept static before the go cue.</p><p>Monkey M was trained to perform a random-target (RT) task, reaching a sequence of three targets presented in random locations on the screen to complete a single trial. The RT task used the same apparatus as the CO reach task. At the beginning of each trial the monkey also needed to move the hand to the center of the workspace. Three targets were then presented to the monkey sequentially, and the monkey was required to move the cursor into each of them within 2.0 s after viewing each target. The positions of these targets were randomly selected, thus the cursor trajectory for each trial presented a ‘random-target’ manner (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref>). For this task we extracted trials from ‘trial start’ to ‘trial end’.</p><p>All surgical and experimental procedures were approved by the Institutional Animal Care and Use Committee (IACUC) of Northwestern University under protocol #IS00000367, and are consistent with the Guide for the Care and Use of Laboratory Animals.</p></sec><sec id="s4-2"><title>Implants and data recordings</title><p>Depending on the task, we implanted a 96-channel Utah electrode array (Blackrock Neurotech, Inc) in either the hand or arm representation area of the primary motor cortex (M1), contralateral to the arm being used for the task (see <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>). The implant site was pre-planned and finally determined during the surgery with reference to the sulcal patterns and the muscle contractions evoked by intraoperative surface cortical stimulation. For each of monkeys J, S, G, and P, we also implanted intramuscular leads in forearm and hand muscles of the arm used for the task in a separate procedure (see <xref ref-type="supplementary-material" rid="fig2sdata1">Figure 2—source data 1</xref>). Electrode locations were verified during surgery by stimulating each lead.</p><p>M1 activity was recorded during task performance using a Cerebus system (Blackrock Neurotech, Inc). The signals on each channel were digitalized, bandpass filtered (250~5000 Hz) and converted to spike times based on threshold crossings. The threshold was set with respect to the root-mean square (RMS) activity on each channel and kept consistent across different recording sessions (monkeys J, C and M: –5.5 x RMS; monkey S: –6.25 x RMS; monkey P: –4.75 x RMS; monkey G: –5.25 x RMS). The time stamp and a 1.6 ms snippet of each spike surrounding the time of threshold crossing were recorded. For all analyses in this study, we used multiunit threshold crossings on each channel instead of discriminating well isolated single units. We applied a Gaussian kernel (S.D.=100 ms) to the spike counts in 50 ms, non-overlapping bins to obtain a smoothed estimate of firing rate as function of time for each channel.</p><p>The EMG signals were differentially amplified, band-pass filtered (4-pole, 50~500 Hz) and sampled at 2000 Hz. The EMGs were subsequently digitally rectified and low-pass filtered (4-pole, 10 Hz, Butterworth) and subsampled to 20 Hz. EMG channels with substantial noise were not included in the analyses, and data points of each channel were clipped to be no larger than the mean plus 6 times the S.D. of that channel. Within each recording session, we removed the baseline of each EMG channel by subtracting the 2nd percentile of the amplitudes and normalized each channel to the 90th percentile. For monkeys C and M, we recorded the positions of the endpoint of the reach manipulandum at a sampling frequency of 1000 Hz using encoders in the two joints of the manipulandum.</p></sec><sec id="s4-3"><title>iBCI day-0 decoder</title><p>The day-0 decoder was a Wiener filter of the type that we have used in several previous studies (<xref ref-type="bibr" rid="bib6">Cherian et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Naufel et al., 2019</xref>). The filter was fit using linear regression to predict the motor outputs (either EMG or hand velocity) at time <italic>t</italic> given neural responses from time <italic>t</italic> to time <italic>t -</italic> T, where we set T=4 (200 ms) for all decoders used in this study. As the aligners being tested worked with either low-dimensional manifolds or the full neural population, and required the associated day-0 decoders to be compatible, we implemented different day-0 decoders to match the outputs of the aligners. For Cycle-GAN, we trained a Wiener filter using the full-dimensional neural firing rates recorded on day-0. For ADAN and PAF, we performed dimensionality reduction (ADAN: autoencoder, PAF: Factor Analysis; dimensionality = 10 for both) to find a low-dimensional latent space, and trained the decoder using the projections of the neural signals into this latent space. The Wiener filters were trained using the day-0 data with four-fold cross validation, and the filter corresponding to the fold with the best R<sup>2</sup> was selected as the fixed day-0 decoder. The parameters for the dimensionality reduction procedures and the Wiener filter from the day-0 data were kept fixed for decoding on subsequent days.</p></sec><sec id="s4-4"><title>iBCI aligners</title><sec id="s4-4-1"><title>Adversarial domain adaptation network (ADAN)</title><p>We adhered to the main architecture and the training procedures of the ADAN as described in <xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>. Briefly, we first find a nonlinear latent space by jointly training an autoencoder and a long short-term memory (LSTM) neural network-based iBCI decoder using day-0 data. (Note that this LSTM based decoder is only used for latent space discovery, not the later decoding stage that is used for performance evaluation (see ‘ADAN day-0 training’ in <bold>Appendix</bold> for full details)). We then construct an adversarial aligner comprised of a distribution alignment module (generator network G) and a discriminator network D (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>), where G is a shallow feedforward neural network, and D is an autoencoder with the same architecture as that used for the day-0 latent space discovery. During training of the aligner, G is fed with day-k neural firing rates and applies a nonlinear transform over these data to match them to the day-0 neuron response distributions. The output of G<italic>,</italic> and the true day-0 neural firing rates are then passed to D, which passes both inputs through the autoencoder: namely, it projects each signal into the latent space and then reconstructs it. The distributions of the residuals between the autoencoder inputs and the reconstructions are computed for both the generator output and the true day-0 data, and a lower bound to the Wasserstein distance is used to measure the dissimilarity between the two distributions. The goal of adversarial learning is to find a discriminator D that maximizes the dissimilarity between responses of D to true day-0 firing rates and to outputs of G, while also finding a generator G that minimizes the dissimilarity between true day-0 firing rates and the outputs of G; this objective is called the adversarial loss. When the training is completed, G will have been trained to ‘align’ the neural firing rates on day-k with those on day-0. For a full description of the ADAN architecture and its training strategy, please refer to <bold>Appendix</bold> and (<xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>).</p></sec><sec id="s4-4-2"><title>Cycle-GAN</title><p>The Cycle-GAN aligner is based on the structure proposed in <xref ref-type="bibr" rid="bib55">Zhu et al., 2017</xref>. Like ADAN, Cycle-GAN does not consider any dynamic information, aligning only the point clouds representing the instantaneous firing rate of M1 neurons. Unlike ADAN, it converts the full-dimensional neural firing rates collected on day-k into a form resembling those collected on day-0, with no dimensionality reduction. Cycle-GAN consists of two feedforward generator neural networks (G<sub>1</sub> and G<sub>2</sub>) and two discriminator networks (D<sub>1</sub> and D<sub>2</sub>, see <xref ref-type="fig" rid="app1fig1">Appendix—figure 1B</xref>). These form two pairs of adversarial networks: G<sub>1</sub> maps data from the day-k domain to the day-0 domain, while D<sub>1</sub> aims to distinguish between the day-0 samples and the output of G<sub>1</sub>. And in parallel, G<sub>2</sub> maps data in the day-0 domain to the day-k domain, while D<sub>2</sub> distinguishes day-k data from output of G<sub>2</sub>. In contrast to ADAN, the cycle-GAN discriminator networks operate directly on neural responses, rather than the residuals between low-dimensional and full-dimensional responses.</p><p>The objective function for network training has two major terms. The first is an adversarial loss, defined for both generator-discriminator pairs (G<sub>1</sub> + D<sub>1</sub> and G<sub>2</sub> + D<sub>2</sub>) as in ADAN. The second term is known as the cycle-consistency loss, which pushes the mappings G<sub>1</sub> and G<sub>2</sub> to become inverses of each other: that is, a sample from one specific domain should be recovered to its original form after going through the cycle composed of the two mappings. As argued by Zhu et al, the introduction of the cycle-consistency loss regularizes the learning of the mapping functions, thereby reducing the search space. In (<xref ref-type="fig" rid="app1fig1">Appendix—figure 1B</xref>) the purple arrows through G<sub>1</sub> and G<sub>2</sub> reflect the transformation of each sample from the day-k domain into the day-0 domain by G<sub>1</sub>, followed by the recovery from the day-0 domain into the day-k domain by G<sub>2</sub>. Likewise, the orange arrows through G<sub>2</sub> and G<sub>1</sub> reflect a transformation from the day-0 domain to the day-k domain and back to the day-0 domain. Further details about the Cycle-GAN based aligner are provided in <bold>Appendix</bold>.</p></sec><sec id="s4-4-3"><title>GAN training and architecture</title><p>Both ADAN and Cycle-GAN were trained using the ADAM optimizer (<xref ref-type="bibr" rid="bib29">Kingma and Ba, 2015</xref>) with a four-fold cross validation. We used 400 training epochs and reported the alignment result that produced the best decoder performance on a held-out validation set of trials. In addition to the learning hyperparameters explored in the Results section, we examined several different architectures for the aligner neural network of both ADAN and Cycle-GAN (varying the number of layers and neurons per layer), and replaced the least absolute deviations (L1) for both the adversarial and cycle-consistency loss with the least square error (L2) (<xref ref-type="bibr" rid="bib35">Mao et al., 2016</xref>). None of the manipulations substantially improved performance.</p></sec><sec id="s4-4-4"><title>Procrustes alignment of factors (PAF)</title><p>We compared ADAN and Cycle-GAN aligners with a manifold-based stabilization method proposed by <xref ref-type="bibr" rid="bib8">Degenhart et al., 2020</xref>, the Procrustes Alignment of Factors (PAF, our term). PAF finds a low-dimensional manifold using Factor Analysis, then applies a Procrustes transformation to the neural manifold of day-0 to align it to that of day-k. The original application of PAF additionally removes electrodes identified as “unstable” and unlikely to contribute to alignment; these are defined as electrodes on day-k that have changed the most with respect to the day-0 manifold, and are removed iteratively until a criterion is met. However, we found that alignment performance did not degrade with the number of included electrodes, so we decided to omit this stability criterion and use all recorded electrodes for all the datasets. As for the GAN aligners, we trained and tested PAF using a Wiener filter and four-fold cross validation.</p></sec></sec><sec id="s4-5"><title>Performance measures</title><sec id="s4-5-1"><title>Decoder accuracy</title><p>To evaluate the performance of decoders mapping M1 neural recordings to motor outputs (either EMG or hand velocity), we used the coefficient of determination (R<sup>2</sup>). The R<sup>2</sup> indicates the proportion of variation of the actual motor output that was predicted by the iBCI decoder; this approach is common in evaluation of iBCI systems (<xref ref-type="bibr" rid="bib36">Morrow and Miller, 2003</xref>). As the motor outputs being decoded are multi-dimensional (7 dimensions for EMG, 2 dimensions for hand velocity), we computed a multivariate R<sup>2</sup> in which, after computing the R<sup>2</sup> for all the single dimensions, we take a weighted average across dimensions, with weights determined by the variance of each dimension. This was implemented using the ‘r2_score’ function of the scikit-learn python package with ‘variance weighted’ for the ‘multioutput’ parameter (<xref ref-type="bibr" rid="bib43">Pedregosa et al., 2011</xref>).</p></sec><sec id="s4-5-2"><title>Maximum mean discrepancy (MMD)</title><p>We used maximum mean discrepancy (MMD) in two contexts. First, we used MMD to evaluate the similarity between the distribution of the aligned day-k neural activity and the day-0 neural activity, as a way to examine the alignment performance (<xref ref-type="fig" rid="fig6">Figure 6</xref>). MMD provides a measure of distance between two multivariate distributions, based on the distances between the mean embeddings of samples drawn from each distribution in a reproducing kernel Hilbert space (<xref ref-type="bibr" rid="bib21">Gretton et al., 2012a</xref>). MMD is symmetric in the two distributions and equals zero if and only if the two distributions are the same. To select our kernel, we followed a technique that has been proved feasible for optimizing kernel choice (<xref ref-type="bibr" rid="bib22">Gretton et al., 2012b</xref>): specifically we employed a family of four Gaussian kernels with width between 5 Hz and 50 Hz. To define a ‘smallest possible’ MMD between aligned day-k and day-0 distributions, we divided neural signals recorded on the same day into non-overlapping folds, and computed MMD between them; we call this the ‘within-session MMD’ in <xref ref-type="fig" rid="fig6">Figure 6</xref>.</p><p>We also use the MMD to quantify the similarity of the distributions of neural activity or motor outputs between pairs of separate recording sessions for each dataset, as a way to quantify the recordings instabilities (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplements 2C</xref> and <xref ref-type="fig" rid="fig2s3">3</xref>). For a pair of sessions, we divided each of them into four non-overlapping folds, and computed the MMD between each fold and its counterpart in the other session, then reported the mean value across folds. We also computed the ‘within-session MMD’ for neural activity/motor outputs for each session, using the same way described above.</p></sec><sec id="s4-5-3"><title>Principal angles</title><p>To evaluate the similarity between neural manifolds of day-0 and day-k before and after alignment, we used principal angles (<xref ref-type="bibr" rid="bib30">Knyazev and Argentati, 2002</xref>). Principal angles provide a metric to quantify the alignment of two subspaces embedded in a higher-dimensional space. For any pair of <italic>C</italic>-dimensional hyperplanes, there are <italic>C</italic> principal angles that exist between them. Following the approach outlined in <xref ref-type="bibr" rid="bib30">Knyazev and Argentati, 2002</xref> and <xref ref-type="bibr" rid="bib10">Elsayed et al., 2016</xref>, these angles are computed as follows: first, we reduce each signal (here the day-0 and day-k neuron firing rates) to 10 dimensions using PCA. Next, recursively for each C=1...10, we identify the pair of principal vectors that are separated by the smallest angle and that are also perpendicular to the prior selected pairs, and report that angle. When two hyperplanes are well-aligned, the leading principal angles between them can be very small, but often the last few angles are quite large. We computed the principal angles using the ‘subspace_angles’ function of the SciPy python package (<xref ref-type="bibr" rid="bib51">Virtanen et al., 2020</xref>).</p><p>To assess whether the angles after neural alignment were significantly small, we compared them to an upper bound provided by the angle between two surrogate subspaces, using the strategy described in <xref ref-type="bibr" rid="bib10">Elsayed et al., 2016</xref>. Briefly, we generated 10,000 random pairs of day-0-like and day-95-like subspaces in which we shuffled the timing of spikes within each neuron, destroying correlation structure while preserving the statistics of neural firing rates within each day. We then computed the principal angles between each pair, and used the 0.1th percentile of the principal angle distribution as the threshold below which angles could be considered smaller than expected by chance given firing rate statistics alone. We also defined a ‘within-day’ bound by computing the principal angles between the day-0 neural recordings of even-numbered and odd-numbered trials, to reduce to a minimum the effect of any within-day drift. If the alignment process is successful, we expect the neural manifolds of day-0 and day-k to have principal angles similar to those of the within-day bound.</p></sec></sec><sec id="s4-6"><title>Statistics</title><p>We applied statistical tests to compare the decoding accuracy over time after neural alignment with Cycle-GAN, ADAN, and PAF. For these comparisons, we ran a linear mixed-effect model with the type of aligner and the number of days elapsed from decoder training as fixed factors and the type of task as a random factor. In addition, we compared the performance of Cycle-GAN and ADAN with different hyperparameter settings, including generator and discriminator learning rates, as well as batch size. For all these comparisons, we used a two-sided Wilcoxon’s signed rank test. We also used a two-sided Wilcoxon’s signed rank to test whether there was a significant difference between any two methods when limited amount of training data was used for alignment. Finally, we compared the MMD of neural distributions between all pairs of day-0/day-k sessions before and after alignment with Cycle-GAN and ADAN. Since the distributions pre and after alignment are independent, we used a two-sided Wilcoxon’s rank sum test. For all the statistical models, we used a significance threshold of <italic>α</italic>=0.05. When making pairwise comparisons between the three aligners, we used a Bonferroni correction of 3. Sample sizes are reported in the corresponding results section.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Investigation</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Supervision, Funding acquisition, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Supervision, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All surgical and experimental procedures were approved by the Institutional Animal Care and Use Committee (IACUC) of Northwestern University under protocol #IS00000367, and are consistent with the Guide for the Care and Use of Laboratory Animals.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-84296-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data from Monkey J is available via Dryad at: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.cvdncjt7n">https://doi.org/10.5061/dryad.cvdncjt7n</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Rizzoglio</surname><given-names>F</given-names></name><name><surname>Thacker</surname><given-names>S</given-names></name><name><surname>Miller</surname><given-names>L</given-names></name></person-group><source>Dryad Digital Repository</source><year iso-8601-date="2023">2023</year><data-title>Using adversarial networks to extend brain computer interface decoding accuracy over time</data-title><pub-id pub-id-type="doi">10.5061/dryad.cvdncjt7n</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Ali Farshchian, Sara Solla and Ege Altan for valuable discussions. We thank current and former members of the Miller Limb Lab, including Stephanie Naufel, Matthew Perich, and Christian Ethier, for their contributions to data collection. The work was supported in part by grants to LEM (R01 NS053603, R01 NS074044).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Almahairi</surname><given-names>A</given-names></name><name><surname>Rajeshwar</surname><given-names>S</given-names></name><name><surname>Sordoni</surname><given-names>A</given-names></name><name><surname>Bachman</surname><given-names>P</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Augmented cyclegan: Learning many-to-many mappings from unpaired data</article-title><conf-name>International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altan</surname><given-names>E</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Perreault</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Estimating the dimensionality of the manifold underlying multi-electrode neural recordings</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008591</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008591</pub-id><pub-id pub-id-type="pmid">34843461</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Arjovsky</surname><given-names>M</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards principled methods for training generative adversarial networks</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1701.07875">https://arxiv.org/abs/1701.07875</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Arjovsky</surname><given-names>M</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Wasserstein generative adversarial networks</article-title><conf-name>Proceedings of the 34th International Conference on Machine Learning (ICML)</conf-name><fpage>214</fpage><lpage>223</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Brock</surname><given-names>A</given-names></name><name><surname>Donahue</surname><given-names>J</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Large Scale GAN Training for High Fidelity Natural Image Synthesis</article-title><conf-name>7th International Conference on Learning Representations, ICLR 2019</conf-name></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cherian</surname><given-names>A</given-names></name><name><surname>Krucoff</surname><given-names>MO</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Motor cortical prediction of EMG: evidence that a kinetic brain-machine interface may be robust across altered movement dynamics</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>564</fpage><lpage>575</lpage><pub-id pub-id-type="doi">10.1152/jn.00553.2010</pub-id><pub-id pub-id-type="pmid">21562185</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dangi</surname><given-names>S</given-names></name><name><surname>Gowda</surname><given-names>S</given-names></name><name><surname>Moorman</surname><given-names>HG</given-names></name><name><surname>Orsborn</surname><given-names>AL</given-names></name><name><surname>So</surname><given-names>K</given-names></name><name><surname>Shanechi</surname><given-names>M</given-names></name><name><surname>Carmena</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Continuous closed-loop decoder adaptation with a recursive maximum likelihood algorithm allows for rapid performance acquisition in brain-machine interfaces</article-title><source>Neural Computation</source><volume>26</volume><fpage>1811</fpage><lpage>1839</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00632</pub-id><pub-id pub-id-type="pmid">24922501</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Degenhart</surname><given-names>AD</given-names></name><name><surname>Bishop</surname><given-names>WE</given-names></name><name><surname>Oby</surname><given-names>ER</given-names></name><name><surname>Tyler-Kabara</surname><given-names>EC</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Yu</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Stabilization of a brain-computer interface via the alignment of low-dimensional spaces of neural activity</article-title><source>Nature Biomedical Engineering</source><volume>4</volume><fpage>672</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1038/s41551-020-0542-9</pub-id><pub-id pub-id-type="pmid">32313100</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Downey</surname><given-names>JE</given-names></name><name><surname>Schwed</surname><given-names>N</given-names></name><name><surname>Chase</surname><given-names>SM</given-names></name><name><surname>Schwartz</surname><given-names>AB</given-names></name><name><surname>Collinger</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Intracortical recording stability in human brain-computer interface users</article-title><source>Journal of Neural Engineering</source><volume>15</volume><elocation-id>046016</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aab7a0</pub-id><pub-id pub-id-type="pmid">29553484</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elsayed</surname><given-names>GF</given-names></name><name><surname>Lara</surname><given-names>AH</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reorganization between preparatory and movement population responses in motor cortex</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13239</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13239</pub-id><pub-id pub-id-type="pmid">27807345</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Farahani</surname><given-names>A</given-names></name><name><surname>Voghoei</surname><given-names>S</given-names></name><name><surname>Rasheed</surname><given-names>K</given-names></name><name><surname>Arabnia</surname><given-names>HR</given-names></name></person-group><year iso-8601-date="2021">2021</year><chapter-title>A brief review of domain adaptation</chapter-title><person-group person-group-type="editor"><name><surname>Stahlbock</surname><given-names>R</given-names></name><name><surname>Weiss</surname><given-names>GM</given-names></name><name><surname>Abou-Nasr</surname><given-names>M</given-names></name><name><surname>Yang</surname><given-names>CY</given-names></name><name><surname>Arabnia</surname><given-names>HR</given-names></name><name><surname>Deligiannidis</surname><given-names>L</given-names></name></person-group><source>Advances in Data Science and Information Engineering</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name><fpage>877</fpage><lpage>894</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-71704-9</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Farnia</surname><given-names>F</given-names></name><name><surname>Ozdaglar</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Do GANs always have Nash equilibria</article-title><conf-name>Proceedings of the 37th International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Farshchian</surname><given-names>A</given-names></name><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Cohen</surname><given-names>JP</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Adversarial Domain Adaptation for Stable Brain-Machine Interfaces</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1810.00045">https://arxiv.org/abs/1810.00045</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><volume>94</volume><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id><pub-id pub-id-type="pmid">28595054</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Chowdhury</surname><given-names>RH</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Long-term stability of cortical population dynamics underlying consistent behavior</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>260</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0555-4</pub-id><pub-id pub-id-type="pmid">31907438</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ganin</surname><given-names>Y</given-names></name><name><surname>Lempitsky</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Unsupervised domain adaptation by backpropagation</article-title><conf-name>International Conference on Machine Learning</conf-name></element-citation></ref><ref id="bib17"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>B</given-names></name><name><surname>Dutta</surname><given-names>IK</given-names></name><name><surname>Carlson</surname><given-names>A</given-names></name><name><surname>Totaro</surname><given-names>M</given-names></name><name><surname>Bayoumi</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>An Empirical Analysis of Generative Adversarial Network Training Times with Varying Batch Sizes</article-title><conf-name>2020 11th IEEE Annual Ubiquitous Computing, Electronics &amp; Mobile Communication Conference (UEMCON</conf-name><conf-loc>New York, NY, USA</conf-loc><pub-id pub-id-type="doi">10.1109/UEMCON51285.2020.9298092</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Goodfellow</surname><given-names>IJ</given-names></name><name><surname>Pouget-Abadie</surname><given-names>J</given-names></name><name><surname>Mirza</surname><given-names>M</given-names></name><name><surname>Xu</surname><given-names>B</given-names></name><name><surname>Warde-Farley</surname><given-names>D</given-names></name><name><surname>Ozair</surname><given-names>S</given-names></name><name><surname>Courville</surname><given-names>A</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Generative Adversarial Networks</article-title><conf-name>NeurlPS Proceedings</conf-name><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gower</surname><given-names>JC</given-names></name><name><surname>Dijksterhuis</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>Procrustes Problems</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780198510581.001.0001</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Goyal</surname><given-names>P</given-names></name><name><surname>Dollár</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>Noordhuis</surname><given-names>P</given-names></name><name><surname>Wesolowski</surname><given-names>L</given-names></name><name><surname>Kyrola</surname><given-names>A</given-names></name><name><surname>Tulloch</surname><given-names>A</given-names></name><name><surname>Jia</surname><given-names>Y</given-names></name><name><surname>He</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.02677">https://arxiv.org/abs/1706.02677</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gretton</surname><given-names>A</given-names></name><name><surname>Borgwardt</surname><given-names>KM</given-names></name><name><surname>Rasch</surname><given-names>MJ</given-names></name><name><surname>Schölkopf</surname><given-names>B</given-names></name><name><surname>Smola</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012a</year><article-title>A kernel two-sample test</article-title><source>The Journal of Machine Learning Research</source><volume>13</volume><fpage>723</fpage><lpage>773</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gretton</surname><given-names>A</given-names></name><name><surname>Sejdinovic</surname><given-names>D</given-names></name><name><surname>Strathmann</surname><given-names>H</given-names></name><name><surname>Balakrishnan</surname><given-names>S</given-names></name><name><surname>Pontil</surname><given-names>M</given-names></name><name><surname>Fukumizu</surname><given-names>K</given-names></name><name><surname>Sriperumbudur</surname><given-names>BK</given-names></name></person-group><year iso-8601-date="2012">2012b</year><article-title>Optimal kernel choice for large-scale two-sample tests</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Heusel</surname><given-names>M</given-names></name><name><surname>Ramsauer</surname><given-names>H</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Nessler</surname><given-names>B</given-names></name><name><surname>Hochreiter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Gans trained by a two time-scale update rule converge to a local nash equilibrium</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hochreiter</surname><given-names>S</given-names></name><name><surname>Schmidhuber</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Long short-term memory</article-title><source>Neural Computation</source><volume>9</volume><fpage>1735</fpage><lpage>1780</lpage><pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Isola</surname><given-names>P</given-names></name><name><surname>Zhu</surname><given-names>JY</given-names></name><name><surname>Zhou</surname><given-names>T</given-names></name><name><surname>Efros</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Image-to-Image Translation with Conditional Adversarial Networks</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Honolulu, HI</conf-loc><fpage>1125</fpage><lpage>1134</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.632</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarosiewicz</surname><given-names>B</given-names></name><name><surname>Sarma</surname><given-names>AA</given-names></name><name><surname>Bacher</surname><given-names>D</given-names></name><name><surname>Masse</surname><given-names>NY</given-names></name><name><surname>Simeral</surname><given-names>JD</given-names></name><name><surname>Sorice</surname><given-names>B</given-names></name><name><surname>Oakley</surname><given-names>EM</given-names></name><name><surname>Blabe</surname><given-names>C</given-names></name><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>Gilja</surname><given-names>V</given-names></name><name><surname>Cash</surname><given-names>SS</given-names></name><name><surname>Eskandar</surname><given-names>EN</given-names></name><name><surname>Friehs</surname><given-names>G</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Virtual typing by people with tetraplegia using a self-calibrating intracortical brain-computer interface</article-title><source>Science Translational Medicine</source><volume>7</volume><elocation-id>313ra179</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aac7328</pub-id><pub-id pub-id-type="pmid">26560357</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Karpowicz</surname><given-names>BM</given-names></name><name><surname>Ali</surname><given-names>YH</given-names></name><name><surname>Wimalasena</surname><given-names>LN</given-names></name><name><surname>Sedler</surname><given-names>AR</given-names></name><name><surname>Keshtkaran</surname><given-names>MR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Stabilizing Brain-Computer Interfaces through Alignment of Latent Dynamics Leveraging Manifolds and Dynamics to Stabilize iBCI Decoding</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.04.06.487388</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Keskar</surname><given-names>NS</given-names></name><name><surname>Mudigere</surname><given-names>D</given-names></name><name><surname>Nocedal</surname><given-names>J</given-names></name><name><surname>Smelyanskiy</surname><given-names>M</given-names></name><name><surname>Tang</surname><given-names>PTP</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>On large-batch training for deep learning: generalization gap and sharp minima</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.04836">https://arxiv.org/abs/1609.04836</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adam: A method for stochastic optimization</article-title><conf-name>3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings</conf-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knyazev</surname><given-names>AV</given-names></name><name><surname>Argentati</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Principal angles between subspaces in an <italic>A</italic> -Based scalar product: algorithms and perturbation estimates</article-title><source>SIAM Journal on Scientific Computing</source><volume>23</volume><fpage>2008</fpage><lpage>2040</lpage><pub-id pub-id-type="doi">10.1137/S1064827500377332</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kurach</surname><given-names>K</given-names></name><name><surname>Lucic</surname><given-names>M</given-names></name><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Michalski</surname><given-names>M</given-names></name><name><surname>Gelly</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The gan landscape: losses, architectures, regularization, and normalization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1807.04720">https://arxiv.org/abs/1807.04720</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2023">2023a</year><data-title>Adversarial_BCI</data-title><version designator="swh:1:rev:187857d4963dcffbdbf633502b1e41dafa4cd09a">swh:1:rev:187857d4963dcffbdbf633502b1e41dafa4cd09a</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:61f8576b46cb85e1c8733545c9e174609fef1986;origin=https://github.com/limblab/adversarial_BCI;visit=swh:1:snp:41e68e953c61172eddae0e0f6333b52cc901855f;anchor=swh:1:rev:187857d4963dcffbdbf633502b1e41dafa4cd09a">https://archive.softwareheritage.org/swh:1:dir:61f8576b46cb85e1c8733545c9e174609fef1986;origin=https://github.com/limblab/adversarial_BCI;visit=swh:1:snp:41e68e953c61172eddae0e0f6333b52cc901855f;anchor=swh:1:rev:187857d4963dcffbdbf633502b1e41dafa4cd09a</ext-link></element-citation></ref><ref id="bib33"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2023">2023b</year><data-title>Xds</data-title><version designator="swh:1:rev:104719352b92cfa9200f2dd91902151295aceea9">swh:1:rev:104719352b92cfa9200f2dd91902151295aceea9</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e97a97a1e6099a7040b6ea9182fc986df4483179;origin=https://github.com/limblab/xds;visit=swh:1:snp:b2e04fad3346056accff8ba0c0945cada3e782b0;anchor=swh:1:rev:104719352b92cfa9200f2dd91902151295aceea9">https://archive.softwareheritage.org/swh:1:dir:e97a97a1e6099a7040b6ea9182fc986df4483179;origin=https://github.com/limblab/xds;visit=swh:1:snp:b2e04fad3346056accff8ba0c0945cada3e782b0;anchor=swh:1:rev:104719352b92cfa9200f2dd91902151295aceea9</ext-link></element-citation></ref><ref id="bib34"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2023">2023c</year><data-title>Decoder_Standard</data-title><version designator="swh:1:rev:032a8491381a9ac9267b0bd8003d84c10743aa35">swh:1:rev:032a8491381a9ac9267b0bd8003d84c10743aa35</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d7a16712f127ca4ed63f266b4340286fd3168fd9;origin=https://github.com/xuanma/decoder_standard;visit=swh:1:snp:2027d231fb61aae42b39f3d1c7f35de755872401;anchor=swh:1:rev:032a8491381a9ac9267b0bd8003d84c10743aa35">https://archive.softwareheritage.org/swh:1:dir:d7a16712f127ca4ed63f266b4340286fd3168fd9;origin=https://github.com/xuanma/decoder_standard;visit=swh:1:snp:2027d231fb61aae42b39f3d1c7f35de755872401;anchor=swh:1:rev:032a8491381a9ac9267b0bd8003d84c10743aa35</ext-link></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>X</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Xie</surname><given-names>H</given-names></name><name><surname>Lau</surname><given-names>RYK</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Multi-Class Generative Adversarial Networks with the L2 Loss Function</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1611.04076">https://arxiv.org/abs/1611.04076</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrow</surname><given-names>MM</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Prediction of muscle activity by populations of sequentially recorded primary motor cortex neurons</article-title><source>Journal of Neurophysiology</source><volume>89</volume><fpage>2279</fpage><lpage>2288</lpage><pub-id pub-id-type="doi">10.1152/jn.00632.2002</pub-id><pub-id pub-id-type="pmid">12612022</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Nagarajan</surname><given-names>V</given-names></name><name><surname>Kolter</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Gradient descent GAN optimization is locally stable</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naufel</surname><given-names>S</given-names></name><name><surname>Glaser</surname><given-names>JI</given-names></name><name><surname>Kording</surname><given-names>KP</given-names></name><name><surname>Perreault</surname><given-names>EJ</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A muscle-activity-dependent gain between motor cortex and EMG</article-title><source>Journal of Neurophysiology</source><volume>121</volume><fpage>61</fpage><lpage>73</lpage><pub-id pub-id-type="doi">10.1152/jn.00329.2018</pub-id><pub-id pub-id-type="pmid">30379603</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orsborn</surname><given-names>AL</given-names></name><name><surname>Dangi</surname><given-names>S</given-names></name><name><surname>Moorman</surname><given-names>HG</given-names></name><name><surname>Carmena</surname><given-names>JM</given-names></name><name><surname>Member</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Closed-loop decoder adaptation on intermediate time-scales facilitates rapid BMI performance improvements independent of decoder initialization conditions</article-title><source>IEEE Transactions on Neural Systems and Rehabilitation Engineering</source><volume>20</volume><fpage>468</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1109/TNSRE.2012.2185066</pub-id><pub-id pub-id-type="pmid">22772374</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>SJ</given-names></name><name><surname>Tsang</surname><given-names>IW</given-names></name><name><surname>Kwok</surname><given-names>JT</given-names></name><name><surname>Yang</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Domain adaptation via transfer component analysis</article-title><source>IEEE Transactions on Neural Networks</source><volume>22</volume><fpage>199</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1109/TNN.2010.2091281</pub-id><pub-id pub-id-type="pmid">21095864</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pan</surname><given-names>Z</given-names></name><name><surname>Yu</surname><given-names>W</given-names></name><name><surname>Yi</surname><given-names>X</given-names></name><name><surname>Khan</surname><given-names>A</given-names></name><name><surname>Yuan</surname><given-names>F</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recent progress on generative adversarial networks (GANs): a survey</article-title><source>IEEE Access</source><volume>7</volume><fpage>36322</fpage><lpage>36333</lpage><pub-id pub-id-type="doi">10.1109/ACCESS.2019.2905015</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pandarinath</surname><given-names>C</given-names></name><name><surname>O’Shea</surname><given-names>DJ</given-names></name><name><surname>Collins</surname><given-names>J</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Trautmann</surname><given-names>EM</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Sussillo</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Inferring single-trial neural population dynamics using sequential auto-encoders</article-title><source>Nature Methods</source><volume>15</volume><fpage>805</fpage><lpage>815</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0109-9</pub-id><pub-id pub-id-type="pmid">30224673</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Michel</surname><given-names>V</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Grisel</surname><given-names>O</given-names></name><name><surname>Blondel</surname><given-names>M</given-names></name><name><surname>Prettenhofer</surname><given-names>P</given-names></name><name><surname>Weiss</surname><given-names>R</given-names></name><name><surname>Dubourg</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>The Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perge</surname><given-names>JA</given-names></name><name><surname>Homer</surname><given-names>ML</given-names></name><name><surname>Malik</surname><given-names>WQ</given-names></name><name><surname>Cash</surname><given-names>S</given-names></name><name><surname>Eskandar</surname><given-names>E</given-names></name><name><surname>Friehs</surname><given-names>G</given-names></name><name><surname>Donoghue</surname><given-names>JP</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Intra-day signal instabilities affect decoding performance in an intracortical neural interface system</article-title><source>Journal of Neural Engineering</source><volume>10</volume><elocation-id>036004</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/10/3/036004</pub-id><pub-id pub-id-type="pmid">23574741</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Salimans</surname><given-names>T</given-names></name><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Zaremba</surname><given-names>W</given-names></name><name><surname>Cheung</surname><given-names>V</given-names></name><name><surname>Radford</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Improved techniques for training GANs</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Saxena</surname><given-names>D</given-names></name><name><surname>Cao</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Generative adversarial networks (GANs) challenges, solutions, and future directions</article-title><source>ACM Computing Surveys</source><volume>54</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1145/3446374</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schönemann</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="1966">1966</year><article-title>A generalized solution of the orthogonal procrustes problem</article-title><source>Psychometrika</source><volume>31</volume><fpage>1</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1007/BF02289451</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Jozefowicz</surname><given-names>R</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Pandarinath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>LFADS - Latent Factor Analysis via Dynamical Systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1608.06315">https://arxiv.org/abs/1608.06315</ext-link></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sussillo</surname><given-names>D</given-names></name><name><surname>Stavisky</surname><given-names>SD</given-names></name><name><surname>Kao</surname><given-names>JC</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Making brain-machine interfaces robust to future neural variability</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13749</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13749</pub-id><pub-id pub-id-type="pmid">27958268</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tzeng</surname><given-names>E</given-names></name><name><surname>Hoffman</surname><given-names>J</given-names></name><name><surname>Saenko</surname><given-names>K</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Adversarial Discriminative Domain Adaptation</article-title><conf-name>2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</conf-name><conf-loc>Honolulu, HI</conf-loc><fpage>7167</fpage><lpage>7176</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2017.316</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Virtanen</surname><given-names>P</given-names></name><name><surname>Gommers</surname><given-names>R</given-names></name><name><surname>Oliphant</surname><given-names>TE</given-names></name><name><surname>Haberland</surname><given-names>M</given-names></name><name><surname>Reddy</surname><given-names>T</given-names></name><name><surname>Cournapeau</surname><given-names>D</given-names></name><name><surname>Burovski</surname><given-names>E</given-names></name><name><surname>Peterson</surname><given-names>P</given-names></name><name><surname>Weckesser</surname><given-names>W</given-names></name><name><surname>Bright</surname><given-names>J</given-names></name><name><surname>van der Walt</surname><given-names>SJ</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Wilson</surname><given-names>J</given-names></name><name><surname>Millman</surname><given-names>KJ</given-names></name><name><surname>Mayorov</surname><given-names>N</given-names></name><name><surname>Nelson</surname><given-names>ARJ</given-names></name><name><surname>Jones</surname><given-names>E</given-names></name><name><surname>Kern</surname><given-names>R</given-names></name><name><surname>Larson</surname><given-names>E</given-names></name><name><surname>Carey</surname><given-names>CJ</given-names></name><name><surname>Polat</surname><given-names>İ</given-names></name><name><surname>Feng</surname><given-names>Y</given-names></name><name><surname>Moore</surname><given-names>EW</given-names></name><name><surname>VanderPlas</surname><given-names>J</given-names></name><name><surname>Laxalde</surname><given-names>D</given-names></name><name><surname>Perktold</surname><given-names>J</given-names></name><name><surname>Cimrman</surname><given-names>R</given-names></name><name><surname>Henriksen</surname><given-names>I</given-names></name><name><surname>Quintero</surname><given-names>EA</given-names></name><name><surname>Harris</surname><given-names>CR</given-names></name><name><surname>Archibald</surname><given-names>AM</given-names></name><name><surname>Ribeiro</surname><given-names>AH</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>van Mulbregt</surname><given-names>P</given-names></name><collab>SciPy 1.0 Contributors</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Author Correction: SciPy 1.0: fundamental algorithms for scientific computing in Python</article-title><source>Nature Methods</source><volume>17</volume><fpage>261</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1038/s41592-020-0772-5</pub-id><pub-id pub-id-type="pmid">32094914</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Volgushev</surname><given-names>M</given-names></name><name><surname>Chauvette</surname><given-names>S</given-names></name><name><surname>Mukovski</surname><given-names>M</given-names></name><name><surname>Timofeev</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Precise long-range synchronization of activity and silence in neocortical neurons during slow-wave oscillations [corrected]</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>5665</fpage><lpage>5672</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0279-06.2006</pub-id><pub-id pub-id-type="pmid">16723523</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willett</surname><given-names>FR</given-names></name><name><surname>Avansino</surname><given-names>DT</given-names></name><name><surname>Hochberg</surname><given-names>LR</given-names></name><name><surname>Henderson</surname><given-names>JM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>High-performance brain-to-text communication via handwriting</article-title><source>Nature</source><volume>593</volume><fpage>249</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03506-2</pub-id><pub-id pub-id-type="pmid">33981047</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>J</given-names></name><name><surname>Mathieu</surname><given-names>M</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Energy-Based Generative Adversarial Network</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1609.03126">https://arxiv.org/abs/1609.03126</ext-link></element-citation></ref><ref id="bib55"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Zhu</surname><given-names>JY</given-names></name><name><surname>Park</surname><given-names>T</given-names></name><name><surname>Isola</surname><given-names>P</given-names></name><name><surname>Efros</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial Networks</article-title><conf-name>2017 IEEE International Conference on Computer Vision (ICCV</conf-name><conf-loc>Venice</conf-loc><fpage>2223</fpage><lpage>2232</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.244</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Detailed methods for iBCI decoders and aligners</title><sec sec-type="appendix" id="s8-1"><title>Testing neural alignment on your data</title><p>We provide a step-by-step tutorial on the use of CycleGAN and ADAN for neural alignment on GitHub in our adversarial_BCI repository: <ext-link ext-link-type="uri" xlink:href="https://github.com/limblab/adversarial_BCI">https://github.com/limblab/adversarial_BCI</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:61f8576b46cb85e1c8733545c9e174609fef1986;origin=https://github.com/limblab/adversarial_BCI;visit=swh:1:snp:41e68e953c61172eddae0e0f6333b52cc901855f;anchor=swh:1:rev:187857d4963dcffbdbf633502b1e41dafa4cd09a">swh:1:rev:187857d4963dcffbdbf633502b1e41dafa4cd09a</ext-link>; <xref ref-type="bibr" rid="bib32">Ma, 2023a</xref>) in the Jupyter notebooks ADAN_aligner.ipynb and Cycle_GAN_aligner.ipynb. Briefly, the steps covered by these notebooks are as follows:</p><list list-type="order"><list-item><p>Set up requirements. In addition to common Python data science libraries, our alignment code makes use of the following more specialized packages:</p><list list-type="alpha-lower"><list-item><p>XDS cross-platform data structure, documentation for which can be found at <ext-link ext-link-type="uri" xlink:href="https://github.com/limblab/XDS">https://github.com/limblab/XDS</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:e97a97a1e6099a7040b6ea9182fc986df4483179;origin=https://github.com/limblab/xds;visit=swh:1:snp:b2e04fad3346056accff8ba0c0945cada3e782b0;anchor=swh:1:rev:104719352b92cfa9200f2dd91902151295aceea9">swh:1:rev:104719352b92cfa9200f2dd91902151295aceea9</ext-link>; <xref ref-type="bibr" rid="bib33">Ma, 2023b</xref>). Datasets should be packaged into the XDS format for analysis using the provided notebooks, or else datasets should be formatted into lists of numpy arrays as described in the notebook (see documentation on variables day0_spike and day0_EMG in the notebook).</p></list-item><list-item><p>A simple Wiener filter decoder module, found at <ext-link ext-link-type="uri" xlink:href="https://github.com/xuanma/decoder_standard">https://github.com/xuanma/decoder_standard</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d7a16712f127ca4ed63f266b4340286fd3168fd9;origin=https://github.com/xuanma/decoder_standard;visit=swh:1:snp:2027d231fb61aae42b39f3d1c7f35de755872401;anchor=swh:1:rev:032a8491381a9ac9267b0bd8003d84c10743aa35">swh:1:rev:032a8491381a9ac9267b0bd8003d84c10743aa35</ext-link>; <xref ref-type="bibr" rid="bib34">Ma, 2023c</xref>).</p></list-item><list-item><p>Pytorch, a Python library for working with deep neural network models, is required for Cycle-GAN. Tensorflow 1.* is required for ADAN. Note that because our alignment models are quick to train, they do not require a computer with a GPU.</p></list-item></list></list-item><list-item><p>Data preprocessing. Given extracellular spike trains from an implanted recording device (here a 96-channel Utah array) we compute spike counts per channel using 50 ms time binning, then smooth these spike counts using a Gaussian kernel with a standard deviation of 100 ms. Our provided notebook loads and pre-processes neural recording data from two days of experiments, namely the source and target days. Similarly, raw EMG recordings are pre-processed via rectification and filtering, as described in the Methods section of this manuscript.</p></list-item><list-item><p>Trial selection. In our demo notebook, we use only the first 160 trials on a given recording day. Selection of trials is achieved by indexing into our day0_spike, day0_EMG, dayk_spike, and dayk_EMG lists, which contain the now-preprocessed data following spike count smoothing and EMG envelope extraction, segmented into trials.</p></list-item><list-item><p>Train the day-0 decoders. The goal of alignment is to be able to use a previously trained neural decoder to predict EMG activity on neural recording data from a new experimental session. In our work, we use a simple Wiener filter decoder as our “previously trained decoder”. In the provided notebook, we explain the design of the Wiener filter decoder, and provide a function train_wiener_filter to train a Wiener filter decoder on the day-0 data as well as wrapper code implementing four-fold cross-validation. The training code applies data splits, formats data for the decoder, and trains and tests the Wiener filter decoder for each split, reporting back multi-variate R<sup>2</sup> values and saving the best-performing decoder to a .npy file for further use. The notebook also uses a function plot_actual_and_pred_EMG to plot the predicted EMG signals using the decoder alongside the corresponding ground-truth EMG signals.</p></list-item><list-item><p>Define the blocks for the Cycle-GAN (or ADAN) aligner. The next step is to define the architecture of the alignment model. Model definition code is provided in both notebooks; for example, they Cycle-GAN notebook defines Generator and Discriminator classes, each of which has an __init__ function to initialize the network architecture and a function forward which takes an input firing rate signal and returns a transformed version of that signal.</p></list-item><list-item><p>Train the Cycle-GAN (or ADAN) aligner. Having designed the architecture of our model, we next provide a function train_cycle_gan_aligner to carry out training. This function first carries out several setup steps:</p><list list-type="alpha-lower"><list-item><p>Specifying the value of model hyperparameters (which can be set by the user).</p></list-item><list-item><p>Initializing two Generator and two Discriminator networks.</p></list-item><list-item><p>Defining the type of loss function (MSE or L1-penalized) and optimizer to be used by the model; these are standard terms provided by torch.</p></list-item><list-item><p>Initializing DataLoader objects to feed the training or test dataset into the torch model.</p><p>It then carries out the GAN training loop. Briefly, each iteration does the following, where “loss” is by default the mean-squared error between two signals:</p></list-item><list-item><p>Sample a pair of neural recording trials, one from day-0 and one from day-k.</p></list-item><list-item><p>Compute the identity loss, which takes the error between day-k data and its transformation by the day-0 Generator (and similarly for day-0 and the day-k Generator). This loss regularizes the Generator to be close to an identity mapping when provided with samples from its target domain, an approach used in the original Cycle-GAN manuscript and adopted from Taigman, Polyak, &amp; Wolf 2017.</p></list-item><list-item><p>Compute the GAN loss for the day-0 data using the day-0 Generator + Discriminator (and similarly for day-k). For the day-0 Generator, this loss is the accuracy of the Discriminator in distinguishing true day-k data from synthetic day-k data; for the day-0 Discriminator, the loss is the error rather than the accuracy.</p></list-item><list-item><p>Compute the cycle-consistency loss for the day-0 data by feeding the synthetic day-k data through the day-k Generator and computing the error between this output and the original day-0 data (and similarly for day-k).</p></list-item><list-item><p>Sum applicable losses for each Generator (identity, GAN-Generator, and cycle-consistency) and each Discriminator (GAN-Discriminator only).</p></list-item><list-item><p>Compute the gradient with respect to each loss, and pass this information to the optimizer, which will update the model parameters at the end of each epoch.</p></list-item><list-item><p>To monitor training progress, the aligner is evaluated on the validation set every 10 epochs, and performance is logged.</p></list-item></list></list-item><list-item><p>Test the trained aligner. The provided function test_cycle_gan_aligner takes as input a trained aligner and a neural dataset, and returns as output the aligned version of that dataset. It does this by passing the data through the trained model network.</p></list-item><list-item><p>Plot performance. The notebook next shows how to evaluate the quality of the previously trained day-0 decoder when fed aligned neural signals. As in step 3 above, the provided function plot_actual_and_pred_EMG is used, but now we are feeding aligned day-k neural activity into the decoder and comparing the decoder’s prediction to the day-0 EMG.</p></list-item></list><p>We advise the reader to consult the complete Jupyter notebook for additional commentary and documentation of these steps. In addition to this practical guide to use of Cycle-GAN for alignment, we have included additional technical documentation of the alignment process in the following sections.</p></sec><sec sec-type="appendix" id="s8-2"><title>iBCI day-0 decoders</title><p>We used a Wiener filter (<xref ref-type="bibr" rid="bib6">Cherian et al., 2011</xref>) as the day-0 iBCI decoder:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mtext>T</mml:mtext><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a q-dimensional vector (q is 2 for hand velocity prediction and varied with the number of recorded EMGs for EMG prediction, see <xref ref-type="table" rid="app1table1">Appendix 1—table 1</xref>) representing the motor outputs to be predicted at time <italic>t</italic>, while <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a p-dimensional vector for the inputs to the Wiener filter at time <italic>t</italic>, and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">β</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>τ</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a <inline-formula><mml:math id="inf4"><mml:semantics><mml:mrow><mml:mtext>q</mml:mtext><mml:mo>×</mml:mo><mml:mtext>p</mml:mtext></mml:mrow></mml:semantics></mml:math></inline-formula> matrix corresponding to the filter parameters for time step <inline-formula><mml:math id="inf5"><mml:semantics><mml:mi>τ</mml:mi></mml:semantics></mml:math></inline-formula>. For Cycle-GAN, <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the full-dimensional neural firing rates, thus p equals to the number of the electrodes in the cortical array (denoted as C). For ADAN, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the projection of the neural firing rates in a nonlinear latent space found by an autoencoder (see next section for details). For PAF, <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is the projection of the neural firing rates in a linear latent space found by factor analysis. We set <inline-formula><mml:math id="inf9"><mml:semantics><mml:mrow><mml:mtext>p</mml:mtext><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula> for both ADAN and PAF. We can also write <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> in matrix form:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mi mathvariant="bold-italic">Y</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mi mathvariant="bold-italic">B</mml:mi></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic><bold>Y</bold></italic> is a <inline-formula><mml:math id="inf10"><mml:semantics><mml:mrow><mml:mtext>M</mml:mtext><mml:mo>×</mml:mo><mml:mtext>q</mml:mtext></mml:mrow></mml:semantics></mml:math></inline-formula> matrix for the motor outputs to be predicted with M being the number of samples, <italic><bold>X</bold></italic> is a <inline-formula><mml:math id="inf11"><mml:semantics><mml:mrow><mml:mtext>M</mml:mtext><mml:mo>×</mml:mo><mml:mo>(</mml:mo><mml:mtext>T</mml:mtext><mml:mo>×</mml:mo><mml:mtext>p</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:semantics></mml:math></inline-formula> matrix, and <italic><bold>B</bold></italic> is a <inline-formula><mml:math id="inf12"><mml:semantics><mml:mrow><mml:mo>(</mml:mo><mml:mtext>T</mml:mtext><mml:mo>×</mml:mo><mml:mtext>p</mml:mtext><mml:mo>)</mml:mo><mml:mo>×</mml:mo><mml:mtext>q</mml:mtext></mml:mrow></mml:semantics></mml:math></inline-formula> matrix for the regression coefficients to be estimated. We also added an additional bias term for both <italic><bold>X</bold></italic> and <italic><bold>B</bold>. <bold>B</bold></italic> was determined by a ridge regression estimator:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">B</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mtext>T </mml:mtext></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mi mathvariant="bold-italic">I</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:msup><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mtext>T </mml:mtext></mml:mrow></mml:msup><mml:mi mathvariant="bold-italic">Y</mml:mi></mml:mrow></mml:math></disp-formula></p><p>We chose a ridge regression to limit the risk of decoder overfitting by penalizing solutions with large regression coefficients with the regularization term <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The value of <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was chosen by sweeping a range of 20 values between 10 and 10<sup>5</sup> on a logarithmic scale. We used a 4-fold cross validation to train the decoder for each aligner type and ultimately selected the model with the highest R<sup>2</sup> on the test set as the fixed day-0 decoder.</p></sec><sec sec-type="appendix" id="s8-3"><title>ADAN day-0 training</title><p>The day-0 wiener filter for ADAN was built from a nonlinear latent space estimated from day-0 neural firing rates using an autoencoder (AE) originally described in <xref ref-type="bibr" rid="bib13">Farshchian et al., 2018</xref>. The AE consists of an input layer, five hidden layers and an output layer. The input and the output layers have C units, while the hidden layers (from input to output) have 64, 32, 10, 32 and 64 units, respectively. Hence, the AE compresses the C-dimensional neural firing rates into a 10-dimensional latent representation. The units in the layer and the output layers as well as those in the latent layer have linear activation functions, while units in the remaining hidden layers have a nonlinear one (exponential linear unit, ELU). The AE is trained to minimize the reconstruction error defined as the mean square error (MSE) between the input and the output data. When day-0 neural firing rates <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are fed through the AE, the latent layer activity <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the corresponding reconstructions <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> are obtained. The 10-dimensional latent activity <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is then mapped onto the q-dimensional motor output vector through a long-short-term memory (LSTM, <xref ref-type="bibr" rid="bib24">Hochreiter and Schmidhuber, 1997</xref>):<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the actual motor output (either EMG or hand velocity) recorded at day-0 and <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is its prediction with the LSTM. The LSTM is designed with one layer and a number of units that equals the number of recorded EMGs (if the motor output is EMG) or two (if the motor output is hand velocity). The AE and the LSTM are simultaneously trained by minimizing a loss function that accounts for both the MSE of the reconstruction of the firing rates (<inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">E</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>) and the MSE of the motor output predictions (<inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mi>λ</mml:mi><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi mathvariant="normal">M</mml:mi></mml:mfrac><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="italic">i</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>λ</mml:mi><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo symmetric="true">‖</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">y</mml:mi><mml:mo mathvariant="bold" stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">y</mml:mi></mml:mrow><mml:mo symmetric="true">‖</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where M is the total number of training samples. The weighting factor <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi>λ</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> equalizes the contribution of the two terms so that the learning algorithm does not prioritize one over the other. For each training epoch, <inline-formula><mml:math id="inf24"><mml:semantics><mml:mi>λ</mml:mi></mml:semantics></mml:math></inline-formula> is updated as the ratio between the values of <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">A</mml:mi><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">L</mml:mi><mml:mi mathvariant="normal">S</mml:mi><mml:mi mathvariant="normal">T</mml:mi><mml:mi mathvariant="normal">M</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> at the end of the preceding epoch.</p><p>The simultaneous training of the AE and the LSTM allows extracting a low-dimensional space of neural activity constrained to include features related to movement intent. Such neural manifold is then used to train the Wiener filter used as the fixed day-0 decoder for this study. At each epoch of training, the current latent signal <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was used as input for <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> to obtain a linear prediction of the actual motor output. We used 400 epochs of training and ultimately selected the parameters of the wiener filter at the epoch that had the best performance (in the R<sup>2</sup> sense) on the held-out test set.</p><p>ADAN based aligner. The discriminator D of ADAN is an autoencoder (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>), and has the same architecture as that used to find the nonlinear latent space on day-0 (day-0 AE). The parameters of D (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mtext>D</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are initialized with the parameters of the day-0 AE. The generator G is a feedforward neural network with one hidden layer with C neurons (i.e., the number of the electrodes in the cortical array). The parameters of G (<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mtext>G</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are initialized as identity matrices. We set a nonlinear activation function (ELU) for the hidden layer, and a linear one for the output layer.</p><p>Here we denote the day-0 neural firing rates as  <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>M</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and the day-k neural firing rates as <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mrow><mml:mo>{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mtext>N</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, where both <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are C-dimensional vectors representing the neural firing rates from C electrodes at a given time bin, and M and N are the total number of samples for day-0 and day-k data respectively. Since at one time we fed the networks with S training samples as a batch, we can write a training batch from <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mi mathvariant="bold-italic">z</mml:mi><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> in matrix form as <italic><bold>X</bold></italic> or <italic><bold>Z</bold></italic>. During training, we fed <italic><bold>Z</bold></italic> to G and got G(<italic><bold>Z</bold></italic>) as the aligned day-k neural firing rates. At the same time, we fed D with both G(<italic><bold>Z</bold></italic>) and <italic><bold>X</bold></italic>. As D is an autoencoder, it would produce the reconstructions of them from the latent space, which can be written as <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Hence, we could get the residuals between the true data and these reconstructions by computing:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext>G</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mover><mml:mrow><mml:mtext>G</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p><bold><italic>R<sub>X</sub></italic></bold> and <bold><italic>R</italic></bold><sub>G(<italic><bold>Z</bold></italic>)</sub> are both <inline-formula><mml:math id="inf38"><mml:semantics><mml:mrow><mml:mtext>S</mml:mtext><mml:mo>×</mml:mo><mml:mtext>C</mml:mtext></mml:mrow></mml:semantics></mml:math></inline-formula> matrices. We then computed the scalar reconstruction losses as the L<sub>1</sub> norm of each column of <bold><italic>R<sub>X</sub></italic></bold> and <bold><italic>R</italic></bold><sub>G(<bold><italic>Z</italic></bold>)</sub>. Let <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> represent the distributions of these scalar losses, and let <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> be the corresponding means of <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. We measured the dissimilarity between <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> by a lower bound to the Wasserstein distance (<xref ref-type="bibr" rid="bib4">Arjovsky et al., 2017</xref>), which is given by the absolute value of the difference between <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>:<inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtext>W</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>≥</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The parameters of the generator (<inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mtext>G</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) and discriminator (<inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mtext>D</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) are updated via batch gradient descent by minimizing their corresponding cost functions:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>D</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mtext>G</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>μ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>For each epoch of training,<inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>G</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is first minimized and followed by <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>D</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Minimizing <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>G</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> implies bringing the output of the generator (<italic>i.e</italic>., the aligned day-k neural data, G(<bold><italic>Z</italic></bold>)) close to the day-0 data <bold><italic>X</italic></bold>. When G(<bold><italic>Z</italic></bold>) is fed through D, residuals with mean <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are obtained. Since D is initialized with the day-0 AE weights, <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> can be reduced if <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mtext>G</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are updated to appropriately modify G(<bold><italic>Z</italic></bold>) and make it resemble <bold><italic>X</italic></bold>. When <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>G</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is minimized, the gradients flow through both D and G, but only the parameters <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow><mml:mtext>G</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> are updated at this stage.</p><p>While G is trying to decrease <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, D is working as an adversary. Minimizing <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>D</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> implies maximizing the difference between <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">R</mml:mi><mml:mrow><mml:mtext>G</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> (<italic>i.e</italic>., their Wasserstein distance W). Again, since D is initialized with the day-0 AE weights (and the generator is an identity matrix when training begins), the residuals of the day-k data will be greater than those of the day-0 data, hence <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, if <inline-formula><mml:math id="inf65"><mml:semantics><mml:mrow><mml:msub><mml:mstyle mathvariant="bold" mathsize="normal"><mml:mi>θ</mml:mi></mml:mstyle><mml:mtext>D</mml:mtext></mml:msub></mml:mrow></mml:semantics></mml:math></inline-formula> are updated to maximize <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, or equivalently minimize <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, this relation is maintained during training. Since scalar residuals and their means are always nonnegative, maximization of W is achieved by decreasing <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> while increasing <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. The adversarial mechanism between G and D ensures that the neural alignment is achieved in an unsupervised manner.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>ADAN hyperparameters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">parameter</th><th align="left" valign="bottom">value</th></tr></thead><tbody><tr><td align="left" valign="bottom">Total number of trainable parameters</td><td align="left" valign="bottom">35,946</td></tr><tr><td align="left" valign="bottom">Batch size</td><td align="left" valign="bottom">8</td></tr><tr><td align="left" valign="bottom">Discriminator (<inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">D</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) learning rate</td><td align="left" valign="bottom">0.00005</td></tr><tr><td align="left" valign="bottom">Generator (<inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) learning rate</td><td align="left" valign="bottom">0.0001</td></tr><tr><td align="left" valign="bottom">Number of training epochs</td><td align="left" valign="bottom">200</td></tr></tbody></table></table-wrap></sec><sec sec-type="appendix" id="s8-4"><title>Cycle-GAN based aligner</title><p>The Cycle-GAN generators, G<sub>1</sub> and G<sub>2</sub> are both shallow feedforward neural networks with one hidden layer with C neurons. We set a nonlinear activation function (RELU) for the hidden layer, and a linear one for the output layer. The discriminators, D<sub>1</sub> and D<sub>2</sub> are also shallow feedforward neural networks with one hidden layer. The input layer and the hidden layer both have C neurons, while the output layer has 1 neuron, as the output is a class label indicating which distribution the input sample belongs to. Same as G<sub>1</sub> and G<sub>2</sub>, the hidden layer of D<sub>1</sub> and D<sub>2</sub> uses a nonlinear activation function (RELU), and the output layer uses a linear one. The layer weights of each network were initialized through Xavier initialization.</p><p>As shown in (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>), we fed the day-k neural firing rates <italic><bold>Z</bold></italic> to G<sub>1</sub> to get the aligned day-k neural firing rates (G<sub>1</sub>(<italic><bold>Z</bold></italic>)), and the day-0 neural firing rates <italic><bold>X</bold></italic> to G<sub>2</sub> to convert data in the day-0 domain back into the day-k domain (G<sub>2</sub>(<italic><bold>X</bold></italic>)). Meanwhile, the discriminator D<sub>1</sub> was fed with <italic><bold>X</bold></italic> and (G<sub>1</sub>(<italic><bold>Z</bold></italic>)) to distinguish between the ‘real and the ‘fake’ day-0 data, while D<sub>2</sub> was fed with <italic><bold>Z</bold></italic> and (G<sub>2</sub>(<italic><bold>X</bold></italic>)) to distinguish between the ‘real’ and the ‘fake’ day-k data. Specifically, the discriminators would assign each sample a class label to tell if it belonged to the C-dimensional distribution of the real data (<inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ρ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) or from the distribution of the fake data generated by G<sub>1</sub> or G<sub>2</sub>.</p><p>For the network training, we expected G<sub>1</sub> and G<sub>2</sub> to generate more convincing samples, while D<sub>1</sub> and D<sub>2</sub> to be more perceptive to better discriminate between the true and the fake samples. The performances of the networks in such contest could be quantified by adversarial losses. As with ADAN, here we adopted the mean absolute error (MAE), or L<sub>1</sub> loss, as the adversarial loss function. For G<sub>1</sub> and D<sub>1</sub>, the adversarial loss can be expressed as follows:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf74"><mml:semantics><mml:mi>a</mml:mi></mml:semantics></mml:math></inline-formula> is the label for the fake neural firing rates, <italic>b</italic> is the label for the real neural firing rates, and <italic>c</italic> is the value that G<sub>1</sub> wants D<sub>1</sub> to believe for fake neural firing rates. Typically, we can set <inline-formula><mml:math id="inf75"><mml:semantics><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>, and <inline-formula><mml:math id="inf76"><mml:semantics><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:semantics></mml:math></inline-formula>. For D<sub>2</sub> and G<sub>2</sub>, the adversarial loss <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> have a similar form:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>b</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∼</mml:mo><mml:msub><mml:mtext>p</mml:mtext><mml:mrow><mml:mtext>data</mml:mtext></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>The core idea of Cycle-GAN is to make the learned mapping functions cycle-consistent so as to reduce the space of possible mapping functions. As shown in (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1</xref>), the two highlighted cycles should be able to bring the corresponding data back to the original domain, for example, the distribution of the recovered day-k neural firing rates G<sub>2</sub>(G<sub>1</sub>(<bold><italic>Z</italic></bold>)) should be similar to the distribution of the real day-k neural firing rates <bold><italic>Z</italic></bold>. Therefore, we define the cycle consistency loss as follows:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>∼</mml:mo><mml:mtext>p</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">X</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mtext>E</mml:mtext><mml:mrow><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>∼</mml:mo><mml:mtext>p</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi mathvariant="bold-italic">Z</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Note here we also applied the L<sub>1</sub> loss.</p><p>Taken together, the full loss function is written as:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>d</mml:mi><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>y</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and the training process is to solve this min-max optimization problem:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msubsup><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mtext>arg</mml:mtext><mml:mspace width="thickmathspace"/><mml:munder><mml:mtext>min</mml:mtext><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mspace width="thickmathspace"/><mml:munder><mml:mtext>max</mml:mtext><mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><table-wrap id="app1table2" position="float"><label>Appendix 1—table 2.</label><caption><title>Cycle-GAN hyperparameters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">parameter</th><th align="left" valign="bottom">value</th></tr></thead><tbody><tr><td align="left" valign="bottom">Total number of trainable parameters</td><td align="left" valign="bottom">74,208</td></tr><tr><td align="left" valign="bottom">Batch size</td><td align="left" valign="bottom">256</td></tr><tr><td align="left" valign="bottom">Discriminator (<inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) learning rate</td><td align="left" valign="bottom">0.01</td></tr><tr><td align="left" valign="bottom">Discriminator (<inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>D</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) learning rate</td><td align="left" valign="bottom">0.01</td></tr><tr><td align="left" valign="bottom">Generator (<inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) learning rate</td><td align="left" valign="bottom">0.001</td></tr><tr><td align="left" valign="bottom">Generator (<inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mtext>G</mml:mtext><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) learning rate</td><td align="left" valign="bottom">0.001</td></tr><tr><td align="left" valign="bottom">Number of training epochs</td><td align="left" valign="bottom">200</td></tr></tbody></table></table-wrap><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Adversarial neural networks proposed for iBCI stabilization.</title><p>(<bold>A</bold>) The architecture of ADAN. A feedforward network (the generator, ‘G’) takes the neural firing rates on day-k (‘FR<sub>day-k</sub>’) as input and applies a transform on them to produce the aligned neural firing rates (‘Aligned FR<sub>day-k</sub>’). Next, an autoencoder (the ‘Discriminator’) takes as input both the firing rates on day-0 (‘FR<sub>day-0</sub>’) and the Aligned FR<sub>day-k</sub> and aims to discriminate between them, giving the adversarial loss. (<bold>B</bold>) The architecture of CycleGAN used as an aligner for an iBCI. A feedforward neural network (‘G1’) takes FR<sub>day-k</sub> as input and produces Aligned FR<sub>day-k</sub> after applying a transformation. Another feedforward network (‘D1’) aims to discriminate between Aligned FR<sub>day-k</sub> and FR<sub>day-0</sub>; the performance of D1 contributes the first adversarial loss. A second pair of feedforward networks (‘G2’ and ‘D2’) function in the same way, but aim to convert FR<sub>day-0</sub> into an Aligned FR<sub>day-0</sub> that resembles FR<sub>day-k</sub>; these contribute to the second adversarial loss. The discrepancy between the real FR<sub>day-k</sub> and Recovered FR<sub>day-k</sub> (generated by passing FR<sub>day-k</sub> through G1 followed by G2) contributes a cycle loss (and similarly for FR<sub>day-0</sub> and Recovered FR<sub>day-0</sub>). The purple and orange arrows highlight these two cyclical paths through the two networks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84296-app1-fig1-v1.tif"/></fig></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84296.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.08.26.504777" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.26.504777"/></front-stub><body><p>This paper reports a new way to deal with the drift of neural signals and representations over time in a BCI. Given the context of the rapidly advancing field, the reviewers assessed the findings to be useful and potentially valuable. With the code provided for other investigators to use, the strength of evidence was convincing.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84296.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Kemere</surname><given-names>Caleb</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.26.504777">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.26.504777v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Using adversarial networks to extend brain computer interface decoding accuracy over time&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, including Caleb Kemere as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Joshua Gold as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) Recognizing that it may seem unfair given the length of time that your work has been in review, for the general <italic>eLife</italic> audience, the reviewers felt that it was required was to address the performance of the NoMAD approach (https://www.biorxiv.org/content/10.1101/2022.04.06.487388v1). Ideally, this would be a direct comparison. More generally, it would be valuable to discuss the relative merits of alignment approaches based only on the moment-by-moment cofiring of neurons (e.g., CycleGAN) versus alignment approaches which further leverage the dynamics in the latent space.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>In this paper, Ma et al. tackle the problem of how to allow intracortical BCIs to sustain a high level of performance when there is changes in the neural signals recorded from the array and the behavior of the monkey. Such changes could be due to changes in signal quality, the tuning of the neurons, turnover of recorded neurons etc. In an ideal world, for patients using this day in and day out, there would be a quick approach to understand what the current state of the decoder is and quickly and readily adapt to the current setting so that the patient sees no drop in performance. This is a somewhat well studied question and barring older work, Stavisky, Sussillo et al. 2016 proposed a solution to this problem by using multiplicative recurrent neural networks (RNNs) that can select the best decoder given the neural data by learning from many different samples. The Miller lab in 2018 proposed using GANs to solve this problem, and again in collaboration with Dr. Pandarinath's lab has developed an approach using LFADS (called NOMAD, Karpowicz et al. 2022) to solve this problem. Here they use a different type of GAN to solve this problem. The paper is well structured, reasonably clear, the datasets are impressive and the authors have applied their approach to these datasets and compared to an approach which is based on factor analysis.</p><p>However, currently I am unsure the degree of advance provided by this paper. In particular, given that two of the datasets studied in this paper (Monkey J and Monkey C center out reach) are also present in the Karpowicz et al. 2022 paper, we need to rigorously compare both of them. The improvement from the ADAN approach seems somewhat minor in my opinion.</p><p>1. I find the results only modestly improve over their own existing approach (ADAN) and yes it does better than a simple factor analysis based method but that is simply stated as a powerful neural network is way better than a simple set of linear operations. I mean this is a little bit like a sprint race between me and Usain Bolt, there is just no contest there.</p><p>2. The related issue is that they are at best proposing a minor improvement over their own Cycle-GAN study. More worryingly, their approach does not seem to be better than the NoMAD Study from Karpowicz et al. 2022? I am all for many different approaches, but I am tad worried that there is just minimal improvement over and above their previous approach. It also feels like we are not performing a fair comparison to the state of the art, which some subset of authors in this paper has worked on! I think at a minimum they need to run NoMAD on the same datasets with whatever binsizes they choose and show that their method is comparable. I say this from the perspective that these are all offline decoding analyses and yes it is computationally expensive but does not need new experiments. In fact NoMAD runs better on this dataset with a 20 ms bin compared to a 50 ms bin.</p><p>Karpowicz et al. 2022 (bioRxiv), shares considerable author overlap with Ma et al. 2022</p><p>(Xuan Ma, Lee Miller)</p><p>The reference for this is totally mangled btw.</p><p>3. Why do I say this. Any reader who is aware of NoMAD would be like this is a strawman comparison. I think putting all of these methods on equal footing is necessary to move the field forward! I hope the authors don't feel like this is unreasonable. In addition it is the same data from I think a rockstar monkey J (95 days of data, similar task etc). Monkey J is also used in the NoMAD paper. So same dataset, multiple papers and two to three different methods :)! Figure 3 at a minimum needs a plot of the NoMAD results.</p><p>4. Of interest would be to discuss the number of parameters in each of these approaches. If the authors want, it might make sense to show how long it takes for PAF, ADAN, Cycle-GAN and NoMAD and this could be a supplementary figure. Maybe NoMAD will need way more training trials. It looks like PAF should have minimal parameters but Cycle-GAN is at least 2x as expressive as ADAN.</p><p>5. There is a theoretical point here. The GANs are trying to make the data indistinguishable from one another but as the neural data analysis shows the principal angle is still pretty substantial for 10 dimensions (~50 degrees). This will hurt their decoders. This might be an inherent disadvantage of GANs because they will likely stop once the data look like they are similar to the original distribution. But what you want is ideally something that adjusts the Day-k data to be near identical to the Day-0 data, in which case methods that maximize alignment might be a better approach. This should be discussed in the manuscript.</p><p>6. A weakness of all of these studies is that it is all done offline, what approach wins best online is an open question. Of note Stavisky, Sussillo et al. worked online. This should be a caveat in the discussion of these studies as it is an open question which of these approaches will be most successful online.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Using adversarial networks to extend brain computer interface decoding accuracy over time&quot; for further consideration by <italic>eLife</italic>. Your response and revised article has been evaluated by Joshua Gold (Senior Editor) and a Reviewing Editor as well as the original reviewers.</p><p>The reviewers appreciated your thorough responses to their comments. However, upon discussion, there was a consensus that two important issues remain that should be addressed:</p><p>1. The comparison to NoMAD seems important enough that adding to the manuscript details from the response letter (point #3 from R2) would be useful, particularly in terms of your contention that best within-time-bin alignment is likely a valuable component of more complex systems.</p><p>2. Given that this is a Tools and Resources article, we believe that the description of the approach in Appendix 4 is still insufficient. In addition, we request code or pseudo-code that implements those algorithms in a way that a community member would be able to rapidly use them.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84296.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) Recognizing that it may seem unfair given the length of time that your work has been in review, for the general eLife audience, the reviewers felt that it was required was to address the performance of the NoMAD approach (https://www.biorxiv.org/content/10.1101/2022.04.06.487388v1). Ideally, this would be a direct comparison. More generally, it would be valuable to discuss the relative merits of alignment approaches based only on the moment-by-moment cofiring of neurons (e.g., CycleGAN) versus alignment approaches which further leverage the dynamics in the latent space.</p></disp-quote><p>We appreciate the reviewers’ feedback, and we absolutely recognize the importance of being able to compare between different methods for neural representation alignment. While we would like to push back against the notion of NoMAD, or any one method, as being state-of-the-art for alignment, we have made an effort to respond to the reviewers’ concerns in our rebuttal to Reviewer #2 Points 2-3, and in the Discussion section of the manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>In this paper, Ma et al. tackle the problem of how to allow intracortical BCIs to sustain a high level of performance when there is changes in the neural signals recorded from the array and the behavior of the monkey. Such changes could be due to changes in signal quality, the tuning of the neurons, turnover of recorded neurons etc. In an ideal world, for patients using this day in and day out, there would be a quick approach to understand what the current state of the decoder is and quickly and readily adapt to the current setting so that the patient sees no drop in performance. This is a somewhat well studied question and barring older work, Stavisky, Sussillo et al. 2016 proposed a solution to this problem by using multiplicative recurrent neural networks (RNNs) that can select the best decoder given the neural data by learning from many different samples. The Miller lab in 2018 proposed using GANs to solve this problem, and again in collaboration with Dr. Pandarinath's lab has developed an approach using LFADS (called NOMAD, Karpowicz et al. 2022) to solve this problem. Here they use a different type of GAN to solve this problem. The paper is well structured, reasonably clear, the datasets are impressive and the authors have applied their approach to these datasets and compared to an approach which is based on factor analysis.</p><p>However, currently I am unsure the degree of advance provided by this paper. In particular, given that two of the datasets studied in this paper (Monkey J and Monkey C center out reach) are also present in the Karpowicz et al. 2022 paper, we need to rigorously compare both of them. The improvement from the ADAN approach seems somewhat minor in my opinion.</p><p>1. I find the results only modestly improve over their own existing approach (ADAN) and yes it does better than a simple factor analysis based method but that is simply stated as a powerful neural network is way better than a simple set of linear operations. I mean this is a little bit like a sprint race between me and Usain Bolt, there is just no contest there.</p></disp-quote><p>Although we acknowledge that nonlinear methods are theoretically superior to linear methods (at least when applied to nonlinear systems), we do not believe that this should detract from the significance of our paper. As the reviewer noted, the primary objective of our study was to compare a novel approach (Cycle-GAN) to two established techniques (ADAN and Procrustes alignment of factors), all intended to align neural data. While the best-case performance boost of Cycle-GAN over ADAN is not large (although see our second point below), here we note several reasons to believe that Cycle-GAN is a much more promising technique.</p><p>First, Cycle-GAN is much more robust to hyperparameter tuning than ADAN. This finding is not trivial, as GANs are notoriously difficult to train, as was the case for ADAN. Part of the reason we adopted Cycle-GAN was ADAN’s very poor performance (and our considerable concern) in our initial tests in applying it to a broader range of data. ADAN must be hand-tuned by someone with machine learning expertise for each new dataset. Our hyperparameter analysis (Figure 4) suggests that Cycle-GAN is more likely to be effective ‘out-of-the-box’, and at working with different data binning and sampling rates. In your sprint analogy, ADAN might be seen as a blindfolded Usain Bolt -- he might still outrun you, but only if his trainer leads him to the track and carefully lines him up facing in the right direction before the race.</p><p>Second, we have expanded our analysis in the revised manuscript to include neural alignment during continuous recordings, and demonstrated that Cycle-GAN performs much better than ADAN (and PAF) in that setting. In our original submission, we exclude periods of time when the monkey is not engaged in the task from the datasets used for alignment (for all three methods.) However, in a true iBCI setting, the user has uninterrupted control, so it would not be ideal to train the aligner excluding portions of a recording session that are not task related (as in our previous analysis). Our evaluation in the continuous recording setting is therefore a more accurate reflection of how each method might perform in a clinical setting- and here, the improvement of Cycle-GAN over ADAN is clear.</p><p>And third, as discussed in the manuscript and above, Cycle-GAN can be used directly with any previously trained spike-rate based decoder. This is in contrast to ADAN and PAF, which only align the neural latent space and therefore require either a new, latent space decoder to be trained, or an additional post-alignment, backwards-projection step to convert the latent representation into a predicted set of spikes. The backwards-projection step leads to lower decoding performance for ADAN, and complete failure for PAF, as shown in Appendix 3 – Figure 1. Thus, Cycle-GAN is a more versatile and practical solution because of its flexibility in integrating with other existing iBCI pipelines.</p><p>Taking all these points together, Cycle-GAN indeed represents a substantial improvement over existing techniques for improving the stability of iBCI decoders over time.</p><p>Finally, from a scientific perspective (and related to our third point), Cycle-GAN is interesting as it is the first neural alignment approach that has been demonstrated to perform well without relying on the computation (and stability) of latent manifolds. While not explored in this manuscript, this property might make Cycle-GAN of potential interest in applications where neural dynamics are higher-dimensional, such as in cognitive tasks (Rigotti et al., 2013).</p><p>Rigotti, M., Barak, O., Warden, M. R., Wang, X. J., Daw, N. D., Miller, E. K., and Fusi, S. (2013). The importance of mixed selectivity in complex cognitive tasks. Nature, 497(7451), 585-590.</p><p>Action in the text (pages 9-10): We have added some discussion points from the response above to the first Discussion section, as well as the ‘iBCI stabilization <italic>without manifolds’</italic> Discussion section, to better highlight the advantages of Cycle-GAN.</p><disp-quote content-type="editor-comment"><p>2. The related issue is that they are at best proposing a minor improvement over their own Cycle-GAN study. More worryingly, their approach does not seem to be better than the NoMAD Study from Karpowicz et al. 2022? I am all for many different approaches, but I am tad worried that there is just minimal improvement over and above their previous approach. It also feels like we are not performing a fair comparison to the state of the art, which some subset of authors in this paper has worked on! I think at a minimum they need to run NoMAD on the same datasets with whatever binsizes they choose and show that their method is comparable. I say this from the perspective that these are all offline decoding analyses and yes it is computationally expensive but does not need new experiments. In fact NoMAD runs better on this dataset with a 20 ms bin compared to a 50 ms bin.</p><p>Karpowicz et al. 2022 (bioRxiv), shares considerable author overlap with Ma et al. 2022</p><p>(Xuan Ma, Lee Miller)</p></disp-quote><p>(See next point for response)</p><disp-quote content-type="editor-comment"><p>The reference for this is totally mangled btw.</p><p>3. Why do I say this. Any reader who is aware of NoMAD would be like this is a strawman comparison. I think putting all of these methods on equal footing is necessary to move the field forward! I hope the authors don't feel like this is unreasonable. In addition it is the same data from I think a rockstar monkey J (95 days of data, similar task etc). Monkey J is also used in the NoMAD paper. So same dataset, multiple papers and two to three different methods :)! Figure 3 at a minimum needs a plot of the NoMAD results.</p></disp-quote><p>We grouped the response to the reviewer’s two comments concerning NoMAD here, and have added text on this subject to the Introduction and Discussion of the text.</p><p>First, we would like to push back against the notion of NoMAD, or any other single study, as being state-of-the-art for alignment. An equal footing for these comparisons is indeed important, but there is as yet no consensus benchmark dataset or metric with which to contrast different alignment methods. This is why we make an effort in this paper to establish a rigorous framework to fairly compare multiple alignment methods, by (1) controlling for preprocessing and decoder architecture, (2) applying a fixed, appropriate set of evaluation metrics to a large ensemble of tasks and multiple monkeys, and (3) exploring other aspects of performance beyond accuracy, such as sensitivity to hyperparameters.</p><p>Comparison to NoMAD within this framework turns out to be problematic because NoMAD and Cycle-GAN are solving overlapping but different problems. A stable iBCI device has several interacting components: data preprocessing, an aligner that registers neural representations across days, and a decoder that translates neural activity to a predicted motor command. Higher iBCI performance could arise from an improvement to any of these processes. NoMAD includes the first two steps, performing both alignment of the neural representations via Kullback-Leibler Divergence (KLD) minimization and data preprocessing via LFADS-based smoothing. Because Karpowicz et al., contrast NoMAD (alignment + powerful dynamics-based smoothing) to two methods that perform alignment with only very simple linear smoothing, it is not possible to tell from their manuscript the extent to which NoMAD’s higher performance arises from better alignment vs their use of LFADS for data smoothing.</p><p>Nevertheless, the effects of the preprocessing can be inferred from their results: because of its more powerful dynamics preprocessing, NoMAD outperforms ADAN (and PAF) not only at day-k, but also on day-0, where no neural alignment is involved. The day 0 performance makes it clear that a substantial portion of NoMAD’s higher performance comes not from alignment but from how data are pre-processed.</p><p>We can also draw conclusions purely from the method NoMAD uses for alignment, namely by minimizing the KL divergence between the distributions of day-0 and day-k states that come out of a day-0 LFADS Generator network. This alignment strategy is very similar to the KLD minimization method tested in Farshchian et al., 2018; in that study, KLD minimization between neural latent states (obtained via an autoencoder) had inferior performance compared even to ADAN. This suggests that the apparent performance improvement of NoMAD over ADAN is a direct consequence of its embedded LFADS model rather than being due to a better alignment strategy. Theoretically, one could therefore replace the KLD-based alignment in NoMAD with a Cycle-GAN-based aligner and achieve even better performance.</p><p>It should also be noted that NoMAD cannot be used in combination with an existing neural decoder unless that decoder also gets (and was trained on) an LFADS-based smoothing.</p><p>Regarding authorship: as listed at the end of Karpowicz et al. 2022 (bioRxiv), Xuan Ma’s contribution was in data preparation, while Lee Miller’s contribution was in conceptualization, funding acquisition and manuscript revision. Neither author was substantially involved in the design and tests of the algorithms proposed in that work.</p><p>Action in the text (pages 9-10, lines 454-468): We have added some discussion points from the response above to the ‘Comparison of GANs to other methods for iBCI stabilization’ Discussion section.</p><disp-quote content-type="editor-comment"><p>4. Of interest would be to discuss the number of parameters in each of these approaches. If the authors want, it might make sense to show how long it takes for PAF, ADAN, Cycle-GAN and NoMAD and this could be a supplementary figure. Maybe NoMAD will need way more training trials. It looks like PAF should have minimal parameters but Cycle-GAN is at least 2x as expressive as ADAN.</p></disp-quote><p>Yes, PAF has a minimal number of parameters (102 ~ 103) as it is a classical linear algebra-based method. Cycle-GAN has roughly twice the parameters of ADAN due to the presence of an additional pair of generator and discriminator. For aligning two 96-channel neural datasets, Cycle-GAN has 74,208 parameters while ADAN has 35,946. We could not find information about the parameter count of NoMAD in the Karpowicz et al., preprint, however we expect it to be considerably higher, as it encompasses LFADS-based smoothing in addition to alignment.</p><p>We already provide a comparison of PAF, ADAN and Cycle-GAN training times in Figure 4B of our manuscript. We note that while Cycle-GAN training time for a given batch size is longer, is can actually be faster to train in practice because Cycle-GAN can be trained using larger batch sizes, whereas ADAN must be trained with small batch sizes to properly converge (as shown in Figure 4A). Again, we could not find any information about training time in the pre-print of NoMAD.</p><p>Finally, as stated earlier, we note that while forward-backward mapping between days does occur during Cycle-GAN training, only the forward mapping is performed during inference. Because of this, the inference speed for Cycle-GAN is comparable to that of ADAN, with both models completing the forward map of one sample (one 50 ms-binned vector of firing rates) in well under 1 ms.</p><p>Action in the text (Appendix 4 Tables 1-2): We have added a summary table including the total number of parameters for each method (and the related hyperparameter values) in Appendix 4 (ADAN: Appendix 4 Table 1; Cycle-GAN: Appendix 4 Table 2).</p><p>Action in the text (page 7, lines 303-306): added inference time for Cycle-GAN and ADAN.</p><disp-quote content-type="editor-comment"><p>5. There is a theoretical point here. The GANs are trying to make the data indistinguishable from one another but as the neural data analysis shows the principal angle is still pretty substantial for 10 dimensions (~50 degrees). This will hurt their decoders. This might be an inherent disadvantage of GANs because they will likely stop once the data look like they are similar to the original distribution. But what you want is ideally something that adjusts the Day-k data to be near identical to the Day-0 data, in which case methods that maximize alignment might be a better approach. This should be discussed in the manuscript.</p></disp-quote><p>It is important to note here that ~50 degrees is not THE principal angle between two 10-dimensional spaces: it is the 10th-smallest principal angle between those spaces. For any pair of N-dimensional hyperplanes, there are N principal angles that exist between them. Following the approach outlined in Knyazev and Argentati, 2002 and Elsayed et al., 2016, these are computed as follows: first, we reduce each sample (here the Day-X neuron firing rates) to 10 dimensions using PCA. Next, recursively for each n = 1…10, we identify the pair of principal vectors that are separated by the smallest angle and perpendicular to the prior selected pairs, and report that angle. When two hyperplanes are well-aligned, the leading principal angles between them can be very small, but often the last few angles are quite large.</p><p>Figure 7B shows that the ten principal angles between sessions many days apart (gray curves) are all substantially larger than the correspondingly ordered principal angles obtained between the day-0 neural recordings of even-numbered trials and odd-numbered trials (this was done to reduce the effect of any within-day drift). After processing by ADAN and Cycle-GAN, the principal angles become considerably smaller. For ADAN (red curves), the first three principal angles are close to the within-day values, but the remaining angles are much larger. In contrast, for Cycle-GAN, all principal angles except the last are even smaller than the within-day principal angles! This suggests that the aligner has effectively maximized the alignment of day-0 and day-k data, indeed making them as well or better aligned then two samples from the same day.</p><p>Action in the text (page 15, lines 727-752): We clarified our description of the principal angles computation and its interpretation in the related Methods section.</p><disp-quote content-type="editor-comment"><p>6. A weakness of all of these studies is that it is all done offline, what approach wins best online is an open question. Of note Stavisky, Sussillo et al. worked online. This should be a caveat in the discussion of these studies as it is an open question which of these approaches will be most successful online.</p></disp-quote><p>We believe that offline analyses are actually essential for the development of BCI systems because they allow for a comprehensive evaluation of the efficacy of a BCI algorithm without involving the complication of a user who can learn and adapt. We intend to add online tests in future work, and have allocated resources to develop online BCI platforms for both monkey and human subjects, This, however, is well out of the scope of what we can report in the current manuscript.</p><p>As discussed in the ‘Comparison of GANs to other methods for iBCI stabilization’ section, the approach taken by Stavisky, Sussillo et al., differed fundamentally from all our tested methods. They harnessed a large training dataset recorded over a span of many months to build a robust decoder for the monkey. While their results were impressive, it would be challenging to accumulate such a vast amount of training data in many applications, including those with paralyzed humans in the loop.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The reviewers appreciated your thorough responses to their comments. However, upon discussion, there was a consensus that two important issues remain that should be addressed:</p><p>1. The comparison to NoMAD seems important enough that adding to the manuscript details from the response letter (point #3 from R2) would be useful, particularly in terms of your contention that best within-time-bin alignment is likely a valuable component of more complex systems.</p></disp-quote><p>Comparison to NoMAD: as requested, we have added text from our previous response letter (Reviewer 2 point #3) to the Discussion of the manuscript. This can be found on lines 456 – 483.</p><disp-quote content-type="editor-comment"><p>2. Given that this is a Tools and Resources article, we believe that the description of the approach in Appendix 4 is still insufficient. In addition, we request code or pseudo-code that implements those algorithms in a way that a community member would be able to rapidly use them.</p></disp-quote><p>Formatting of appendices: we have converted all appendix figures into supplements of main figures in the text, and have integrated the text of appendices 1 and 2 into the figure legends and methods section. As discussed in my call with Josh Gold on June 1, Appendix 4 is far too bulky to include in the main text, so we have kept its appendix status here (submitted as “Supplementary File – Appendix 1”). We have also extended this appendix to include a more “practical” walk-through of how Cycle-GAN works. This section takes the reader through the steps of our Jupyter notebook tutorial posted at https://github.com/limblab/adversarial_BCI/, summarizing what is done in each code block of that notebook. With the provided tutorial notebooks, their summary in the Appendix, and the additional theoretical documentation in the Appendix, we believe that our manuscript now provides readers with ample support to apply Cycle-GAN alignment to their own data.</p></body></sub-article></article>