<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">91949</article-id><article-id pub-id-type="doi">10.7554/eLife.91949</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.91949.3</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>osl-dynamics, a toolbox for modeling fast dynamic brain activity</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-331736"><name><surname>Gohil</surname><given-names>Chetan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0888-1207</contrib-id><email>chetan.gohil@psych.ox.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-331742"><name><surname>Huang</surname><given-names>Rukuang</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6545-7517</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-331743"><name><surname>Roberts</surname><given-names>Evan</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-272362"><name><surname>van Es</surname><given-names>Mats WJ</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7133-509X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-331744"><name><surname>Quinn</surname><given-names>Andrew J</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-232714"><name><surname>Vidaurre</surname><given-names>Diego</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9650-2229</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-7718"><name><surname>Woolrich</surname><given-names>Mark W</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>Oxford Centre for Human Brain Activity, Wellcome Centre for Integrative Neuroimaging, Department of Psychiatry, University of Oxford</institution></institution-wrap><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03angcq70</institution-id><institution>Centre for Human Brain Health, School of Psychology, University of Birmingham</institution></institution-wrap><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01aj84f44</institution-id><institution>Center for Functionally Integrative Neuroscience, Department of Clinical Medicine, Aarhus University</institution></institution-wrap><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ray</surname><given-names>Supratim</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>29</day><month>01</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP91949</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-09-14"><day>14</day><month>09</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-08-08"><day>08</day><month>08</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.08.07.549346"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-11-09"><day>09</day><month>11</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91949.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-12-20"><day>20</day><month>12</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.91949.2"/></event></pub-history><permissions><copyright-statement>© 2023, Gohil et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Gohil et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-91949-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-91949-figures-v1.pdf"/><abstract><p>Neural activity contains rich spatiotemporal structure that corresponds to cognition. This includes oscillatory bursting and dynamic activity that span across networks of brain regions, all of which can occur on timescales of tens of milliseconds. While these processes can be accessed through brain recordings and imaging, modeling them presents methodological challenges due to their fast and transient nature. Furthermore, the exact timing and duration of interesting cognitive events are often a priori unknown. Here, we present the OHBA Software Library Dynamics Toolbox (osl-dynamics), a Python-based package that can identify and describe recurrent dynamics in functional neuroimaging data on timescales as fast as tens of milliseconds. At its core are machine learning generative models that are able to adapt to the data and learn the timing, as well as the spatial and spectral characteristics, of brain activity with few assumptions. osl-dynamics incorporates state-of-the-art approaches that can be, and have been, used to elucidate brain dynamics in a wide range of data types, including magneto/electroencephalography, functional magnetic resonance imaging, invasive local field potential recordings, and electrocorticography. It also provides novel summary measures of brain dynamics that can be used to inform our understanding of cognition, behavior, and disease. We hope osl-dynamics will further our understanding of brain function, through its ability to enhance the modeling of fast dynamic processes.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>oscillations</kwd><kwd>bursts</kwd><kwd>networks</kwd><kwd>dynamics</kwd><kwd>brain</kwd><kwd>machine learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/215573</award-id><principal-award-recipient><name><surname>Gohil</surname><given-names>Chetan</given-names></name><name><surname>van Es</surname><given-names>Mats WJ</given-names></name><name><surname>Woolrich</surname><given-names>Mark W</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/S02428X/1</award-id><principal-award-recipient><name><surname>Huang</surname><given-names>Rukuang</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>EP/L016044/1</award-id><principal-award-recipient><name><surname>Roberts</surname><given-names>Evan</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/106183</award-id><principal-award-recipient><name><surname>van Es</surname><given-names>Mats WJ</given-names></name><name><surname>Woolrich</surname><given-names>Mark W</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100016334</institution-id><institution>Dementia Research UK</institution></institution-wrap></funding-source><award-id>RG94383/RG89702</award-id><principal-award-recipient><name><surname>van Es</surname><given-names>Mats WJ</given-names></name><name><surname>Woolrich</surname><given-names>Mark W</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100009708</institution-id><institution>Novo Nordisk Fonden</institution></institution-wrap></funding-source><award-id>NNF19OC-0054895</award-id><principal-award-recipient><name><surname>Vidaurre</surname><given-names>Diego</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A generative-model-based, unsupervised learning toolbox for characterizing oscillatory bursting and brain network dynamics in univariate or multivariate time series.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>There is growing evidence for the importance of oscillatory activity in brain function (<xref ref-type="bibr" rid="bib6">Buzsáki, 2006</xref>). Neural oscillations have been linked to cognitive processes, such as information encoding and processing, as well as attention (<xref ref-type="bibr" rid="bib55">Ward, 2003</xref>) and distinct oscillatory activity has been observed in different states of consciousness (<xref ref-type="bibr" rid="bib10">Engel and Fries, 2016</xref>). Furthermore, the synchronization of neural oscillations has been proposed as a mechanism for communication (<xref ref-type="bibr" rid="bib11">Fries, 2015</xref>). Neural oscillations have also been a useful tool for understanding brain dysfunction; for example, changes have been observed in the oscillatory activity of diseased and healthy cohorts (<xref ref-type="bibr" rid="bib3">Başar and Güntekin, 2008</xref>).</p><p>An aspect of neural oscillations that remains to be fully understood is its dynamic nature, particularly at fast timescales (<xref ref-type="bibr" rid="bib46">van Ede et al., 2018</xref>). Recently, it has been proposed that neuronal populations exhibit short <italic>bursts</italic> of oscillatory activity on timescales of 100 ms (<xref ref-type="bibr" rid="bib19">Jones, 2016</xref>; <xref ref-type="bibr" rid="bib40">Shin et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Seedat et al., 2020</xref>) rather than the classical view of ongoing oscillations that are modulated in amplitude. This has important implications for how we should be modeling oscillatory activity changes in cognition and disease (<xref ref-type="bibr" rid="bib43">Stevner et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Khawaldeh et al., 2022</xref>; <xref ref-type="bibr" rid="bib29">Moraud et al., 2018</xref>). Unfortunately, the methods available to detect bursts in oscillatory data are limited, often requiring arbitrary choices for parameters relating to the frequency content, amplitude, and duration (<xref ref-type="bibr" rid="bib2">Bakkum et al., 2013</xref>). These choices can significantly impact the conclusions reached by such analyses.</p><p>Furthermore, oscillatory bursting is not isolated to individual brain regions. It has been shown that bursting can occur across cortical networks (<xref ref-type="bibr" rid="bib38">Seedat et al., 2020</xref>), and there are bursts of coherent activity in networks that last on the order of 50–100 ms in both resting-state (<xref ref-type="bibr" rid="bib52">Vidaurre et al., 2018b</xref>) and in task (<xref ref-type="bibr" rid="bib32">Quinn et al., 2018</xref>). Precise knowledge of these fast network dynamics is a valuable insight that can help us understand cognitive processes; for example, the dynamics of specific functional resting-state networks have been linked to memory replay (a≤50 ms process that occurs in memory consolidation) (<xref ref-type="bibr" rid="bib17">Higgins et al., 2021</xref>). Changes in the dynamics of functional networks have also been shown to be predictive of behavioral traits (<xref ref-type="bibr" rid="bib23">Liégeois et al., 2019</xref>) and disease (<xref ref-type="bibr" rid="bib41">Sitnikova et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Du et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Van Schependom et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Salvan et al., 2021</xref>; <xref ref-type="bibr" rid="bib39">Sharma et al., 2021</xref>; <xref ref-type="bibr" rid="bib25">Ma et al., 2020</xref>). The key barrier that prevents us from fully utilizing a network perspective is that the accurate estimation of dynamic functional networks is challenging. This is in part due to the timing and duration of interesting cognitive events, and the corresponding activity in functional networks, not being known. Consequently, we need to rely on methods that can adapt to the data and automatically identify when networks activate.</p><p>Here, we present the OHBA Software Library Dynamics Toolbox (osl-dynamics), a Python package that meets two far-reaching methodological challenges that limit the field of cognitive neuroscience: burst detection and the identification of dynamic functional brain networks. It does so by deploying data-driven generative models that have a proven ability to adapt to the data from a wide range of imaging modalities, and can learn the spatiotemporal characteristics of brain activity, with few assumptions and at fast timescales (<xref ref-type="bibr" rid="bib49">Vidaurre et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Vidaurre et al., 2018a</xref>; <xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>).</p><p>In applications for burst detection, osl-dynamics can automatically detect oscillatory bursts without the need to specify the frequencies, amplitude threshold, or duration of the bursts. This allows osl-dynamics to answer questions such as: when do oscillatory bursts occur; what is their oscillatory frequency; and what are their characteristic features (e.g. average lifetime, interval, occurrence, and amplitude)?</p><p>In the detection of dynamic functional brain networks, osl-dynamics can automatically detect network dynamics at fast timescales with few assumptions. This allows osl-dynamics to answer questions such as: what large-scale functional networks do individuals or groups exhibit; when do these functional networks activate and what are their characteristic dynamics; what functional networks activate in response to a task; do individuals differ in their functional network activations? On top of this, osl-dynamics can characterize functional networks from a more conventional, static (time-averaged), perspective using the same methodology where appropriate as the dynamic methods.</p><p>Here, we will illustrate the use of osl-dynamics using publicly available magnetoencephalography (MEG) datasets. However, we emphasize that the scope of the toolbox extends well beyond MEG, containing approaches that can be used, and have been used, to elucidate network and oscillatory dynamics in a range of applications that include electroencephalography (<xref ref-type="bibr" rid="bib18">Hunyadi et al., 2019</xref>; <xref ref-type="bibr" rid="bib14">Ghimatgar et al., 2019</xref>), functional magnetic resonance imaging (fMRI) (<xref ref-type="bibr" rid="bib51">Vidaurre et al., 2018a</xref>; <xref ref-type="bibr" rid="bib50">Vidaurre et al., 2017</xref>), invasive local field potential recordings (<xref ref-type="bibr" rid="bib20">Khawaldeh et al., 2022</xref>; <xref ref-type="bibr" rid="bib12">Garwood et al., 2021</xref>), and electrocorticography (<xref ref-type="bibr" rid="bib56">Wissel et al., 2013</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>In this section, we outline examples of uses of osl-dynamics to study source reconstructed MEG data. Section 2.1 presents the results of an oscillatory burst analysis pipeline. Sections 2.2–2.4 present the results of various dynamic network analysis pipelines. For comparison, we also include the results of a static network analysis pipeline in Section 2.5. For a description of the methods and dataset see Section 5.</p><sec id="s2-1"><title>2.1 Burst detection using a single-region TDE-HMM</title><p>The pipeline in Figure 10 was applied to do burst detection on a single parcel in the left motor cortex. The source data was calculated using the CTF rest MEG dataset. All subjects were concatenated temporally and used to train the TDE-HMM. The results are shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Burst detection: single region source reconstructed magnetoencephalography (MEG) data (left motor cortex) shows short-lived bursts of oscillatory activity.</title><p>(<bold>A.I</bold>) Dynamic spectral properties of the first 20  s of the time series from the first subject. (<bold>A.II</bold>) Amplitude envelope calculated after bandpass filtering the time series over the <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-band (top), <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula>-band (middle), and <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:mstyle></mml:math></inline-formula>-band (bottom). (<bold>B.I</bold>) The inferred state probability time course for the first 20  s of the first subject. (<bold>B.II</bold>) The power spectral density (PSD) of each state. (<bold>B.III</bold>) Pearson correlation of each state probability time course with the amplitude envelopes for different frequency bands. (<bold>B.IV</bold>) Distribution over subjects for summary statistics characterizing the bursts. Note that no additional bandpass filtering was done to the source data when calculating the mean amplitude. The script used to generate the results in this figure is here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_hmm_bursts.py">https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_hmm_bursts.py</ext-link>; (<xref ref-type="bibr" rid="bib16">Gohil, 2024</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Reproducibility of the CTF rest magnetoencephalography (MEG) dataset single-region TDE-HMM analysis.</title><p>(<bold>A</bold>) Variational free energy for three sets of 10 runs. (<bold>B</bold>) Summary statistics for the best run from each set (indicated by the red dot in A).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig1-figsupp1-v1.tif"/></fig></fig-group><p>We see from the wavelet transform in <xref ref-type="fig" rid="fig1">Figure 1AI</xref> that there are short bursts of oscillatory activity in this time series. This illustrates how it would be non-trivial, using conventional bandpass filtering and thresholding methods, to identify when exactly a burst occurs and what frequencies are contained within it. Instead of a conventional burst detection method, we use a three state TDE-HMM to identify bursts in a data-driven fashion. We see from the inferred state probability time course (<xref ref-type="fig" rid="fig1">Figure 1BI</xref>) that there are short-lived states that describe this data. We can see from <xref ref-type="fig" rid="fig1">Figure 1BII</xref> that each state corresponds to unique oscillatory activity. State 1 is interpreted as a non-oscillatory background state because it does not show any significant peaks in its PSD. States 2 and 3 show oscillatory activity in the <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:mstyle></mml:math></inline-formula> band (1–7  Hz) and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula> band (7–30  Hz), respectively. <xref ref-type="fig" rid="fig1">Figure 1BIII</xref> shows the correlation of each state probability time course with the AEs for different frequency bands (<xref ref-type="fig" rid="fig1">Figure 1AII</xref>). Based on this, we identify state 2 as a <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>/</mml:mo></mml:mrow><mml:mi>θ</mml:mi></mml:mstyle></mml:math></inline-formula>-burst state and state 3 as a <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-burst state. We can see from <xref ref-type="fig" rid="fig1">Figure 1BIV</xref> that these bursts have a variety of lifetimes ranging from a hundred to several hundred milliseconds. The reproducibility of these results is shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>.</p></sec><sec id="s2-2"><title>2.2 Detecting network dynamics using a multi-region AE-HMM</title><p>The AE-HMM pipeline in Figure 11 was applied to source reconstructed data from the Elekta task MEG dataset to identify amplitude-based network dynamics. All subjects and runs were concatenated temporally to train the model. The results are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref> with an example of a group-level analysis on the HMM state time courses (calculation of a group-averaged network response).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Dynamic network detection: a multi-region AE-HMM trained on the Elekta task magnetoencephalography (MEG) dataset reveals functional networks with fast dynamics that are related to the task.</title><p>(<bold>A.I</bold>) For each state, group-averaged amplitude maps relative to the mean across states (top) and absolute amplitude envelope correlation networks (bottom). (<bold>A.II</bold>) State probability time course for the first 8 s of the first subject. (<bold>A.III</bold>) Distribution over subjects for the summary statistics for each state. (<bold>B.I</bold>) State time courses (Viterbi path) epoched around the presentation of visual stimuli. The horizontal bars indicate time points with p&lt;0.05. The maximum statistic pooling over states and time points was used in permutation testing to control for the family-wise error rate. The script used to generate the results in this figure is here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/elekta_task/ae_hmm.py">https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/elekta_task/ae_hmm.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Reproducibility of the Elekta task magnetoencephalography (MEG) dataset multi-region AE-HMM analysis.</title><p>(<bold>A</bold>) Variational free energy for three sets of 10 runs. (<bold>B</bold>) Summary statistics for the best run from each set (indicated by the red dot in A). (<bold>C</bold>) Amplitude maps relative to the mean across states for the best run from sets 2 and 3. (<bold>D</bold>) Network response to the visual task for sets 2 and 3. The horizontal bars indicate time points with p&lt;0.05. The maximum statistic pooling over states and time was used in permutation testing to control for the family-wise error rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig2-figsupp1-v1.tif"/></fig></fig-group><p>We see the AE-HMM identifies plausible functional networks (<xref ref-type="bibr" rid="bib5">Buckner et al., 2011</xref>) with fast dynamics, typically with lifetimes of around 50 ms (<xref ref-type="fig" rid="fig2">Figure 2AIII</xref>). We identify a default mode network (state 1); two visual networks (states 2 and 6); a frontotemporal networks (states 3 and 7); and a sensorimotor network (state 4).</p><p>The AE-HMM was trained on the continuous source reconstructed data in an <italic>unsupervised</italic> manner, i.e., without any knowledge of the task. Post-HMM training, we can epoch the inferred state time course (Veterbi path) around the task (presentation of a visual stimuli [A picture of a familiar, unfamiliar, or scrambled face]) and average over trials. This gives the probability of each state being activate around a visual event. This is shown in <xref ref-type="fig" rid="fig2">Figure 2BI</xref>. We observe a significant increase (p&lt;0.05) in the activation of the visual networks (states 2 and 6) between 50–100 ms after the presentation of the visual stimuli as expected. We also observe a significant activation (p&lt;0.05) of the frontotemporal network (state 7) 300–900 ms after the visual stimuli as well as a deactivation of the visual networks (states 2 and 6). The reproducibility of these results is shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p></sec><sec id="s2-3"><title>2.3 Detecting network dynamics using a multi-region TDE-HMM</title><p>The TDE-HMM pipeline in Figure 11 was also applied to the Elekta task MEG dataset. All subjects and runs were concatenated temporally and used to train the model. The results are shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Dynamic network detection: a multi-region TDE-HMM trained on the Elekta task magnetoencephalography (MEG) dataset reveals spectrally distinct functional networks with fast dynamics.</title><p>(<bold>A.I</bold>) For each state, group-averaged power maps relative to the mean across states (top), coherence networks (middle), and power spectral density (PSD) averaged over regions (bottom), both the state-specific (colored solid line), and static PSD (i.e. the average across states, dashed black line) are shown. (<bold>A.II</bold>) State probability time course for the first 8 s of the first subject. (<bold>A.III</bold>) Distribution over subjects for the summary statistics for each state. (<bold>B.I</bold>) State time courses (Viterbi path) epoched around the presentation of visual stimuli. The horizontal bars indicate time points with p&lt;0.05. The maximum statistic pooling over states and time points was used in permutation testing to control for the family-wise error rate. The script used to generate the results in this figure is here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/elekta_task/tde_hmm.py">https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/elekta_task/tde_hmm.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Reproducibility of the Elekta task magnetoencephalography (MEG) dataset multi-region TDE-HMM analysis.</title><p>(<bold>A</bold>) Variational free energy for three sets of 10 runs. (<bold>B</bold>) Summary statistics for the best run from each set (indicated by the red dot in A). (<bold>C</bold>) Power maps relative to the mean across states for the best run from sets 2 and 3. (<bold>D</bold>) Network response to the visual task for sets 2 and 3. The horizontal bars indicate time points with p&lt;0.05. The maximum statistic pooling over states and time was used in permutation testing to control for the family-wise error rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig3-figsupp1-v1.tif"/></fig></fig-group><p>For the high-power networks (states 1–4), we see the same spatial patterns in TDE-HMM power maps (<xref ref-type="fig" rid="fig3">Figure 3AI</xref>, top) and AE-HMM amplitude maps (<xref ref-type="fig" rid="fig2">Figure 2AI</xref>). We can see from the state PSDs (<xref ref-type="fig" rid="fig3">Figure 3AI</xref>, bottom) that the networks identified by the TDE-HMM exhibit distinct spectral (oscillatory) activity. The TDE-HMM networks also have fast dynamics (<xref ref-type="fig" rid="fig3">Figure 3AIII</xref>) with lifetimes of around 50 ms. In <xref ref-type="fig" rid="fig3">Figure 3BI</xref>, we can see we are able to reproduce the network response analysis we did using the AE-HMM (<xref ref-type="fig" rid="fig2">Figure 2BI</xref>). The reproducibility of these results is shown in <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>.</p><p>The Elekta MEG dataset was recorded during a visual perception task. For comparison, we perform the same analysis on the CTF rest MEG dataset. All subjects were concatenated temporally and used to train the model. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the results of applying a TDE-HMM pipeline to this dataset. We observe similar networks in rest (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) as in task (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), which is a known result from fMRI studies (<xref ref-type="bibr" rid="bib42">Smith et al., 2009</xref>). We also include the coherence networks in <xref ref-type="fig" rid="fig3">Figures 3AI</xref> and <xref ref-type="fig" rid="fig4">4AI</xref>. We observe regions with high power activations have high connectivity (coherence). These networks also have fast dynamics (<xref ref-type="fig" rid="fig4">Figure 4AIII</xref>) with lifetimes of 50–100 ms.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Dynamic network detection: a multi-region TDE-HMM trained on the CTF rest magnetoencephalography (MEG) dataset identifies the same functional networks to those found with the Elekta task MEG dataset and reveals differences in the dynamics for young vs old groups.</title><p>(<bold>A.I</bold>) For each state, group-averaged power maps relative to mean across states (top), absolute coherence networks (middle) and power spectral density (PSD) averaged over regions (bottom), both the state-specific (colored solid line) and static PSD (i.e. the average across states, dashed black line) are shown. (<bold>A.II</bold>) State probability time course for the first 8 s of the first subject and run. (<bold>A.III</bold>) Distribution over subjects for the summary statistics of each state. (<bold>B.I</bold>) Comparison of the summary statistics for a young (18–34 years old) and old (34–60 years old) group. The star indicates a p&lt;0.05. The maximum statistic pooling over states and metrics was used in permutation testing to control for the family-wise error rate. The script used to generate the results in this figure is here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_hmm_networks.py">https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_hmm_networks.py</ext-link>; (<xref ref-type="bibr" rid="bib16">Gohil, 2024</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Reproducibility of the CTF rest magnetoencephalography (MEG) dataset multi-region TDE-HMM analysis.</title><p>(<bold>A</bold>) Variational free energy for three sets of 10 runs. (<bold>B</bold>) Summary statistics for the best run from each set (indicated by the red dot in A). (<bold>C</bold>) Power maps relative to the mean across states for the best run from sets 2 and 3. (<bold>D</bold>) Comparison of the summary statistics for a young (18–34 years old) and old (34–60 years old) group. The star indicates a p&lt;0.05. The maximum statistic pooling over states and metrics was used to control for the family-wise error rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig4-figsupp1-v1.tif"/></fig></fig-group><p>To illustrate a group-level analysis we could do with a dynamic network perspective, we compared two groups: 27 subjects in a young group (18–34 years old) and 38 subjects in an old group (34–60 years). <xref ref-type="fig" rid="fig4">Figure 4BI</xref>.I shows summary statistics for each group. We see the fractional occupancy and switching rate of the sensorimotor network (state 4) is increased in the older group (p&lt;0.05). The mean lifetime of the visual network (state 6) is also decreased in the older group (p&lt;0.05). The older group also has a wider distribution of mean intervals for the default mode network (state 1) and suppressed state (8) (p&lt;0.05). The reproducibility of these results is shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. The age-related differences we observe here are consistent with existing studies (<xref ref-type="bibr" rid="bib8">Coquelet et al., 2020</xref>). We will discuss the young vs old comparison further in Section 2.5.</p></sec><sec id="s2-4"><title>2.4 Dynamic network detection using multi-region TDE-DyNeMo</title><p>The TDE-DyNeMo pipeline in Figure 11 was applied to the CTF rest MEG dataset. All subjects were concatenated temporally and used to train the model. The results are shown in <xref ref-type="fig" rid="fig5">Figure 5</xref>. Note, for DyNeMo we found that learning seven modes (rather than 8) led to more reproducible results. Therefore, we present the 7-mode fit in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Dynamic network detection: a multi-region TDE-DyNeMo trained on the CTF rest magnetoencephalography (MEG) dataset reveals spectrally distinct modes that are more localized than Hidden Markov Model (HMM) states and overlap in time.</title><p>(<bold>A.I</bold>) For each mode, group-averaged power maps relative to the mean across modes (top), absolute coherence networks (middle), and power spectral density (PSD) averaged over regions (bottom), both the mode-specific (colored solid line) and static PSD (i.e. the average across modes, dashed black line) are shown. (<bold>A.II</bold>) Mode time course (mixing coefficients) renormalized using the trace of the mode covariances. (<bold>A.III</bold>) Pearson correlation between renormalized mode time courses calculated by concatenating the time series from each subject. (<bold>A.IV</bold>) Distribution over subjects for summary statistics (mean and standard deviation) of the renormalized mode time courses. (<bold>B.I</bold>) Comparison of the summary statistics for a young (18–34 years old) and old (34–60 years old) group. The maximum statistic pooling over modes and metrics was used in permutation testing to control for the family-wise error rate. The script used to generate the results in this figure is here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_dynemo_networks.py">https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_dynemo_networks.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Reproducibility of the CTF rest magnetoencephalography (MEG) dataset multi-region TDE-DyNeMo analysis.</title><p>(<bold>A</bold>) Variational free energy for three sets of 10 runs. (<bold>B</bold>) Summary statistics for the best run from each set (indicated by the red dot in A). (<bold>C</bold>) Power maps relative to the mean across modes for the best run from sets 2 and 3. (<bold>D</bold>) Comparison of the summary statistics for a young (18–34 years old) and old (34–60 years old) group. The double star indicates a p&lt;0.01. The maximum statistic pooling over modes and metrics was used to control for the family-wise error rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig5-figsupp1-v1.tif"/></fig></fig-group><p>We can see from the power maps and coherence networks (<xref ref-type="fig" rid="fig5">Figure 5AI</xref>) that DyNeMo identifies much more localized power activations and a cleaner network structure than was seen with the TDE-HMM. We can see from the PSDs (<xref ref-type="fig" rid="fig5">Figure 5AI</xref>, bottom) that these networks also exhibit distinct spectral characteristics.</p><p>From the (renormalized) mode time course (<xref ref-type="fig" rid="fig5">Figure 5AII</xref>) we see the description provided by DyNeMo is that of overlapping networks that dynamically vary in mixing ratios. This is a complementary perspective to the state description provided by the HMM. Co-activations of each mode can be understood by looking at the Pearson correlation between (renormalized) mode time courses (<xref ref-type="fig" rid="fig5">Figure 5AIII</xref>). We observe modes with activity in neighboring regions show more co-activation. We summarize the (renormalized) mode time course using statistics (the mean and standard deviation) in <xref ref-type="fig" rid="fig5">Figure 5AIV</xref>.</p><p>To compare DyNeMo to the HMM in a group-level analysis, we repeat the young vs old study using the DyNeMo-specific summary statistics (i.e. the mean and standard deviation of the renormalized mode time courses). <xref ref-type="fig" rid="fig5">Figure 5BI</xref> shows significant group differences for young (18–34 years old) and old (34–60 years old) participants. We can see an increased mode contribution (mean renormalized mode time course) for the sensorimotor network (mode 4), which reflects the increase in fractional occupancy we saw in the TDE-HMM (<xref ref-type="fig" rid="fig4">Figure 4BI</xref>). We see DyNeMo is able to reveal a stronger effect size with a p&lt;0.01 compared to the TDE-HMM, which had a p&lt;0.05. DyNeMo also shows a decrease in the variability (standard deviation of the renormalized mode time course) for the left temporal network (mode 5, p&lt;0.01). We will discuss the young vs old comparison further in Section 2.5. The reproducibility of these results is shown in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>.</p></sec><sec id="s2-5"><title>2.5 Estimating static functional networks</title><p>For comparison, we also apply a typical static network analysis pipeline (including static functional connectivity) to the CTF rest MEG dataset. We also consider how the static perspective in a young vs old group-level analysis compares to the dynamic perspective provided by the TDE-HMM in <xref ref-type="fig" rid="fig4">Figure 4</xref> and TDE-DyNeMo in <xref ref-type="fig" rid="fig5">Figure 5</xref>, illustrating the benefits of being able to do static and dynamic analyses within the same toolbox.</p><p><xref ref-type="fig" rid="fig6">Figure 6</xref> shows the group-averaged PSD (A.I), power maps (A.II), coherence networks (A.III) and amplitude envelope correlation (AEC) networks (A.IV) were calculated using all subjects. We observe <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula>-power is strongest in anterior regions and <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula>-power is strongest in posterior regions. We also observe qualitatively similar coherence and AEC networks. In particular, we see strong occipital connectivity in the <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula>-band in both the coherence and AEC networks.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Static network detection: osl-dynamics can also be used to perform static network analyses (including functional connectivity).</title><p>In the CTF rest magnetoencephalography (MEG) dataset, this reveals frequency-specific differences in the static functional networks of young (18–34 years old) and old (34–60 years old) participants. Group-average power spectral density (PSD) (averaged over subjects and parcels, (<bold>A.I</bold>) power maps (<bold>A.II</bold>) coherence networks (<bold>A.III</bold>) and AEC networks (<bold>A.IV</bold>) for the canonical frequency bands (<inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi><mml:mo>,</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>). (<bold>B.I</bold>) Power difference for old minus young (top) and p-values (bottom). Only frequency bands with at least one parcel with a p&lt;0.05 are shown, the rest are marked with n.s. (none significant). B.II) Amplitude envelope correlation (AEC) difference for old minus young only showing edges with a p&lt;0.05. The maximum statistic pooling over frequency bands and parcels/edges was used in permutation testing to control for the family-wise error rate. The script used to generate the results in this figure is here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/static_networks.py">https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/static_networks.py</ext-link>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig6-v1.tif"/></fig><p><xref ref-type="fig" rid="fig6">Figure 6B</xref> shows significant (p&lt;0.05) differences in the power maps (B.I) and AEC networks (B.II) for old (34–60 years old) minus young (18–34 years old) groups. We observe a significant reduction in temporal <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>δ</mml:mi></mml:mstyle></mml:math></inline-formula>-power and an increase in sensorimotor <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-power. We also observe a significant increase in sensorimotor AEC in the <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-band (<xref ref-type="fig" rid="fig6">Figure 6BII</xref>).</p><p>The static (time-averaged) differences we see in young vs old participants can arise in many ways from the underlying dynamics of resting-state networks (<xref ref-type="fig" rid="fig4">Figures 4AI</xref> and <xref ref-type="fig" rid="fig5">5AI</xref>). For example, an increase in static power could be due to more frequent activations of a particular network. Conversely, the dynamics of the networks may be unaffected and the power within a network could be altered. Studying the dynamic network perspective using the HMM and/or DyNeMo can help provide further insights into how the static differences arise. Looking at the dynamic network perspective provided by the TDE-HMM, we see an increase in the fractional occupancy of state 4 (<xref ref-type="fig" rid="fig4">Figure 4BI</xref>), which is a network with high <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-power and connectivity (coherence) in the sensorimotor region. This is consistent with the static increase in <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-power and AEC connectivity we observe here; i.e., the increase in static <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-power and connectivity with age can be linked to a larger fraction of time spent in the sensorimotor network. The perspective provided by TDE-DyNeMo shows an increase with age in the contribution of mode 4 (<xref ref-type="fig" rid="fig5">Figure 5BI</xref>), which represents a sensorimotor network. This is a complementary explanation for the increase in static <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-power and connectivity as a larger contribution from the sensorimotor network to the overall brain activity of older participants.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In Section 2.1, we use the TDE-HMM to identify oscillatory bursts in a data-driven manner with much fewer assumptions than conventional burst detection methods based on amplitude thresholding. The advantages of using a data-driven approach like the TDE-HMM are discussed further in <xref ref-type="bibr" rid="bib38">Seedat et al., 2020</xref>; <xref ref-type="bibr" rid="bib26">Masaracchia, 2023</xref>. In short, with a conventional approach, we must pre-specify a frequency of interest and we may miss oscillatory bursts that do not reach an arbitrary threshold. In contrast, the TDE-HMM is less sensitive to the amplitude (it is better able to identify low-amplitude oscillatory bursts) and can identify the frequency of oscillations automatically.</p><p>In Sections 2.2 and 2.3, we presented the functional networks identified by HMMs in a variety of settings. These networks were identified automatically at fast (sub-second) timescales from the data (unsupervised) with no input from the user. We found a set of plausible networks that were related to task (<xref ref-type="fig" rid="fig2">Figure 2</xref>) and demographics (<xref ref-type="fig" rid="fig4">Figure 4</xref>). These networks were very reproducible: across multiple HMM runs; across different data preparation techniques (AE and TDE); across different experimental paradigms (task and rest); and across different scanners (Elekta and CTF).</p><p>Given we observe similar networks with the AE-HMM and TDE-HMM (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig3">3</xref>, respectively), one may ask which pipeline is recommended. The TDE-HMM approach is able to model dynamics in oscillatory amplitude and phase synchronization whereas the AE-HMM can only model dynamics in amplitude. This means the TDE-HMM is generally a better model for oscillatory dynamics. An occasion where the AE-HMM may be preferred is if the extra computational load of training on TDE/PCA data prohibits the TDE-HMM.</p><p>An important choice that has to be made when training an HMM/DyNeMo is the number of states/modes. For burst detection, we are often interested in identifying the time points when a burst occurs. This can be achieved by fitting a two-state HMM: an ‘on’ and ‘off’ state. If we’re interested in multiple burst types, we can increase the number of states. In this work, we chose a three-state HMM to stay close to the on/off description while allowing for multiple burst types. For the dynamic network analysis, we want a low number of states/modes to give us a compact representation of the data. A common choice is between 6 and 12. We can use the reproducibility analysis (section 2 in the SI) to show a given number of states/modes is reproducible and use this to find an upper limit for the number of states/modes that can be reliably inferred.</p><p>osl-dynamics offers a choice of two generative models for detecting network dynamics: the HMM and DyNeMo. The HMM assumes that there are mutually exclusive network states, whereas DyNeMo assumes the network modes are mixed differently at each time point. While DyNeMo’s assumption is arguably more realistic, the HMM’s stronger assumption has the benefit of simplifying the decomposition, which can make interpreting the network dynamics more straightforward. In short, the HMM and DyNeMo provide complementary descriptions of network dynamics, with either one being potentially useful depending on the context (<xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>). DyNeMo does have the additional advantage of using a richer temporal regularisation through the use of a deep recurrent network. This has been shown to capture longer-range temporal structure than the HMM (<xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>), and exploring the cognitive importance of long-range temporal structure is an interesting area of future investigation (<xref ref-type="bibr" rid="bib47">van Es, 2023</xref>). It is possible to quantify which model is better using a downstream task. In this manuscript, the downstream tasks are the evoked network response and young vs old group differences. We argue a better performance in the downstream task indicates a more useful model.</p><p>osl-dynamics can also be used to compute static network descriptions, including conventional static functional connectivity. This uses the same methodology as the state (or mode) specific network estimation in the dynamic approaches, making comparisons between dynamic and static perspectives more straightforward. In Section 2.5, we used this feature to relate the static functional network description to a dynamic perspective. We would like to stress that the young vs old study is used as an example of the type of group analyses that can be performed with this toolbox and that a more rigorous study with a larger population dataset is needed to understand the impact of aging on functional networks. The results in Section 2.5 should be taken as just an indication of possible ageing effects that can be investigated in a future study. In this report, we focus on the presentation of the tools needed to make such studies possible.</p></sec><sec id="s4" sec-type="conclusions"><title>Conclusions</title><p>We present a new toolbox for studying time series data: osl-dynamics. This is an open-source package written in Python. We believe the availability of this package in Python improves the accessibility of these tools, in particular for non-technical users. Additionally, it avoids the need for a paid license. Using Python also enables us to take advantage of modern deep learning libraries (in particular <xref ref-type="bibr" rid="bib45">TensorFlow, 2023</xref>) which enables us to scale these methods to very large datasets, something that is currently not possible with existing toolboxes.</p><p>osl-dynamics can be used, and has been used, in a wide range of applications and on a variety of data modalities: electrophysiological, invasive local field potential, functional magnetic resonance imaging, etc. Here, we illustrated its use in applications of burst detection and dynamic network analysis using MEG data. This package also allows the user to study the static (time-averaged) properties of a time series alongside dynamics within the same toolbox. The methods contained in osl-dynamics provide novel summary measures for dynamics and group-level analysis tools that can be used to inform our understanding of cognition, behavior, and disease.</p></sec><sec id="s5" sec-type="materials|methods"><title>Materials and methods</title><sec id="s5-1"><title>5.1 Generative models</title><p>In the study of dynamics, we are often interested in the properties of a time series, such as power spectral density (PSD), mean, covariance, etc., at a given time point. A common heuristic approach for calculating this is to use a sliding window. However, this approach only utilizes a short window around the time point of interest and suffers from a tradeoff between the temporal precision of dynamics and an accurate estimation of the properties (via a sufficiently large window). In <xref ref-type="bibr" rid="bib24">Liuzzi et al., 2019</xref>, it was shown that this approach is inadequate for studying fast changes in functional connectivity. In osl-dynamics, we adopt an alternative approach based on <italic>generative models</italic> (<xref ref-type="bibr" rid="bib22">Lamb, 2021</xref>). These are models that learn the probability distribution of the training data. In this report, we will focus on two generative models for time series data: the Hidden Markov Model (HMM) (<xref ref-type="bibr" rid="bib36">Rezek and Roberts, 2005</xref>) and Dynamic Network Modes (DyNeMo) (<xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>). Both of these models (discussed further below) incorporate an underlying dynamic latent variable in the generative process. The objective during training is to learn the most likely latent variables to have generated the observed time series (we minimize the <italic>variational free energy</italic> [This is equivalent to maximizing the negative variational free energy, which is also known as the evidence lower bound (ELBO) <xref ref-type="bibr" rid="bib4">Bishop and Nasrabadi, 2006</xref>] <xref ref-type="bibr" rid="bib4">Bishop and Nasrabadi, 2006</xref>). In doing this, the model can identify non-contiguous segments of the time series that share the same latent variable. Pooling this information leads to more robust estimates of the local properties of the data.</p><p>The generative model for the HMM (shown in <xref ref-type="fig" rid="fig7">Figure 7A</xref>) is<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula></p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Generative models implemented in osl-dynamics.</title><p>(<bold>A</bold>) Hidden Markov Model (HMM) (<xref ref-type="bibr" rid="bib49">Vidaurre et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Vidaurre et al., 2018a</xref>). Here, data is generated using a hidden state (<inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) and observation model, which in our case is a multivariate normal distribution parameterized by a state mean (<inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) and covariance (<inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>). Only one state can be active at a given time point. Dynamics are modeled via state switching using a transition probability matrix (<inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>), which forecasts the probability of the current state based on the previous state. (<bold>B</bold>) Dynamic Network Modes (DyNeMo) (<xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>). Here, the data is generated using a linear combination of <italic>modes</italic> (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) and dynamics are modelled using a recurrent neural network (RNN: <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>g</mml:mi></mml:mstyle></mml:math></inline-formula>), which forecasts the probability of a particular mixing ratio (<inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>) based on a long history of previous values via the underlying logits (<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig7-v1.tif"/></fig><p>where <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>K</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> is the latent state at time <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>K</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of states, and <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the generated data. <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the <italic>observation model</italic>. Here, we use<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> is a state mean and <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> is a state covariance. Dynamics in the time series are generated through state switching, which is characterized by the transition probability <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. Each pairwise state transition forms the <italic>transition probability matrix</italic>, (<xref ref-type="bibr" rid="bib36">Rezek and Roberts, 2005</xref>)<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:msub><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>j</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>.</mml:mo></mml:math></disp-formula></p><p>osl-dynamics uses variational Bayesian inference <xref ref-type="bibr" rid="bib4">Bishop and Nasrabadi, 2006</xref> to learn the most likely state to have generated the observed data. This has the advantage of being able to account for uncertainty in the latent state. For more information regarding the implementation of the HMM in osl-dynamics see the documentation: <ext-link ext-link-type="uri" xlink:href="https://osl-dynamics.readthedocs.io/en/latest/models/hmm.html">available here</ext-link>. The HMM has been successfully used to study dynamics in neuroimaging data in a variety of settings (<xref ref-type="bibr" rid="bib38">Seedat et al., 2020</xref>; <xref ref-type="bibr" rid="bib20">Khawaldeh et al., 2022</xref>; <xref ref-type="bibr" rid="bib52">Vidaurre et al., 2018b</xref>; <xref ref-type="bibr" rid="bib32">Quinn et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Higgins et al., 2021</xref>; <xref ref-type="bibr" rid="bib41">Sitnikova et al., 2018</xref>; <xref ref-type="bibr" rid="bib48">Van Schependom et al., 2019</xref>; <xref ref-type="bibr" rid="bib49">Vidaurre et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Vidaurre et al., 2018a</xref>; <xref ref-type="bibr" rid="bib50">Vidaurre et al., 2017</xref>).</p><p>DyNeMo is a recently proposed model that overcomes two key limitations of the HMM: the mutually exclusive states and limited memory (<xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>). The generative model for DyNeMo (shown in <xref ref-type="fig" rid="fig7">Figure 7B</xref>) is<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:munderover><mml:mo>∏</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is a latent vector at time <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> (referred to as a <italic>logit</italic>) and <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is the generated data. The observation model we use is<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">C</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>J</mml:mi></mml:munderover><mml:msub><mml:mi>α</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">μ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>J</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> is a mode mean, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">D</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>J</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula> is a mode covariance, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>J</mml:mi></mml:mstyle></mml:math></inline-formula> is the number of modes and<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo><mml:mi>j</mml:mi></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>is the mixing coefficient for mode <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>j</mml:mi></mml:mstyle></mml:math></inline-formula>. Dynamics in the latent vector are generated through <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, which is a distribution parameterized using a recurrent neural network (<xref ref-type="bibr" rid="bib13">Géron, 2022</xref>). Specifically,<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mtable columnalign="right left right left right left right left right left right left" rowspacing="3pt" columnspacing="0em 2em 0em 2em 0em 2em 0em 2em 0em 2em 0em" displaystyle="true"><mml:mtr><mml:mtd><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi mathvariant="bold-italic">σ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>θ</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:mtd><mml:mtd><mml:mi/><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>g</mml:mi></mml:mstyle></mml:math></inline-formula> are calculated using a recurrent neural network. osl-dynamics uses <italic>amortized</italic> variational Bayesian inference <xref ref-type="bibr" rid="bib21">Kingma and Welling, 2013</xref> to learn the most likely latent vector to have generated the observed data. This is a highly efficient inference scheme that is scalable to large datasets. For more information regarding the implementation of DyNeMo in osl-dynamics see the documentation: <ext-link ext-link-type="uri" xlink:href="https://osl-dynamics.readthedocs.io/en/latest/tutorials_build/data_preparation.html">available here</ext-link>.</p><p>Once trained, both models reveal a dynamic latent description of the training data. For the HMM, the latent description is a hidden state time course (Also known as the Viterbi path), which is the most likely state inferred at each time point in the training data. For DyNeMo, it is a mode time course, which is the mixing coefficient time series for each mode inferred from the training data. We will discuss in Sections 5.5.1 and 5.5.2 how these latent descriptions can be used to summarize dynamics in the training data.</p></sec><sec id="s5-2"><title>5.2 Datasets</title><p>We make use of two publicly available datasets:</p><list list-type="bullet"><list-item><p>CTF rest MEG dataset. This contains resting-state (eyes open) MEG data collected using a 275-channel CTF scanner. This dataset contains 5 min recordings from 65 healthy participants. It was collected at Nottingham University, UK as part of the MEGUK partnership (<xref ref-type="bibr" rid="bib28">Meg scientific research community, 2023</xref>).</p></list-item><list-item><p>Elekta task MEG dataset. This contains MEG data recorded during a visual perception task (<xref ref-type="bibr" rid="bib54">Wakeman and Henson, 2015</xref>). Six runs from 19 healthy participants were recorded using an Elekta Neuromag Vectorview 306 scanner. This dataset was collected at Cambridge University, UK.</p></list-item></list></sec><sec id="s5-3"><title>5.3 Preprocessing and source reconstruction</title><p>The steps involved in estimating source data from an MEG recording are shown in <xref ref-type="fig" rid="fig8">Figure 8</xref>. This part of the pipeline can be performed with the OHBA Software Library (OSL) (<xref ref-type="bibr" rid="bib27">Mats, 2023</xref>; <xref ref-type="bibr" rid="bib35">Quinn et al., 2022b</xref>), which is a separate Python package for M/EEG analysis. The exact steps applied to the raw data for each dataset were:</p><list list-type="order"><list-item><p>MaxFilter (only applied to the Elekta dataset).</p></list-item><list-item><p>Bandpass filter 0.5–125 Hz.</p></list-item><list-item><p>Notch filter 50 Hz and 100 Hz.</p></list-item><list-item><p>Downsample to 250 Hz.</p></list-item><list-item><p>Automated bad segment removal and bad channel detection (See osl.preprocessing.osl_wrappers.detect_badsegments and detect_badchannels in <xref ref-type="bibr" rid="bib27">Mats, 2023</xref>).</p></list-item><list-item><p>Automated ICA cleaning using the correlation of the EOG/ECG channel to select artifact components (See osl.preprocessing.mne_wrappers.run_mne_ica_autoreject in <xref ref-type="bibr" rid="bib27">Mats, 2023</xref>).</p></list-item><list-item><p>Coregistration (using polhemus headshape points/fiducials and a structural MRI).</p></list-item><list-item><p>Bandpass filter 1–45 Hz.</p></list-item><list-item><p>Linearly Constrained Minimum Variance (LCMV) beamformer.</p></list-item><list-item><p>Parcellate to regions of interest. In this work, we used 38 parcels (We used a parcellation based on anatomy: fmri_d100_parcellation_with_PCC_reduced_2 mm_ss5mm_ds8mm.nii.gz <xref ref-type="bibr" rid="bib27">Mats, 2023</xref>).</p></list-item><list-item><p>Symmetric orthogonalization (to correct source leakage <xref ref-type="bibr" rid="bib7">Colclough et al., 2015</xref>).</p></list-item><list-item><p>Dipole sign flipping (to align the sign of parcel time courses across subjects/runs) (See osl.source_recon.sign_flipping in <xref ref-type="bibr" rid="bib27">Mats, 2023</xref>. Note, this step can be skipped if you are training on amplitude envelope data).</p></list-item><list-item><p>Downsample to 100  Hz (only included in the burst detection pipeline).</p></list-item></list><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Preprocessing and source reconstruction.</title><p>First, the sensor-level recordings are cleaned using standard signal processing techniques. This includes filtering, downsampling, and artifact removal. Following this, the sensor-level recordings are used to estimate source activity using a beamformer. Finally, we parcellate the data and perform corrections (orthogonalization and dipole sign flipping). Acronyms: electrocardiogram (ECG), electrooculogram (EOG), independent component analysis (ICA), linearly constrained minimum variance (LCMV). These steps can be performed with the OHBA Software Library: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl">https://github.com/OHBA-analysis/osl</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib30">OHBA-analysis, 2024</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig8-v1.tif"/></fig><p>These preprocessing steps have been found to work well for a wide variety of datasets when studying dynamics. The scripts used for preprocessing and source reconstruction can be found here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/tree/main/examples/toolbox_paper">https://github.com/OHBA-analysis/osl-dynamics/tree/main/examples/toolbox_paper</ext-link>.</p></sec><sec id="s5-4"><title>5.4 Data preparation</title><p>We usually prepare the source data before training a model. The data preparation can be different depending on what aspect of the data we are interested in studying.</p><p>Amplitude envelope (AE). If we are interested in studying dynamics in the amplitude of oscillations, we can train a model on AE data. Here, we typically bandpass filter a frequency range of interest and calculate an AE using the absolute value of a Hilbert transform. <xref ref-type="fig" rid="fig9">Figure 9B</xref> shows what happens when we calculate the AE of oscillatory data. We can see the AE data tracks changes in the amplitude of oscillations.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Methods for preparing training data.</title><p>(<bold>A.I</bold>) Original (simulated) time series data. Only a short segment (0.2  s) is shown. Channel 1 (2) is a modulated sine wave at 15  Hz (30  Hz) with <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> noise added. (<bold>A.II</bold>) Covariance of the original data. (<bold>B</bold>) Amplitude envelope (AE) data (solid red line) and original data (dashed blue line). (<bold>C.I</bold>) Time-delay embedded (TDE) time series. An embedding window of ±7 lags was used. (<bold>C.II</bold>) Covariance of TDE data. (<bold>C.III</bold>) Spectral properties of the original data were estimated using the covariance matrix of TDE data. Acronyms: Autocorrelation function (ACF), Power spectral density (PSD).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig9-v1.tif"/></fig><p>Time-delay embedding (TDE). Studying the amplitude dynamics of oscillations does not reveal any insights into how different regions interact via phase synchronization. For this, we need to prepare the data using TDE (<xref ref-type="bibr" rid="bib44">Strogatz, 2018</xref>). This augments the time series with extra channels containing time-lagged versions of the original channels. <xref ref-type="fig" rid="fig9">Figure 9CI</xref> shows an example of this. To perform TDE, we need to specify the number of lagged channels to add (number of embeddings) and the lag to shift each additional channel by. In osl-dynamics, we always shift by one-time point, so we only need to specify the number of lags. By adding extra channels, we embed the autocorrelation function (ACF) of the original data (as well as the cross-correlation function) into the covariance matrix of the TDE data. This is illustrated in <xref ref-type="fig" rid="fig9">Figure 9CII</xref>. We plot the ACF taken from the TDE covariance matrix and the PSD (calculated using a Fourier transform) in <xref ref-type="fig" rid="fig9">Figure 9CIII</xref>. By using TDE data we make the covariance matrix sensitive to the frequency of oscillations in the original data. The covariance matrix is also sensitive to cross channel phase synchronization via the off-diagonal elements. Training on TDE data allows us to study dynamics in oscillatory amplitude and phase synchronization between channels. When we prepare TDE data, we are normally only interested in looking for dynamics in the auto/cross correlation function via the covariance matrix, so we fix the mean to zero in the generative model.</p><p>For further details and example code for preparing data in osl-dynamics see the tutorial: <ext-link ext-link-type="uri" xlink:href="https://osl-dynamics.readthedocs.io/en/latest/tutorials_build/data_preparation.html">available here</ext-link>.</p></sec><sec id="s5-5"><title>5.5 First-level and group-level analysis</title><p>Starting from the source reconstructed data, we study a dataset with a two-stage process:</p><list list-type="order"><list-item><p>First-level analysis. Here, our objective is to estimate subject-specific quantities. In the static (time-averaged) analysis, we calculate these quantities directly from the source data. However, if we are doing a dynamic analysis, we first train a generative model, such as the HMM or DyNeMo. Note, the HMM/DyNeMo are typically trained as group-level models using the concatenated data from all subjects. This means the models are unaware that the data originates from different subjects and allows the model to pool information across subjects, which can lead to more robust estimates of dynamic quantities. We use the latent description provided by the model with the source data to estimate the quantities of interest - this approach is known as <italic>dual estimation</italic> (This step is analogous to dual regression in independent component analysis) (<xref ref-type="bibr" rid="bib53">Vidaurre et al., 2021</xref>).</p></list-item><list-item><p>Group-level analysis. Quantities estimated for individual subjects, such as network metrics or summary statistics for dynamics, are used to model a group. For example, this could be predicting behavioral traits or characteristics of individual subjects, comparing two groups, or calculating the group average of network response to a task. Typically, statistical significance testing is done at the group level to verify that any observed differences or inferred relationships are not simply due to chance.</p></list-item></list><p>We will present the results of applying five pipelines to source reconstructed data calculated from the datasets mentioned in Section 5.2: a burst detection pipeline based on the HMM (discussed in Section 5.5.1); three dynamic network analysis pipelines based on the HMM and DyNeMo (discussed in Section 5.5.2) and a static network analysis pipeline (discussed in Section 5.5.3). Both the HMM and DyNeMo have been validated on simulated data for each application. See <xref ref-type="bibr" rid="bib33">Quinn et al., 2019</xref> for a demonstration of the HMM’s ability to identify oscillatory bursts and <xref ref-type="bibr" rid="bib49">Vidaurre et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref> for a demonstration of the HMM/DyNeMo’s ability to identify dynamic networks.</p><sec id="s5-5-1"><title>5.5.1 Burst detection</title><p>We use an approach based on the HMM to detect bursts of oscillatory activity. In this approach, we prepare the source data using TDE. A typical TDE-HMM burst detection pipeline is shown in <xref ref-type="fig" rid="fig10">Figure 10</xref>. When the HMM state time courses are inferred from the training data, each ‘visit’ to a particular state corresponds to a burst, or transient spectral event, with spectral properties specific to the state (e.g. an increase in <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi></mml:mstyle></mml:math></inline-formula>-band power). This approach assumes that we are looking for bursting in a single channel (brain region) at a time; separate HMMs can be used to detect bursting in each channel. We use the state time course to calculate summary statistics that characterize the dynamics of bursts. Typical summary statistics are:</p><list list-type="bullet"><list-item><p>Mean lifetime (The lifetime is also known as <italic>dwell time</italic>). This is the average duration a state is active.</p></list-item><list-item><p>Mean interval. This is the average duration between successive state activations.</p></list-item><list-item><p>Burst count. This is the number of times a state activates in a second on average.</p></list-item><list-item><p>Mean amplitude. This is the average value of the AE of the source data when each state is active.</p></list-item></list><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>TDE-HMM burst detection pipeline.</title><p>This is run on a single region’s parcel time course. Separate Hidden Markov Models (HMMs) are trained for each region. (<bold>A</bold>) Source reconstructed data is prepared by performing time-delay embedding and standardization (z-transform). Following this an HMM is trained on the data and statistics that summarize the bursts are calculated from the inferred state time course. (<bold>B</bold>) Subject-specific metrics summarizing the bursts at a particular region are used in group-level analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig10-v1.tif"/></fig><p>We calculate each of these for a particular state and subject. The averages are taken over all state activations. Given when a state is active we can use the source data to calculate the PSD of each burst type. We use the multitaper approach described in <xref ref-type="bibr" rid="bib49">Vidaurre et al., 2016</xref> to do this due to its ability to accurately estimate spectra. We present the results of applying a TDE-HMM burst detection pipeline to the CTF rest MEG dataset in Section 2.1.</p></sec><sec id="s5-5-2"><title>5.5.2 Identifying dynamic functional networks</title><p>osl-dynamics provides more options for modeling dynamic functional networks. Note, in this case, we train on multivariate data containing the activity at multiple regions of interest, rather than a single region, which is what we did in the burst detection pipeline (Section 5.5.1). Indeed, one perspective on using osl-dynamics to model dynamic functional networks, is that it is identifying bursts that span across multiple brain regions. <xref ref-type="fig" rid="fig11">Figure 11</xref> shows the different combinations of data preparation and generative models that are available for a dynamic network analysis pipeline. We discuss each of these options and when they should be used below.</p><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Dynamic functional network analysis pipeline.</title><p>(<bold>A</bold>) First-level modeling. This includes data preparation (shown in the blue boxes), model training, and post-hoc analysis (shown in the red boxes). The first-level modelling is used to derive subject-specific quantities. (<bold>B</bold>) Group-level modelling. This involves using the subject-specific description from the first-level modeling to model a group.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig11-v1.tif"/></fig><p>AE-HMM. If we are interested in identifying dynamics in amplitude, we can train on AE data. Once we have trained a model, we can estimate subject and state-specific networks (amplitude maps) using the training data and inferred state time course. Additionally, we can calculate summary statistics that characterize the dynamics from the inferred state time course. These summary statistics are:</p><list list-type="bullet"><list-item><p>Fractional occupancy. This is the fraction of the total time that each state is active.</p></list-item><list-item><p>Mean lifetime. This is the average duration that a state is active.</p></list-item><list-item><p>Mean interval. This is the average duration between successive state visits.</p></list-item><list-item><p>Switching rate. This is the number of activations per second of a particular state.</p></list-item></list><p>We calculate each of these for a particular state and subject. The averages are taken over all state activations. We present the results of an AE-HMM pipeline on the Elekta task MEG dataset in Section 2.2.</p><p>TDE-HMM. We can use TDE data to study dynamics in phase synchronization as well as dynamics in amplitude. In a dynamic network analysis pipeline we train on a multivariate time series (i.e. the time series for all regions of interest together). This means after TDE we have a very large number of channels (number of embeddings times the number of regions). Therefore, we often need to perform principal component analysis (PCA) for dimensionality reduction to ensure the data fits into computer memory.</p><p>In the TDE-HMM pipeline, we can calculate the same summary statistics as the AE-HMM pipeline. However, to estimate the functional networks we use the multitaper approach described in <xref ref-type="bibr" rid="bib49">Vidaurre et al., 2016</xref>. Here, we use the source data and inferred state time course to estimate subject, region and state-specific PSDs and cross PSDs. When then use the PSDs to calculate power maps and cross PSDs to calculate coherence networks, see <xref ref-type="bibr" rid="bib49">Vidaurre et al., 2016</xref> for further details. Note that we also use the spectral decomposition approach introduced in <xref ref-type="bibr" rid="bib52">Vidaurre et al., 2018b</xref> to specify a frequency range for calculating power maps and coherence networks. This involves applying non-negative matrix factorization to the stacked subject and state-specific coherence spectra to identify common frequency bands of coherent activity. In this report, we fit two spectral components and only present the networks for the first band, which typically covers 1–25  Hz. We will see the results of applying a TDE-HMM pipeline for dynamic network analysis on both the CTF rest and Elekta task MEG dataset in Section 2.3.</p><p>TDE-DyNeMo. This this pipeline, we replace the HMM with DyNeMo and train on TDE data. Unlike the mutually exclusive state description provided by the HMM, DyNeMo infers mode time courses, which describe the mixing ratio of each mode at each time point (<xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>). This mixture description complicates the calculation of subject-specific quantities, such as networks and summary statistics. To calculate mode and region-specific PSDs, we use the approach based on the General Linear Model (GLM) proposed in <xref ref-type="bibr" rid="bib34">Quinn et al., 2022a</xref> where we regress the mixing coefficients onto a (cross) spectrogram, see <xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref> for further details. We then use the mode PSDs and cross PSDs to calculate power maps and coherence networks, respectively. We can summarize the dynamics of each mode time course with quantities such as the mean, standard deviation, and pairwise Pearson correlation. Alternatively, if we were interested in calculating the same summary statistics as the HMM (fractional occupancy, lifetime, interval, switching rate) we would first need to binarise the mode time courses. This can be done using a two-component Gaussian Mixture Model (GMM), which is discussed in <xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>. Note that an additional complication related to the mode time course is that it does not contain any information regarding the relative magnitude of each mode covariance. For example, a mode with a small value for the mixing ratio can still be a large contributor to the instantaneous covariance if the values in the mode covariance matrix are relatively large. We account for this by renormalizing the mode time course (We weigh each mode time course by the trace of its mode covariance and divide by the sum over modes at each time point to ensure the renormalized mode time course sums to one), this is discussed further in <xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>. We present the results of a TDE-DyNeMo pipeline on the CTF rest MEG dataset in Section 2.4.</p><p>AE-DyNeMo. The final option is to train DyNeMo on AE data. In this case, the amplitude maps are calculated using the GLM approach by regressing the mixing coefficients on a sliding window AE time course. Summary statistics for dynamics are calculated in the same way as the TDE-DyNeMo pipeline.</p><p>When we display the networks inferred by each of the pipelines above, we will threshold them to only show the strongest connections. In this work, we will specify the threshold using a data-driven approach where we fit a two-component GMM to the distribution of connections in each network. We interpret one of the components as the distribution for background connections and the other as the distribution for atypically strong connections, which is what we display in each plot.</p></sec><sec id="s5-5-3"><title>5.5.3 Identifying static functional networks</title><p>A feature of osl-dynamics is that more conventional, static (time-averaged), network analyses can be carried out using the same methodology that we use in the dynamic methods. This allows for a much more straightforward comparison between static and dynamic analyses. To model static functional networks we simply need to specify the metrics we would like to use to summarize the networks and we calculate these directly from the source data. <xref ref-type="fig" rid="fig12">Figure 12</xref> shows a typical static network analysis pipeline. We present the result of a static network analysis pipeline on the CTF rest MEG dataset in Section 2.5. Note, for the static networks we select the top 5% of connections to display in each plot rather than the GMM approach we used to threshold the dynamic functional networks.</p><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Static functional network analysis pipeline.</title><p>(<bold>A</bold>) The source reconstructed data is used to calculate metrics that describe networks. (<bold>B</bold>) The subject-specific metrics are used to model a group. Acronyms: Amplitude envelope correlation (AEC), Power spectral density (PSD).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-91949-fig12-v1.tif"/></fig></sec></sec><sec id="s5-6"><title>5.6 Run-to-run variability</title><p>The HMM and DyNeMo are trained by minimizing a cost function (in osl-dynamics, we use the <italic>variational free energy</italic> <xref ref-type="bibr" rid="bib36">Rezek and Roberts, 2005</xref>; <xref ref-type="bibr" rid="bib15">Gohil et al., 2022</xref>). As is typical, this approach suffers from a local optimum issue, where the model can converge to different explanations (latent descriptions) of the data during training. That is, different state/mode time courses can lead to similar values for the variational free energy. The final description can be sensitive to the stochasticity in updating model parameters and the initial parameter values.</p><p>A strategy for dealing with this that has worked well in the past is to train multiple models from scratch (each model is referred to as a <italic>run</italic>) and only the model with the lowest variational free energy is analyzed. We consider this model as the best description of the data. We ensure any conclusions based on this model are reproducible in the best model from another set of independent runs. In all of our figures here, we present the results from the best run from a set of 10. In the supplementary information (SI) we show these results are reproducible in an independent set of runs. Other strategies for dealing with run-to-run variability involve combining multiple runs, see <xref ref-type="bibr" rid="bib1">Alonso and Vidaurre, 2023</xref> for a discussion of these techniques.</p></sec><sec id="s5-7"><title>5.7 Reproducibility</title><p>We use variational Bayesian inference to learn model parameters (state/mode time courses and observation model means/covariances). This process involves updating the model parameters to minimize the <italic>variational free energy</italic> (<xref ref-type="bibr" rid="bib4">Bishop and Nasrabadi, 2006</xref>).</p><p>Each time we train an HMM or DyNeMo , we start from random model parameters and use a stochastic procedure to update the parameters. This leads to some variability in the final model parameters we converge to and their corresponding variational free energy. We deem the model parameters with the lowest free energy as the ones that best describe the data and use these for subsequence analysis. We deem a set of results as reproducible if we are consistently able to infer the same model parameters from a dataset. Empirically, we find picking the best run from a set of 10 consistently finds the same set of model parameters. Note, for a given set of hyperparameters, only the relative difference between values for the variational free energy is important.</p><p>We show the variational free energy for three sets of 10 runs in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref> – <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref>. In red, we highlight the variational free energy for the best run (i.e. the one with the lowest value). Within a set of runs, we see large variability in the variational free energy which would reflect variability in the model parameters. Only looking at the best run across sets we see similar values for the variational free energy. In <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref> – <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref>, we see the summary statistics for the best run from each set are very similar indicating we infer the same dynamics in each best run. In <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref> – <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref>, we see we infer the same amplitude/power maps. In <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D</xref> – <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1D</xref>, we see we observe the same results from the group-level analysis across all three sets.</p></sec></sec></body><back><sec sec-type="additional-information" id="s6"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Validation, Methodology</p></fn><fn fn-type="con" id="con3"><p>Software</p></fn><fn fn-type="con" id="con4"><p>Software, Methodology</p></fn><fn fn-type="con" id="con5"><p>Methodology</p></fn><fn fn-type="con" id="con6"><p>Methodology, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Data curation, Supervision, Methodology, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s7"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-91949-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s8"><title>Data availability</title><p>Raw sensor-level MEG recordings are publicly available: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.18112/openneuro.ds000117.v1.0.5">https://doi.org/10.18112/openneuro.ds000117.v1.0.5</ext-link> (Elekta Task Dataset); <ext-link ext-link-type="uri" xlink:href="https://meguk.ac.uk/database/">https://meguk.ac.uk/database/</ext-link> (CTF Rest Dataset, Nottingham). Source reconstructed data for both datasets is available here: <ext-link ext-link-type="uri" xlink:href="https://osf.io/by2tc/">https://osf.io/by2tc/</ext-link>. Scripts to train models and reproduce results are available here: <ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/tree/main/examples/toolbox_paper">https://github.com/OHBA-analysis/osl-dynamics/tree/main/examples/toolbox_paper</ext-link> (copy archived at <xref ref-type="bibr" rid="bib31">OHBA Analysis Group, 2024</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Gohil</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>OSL Dynamics Toolbox</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/by2tc/">by2tc</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Wakeman</surname><given-names>DC</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2015">2015</year><data-title>Multisubject, multimodal face processing</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds000117.v1.0.5</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alonso</surname><given-names>S</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Toward stability of dynamic FC estimates in neuroimaging and electrophysiology: Solutions and limits</article-title><source>Network Neuroscience</source><volume>7</volume><fpage>1389</fpage><lpage>1403</lpage><pub-id pub-id-type="doi">10.1162/netn_a_00331</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bakkum</surname><given-names>DJ</given-names></name><name><surname>Radivojevic</surname><given-names>M</given-names></name><name><surname>Frey</surname><given-names>U</given-names></name><name><surname>Franke</surname><given-names>F</given-names></name><name><surname>Hierlemann</surname><given-names>A</given-names></name><name><surname>Takahashi</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Parameters for burst detection</article-title><source>Frontiers in Computational Neuroscience</source><volume>7</volume><elocation-id>193</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2013.00193</pub-id><pub-id pub-id-type="pmid">24567714</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Başar</surname><given-names>E</given-names></name><name><surname>Güntekin</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A review of brain oscillations in cognitive disorders and the role of neurotransmitters</article-title><source>Brain Research</source><volume>1235</volume><fpage>172</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2008.06.103</pub-id><pub-id pub-id-type="pmid">18640103</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bishop</surname><given-names>CM</given-names></name><name><surname>Nasrabadi</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Pattern Recognition and Machine Learning</source><publisher-loc>New York</publisher-loc><publisher-name>springer</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Krienen</surname><given-names>FM</given-names></name><name><surname>Castellanos</surname><given-names>A</given-names></name><name><surname>Diaz</surname><given-names>JC</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The organization of the human cerebellum estimated by intrinsic functional connectivity</article-title><source>Journal of Neurophysiology</source><volume>106</volume><fpage>2322</fpage><lpage>2345</lpage><pub-id pub-id-type="doi">10.1152/jn.00339.2011</pub-id><pub-id pub-id-type="pmid">21795627</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Buzsáki</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Rhythms of the Brain</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195301069.001.0001</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colclough</surname><given-names>GL</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A symmetric multivariate leakage correction for MEG connectomes</article-title><source>NeuroImage</source><volume>117</volume><fpage>439</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.03.071</pub-id><pub-id pub-id-type="pmid">25862259</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coquelet</surname><given-names>N</given-names></name><name><surname>Wens</surname><given-names>V</given-names></name><name><surname>Mary</surname><given-names>A</given-names></name><name><surname>Niesen</surname><given-names>M</given-names></name><name><surname>Puttaert</surname><given-names>D</given-names></name><name><surname>Ranzini</surname><given-names>M</given-names></name><name><surname>Vander Ghinst</surname><given-names>M</given-names></name><name><surname>Bourguignon</surname><given-names>M</given-names></name><name><surname>Peigneux</surname><given-names>P</given-names></name><name><surname>Goldman</surname><given-names>S</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name><name><surname>De Tiège</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Changes in electrophysiological static and dynamic human brain functional architecture from childhood to late adulthood</article-title><source>Scientific Reports</source><volume>10</volume><elocation-id>18986</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-020-75858-0</pub-id><pub-id pub-id-type="pmid">33149179</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname><given-names>Y</given-names></name><name><surname>Fu</surname><given-names>Z</given-names></name><name><surname>Calhoun</surname><given-names>VD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Classification and prediction of brain disorders using functional connectivity: promising but challenging</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>525</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00525</pub-id><pub-id pub-id-type="pmid">30127711</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><chapter-title>Neuronal Oscillations, coherence, and consciousness</chapter-title><person-group person-group-type="editor"><name><surname>Engel</surname><given-names>AK</given-names></name><name><surname>Fries</surname><given-names>P</given-names></name></person-group><source>The Neurology of Consciousness</source><publisher-name>Academic Press</publisher-name><fpage>49</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/B978-0-12-800948-2.00003-0</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fries</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Rhythms for cognition: communication through coherence</article-title><source>Neuron</source><volume>88</volume><fpage>220</fpage><lpage>235</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.09.034</pub-id><pub-id pub-id-type="pmid">26447583</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garwood</surname><given-names>IC</given-names></name><name><surname>Chakravarty</surname><given-names>S</given-names></name><name><surname>Donoghue</surname><given-names>J</given-names></name><name><surname>Mahnke</surname><given-names>M</given-names></name><name><surname>Kahali</surname><given-names>P</given-names></name><name><surname>Chamadia</surname><given-names>S</given-names></name><name><surname>Akeju</surname><given-names>O</given-names></name><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Brown</surname><given-names>EN</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A hidden Markov model reliably characterizes ketamine-induced spectral dynamics in macaque local field potentials and human electroencephalograms</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1009280</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1009280</pub-id><pub-id pub-id-type="pmid">34407069</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Géron</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><source>Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow</source><publisher-name>O’Reilly Media, Inc</publisher-name></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghimatgar</surname><given-names>H</given-names></name><name><surname>Kazemi</surname><given-names>K</given-names></name><name><surname>Helfroush</surname><given-names>MS</given-names></name><name><surname>Aarabi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An automatic single-channel EEG-based sleep stage scoring method based on hidden Markov Model</article-title><source>Journal of Neuroscience Methods</source><volume>324</volume><elocation-id>108320</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108320</pub-id><pub-id pub-id-type="pmid">31228517</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gohil</surname><given-names>C</given-names></name><name><surname>Roberts</surname><given-names>E</given-names></name><name><surname>Timms</surname><given-names>R</given-names></name><name><surname>Skates</surname><given-names>A</given-names></name><name><surname>Higgins</surname><given-names>C</given-names></name><name><surname>Quinn</surname><given-names>A</given-names></name><name><surname>Pervaiz</surname><given-names>U</given-names></name><name><surname>van Amersfoort</surname><given-names>J</given-names></name><name><surname>Notin</surname><given-names>P</given-names></name><name><surname>Gal</surname><given-names>Y</given-names></name><name><surname>Adaszewski</surname><given-names>S</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Mixtures of large-scale dynamic functional brain network modes</article-title><source>NeuroImage</source><volume>263</volume><elocation-id>119595</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2022.119595</pub-id><pub-id pub-id-type="pmid">36041643</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gohil</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Osl-Dynamics</data-title><version designator="8085809">8085809</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_hmm_bursts.py">https://github.com/OHBA-analysis/osl-dynamics/blob/main/examples/toolbox_paper/ctf_rest/tde_hmm_bursts.py</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>C</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name><name><surname>Dolan</surname><given-names>R</given-names></name><name><surname>Behrens</surname><given-names>T</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Replay bursts in humans coincide with activation of the default mode and parietal alpha networks</article-title><source>Neuron</source><volume>109</volume><fpage>882</fpage><lpage>893</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.12.007</pub-id><pub-id pub-id-type="pmid">33357412</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunyadi</surname><given-names>B</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>De Vos</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A dynamic system of brain networks revealed by fast transient EEG fluctuations and their fMRI correlates</article-title><source>NeuroImage</source><volume>185</volume><fpage>72</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.09.082</pub-id><pub-id pub-id-type="pmid">30287299</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>When brain rhythms aren’t “rhythmic”: implication for their mechanisms and meaning</article-title><source>Current Opinion in Neurobiology</source><volume>40</volume><fpage>72</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.06.010</pub-id><pub-id pub-id-type="pmid">27400290</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khawaldeh</surname><given-names>S</given-names></name><name><surname>Tinkhauser</surname><given-names>G</given-names></name><name><surname>Torrecillos</surname><given-names>F</given-names></name><name><surname>He</surname><given-names>S</given-names></name><name><surname>Foltynie</surname><given-names>T</given-names></name><name><surname>Limousin</surname><given-names>P</given-names></name><name><surname>Zrinzo</surname><given-names>L</given-names></name><name><surname>Oswal</surname><given-names>A</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Tan</surname><given-names>H</given-names></name><name><surname>Litvak</surname><given-names>V</given-names></name><name><surname>Kühn</surname><given-names>A</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name><name><surname>Brown</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Balance between competing spectral states in subthalamic nucleus is linked to motor impairment in Parkinson’s disease</article-title><source>Brain</source><volume>145</volume><fpage>237</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1093/brain/awab264</pub-id><pub-id pub-id-type="pmid">34264308</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Auto-Encoding Variational Bayes</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1312.6114">https://doi.org/10.48550/arXiv.1312.6114</ext-link></element-citation></ref><ref id="bib22"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lamb</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Brief Introduction to Generative Models</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.2103.00265">https://doi.org/10.48550/arXiv.2103.00265</ext-link></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liégeois</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>J</given-names></name><name><surname>Kong</surname><given-names>R</given-names></name><name><surname>Orban</surname><given-names>C</given-names></name><name><surname>Van De Ville</surname><given-names>D</given-names></name><name><surname>Ge</surname><given-names>T</given-names></name><name><surname>Sabuncu</surname><given-names>MR</given-names></name><name><surname>Yeo</surname><given-names>BTT</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Resting brain dynamics at different timescales capture distinct aspects of human behavior</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2317</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10317-7</pub-id><pub-id pub-id-type="pmid">31127095</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liuzzi</surname><given-names>L</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>O’Neill</surname><given-names>GC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Hillebrand</surname><given-names>A</given-names></name><name><surname>Tewarie</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>How sensitive are conventional meg functional connectivity metrics with sliding windows to detect genuine fluctuations in dynamic functional connectivity?</article-title><source>Frontiers in Neuroscience</source><volume>13</volume><elocation-id>797</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2019.00797</pub-id><pub-id pub-id-type="pmid">31427920</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>X</given-names></name><name><surname>Zhuo</surname><given-names>Z</given-names></name><name><surname>Wei</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Altered temporal organization of brief spontaneous brain activities in patients with Alzheimer’s Disease</article-title><source>Neuroscience</source><volume>425</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2019.11.025</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Masaracchia</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Dissecting Unsupervised Learning through Hidden Markov Modelling in Electrophysiological Data</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.01.19.524547</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Mats</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>OHBA-analysis/OSL</data-title><version designator="66b2ccc">66b2ccc</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/OHBA-analysis/osl">https://github.com/OHBA-analysis/osl</ext-link></element-citation></ref><ref id="bib28"><element-citation publication-type="web"><person-group person-group-type="author"><collab>Meg scientific research community</collab></person-group><year iso-8601-date="2023">2023</year><article-title>The scientific community for MEG researchin the United Kingdom and Ireland</article-title><ext-link ext-link-type="uri" xlink:href="https://meguk.ac.uk">https://meguk.ac.uk</ext-link><date-in-citation iso-8601-date="2023-12-15">December 15, 2023</date-in-citation></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Moraud</surname><given-names>EM</given-names></name><name><surname>Tinkhauser</surname><given-names>G</given-names></name><name><surname>Agrawal</surname><given-names>M</given-names></name><name><surname>Brown</surname><given-names>P</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Predicting beta bursts from local field potentials to improve closed-loop DBS paradigms in Parkinson’s patients</article-title><conf-name>Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual International Conference</conf-name><fpage>3766</fpage><lpage>3796</lpage><pub-id pub-id-type="doi">10.1109/EMBC.2018.8513348</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="software"><person-group person-group-type="author"><collab>OHBA-analysis</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Osl</data-title><version designator="swh:1:rev:5e7fe04ed391758c8a2443ac7cd23a95d632a9c5">swh:1:rev:5e7fe04ed391758c8a2443ac7cd23a95d632a9c5</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:42e2e4a464ce3d6d3c607d3628860e873d105fe0;origin=https://github.com/OHBA-analysis/osl;visit=swh:1:snp:0ce92923ded6979416b1ece4c29ded1bba9a35e1;anchor=swh:1:rev:5e7fe04ed391758c8a2443ac7cd23a95d632a9c5">https://archive.softwareheritage.org/swh:1:dir:42e2e4a464ce3d6d3c607d3628860e873d105fe0;origin=https://github.com/OHBA-analysis/osl;visit=swh:1:snp:0ce92923ded6979416b1ece4c29ded1bba9a35e1;anchor=swh:1:rev:5e7fe04ed391758c8a2443ac7cd23a95d632a9c5</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="software"><person-group person-group-type="author"><collab>OHBA Analysis Group</collab></person-group><year iso-8601-date="2024">2024</year><data-title>Osl-Dynamics</data-title><version designator="swh:1:rev:cded3b2ea95efef7b6c089c5e111fd565b716922">swh:1:rev:cded3b2ea95efef7b6c089c5e111fd565b716922</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:24e90ba9db006336535b97767b1734a748ace81d;origin=https://github.com/OHBA-analysis/osl-dynamics;visit=swh:1:snp:c71097c212fde5128a5a1c1ddfc86310538e391d;anchor=swh:1:rev:cded3b2ea95efef7b6c089c5e111fd565b716922">https://archive.softwareheritage.org/swh:1:dir:24e90ba9db006336535b97767b1734a748ace81d;origin=https://github.com/OHBA-analysis/osl-dynamics;visit=swh:1:snp:c71097c212fde5128a5a1c1ddfc86310538e391d;anchor=swh:1:rev:cded3b2ea95efef7b6c089c5e111fd565b716922</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Abeysuriya</surname><given-names>R</given-names></name><name><surname>Becker</surname><given-names>R</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Task-evoked dynamic network analysis through hidden markov modeling</article-title><source>Frontiers in Neuroscience</source><volume>12</volume><elocation-id>603</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2018.00603</pub-id><pub-id pub-id-type="pmid">30210284</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Heideman</surname><given-names>SG</given-names></name><name><surname>Nowak</surname><given-names>M</given-names></name><name><surname>Seedat</surname><given-names>ZA</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Zich</surname><given-names>C</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unpacking transient event dynamics in electrophysiological power spectra</article-title><source>Brain Topography</source><volume>32</volume><fpage>1020</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1007/s10548-019-00745-5</pub-id><pub-id pub-id-type="pmid">31754933</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Atkinson</surname><given-names>LZ</given-names></name><name><surname>Gohil</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>O</given-names></name><name><surname>Pitt</surname><given-names>J</given-names></name><name><surname>Zich</surname><given-names>C</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2022">2022a</year><article-title>The GLM-Spectrum: A Multilevel Framework for Spectrum Analysis with Covariate and Confound Modelling</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.11.14.516449</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>van Es</surname><given-names>MWJ</given-names></name><name><surname>Gohil</surname><given-names>C</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2022">2022b</year><data-title>OHBA software library in python (OSL)</data-title><version designator="Version 0.1.1">Version 0.1.1</version><source>Zenodo</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.6875059">https://doi.org/10.5281/zenodo.6875059</ext-link></element-citation></ref><ref id="bib36"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rezek</surname><given-names>I</given-names></name><name><surname>Roberts</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Ensemble hidden Markov models with extended observation densities for Biosignal analysis</chapter-title><person-group person-group-type="editor"><name><surname>Husmeier</surname><given-names>D</given-names></name><name><surname>Dybowski</surname><given-names>R</given-names></name><name><surname>Roberts</surname><given-names>S</given-names></name></person-group><source>Probabilistic Modeling in Bioinformatics and Medical Informatics</source><publisher-name>Springer</publisher-name><fpage>419</fpage><lpage>450</lpage><pub-id pub-id-type="doi">10.1007/1-84628-119-9_14</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvan</surname><given-names>P</given-names></name><name><surname>Lazari</surname><given-names>A</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Mandino</surname><given-names>F</given-names></name><name><surname>Johansen-Berg</surname><given-names>H</given-names></name><name><surname>Grandjean</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Frequency modulation of entorhinal cortex neuronal activity drives distinct frequency-dependent states of brain-wide dynamics</article-title><source>Cell Reports</source><volume>37</volume><elocation-id>109954</elocation-id><pub-id pub-id-type="doi">10.1016/j.celrep.2021.109954</pub-id><pub-id pub-id-type="pmid">34731612</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seedat</surname><given-names>ZA</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Liuzzi</surname><given-names>L</given-names></name><name><surname>Gascoyne</surname><given-names>LE</given-names></name><name><surname>Hunt</surname><given-names>BAE</given-names></name><name><surname>O’Neill</surname><given-names>GC</given-names></name><name><surname>Pakenham</surname><given-names>DO</given-names></name><name><surname>Mullinger</surname><given-names>KJ</given-names></name><name><surname>Morris</surname><given-names>PG</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The role of transient spectral “bursts” in functional connectivity: A magnetoencephalography study</article-title><source>NeuroImage</source><volume>209</volume><elocation-id>116537</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.116537</pub-id><pub-id pub-id-type="pmid">31935517</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sharma</surname><given-names>A</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Vesper</surname><given-names>J</given-names></name><name><surname>Schnitzler</surname><given-names>A</given-names></name><name><surname>Florin</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Differential dopaminergic modulation of spontaneous cortico-subthalamic activity in Parkinson’s disease</article-title><source>eLife</source><volume>10</volume><elocation-id>e66057</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.66057</pub-id><pub-id pub-id-type="pmid">34085932</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>H</given-names></name><name><surname>Law</surname><given-names>R</given-names></name><name><surname>Tsutsui</surname><given-names>S</given-names></name><name><surname>Moore</surname><given-names>CI</given-names></name><name><surname>Jones</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The rate of transient beta frequency events predicts behavior across tasks and species</article-title><source>eLife</source><volume>6</volume><elocation-id>e29086</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.29086</pub-id><pub-id pub-id-type="pmid">29106374</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sitnikova</surname><given-names>TA</given-names></name><name><surname>Hughes</surname><given-names>JW</given-names></name><name><surname>Ahlfors</surname><given-names>SP</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Salat</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Short timescale abnormalities in the states of spontaneous synchrony in the functional neural networks in Alzheimer’s disease</article-title><source>NeuroImage. Clinical</source><volume>20</volume><fpage>128</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.nicl.2018.05.028</pub-id><pub-id pub-id-type="pmid">30094163</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Fox</surname><given-names>PT</given-names></name><name><surname>Miller</surname><given-names>KL</given-names></name><name><surname>Glahn</surname><given-names>DC</given-names></name><name><surname>Fox</surname><given-names>PM</given-names></name><name><surname>Mackay</surname><given-names>CE</given-names></name><name><surname>Filippini</surname><given-names>N</given-names></name><name><surname>Watkins</surname><given-names>KE</given-names></name><name><surname>Toro</surname><given-names>R</given-names></name><name><surname>Laird</surname><given-names>AR</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Correspondence of the brain’s functional architecture during activation and rest</article-title><source>PNAS</source><volume>106</volume><fpage>13040</fpage><lpage>13045</lpage><pub-id pub-id-type="doi">10.1073/pnas.0905267106</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stevner</surname><given-names>ABA</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Cabral</surname><given-names>J</given-names></name><name><surname>Rapuano</surname><given-names>K</given-names></name><name><surname>Nielsen</surname><given-names>SFV</given-names></name><name><surname>Tagliazucchi</surname><given-names>E</given-names></name><name><surname>Laufs</surname><given-names>H</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Van Someren</surname><given-names>E</given-names></name><name><surname>Kringelbach</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Discovery of key whole-brain transitions and dynamics during human wakefulness and non-REM sleep</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>1035</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-08934-3</pub-id><pub-id pub-id-type="pmid">30833560</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Strogatz</surname><given-names>SH</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Nonlinear Dynamics and Chaos with Student Solutions Manual: With Applications to Physics, Biology, Chemistry, and Engineering</source><publisher-name>CRC press</publisher-name><pub-id pub-id-type="doi">10.1201/9780429399640</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="web"><person-group person-group-type="author"><collab>TensorFlow</collab></person-group><year iso-8601-date="2023">2023</year><article-title>Create production-grade machine learning models with TensorFlow</article-title><ext-link ext-link-type="uri" xlink:href="https://www.tensorflow.org/">https://www.tensorflow.org/</ext-link><date-in-citation iso-8601-date="2023-12-15">December 15, 2023</date-in-citation></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ede</surname><given-names>F</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural oscillations: sustained rhythms or transient burst-events?</article-title><source>Trends in Neurosciences</source><volume>41</volume><fpage>415</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.04.004</pub-id><pub-id pub-id-type="pmid">29739627</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>van Es</surname><given-names>MWJ</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Large-Scale Cortical Networks Are Organized in Structured Cycles</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2023.07.25.550338</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Schependom</surname><given-names>J</given-names></name><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Costers</surname><given-names>L</given-names></name><name><surname>Sjøgård</surname><given-names>M</given-names></name><name><surname>D’hooghe</surname><given-names>MB</given-names></name><name><surname>D’haeseleer</surname><given-names>M</given-names></name><name><surname>Wens</surname><given-names>V</given-names></name><name><surname>De Tiège</surname><given-names>X</given-names></name><name><surname>Goldman</surname><given-names>S</given-names></name><name><surname>Woolrich</surname><given-names>M</given-names></name><name><surname>Nagels</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Altered transient brain dynamics in multiple sclerosis: Treatment or pathology?</article-title><source>Human Brain Mapping</source><volume>40</volume><fpage>4789</fpage><lpage>4800</lpage><pub-id pub-id-type="doi">10.1002/hbm.24737</pub-id><pub-id pub-id-type="pmid">31361073</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Baker</surname><given-names>AP</given-names></name><name><surname>Dupret</surname><given-names>D</given-names></name><name><surname>Tejero-Cantero</surname><given-names>A</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Spectrally resolved fast transient brain states in electrophysiological data</article-title><source>NeuroImage</source><volume>126</volume><fpage>81</fpage><lpage>95</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.11.047</pub-id><pub-id pub-id-type="pmid">26631815</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Brain network dynamics are hierarchically organized in time</article-title><source>PNAS</source><volume>114</volume><fpage>12827</fpage><lpage>12832</lpage><pub-id pub-id-type="doi">10.1073/pnas.1705120114</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Abeysuriya</surname><given-names>R</given-names></name><name><surname>Becker</surname><given-names>R</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Alfaro-Almagro</surname><given-names>F</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2018">2018a</year><article-title>Discovering dynamic brain networks from big data in rest and task</article-title><source>NeuroImage</source><volume>180</volume><fpage>646</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.06.077</pub-id><pub-id pub-id-type="pmid">28669905</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Quinn</surname><given-names>AJ</given-names></name><name><surname>Hunt</surname><given-names>BAE</given-names></name><name><surname>Brookes</surname><given-names>MJ</given-names></name><name><surname>Nobre</surname><given-names>AC</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2018">2018b</year><article-title>Spontaneous cortical activity transiently organises into frequency specific phase-coupling networks</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2987</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05316-z</pub-id><pub-id pub-id-type="pmid">30061566</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vidaurre</surname><given-names>D</given-names></name><name><surname>Llera</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Behavioural relevance of spontaneous, transient brain network interactions in fMRI</article-title><source>NeuroImage</source><volume>229</volume><elocation-id>117713</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117713</pub-id><pub-id pub-id-type="pmid">33421594</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wakeman</surname><given-names>DG</given-names></name><name><surname>Henson</surname><given-names>RN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A multi-subject, multi-modal human neuroimaging dataset</article-title><source>Scientific Data</source><volume>2</volume><elocation-id>150001</elocation-id><pub-id pub-id-type="doi">10.1038/sdata.2015.1</pub-id><pub-id pub-id-type="pmid">25977808</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ward</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Synchronous neural oscillations and cognitive processes</article-title><source>Trends in Cognitive Sciences</source><volume>7</volume><fpage>553</fpage><lpage>559</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2003.10.012</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wissel</surname><given-names>T</given-names></name><name><surname>Pfeiffer</surname><given-names>T</given-names></name><name><surname>Frysch</surname><given-names>R</given-names></name><name><surname>Knight</surname><given-names>RT</given-names></name><name><surname>Chang</surname><given-names>EF</given-names></name><name><surname>Hinrichs</surname><given-names>H</given-names></name><name><surname>Rieger</surname><given-names>JW</given-names></name><name><surname>Rose</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Hidden Markov model and support vector machine based decoding of finger movements using electrocorticography</article-title><source>Journal of Neural Engineering</source><volume>10</volume><elocation-id>056020</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2560/10/5/056020</pub-id><pub-id pub-id-type="pmid">24045504</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91949.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ray</surname><given-names>Supratim</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Indian Institute of Science Bangalore</institution><country>India</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Fundamental</kwd></kwd-group></front-stub><body><p>The authors present a comprehensive set of tools to compactly characterize the time-frequency interactions across a network. The utility of the toolbox is <bold>compelling</bold> and demonstrated through a series of exemplar brain imaging datasets. This <bold>fundamental</bold> work adds to the repertoire of techniques that can be used to study high-dimensional data.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91949.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In their revised manuscript, the authors have addressed all the concerns raised earlier (written below for completeness).</p><p>Summary:</p><p>These types of analyses use many underlying assumptions about the data, which are not easy to verify. Hence, one way to test how the algorithm is performing in a task is to study its performance on synthetic data in which the properties of the variable of interest can be apriori fixed. For example, for burst detection, synthetic data can be generated by injected bursts of known durations, and checking if the algorithm can pick it up. Burst detection is difficult in the spectral domain since direct spectral estimators have high variance (see Subhash Chandran et al., 2018, J Neurophysiol). Therefore, detected burst lengths are typically much lower than injected burst lengths (see their Figure 3). This problem can be solved by doing burst estimation in the time domain itself, for example, using Matching Pursuit (MP). I think the approach presented in this paper would also work since this model is also trained on data in the time domain. Indeed, the synthetic data can be made more &quot;challenging&quot; by injecting multiple oscillatory bursts that are overlapping in time, for which a greedy approach like MP may fail. It would be very interesting to test whether this method can &quot;keep up&quot; as the data is made more challenging. While showing results from brain signals directly (e.g., Figure 7) is nice, it will be even more impactful if it is backed up with results obtained from synthetic data with known properties.</p><p>I was wondering about what kind of &quot;synthetic data&quot; could be used for the results shown in Figure 8-12 but could not come up with a good answer. Perhaps data in which different sensory systems are activated (visual versus auditory) or sensory versus movement epochs are compared to see if the activation maps change as expected? We see similarities between states across multiple runs (reproducibility analysis) and across tasks (e.g. Figure 8 vs 9) and even methods (Figure 8 vs 10), which is great. However, we should also expect emergence of new modes specific to sensory activation (say auditory cortex for an auditory task). This will allow us to independently check the performance of this method.</p><p>The authors should explain the reproducibility results (variational free energy and best run analysis) in the Results section itself, to better orient the reader on what to look for.</p><p>Page 15: the comparison across subjects is interesting, but it is not clear why sensory-motor areas show a difference and the mean lifetime of the visual network decreases. Can you please explain this better? The promised discussion in section 3.5 can be expanded as well.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91949.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors have developed a comprehensive set of tools to describe dynamics within a single time-series or across multiple time-series. The motivation is to better understand interacting networks within the human brain. The time-series used here are from direct estimates of the brain's electrical activity; however the tools have been used with other metrics of brain function and would be applicable to many other fields.</p><p>Strengths:</p><p>The methods described are principled, based on generative probabilistic models.</p><p>This makes them compact descriptors of the complex time-frequency data.</p><p>Few initial assumptions are necessary in order to reveal this compact description.</p><p>The methods are well described and demonstrated within multiple peer reviewed articles.</p><p>This toolbox will be a great asset to the brain imaging community.</p><p>Weaknesses:</p><p>The only question I had (originally) was how to objectively/quantitatively compare different network models. This has now been addressed by the authors in the latest revision.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.91949.3.sa3</article-id><title-group><article-title>Author Response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gohil</surname><given-names>Chetan</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Huang</surname><given-names>Rukuang</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Roberts</surname><given-names>Evan</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>van Es</surname><given-names>Mats WJ</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Quinn</surname><given-names>Andrew J</given-names></name><role specific-use="author">Author</role><aff><institution>University of Birmingham</institution><addr-line><named-content content-type="city">Birmingham</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Vidaurre</surname><given-names>Diego</given-names></name><role specific-use="author">Author</role><aff><institution>Aarhus University</institution><addr-line><named-content content-type="city">Aarhus</named-content></addr-line><country>Denmark</country></aff></contrib><contrib contrib-type="author"><name><surname>Woolrich</surname><given-names>Mark W</given-names></name><role specific-use="author">Author</role><aff><institution>University of Oxford</institution><addr-line><named-content content-type="city">Oxford</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>We thank the reviewers for their feedback. Our response and a summary of the changes made to the manuscript are shown below. In addition to the changes made in response to the reviewer’s comments, we made the following changes to improve the manuscript:</p><list list-type="bullet"><list-item><p>We updated figures 8 and 9 using data with improved preprocessing and source reconstruction. We now also include graphical network plots. This helps in the cross method (figure 8 vs 9) and cross dataset (figure 9 vs 10) comparison.</p></list-item><list-item><p>We added funding acknowledgments and a credit author statement.</p></list-item></list><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>These types of analyses use many underlying assumptions about the data, which are not easy to verify. Hence, one way to test how the algorithm is performing in a task is to study its performance on synthetic data in which the properties of the variable of interest can be apriori fixed. For example, for burst detection, synthetic data can be generated by injected bursts of known durations, and checking if the algorithm is able to pick it up. Burst detection is difficult in the spectral domain since direct spectral estimators have high variance (see Subhash Chandran et al., 2018, J Neurophysiol). Therefore, detected burst lengths are typically much lower than injected burst lengths (see Figure 3). This problem can be solved by doing burst estimation in the time domain itself, for example, using Matching Pursuit (MP). I think the approach presented in this paper would also work since this model is also trained on data in the time domain. Indeed, the synthetic data can be made more &quot;challenging&quot; by injecting multiple oscillatory bursts that are overlapping in time, for which a greedy approach like MP may fail. It would be very interesting to test whether this method can &quot;keep up&quot; as the data is made more challenging. While showing results from brain signals directly (e.g., Figure 7) is nice, it will be even more impactful if it is backed up with results obtained from synthetic data with known properties.</p></disp-quote><p>We completely agree with the reviewer that testing the methods using synthetic data is an important part of validating such an approach. Each of the original papers that apply these methods to a particular application do this. The focus of this manuscript is to present a toolbox for applying these methods rather than to introduce/validate the methods themselves. For a detailed validation of the methods, the reader should see the citations. For example, the following paper introduces the HMM as a method for oscillatory burst detection:</p><list list-type="bullet"><list-item><p>A.J. Quinn, et al. “Unpacking transient event dynamics in electrophysiological power spectra”. Brain topography 32.6 (2019): 1020-1034. See figures 2 and 3 for an evaluation of the HMM’s performance in detecting single-channel bursts using synthetic data.</p></list-item></list><p>We have added text to paragraph 2 in section 2.5 to clarify this burst detection method has been validated using simulated data and added references.</p><disp-quote content-type="editor-comment"><p>I was wondering about what kind of &quot;synthetic data&quot; could be used for the results shown in Figure 8-12 but could not come up with a good answer. Perhaps data in which different sensory systems are activated (visual versus auditory) or sensory versus movement epochs are compared to see if the activation maps change as expected. We see similarities between states across multiple runs (reproducibility analysis) and across tasks (e.g. Figure 8 vs 9) and even methods (Figure 8 vs 10), which is great. However, we should also expect the emergence of new modes specific to sensory activation (say auditory cortex for an auditory task). This will allow us to independently check the performance of this method.</p></disp-quote><p>The following papers study the performance of the HMM and DyNeMo in detecting networks using synthetic data:</p><list list-type="bullet"><list-item><p>D. Vidaurre, et al. “Spectrally resolved fast transient brain states in electrophysiological data”. Neuroimage 126 (2016): 81-95. See figure 3 in this paper for an evaluation of the HMM’s performance in detecting oscillatory networks using simulation data.</p></list-item><list-item><p>C. Gohil, et al. “Mixtures of large-scale dynamic functional brain network modes”. Neuroimage 263 (2022): 119595. See figures 4 and 5 for an evaluation of DyNeMo performance in detecting overlapping networks and long-range temporal structure in the data.</p></list-item></list><p>We have added text to paragraph 2 in section 2.5 to clarify these methods have been well tested on simulated data and added references.</p><disp-quote content-type="editor-comment"><p>The authors should explain the reproducibility results (variational free energy and best run analysis) in the Results section itself, to better orient the reader on what to look for.</p></disp-quote><p>Considering the second reviewer’s comments, we moved the reproducibility results to the supplementary information (SI). This means the reproducibility results are no longer part of the main figures/text. However, we have added some text to help the reader understand what aspects indicate the results are reproducible in section 2 of the SI.</p><disp-quote content-type="editor-comment"><p>Page 15: the comparison across subjects is interesting, but it is not clear why sensory-motor areas show a difference and the mean lifetime of the visual network decreases. Can you please explain this better? The promised discussion in section 3.5 can be expanded as well.</p></disp-quote><p>It is well known that the frequency and amplitude of neuronal oscillations changes with age. E.g. see the following review: Ishii, Ryouhei, et al. &quot;Healthy and pathological brain aging: from the perspective of oscillations, functional connectivity, and signal complexity.&quot; Neuropsychobiology 75.4 (2018): 151-161. We observe older people have more beta activity and less alpha activity. These changes are seen in time-averaged calculations, i.e. the amplitude of oscillations are calculated using the entire time series for each subject.</p><p>The dynamic analysis presented in the paper provides further insight into how changes in the time-averaged quantities can occur through changes in the dynamics of frequency-specific networks. The sensorimotor network, which is a network with high beta activity, has a higher fractional occupancy. This indicates the change we observe in time-average beta power may be due to a longer amount of time spent in the sensorimotor network. The visual network, which is a network with high alpha activity, shows reduced lifetimes, which can explain the reduced time-averaged alpha activity seen with ageing.</p><p>We hope the improved text in the last paragraph of section 3.5 clarifies this. It should also be taken into account that the focus of this manuscript is the tools rather than an in-depth analysis of ageing. We use the age effect as an example of the potential analysis this toolbox enables.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors have developed a comprehensive set of tools to describe dynamics within a single time-series or across multiple time-series. The motivation is to better understand interacting networks within the human brain. The time-series used here are from direct estimates of the brain's electrical activity; however, the tools have been used with other metrics of brain function and would be applicable to many other fields.</p><p>Strengths:</p><p>The methods described are principled, and based on generative probabilistic models.</p><p>This makes them compact descriptors of the complex time-frequency data.</p><p>Few initial assumptions are necessary in order to reveal this compact description.</p><p>The methods are well described and demonstrated within multiple peer-reviewed articles.</p><p>This toolbox will be a great asset to the brain imaging community.</p><p>Weaknesses:</p><p>The only question I had was how to objectively/quantitatively compare different network models. This is possibly easily addressed by the authors.</p></disp-quote><p>We thank the reviewer for his/her comments. We address the weaknesses in our response in the “Recommendations For The Authors” section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Figure 2 legend: Please add the acronym for LCMV also.</p></disp-quote><p>We have now done this.</p><disp-quote content-type="editor-comment"><p>Section 2.5.1 page 8: the pipeline is shown in Figure 4, not 3.</p></disp-quote><p>This has been fixed.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>This is a great paper outlining a resource that can be applied to many different fields. I have relatively minor comments apart from one.</p><p>How does one quantitatively compare network descriptors (from DyNeMo and TDE-HMM for example)? At the moment the word 'cleaner' (P17) is used, but is there any non-subjective way? (eg Free energy/ cross validation etc). At the moment it is useful that one method gives a larger effect size (in a comparison between groups).. but could the authors say something about the use of these methods as more/less faithful descriptors of the data? Or in other words, do all methods generate datasets (from the latent space) that can be quantitatively compared with the original data?</p></disp-quote><p>In principle, the variational free energy could be used to compare models. However, because we use an approximate variational free energy (an exact measure is not attainable) for DyNeMo and an exact free energy for the HMM, it is possible that any differences we see in the variational free energy between the HMM and Dynemo are caused by the errors in its approximation. This makes it unreliable for comparing across models. That said, we can still use the variational free energy to compare within models. Indeed, we use the variational free energy for quantitative model comparisons when we select the best run to analyse from a set of 10.</p><p>One viable approach for comparing models is to assess their performance on downstream tasks. In this manuscript, examples of downstream tasks are the evoked network response and the young vs old group difference. We argue a better performance in the downstream task indicates a more useful model within that context. This performance is a quantitative measure. Note, there is no straightforward answer to which is the best model. It is likely different models will be useful for different downstream tasks.</p><p>In terms of which model provides a more faithful description of the data. The more flexible generative model for DyNeMo means it will generate more realistic data. However, this doesn’t necessarily mean it’s the best model (for a particular downstream task). Both the HMM and DyNeMo provide complementary descriptions that can be useful.</p><p>We have clarified the above in paragraph 5 of section 4.</p><disp-quote content-type="editor-comment"><p>Other comments:</p><list list-type="bullet"><list-item><p>Footnote 6 - training on concatenated group data seems to be important. It could be more useful in the main manuscript where the limitations of this could be discussed.</p></list-item></list></disp-quote><p>By concatenating the data across subjects, we learn a group-level model. By doing this, we pool information across all subjects to estimate the networks. This can lead to more robust estimates. We have moved this footnote to the main text in paragraph 1 of section 2.5 and added further information.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>In the TDE burst detection section- please expand on why/how a specific number of states was chosen.</p></list-item></list></disp-quote><p>As with the HMM dynamic network analysis, the number of states must be pre-specified. For burst detection, we are often interested in an on/off type segmentation, which can be achieved with a 2 state HMM. However, if there are multiple burst types, these will all be combined into a single ‘on’ state. Therefore, we might want to increase the number of states to model multiple burst types. 3 was chosen as a trade-off to stay close to the on/off description but allow the model to learn more than 1 burst type. We have added text discussing this in paragraph 4 of section 4.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Normally the value of free energy is just a function of the data - and only relative magnitude is important. I think figures (eg 7c) would be clearer if the offset could be removed.</p></list-item></list></disp-quote><p>We agree only the relative magnitude is important. We added text clarifying this in section 2 of the SI. We think it would still be worthwhile to include the offset so that future users can be sure they have correctly trained a model and calculated the free energy.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Related to the above- there are large differences in model evidence shown between sets. Yet all sets are the same data, and all parameter estimates are more or less the same. Could the authors account for this please (i.e. is there some other parameter that differentiates the best model in one set from the other sets, or is the free energy estimate a bit variable).</p></list-item></list></disp-quote><p>We would like to clarify only the model parameters for the best run are shown in the group-level analysis. This is the run with the lowest variational free energy, which is highlighted in red. We have now clarified this in the caption of each figure. The difference in free energy for the best runs (across sets) is relatively small compared to the variation across runs within a set. If we were to plot the model parameters for each of the 10 runs in a set, we would see more variability. We have now clarified this in section 2 of the SI.</p><p>Also note, the group analysis usually involves taking an average. Small differences in the variational free energy could reflect small differences in subject-specific model parameters, which are then averaged out, giving virtually identical group effects.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>And related once again, if the data are always the same, I wonder if the free-energy plots and identical parameter estimates could be removed to free up space in figures?</p></list-item></list></disp-quote><p>The reproducibility results have now been moved to the supplementary information (SI).</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>When citing p-values please specify how they are corrected (and over what please eg over states, nodes, etc?). This would be useful didactically as I imagine most users will follow the format of the presentation in this paper.</p></list-item></list></disp-quote><p>We now include in the caption further details of how the permutation significance testing was done.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Not sure of the value of tiny power maps in 9C. Would consider making it larger or removing it?</p></list-item></list></disp-quote><p>The scale of these power maps is identical to part (A.I). We have moved the reproducibility analysis to the SI, enlarged the figure and added colour bars. We hope the values are now legible.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Figure 3. I think the embedding in the caption doesn't match the figure (+-5 vs +-7 lags). Would be useful to add in the units of covariance (cii).</p></list-item></list></disp-quote><p>The number of embeddings in the caption has been fixed. Regarding the units for the covariances, as this is simulated data there aren’t really any units. Note, there is already a colour bar to indicate the values of each element.</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Minimize variational free energy - it may be confusing for some readers that other groups maximize the negative free energy. Maybe a footnote?</p></list-item></list></disp-quote><p>We thank the reviewer for their suggestion. We have added a footnote (1).</p><disp-quote content-type="editor-comment"><list list-type="bullet"><list-item><p>Final question- and related to the Magnetoencephalography (MEG) data presented. These data are projected into source space using a beamformer algorithm (with its own implicit assumptions and vulnerabilities). Would be interested in the authors' opinion on what is standing between this work and a complete generative model of the MEG data - i.e. starting with cortical electrical current sources with interactions modeled and a dynamic environmental noise model (i.e. packing all assumptions into one model)?</p></list-item></list></disp-quote><p>In principle, there is nothing preventing us from including the forward model in the generative model and training on sensor level MEG data. This would be a generative model starting from the dipoles inside the brain to the MEG sensors. This is under active research. If the reviewer is referring to a biophysical model for brain activity, the main barrier for this is the inference of model parameters. However, note that the new inference framework presented in the DyNeMo paper (Gohil, et al. 2022) actually makes this more feasible. Given the scope of this manuscript is to present a toolbox for studying dynamics with existing methods, we leave this topic as future work.</p></body></sub-article></article>