<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75027</article-id><article-id pub-id-type="doi">10.7554/eLife.75027</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Social-affective features drive human representations of observed actions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-258534"><name><surname>Dima</surname><given-names>Diana C</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9612-5574</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-260377"><name><surname>Tomita</surname><given-names>Tyler M</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-73038"><name><surname>Honey</surname><given-names>Christopher J</given-names></name><contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-0745-5089</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-175557"><name><surname>Isik</surname><given-names>Leyla</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Cognitive Science</institution>, <institution>Johns Hopkins University</institution>, <addr-line><named-content content-type="city">Baltimore</named-content></addr-line>, <country>United States</country></aff><aff id="aff2"><institution content-type="dept">Department of Psychological and Brain Sciences</institution>, <institution>Johns Hopkins University</institution>, <addr-line><named-content content-type="city">Baltimore</named-content></addr-line>, <country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor" id="author-28129"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution>, <country>United States</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>ddima@jhu.edu</email> (DD);</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>24</day><month>05</month><year>2022</year></pub-date><volume>11</volume><elocation-id>e75027</elocation-id><history><date date-type="received"><day>26</day><month>10</month><year>2021</year></date><date date-type="accepted"><day>24</day><month>05</month><year>2022</year></date></history><permissions><copyright-statement>Â© 2022, Dima et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Dima et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link> permitting unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75027-v1.pdf"/><abstract><p>Humans observe actions performed by others in many different visual and social settings. What features do we extract and attend when we view such complex scenes, and how are they processed in the brain? To answer these questions, we curated two large-scale sets of naturalistic videos of everyday actions and estimated their perceived similarity in two behavioral experiments. We normed and quantified a large range of visual, action-related and social-affective features across the stimulus sets. Using a cross-validated variance partitioning analysis, we found that social-affective features predicted similarity judgments better than, and independently of, visual and action features in both behavioral experiments. Next, we conducted an electroencephalography (EEG) experiment, which revealed a sustained correlation between neural responses to videos and their behavioral similarity. Visual, action, and social-affective features predicted neural patterns at early, intermediate and late stages respectively during this behaviorally relevant time window. Together, these findings show that social-affective features are important for perceiving naturalistic actions, and are extracted at the final stage of a temporal gradient in the brain.</p></abstract><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>CCF-1231216</award-id><principal-award-recipient><name><surname>Isik</surname><given-names>Leyla</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group></article-meta></front><back><sec id="s1" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interest</title><fn fn-type="conflict" id="conf1"><p>The authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All procedures for data collection were approved by the Johns Hopkins University Institutional Review Board, with protocol numbers HIRB00009730 for the behavioral experiments and HIRB00009835 for the EEG experiment. Informed consent was obtained from all participants.</p></fn></fn-group></sec><sec id="s2" sec-type="supplementary-material"><title>Additional Files</title><sec id="s3" sec-type="data-availability"><title>Data availability</title><p>Behavioral and EEG data and results have been archived as an Open Science Framework repository (https://osf.io/hrmxn/). Analysis code is available on GitHub (https://github.com/dianadima/mot_action).</p><p>The following datasets were generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><collab>Dima DC</collab><collab>Tomita TM</collab><collab>Honey</collab><collab>CJ</collab><collab>Isik</collab><collab>L</collab></person-group><year iso-8601-date="2021">2021</year><source>Social-affective features drive human representations of observed actions</source><ext-link ext-link-type="uri" xlink:href="https://osf.io/hrmxn/">https://osf.io/hrmxn/</ext-link><comment>Open Science Framework, https://osf.io/hrmxn/</comment></element-citation></p></sec><supplementary-material><ext-link xlink:href="elife-75027-supp-v1.zip">Download zip</ext-link><p>Any figures and tables for this article are included in the PDF. The zip folder contains additional supplemental files.</p></supplementary-material></sec></back></article>