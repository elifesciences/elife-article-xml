<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">81701</article-id><article-id pub-id-type="doi">10.7554/eLife.81701</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Object representation in a gravitational reference frame</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-288781"><name><surname>Emonds</surname><given-names>Alexandriya MX</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="pa1">‡</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-288782"><name><surname>Srinath</surname><given-names>Ramanujan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1832-7250</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="pa2">§</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-280625"><name><surname>Nielsen</surname><given-names>Kristina J</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9155-2972</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="fn1">#</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-12393"><name><surname>Connor</surname><given-names>Charles E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8306-2818</contrib-id><email>connor@jhu.edu</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="fn1">#</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Department of Biomedical Engineering, Johns Hopkins University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Zanvyl Krieger Mind/Brain Institute, Johns Hopkins University</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00za53h95</institution-id><institution>Solomon H. Snyder Department of Neuroscience, Johns Hopkins University School of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Baltimore</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Arun</surname><given-names>SP</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute, Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>‡</label><p>University of Chicago, Chicago, United States</p></fn><fn fn-type="present-address" id="pa2"><label>§</label><p>Department of Neurobiology and Neuroscience Institute, University of Chicago, Chicago, United States</p></fn><fn fn-type="other" id="fn1"><label>#</label><p>Senior authors</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>10</day><month>08</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e81701</elocation-id><history><date date-type="received" iso-8601-date="2022-07-08"><day>08</day><month>07</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-07-19"><day>19</day><month>07</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-08-07"><day>07</day><month>08</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.08.06.503060"/></event></pub-history><permissions><copyright-statement>© 2023, Emonds et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Emonds et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-81701-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-81701-figures-v1.pdf"/><abstract><p>When your head tilts laterally, as in sports, reaching, and resting, your eyes counterrotate less than 20%, and thus eye images rotate, over a total range of about 180°. Yet, the world appears stable and vision remains normal. We discovered a neural strategy for rotational stability in anterior inferotemporal cortex (IT), the final stage of object vision in primates. We measured object orientation tuning of IT neurons in macaque monkeys tilted +25 and –25° laterally, producing ~40° difference in retinal image orientation. Among IT neurons with consistent object orientation tuning, 63% remained stable with respect to gravity across tilts. Gravitational tuning depended on vestibular/somatosensory but also visual cues, consistent with previous evidence that IT processes scene cues for gravity’s orientation. In addition to stability across image rotations, an internal gravitational reference frame is important for physical understanding of a world where object position, posture, structure, shape, movement, and behavior interact critically with gravity.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>object processing</kwd><kwd>gravity</kwd><kwd>scene vision</kwd><kwd>it cortex</kwd><kwd>vision</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>EY029420</award-id><principal-award-recipient><name><surname>Nielsen</surname><given-names>Kristina J</given-names></name><name><surname>Connor</surname><given-names>Charles E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-22206</award-id><principal-award-recipient><name><surname>Connor</surname><given-names>Charles E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-18-1-2119</award-id><principal-award-recipient><name><surname>Connor</surname><given-names>Charles E</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>NS086930</award-id><principal-award-recipient><name><surname>Connor</surname><given-names>Charles E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Neural recordings in monkey IT show that the primate visual system transforms object representations into a reference frame aligned with gravity and independent of how the head and eyes are tilted.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Reflexive eye movements compensate for up/down and right/left head movements, but when your head tilts laterally, as during sports, driving (<xref ref-type="bibr" rid="bib57">Zikovitz and Harris, 1999</xref>), social communication (<xref ref-type="bibr" rid="bib27">Halberstadt and Saitta, 1987</xref>; <xref ref-type="bibr" rid="bib34">Mignault and Chaudhuri, 2003</xref>; <xref ref-type="bibr" rid="bib30">Krumhuber et al., 2007</xref>; <xref ref-type="bibr" rid="bib32">Mara and Appel, 2015</xref>), working in cramped environments, reaching for distant objects, and resting in bed, your eyes compensate less than 20% (<xref ref-type="bibr" rid="bib35">Miller, 1962</xref>; <xref ref-type="bibr" rid="bib47">Schworm et al., 2002</xref>), so retinal images rotate around the point of fixation. But the perceptual compensation for this is so automatic and complete that we are usually unaware of the image rotation, and visual abilities are not strongly affected. This perceptual stability is more than just a generalization of recognition across orientations. Critically, our perceptual reference frame for objects remains stable with respect to the environment and gravity. As a result, trees still appear vertical and apples still appear to fall straight to the ground, even though their orientations and trajectories on the retina have changed.</p><p>Here, we explored the hypothesis that this perceptual stability is produced by transforming visual objects into a stable, non-retinal reference frame. Our previous work has shown that the primate ventral visual pathway (<xref ref-type="bibr" rid="bib22">Felleman and Van Essen, 1991</xref>) implements an object-centered reference frame (<xref ref-type="bibr" rid="bib40">Pasupathy and Connor, 1999</xref>; <xref ref-type="bibr" rid="bib41">Pasupathy and Connor, 2001</xref>; <xref ref-type="bibr" rid="bib42">Pasupathy and Connor, 2002</xref>; <xref ref-type="bibr" rid="bib10">Carlson et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Srinath et al., 2021</xref>; <xref ref-type="bibr" rid="bib7">Brincat and Connor, 2004</xref>; <xref ref-type="bibr" rid="bib8">Brincat and Connor, 2006</xref>; <xref ref-type="bibr" rid="bib56">Yamane et al., 2008</xref>; <xref ref-type="bibr" rid="bib28">Hung et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Connor and Knierim, 2017</xref>), stabilizing against position and size changes on the retina. But this still leaves open the <italic>orientation</italic> of the ventral pathway reference frame. Our recent work has shown that one channel in anterior ventral pathway processes scene-level visual cues for the orientation of the gravitational reference frame (<xref ref-type="bibr" rid="bib53">Vaziri et al., 2014</xref>; <xref ref-type="bibr" rid="bib54">Vaziri and Connor, 2016</xref>), raising the possibility that the ventral pathway reference frame is aligned with gravity. Here, we confirmed this hypothesis in anterior IT (<xref ref-type="bibr" rid="bib22">Felleman and Van Essen, 1991</xref>), and found that gravitational alignment depends on both visual and vestibular/somatosensory (<xref ref-type="bibr" rid="bib6">Brandt et al., 1994</xref>; <xref ref-type="bibr" rid="bib4">Baier et al., 2012</xref>) cues. To a lesser extent, we observed tuning aligned with the retinal reference frame, and object orientation in either reference frame was linearly decodable from IT population responses with high accuracy. This is consistent with psychophysical results showing voluntary perceptual access to either reference frame (<xref ref-type="bibr" rid="bib2">Attneave and Reid, 1968</xref>). The dominant, gravitationally aligned reference frame not only confers stability across image rotations but also enables physical understanding of objects in a world dominated by the force of gravity.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Object tuning in a gravitational reference frame</title><p>Monkeys performed a dot fixation task while we flashed object stimuli on a high-resolution LED monitor spanning 100° of the visual field in the horizontal direction. We used evolving stimuli guided by a genetic algorithm (<xref ref-type="bibr" rid="bib10">Carlson et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Srinath et al., 2021</xref>; <xref ref-type="bibr" rid="bib56">Yamane et al., 2008</xref>; <xref ref-type="bibr" rid="bib28">Hung et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Connor and Knierim, 2017</xref>; <xref ref-type="bibr" rid="bib53">Vaziri et al., 2014</xref>; <xref ref-type="bibr" rid="bib54">Vaziri and Connor, 2016</xref>) to discover 3D objects that drove strong responses from IT neurons. We presented these objects centered at fixation, across a range of screen orientations, with the monkey head-fixed and seated in a rotating chair tilted clockwise (–) or counterclockwise (+) by 25° about the axis of gaze (through the fixation point and the interpupillary midpoint; <xref ref-type="fig" rid="fig1">Figure 1a and b</xref>). Compensatory ocular counter-rolling was measured to be 6° based on iris landmarks visible in high-resolution photographs, consistent with previous measurements in humans (<xref ref-type="bibr" rid="bib35">Miller, 1962</xref>; <xref ref-type="bibr" rid="bib47">Schworm et al., 2002</xref>) and larger than previous measurements in monkeys (<xref ref-type="bibr" rid="bib45">Rosenberg and Angelaki, 2014</xref>), making it unlikely that we failed to adequately account for the effects of counterroll. Eye rotation would need to be five times greater than previously observed to mimic gravitational tuning. Our rotation measurements required detailed color photographs that could only be obtained with full lighting and closeup photography. This was not possible within the experiments themselves, where only low-resolution monochromatic infrared images of the eyes were available. Importantly, our analytical compensation for counter-rotation did not depend on our measurement of ocular rotation. Instead, we tested our data for correlation in retinal coordinates across a wide range of rotational compensation values. The fact that maximum correspondence, for those neurons tuned in the retinal reference frame, was observed at a compensation value of 6° (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) indicates that counterrotation during the experiments was consistent with our measurements outside the experiments.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Example neuron tuned for object orientation in a gravitational reference frame.</title><p>(<bold>a, b</bold>) Stimuli demonstrating example object orientations in the full scene condition. At each object orientation, the object was positioned on the ground-like surface naturalistically by virtually immersing or ‘planting’ 15% of its mass below ground, providing physical realism for orientations that would otherwise be visibly unbalanced and ensuring that most of the object was visible at each orientation. The high-response object shape and orientation discovered in the genetic algorithm experiments was always at the center of the tested orientation range and labeled 0°. The two monkey tilt conditions are diagrammed at left. The small <italic>white dots</italic> at the center of the head (connected by vertical <italic>dashed lines</italic>) represent the virtual axis of rotation produced by a circular sled supporting the chair. Stimuli were presented on a 100°-wide display screen for 750ms (separated by 250ms blank screen intervals) while the monkey fixated a central dot. Stimuli were presented in random order for a total of 5 repetitions each. (<bold>c,d</bold>) Responses of an example IT neuron to full scene stimuli, as a function of object orientation on the screen and thus with respect to gravity, across a 100° orientation range, while the monkey was tilted –25° (<bold>c</bold>) and 25° (<bold>d</bold>). Response values are averaged across the 750ms presentation time and across 5 repetitions and smoothed with a boxcar kernel of width 50° (3 orientation values). For this neuron, object orientation tuning remained consistent with respect to gravity across the two tilt conditions, with a peak response centered at 0° (<italic>dashed vertical line</italic>). The <italic>pink triangles</italic> indicate the object orientations compared across tilts in the gravitational alignment analysis. The two leftmost values are eliminated to equate the number of comparisons with the retinal alignment analysis. (<bold>e,f</bold>) The same data plotted against orientation on the retina, corrected for 6° counter-rolling of the eyes (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The <italic>cyan triangles</italic> indicate the response values compared across tilts in the retinal analysis. Due to 6° the shift produced by ocular counter-rolling, these comparison values were interpolated between tested screen orientations using a Catmull-Rom spline. Since for this cell orientation tuning was consistent in gravitational space, the peaks are shifted right or left by 19° each, that is 25° minus the 6° compensation for ocular counter-rotation. (<bold>g–j</bold>) Similar results were obtained for this neuron with isolated object stimuli.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Analysis of eye counter-rotation during tilt.</title><p>Eye orientation was estimated with lines connecting the center of the pupil with visualizable features at the edge of the iris. For the right and left eyes in the same monkey, the measured difference in eye orientation relative to the head was 12.65° for the right eye and 12.00° for the left eye (<italic>upper right</italic>). For the tilt experiments on all the neurons, combined across monkeys, we searched for the counterroll compensation that would produce the strongest agreement in retinal coordinates. At each compensation level tested, we normalized and summed the mean squared error (MSE) between responses at corresponding retinal positions. The best agreement in retinal coordinates (minimum MSE) was measured at 12° offset, corresponding to 6° rotation from normal in each of the tilt conditions (<italic>lower left</italic>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Example neurons tuned in gravitational space and retinal space.</title><p>(<bold>a, b</bold>) Stimuli demonstrating example object orientations used to study the two IT neurons. The orientation discovered in the genetic algorithm experiments is arbitrarily labeled 0°. The two monkey tilt conditions are diagrammed at left. (<bold>c, d</bold>) Responses of a gravitationally tuned IT neuron studied with the stimuli shown in (<bold>a</bold>), as a function of object orientation on the screen and thus with respect to gravity, across a 100° orientation range, while the monkey was tilted –25° (<bold>c</bold>) and 25° (<bold>d</bold>). Response values are averaged across the 750ms presentation time and across 5 repetitions and smoothed with a boxcar kernel of width 50° (3 orientation values). For this neuron, object orientation tuning remained consistent in screen/gravity space across the two tilt conditions. Other details as in <xref ref-type="fig" rid="fig1">Figure 1</xref>. (<bold>e, f</bold>) The same data plotted against orientation on the retina, corrected for 6° counter-rolling of the eyes in each tilt condition. Due to the shift produced by ocular counter-rolling, these comparison values were interpolated between tested screen orientations using a Catmull-Rom spline. Since orientation tuning was consistent in gravitational space, the tuning functions are shifted right or left by about 20° each. (<bold>g, h</bold>) Responses of a retinally-tuned IT neuron studied with the stimuli shown in (<bold>b</bold>), as a function of object orientation on the screen and thus with respect to gravity, across a 100° orientation range, while the monkey was tilted –25° (<bold>c</bold>) and 25° (<bold>d</bold>). In this case, the tuning peak was shifted about 40°, in the direction expected for orientation tuning in retinal space. (<bold>i,j</bold>) The same data plotted against orientation on the retina, corrected for 6° counter-rolling of the eyes in each tilt condition. The correspondence between curves in (<bold>i</bold>) and (<bold>j</bold>), with peaks at near 0°, is consistent with orientation tuning in retinal space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Expanded representation of results in panels (<bold>c</bold>) and (<bold>d</bold>) of <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of 5 repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Expanded representation of results in panels (<bold>e</bold>) and (<bold>f</bold>) of <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of 5 repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp4-v1.tif"/></fig><fig id="fig1s5" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 5.</label><caption><title>Expanded representation of results in panels (<bold>g</bold>) and (<bold>h</bold>) of <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of 5 repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp5-v1.tif"/></fig><fig id="fig1s6" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 6.</label><caption><title>Expanded representation of results in panels (<bold>i</bold>) and (<bold>j</bold>) of <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of five repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp6-v1.tif"/></fig><fig id="fig1s7" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 7.</label><caption><title>Expanded representation of results in <xref ref-type="fig" rid="fig1">Figure 1</xref>.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of five repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp7-v1.tif"/></fig><fig id="fig1s8" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 8.</label><caption><title>Additional example of gravitational tuning in an expanded format.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of five repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp8-v1.tif"/></fig><fig id="fig1s9" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 9.</label><caption><title>Additional example of gravitational tuning in explanded format.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of five repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp9-v1.tif"/></fig><fig id="fig1s10" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 10.</label><caption><title>Additional example of gravitational tuning in expanded format.</title><p>The complete set of stimuli and boxcar-smoothed (across neighboring orientations) response functions for each of five repetitions for full scene stimuli (top) and isolated object stimuli (bottom).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig1-figsupp10-v1.tif"/></fig></fig-group><p>The <xref ref-type="fig" rid="fig1">Figure 1</xref> example neuron was tested with both full scene stimuli (<xref ref-type="fig" rid="fig1">Figure 1a</xref>), which included a textured ground surface and horizon, providing visual cues for the orientation of gravity, and isolated objects (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), presented on a gray background, so that primarily vestibular and somatosensory cues indicated the orientation of gravity. The contrast between the two conditions helps to elucidate the additional effects of visual cues on top of vestibular/somatosensory cues. In addition, the isolated object condition controls for the possibility that tuning is affected by a shape-configuration (i.e. overlapping orientation) interaction between the object and the horizon or by differential occlusion of the object fragment buried in the ground (which was done to make the scene condition physically realistic for the wide variety of object orientations that would otherwise appear improbably balanced on a hard ground surface).</p><p>In <xref ref-type="fig" rid="fig1">Figure 1c and d</xref>, responses for the full scene condition are plotted as a function of orientation in the gravitational reference frame, that is orientation on the display screen. Despite the difference in body, head, and eye orientation between <xref ref-type="fig" rid="fig1">Figure 1c and d</xref>, the object orientation tuning pattern is stable; for example the peak at 0° lines up (<italic>vertical dashed line</italic>). The correlation between the two curves in gravitational coordinates is 0.99 (t=25.89, p=3.28 X 10<sup>–8</sup>). Thus, the object information signaled by this neuron, which necessarily originates in retinal coordinates, has been transformed into the gravitational reference frame.</p><p>When the same data are plotted in the retinal reference frame (<xref ref-type="fig" rid="fig1">Figure 1e and f</xref>), the peak near 0° shifts right or left by 19° (25° tilt minus 6° counterrotation of the eyes). This reflects the transformation of retinal information into a new reference frame. Because the eyes were rotated in different directions under the two tilt directions, the overlap of tested orientations in retinal coordinates is limited to seven screen orientations. In addition, to account for ocular counterrotation, the tested orientation values (<italic>black dots</italic>) in the two curves must be shifted 6° in the positive direction for the –25° tilt and 6° negative for the +25° tilt. Thus, the appropriate comparison points between <xref ref-type="fig" rid="fig1">Figure 1e and f</xref>, indicated by the <italic>cyan triangles,</italic> must be interpolated from the Catmull-Rom spline curves used to connect the tested orientations (<italic>black dots</italic>). A comparable set of seven comparison points in the gravitational reference frame (<xref ref-type="fig" rid="fig1">Figure 1c and d</xref>, <italic>pink triangles</italic>) falls directly on the tested orientations.</p><p>Object orientation tuning remained stable with respect to gravity across tilts, peaking at orientation 0°, for both full scene (<xref ref-type="fig" rid="fig1">Figure 1c and d</xref>) and isolated object (<xref ref-type="fig" rid="fig1">Figure 1g and h</xref>) stimuli. Correspondingly, orientation tuning profiles shifted relative to retinal orientation by about 40° between the two tilt conditions (<xref ref-type="fig" rid="fig1">Figure 1e, f, i and j</xref>), shifting the peak to the right and left of 0°. A similar example neuron is presented in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, along with an example neuron for which tuning aligned with the retina, and thus shifted with respect to gravity. Expanded versions of the stimuli and neural data for these examples and others are shown in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplements 3</xref>–<xref ref-type="fig" rid="fig1s10">10</xref>.</p></sec><sec id="s2-2"><title>Distribution of gravity- and retina-aligned tuning</title><p><xref ref-type="fig" rid="fig2">Figure 2a</xref> scatterplots correlation values between object orientation tuning functions in the two tilt conditions calculated with respect to retinal orientation (<italic>x axis</italic>) and gravity (<italic>y axis</italic>), for a sample of 89 IT neurons tested with full scene stimuli. In both the scatterplot and the marginal histograms, color indicates the result of a 1-tailed randomization t-test on each cell for significant positive correlation (p&lt;0.05) in the gravitational reference frame (<italic>pink</italic>), retinal reference frame (cyan), or both reference frames (<italic>dark gray</italic>) presumably due to the broad object orientation tuning of some IT neurons (<xref ref-type="bibr" rid="bib28">Hung et al., 2012</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Scatterplots of object orientation tuning function correlations across tilts.</title><p>(<bold>a</bold>) Scatterplot of correlations for full scene stimuli. Correlations of tuning in the gravitational reference frame (<italic>y axis</italic>) are plotted against correlations in the retinal reference frame (<italic>x axis</italic>). Marginal distributions are shown as histograms. Neurons with significant correlations with respect to gravity are colored <italic>pink</italic> and neurons with significant correlations with respect to the retinae are colored <italic>cyan</italic>. Neurons with significant correlations in both dimensions are colored <italic>dark gray</italic>, and neurons with no significant correlation are colored <italic>light gray</italic>. (<bold>b</bold>) Scatterplot for isolated object stimuli. Conventions the same as in (<bold>a</bold>). (<bold>c</bold>) Same scatterplot as in (<bold>a</bold>), but balanced for number of comparison orientations between gravitational and retinal analysis. (<bold>d</bold>) Same as (<bold>b</bold>), but balanced for number of comparison orientations between gravitational and retinal analysis. Comparable plots based on individual monkeys are shown in . Anatomical locations of neurons in individual monkeys are shown in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplements 4</xref> and <xref ref-type="fig" rid="fig2s5">5</xref> .</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Scatterplot of object orientation tuning function correlations for isolated objects surrounded by a circular aperture.</title><p>The circular aperture, with gray inside and black screen outside, provided a nearby, high-contrast frame for the object, intended to overwhelm the framing effects of the screen edges and thus diminish visual cues for the orientation of gravity. The circular surround conveys no visual information about the direction of gravity and would maintain a constant relationship to the object regardless of object-tilt. Details as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The results in this condition were comparable to the main results, with significant tendencies toward object orientation functions correlated with gravity (pink, p = 1.5259 X 10<sup>-10</sup>, two-tailed randomization t-test for center-of-mass relative to 0) and with the retinae (cyan, p = 8.8879 X 10<sup>-6</sup>). This supports the proposition that vestibular/somatosensory cues alone suffice for gravitationally aligned tuning in AIT.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Results in Figure 2 plotted only for monkey 2.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Results in Figure 2 plotted only for monkey 1.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Anatomical locations of neurons in individual monkeys plotted in saggital projections.</title><p>Anatomical locations of neurons in the three recording hemispheres (columns) (horizontal axis represents mm rostral to ear bars, vertical axis represents mm dorsal to ear bars). Color conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Top row (<bold>a, b, c</bold>) shows results for the full scene experiments, bottom row (<bold>d, e, f</bold>) shows results for the isolated object experiments.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig2-figsupp4-v1.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Anatomical locations of neurons in individual monkeys plotted in horizontal projections.</title><p>Anatomical locations of neurons in the three recording hemispheres (columns) (horizontal axis represents mm lateral to midline, vertical axis represents mm rostral to ear bars). Color conventions as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Top row (<bold>a, b, c</bold>) shows results for the full scene experiments, bottom row (<bold>d, e, f</bold>) shows results for the isolated object experiments. Based on locations in <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplements 4</xref> and <xref ref-type="fig" rid="fig2s5">5</xref>, slightly over 80% of neurons were recorded from STS (lower bank of superior temporal sulcus, where neurons have been found to be primarily object-sensitive <xref ref-type="bibr" rid="bib53">Vaziri et al., 2014</xref>), 10% from Ted (lateral convexity of inferior temporal lobe, where a majority of neurons have been found to be scene-sensitive), and a small number from TEv (the basal surface of the inferior temporal lobe) and the bottom lip of the superior temporal sulcus.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig2-figsupp5-v1.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>Scatterplot of object orientation tuning function correlations in gravitational space as measured in the scene conditions (0° horizon experiment) and the isolated object condition (floating).</title><p>Significant correlations (pink) included 8 exclusive to the isolated object condition (triangles), 11 exclusive to the scene condition (squares), and 23 apparent in both conditions (circles). Thus, gravitational alignment was observable in both conditions for a majority of cases. The 11 cells exclusive to the scene condition might suggest a necessary contribution of visual cues in some cases. However, the converse result with eight cells suggests a more complex dependency on cues and/or conflicting interactions produced by scene backgrounds. Across all 79 cells tested this way, there was no significant difference in the correlations under the two conditions (two-tailed paired t-test of correlation differences; p = 0. 1045, mean difference = 0.1077). This suggests that visual cues had little influence in our main experiments beyond the effects of clear, static vestibular and somatosensory cues.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig2-figsupp6-v1.tif"/></fig></fig-group><p>Of the 52 neurons with consistent object orientation tuning in one or both reference frames, 63% (33/52) were aligned with gravity, 21% (11/52) were aligned with the retinae, and 15% (8/52) were aligned with both. The population tendency toward positive correlation was strongly significant along the gravitational axis (two-tailed randomization t-test for center-of-mass relative to 0; p=6.49 X 10<sup>–29</sup>) and also significant though less so along the retinal axis (p=5.76 X 10<sup>–10</sup>). Similar results were obtained for a partially overlapping sample of 99 IT neurons tested with isolated object stimuli with no background (i.e. no horizon or ground plane; <xref ref-type="fig" rid="fig2">Figure 2b</xref>). In this case, 60% of the 53 neurons with significant object orientation tuning in one or both reference frames (32/53) showed significant correlation in the gravitational reference frame, 26% (14/53) significant correlation in the retinal reference frame, and within these groups 13% (7/53) were significant in both reference frames. The population tendency toward positive correlation was again significant in this experiment along both gravitational (p=3.63 X 10<sup>–22</sup>) and retinal axes (p=1.63 X 10<sup>–7</sup>). This suggests that gravitational tuning can depend primarily on vestibular/somatosensory cues for self-orientation. However, we cannot rule out a contribution of visual cues for gravity in the visual periphery, including screen edges and other horizontal and vertical edges and planes, which in the real world are almost uniformly aligned with gravity (but see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Nonetheless, the <xref ref-type="fig" rid="fig2">Figure 2b</xref> result confirms that gravitational tuning did not depend on the horizon or ground surface in the background condition. This is further confirmed through cell-by-bell comparison between scene and isolated for those cells tested with both (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>).</p><p>The analyses above were based on the full set of orientation comparisons possible for the gravitational reference frame (7), while the experimental design inevitably produced fewer comparisons for the retinal reference frame (5). Rerunning the analyses based on just 5 comparable object orientations in both reference frames (<xref ref-type="fig" rid="fig1">Figure 1</xref>, <italic>pink</italic> and <italic>cyan triangles</italic>) produced the results shown in <xref ref-type="fig" rid="fig2">Figure 2c and d</xref>. For full scene stimuli, this yielded 56% (23/41) significant gravitational alignment, 27% (11/41) retinal alignment, and 17% (7/41) dual alignment (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). For isolated object stimuli, this reanalysis yielded 58% (28/48) gravitational alignment, 29% (14/48) retinal alignment, and 13% (6/48) dual alignment (<xref ref-type="fig" rid="fig2">Figure 2d</xref>).</p></sec><sec id="s2-3"><title>Population coding of orientation in both reference frames</title><p>Neurons with no significant correlation in either reference frame might actually combine signals from both reference frames, as in other brain systems that interact with multiple reference frames (<xref ref-type="bibr" rid="bib51">Stricanne et al., 1996</xref>; <xref ref-type="bibr" rid="bib9">Buneo et al., 2002</xref>; <xref ref-type="bibr" rid="bib3">Avillac et al., 2005</xref>; <xref ref-type="bibr" rid="bib38">Mullette-Gillman et al., 2005</xref>; <xref ref-type="bibr" rid="bib15">Cohen and Groh, 2009</xref>; <xref ref-type="bibr" rid="bib11">Caruso et al., 2021</xref>; <xref ref-type="bibr" rid="bib13">Chang and Snyder, 2010</xref>; <xref ref-type="bibr" rid="bib33">McGuire and Sabes, 2011</xref>; <xref ref-type="bibr" rid="bib14">Chen et al., 2013</xref>). This would be consistent with human psychophysical results showing mixed influences of retinal and gravitational reference frames, with stronger weight for gravitational (<xref ref-type="bibr" rid="bib5">Bock and Dalecki, 2015</xref>; <xref ref-type="bibr" rid="bib17">Corballis et al., 1978</xref>). For mixed reference frame tuning of this kind, it has been shown that simple linear decoding can extract information in any one reference frame with an appropriate weighting pattern across neurons (<xref ref-type="bibr" rid="bib51">Stricanne et al., 1996</xref>; <xref ref-type="bibr" rid="bib18">Deneve et al., 2001</xref>; <xref ref-type="bibr" rid="bib43">Pouget et al., 2002</xref>). We tested that idea here and found that object orientation information in either gravitational or retinal space could be decoded with high accuracy from the responses of the IT neurons in our sample. The decoding task was to determine whether two population responses, across the 89 neurons tested with (different) full scene stimuli, were derived from same or different orientations (the same two orientation values were chosen for each stimulus peculiar to each neuron), either in gravitational space or retinal space (corrected for counter-rolling). This match/non-match task allowed us to analyze population information about orientation equivalence even though individual neurons were tested using different stimuli with no comparability between orientations. (Across neurons, orientations were aligned according to their order in the tested range, so that each non-match trial involved the same orientation difference, in the same direction, for each neuron.) Our decoding method was linear discriminant analysis of the neural population response patterns for each stimulus pair, implemented with Matlab function fitcdiscr.</p><p>The accuracy of these linear models for orientation match/non-match in the gravitational reference frame was 97% (10-fold cross-validation). The accuracy of models for orientation match/non-match in the retinal reference frame was 98%. (The accuracies for analyses based on the partially overlapping population of 99 neurons tested with isolated objects were 81% gravitational and 90% retinal.) The success of these simple linear models shows that information in both reference frames was easily decodable as weighted sums across the neural population. No complex, nonlinear, time-consuming neural processing would be required. This easy, linear decoding of information in both reference frames is consistent with psychophysical results showing that humans have voluntary access to either reference frame (<xref ref-type="bibr" rid="bib2">Attneave and Reid, 1968</xref>). High accuracy was obtained even with models based solely on neurons that showed no significant correlation in either gravitational or retinal reference frames (<xref ref-type="fig" rid="fig2">Figure 2a</xref>, <italic>light gray</italic>): 89% for gravitational discrimination and 97% for retinal discrimination. This supports the idea that these neurons carry a mix of retinal and gravitational object orientation signals.</p></sec><sec id="s2-4"><title>Gravity-aligned tuning based on purely visual cues</title><p>The results for isolated object stimuli in <xref ref-type="fig" rid="fig2">Figure 2b and d</xref> indicate that alignment of object information with gravity does not require the visual cues present in the full scene stimuli (ground surface and horizon) and can be based purely on vestibular and somatosensory cues for the direction of gravity in a dark room. We also tested the converse question of whether purely visual cues (tilted horizon and ground surface) could produce alignment of object orientation tuning with the visually apparent orientation of gravity, even in the presence of conflicting vestibular and somatosensory cues (i.e. with the monkey in a normal upright orientation). In spite of the conflict, many neurons showed object orientation tuning functions aligned with the visually cued direction of gravity, as exemplified in Fig. 3a,c–f. The five object orientations that were comparable in a gravitational reference frame (<italic>pink triangles</italic>) produced consistent responses to object orientations relative to the ground surface and horizon (<xref ref-type="fig" rid="fig3">Figure 3c and d</xref>). For example, the top left stimulus in <xref ref-type="fig" rid="fig3">Figure 3a</xref> (horizon tilt –25°, retinal orientation –25°) has the same orientation with respect to the ground surface as the bottom right stimulus (horizon tilt +25°, retinal orientation +25°). Thus, in the visually-cued gravitational reference frame, these two stimuli line up at 0° orientation in both <xref ref-type="fig" rid="fig3">Figure 3c and d</xref>, and they evoke similar responses. Conversely, the nine orientations comparable in the retinal reference (<italic>black dots</italic> and <italic>cyan triangles</italic>) produced inconsistent responses (<xref ref-type="fig" rid="fig3">Figure 3e and f</xref>). A different example neuron (Fig. 3b,g–j) exhibited object orientation tuning aligned with the retinae (<xref ref-type="fig" rid="fig3">Figure 3i and j</xref>) and not gravity (<xref ref-type="fig" rid="fig3">Figure 3g and h</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Example neurons tested with tilted horizon stimuli while the monkey remained in an upright orientation.</title><p>(<bold>a, b</bold>) Stimuli used to study two different neurons, demonstrating example object orientations in two conditions, with the ground surface, horizon, and sky gradient tilted –25° (clockwise, top row) or with ground surface, etc. tilted +25° (counterclockwise, second row). The monkey was in a normal upright orientation during these experiments, producing conflicting vestibular/somatosensory cues. The retinal orientation discovered in the genetic algorithm experiments is arbitrarily labeled 0°. (<bold>c, d</bold>) For one of the example IT neurons, tested with the stimuli in (<bold>a</bold>), object orientation tuning with respect to the visually cued direction of gravity was consistent across the two ground tilts. (<bold>e, f</bold>) Correspondingly, the neuron gave very different responses to retinal object orientation values between the two ground tilts. (<bold>g, h</bold>) This different example IT neuron, tested with the stimuli in (<bold>b</bold>), did not exhibit consistent object orientation tuning in visually-cued gravitational space. (<bold>i, j</bold>) Instead, this neuron maintained consistent tuning for retinal-screen orientation despite changes in ground tilt.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig3-v1.tif"/></fig><p>Across a sample of 228 IT neurons studied in this cue conflict experiment, 123 showed significant correlation across visual ground/horizon tilt in one or both reference frames. Of these, 54% (67/123) showed object orientation tuning aligned with the retinal reference frame, 35% (43/123) with the gravitational reference frame, and 11% (13/123) with both (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). The population tendency toward retina-aligned orientation tuning was significant (two-tailed randomization t-test for center-of-mass relative to 0; p=8.14 X 10<sup>–28</sup>) as was the tendency toward gravity-aligned orientation tuning (p=6.23 X 10<sup>–6</sup>). The experimental design in this case produced more comparisons in the retinal reference frame, and balancing the numbers of comparisons resulted in more equal percentages (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). The main result in this experiment, that many IT neurons exhibit object orientation tuning aligned with visual cues for the direction of gravity, even in the presence of conflicting vestibular/somatosensory cues, argues that visual cues contribute to gravity-aligned tuning under normal circumstances, where they combine with convergent vestibular/somatosensory cues. That would be consistent with our previous discovery that many neurons in IT are strongly tuned for the orientation of large-scale ground surfaces and edges, in the orientation ranges experienced across head tilts (<xref ref-type="bibr" rid="bib7">Brincat and Connor, 2004</xref>; <xref ref-type="bibr" rid="bib8">Brincat and Connor, 2006</xref>), and more generally with the strong visual representation of scene information in temporal lobe (<xref ref-type="bibr" rid="bib20">Epstein and Kanwisher, 1998</xref>; <xref ref-type="bibr" rid="bib21">Epstein, 2008</xref>; <xref ref-type="bibr" rid="bib31">Lescroart and Gallant, 2019</xref>; <xref ref-type="bibr" rid="bib29">Kornblith et al., 2013</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Scatterplots of object orientation tuning function correlations across visual horizon tilts on the screen, with the monkey in an upright orientation.</title><p>(<bold>a</bold>) Scatterplot of correlations for full scene stimuli. Correlations of tuning in gravitational space as cued by horizon tilt (<italic>y axis</italic>) are plotted against correlations in retinal space (<italic>x axis</italic>). Marginal distributions are shown as histograms. Neurons with significant correlations in visually-cued gravitational space are colored <italic>pink</italic> and neurons with significant correlations in retinal space are colored <italic>cyan</italic>. Neurons with significant correlations in both dimensions are colored <italic>dark gray</italic>, and neurons with no significant correlation are colored <italic>light gray</italic>. (<bold>b</bold>) Same scatterplot as in (<bold>a</bold>), but with correlation values balanced for number of comparison orientations between gravitational and retinal analysis.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-81701-fig4-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The fundamental goal of visual processing is to transform photoreceptor sheet responses into usable, essential information—readable, compressed, stable signals, for the specific things we need to understand about the world. In this sense, the transformation described here achieves both stability and specificity of object information. The gravitational reference frame remains stable across retinal image rotations, a critical advantage for vision from the tilting platform of the head and body. And, it enables understanding of object structure, posture, shape, motion, and behavior relative to the strong gravitational force that constrains and interacts with all these factors. It provides information about whether and how objects and object parts are supported and balanced against gravity, how flexible, motoric objects like bodies are interacting energetically with gravity, what postural or locomotive behaviors are possible or likely, and about potential physical interactions with other objects or with the observer under the influence of gravity. In other words, it provides information critical for guiding our mechanistic understanding of and skillful interactions with the world.</p><p>It is important to distinguish this result from the notion of increasing invariance, including rotational invariance, at higher levels in the ventral pathway. There is a degree of rotational invariance in IT, but even by the broadest definition of invariance (angular range across which responses to an optimal stimulus remain significantly greater than average responses to random stimuli) the average is ~90° for in-plane rotation and less for out-of-plane rotations (<xref ref-type="bibr" rid="bib28">Hung et al., 2012</xref>). It has often been suggested that the ventral pathway progressively discards information about spatial positions, orientations, and sizes as a way to standardize the neural representations of object identities. But, in fact, these critical dimensions for understanding the physical world of objects and environments are not discarded but rather transformed. In particular, spatial position information is transformed from retinal coordinates into relative spatial relationships between parts of contours, surfaces, object parts, and objects (<xref ref-type="bibr" rid="bib16">Connor and Knierim, 2017</xref>). Our results here indicate a novel kind of transformation of orientation information in the ventral pathway, from the original reference frame of the eyes to the gravitational reference frame that defines physical interactions in the world. Because this is an allocentric reference frame, the representation of orientation with respect to gravity is invariant to changes in the observer system (especially lateral head tilt), making representation more stable and more relevant to external physical events. However, our results do not suggest a change in orientation tuning breadth, increased invariance to object rotation, or a loss of critical object orientation information.</p><p>A similar hypothesis about gravity-related tuning for tilted planes has been tested in parietal area CIP (central intraparietal area). <xref ref-type="bibr" rid="bib45">Rosenberg and Angelaki, 2014</xref> measured the responses of 46 CIP neurons with two monkey tilts, right and left 30°, and fit the responses with linear models. They reported significant alignment with eye orientation for 45 of 92 (49%) tilt tests (two separate tests for each neuron, right and left), intermediate between eye and gravity for 26/92 tilt tests (28%), and alignment with gravity for 6/92 tilt tests (7%). However, of the 5 neurons in this last category, only one appeared to show significant alignment with gravity for both tilt directions (<xref ref-type="bibr" rid="bib45">Rosenberg and Angelaki, 2014</xref>; <xref ref-type="fig" rid="fig4">Figure 4D</xref>). Thus, while orientation tuning of ~35% of CIP neurons was sensitive to monkey tilt and gravity-aligned information could be extracted with a neural network (<xref ref-type="bibr" rid="bib45">Rosenberg and Angelaki, 2014</xref>), there was no explicit tuning in a gravitational reference frame or dominance of gravitational information as found here. There is however compelling human fMRI evidence that parietal and frontal cortex are deeply involved in perceiving and predicting physical events (<xref ref-type="bibr" rid="bib23">Fischer et al., 2016</xref>), and have unique abstract signals for stability not detected in ventral pathway (<xref ref-type="bibr" rid="bib44">Pramod et al., 2022</xref>), though these could reflect decision-making processes (<xref ref-type="bibr" rid="bib48">Shadlen and Newsome, 2001</xref>; <xref ref-type="bibr" rid="bib26">Gold and Shadlen, 2007</xref>). Our results and others (<xref ref-type="bibr" rid="bib24">Gallivan et al., 2014</xref>; <xref ref-type="bibr" rid="bib25">Gallivan et al., 2016</xref>; <xref ref-type="bibr" rid="bib12">Cesanek et al., 2021</xref>) suggest nonetheless that ventral pathway object and scene processing may be a critical source of information about gravity and its effects on objects, especially when detailed object representations are needed to assess precise shape, structure, support, strength, flexibility, compressibility, brittleness, specific gravity, mass distribution, and mechanical features to understand real world physical situations.</p><p>Our results raise the interesting question of <italic>how</italic> visual information is transformed into a gravity-aligned reference frame, and how that transformation incorporates vestibular, somatosensory, and visual cues for the direction of gravity. Previous work on reference frame transformation has involved <italic>shifts</italic> in the position of the reference frame. There is substantial evidence that these shifts are causally driven by anticipatory signals for attentional shifts and eye movements from prefrontal cortex, acting on ventral pathway cortex to activate neurons with newly relevant spatial sensitivities (<xref ref-type="bibr" rid="bib52">Tolias et al., 2001</xref>; <xref ref-type="bibr" rid="bib36">Moore and Armstrong, 2003</xref>; <xref ref-type="bibr" rid="bib37">Moore et al., 2003</xref>; <xref ref-type="bibr" rid="bib1">Armstrong et al., 2006</xref>; <xref ref-type="bibr" rid="bib46">Schafer and Moore, 2011</xref>; <xref ref-type="bibr" rid="bib39">Noudoost and Moore, 2011</xref>). Here, the more difficult geometric problem is <italic>rotation</italic> of visual information, such that “up”, “down”, “right” and “left” become associated with signals from different parts of the retina, based on a change in the perceived direction of gravity. This could also involve spatial remapping, but in circular directions, within an object-centered reference frame (<xref ref-type="bibr" rid="bib41">Pasupathy and Connor, 2001</xref>; <xref ref-type="bibr" rid="bib42">Pasupathy and Connor, 2002</xref>; <xref ref-type="bibr" rid="bib10">Carlson et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Srinath et al., 2021</xref>; <xref ref-type="bibr" rid="bib7">Brincat and Connor, 2004</xref>; <xref ref-type="bibr" rid="bib8">Brincat and Connor, 2006</xref>; <xref ref-type="bibr" rid="bib56">Yamane et al., 2008</xref>; <xref ref-type="bibr" rid="bib28">Hung et al., 2012</xref>; <xref ref-type="bibr" rid="bib16">Connor and Knierim, 2017</xref>). Humans can perform tasks requiring mental rotation of shapes, but this is time consuming in proportion to the angle of required rotation (<xref ref-type="bibr" rid="bib49">Shepard and Metzler, 1971</xref>), and seems to rely on an unusual strategy of covert motor simulation (<xref ref-type="bibr" rid="bib55">Wexler et al., 1998</xref>). The rotation required here is fast and so automatic as to be unnoticeable. Discovering the underlying transformation mechanism will likely require extensive theoretical, computational, and experimental investigation.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Behavioral task, stimulus presentation, and electrophysiological recording</title><p>Two head-restrained male rhesus monkeys (<italic>Macaca mulatta</italic>) were trained to maintain fixation within 1° (radius) of a 0.3° diameter spot for 4 s to obtain a juice reward. Eye position was monitored with an infrared eye tracker (EyeLink). Image stimuli were displayed on a 3840x2160 resolution, 80.11 DPI television screen placed directly in front of the monkey, centered at eye level at a distance of 60 cm. The screen subtended 70° vertically and 100° horizontally. Monkeys were seated in a primate chair attached to a+/–25° full-body rotation mechanism with the center of rotation at the midpoint between the eyes, so that the angle of gaze toward the fixation point remained constant across rotations. The rotation mechanism locked at body orientations of –25° (tilted clockwise), 0°, and +25° (counterclockwise). After fixation was initiated by the monkey, 4 stimuli were presented sequentially, for 750ms each, separated by 250ms intervals with a blank, gray background. All stimuli in a given generation were tested in random order for a total of five repetitions. The electrical activity of well-isolated single neurons was recorded with epoxy-coated tungsten electrodes (FHC Microsystems). Action potentials of individual neurons were amplified and electrically isolated using a Tucker-Davis Technologies recording system. Recording positions ranged from 5 to 25 mm anterior to the external auditory meatus within the inferior temporal lobe, including the ventral bank of the superior temporal sulcus, lateral convexity, and basal surface. Positions were determined on the basis of structural magnetic resonance images and the sequence of sulci and response characteristics observed while lowering the electrode. A total of 368 object-selective IT neurons were studied with different combinations of experiments. All animal procedures were approved by the Johns Hopkins Animal Care and Use Committee (protocol # PR21M442) and conformed to US National Institutes of Heath and US Department of Agriculture guidelines.</p></sec><sec id="s4-2"><title>Stimulus generation</title><p>Initially random 3D stimuli evolved through multiple generations under control of a genetic algorithm (<xref ref-type="bibr" rid="bib10">Carlson et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Srinath et al., 2021</xref>; <xref ref-type="bibr" rid="bib7">Brincat and Connor, 2004</xref>; <xref ref-type="bibr" rid="bib8">Brincat and Connor, 2006</xref>; <xref ref-type="bibr" rid="bib56">Yamane et al., 2008</xref>), leading to high-response stimuli used to test object orientation tuning as a function of eye/head/body rotation. Random shapes were created by defining 3D mesh surfaces surrounding medial axis skeletons (<xref ref-type="bibr" rid="bib50">Srinath et al., 2021</xref>). These shapes were assigned random or evolved optical properties including color, surface roughness, specularity/reflection, translucency/transparency, and subsurface scattering. They were depicted as projecting from (partially submerged in) planar ground surfaces covered with a random scrub grass texture extending toward a distant horizon meeting a blue, featureless sky, with variable ambient light color and lighting direction consistent with random or evolved virtual times of day. Ground surface tilt and object orientation were independent variables of interest as described in the main text. These varied across ranges of 100–200° at intervals of 12.5 or 25°. Ground surface slant, texture gradient, and horizon level, as well as object size and appearance, varied with random or evolved virtual viewing distances. The entire scenes were rendered with multi-step ray tracing using Blender Cycles running on a cluster of GPU-based machines.</p></sec><sec id="s4-3"><title>Data analysis and statistics</title><p>Response rates for each stimulus were calculated by counting action potentials during the presentation window and averaging across five repetitions. Orientation tuning functions were smoothed with boxcar averaging across three neighboring values. Pearson correlation coefficients between object orientation tuning functions in different conditions (in some cases corrected for ocular counter-rolling) were calculated for the averaged, smoothed values. Significance of positive correlations were measured with a one-tailed randomization t-test, p=0.05. (There was no a priori reason to predict or test for negative correlations between orientation tuning functions.) A null distribution was created by randomly assigning response values across the tested orientations within each of the two tuning functions and recalculating the t-statistic 10,000 times. Significant biases of population correlation distributions toward positive or negative values were measured with two-tailed randomization t-tests, with exact p-values reported. A null distribution was created by randomly assigning response values across the tested orientations within each of the two tuning functions for each neuron, recalculating the t-statistic for each neuron, and recalculating the correlation distribution center of mass on the correlation domain 10,000 times.</p></sec><sec id="s4-4"><title>Population decoding analysis</title><p>We pooled data across 89 neurons tested with full scene stimuli at the two monkey tilts and used cross-validated linear discriminant analysis to discriminate matching from non-matching orientations in both the retinal and gravitational reference frames. Ground truth matches were identical (in either gravitational or counter-rolling corrected retinal coordinates, depending on which reference frame was being tested). Ground truth non-matches differed by more than 25°. We equalized the numbers of retinal and gravitational match and non-match conditions by subsampling. This yielded five potential pairs of matches and 20 potential pairs of non-matches for each reference frame. For each member of a test pair, we randomly selected one raw response value for each neuron from among the five individual repetitions for that object orientation. We generated a dataset for all possible test pairs under these conditions. We used Matlab function fitcdiscr to build optimal models for linear discrimination of matches from non-matches based on response patterns across the 89 neurons. We built separate models for retinal and gravitational reference frame match/non-match discrimination. We report the accuracy of the models as 1 – misclassification rate using 10-fold cross validation.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Software, Formal analysis</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All animal procedures were approved by the Johns Hopkins Animal Care and Use Committee (protocol # PR21M422) and conformed to US National Institutes of Health and US Department of Agriculture guidelines.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-81701-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All relevant data is publicly accessible in our GitHub repository <ext-link ext-link-type="uri" xlink:href="https://github.com/amxemonds/ObjectGravity">https://github.com/amxemonds/ObjectGravity</ext-link> (copy archived at <xref ref-type="bibr" rid="bib19">Emonds, 2023</xref>). Further information requests should be directed to and will be fulfilled by the corresponding author, Charles E. Connor (connor@jhu.edu).</p></sec><ack id="ack"><title>Acknowledgements</title><p>The authors acknowledge the design and manufacturing contributions of William Nash, William Quinlan, and James Garmon, the software and hardware engineering contributions of Justin Killibrew, and the animal care, handling, training, and surgery contributions of Ofelia Garalde. Dr. Amy Bastian commented on the manuscript.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Armstrong</surname><given-names>KM</given-names></name><name><surname>Fitzgerald</surname><given-names>JK</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Changes in visual receptive fields with microstimulation of frontal cortex</article-title><source>Neuron</source><volume>50</volume><fpage>791</fpage><lpage>798</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.05.010</pub-id><pub-id pub-id-type="pmid">16731516</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Attneave</surname><given-names>F</given-names></name><name><surname>Reid</surname><given-names>KW</given-names></name></person-group><year iso-8601-date="1968">1968</year><article-title>Voluntary control of frame of reference and slope equivalence under head rotation</article-title><source>Journal of Experimental Psychology</source><volume>78</volume><fpage>153</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1037/h0026150</pub-id><pub-id pub-id-type="pmid">5682953</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avillac</surname><given-names>M</given-names></name><name><surname>Denève</surname><given-names>S</given-names></name><name><surname>Olivier</surname><given-names>E</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Duhamel</surname><given-names>J-R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Reference frames for representing visual and tactile locations in parietal cortex</article-title><source>Nature Neuroscience</source><volume>8</volume><fpage>941</fpage><lpage>949</lpage><pub-id pub-id-type="doi">10.1038/nn1480</pub-id><pub-id pub-id-type="pmid">15951810</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baier</surname><given-names>B</given-names></name><name><surname>Thömke</surname><given-names>F</given-names></name><name><surname>Wilting</surname><given-names>J</given-names></name><name><surname>Heinze</surname><given-names>C</given-names></name><name><surname>Geber</surname><given-names>C</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A pathway in the brainstem for roll-tilt of the subjective visual vertical: evidence from A lesion-behavior mapping study</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>14854</fpage><lpage>14858</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0770-12.2012</pub-id><pub-id pub-id-type="pmid">23100408</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bock</surname><given-names>OL</given-names></name><name><surname>Dalecki</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mental rotation of letters, body parts and scenes during whole-body tilt: role of a body-centered versus a gravitational reference frame</article-title><source>Human Movement Science</source><volume>40</volume><fpage>352</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1016/j.humov.2015.01.017</pub-id><pub-id pub-id-type="pmid">25682375</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandt</surname><given-names>T</given-names></name><name><surname>Dieterich</surname><given-names>M</given-names></name><name><surname>Danek</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Vestibular cortex lesions affect the perception of verticality</article-title><source>Annals of Neurology</source><volume>35</volume><fpage>403</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1002/ana.410350406</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brincat</surname><given-names>SL</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Underlying principles of visual shape selectivity in posterior inferotemporal cortex</article-title><source>Nature Neuroscience</source><volume>7</volume><fpage>880</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1038/nn1278</pub-id><pub-id pub-id-type="pmid">15235606</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brincat</surname><given-names>SL</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Dynamic shape synthesis in posterior inferotemporal cortex</article-title><source>Neuron</source><volume>49</volume><fpage>17</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.11.026</pub-id><pub-id pub-id-type="pmid">16387636</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buneo</surname><given-names>CA</given-names></name><name><surname>Jarvis</surname><given-names>MR</given-names></name><name><surname>Batista</surname><given-names>AP</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Direct visuomotor transformations for reaching</article-title><source>Nature</source><volume>416</volume><fpage>632</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1038/416632a</pub-id><pub-id pub-id-type="pmid">11948351</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carlson</surname><given-names>ET</given-names></name><name><surname>Rasquinha</surname><given-names>RJ</given-names></name><name><surname>Zhang</surname><given-names>K</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A sparse object coding scheme in area V4</article-title><source>Current Biology</source><volume>21</volume><fpage>288</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.01.013</pub-id><pub-id pub-id-type="pmid">21315595</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caruso</surname><given-names>VC</given-names></name><name><surname>Pages</surname><given-names>DS</given-names></name><name><surname>Sommer</surname><given-names>MA</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Compensating for a shifting world: evolving reference frames of visual and auditory signals across three multimodal brain areas</article-title><source>Journal of Neurophysiology</source><volume>126</volume><fpage>82</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1152/jn.00385.2020</pub-id><pub-id pub-id-type="pmid">33852803</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cesanek</surname><given-names>E</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>Ingram</surname><given-names>JN</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Motor memories of object dynamics are categorically organized</article-title><source>eLife</source><volume>10</volume><elocation-id>e71627</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71627</pub-id><pub-id pub-id-type="pmid">34796873</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>SWC</given-names></name><name><surname>Snyder</surname><given-names>LH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Idiosyncratic and systematic aspects of spatial representations in the macaque parietal cortex</article-title><source>PNAS</source><volume>107</volume><fpage>7951</fpage><lpage>7956</lpage><pub-id pub-id-type="doi">10.1073/pnas.0913209107</pub-id><pub-id pub-id-type="pmid">20375282</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>X</given-names></name><name><surname>Deangelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Diverse spatial reference frames of vestibular signals in parietal cortex</article-title><source>Neuron</source><volume>80</volume><fpage>1310</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.006</pub-id><pub-id pub-id-type="pmid">24239126</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>YE</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Motor-related signals in the intraparietal cortex encode locations in a hybrid, rather than eye-centered reference frame</article-title><source>Cerebral Cortex</source><volume>19</volume><fpage>1761</fpage><lpage>1775</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhn207</pub-id><pub-id pub-id-type="pmid">19068491</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Connor</surname><given-names>CE</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Integration of objects and space in perception and memory</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1493</fpage><lpage>1503</lpage><pub-id pub-id-type="doi">10.1038/nn.4657</pub-id><pub-id pub-id-type="pmid">29073645</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Corballis</surname><given-names>MC</given-names></name><name><surname>Nagourney</surname><given-names>BA</given-names></name><name><surname>Shetzer</surname><given-names>LI</given-names></name><name><surname>Stefanatos</surname><given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Mental rotation under head tilt: Factors influencing the location of the subjective reference frame</article-title><source>Perception &amp; Psychophysics</source><volume>24</volume><fpage>263</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.3758/BF03206098</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Efficient computation and cue integration with noisy population codes</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>826</fpage><lpage>831</lpage><pub-id pub-id-type="doi">10.1038/90541</pub-id><pub-id pub-id-type="pmid">11477429</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Emonds</surname><given-names>AMX</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Objectgravity</data-title><version designator="swh:1:rev:716893ee620743d40ae8e6d829f809349204d963">swh:1:rev:716893ee620743d40ae8e6d829f809349204d963</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:d482b69bea212731388b2c0c83a9d8e864e23685;origin=https://github.com/amxemonds/ObjectGravity;visit=swh:1:snp:0157e1a5292a034cbee8a72103debc3b3a941c1f;anchor=swh:1:rev:716893ee620743d40ae8e6d829f809349204d963">https://archive.softwareheritage.org/swh:1:dir:d482b69bea212731388b2c0c83a9d8e864e23685;origin=https://github.com/amxemonds/ObjectGravity;visit=swh:1:snp:0157e1a5292a034cbee8a72103debc3b3a941c1f;anchor=swh:1:rev:716893ee620743d40ae8e6d829f809349204d963</ext-link></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>R</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A cortical representation of the local visual environment</article-title><source>Nature</source><volume>392</volume><fpage>598</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.1038/33402</pub-id><pub-id pub-id-type="pmid">9560155</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Epstein</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Parahippocampal and retrosplenial contributions to human spatial navigation</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>388</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.07.004</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Felleman</surname><given-names>DJ</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Distributed hierarchical processing in the primate cerebral cortex</article-title><source>Cerebral Cortex</source><volume>1</volume><fpage>1</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1093/cercor/1.1.1-a</pub-id><pub-id pub-id-type="pmid">1822724</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischer</surname><given-names>J</given-names></name><name><surname>Mikhael</surname><given-names>JG</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Functional neuroanatomy of intuitive physical inference</article-title><source>PNAS</source><volume>113</volume><fpage>E5072</fpage><lpage>E5081</lpage><pub-id pub-id-type="doi">10.1073/pnas.1610344113</pub-id><pub-id pub-id-type="pmid">27503892</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Cant</surname><given-names>JS</given-names></name><name><surname>Goodale</surname><given-names>MA</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Representation of object weight in human ventral visual cortex</article-title><source>Current Biology</source><volume>24</volume><fpage>1866</fpage><lpage>1873</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.06.046</pub-id><pub-id pub-id-type="pmid">25065755</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallivan</surname><given-names>JP</given-names></name><name><surname>Johnsrude</surname><given-names>IS</given-names></name><name><surname>Flanagan</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Planning ahead: object-directed sequential actions decoded from human frontoparietal and occipitotemporal networks</article-title><source>Cerebral Cortex</source><volume>26</volume><fpage>708</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhu302</pub-id><pub-id pub-id-type="pmid">25576538</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Halberstadt</surname><given-names>AG</given-names></name><name><surname>Saitta</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Gender, nonverbal behavior, and perceived dominance: A test of the theory</article-title><source>Journal of Personality and Social Psychology</source><volume>53</volume><fpage>257</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1037/0022-3514.53.2.257</pub-id><pub-id pub-id-type="pmid">3559897</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hung</surname><given-names>CC</given-names></name><name><surname>Carlson</surname><given-names>ET</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Medial axis shape coding in macaque inferotemporal cortex</article-title><source>Neuron</source><volume>74</volume><fpage>1099</fpage><lpage>1113</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.029</pub-id><pub-id pub-id-type="pmid">22726839</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kornblith</surname><given-names>S</given-names></name><name><surname>Cheng</surname><given-names>X</given-names></name><name><surname>Ohayon</surname><given-names>S</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A network for scene processing in the macaque temporal lobe</article-title><source>Neuron</source><volume>79</volume><fpage>766</fpage><lpage>781</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.06.015</pub-id><pub-id pub-id-type="pmid">23891401</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krumhuber</surname><given-names>E</given-names></name><name><surname>Manstead</surname><given-names>ASR</given-names></name><name><surname>Kappas</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal aspects of facial displays in person and expression perception: the effects of smile dynamics, head-tilt, and gender</article-title><source>Journal of Nonverbal Behavior</source><volume>31</volume><fpage>39</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1007/s10919-006-0019-x</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lescroart</surname><given-names>MD</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human scene-selective areas represent 3D configurations of surfaces</article-title><source>Neuron</source><volume>101</volume><fpage>178</fpage><lpage>192</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.11.004</pub-id><pub-id pub-id-type="pmid">30497771</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mara</surname><given-names>M</given-names></name><name><surname>Appel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Effects of lateral head tilt on user perceptions of humanoid and android robots</article-title><source>Computers in Human Behavior</source><volume>44</volume><fpage>326</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1016/j.chb.2014.09.025</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGuire</surname><given-names>LMM</given-names></name><name><surname>Sabes</surname><given-names>PN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Heterogeneous representations in the superior parietal lobule are common across reaches to visual and proprioceptive targets</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>6661</fpage><lpage>6673</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2921-10.2011</pub-id><pub-id pub-id-type="pmid">21543595</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mignault</surname><given-names>A</given-names></name><name><surname>Chaudhuri</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The many faces of a neutral face: Head tilt and perception of dominance and emotion</article-title><source>Journal of Nonverbal Behavior</source><volume>27</volume><fpage>111</fpage><lpage>132</lpage><pub-id pub-id-type="doi">10.1023/A:1023914509763</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EF</given-names></name></person-group><year iso-8601-date="1962">1962</year><article-title>Counterrolling of the human eyes produced by head tilt with respect to gravity</article-title><source>Acta Oto-Laryngologica</source><volume>54</volume><fpage>479</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.3109/00016486209126967</pub-id><pub-id pub-id-type="pmid">14473991</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Armstrong</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Selective gating of visual signals by microstimulation of frontal cortex</article-title><source>Nature</source><volume>421</volume><fpage>370</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1038/nature01341</pub-id><pub-id pub-id-type="pmid">12540901</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Armstrong</surname><given-names>KM</given-names></name><name><surname>Fallah</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Visuomotor origins of covert spatial attention</article-title><source>Neuron</source><volume>40</volume><fpage>671</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00716-5</pub-id><pub-id pub-id-type="pmid">14622573</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullette-Gillman</surname><given-names>OA</given-names></name><name><surname>Cohen</surname><given-names>YE</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Eye-centered, head-centered, and complex coding of visual and auditory targets in the intraparietal sulcus</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>2331</fpage><lpage>2352</lpage><pub-id pub-id-type="doi">10.1152/jn.00021.2005</pub-id><pub-id pub-id-type="pmid">15843485</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Noudoost</surname><given-names>B</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Control of visual cortical signals by prefrontal dopamine</article-title><source>Nature</source><volume>474</volume><fpage>372</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1038/nature09995</pub-id><pub-id pub-id-type="pmid">21572439</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Responses to contour features in macaque area V4</article-title><source>Journal of Neurophysiology</source><volume>82</volume><fpage>2490</fpage><lpage>2502</lpage><pub-id pub-id-type="doi">10.1152/jn.1999.82.5.2490</pub-id><pub-id pub-id-type="pmid">10561421</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Shape representation in area V4: position-specific tuning for boundary conformation</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>2505</fpage><lpage>2519</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.5.2505</pub-id><pub-id pub-id-type="pmid">11698538</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pasupathy</surname><given-names>A</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Population coding of shape in area V4</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>1332</fpage><lpage>1338</lpage><pub-id pub-id-type="doi">10.1038/nn972</pub-id><pub-id pub-id-type="pmid">12426571</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Deneve</surname><given-names>S</given-names></name><name><surname>Duhamel</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A computational perspective on the neural basis of multisensory spatial representations</article-title><source>Nature Reviews. Neuroscience</source><volume>3</volume><fpage>741</fpage><lpage>747</lpage><pub-id pub-id-type="doi">10.1038/nrn914</pub-id><pub-id pub-id-type="pmid">12209122</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pramod</surname><given-names>RT</given-names></name><name><surname>Cohen</surname><given-names>MA</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Invariant representation of physical stability in the human brain</article-title><source>eLife</source><volume>11</volume><elocation-id>e71736</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.71736</pub-id><pub-id pub-id-type="pmid">35635277</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenberg</surname><given-names>A</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Gravity influences the visual representation of object tilt in parietal cortex</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>14170</fpage><lpage>14180</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2030-14.2014</pub-id><pub-id pub-id-type="pmid">25339732</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schafer</surname><given-names>RJ</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Selective attention from voluntary control of neurons in prefrontal cortex</article-title><source>Science</source><volume>332</volume><fpage>1568</fpage><lpage>1571</lpage><pub-id pub-id-type="doi">10.1126/science.1199892</pub-id><pub-id pub-id-type="pmid">21617042</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schworm</surname><given-names>HD</given-names></name><name><surname>Ygge</surname><given-names>J</given-names></name><name><surname>Pansell</surname><given-names>T</given-names></name><name><surname>Lennerstrand</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Assessment of ocular counterroll during head tilt using binocular video oculography</article-title><source>Investigative Ophthalmology &amp; Visual Science</source><volume>43</volume><fpage>662</fpage><lpage>667</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>1916</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.4.1916</pub-id><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>RN</given-names></name><name><surname>Metzler</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Mental rotation of three-dimensional objects</article-title><source>Science</source><volume>171</volume><fpage>701</fpage><lpage>703</lpage><pub-id pub-id-type="doi">10.1126/science.171.3972.701</pub-id><pub-id pub-id-type="pmid">5540314</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srinath</surname><given-names>R</given-names></name><name><surname>Emonds</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>Q</given-names></name><name><surname>Lempel</surname><given-names>AA</given-names></name><name><surname>Dunn-Weiss</surname><given-names>E</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name><name><surname>Nielsen</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Early emergence of solid shape coding in natural and deep network vision</article-title><source>Current Biology</source><volume>31</volume><fpage>51</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2020.09.076</pub-id><pub-id pub-id-type="pmid">33096039</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stricanne</surname><given-names>B</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Mazzoni</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Eye-centered, head-centered, and intermediate coding of remembered sound locations in area LIP</article-title><source>Journal of Neurophysiology</source><volume>76</volume><fpage>2071</fpage><lpage>2076</lpage><pub-id pub-id-type="doi">10.1152/jn.1996.76.3.2071</pub-id><pub-id pub-id-type="pmid">8890315</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolias</surname><given-names>AS</given-names></name><name><surname>Moore</surname><given-names>T</given-names></name><name><surname>Smirnakis</surname><given-names>SM</given-names></name><name><surname>Tehovnik</surname><given-names>EJ</given-names></name><name><surname>Siapas</surname><given-names>AG</given-names></name><name><surname>Schiller</surname><given-names>PH</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Eye movements modulate visual receptive fields of V4 neurons</article-title><source>Neuron</source><volume>29</volume><fpage>757</fpage><lpage>767</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00250-1</pub-id><pub-id pub-id-type="pmid">11301034</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaziri</surname><given-names>S</given-names></name><name><surname>Carlson</surname><given-names>ET</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A channel for 3D environmental shape in anterior inferotemporal cortex</article-title><source>Neuron</source><volume>84</volume><fpage>55</fpage><lpage>62</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.043</pub-id><pub-id pub-id-type="pmid">25242216</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaziri</surname><given-names>S</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Representation of gravity-aligned scene structure in ventral pathway visual cortex</article-title><source>Current Biology</source><volume>26</volume><fpage>766</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.01.022</pub-id><pub-id pub-id-type="pmid">26923785</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wexler</surname><given-names>M</given-names></name><name><surname>Kosslyn</surname><given-names>SM</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Motor processes in mental rotation</article-title><source>Cognition</source><volume>68</volume><fpage>77</fpage><lpage>94</lpage><pub-id pub-id-type="doi">10.1016/s0010-0277(98)00032-8</pub-id><pub-id pub-id-type="pmid">9775517</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamane</surname><given-names>Y</given-names></name><name><surname>Carlson</surname><given-names>ET</given-names></name><name><surname>Bowman</surname><given-names>KC</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Connor</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A neural code for three-dimensional object shape in macaque inferotemporal cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1352</fpage><lpage>1360</lpage><pub-id pub-id-type="doi">10.1038/nn.2202</pub-id><pub-id pub-id-type="pmid">18836443</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zikovitz</surname><given-names>DC</given-names></name><name><surname>Harris</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Head tilt during driving</article-title><source>Ergonomics</source><volume>42</volume><fpage>740</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1080/001401399185414</pub-id><pub-id pub-id-type="pmid">10722313</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81701.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Arun</surname><given-names>SP</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><country>India</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.08.06.503060" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.06.503060"/></front-stub><body><p>In this study, the authors investigate whether neurons in the inferior temporal (IT) cortex encode features relative to the absolute gravitational vertical, by recording responses to objects in varying orientations while monkeys viewed them sitting in physically rotated chairs. They find surprising and compelling evidence that neural tuning is unaffected by physical whole-body tilt, which cannot be explained by any compensatory torsional rotations of the eyes. These findings are of fundamental importance because they indicate that IT neurons may play a role not only in object recognition but more broadly in physical scene understanding.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81701.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Arun</surname><given-names>SP</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04dese585</institution-id><institution>Indian Institute of Science Bangalore</institution></institution-wrap><country>India</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.08.06.503060">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.08.06.503060v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Object representation in a gravitational reference frame&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) The high tuning correlation between the whole-body tilt conditions could also occur if IT neurons encoded the angle between the horizon and the object in the object-with-horizon experiment, and/or the angle between the object and the frame of the computer monitor which may potentially be visible in the object-alone conditions. The authors will need to think carefully about how to address this confound, or acknowledge clearly that this is an alternate explanation for their findings, which would also dilute the overall novelty of the results. One possibility could be to perform identical analyses on pre-trained deep neural networks. Another could be to quantify the luminance of the monitor, and maybe also how brightly lit other objects are by the monitor in their setup. Finally, object-orientation tuning could be compared in the object-alone and object-in-scene conditions.</p><p>2) The authors should provide more details about the torsional eye movements they have measured in each animal. For instance, have the authors measured torsional eye rotations on every trial? Is it fixed always at {plus minus}6{degree sign} or does it change from trial to trial? If it changes, then could the high tuning correlation between the whole-body rotations be simply driven by trials in which the eyes compensated more?</p><p>3) A lot of details are dense in the manuscript. The authors should clearly present their control analyses and also the correlation analyses reported in the main figures. Please refer to the specific comments in the individual reviews for details.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>In addition to the comments in the public review, It would also be good if the authors can quantify the overall tendency of the population in some way, rather than reporting the proportions of neurons that show a high correlation in the two reference frames. For instance, is the average tuning correlation in the absolute gravitational reference frame stronger than the average tuning correlation for the retinal reference frame? Are the proportions of neurons encoding the two reference frames different in the two experiments?</p><p>Specific comments:</p><p>In Figure 1a, the object orientations given are -50{degree sign}, 0{degree sign}, and +50{degree sign}, but in figures 1c and 1d, we can see that orientations go up to 100{degree sign}, in both directions. For these bigger rotations, do the objects penetrate the ground surface? Can the authors show more object orientations?</p><p>Figure 1a: please rearrange the columns to show the object rotating consistently in one direction (CW or CCW). For instance, swap the leftmost and rightmost columns in the stimuli.</p><p>Figure 1e, f – Can the authors quantify the shift from 1c and 1d explicitly? In line 102, it says the shift is about 20o. Is there any variability in the magnitude of shift across trials/neurons etc? If so, can the authors explain it clearly?</p><p>Figure 1,3: The Cyan and pink triangles are not explained clearly at all. The authors should elaborate on this in the Results and in the figure legends.</p><p>Figure 1e, f, i, j – We understand that x-axis values are estimated from monkey tilt and torsional rotation. Can authors show some details on torsional rotation, as in is this observed for every trial? Is there trial-to-trial variability here? Are there any trials, for which there is complete compensation by ocular counter-rolling? Though it is mentioned in the supplementary section (line 548), it is not very clear, what is meant by the comment &quot;For all the data from both monkeys&quot;.</p><p>Figure 2c, d – I suggest the authors move panels c and d to supplementary material, as it is not central to the arguments. Can the authors explain the matched analysis in detail on how it was done?</p><p>Line 135 – It says a sample of 99 neurons, but in lines 136-138 while giving the % of each set of neurons, the denominator is 53. Please clarify.</p><p>Figure 3: Since there are two neurons shown in this figure, label them as Cell 1 and Cell 2 in the figure itself. Also, it would be better to explicitly mention, which one of the figures 3c or 3e, has the x-axis inferred.</p><p>Line 476: Materials and methods: Provide the details of recording sites – left/right hemisphere, probe, and data acquisition process.</p><p>Methods: Can the authors show one full set of example stimuli indicating all object orientations used in each experiment?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– The data is presented in a very compact form right now. For both Figure 1 and Figure 3, I would have found helpful a figure showing the responses of a cell to the 5 repeated presentations and showing 'each of the stimuli' (and monkey physical orientation) presented for each condition in the gravitational and retinal reference frame comparisons.</p><p>And to show such a plot (possibly in Supplementary data) for more example cells. A huge amount of work went into collecting this data, and I think it would really help to bring readers closer to the raw data through more examples and a complete and hand-holding presentation format (even if takes up more pdf pages).</p><p>– For plots of single-cell tuning curves, error bars indicating SEM would be helpful.</p><p>– The result of the decoding analysis, that one can build decoders for both the gravitational reference frame and the retinal reference frame same-different task, is interesting. To what extent does this depend on specialized mechanisms? If one were to attempt the same decoding using a deep neural network trained on classification by presenting the same images presented to the monkey in the experiment, could one achieve similar decoding for the gravitational frame same/different task? Or would it completely fail?</p><p>– Additional discussion of the relation of current findings to known functional architecture of IT would be helpful. For example, the recordings were from AP5 to AP25. Were any differences observed across this span? Were cells recorded in object or scene regions of IT (cf. Vaziri and Connor)?</p><p>– Also, how do results relate to the notion of IT cells generating an invariant representation? If IT cells were completely rotation invariant, then all the points should cluster in the top right in their scatter plots, and that is clearly not the case. Is the suggestion then that in general IT cells are less invariant to rotations than to translations, scalings, etc., and furthermore that this selectivity for rotation angle is represented in a mixed reference frame, enabling robust decoding of identity and orientation in retinal and gravitational coordinates? A more explicit statement on the relation of the findings to the idea of IT encoding a hierarchy of increasingly invariant representations would be helpful.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. The authors employ a correlation analysis to examine quantitatively the effect of tilt on orientation tuning. However, it is not clear to me how well the correlation analysis can distinguish the two reference frames (retinal versus gravitational). For instance, for the data in Figure 1, I expect that the retinal reference frame also would have provided a positive correlation although the orientation tuning shifted as predicted in retinal coordinates. Furthermore, a lack of correlation can reflect an absence of orientation tuning. Therefore, I suggest that the authors select first those neurons that show a significant orientation tuning for at least one of the two tilts. For those neurons, they can determine for each tilt the preferred orientation and examine the difference in preferred orientation between the two tilts. Each of the two reference frames provides clear predictions about the expected (absence of) difference between the preferred orientations for the two tilts. Using such an analysis they can also determine whether neurons tested with and without a scene background show effects of tilt on orientation preference that are consistent across the scene manipulation (i.e. without and with scene background). Then the scene data would be useful.</p><p>2. I have two issues with the population decoding analysis. First, the authors should provide a better description of the implementation of the decoding analysis. It was unclear to me how the match-nonmatch task was implemented. Second, they should perform this analysis for the object without scene background data, since as I discussed above, the scene data are difficult to interpret.</p><p>3. The authors pooled the data of the two monkeys. They should provide also individual monkey data so that the reader knows how consistent the effects are for the two animals.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.81701.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) The high tuning correlation between the whole-body tilt conditions could also occur if IT neurons encoded the angle between the horizon and the object in the object-with-horizon experiment, and/or the angle between the object and the frame of the computer monitor which may potentially be visible in the object-alone conditions. The authors will need to think carefully about how to address this confound, or acknowledge clearly that this is an alternate explanation for their findings, which would also dilute the overall novelty of the results. One possibility could be to perform identical analyses on pre-trained deep neural networks. Another could be to quantify the luminance of the monitor, and maybe also how brightly lit other objects are by the monitor in their setup. Finally, object-orientation tuning could be compared in the object-alone and object-in-scene conditions.</p></disp-quote><p>We agree that a shape-configuration (i.e. overlapping orientation) interaction between horizon and object was possible, as opposed to the horizon serving purely as a gravitational cue. That is why we tested neurons in the isolated object condition. We now make that concern and the control importance of the isolated object condition explicit in the text discussion of Figure 1 (where we also eliminate the claim that the room was otherwise dark): The Figure 1 example neuron was tested with both full scene stimuli (Figure 1a), which included a textured ground surface and horizon, providing visual cues for the orientation of gravity, and isolated objects (Figure 1b), presented on a gray background, so that primarily vestibular and somatosensory cues indicated the orientation of gravity. The contrast between the two conditions helps to elucidate the additional effects of visual cues on top of vestibular/somatosensory cues. In addition, the isolated object condition controls for the possibility that tuning is affected by a shape-configuration (i.e. overlapping orientation) interaction between the object and the horizon or by differential occlusion of the object fragment buried in the ground (which was done to make the scene condition physically realistic for the wide variety of object orientations that would otherwise appear improbably balanced on a hard ground surface).</p><p>This control condition, in which the main results in Figure 2b were replicated, addresses the reasonable concern about the horizon/object shape configuration interaction. In addition, we recognize that remaining visual cues for gravity in the room, including the screen edges, could still contribute to tuning in gravitational coordinates: Similar results were obtained for a partially overlapping sample of 99 IT neurons tested with isolated object stimuli with no background (i.e. no horizon or ground plane) (Figure 2b). In this case, 60% of neurons (32/53) showed significant correlation in the gravitational reference frame, 26% (14/53) significant correlation in the retinal reference frame, and within these groups 13% (7/53) were significant in both reference frames. The population tendency toward positive correlation was again significant in this experiment along both gravitational (p = 3.63 X 10<sup>–22</sup>) and retinal axes (p = 1.63 X 10<sup>–7</sup>). This suggests that gravitational tuning can depend primarily on vestibular/somatosensory cues for self-orientation. However, we cannot rule out a contribution of visual cues for gravity in the visual periphery, including screen edges and other horizontal and vertical edges and planes, which in the real world are almost uniformly aligned with gravity and thus strong cues for its orientation (but see Figure 2–supplement figure 1). Nonetheless, the Figure 2b result confirms that gravitational tuning did not depend on the horizon or ground surface in the background condition.</p><p>Figure 2–supplement figure 1shows that the results were comparable for a subset of cells studied with a circular aperture surrounding the floating object, with gray background in the circular aperture and black screen outside it. Under this condition, the circular aperture edge, which conveys no information about the direction of gravity and would maintain a constant relationship to the object regardless of object-tilt, would be more high-contrast, salient, and closer to the object than the screen edges.</p><p>Finally, we show the reviewer-suggested cell-by-cell comparisons of scene and isolated stimuli, for those cells tested with both, in Figure 2–supplement figure 6. This figure shows 8 neurons with significant gravitational tuning only in the floating object condition, 11 neurons with tuning only in the gravitational condition, and 23 neurons with significant tuning in both. Thus, a majority of significantly tuned neurons were tuned in both conditions. A two-tailed paired t-test across all 79 neurons tested in this way showed that there was no significant tendency toward stronger tuning in the scene condition. The 11 neurons with tuning only in the gravitational condition by themselves might suggest a critical role for visual cues in some neurons. However, the converse result for 8 cells, with tuning only in the floating condition, suggests a more complex dependence on cues or a conflicting effect of interaction with the background scene for a minority of cells.</p><p>Main text: “This is further confirmed through cell-by-bell comparison between scene and isolated for those cells tested with both (Figure 2–supplement figure 6).”</p><p>We do not think the further suggestion of orientation interactions between object and screen edges in the isolated object condition needs mentioning in the paper itself, given that the closest screen edges on our large display were 28° in the periphery, and there is no reason to suspect that IT encodes orientation relationships between distant, disconnected visual elements. Screen edges have been present in most studies of IT, and no such interactions have been reported. However, we will also discuss this point in online responses.</p><disp-quote content-type="editor-comment"><p>2) The authors should provide more details about the torsional eye movements they have measured in each animal. For instance, have the authors measured torsional eye rotations on every trial? Is it fixed always at {plus minus}6{degree sign} or does it change from trial to trial? If it changes, then could the high tuning correlation between the whole-body rotations be simply driven by trials in which the eyes compensated more?</p></disp-quote><p>We now clarify that we could only measure ocular rotation outside the experiment with high-resolution closeup color photography. Our measurements were consistent with previous reports showing that counterroll is limited to 20% of tilt. Moreover, they are consistent with our analyses showing that maximum correlation with retinal coordinates is obtained with a 6° correction for counterroll, indicating equivalent counterroll during experiments. Counterroll would need to be five times greater than previous observations to completely compensate for tilt and mimic the gravitational tuning we observed. For these reasons, counterroll is not a reasonable explanation for our results:</p><p>“Compensatory ocular counter-rolling was measured to be 6° based on iris landmarks visible in high-resolution photographs, consistent with previous measurements in humans<sup>6,7</sup>, and larger than previous measurements in monkeys<sup>41</sup>, making it unlikely that we failed to adequately account for the effects of counterroll. Eye rotation would need to be five times greater than previously observed to mimic gravitational tuning. Our rotation measurements required detailed color photographs that could only be obtained with full lighting and closeup photography. This was not possible within the experiments themselves, where only low-resolution monochromatic infrared images were available. Importantly, our analytical compensation for counter-rotation did not depend on our measurement of ocular rotation. Instead, we tested our data for correlation in retinal coordinates across a wide range of rotational compensation values. The fact that maximum correspondence was observed at a compensation value of 6° (Figure 1­–supplement figure 1) indicates that counterrotation during the experiments was consistent with our measurements outside the experiments.”</p><disp-quote content-type="editor-comment"><p>3) A lot of details are dense in the manuscript. The authors should clearly present their control analyses and also the correlation analyses reported in the main figures. Please refer to the specific comments in the individual reviews for details.</p></disp-quote><p>See below.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>In addition to the comments in the public review, It would also be good if the authors can quantify the overall tendency of the population in some way, rather than reporting the proportions of neurons that show a high correlation in the two reference frames. For instance, is the average tuning correlation in the absolute gravitational reference frame stronger than the average tuning correlation for the retinal reference frame? Are the proportions of neurons encoding the two reference frames different in the two experiments?</p></disp-quote><p>We scatterplot the complete distributions of joint tuning values in Figure 2 (with marginals for the two tuning dimensions), which is the most direct way to convey the entire underlying datasets. We report overall tendencies in terms of the significance of the distance of the mean or center-of-mass from 0 in the positive direction. This conveys the strength of tuning tendencies conditioned by variability in the data. We now point out the comparative strength of the p values:</p><p>“Of the 52 neurons with consistent object orientation tuning in one or both reference frames, 63% (33/52) were aligned with gravity, 21% (11/52) were aligned with the retinae, and 15% (8/52) were aligned with both. The population tendency toward positive correlation was strongly significant along the gravitational axis (two-tailed randomization t-test for center-of-mass relative to 0; p = 6.49 X 10<sup>–29</sup>) and also significant though less so along the retinal axis (p = 5.76 X 10<sup>–10</sup>).”</p><disp-quote content-type="editor-comment"><p>Specific comments:</p><p>In Figure 1a, the object orientations given are -50{degree sign}, 0{degree sign}, and +50{degree sign}, but in figures 1c and 1d, we can see that orientations go up to 100{degree sign}, in both directions. For these bigger rotations, do the objects penetrate the ground surface? Can the authors show more object orientations?</p></disp-quote><p>We now explain in the Figure 1 caption that at each orientation 15% of the virtual object mass was planted in the ground to provide a physically realistic presentation of an orientation that would be unbalanced if it merely rested on the ground. Additional examples of how object orientation interacted with the ground are shown in Figure 3 and Figure 1—figure supplements 3–10.</p><p>“At each object orientation, the object was virtually placed on the ground-like surface naturalistically by immersing or “planting” 15% of its mass below ground, providing physical realism for orientations that would otherwise be visibly unbalanced, and ensuring that most of the object was visible at each orientation. The high-response object shape and orientation discovered in the genetic algorithm experiments was always at the center of the tested range and labeled 0°.”</p><disp-quote content-type="editor-comment"><p>Figure 1a: please rearrange the columns to show the object rotating consistently in one direction (CW or CCW). For instance, swap the leftmost and rightmost columns in the stimuli.</p></disp-quote><p>We are not sure what is desired here. All of the subplots in Figure 1 effectively rotate counterclockwise going from left to right as they are now. This makes sense so that the rotation scale in the response plots can progress from negative numbers to positive numbers going left to right, as is the convention, given the additional convention that counterclockwise rotations are usually considered positive. Maybe there is a confusion about the fact that “0” is the orientation found by the genetic algorithm, and the stimuli were rotated in both directions away from this roughly optimum orientation; this should be cleared up by the new text in the Figure 1 legend.</p><disp-quote content-type="editor-comment"><p>Figure 1e, f – Can the authors quantify the shift from 1c and 1d explicitly? In line 102, it says the shift is about 20o. Is there any variability in the magnitude of shift across trials/neurons etc? If so, can the authors explain it clearly?</p></disp-quote><p>We have changed this to:</p><p>“the peaks are shifted right or left by 19° each, i.e. 25° minus the 6° compensation for ocular counter-rotation.”</p><disp-quote content-type="editor-comment"><p>Figure 1,3: The Cyan and pink triangles are not explained clearly at all. The authors should elaborate on this in the Results and in the figure legends.</p></disp-quote><p>We have changed the main text to clarify this:</p><p>“When the same data are plotted in the retinal reference frame (Figure 1e and f), the peak near 0° shifts right or left by 19° (25° tilt minus 6° counterrotation of the eyes). This reflects the transformation of retinal information into a new reference frame. Because the eyes were rotated in different directions under the two tilt directions, the overlap of tested orientations in retinal coordinates is limited to seven screen orientations. In addition, to account for ocular counterrotation, the tested orientation values (<italic>black dots</italic>) in the two curves must be shifted 6° in the positive direction for the –25° tilt and 6° negative for the +25° tilt. Thus, the appropriate comparison points between Figure 1e and f, indicated by the <italic>cyan triangles,</italic> must be interpolated from the Catmull-Rom spline curves used to connect the tested orientations (<italic>black dots</italic>). A comparable set of seven comparison points in the gravitational reference frame (Figure 1c and d, <italic>pink triangles</italic>) falls directly on the tested orientations.”</p><disp-quote content-type="editor-comment"><p>Figure 1e, f, i, j – We understand that x-axis values are estimated from monkey tilt and torsional rotation. Can authors show some details on torsional rotation, as in is this observed for every trial? Is there trial-to-trial variability here? Are there any trials, for which there is complete compensation by ocular counter-rolling? Though it is mentioned in the supplementary section (line 548), it is not very clear, what is meant by the comment &quot;For all the data from both monkeys&quot;.</p></disp-quote><p>We expanded and clarified the description of the analysis of compensation of ocular rotation:</p><p>“For the tilt experiments on all the neurons, combined across monkeys, we searched for the counterroll compensation that would produce the strongest agreement in retinal coordinates. At each compensation level tested, we normalized and summed the mean squared error (MSE) between responses at corresponding retinal positions. The best agreement in retinal coordinates (minimum MSE) was measured at 12° offset, corresponding to 6° rotation from normal in each of the tilt conditions (<italic>lower left</italic>).”</p><p>As mentioned above, we now clarify as much as possible the methods and limitations of our eye rotation measurements, and we emphasize that our method for compensation did not depend on these measurements but was instead optimized for retinal correlation:</p><p>“Compensatory ocular counter-rolling was measured to be 6° based on iris landmarks visible in high-resolution photographs, consistent with previous measurements in humans<sup>6,7</sup>, and larger than previous measurements in monkeys<sup>41</sup>, making it unlikely that we failed to adequately account for the effects of counterroll. Eye rotation would need to be five times greater than previously observed to mimic gravitational tuning. Our rotation measurements required detailed color photographs that could only be obtained with full lighting and closeup photography. This was not possible within the experiments themselves, where only low-resolution monochromatic infrared images were available. Importantly, our analytical compensation for counter-rotation did not depend on our measurement of ocular rotation. Instead, we tested our data for correlation in retinal coordinates across a wide range of rotational compensation values. The fact that maximum correspondence was observed at a compensation value of 6° (Figure 1—figure supplement 1) indicates that counterrotation during the experiments was consistent with our measurements outside the experiments.”</p><disp-quote content-type="editor-comment"><p>Figure 2c, d – I suggest the authors move panels c and d to supplementary material, as it is not central to the arguments. Can the authors explain the matched analysis in detail on how it was done?</p></disp-quote><p>Figure 2c and 2d are important because the larger number of matching positions in the gravitational comparison may bias the results toward gravitational correlation. This is explained in main text:</p><p>“The analyses above were based on the full set of orientation comparisons possible for the gravitational reference frame (7), while the experimental design inevitably produced fewer comparisons for the retinal reference frame (5). Rerunning the analyses based on just 5 comparable object orientations in both reference frames (Figure 1, <italic>pink</italic> and <italic>cyan triangles</italic>) produced the results shown in Figures 2c and d. For full scene stimuli, this yielded 56% (23/41) significant gravitational alignment, 27% (11/41) retinal alignment, and 17% (7/41) dual alignment (Figure 2c). For isolated object stimuli, this reanalysis yielded 58% (28/48) gravitational alignment, 29% (14/48) retinal alignment, and 13% (6/48) dual alignment (Figure 2d).”</p><disp-quote content-type="editor-comment"><p>Line 135 – It says a sample of 99 neurons, but in lines 136-138 while giving the % of each set of neurons, the denominator is 53. Please clarify.</p></disp-quote><p>As in the description of the scene condition results, percentages are given for neurons with one or both significant results; now clarified:</p><p>“In this case, 60% of the 53 neurons with significant object orientation tuning in one or both reference frames (32/53)”</p><disp-quote content-type="editor-comment"><p>Figure 3: Since there are two neurons shown in this figure, label them as Cell 1 and Cell 2 in the figure itself. Also, it would be better to explicitly mention, which one of the figures 3c or 3e, has the x-axis inferred.</p></disp-quote><p>This is now clarified in the figure legend:</p><p>(a,b) Stimuli used to study two different neurons, demonstrating example object orientations in two conditions, with the ground surface, horizon, and sky gradient tilted –25° (clockwise, top row), and with ground surface, etc. tilted +25° (counterclockwise, second row). The monkey was in a normal upright orientation during these experiments, producing conflicting vestibular/somatosensory cues. The retinal orientation discovered in the genetic algorithm experiments is arbitrarily labeled 0°. (c,d) For one of the example IT neurons, tested with the stimuli in (a), object orientation tuning with respect to the visually cued direction of gravity was consistent across the two ground tilts. (e,f) Correspondingly, the neuron gave very different responses to retinal object orientation values between the two ground tilts. (g,h) This different example IT neuron, tested with the stimuli in (b), did not exhibit consistent object orientation tuning in visually-cued gravitational space. (i,j) Instead, this neuron maintained consistent tuning for retinal-screen orientation despite changes in ground tilt.</p><disp-quote content-type="editor-comment"><p>Line 476: Materials and methods: Provide the details of recording sites – left/right hemisphere, probe, and data acquisition process.</p></disp-quote><p>The electrical activity of well-isolated single neurons was recorded with epoxy-coated tungsten electrodes (FHC Microsystems). Action potentials of individual neurons were amplified and electrically isolated using a Tucker-Davis Technologies recording system. Recording positions ranged from 5 to 25 mm anterior to stereotaxic 0 within the left inferior temporal lobe, including the ventral bank of the superior temporal sulcus, lateral convexity, and basal surface. Positions were determined on the basis of structural magnetic resonance images and the sequence of sulci and response characteristics observed while lowering the electrode.</p><p>In addition, locations of individual neurons and distribution between subdivisions of IT are now described in Figure 2—figure supplements 4,5.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>– The data is presented in a very compact form right now. For both Figure 1 and Figure 3, I would have found helpful a figure showing the responses of a cell to the 5 repeated presentations and showing 'each of the stimuli' (and monkey physical orientation) presented for each condition in the gravitational and retinal reference frame comparisons.</p><p>And to show such a plot (possibly in Supplementary data) for more example cells. A huge amount of work went into collecting this data, and I think it would really help to bring readers closer to the raw data through more examples and a complete and hand-holding presentation format (even if takes up more pdf pages).</p><p>– For plots of single-cell tuning curves, error bars indicating SEM would be helpful.</p></disp-quote><p>We have added expanded data presentations in Figure 1—figure supplements 3–10, reproduced above, to show each of the stimuli for the two examples (one gravitationally tuned and one retinally tuned) in that figure. In these plots, the individual temporally smoothed tuning curves for each of 5 repetitions are shown to indicate variability of responses directly. Temporal smoothing is critical because low number of stimulus repetitions (5) is balanced by close sampling of stimulus orientation in our experimental design.</p><disp-quote content-type="editor-comment"><p>– The result of the decoding analysis, that one can build decoders for both the gravitational reference frame and the retinal reference frame same-different task, is interesting. To what extent does this depend on specialized mechanisms? If one were to attempt the same decoding using a deep neural network trained on classification by presenting the same images presented to the monkey in the experiment, could one achieve similar decoding for the gravitational frame same/different task? Or would it completely fail?</p></disp-quote><p>We should have explained in the main text that our match/nonmatch decoding model was a simple linear discriminant analysis implemented with the Matlab function fitcdiscr. Given that linear discrimination worked with high accuracy, there was no point in exploring more complex, nonlinear classification schemes like deep networks, which could easily capture linear decoding mechanisms. This is now clarified:</p><p>This match/non-match task allowed us to analyze population information about orientation equivalence even though individual neurons were tested using different stimuli with no comparability between orientations. (Across neurons, orientations were aligned according to their order in the tested range, so that each non-match trial involved the same orientation difference, in the same direction, for each neuron.) Our decoding method was linear discriminant analysis of the neural population response patterns for each stimulus pair, implemented with Matlab function fitcdiscr.</p><p>The accuracy of these linear models for orientation match/non-match in the gravitational reference frame was 97% (10-fold cross-validation). The accuracy of models for orientation match/non-match in the retinal reference frame was 98%. (The accuracies for analyses based on the partially overlapping population of 99 neurons tested with isolated objects were 81% gravitational and 90% retinal.) The success of these simple linear models shows that information in both reference frames was decodable as weighted sums across the neural population. No complex, nonlinear, time-consuming neural processing would be required. This easy, linear decoding of information in both reference frames is consistent with psychophysical results showing that humans have voluntary access to either reference frame<sup>23</sup>. High accuracy was obtained even with models based solely on neurons that showed no significant correlation in either gravitational or retinal reference frames (Figure 2a, <italic>light gray</italic>): 89% for gravitational discrimination and 97% for retinal discrimination. This supports the idea that these neurons carry a mix of retinal and gravitational object orientation signals.</p><disp-quote content-type="editor-comment"><p>– Additional discussion of the relation of current findings to known functional architecture of IT would be helpful. For example, the recordings were from AP5 to AP25. Were any differences observed across this span? Were cells recorded in object or scene regions of IT (cf. Vaziri and Connor)?</p></disp-quote><p>We have added anatomical plots and a table to present these results in Figure 2—figure supplements 4,5.</p><disp-quote content-type="editor-comment"><p>– Also, how do results relate to the notion of IT cells generating an invariant representation? If IT cells were completely rotation invariant, then all the points should cluster in the top right in their scatter plots, and that is clearly not the case. Is the suggestion then that in general IT cells are less invariant to rotations than to translations, scalings, etc., and furthermore that this selectivity for rotation angle is represented in a mixed reference frame, enabling robust decoding of identity and orientation in retinal and gravitational coordinates? A more explicit statement on the relation of the findings to the idea of IT encoding a hierarchy of increasingly invariant representations would be helpful.</p></disp-quote><p>Terrific suggestion; this really is a point of confusion throughout the ventral pathway field. We have added a new second paragraph to the discussion:</p><p>“It is important to distinguish this result from the notion of increasing invariance, including rotational invariance, at higher levels in the ventral pathway. There is a degree of rotational invariance in IT, but even by the broadest definition of invariance (angular range across which responses to an optimal stimulus remain significantly greater than average responses to random stimuli) the average is ~90° for in-plane rotation and less for out-of-plane rotations.<sup>17</sup> It has often been suggested that the ventral pathway progressively discards information about spatial positions, orientations, and sizes as a way to standardize the neural representations of object identities. But, in fact, these critical dimensions for understanding the physical world of objects and environments are not discarded but rather transformed. In particular, spatial position information is transformed from retinal coordinates into relative spatial relationships between parts of contours, surfaces, object parts, and objects.<sup>18</sup> Our results here indicate a novel kind of transformation of orientation information in the ventral pathway, from the original reference frame of the eyes to the gravitational reference frame that defines physical interactions in the world. Because this is an allocentric reference frame, the representation of orientation with respect to gravity is invariant to changes in the observer system (especially lateral head tilt), making representation more stable and more relevant to external physical events. However, our results do not suggest a change in orientation tuning breadth, increased invariance to object rotation, or a loss of critical object orientation information.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. The authors employ a correlation analysis to examine quantitatively the effect of tilt on orientation tuning. However, it is not clear to me how well the correlation analysis can distinguish the two reference frames (retinal versus gravitational). For instance, for the data in Figure 1, I expect that the retinal reference frame also would have provided a positive correlation although the orientation tuning shifted as predicted in retinal coordinates. Furthermore, a lack of correlation can reflect an absence of orientation tuning. Therefore, I suggest that the authors select first those neurons that show a significant orientation tuning for at least one of the two tilts. For those neurons, they can determine for each tilt the preferred orientation and examine the difference in preferred orientation between the two tilts.</p></disp-quote><p>We understand the intent of this suggestion, and it is certainly desirable to have a measure that definitively differentiates between the two reference frames for each neuron. However, the response profiles for object orientations for IT neurons are not always unimodal. Worse, in most cases the breadth of tuning characteristic of IT neurons makes the orientation peak response negligibly different from a wide range of neighboring orientation responses. This can be seen in the examples in notes to Figure S2. As a result, using peak or preferred orientation would be hopelessly noisy and uninformative. The suggested analysis would be good for narrow V1 bar/grating orientation tuning but not for IT object orientation tuning. The best only way to measure similarity of orientation tuning is correlation across all the tested orientations, and that is why we use that as the measure of reference frame alignment throughout the paper.</p><disp-quote content-type="editor-comment"><p>Each of the two reference frames provides clear predictions about the expected (absence of) difference between the preferred orientations for the two tilts. Using such an analysis they can also determine whether neurons tested with and without a scene background show effects of tilt on orientation preference that are consistent across the scene manipulation (i.e. without and with scene background). Then the scene data would be useful.</p></disp-quote><p>We now make this comparison, using correlation for the reasons just explained, between the two experimental conditions in Figure 2—figure supplement 6.</p><disp-quote content-type="editor-comment"><p>2. I have two issues with the population decoding analysis. First, the authors should provide a better description of the implementation of the decoding analysis. It was unclear to me how the match-nonmatch task was implemented. Second, they should perform this analysis for the object without scene background data, since as I discussed above, the scene data are difficult to interpret.</p></disp-quote><p>We now specify exactly how this analysis was done and report the results for isolated object experiments:</p><p>“Our decoding method was linear discriminant analysis of the neural population response patterns for each stimulus pair, implemented with Matlab function fitcdiscr.”</p><p>The accuracy of these linear models for orientation match/non-match in the gravitational reference frame was 97% (10-fold cross-validation). The accuracy of models for orientation match/non-match in the retinal reference frame was 98%. (The accuracies for analyses based on the partially overlapping population of 99 neurons tested with isolated objects were 81% gravitational and 90% retinal.) The success of these simple linear models shows that information in both reference frames was easily decodable as weighted sums across the neural population. No complex, nonlinear, time-consuming neural processing would be required.</p><disp-quote content-type="editor-comment"><p>3. The authors pooled the data of the two monkeys. They should provide also individual monkey data so that the reader knows how consistent the effects are for the two animals.</p></disp-quote><p>This is now done in Figure 2—figure supplements 2,3.</p></body></sub-article></article>