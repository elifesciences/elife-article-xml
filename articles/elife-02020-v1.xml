<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1d3 20150301//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1d3" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="hwp">eLife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">02020</article-id><article-id pub-id-type="doi">10.7554/eLife.02020</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Medicine</subject></subj-group></article-categories><title-group><article-title>Diagnostically relevant facial gestalt information from ordinary photos</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-9795"><name><surname>Ferry</surname><given-names>Quentin</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="aff" rid="aff2"/><xref ref-type="other" rid="par-3"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf2"/><xref ref-type="other" rid="dataro1"/></contrib><contrib contrib-type="author" id="author-9796"><name><surname>Steinberg</surname><given-names>Julia</given-names></name><xref ref-type="aff" rid="aff2"/><xref ref-type="aff" rid="aff3"/><xref ref-type="other" rid="par-2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/><xref ref-type="other" rid="dataro1"/></contrib><contrib contrib-type="author" id="author-3729"><name><surname>Webber</surname><given-names>Caleb</given-names></name><xref ref-type="aff" rid="aff2"/><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/><xref ref-type="other" rid="dataro1"/></contrib><contrib contrib-type="author" id="author-9797"><name><surname>FitzPatrick</surname><given-names>David R</given-names></name><xref ref-type="aff" rid="aff4"/><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/><xref ref-type="other" rid="dataro1"/></contrib><contrib contrib-type="author" id="author-1023"><name><surname>Ponting</surname><given-names>Chris P</given-names></name><xref ref-type="aff" rid="aff2"/><xref ref-type="other" rid="par-1"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataro1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-9799"><name><surname>Zisserman</surname><given-names>Andrew</given-names></name><xref ref-type="aff" rid="aff1"/><xref ref-type="corresp" rid="cor1">*</xref><xref ref-type="other" rid="par-3"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/><xref ref-type="other" rid="dataro1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-9463"><name><surname>Nellåker</surname><given-names>Christoffer</given-names></name><xref ref-type="aff" rid="aff2"/><xref ref-type="corresp" rid="cor2">*</xref><xref ref-type="other" rid="par-4"/><xref ref-type="other" rid="par-5"/><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/><xref ref-type="other" rid="dataro1"/></contrib><aff id="aff1"><institution content-type="dept">Department of Engineering Science</institution>, <institution>University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff2"><institution content-type="dept">Medical Research Council Functional Genomics Unit, Department of Physiology, Anatomy and Genetics</institution>, <institution>University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff3"><institution>The Wellcome Trust Centre for Human Genetics, University of Oxford</institution>, <addr-line><named-content content-type="city">Oxford</named-content></addr-line>, <country>United Kingdom</country></aff><aff id="aff4"><institution content-type="dept">Medical Research Council Human Genetics Unit</institution>, <institution>Institute of Genetics and Molecular Medicine</institution>, <addr-line><named-content content-type="city">Edinburgh</named-content></addr-line>, <country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Tollman</surname><given-names>Stephen</given-names></name><role>Reviewing editor</role><aff><institution>Wits University</institution>, <country>South Africa</country></aff></contrib></contrib-group><author-notes><corresp id="cor1"><label>*</label>For correspondence: <email>az@robots.ox.ac.uk</email> (AZ);</corresp><corresp id="cor2"><label>*</label>For correspondence: <email>christoffer.nellaker@dpag.ox.ac.uk</email> (CN)</corresp></author-notes><pub-date date-type="pub" publication-format="electronic"><day>24</day><month>06</month><year>2014</year></pub-date><pub-date pub-type="collection"><year>2014</year></pub-date><volume>3</volume><elocation-id>e02020</elocation-id><history><date date-type="received"><day>06</day><month>12</month><year>2013</year></date><date date-type="accepted"><day>25</day><month>05</month><year>2014</year></date></history><permissions><copyright-statement>© 2014, Ferry et al</copyright-statement><copyright-year>2014</copyright-year><copyright-holder>Ferry et al</copyright-holder><license xlink:href="http://creativecommons.org/licenses/by/3.0/"><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/3.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-02020-v1.pdf"/><abstract><object-id pub-id-type="doi">10.7554/eLife.02020.001</object-id><p>Craniofacial characteristics are highly informative for clinical geneticists when diagnosing genetic diseases. As a first step towards the high-throughput diagnosis of ultra-rare developmental diseases we introduce an automatic approach that implements recent developments in computer vision. This algorithm extracts phenotypic information from ordinary non-clinical photographs and, using machine learning, models human facial dysmorphisms in a multidimensional 'Clinical Face Phenotype Space'. The space locates patients in the context of known syndromes and thereby facilitates the generation of diagnostic hypotheses. Consequently, the approach will aid clinicians by greatly narrowing (by 27.6-fold) the search space of potential diagnoses for patients with suspected developmental disorders. Furthermore, this Clinical Face Phenotype Space allows the clustering of patients by phenotype even when no known syndrome diagnosis exists, thereby aiding disease identification. We demonstrate that this approach provides a novel method for inferring causative genetic variants from clinical sequencing data through functional genetic pathway comparisons.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.001">http://dx.doi.org/10.7554/eLife.02020.001</ext-link></p></abstract><abstract abstract-type="executive-summary"><object-id pub-id-type="doi">10.7554/eLife.02020.002</object-id><title>eLife digest</title><p>Rare genetic disorders affect around 8% of people, many of whom live with symptoms that greatly reduce their quality of life. Genetic diagnoses can provide doctors with information that cannot be obtained by assessing clinical symptoms, and this allows them to select more suitable treatments for patients. However, only a minority of patients currently receive a genetic diagnosis.</p><p>Alterations in the face and skull are present in 30–40% of genetic disorders, and these alterations can help doctors to identify certain disorders, such as Down’s syndrome or Fragile X. Extending this approach, Ferry et al. trained a computer-based model to identify the patterns of facial abnormalities associated with different genetic disorders. The model compares data extracted from a photograph of the patient’s face with data on the facial characteristics of 91 disorders, and then provides a list of the most likely diagnoses for that individual. The model used 36 points to describe the space, including 7 for the jaw, 6 for the mouth, 7 for the nose, 8 for the eyes and 8 for the brow.</p><p>This approach of Ferry et al. has three advantages. First, it provides clinicians with information that can aid their diagnosis of a rare genetic disorder. Second, it can narrow down the range of possible disorders for patients who have the same ultra-rare disorder, even if that disorder is currently unknown. Third, it can identify groups of patients who can have their genomes sequenced in order to identify the genetic variants that are associated with specific disorders.</p><p>The work by Ferry et al. lays out the basic principles for automated approaches to analyze the shape of the face and skull. The next challenge is to integrate photos with genetic data for use in clinical settings.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.002">http://dx.doi.org/10.7554/eLife.02020.002</ext-link></p></abstract><kwd-group kwd-group-type="author-keywords"><title>Author keywords</title><kwd>phenotyping</kwd><kwd>computer vision</kwd><kwd>clinical genetics</kwd><kwd>computational biology</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="par-1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council (MRC)</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Webber</surname><given-names>Caleb</given-names></name><name><surname>FitzPatrick</surname><given-names>David R</given-names></name><name><surname>Ponting</surname><given-names>Chris P</given-names></name></principal-award-recipient></award-group><award-group id="par-2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Steinberg</surname><given-names>Julia</given-names></name></principal-award-recipient></award-group><award-group id="par-3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>228180 VisRec</award-id><principal-award-recipient><name><surname>Ferry</surname><given-names>Quentin</given-names></name><name><surname>Zisserman</surname><given-names>Andrew</given-names></name></principal-award-recipient></award-group><award-group id="par-4"><funding-source><institution-wrap><institution>Oxford Biomedical Research Centre</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Nellåker</surname><given-names>Christoffer</given-names></name></principal-award-recipient></award-group><award-group id="par-5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council (MRC)</institution></institution-wrap></funding-source><award-id>Centenary Award</award-id><principal-award-recipient><name><surname>Ferry</surname><given-names>Quentin</given-names></name><name><surname>Nellåker</surname><given-names>Christoffer</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta><meta-name>elife-xml-version</meta-name><meta-value>2</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An automatic computer vision analysis approach can assist in the diagnosis of rare genetic disorders by using ordinary photographs of patients.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Genetic disorders affect almost 8% of people (<xref ref-type="bibr" rid="bib4">Baird et al., 1988</xref>), about a third of whom will have symptoms that greatly reduce their quality of life. While there are over 7000 known inherited disorders, only a minority of patients with a suspected developmental disorder receive a clinical, let alone a genetic, diagnosis (<xref ref-type="bibr" rid="bib30">Hart and Hart, 2009</xref>). A genetic diagnosis allows more specific therapeutic interventions to be investigated and can aid the identification of primary vs secondary symptoms.</p><p>The introduction of whole genome and exome sequencing into modern clinical medicine will be instrumental in raising the current low rate of genetic diagnoses for ultra-rare diseases. Nevertheless, tools to accurately assign functional and disease relevance to sequence variants are substantially lacking. Projects that apply next generation sequencing to patients in clinical settings fail to report genetic diagnoses for approximately 80% of cases (<xref ref-type="bibr" rid="bib19">de Ligt et al., 2012</xref>). The difficulty lies in identifying the causal variant in an individual patient: even when ignoring experimental error, each individual carries approximately 4 million differences, in the case of whole genome sequencing, relative to the reference genome. Computational analyses currently are able only to interpret the ∼2500 variants that alter protein sequence at evolutionarily conserved positions and ∼400 very rare variants that are likely to be causal for pathogenic processes (<xref ref-type="bibr" rid="bib1">Abecasis et al., 2012</xref>). Notably, of the ∼10% of the genome that is functional all except the 1.2% that is protein-coding is often disregarded (<xref ref-type="bibr" rid="bib62">Weischenfeldt et al., 2013</xref>). Therefore, the prediction of causal inherited variants in an individual can result in high false positive and high false negative rates.</p><p>The most powerful approach to associate a particular gene with an ultra-rare disease is to identify multiple unrelated individuals with the disorder whose genomes harbor deleterious alleles in a shared gene, regulatory element or pathway (<xref ref-type="bibr" rid="bib52">Schuurs-Hoeijmakers et al., 2012</xref>). However, this approach relies on at least two individuals with the same disorder being available for comparison, an unlikely event given that these two individuals are selected for comparison from the roughly 100 million people affected by rare developmental disorders (prevalence of less than 2 per 100,000 around the world) (<xref ref-type="bibr" rid="bib43">Orphanet, 2013</xref>). For the past 65 years, clinical geneticists have studied, diagnosed, and characterized developmental disorders on the basis of common characteristics among patients (<xref ref-type="bibr" rid="bib49">Rimoin and Hirschhorn, 2004</xref>). When a given causal variant is ultra-rare, however, this presents substantial difficulties. Consequently, to realize the full potential of next generation sequencing in clinical diagnostics, phenotypic characterization must also become correspondingly high throughput and sensitive (<xref ref-type="bibr" rid="bib31">Hennekam and Biesecker, 2012</xref>).</p><p>The facial gestalt provides valuable information to identify similarities between patients because 30–40% of genetic disorders manifest craniofacial abnormalities (<xref ref-type="bibr" rid="bib30">Hart and Hart, 2009</xref>). The utility of computer vision for diagnosis and phenotyping of dysmorphic disorders has been explored previously by several groups and with varying approaches (<xref ref-type="bibr" rid="bib39">Loos et al., 2003</xref>; <xref ref-type="bibr" rid="bib28">Hammond et al., 2005</xref>; <xref ref-type="bibr" rid="bib27">Hammond, 2007</xref>; <xref ref-type="bibr" rid="bib11">Boehringer et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">Dalal and Phadke, 2007</xref>; <xref ref-type="bibr" rid="bib60">Vollmar et al., 2008</xref>; <xref ref-type="bibr" rid="bib10">Boehringer et al., 2011</xref>, reviewed in <xref ref-type="bibr" rid="bib29">Hammond and Suttie, 2012</xref>; <xref ref-type="bibr" rid="bib6">Baynam et al., 2013</xref>). The computational analysis of facial morphology using 3D imaging has been applied to conditions such as fetal alcohol syndrome (<xref ref-type="bibr" rid="bib56">Suttie et al., 2013</xref>), schizophrenia (<xref ref-type="bibr" rid="bib15">Buckley et al., 2005</xref>; <xref ref-type="bibr" rid="bib33">Hennessy et al., 2006</xref>, <xref ref-type="bibr" rid="bib32">2007</xref>) and autism (<xref ref-type="bibr" rid="bib2">Aldridge et al., 2011</xref>). While 3D imaging studies have shown high discriminatory power in terms of classification they have relied on specialized imaging equipment and patient cooperation. Previous work with 2D images has relied on manual annotation of images, controlling lighting, pose and expression to allow consistent analyses. These factors greatly limit the availability, and ultimately the potential widespread clinical utility of such approaches.</p><p>We have adopted a complementary approach that takes advantage of the wealth of data available for human faces, an indirect result of the ubiquitous availability of cameras. To do so we provide a new representation ('Clinical Face Phenotype Space'), which is an application of computer vision and machine learning algorithms for analyzing craniofacial dysmorphisms from ordinary photographs. We have ensured that Clinical Face Phenotype Space is robust to spurious variations such as lighting, pose, and image quality which would otherwise bias analyses. The approach is fully automated and provides objective and consistent computational descriptions of facial gestalt. Our method both greatly narrows the search space for investigating known disorders and will increase the power of inferring causative variants in previously unidentified genetic disease.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We sought to construct a database of patient photos within which faces would be automatically identified and their key features annotated. Our intent was to build a model of dysmorphic variation from a set of syndromes that, additionally, would be able to cluster syndromes not used in model training. Our schema by which a patient photo is automatically analyzed within the context of Clinical Face Phenotype Space is provided in <xref ref-type="fig" rid="fig1">Figure 1A</xref>.<fig-group><fig id="fig1" position="float"><object-id pub-id-type="doi">10.7554/eLife.02020.003</object-id><label>Figure 1.</label><caption><title>Overview of the computational approach and average faces of syndromes.</title><p>(<bold>A</bold>) A photo is automatically analyzed to detect faces and feature points are placed using computer vision algorithms. Facial feature annotation points delineate the supra-orbital ridge (8 points), the eyes (mid points of the eyelids and eye canthi, 8 points), nose (nasion, tip, ala, subnasale and outer nares, 7 points), mouth (vermilion border lateral and vertical midpoints, 6 points) and the jaw (zygoma mandibular border, gonion, mental protrubance and chin midpoint, 7 points). Shape and Appearance feature vectors are then extracted based on feature points and these determine the photo's location in Clinical Face Phenotype Space (further details on feature points in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). This location is then analyzed in the context of existing points in Clinical Face Phenotype Space to extract phenotype similarities and diagnosis hypotheses (further details on Clinical Face Phenotype Space with simulation examples in <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). (<bold>B</bold>) Average faces of syndromes in the database constructed using AAM models (‘Materials and methods’) and number of individuals which each average face represents. See online version of this manuscript for animated morphing images that show facial features differing between controls and syndromes (<xref ref-type="other" rid="media1">Figure 2</xref>).</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.003">http://dx.doi.org/10.7554/eLife.02020.003</ext-link></p></caption><graphic xlink:href="elife-02020-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.02020.004</object-id><label>Figure 1—figure supplement 1.</label><caption><p>(<bold>A</bold>) The 36 facial feature points annotated by the automatic image analysis algorithm. Supra-orbital ridge (8 points), the eyes (mid points of the eyelids and eye canthi, 8 points), nose (nasion, tip, ala, subnasale and outer nares, 7 points), mouth (vermilion border lateral and vertical midpoints, 6 points), and the jaw (zygoma mandibular border, gonion, mental protrubance and chin midpoint, 7 points). (<bold>B</bold>) The annotation accuracies relative to the manually annotated ground truth of each of the computer vision modules. Points 1–8 refer to the supra-orbital ridge, points 30–36 refer to the jaw points. Accuracies for the points annotated by the modules FLA, improved FLA and CoE are shown for each syndrome and control groups. Accuracies are shown as the average error relative to the width of an eye.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.004">http://dx.doi.org/10.7554/eLife.02020.004</ext-link></p></caption><graphic xlink:href="elife-02020-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.02020.005</object-id><label>Figure 1—figure supplement 2.</label><caption><title>Phenotypic vs spurious feature variation in Clinical Face Phenotype Space using simulated faces.</title><p>Simulated 3D faces were used to visualize the influence of spurious variation in raw feature space and Clinical Face Phenotype Space. (<bold>A</bold>) 100 faces with controlled phenotype, lighting, and rotation variation were rendered. (<bold>B</bold>) Visualization of a population of simulated faces in the first two Multi-Dimensional Scaling (MDS) modes. Face clustering in raw feature space and Clinical Face Phenotype Space colored by lighting, rotation, and face phenotype, respectively. In the raw feature space lighting is the dominating clustering factor, in Clinical Face Phenotype Space phenotype underlies the primary clustering. (<bold>C</bold>) The first 16 modes of PCA decomposition of the raw feature vectors and in the Clinical Face Phenotype Space colored by lighting and rotation of the simulated faces. In the raw feature space, lighting, and rotation variation are encoded in the 2nd and 1st modes, indicating that clustering is dominated by spurious variation. In the Clinical Face Phenotype Space, lighting is represented in the 9th mode, whereas rotation is no longer represented in the first 16 modes. This shows that the Clinical Face Phenotype Space transformation reduces the influence of spurious variation on clustering of phenotypes.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.005">http://dx.doi.org/10.7554/eLife.02020.005</ext-link></p></caption><graphic xlink:href="elife-02020-fig1-figsupp2-v1.tif"/></fig></fig-group></p><sec id="s2-1"><title>Image database composition</title><p>We first collected a database of 2878 images, including 1515 healthy controls and 1363 pictures for eight known developmental disorders from publically available sources across the internet (<xref ref-type="table" rid="tbl1">Table 1</xref>, references for image sources are available from <xref ref-type="supplementary-material" rid="SD1-data">Supplementary file 1</xref>). Manual checks were performed to exclude images where the face or an eye was not clearly visible, or where an expert clinician (DRF) could not verify the diagnosis. Manual annotation of facial features points was performed on all images to allow training and testing of an automated annotation algorithm. These initial requirements for manual intervention are dispensed with in the final automatic algorithm (see below).<table-wrap id="tbl1" position="float"><object-id pub-id-type="doi">10.7554/eLife.02020.006</object-id><label>Table 1.</label><caption><p>Composition of the database</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.006">http://dx.doi.org/10.7554/eLife.02020.006</ext-link></p></caption><table frame="hsides" rules="groups"><thead><tr><th>Syndrome</th><th>Nr images</th><th>Syndrome</th><th>Nr images</th></tr></thead><tbody><tr><td>Public images online</td><td/><td>Published images</td><td/></tr><tr><td> Angelman</td><td>205</td><td> PACS1</td><td>2</td></tr><tr><td> Apert</td><td>203</td><td> BRAF</td><td>35</td></tr><tr><td> Cornelia de Lange</td><td>179</td><td> CFC</td><td>1</td></tr><tr><td> Down</td><td>199</td><td> Costello</td><td>10</td></tr><tr><td> Fragile X</td><td>164</td><td> ERF</td><td>5</td></tr><tr><td> Progeria</td><td>78</td><td> HRAS</td><td>5</td></tr><tr><td> Treacher Collins</td><td>103</td><td> KRAS</td><td>12</td></tr><tr><td> Williams-Beuren</td><td>232</td><td> MAP2K1</td><td>5</td></tr><tr><td/><td/><td> MAP2K2</td><td>4</td></tr><tr><td> Controls</td><td>1515</td><td> MEK1</td><td>5</td></tr><tr><td/><td/><td> NRAS</td><td>2</td></tr><tr><td> 22q11</td><td>8</td><td> PTPN11</td><td>19</td></tr><tr><td> Marfan</td><td>18</td><td> RAF1</td><td>9</td></tr><tr><td> Sotos</td><td>36</td><td> SHOC2</td><td>8</td></tr><tr><td> Turner</td><td>12</td><td> SOS1</td><td>30</td></tr><tr><td>The Gorlin Collection</td><td/><td/><td/></tr><tr><td> Aarskog</td><td>19</td><td> Klippel-Trenaunay</td><td>10</td></tr><tr><td> Achondroplasia</td><td>12</td><td> Langer-Giedion</td><td>14</td></tr><tr><td> Alagille</td><td>8</td><td> Larsen</td><td>11</td></tr><tr><td> Albright</td><td>7</td><td> Lenz_Majewski</td><td>17</td></tr><tr><td> Angelman</td><td>13</td><td> Lymphedema-Lymphangiectasia-MR</td><td>8</td></tr><tr><td> Apert</td><td>49</td><td> Melnick_Needles</td><td>17</td></tr><tr><td> Beckwith-Wiedemann</td><td>11</td><td> Moebius</td><td>9</td></tr><tr><td> Bloom</td><td>9</td><td> Muenke</td><td>15</td></tr><tr><td> BOF</td><td>15</td><td> Myotonicdystrophy</td><td>9</td></tr><tr><td> Cartilagehair</td><td>13</td><td> Neurofibromatosis</td><td>7</td></tr><tr><td> CHARGE</td><td>12</td><td> Noonan</td><td>29</td></tr><tr><td> Cherubism</td><td>20</td><td> OAVdysplasia</td><td>18</td></tr><tr><td> CleidoCranialdysostosis</td><td>13</td><td> ODD</td><td>21</td></tr><tr><td> Coffin-Lowry</td><td>20</td><td> OFCD</td><td>10</td></tr><tr><td> Costello</td><td>9</td><td> OFD</td><td>18</td></tr><tr><td> CriduChat</td><td>17</td><td> OPD</td><td>31</td></tr><tr><td> Crouzon</td><td>16</td><td> Osteopetrosis</td><td>2</td></tr><tr><td> Crouzonodermoskeletal</td><td>5</td><td> Osteosclerosis</td><td>5</td></tr><tr><td> Cutislaxa</td><td>11</td><td> Otodental</td><td>2</td></tr><tr><td> DeLange</td><td>17</td><td> Poland</td><td>4</td></tr><tr><td> Diastrophicdysplasia</td><td>5</td><td> Prader–Willi</td><td>16</td></tr><tr><td> Down</td><td>8</td><td> Progeria</td><td>14</td></tr><tr><td> Dubowitz</td><td>12</td><td> Proteus</td><td>6</td></tr><tr><td> Dyggve-Melchior-Clausen</td><td>8</td><td> Rieger</td><td>4</td></tr><tr><td> EEC</td><td>6</td><td> Rothmund-Thomson</td><td>13</td></tr><tr><td> Ehlers-Danlos</td><td>17</td><td> Rubinstein-Taybi</td><td>8</td></tr><tr><td> Ellis-vanCreveld</td><td>3</td><td> Saethre-Chotzen</td><td>25</td></tr><tr><td> FG</td><td>11</td><td> Sclerosteosis</td><td>4</td></tr><tr><td> FragileX</td><td>27</td><td> SeckelMOD</td><td>7</td></tr><tr><td> Frontometaphysealdysplasia</td><td>12</td><td> SEDcongenita</td><td>6</td></tr><tr><td> Gorlin</td><td>91</td><td> Sotos</td><td>16</td></tr><tr><td> Gorlin_Chaudry_Moss</td><td>13</td><td> Stickler</td><td>42</td></tr><tr><td> Greig</td><td>7</td><td> TRP</td><td>24</td></tr><tr><td> Hallermann-Streiff</td><td>9</td><td> Waardenburg</td><td>39</td></tr><tr><td> Incontinentiapigmenti</td><td>4</td><td> Weaver</td><td>13</td></tr><tr><td> Kabuki</td><td>25</td><td> Williams-Beuren</td><td>19</td></tr><tr><td> Klippel-Feil</td><td>3</td><td/><td/></tr></tbody></table></table-wrap></p></sec><sec id="s2-2"><title>Computer vision algorithms</title><p>We proceeded to train a computer vision algorithm for automatic annotation of 36 feature points of interest across the face (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Our approach takes advantage of a variety of facial detection algorithms (OpenCV [<xref ref-type="bibr" rid="bib14">Bradski, 2000</xref>], Viola Jones [<xref ref-type="bibr" rid="bib59">Viola and Jones, 2001</xref>] and Everingham [<xref ref-type="bibr" rid="bib21">Everingham et al., 2009</xref>]) and custom learning (consensus of exemplars [<xref ref-type="bibr" rid="bib7">Belhumeur et al., 2011</xref>]) to accurately place feature points on a given face (‘Materials and methods’). Across all images in our database, manual checking found that our algorithm detected and annotated 99.5% of tested faces correctly with accuracies in the range 6–60% of the width of an eye (individual feature point accuracies are provided in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>We used an Active Appearance Model ('Materials and methods') to calculate an average face within any set of images, representing consistent shape and appearance features within the group (<xref ref-type="fig" rid="fig1">Figure 1B</xref> and animated morphs in <xref ref-type="other" rid="media1">Figure 2</xref>). The average faces for each syndrome show that the algorithm effectively captures characteristic features of dysmorphic syndromes (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). For each feature point, the algorithm extracts a feature vector describing appearance of the surrounding patch. The algorithm then constructs a feature vector describing shape based on the relative pairwise distances between all feature points ('Materials and methods'). We next sought to compare the syndrome relevant information content of the feature descriptors to previous studies (<xref ref-type="bibr" rid="bib28">Hammond et al., 2005</xref>; <xref ref-type="bibr" rid="bib11">Boehringer et al., 2006</xref>; <xref ref-type="bibr" rid="bib27">Hammond, 2007</xref>; <xref ref-type="bibr" rid="bib60">Vollmar et al., 2008</xref>). We found that classification analysis based on support vector machines provided similar accuracies to previous work, despite disparities in image variability (average classification accuracy 94.4%, see Figure 4—figure supplement 1, Figure 4—figure supplement 2 and 'Materials and methods').<media content-type="glencoe play-in-place height-250 width-310" id="media1" mime-subtype="gif" mimetype="video" xlink:href="elife-02020-media1.gif"><object-id pub-id-type="doi">10.7554/eLife.02020.008</object-id><label>Figure 2.</label><caption><title>Animated morphs of average faces from controls to syndromes.</title><p>(<bold>A</bold>) Angelman, (<bold>B</bold>) Apert, (<bold>C</bold>) Cornelia de Lange, (<bold>D</bold>) Down, (<bold>E</bold>) Fragile X, (<bold>F</bold>) Progeria, (<bold>G</bold>) Treacher-Collins, (<bold>H</bold>) Williams-Beuren. Delineation of syndrome gestalt relative to controls with distortion graphs in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.008">http://dx.doi.org/10.7554/eLife.02020.008</ext-link></p></caption></media><fig id="fig2s1" position="float"><object-id pub-id-type="doi">10.7554/eLife.02020.009</object-id><label>Figure 2—figure supplement 1.</label><caption><title>Distortion graphs representing the characteristic deformation of syndrome faces relative to the average control face.</title><p>Each line reflects whether the distance is extended or contracted compared with the control face. White—the distance is similar to controls, blue—shorter relative to controls, and red—extended in patients relative to controls.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.009">http://dx.doi.org/10.7554/eLife.02020.009</ext-link></p></caption><graphic xlink:href="elife-02020-fig2-figsupp1-v1.tif"/></fig></p><p>It is important to emphasize that the analyzed images vary greatly, as there were minimal restrictions imposed on image selection placed by the two exclusion criteria (both eyes visible and diagnosis verified by DRF). Photos were analyzed irrespective of the subject's age, gender, facial expression or ethnicity or the background scenery. Principal component analysis (PCA) of facial descriptor vectors illustrates that the main sources of variation among images are indeed lighting, pose, and facial expression, rather than phenotypic features (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p></sec><sec id="s2-3"><title>Constructing a Clinical Face Phenotype Space with metric learning</title><p>We next performed Metric Learning using a Large Margin Nearest Neighbor (<xref ref-type="bibr" rid="bib61">Weinberger and Saul, 2009</xref>) approach for the eight syndromes in the database. This approach linearly transformed the multidimensional space of PCA feature vectors to optimize the separation of syndromes: dimensions informative for dysmorphism phenotypes are expanded while uninformative dimensions are compressed (thus changing the relative importance for clustering). We denote the resulting transformed 270 dimensional space as 'Clinical Face Phenotype Space' (see 'Materials and methods').</p><p>Due to its design, Clinical Face Phenotype Space clusters patient faces based on diagnostically relevant phenotypic features, while tolerating spurious variation. Relative importance of spurious and phenotypic variation for clustering in Clinical Face Phenotype Space was tested using simulated faces ('Materials and methods'). For these faces feature dimensions that reflected known spurious variation such as lighting and head orientation were compressed and hence were of less relevance for clustering (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>).</p><p>For the eight syndromes with which Clinical Face Phenotype Space was created, we performed tests with supervised learning and clustering. A kNN-classifier applied within Clinical Face Phenotype Space was able to correctly classify images with an accuracy of 99.5% using the leave-one-out method. However, to avoid biases introduced by training data size, we also assessed the improvements in clustering by measuring the search space reduction (hereafter referred to as the Clustering Improvement Factor or CIF, 'Materials and methods'). This estimates the factor by which the Clinical Face Phenotype Space improves the clustering of syndromes when compared with random chance (to 95% confidence). On average, the clustering of the eight syndromes within the database was improved by 11.0-fold (geometric mean of improved clustering, CIF range 9.1–23.5, maximum possible mean 12.5; <xref ref-type="fig" rid="fig3">Figure 3</xref>).<fig id="fig3" position="float"><object-id pub-id-type="doi">10.7554/eLife.02020.010</object-id><label>Figure 3.</label><caption><title>Clinical Face Phenotype Space enhances the separation of different dysmorphic syndromes.</title><p>The graph shows a two dimensional representation of the full Clinical Face Phenotype Space, with links to the 10 nearest neighbors of each photo (circle) and photos placed with force-directed graphing. The Clustering Improvement Factor (CIF, fold better clustering than random expectation) estimate for each of the syndromes is shown along the periphery.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.010">http://dx.doi.org/10.7554/eLife.02020.010</ext-link></p></caption><graphic xlink:href="elife-02020-fig3-v1.tif"/></fig></p><p>Next, we tested and confirmed our hypothesis that Clinical Face Phenotype Space could be generalized to dysmorphic syndromes that were not used in the training. We had access to 75 syndromes from the Gorlin collection (a kind gift curated and annotated by Professor Raoul Hennekam, Academic Medical Center, University of Amsterdam), which we supplemented with additional images of 22q11, Marfan and Sotos syndromes. Furthermore, we collected images of patients with verified genetic mutations in <italic>PACS1</italic> or in specific genes from the RAS/MEK pathway (<xref ref-type="supplementary-material" rid="SD1-data">Supplementary file 1</xref> references for image sources in 'Materials and methods'). The number of individuals within each syndrome varied between 2 and 223. The search space reduction was on average 27.6-fold better than random chance (CIF range 1.0–700.0, maximum possible average CIF was 150.0; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). That is to say, that among 2754 patients' faces associated with any of 90 syndromes Clinical Face Phenotype Space makes it 27.6-fold easier to make the correct diagnosis. This demonstrates that Clinical Face Phenotype Space is an effective approach to the identification of multiple individuals sharing ultra-rare, previously undocumented, genetic disorders.<fig-group><fig id="fig4" position="float"><object-id pub-id-type="doi">10.7554/eLife.02020.011</object-id><label>Figure 4.</label><caption><title>Clinical Face Phenotype Space is generalizable to dysmorphic syndromes that are absent from a training set.</title><p>(<bold>A</bold>) Clustering Improvement Factor (CIF) estimates are plotted vs the number of individuals per syndrome grouping in the Gorlin collection or patients with similar genetic variant diagnoses. As expected, the stochastic variance in CIF is inversely proportional to the number of individuals available for sampling. The median CIF across all groups is 27.6-fold over what is expected by clustering syndromes randomly. That is to say, the CIF of a randomly placed set is 1. The maximum CIF is fixed by the total number of images in the database and by the cardinality of a syndrome set: the theoretical maximal CIF upper bound is plotted as a red dotted line. The CIF for the minimum and maximum, Cutislaxa syndrome and Otodental syndrome, were 1.0 and 700.0 respectively. (<bold>B</bold>) Average probabilistic classification accuracies of each individual face placed in Clinical Face Phenotype Space (class prioritization by 20 nearest neighbors weighted by prevalence in the database). The 8 initial syndromes used to train Clinical Face Phenotype Space are shown in color. For syndromes with fewer than 50 examples, accuracies were averaged across all syndromes binned by data set size (i.e., the average accuracy is shown for syndromes with 2–5, 6–10, 11–25, and 26–50 images in the database, <xref ref-type="supplementary-material" rid="SD1-data">Supplementary file 1</xref>). Classification accuracies increase proportional to the number of individuals with the syndrome present in the database. Accuracies using support vector machines with binary and forced choice classifications are shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>. A simulation example of probabilistic querying of Clinical Face Phenotype Space is shown in <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.011">http://dx.doi.org/10.7554/eLife.02020.011</ext-link></p></caption><graphic xlink:href="elife-02020-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.02020.012</object-id><label>Figure 4—figure supplement 1.</label><caption><title>SVM binary classification accuracies among the 8 syndromes in <xref ref-type="table" rid="tbl1">Table 1</xref>.</title><p>SVM classifier accuracies when tuned for equal false positive and false negative error rates.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.012">http://dx.doi.org/10.7554/eLife.02020.012</ext-link></p></caption><graphic xlink:href="elife-02020-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.02020.013</object-id><label>Figure 4—figure supplement 2.</label><caption><title>SVM forced choice classification accuracies among the 8 syndromes in <xref ref-type="table" rid="tbl1">Table 1</xref>.</title><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.013">http://dx.doi.org/10.7554/eLife.02020.013</ext-link></p></caption><graphic xlink:href="elife-02020-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.02020.014</object-id><label>Figure 4—figure supplement 3.</label><caption><title>Simulated example illustrating the Clustering Improvement Factor.</title><p>A random scattering of 100 points in 2 dimensions is used as a background set (black circles with white fill). The 20 red plus symbols (within the red shaded area) are a random set of points lying within the same limits as the background set and have a CIF of 0.9. This is the actual degree of clustering of the red points with respect to the expectation of clustering them with 95% confidence (E(r) = 5.6). The filled green circles (within the green shaded area) are the red points shifted by +0.5 units in each dimension and have a CIF of 2.7. The black points (within the gray shaded area) are the red plus symbol positions scaled by 0.5 and then shifted by +1.5 units in dimension 1. The black points are non-overlapping with the background and represent the maximal CIF (of 5.6) in this example.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.014">http://dx.doi.org/10.7554/eLife.02020.014</ext-link></p></caption><graphic xlink:href="elife-02020-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><object-id pub-id-type="doi">10.7554/eLife.02020.015</object-id><label>Figure 4—figure supplement 4.</label><caption><title>Simulated example of probabilistic querying of Clinical Face Phenotype Space.</title><p>(<bold>A</bold>) Visualization of a population of simulated faces in the first two Multi-Dimensional Scaling (MDS) modes. 7 classes of points (simulated 'syndrome groups') are shown with different distributions and variances. A central 'query' face is indicated by the boxed cross. The 20 nearest neighbors of the query are encircled with a black border. (<bold>B</bold>) Inset bar graph shows diagnosis hypothesis ranked by class priority. The class priority ranking weights the dispersion and prevalence (spread and number) of a class in the Clinical Face Phenotype Space with the nearest neighbors to assign the most probable diagnosis hypotheses. In the example, the ranked diagnosis estimates of the query point would be class 7, then class 6, and thirdly class 4. The scatter plot shows the individual similarity p0p1 estimates, reflecting their relative closeness in the space as compared to local neighborhood, for the 20 nearest neighbors of the query. The first nearest neighbor is estimated to be 2.6-fold closer to the query than the average based on the local density of neighbors. The dotted line indicates the average relative distance between points among the 20 nearest neighbors. (<bold>C</bold>) Inset bar graph shows the number of neighbors of the query per class. A scatterplot of dispersion vs cardinality, i.e. relative spread of points and what proportion of the total number of points belong to that class in the simulated space. Plots (<bold>B</bold>) and (<bold>C</bold>) allow objective assessment of the distribution of points shown in (<bold>A</bold>), and aid the interpretation of classification confidence.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.015">http://dx.doi.org/10.7554/eLife.02020.015</ext-link></p></caption><graphic xlink:href="elife-02020-fig4-figsupp4-v1.tif"/></fig></fig-group></p><p>We proceeded to test if Clinical Face Phenotype Space recapitulates the modularity of genetic diseases, where clusters of phenotypically similar disorders reflect functional relationships among the genes involved (see <xref ref-type="bibr" rid="bib44">Oti and Brunner, 2007</xref> for a review). We have shown that individuals with the same underlying genetic disease automatically cluster in Clinical Face Phenotype Space. We next tested whether disorders caused by mutations in different genes result in meaningful clusters in Clinical Face Phenotype Space. We selected disorders with a known genetic origin, using either gene associations from OMIM or publications describing the identification of causative genes (see 'Materials and methods'). For each pair of genes, the shortest path in a protein–protein interaction network was obtained from Dapple (<xref ref-type="bibr" rid="bib50">Rossin et al., 2011</xref>), giving a protein interaction distance relevant to that gene pair. We compared genes underlying monogenic syndromes linked by 1, 2, or 3 path distances, with those with a path distance of 4 or that was unknown; unknown distances are those where no genes are associated with a syndrome, the syndrome is multigenic, or when DAPPLE has no known interaction documented, see 'Materials and methods'. For each pair of syndromes, an average Euclidean distance in Clinical Face Phenotype Space was calculated. The distance in Clinical Face Phenotype Space is significantly shorter between syndromes associated to genes with protein interaction distances of 1, 2, or 3 compared with syndromes with 4 or no known interactions (p&lt; 0.01, p&lt; 0.05 and p&lt; 0.001 respectively, <xref ref-type="fig" rid="fig5">Figure 5</xref>). This demonstrates that the distance in Clinical Face Phenotype Space partly recapitulates the functional relatedness of underlying developmental processes known to be disrupted in genetic diseases.<fig id="fig5" position="float"><object-id pub-id-type="doi">10.7554/eLife.02020.016</object-id><label>Figure 5.</label><caption><title>Clinical Face Phenotype Space recapitulates features of functional gene links between syndromes.</title><p>Protein–protein interaction distances of 1–3 for genetically characterized syndromes are associated with significantly shorter Euclidean distance (arbitrary units) between syndromes in Clinical Face Phenotype Space as compared to syndromes with distance 4 or no known interaction distance (shown in orange) (Kruskal–Wallis tests with Bonferroni corrected p-values indicated as *p&lt;0.05, **p&lt;0.01, ***p&lt;0.001). The Spearman correlation across all distances was r = 0.09, p&lt;0.001. The numbers of pairwise syndrome comparisons underlying each of the interaction distances are listed within the respective boxes.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.016">http://dx.doi.org/10.7554/eLife.02020.016</ext-link></p></caption><graphic xlink:href="elife-02020-fig5-v1.tif"/></fig></p></sec><sec id="s2-4"><title>Querying Clinical Face Phenotype Space</title><p>Clinical Face Phenotype Space can provide clinical phenotyping and clustering to known genetic disorders that is objective and high-throughput. The method is, however, neither sufficiently accurate nor intended to determine diagnosis, yet it can help to narrow the diagnostic search space in an unprejudiced manner. A clinician could easily photograph a patient and immediately obtain clinically useful diagnostic hypotheses and matching cases. To this end, we implemented two primary methods to automatically and objectively query Clinical Face Phenotype Space.</p><p>For any given image located in Clinical Face Phenotype Space, we obtain confidence ranked classifications to known disorders (see 'Materials and methods' and <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). In addition, we objectively compare the image to others within the space. For any given query image, a probabilistic ranking of similar syndromes is obtained through nearest neighbor representation compared to random expectation of clustering among the 90 syndromes and 2754 faces. The classification confidence for a particular disorder depends on its location within the space, but also on the local densities of similar faces. We find that for the eight initial syndromes used to construct Clinical Face Phenotype Space, 93.1% (range 81.0–99.2%) are correctly classified as the top rank, cumulatively converging on 99.1% (95.8–100%) by the 20<sup>th</sup> rank (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). Of syndromes not part of the Clinical Face Phenotype Space training, the classification accuracies positively correlated strongly with the number of instances in the database (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). For the 20 syndromes where the database held 5 or fewer examples (<xref ref-type="table" rid="tbl1">Table 1</xref>), we classify on average 20.3% correctly by the 6<sup>th</sup> rank (exceeding 16.3-fold better than by chance alone).</p><p>For individuals with a suspected ultra-rare or an undocumented novel disorder, we developed a metric, p0p1, which assesses their similarity to others within Clinical Face Phenotype Space. The metric estimates the relative closeness of two faces given an average local density with the space: a p0p1 value exceeding 1 indicates a potentially new cluster, see 'Materials and methods'. The 2 PACS1 cases reported by <xref ref-type="bibr" rid="bib52">Schuurs-Hoeijmakers et al. (2012)</xref> placed within Clinical Face Phenotype Space have a p0p1 value of 1.05 meaning that they are 5% closer to one another than the geometric mean of the distances to their 20 nearest neighbors. Taking into account that this is a local density estimate among 2754 faces in Clinical Face Phenotype Space, the search space to find them has been reduced ∼690.4-fold (CIF, see 'Materials and methods').</p><p>The combination of syndrome clustering and de novo similarity metrics should aid the diagnosis of known syndromes and provides a means of clustering patients where no documented diagnosis exists.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have developed our algorithm on normal-everyday 2D photographs and have focused on 36 facial feature points. Given the orders of magnitude lower dimensionality of our data as compared to a 3D imaging capture (<xref ref-type="bibr" rid="bib28">Hammond et al., 2005</xref>), we were initially concerned that this would be insufficient to capture facial phenotypes. However, we then demonstrated that the approach is able to describe and discriminate between syndromes with a comparable accuracy to previous studies (<xref ref-type="bibr" rid="bib39">Loos et al., 2003</xref>; <xref ref-type="bibr" rid="bib28">Hammond et al., 2005</xref>; <xref ref-type="bibr" rid="bib27">Hammond, 2007</xref>; <xref ref-type="bibr" rid="bib11">Boehringer et al., 2006</xref>; <xref ref-type="bibr" rid="bib18">Dalal and Phadke, 2007</xref>; <xref ref-type="bibr" rid="bib60">Vollmar et al., 2008</xref>; <xref ref-type="bibr" rid="bib10">Boehringer et al., 2011</xref>). The accessibility of normal 2D photographs (as opposed to 3D imaging) should outweigh any lower data resolution obtained from any one image and in future developments using multiple profile perspectives will allow 3D structure to be inferred. With accurate registration of a person's face from multiple images across time, from a family photo album for instance, it would capture not only the 3D structure but also the progression and development of dysmorphic gestalt. The automatic image analysis algorithm enables phenotypic metrics to be obtained with objective consistency from each image (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><p>Clinical Face Phenotype Space was instantiated using eight syndromes that were well populated in our database so as to be robust against spurious variation. In doing so, it has become a generalizable model for craniofacial dysmorphic variation (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The high fidelity of the current Clinical Face Phenotype Space (<xref ref-type="fig" rid="fig3">Figure 3</xref>) shows promise given that known deficiencies have yet to be addressed: (1) We used only single image examples of individuals. (2) The spectrum of phenotypes represented was limited. (3) The average image quality in the database was low. (4) The current 36 facial feature points only capture full frontal facial phenotypes, and thus miss valuable information from the full cranium and profile perspectives. Among the approaches that will be tested in future work are: increasing the number of feature points across the cranium, using profile images and taking advantage of multiple images of the same individual. Furthermore, we will be exploring performing explicit modelingmodeling of the 3D variation for 2D images (<xref ref-type="bibr" rid="bib45">Ramnath et al., 2008</xref>), other types of feature descriptors, alternative metric learning and dimensionality reduction approaches (<xref ref-type="bibr" rid="bib54">Simonyan et al., 2013</xref>). As Clinical Face Phenotype Space is developed and populated with more individuals, the predictive power to infer novel causative genetics would be expected to increase linearly until it asymptotically approaches a theoretical maximum.</p><p>There are three anticipated primary applications for Clinical Face Phenotype Space in a clinical setting: narrowing the search space for documented developmental disorders, identifying multiple people that share an ultra-rare genetic disorders and aiding the inference of causative variants in clinical genetic sequencing (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>We envisage Clinical Face Phenotype Space becoming a standard tool to support clinical genetic counseling. Since any normal 2D image can be analyzed, this approach is available to any clinician worldwide with access to a camera and a computer. This can also reduce the need for patient inconvenience in a clinical setting because a family photo album could provide the required image(s). A photograph will enable automatic digital phenotyping, and its placement in Clinical Face Phenotype Space will provide an unbiased list of candidate clinical hypotheses (exemplified in <xref ref-type="fig" rid="fig6">Figure 6</xref>). We anticipate that future developments of Clinical Face Phenotype Space will also identify sub-phenotypes or comorbidities. Where no known genetic disease or variant can be assigned, Clinical Face Phenotype Space can identify other patients with phenotypic similarities empowering the identification of ultra-rare genetic disorders.<fig id="fig6" position="float"><object-id pub-id-type="doi">10.7554/eLife.02020.017</object-id><label>Figure 6.</label><caption><title>Class priority of diagnostic classifications for images.</title><p>The full computer vision algorithm and Clinical Face Phenotype Space analysis procedure with diagnostic hypothesis generation exemplified by: (<bold>A</bold>) a patient (<xref ref-type="bibr" rid="bib22">Ferrero et al., 2007</xref>) with Williams-Beuren. (<bold>B</bold>) Abraham Lincoln. The former US President is thought to have had a marfanoid disorder, if not Marfan syndrome (<xref ref-type="bibr" rid="bib25">Gordon, 1962</xref>; <xref ref-type="bibr" rid="bib55">Sotos, 2012</xref>). Bar graphs show class prioritization of diagnostic hypotheses determined by 20 nearest neighbors weighted by prevalence in the database. As expected, the classification of Marfan is not successfully assigned in the first instance as there were only 18 faces of individuals with Marfan in the database (making this an example of a difficult case with the current database). However, the seventh suggestion is Marfan, despite this being among 90 different syndromes and 2754 faces.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.017">http://dx.doi.org/10.7554/eLife.02020.017</ext-link></p></caption><graphic xlink:href="elife-02020-fig6-v1.tif"/><permissions><copyright-statement>© 2007 Elsevier Masson SAS. All rights reserved</copyright-statement><copyright-year>2007</copyright-year><copyright-holder>Elsevier Masson SAS</copyright-holder><license><license-p>The patient figure in Figure 6, part <bold>A</bold> is reproduced from <xref ref-type="bibr" rid="bib22">Ferrero et al., (2007)</xref>, <italic>European Journal of Medical Genetics</italic> with permission.</license-p></license></permissions></fig></p><p>In summary, we have presented an algorithmic approach that provides a critical advance in applying computer vision and machine learning techniques as a tool for clinical geneticists. The conjunction of a computer vision and machine learning algorithm with Clinical Face Phenotype Space makes this approach high-throughput, automatic, objective, and broadly accessible with existing digital photography and computers. Our ongoing research has begun to apply the Clinical Face Phenotype Space approach within large clinical sequencing collaborations. Computer vision for aiding diagnosis of developmental disorders in clinical genetics will be tenable and broadly applicable in the near future.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Database collection</title><p>We built a database of publically available or scientifically published pictures of patients collected across the internet. We collected 100–283 images per syndrome for Angelman, Apert, Cornelia de Lange, Down, Fragile X, Progeria, Treacher Collins, or Williams-Beuren. Images were collected through publically available resources online accessible though search terms relating to each syndrome, primarily through support group pages and awareness event photographs. Source URLs were converted to shortened versions for the purposes of publication using TinyUrl (<ext-link ext-link-type="uri" xlink:href="http://tinyurl.com/">http://tinyurl.com/</ext-link>) (<xref ref-type="supplementary-material" rid="SD1-data">Supplementary file 1</xref>). The links provided are expected to decay with time and should only be considered exemplars of database composition. Images were captured through screen shots and saved as PNG or JPEG file formats.</p><p>The following two exclusion criteria were applied to the images: 1. The face needed to be clearly visible and oriented so that both eyes were visible. 2. The correct diagnosis was confirmed by an expert clinician (DRF). DRF inspected each image to validate the supposed syndrome diagnosis; images not validated were discarded. Variation in lighting, image resolution, pose, and occlusions has only been restricted when it obscures the facial characteristics (such as a hand covering the face). We also sought to avoid multiple images of the same individual at the same age in the database. No further restrictions were placed on variations in pose, facial expression, lighting, occlusions, or image quality.</p><p>In the same manner, smaller numbers of images were collected for Marfan, 22q11, Turner and Sotos syndromes (<xref ref-type="table" rid="tbl1">Table 1</xref>). Furthermore, we collected further published images of patients with confirmed genetic variants in genes of the RAS/MEK signaling pathways as well as in <italic>PACS1</italic> (<xref ref-type="bibr" rid="bib46">Rauen, 1993</xref>, <xref ref-type="bibr" rid="bib47">2006</xref>; <xref ref-type="bibr" rid="bib8">Bertola et al., 2007</xref>; <xref ref-type="bibr" rid="bib26">Gripp et al., 2007</xref>; <xref ref-type="bibr" rid="bib40">Makita et al., 2007</xref>; <xref ref-type="bibr" rid="bib48">Rauen, 2007</xref>; <xref ref-type="bibr" rid="bib64">Zampino et al., 2007</xref>; <xref ref-type="bibr" rid="bib42">Nystrom et al., 2008</xref>; <xref ref-type="bibr" rid="bib51">Schulz et al., 2008</xref>; <xref ref-type="bibr" rid="bib57">Tidyman and Rauen, 2008</xref>; <xref ref-type="bibr" rid="bib17">Cordeddu et al., 2009</xref>; <xref ref-type="bibr" rid="bib36">Kratz et al., 2009</xref>; <xref ref-type="bibr" rid="bib65">Zenker, 2009</xref>; <xref ref-type="bibr" rid="bib3">Allanson et al., 2010</xref>; <xref ref-type="bibr" rid="bib63">Wright and Kerr, 2010</xref>; <xref ref-type="bibr" rid="bib35">Kleefstra et al., 2011</xref>; <xref ref-type="bibr" rid="bib38">Lepri et al., 2011</xref>; <xref ref-type="bibr" rid="bib53">Siegel et al., 2011</xref>; <xref ref-type="bibr" rid="bib52">Schuurs-Hoeijmakers et al., 2012</xref>; <xref ref-type="bibr" rid="bib34">Hopper et al., 2013</xref>; <xref ref-type="bibr" rid="bib58">Twigg et al., 2013</xref>).</p><p>3100 images were collected and manually annotated for training of the algorithms. Of these 2878 were successfully annotated by the automatic pipeline and are reported in the database counts of <xref ref-type="table" rid="tbl1">Table 1</xref>.</p></sec><sec id="s4-2"><title>Data and code availability</title><p>Original database, excluding the Gorlin collection, and previously published images (which are available from the cited original publications) can be requested by contacting CN (<email>christoffer.nellaker@dpag.ox.ac.uk</email>; Ferry Q, Steinberg J, Webber C, FitzPatrick DR, Ponting CP, Zisserman A, Nellåker C, 2014, Diagnostically relevant facial gestalt information from ordinary photos database). Requests will be assessed by a Data Access Committee (DAC) comprised of CPP, DRF, AZ, CN and Dr Zameel Cader of the Division of Clinical Neurology, University of Oxford. The DAC will make data available to researchers in good standing with the relevant institution and funding agencies (i.e., no known sanctions). The data are provided without copyright.</p><p>Pipeline code was written in python 2.7 and uses the module Ruffus (<xref ref-type="bibr" rid="bib24">Goodstadt, 2010</xref>) for task management. The code is available through an open source MIT license at <ext-link ext-link-type="uri" xlink:href="https://github.com/ChristofferNellaker/Clinical_Face_Phenotype_Space_Pipeline">https://github.com/ChristofferNellaker/Clinical_Face_Phenotype_Space_Pipeline</ext-link>.</p></sec><sec id="s4-3"><title>Ethics statement</title><p>The manner and method by which images were collected from publically available sources and stored were acceptable research practices and do not require special consent from a Research Ethics Committee. Advice from legal services, research ethics board members and the Information Commissioner's Office (UK) was sought in arriving at this conclusion.</p></sec><sec id="s4-4"><title>Computer vision algorithm</title><p>The computer vision algorithm analyses a 2D photograph for the location of a face, annotates the facial landmark points, and extracts feature vectors for subsequent machine learning applications. MATLAB (MATLAB. R2011b Natick, Massachusetts: the MathWorks Inc.) with OpenCV (<xref ref-type="bibr" rid="bib14">Bradski, 2000</xref>) was used to write scripts and functions for the algorithm. To identify a putative face in the photo, we used previously published algorithms (<xref ref-type="bibr" rid="bib59">Viola and Jones, 2001</xref>). Within a box bounding the face, a pictorial structure model was used to identify 9 central facial feature points (<xref ref-type="bibr" rid="bib21">Everingham et al., 2009</xref>), which then were used to initialize the placement of an additional 27 feature points. The resulting facial mesh structure was fitted to the image using Active Appearance Models (AAMs) (<xref ref-type="bibr" rid="bib16">Cootes et al., 1998</xref>) to generate average face visualizations (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The placement of the 36 feature points was also further refined using custom written scripts based on consensus of exemplars (<xref ref-type="bibr" rid="bib7">Belhumeur et al., 2011</xref>) (see Methods).</p><p>From the fitted constellation of facial landmarks two feature vectors were extracted. (1) The appearance as a concatenation of the pixel intensities of patches around the 9 inner facial feature points. (2) The shape vector was constructed as the normalized pairwise distances between all 36 facial feature points.</p></sec><sec id="s4-5"><title>Face detection</title><p>Each image was converted to JPEG and submitted to the Facial Detection (FD) module of the algorithm. Face detection was achieved using the OpenCV (<xref ref-type="bibr" rid="bib14">Bradski, 2000</xref>) implementation of the Viola–Jones object detection framework (using Haar like features and a cascade of classifiers) (<xref ref-type="bibr" rid="bib59">Viola and Jones, 2001</xref>). The output takes the form of a square bounding box delimiting the area of the picture where the face was found. Pictures containing the faces of healthy relatives (or others) were either discarded or cropped to only conserve regions with the patient face.</p></sec><sec id="s4-6"><title>Facial landmarks annotation</title><p>Manual annotation of the 36 feature points was performed on 3100 of the images in the image collection. These were used as the ground truth reference point for all subsequent training and test sets for evaluations of automated facial landmark annotation accuracies (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>In the second step of the automatic algorithm detected faces were passed to a facial landmark annotation script (<xref ref-type="bibr" rid="bib21">Everingham et al., 2009</xref>) (FLA module), which annotates the face with an initial set of 9 well-characterized (salient) feature points. The 9 landmarks in that set were the medial and lateral canthi of the eyes, each subnasale, columella and the left and right vermilion border lateral midpoints. The FLA used the returned bounding box to approximate the location and size of the face to be annotated. Automatic annotation relies on a generative model of feature point position combined with a discriminative model of appearances. This joint model was based on the parts-based pictorial structure representation introduced by <xref ref-type="bibr" rid="bib23">Fischler and Elschlager (1973)</xref>. For a given bounding box, the FLA module returns both a constellation of 9 landmarks and a corresponding confidence index computed via appearance mismatch with the model. We used this index to implement more robust, accurate and reliable annotation approaches (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) as described below.</p><p>Improved facial landmark annotation was performed with a custom script designed to refine the output from the FD-FLA modules in terms of annotation inaccuracies, false positive and false negative face detection. The images were transformed iteratively by mirror imaging, partial rotations (±45°), and by adding a frame around the image to produce 100 transformations of the original. Each image was subsequently submitted through the FD -FLA modules and returned a constellation of 9 points with associated confidence scores recorded. A consensus map is constructed by confidence weighted averaging of high confidence feature annotations thus reducing the number of spurious annotations and increasing annotation accuracy (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>From the improved 9 facial feature points, we expanded the feature detection to 36 feature points encompassing the points indicated in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>.</p></sec><sec id="s4-7"><title>Consensus of exemplar</title><p>We developed a computational module inspired by <xref ref-type="bibr" rid="bib7">Belhumeur et al. (2011)</xref> to determine the localization of the 36 landmarks. Consensus of exemplar (CoE) relies on part base classifiers used to localize potential candidate points and a database of face exemplars used to introduce a shape prior in the search for the best constellation.</p><p>While only a constellation of 9 feature points (C9) is required to compute the appearance feature vector, the shape feature vector relies on a constellation of 36 feature points (C36) covering the inner face in greater details along with its outlined (Jaw line). Anatomical landmarks covered in C36 are shown in <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>. We used the C9 obtained via the improved FLA module (see previous section) to initiate the automatic search for C36.</p><p>For each of the facial feature points in C36, we delimited a region of interest (ROI) for the algorithm to consider using the following heuristic: 1000 exemplar faces are sample from among the controls in the database (<xref ref-type="table" rid="tbl1">Table 1</xref>). For each exemplar face i, we registered the C9<sub>i</sub> to the C9 of our query face using Procrustes algorithm. Next the sum of squares error was used to sort exemplars in order of accuracy with which C9<sub>i</sub> registered to C9. The top 20 exemplars were used to map their C36<sub>i</sub> to the query face using the transform T<sub>i</sub> obtained during registration. A consensus C36 for the query face was then derived by averaging all registered C36<sub>i</sub>. Finally, for each feature point in C36, we defined a square ROI centered on the consensus point with two palpebral fissures (PF) length dimensions. The PF length was the average between right and left eyes and estimated based on C9.</p><p>The final C36 was derived from the ROIs by using a combination of part based detectors and a consensus of shape exemplars. Support Vector Machine (SVM) classifiers (using Gaussian kernels) were trained to recognize each feature point in C36. We used a database of manually annotated control patients to obtain positive and negative training sets for each part classifier. The feature vector associated with a particular feature point was obtained by cropping a square patch centered on this point and describing its pixel content with a pyramidal histogram of gradients (PHOG).</p><p>Going back to the query face, each of the 36 ROIs was submitted to the corresponding part detector. From this we obtained a set of 20 potential candidates (PC) for each part within the ROI (where candidates were sorted based on the classifier decision values). Next, we randomly sampled exemplars of C36 from the control database and registered them to the query face in order to enforce a shape prior. To avoid spurious outlier PC to drive the registration off, we randomly select PC to represent three randomly selected points from C36. Exemplar C36<sub>i</sub> were registered via Procrustes algorithm to the query face using only these three PC. The registered C36<sub>i</sub> were scored by submitting its feature points to the part classifiers. We retained the top 20 C36<sub>i</sub>. Finally, each feature point of the final C36 for the query was derived independently as a consensus between probability density maps based on the PCs and the classifier decision values over the corresponding ROI.</p></sec><sec id="s4-8"><title>Appearance descriptor feature vector</title><p>Using the set of 9 inner points annotated to the face by the FLA module and refined by the CoE module, four additional points were generated: left and right center of the eye, nasion (between the eyes) and center of the mouth. The points were then used to register the face, via an affine transformation T, to a canonical set of corresponding points (face shape template). Next, circular patches were generated around the canonical points which were mapped back to the original picture using the inverse of T. This process creates 13 ellipses which were then used to crop the image content by bilinear interpolation. Extraction of the patch was performed as in <xref ref-type="bibr" rid="bib21">Everingham et al. (2009)</xref>.</p><p>The appearance feature vector was obtained by a concatenation of the pixel gray-scale content of the 13 cropped patches (size of the feature vector = 1937). While patch content could be further processed before concatenation (gradients, HOG, PHOG, SIFT, Gabor Filter, Local binary Pattern, etc) the gain in terms of discriminative power and relevant feature extracted was negligible compared to the computational cost in terms of memory consumption.</p></sec><sec id="s4-9"><title>Shape descriptor feature vector</title><p>The features vector was built from the constellation of the 36 landmarks annotations from the CoE module. We described the face shape as a vector d, the set of pair-wise distances through the constellation, resulting in a feature vector with 630 elements.</p><p>To compare the different constellations, each was registered via Procrustes transformation to the average constellation of the AAM model (canonical face mesh, see below). The vector of pair-wise distances was then normalized so that any distance variations were relative to the corresponding distances measured on the average control face template.</p></sec><sec id="s4-10"><title>Average faces and morphs using Active Appearance Model</title><p>Introduced in 1998 by <xref ref-type="bibr" rid="bib16">Cootes et al. (1998)</xref>, the active appearance model (AAM) was designed to identify a set of facial landmarks on a given face. This task is achieved by iteratively modifying both the shape and the position (location) of a structured face mesh in which nodes represent target landmarks.</p><p>We constructed a training set by manual annotation of 3100 patient images with the 36 landmarks (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). All constellations were registered together using an iterative Procrustes algorithm. We computed the average constellation and used it as a reference with which to build a canonical face mesh via Delaunay triangulation. Based on the obtained triangulation, we generated a face mesh dividing each patient's face into sub-regions. Using piecewise affine warping, we independently mapped the pixel content of each sub-region to the corresponding triangle in the canonical face mesh. We thus obtained a registered version of each patient's facial appearance. Shapes and appearance registrations were used in a principal component analysis (PCA) to generate both the shape and appearance models for use with the AAM.</p><p>Using the AAM statistical models of shape and appearance that derive from the training of the AAM, we created a visual representation of canonical traits/phenotypes. Average faces were created from the images of patient with the same genetic disorder. AAM models were generated for each group, and after registration of the constellations (annotation) to each we derived the average shape constellation. A face mesh was generated from these constellations by triangulation (Delaunay) and the appearance of each individual was mapped to the average face mesh by piecewise affine warping. Thus, this average face mesh recapitulated the canonical phenotype of the syndromes.</p><p>Furthermore, we created morphs between control and syndrome faces by registering appearance to the average shape of the controls (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). By computing the average across the syndrome group and the control group one can obtain the average appearance of the syndrome A<sub>0s</sub> and control A<sub>0c</sub>. Given that both A<sub>0s</sub> and A<sub>0c</sub> have identical dimensionality, we morphed appearance from one to the other with appearance for frame k where A<sub>k</sub> = A<sub>0s</sub> + k/K*(A<sub>0c</sub> − A<sub>0s</sub>) and where K is the total number of frames. Similarly, we considered the average face mesh of the syndrome group M<sub>0s</sub> and the control group M<sub>0c</sub> to obtain the face mesh at frame k as M<sub>k</sub> = M<sub>0s</sub> + k/K* (M<sub>0c</sub> − M<sub>0s</sub>). Finally, for each frame k, both shape (face mesh) and appearance were combined by piecewise affine warping of A<sub>k</sub> to M<sub>k</sub>.</p></sec><sec id="s4-11"><title>Support Vector Machine classification</title><p>We used SVMs to fine tune both appearance and shape descriptors. Differences in binary classification accuracies allow us to infer relative feature delineation capacity by our descriptors. We employed Libsvm to perform classification. The accuracies based on the raw feature vectors were comparable to accuracies reported in previous studies (<xref ref-type="bibr" rid="bib11">Boehringer et al., 2006</xref>, <xref ref-type="bibr" rid="bib10">2011</xref>).</p></sec><sec id="s4-12"><title>Binary classification</title><p>SVM classifiers were trained on both shape and appearance feature vectors separately for each 36 pairwise combination of control and 8 syndromes (<xref ref-type="table" rid="tbl1">Table 1</xref>). Each binary classification was repeated 10 times with randomly generated positive/negative training and test sets with a 4:1 ratio. The linear kernel SVM classification accuracies using the final shape and appearance descriptors are shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>. We estimate a total accuracy when fusing information from shape and appearance feature vectors based SVMs by summing decision values returned by the shape and appearance classifiers respectively.</p><p>In all classification experiments, we used both original and mirror versions of each image. Given a binary experiment to distinguish group G1 from G2 (where G refers to a set of images of a syndrome), we randomly partitioned the sets into training and test sets: G1<sub>tr</sub>, G1<sub>te</sub>, G2<sub>tr</sub>, and G2<sub>te</sub> respectively. From these we generated the corresponding mirror image sets: G1<sub>tr</sub><sup>m</sup>, G1<sub>te</sub><sup>m</sup>, G2<sub>tr</sub><sup>m</sup>, and G2<sub>te</sub><sup>m</sup>. The SVM model was trained using G1<sub>tr</sub> + G1<sub>tr</sub><sup>m</sup> as the positive set and G2<sub>tr</sub> + G2<sub>tr</sub><sup>m</sup> as the negative set. Decision values were tuned to give equal error rates (the number of false positives = number of false negatives). With the trained SVM models, we next submitted the sets G1<sub>te</sub> + G2<sub>te</sub> and G1<sub>te</sub><sup>m</sup> + G2<sub>te</sub><sup>m</sup> separately to the classifier. Thus, for each instance in the test set we obtained two decision values. The final classification was determined by the sum of the decision values.</p></sec><sec id="s4-13"><title>Forced choice classification</title><p>We next applied the classification problem to assigning a face as being one of the 8 syndromes (<xref ref-type="table" rid="tbl1">Table 1</xref>). Each of the 8 syndrome groups was randomly split into training and test sets with a ratio of 4:1. We used the training sets to train a linear binary SVM classifier for each of the 28 pairs. Each image in the training set was submitted to all 28 classifiers. The decision value was returned by each classified and used as a probabilistic estimate P for the test instance to belong to the positive class. Thus, after presenting instance <italic>i</italic> to the binary classifier distinguishing syndrome <italic>j</italic> (positive) from syndrome <italic>k</italic> (negative), we assigned <italic>i</italic> a vote of weight P for syndrome <italic>j</italic> and a vote of weight 1-P for syndrome <italic>k</italic>. After summation of the votes from the 28 classifiers, the instance <italic>i</italic> was labeled as belonging to the syndrome diagnosis with the highest probability.</p><p>The confusion matrix averaged from 10 repeats of the forced choice experiment is shown in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p></sec><sec id="s4-14"><title>Clinical Face Phenotype Space construction</title><p>We performed PCA on the shape and appearance feature vectors to reduce dimensionality from 2567 to a concatenation of 340 orthogonal vectors. This was then used to transform the feature space with Large Margin Nearest Neighbor (LMNN, <xref ref-type="bibr" rid="bib61">Weinberger and Saul, 2009</xref>). LMNN is an optimization algorithm that uses a training set of pairs of vector labels (x<sub>i</sub>; l<sub>i</sub>) and learns a Mahalanobis distance that maximizes the kNN classification over a training set. Note that even if the system only considers local information (i.e., number of intruders for each instance) the final metric is global. The Mahalanobis distance was computed as dist(x<sub>i</sub>; x<sub>j</sub>) = (x<sub>i</sub> − x<sub>j</sub>)<sup>t</sup>L<sup>t</sup>L(x<sub>i</sub> − x<sub>j</sub>) where L is a linear matrix. It is equivalent to the Euclidean distance taken in the space after transformation by L. That is to say, LMNN linearly transforms dimensions in feature space to maximize the margins separating classes of labeled instances. This should, in principle expand dimensions with phenotypically relevant information and compresses dimensions uninformative for classification.</p><p>To validate the characteristics of the transformation of spurious and phenotypic vectors in Clinical Face Phenotype Space, we performed a series of experiments based on projected 3D faces. We used the 3D facial model proposed by Blanz &amp; Vetter (<xref ref-type="bibr" rid="bib9">Blanz, 2006</xref>) <ext-link ext-link-type="uri" xlink:href="http://faces.cs.unibas.ch/bfm/">http://faces.cs.unibas.ch/bfm/</ext-link> which allowed us to create faces with a direct control over shape, appearance, lighting, and facial pose. We synthesized 5 test faces at random moving along the first 15 components of both the shape and appearance models. For each face, we generated a set of 20 images for each combination of 5 head rotations and 4 lighting conditions. We use these simulated images to compare similarity measures in the raw feature space and in Clinical Face Phenotype Space (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). We performed a reorientation of the raw feature vectors and Clinical Face Phenotype Space using PCA without dimensionality reduction in order to sort the dimensions by variation magnitude. This allowed us to assess the relative contributions of phenotypic variation and spurious variations to clustering of faces. The strongest influences on clustering would be expected to be encoded in the first modes of variation. Placing the synthetic faces in the reoriented spaces allowed us to describe the PCA signatures of phenotypic variations (shape, appearance) and spurious variation (lighting, head pose).</p></sec><sec id="s4-15"><title>Clinical Face Phenotype Space validation and visualization</title><p>We used several dimensionality reduction methods and metrics for visualization and estimation of the properties of Clinical Face Phenotype Space. We developed an estimate of search space reduction to determine the improvements in clustering in Clinical Face Phenotype Space controlling for the composition of the database. Essentially, this calculates the degree to which intruders in a nearest neighbor search between instances of the same syndrome are excluded in Clinical Face Phenotype Space. This equates to a factor estimate of increased clustering, CIF (details of the procedure are provided below).</p><p>We used a 20 nearest neighbor linkage map to visualize Clinical Face Phenotype Space using force directed graphs implemented through Gephi (<xref ref-type="bibr" rid="bib5">Bastian et al., 2009</xref>).</p><p>Protein–protein interaction data were obtained from DAPPLE (<xref ref-type="bibr" rid="bib50">Rossin et al., 2011</xref>). After conversion to Ensembl gene IDs, 126,586 interactions between 10,442 genes remained. We considered the data as a network of genes, with edges denoting an interaction. The shortest paths between two genes were computed using Dijkstra's algorithm (<xref ref-type="bibr" rid="bib20">Dijkstra, 1959</xref>). We calculated the median pairwise Euclidean distance between syndromes in Clinical Face Phenotype Space. The correlation between these two data sets underlies <xref ref-type="fig" rid="fig5">Figure 5</xref>. Clinical Face Phenotype Space distance between groups was tested using Kruskal–Wallis (<xref ref-type="bibr" rid="bib37">Kruskal and Wallis, 1952</xref>) tests with Bonferroni (<xref ref-type="bibr" rid="bib12">Bonferroni, 1935</xref>, <xref ref-type="bibr" rid="bib13">1936</xref>) multiple testing correction.</p></sec><sec id="s4-16"><title>Estimating improvements in clustering</title><p>Next, we performed estimations of clustering of syndromes in face space. Initial tests using kNN-classifiers showed that the classification accuracies were heavily dependent on spread and cardinality of the syndrome in the database. We went on to develop an estimate of search space reduction, hereafter referred to Clustering Improvement Factor (CIF), to determine the improvements in clustering in Clinical Face Phenotype Space controlling for the composition of the database (a simulated example is provided in <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>).</p><p>We considered a syndrome with N<sub>p</sub> positive and N<sub>n</sub> negative instances in the Clinical Face Phenotype Space. We defined the CIF as<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mtext>CIF</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>expected rank </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mtext> of nearest positive match under random ranking</mml:mtext></mml:mrow><mml:mrow><mml:mtext>observed average rank </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mtext> of nearest positive match</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>O</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>with the average taken across all instances of the syndrome. O(r) was calculated from the observations in the Clinical Face Phenotype Space. To compute E(r), we used probability theory as follows.</p><p>Under a random ranking for a given positive query, the other N<sub>p</sub>−1 positive instances are each placed independently among the N<sub>n</sub> negative instances, with a uniform discrete probability distribution. We defined the random variable N<sub>i</sub> as the number of negative instances ranked higher than the first positive instance, so N<sub>i</sub> takes integer values <inline-formula><mml:math id="inf1"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>For a given positive query, the expected rank of the nearest positive match is the expected value of N<sub>i</sub>+1, denoted by E(N<sub>i</sub>)+1. To calculate E(N<sub>i</sub>), we used the definition of expectation:<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mtext>0</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>jPr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula>since N<sub>i</sub> can only take non-negative integer values, for each possible value j between 0 and N<sub>n</sub>,<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>+</mml:mo><mml:mtext>1</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula>Substituting this in the formula for E(N<sub>i</sub>),<disp-formula id="equ4"><mml:math id="m4"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mtext>0</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>j</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>+</mml:mo><mml:mtext>1</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Rewriting the sum,<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>jPr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mtext>j</mml:mtext><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For a given number j, Pr(N<sub>i</sub> ≥ j) is the probability that all positive instances were placed after j negative instances. For any given individual positive instance, such placement has probability <inline-formula><mml:math id="inf2"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Since placement of all positive instances is independent, this gives <inline-formula><mml:math id="inf3"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>p</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>Therefore, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>p</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>Finally, this gives <inline-formula><mml:math id="inf5"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>p</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-17"><title>Querying Clinical Face Phenotype Space</title><p>We developed two methods to retrieve information about the neighborhood of a given face placed in the Clinical Face Phenotype Space. Firstly, we assigned a syndrome classification based on the identity of its k nearest neighbors in Clinical Face Phenotype Space. Based on the neighbors' labels a list of syndromes to which the new face could belong was created. The number of neighbors supporting each hypothesis was compared with the probability to see N instances of that syndrome when sampling k from the population of faces in Clinical Face Phenotype Space.</p><p>Secondly, we estimate the relative similarity between specific faces given the density of points in a local region of Clinical Face Phenotype Space. This is calculated as <inline-formula><mml:math id="inf6"><mml:mrow><mml:mtext>p</mml:mtext><mml:mn>0</mml:mn><mml:mtext>p</mml:mtext><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:msub><mml:mtext>d</mml:mtext><mml:mrow><mml:mn>0</mml:mn><mml:mtext>,</mml:mtext><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:msqrt><mml:mrow><mml:msub><mml:mtext>d</mml:mtext><mml:mn>0</mml:mn></mml:msub><mml:msub><mml:mtext>d</mml:mtext><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>, where d<sub>0,1</sub> is the similarity measure between the query and its neighbor, d<sub>0</sub> is the average of similarities between the query and k = 20 neighbors and d<sub>1</sub> is the average of similarities between the neighbor of the query and k of the neighbor's neighbors. <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref> illustrates the method and metrics using a simulated example.</p><p>We see Clinical Face Phenotype Space as a means to facilitate collaborative investigations of genetic diseases between clinicians. Of course, sharing of data raises questions regarding ethics approval and data security. These questions are tightly linked to the debate of how clinical sequencing information should be treated in global health care systems. We anticipate that it would be suitable for future implementations of Clinical Face Phenotype Space to follow similar guidelines as for clinical sequencing data.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Dr Zameel Cader for advice at the initiation of this project. Also, we would like to thank Professor Raoul MC Hennekam for kindly allowing us access to the Gorlin collection. We are grateful to Dr Bronwyn Kerr for suggesting publication with RAS/MEK mutation patient images. Author contributions: CN, AZ, CPP, CW, and DRF conceived the study. QF collected the database and annotated the images. QF and CN wrote the code. QF, AZ, JS, and CN performed the analyses. DRF validated the syndrome classification of images. CN drafted the manuscript. All co-authors edited and approved the final manuscript.<italic/></p></ack><sec sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="conflict" id="conf1"><p>CPP: Senior editor, <italic>eLife</italic>.</p></fn><fn fn-type="conflict" id="conf2"><p>The other authors declare that no competing interests exist.</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>QF, Collected the database, annotated the images, wrote the code and performed analyses, edited and revised the manuscript., Acquisition of data, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con2"><p>JS, Performed analyses and edited the manuscript., Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con3"><p>CW, Conceived the study and edited the manuscript., Conception and design</p></fn><fn fn-type="con" id="con4"><p>DRFP, Conceived the study, validated the syndrome classification of images and edited the manuscript., Conception and design, Acquisition of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con5"><p>CPP, Conceived the study and edited the manuscript., Conception and design, Drafting or revising the article</p></fn><fn fn-type="con" id="con6"><p>AZ, Conceived the study, performed data analysis and edited the manuscript., Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn><fn fn-type="con" id="con7"><p>CN, Conceived the study, wrote code, performed analyses and drafted the manuscript., Conception and design, Analysis and interpretation of data, Drafting or revising the article</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The manner and method by which images were collected from publically available sources and stored were acceptable research practices and do not require special consent from a Research Ethics Committee. Advice from legal services, research ethics board members and the Information Commissioner's Office (UK) was sought in arriving at this conclusion.</p></fn></fn-group></sec><sec sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="SD1-data"><object-id pub-id-type="doi">10.7554/eLife.02020.007</object-id><label>Supplementary file 1.</label><caption><p>Tinyurl links to sources for the database. Prefix the 7 characters with <ext-link ext-link-type="uri" xlink:href="http://tinyurl.com/">http://tinyurl.com/</ext-link>. Links are expected to decay with time; the full dataset will be released to researchers at the discretion of a Data Access Committee.</p><p><bold>DOI:</bold><ext-link ext-link-type="doi" xlink:href="10.7554/eLife.02020.007">http://dx.doi.org/10.7554/eLife.02020.007</ext-link></p></caption><media mime-subtype="doc" mimetype="application" xlink:href="elife-02020-supp1-v1.doc"/></supplementary-material><sec sec-type="datasets"><title>Major dataset</title><p>The following dataset was generated:</p><p><related-object content-type="generated-dataset" document-id="Dataset ID and/or url" document-id-type="dataset" document-type="data" id="dataro1"><name><surname>Ferry</surname><given-names>Q</given-names></name>, <name><surname>Steinberg</surname><given-names>J</given-names></name>, <name><surname>Webber</surname><given-names>C</given-names></name>, <name><surname>FitzPatrick</surname><given-names>DR</given-names></name>, <name><surname>Ponting</surname><given-names>CP</given-names></name>, <name><surname>Zisserman</surname><given-names>A</given-names></name>, <name><surname>Nellåker</surname><given-names>C</given-names></name>, <year>2014</year><x>, </x><source>Diagnostically relevant facial gestalt information from ordinary photos database.</source><comment>Original database, excluding the Gorlin collection, and previously published images (which are available from the cited original publications) can be requested by contacting CN (<email>christoffer.nellaker@dpag.ox.ac.uk</email>). Requests will be assessed by a Data Access Committee (DAC) comprised of CPP, DRF, AZ, CN and Dr Zameel Cader of the Division of Clinical Neurology, University of Oxford. The DAC will make data available to researchers in good standing with the relevant institution and funding agencies (i.e., no known sanctions). The data are provided without copyright.</comment></related-object></p></sec></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abecasis</surname><given-names>GR</given-names></name><name><surname>Auton</surname><given-names>A</given-names></name><name><surname>Brooks</surname><given-names>LD</given-names></name><name><surname>Depristo</surname><given-names>MA</given-names></name><name><surname>Durbin</surname><given-names>RM</given-names></name><name><surname>Handsaker</surname><given-names>RE</given-names></name><name><surname>Kang</surname><given-names>HM</given-names></name><name><surname>Marth</surname><given-names>GT</given-names></name><name><surname>Mcvean</surname><given-names>GA</given-names></name></person-group><year>2012</year><article-title>An integrated map of genetic variation from 1,092 human genomes</article-title><source>Nature</source><volume>491</volume><fpage>56</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1038/nature11632</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aldridge</surname><given-names>K</given-names></name><name><surname>George</surname><given-names>ID</given-names></name><name><surname>Cole</surname><given-names>KK</given-names></name><name><surname>Austin</surname><given-names>JR</given-names></name><name><surname>Takahashi</surname><given-names>TN</given-names></name><name><surname>Duan</surname><given-names>Y</given-names></name><name><surname>Miles</surname><given-names>JH</given-names></name></person-group><year>2011</year><article-title>Facial phenotypes in subgroups of prepubertal boys with autism spectrum disorders are correlated with clinical phenotypes</article-title><source>Molecular Autism</source><volume>2</volume><fpage>15</fpage><pub-id pub-id-type="doi">10.1186/2040-2392-2-15</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allanson</surname><given-names>JE</given-names></name><name><surname>Bohring</surname><given-names>A</given-names></name><name><surname>Dorr</surname><given-names>HG</given-names></name><name><surname>Dufke</surname><given-names>A</given-names></name><name><surname>Gillessen-Kaesbach</surname><given-names>G</given-names></name><name><surname>Horn</surname><given-names>D</given-names></name><name><surname>Konig</surname><given-names>R</given-names></name><name><surname>Kratz</surname><given-names>CP</given-names></name><name><surname>Kutsche</surname><given-names>K</given-names></name><name><surname>Pauli</surname><given-names>S</given-names></name><name><surname>Raskin</surname><given-names>S</given-names></name><name><surname>Rauch</surname><given-names>A</given-names></name><name><surname>Turner</surname><given-names>A</given-names></name><name><surname>Wieczorek</surname><given-names>D</given-names></name><name><surname>Zenker</surname><given-names>M</given-names></name></person-group><year>2010</year><article-title>The face of Noonan syndrome: does phenotype predict genotype</article-title><source>American Journal of Medical Genetics</source><volume>152A</volume><fpage>1960</fpage><lpage>1966</lpage><pub-id pub-id-type="doi">10.1002/ajmg.a.33518</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baird</surname><given-names>PA</given-names></name><name><surname>Anderson</surname><given-names>TW</given-names></name><name><surname>Newcombe</surname><given-names>HB</given-names></name><name><surname>Lowry</surname><given-names>RB</given-names></name></person-group><year>1988</year><article-title>Genetic disorders in children and young adults: a population study</article-title><source>American Journal of Human Genetics</source><volume>42</volume><fpage>677</fpage><lpage>693</lpage></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastian</surname><given-names>M</given-names></name><name><surname>Heymann</surname><given-names>S</given-names></name><name><surname>Jacomy</surname><given-names>M</given-names></name></person-group><year>2009</year><article-title>Gephi: An open source software for exploring and manipulating networks</article-title><source>AAAI Publications, Third International AAAI Conference on Weblogs and Social Media</source></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baynam</surname><given-names>G</given-names></name><name><surname>Walters</surname><given-names>M</given-names></name><name><surname>Claes</surname><given-names>P</given-names></name><name><surname>Kung</surname><given-names>S</given-names></name><name><surname>Lesouef</surname><given-names>P</given-names></name><name><surname>Dawkins</surname><given-names>H</given-names></name><name><surname>Gillett</surname><given-names>D</given-names></name><name><surname>Goldblatt</surname><given-names>J</given-names></name></person-group><year>2013</year><article-title>The facial evolution: looking backward and moving forward</article-title><source>Human Mutation</source><volume>34</volume><fpage>14</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1002/humu.22219</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Belhumeur</surname><given-names>PN</given-names></name><name><surname>Jacobs</surname><given-names>DW</given-names></name><name><surname>Kriegman</surname><given-names>DJ</given-names></name><name><surname>Kumar</surname><given-names>N</given-names></name></person-group><year>2011</year><article-title>Localizing parts of faces using a consensus of exemplars. Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition</article-title><publisher-loc>Washington, DC, USA</publisher-loc><publisher-name>IEEE Computer Society</publisher-name><fpage>545</fpage><lpage>552</lpage></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bertola</surname><given-names>DR</given-names></name><name><surname>Pereira</surname><given-names>AC</given-names></name><name><surname>Brasil</surname><given-names>AS</given-names></name><name><surname>Albano</surname><given-names>LM</given-names></name><name><surname>Kim</surname><given-names>CA</given-names></name><name><surname>Krieger</surname><given-names>JE</given-names></name></person-group><year>2007</year><article-title>Further evidence of genetic heterogeneity in Costello syndrome: involvement of the KRAS gene</article-title><source>Journal of Human Genetics</source><volume>52</volume><fpage>521</fpage><lpage>526</lpage><pub-id pub-id-type="doi">10.1007/s10038-007-0146-1</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blanz</surname><given-names>V</given-names></name></person-group><year>2006</year><article-title>Face recognition based on a 3D Morphable model. Proceedings Of the 7th International Conference of Automatic Face and Gesture Recognition</article-title><fpage>617</fpage><lpage>622</lpage></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehringer</surname><given-names>S</given-names></name><name><surname>Guenther</surname><given-names>M</given-names></name><name><surname>Sinigerova</surname><given-names>S</given-names></name><name><surname>Wurtz</surname><given-names>RP</given-names></name><name><surname>Horsthemke</surname><given-names>B</given-names></name><name><surname>Wieczorek</surname><given-names>D</given-names></name></person-group><year>2011</year><article-title>Automated syndrome detection in a set of clinical facial photographs</article-title><source>American Journal of Medical Genetics</source><volume>155A</volume><fpage>2161</fpage><lpage>2169</lpage><pub-id pub-id-type="doi">10.1002/ajmg.a.34157</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehringer</surname><given-names>S</given-names></name><name><surname>Vollmar</surname><given-names>T</given-names></name><name><surname>Tasse</surname><given-names>C</given-names></name><name><surname>Wurtz</surname><given-names>RP</given-names></name><name><surname>Gillessen-Kaesbach</surname><given-names>G</given-names></name><name><surname>Horsthemke</surname><given-names>B</given-names></name><name><surname>Wieczorek</surname><given-names>D</given-names></name></person-group><year>2006</year><article-title>Syndrome identification based on 2D analysis software</article-title><source>European Journal of Human Genetics</source><volume>14</volume><fpage>1082</fpage><lpage>1089</lpage><pub-id pub-id-type="doi">10.1038/sj.ejhg.5201673</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonferroni</surname><given-names>CE</given-names></name></person-group><year>1935</year><article-title>Il calcolo delle assicurazioni su gruppi di teste</article-title><source>Studi in Onore del Professore Salvatore Ortu Carboni</source><publisher-loc>Rome, Italy</publisher-loc></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonferroni</surname><given-names>CE</given-names></name></person-group><year>1936</year><article-title>Teoria statistica delle classi e calcolo delle probabilità</article-title><source>Pubblicazioni del R Istituto Superiore di Scienze Economiche e Commerciali di Firenze</source><fpage>3</fpage><lpage>62</lpage></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bradski</surname><given-names>G.</given-names></name></person-group><year>2000</year><article-title>The OpenCV Library</article-title><source>Dr. Dobb's Journal of Software Tools</source></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buckley</surname><given-names>PF</given-names></name><name><surname>Dean</surname><given-names>D</given-names></name><name><surname>Bookstein</surname><given-names>FL</given-names></name><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Yerukhimovich</surname><given-names>M</given-names></name><name><surname>Min</surname><given-names>KJ</given-names></name><name><surname>Singer</surname><given-names>B</given-names></name></person-group><year>2005</year><article-title>A three-dimensional morphometric study of craniofacial shape in schizophrenia</article-title><source>The American Journal of Psychiatry</source><volume>162</volume><fpage>606</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1176/appi.ajp.162.3.606</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cootes</surname><given-names>TF</given-names></name><name><surname>Edwards</surname><given-names>GJ</given-names></name><name><surname>Taylor</surname><given-names>CJ</given-names></name></person-group><year>1998</year><article-title>Active appearance models. IEEE Transactions on Pattern Analysis and Machine Intelligence</article-title><publisher-name>Springer</publisher-name><fpage>484</fpage><lpage>498</lpage></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cordeddu</surname><given-names>V</given-names></name><name><surname>Di Schiavi</surname><given-names>E</given-names></name><name><surname>Pennacchio</surname><given-names>LA</given-names></name><name><surname>Ma'ayan</surname><given-names>A</given-names></name><name><surname>Sarkozy</surname><given-names>A</given-names></name><name><surname>Fodale</surname><given-names>V</given-names></name><name><surname>Cecchetti</surname><given-names>S</given-names></name><name><surname>Cardinale</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>J</given-names></name><name><surname>Schackwitz</surname><given-names>W</given-names></name><name><surname>Lipzen</surname><given-names>A</given-names></name><name><surname>Zampino</surname><given-names>G</given-names></name><name><surname>Mazzanti</surname><given-names>L</given-names></name><name><surname>Digilio</surname><given-names>MC</given-names></name><name><surname>Martinelli</surname><given-names>S</given-names></name><name><surname>Flex</surname><given-names>E</given-names></name><name><surname>Lepri</surname><given-names>F</given-names></name><name><surname>Bartholdi</surname><given-names>D</given-names></name><name><surname>Kutsche</surname><given-names>K</given-names></name><name><surname>Ferrero</surname><given-names>GB</given-names></name><name><surname>Anichini</surname><given-names>C</given-names></name><name><surname>Selicorni</surname><given-names>A</given-names></name><name><surname>Rossi</surname><given-names>C</given-names></name><name><surname>Tenconi</surname><given-names>R</given-names></name><name><surname>Zenker</surname><given-names>M</given-names></name><name><surname>Merlo</surname><given-names>D</given-names></name><name><surname>Dallapiccola</surname><given-names>B</given-names></name><name><surname>Iyengar</surname><given-names>R</given-names></name><name><surname>Bazzicalupo</surname><given-names>P</given-names></name><name><surname>Gelb</surname><given-names>BD.</given-names></name><name><surname>Tartaglia</surname><given-names>M</given-names></name></person-group><year>2009</year><article-title>Mutation of SHOC2 promotes aberrant protein N-myristoylation and causes Noonan-like syndrome with loose anagen hair</article-title><source>Nature Genetics</source><volume>41</volume><fpage>1022</fpage><lpage>1026</lpage><pub-id pub-id-type="doi">10.1038/ng.425</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dalal</surname><given-names>AB</given-names></name><name><surname>Phadke</surname><given-names>SR</given-names></name></person-group><year>2007</year><article-title>Morphometric analysis of face in dysmorphology</article-title><source>Computer Methods and Programs in Biomedicine</source><volume>85</volume><fpage>165</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.cmpb.2006.10.005</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>de Ligt</surname><given-names>J</given-names></name><name><surname>Willemsen</surname><given-names>MH</given-names></name><name><surname>Van Bon</surname><given-names>BW</given-names></name><name><surname>Kleefstra</surname><given-names>T</given-names></name><name><surname>Yntema</surname><given-names>HG</given-names></name><name><surname>Kroes</surname><given-names>T</given-names></name><name><surname>Vulto-Van Silfhout</surname><given-names>AT</given-names></name><name><surname>Koolen</surname><given-names>DA</given-names></name><name><surname>De Vries</surname><given-names>P</given-names></name><name><surname>Gilissen</surname><given-names>C</given-names></name><name><surname>Del Rosario</surname><given-names>M</given-names></name><name><surname>Hoischen</surname><given-names>A</given-names></name><name><surname>Scheffer</surname><given-names>H</given-names></name><name><surname>De Vries</surname><given-names>BB</given-names></name><name><surname>Brunner</surname><given-names>HG</given-names></name><name><surname>Veltman</surname><given-names>JA</given-names></name><name><surname>Vissers</surname><given-names>LE</given-names></name></person-group><year>2012</year><article-title>Diagnostic exome sequencing in persons with severe intellectual disability</article-title><source>The New England Journal of Medicine</source><volume>367</volume><fpage>1921</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1056/NEJMoa1206524</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dijkstra</surname><given-names>EW</given-names></name></person-group><year>1959</year><article-title>A note on two problems in connexion with graphs</article-title><source>Numerische Mathematik</source><volume>1</volume><fpage>269</fpage><lpage>271</lpage><pub-id pub-id-type="doi">10.1007/BF01386390</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Everingham</surname><given-names>M</given-names></name><name><surname>Sivic</surname><given-names>J</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year>2009</year><article-title>Taking the bite out of automatic naming of characters in TV video</article-title><source>Image and Vision Computing</source><volume>27</volume></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ferrero</surname><given-names>GB</given-names></name><name><surname>Biamino</surname><given-names>E</given-names></name><name><surname>Sorasio</surname><given-names>L</given-names></name><name><surname>Banaudi</surname><given-names>E</given-names></name><name><surname>Peruzzi</surname><given-names>L</given-names></name><name><surname>Forzano</surname><given-names>S</given-names></name><name><surname>Di Cantogno</surname><given-names>LV</given-names></name><name><surname>Silengo</surname><given-names>MC</given-names></name></person-group><year>2007</year><article-title>Presenting phenotype and clinical evaluation in a cohort of 22 Williams-Beuren syndrome patients</article-title><source>European Journal of Medical Genetics</source><volume>50</volume><fpage>327</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1016/j.ejmg.2007.05.005</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fischler</surname><given-names>MA</given-names></name><name><surname>Elschlager</surname><given-names>RA</given-names></name></person-group><year>1973</year><article-title>The representation and matching of pictorial structures</article-title><source>IEEE Transactions on Computer</source><fpage>67</fpage><lpage>92</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goodstadt</surname><given-names>L</given-names></name></person-group><year>2010</year><article-title>Ruffus: a lightweight Python library for computational pipelines</article-title><source>Bioinformatics</source><volume>26</volume><fpage>2778</fpage><lpage>2779</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btq524</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>AM</given-names></name></person-group><year>1962</year><article-title>Abraham Lincoln–a medical appraisal</article-title><source>The Journal of the Kentucky Medical Association</source><volume>60</volume><fpage>249</fpage><lpage>253</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gripp</surname><given-names>KW</given-names></name><name><surname>Lin</surname><given-names>AE</given-names></name><name><surname>Nicholson</surname><given-names>L</given-names></name><name><surname>Allen</surname><given-names>W</given-names></name><name><surname>Cramer</surname><given-names>A</given-names></name><name><surname>Jones</surname><given-names>KL</given-names></name><name><surname>Kutz</surname><given-names>W</given-names></name><name><surname>Peck</surname><given-names>D</given-names></name><name><surname>Rebolledo</surname><given-names>MA</given-names></name><name><surname>Wheeler</surname><given-names>PG</given-names></name><name><surname>Wilson</surname><given-names>W</given-names></name><name><surname>AL-Rahawan</surname><given-names>MM</given-names></name><name><surname>Stabley</surname><given-names>DL</given-names></name><name><surname>Sol-Church</surname><given-names>K</given-names></name></person-group><year>2007</year><article-title>Further delineation of the phenotype resulting from BRAF or MEK1 germline mutations helps differentiate cardio-facio-cutaneous syndrome from Costello syndrome</article-title><source>American Journal of Medical Genetics Part A</source><volume>143A</volume><fpage>1472</fpage><lpage>1480</lpage><pub-id pub-id-type="doi">10.1002/ajmg.a.31815</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammond</surname><given-names>P</given-names></name></person-group><year>2007</year><article-title>The use of 3D face shape modelling in dysmorphology</article-title><source>Archives of Disease in Childhood</source><volume>92</volume><fpage>1120</fpage><lpage>1126</lpage><pub-id pub-id-type="doi">10.1136/adc.2006.103507</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammond</surname><given-names>P</given-names></name><name><surname>Hutton</surname><given-names>TJ</given-names></name><name><surname>Allanson</surname><given-names>JE</given-names></name><name><surname>Buxton</surname><given-names>B</given-names></name><name><surname>Campbell</surname><given-names>LE</given-names></name><name><surname>Clayton-Smith</surname><given-names>J</given-names></name><name><surname>Donnai</surname><given-names>D</given-names></name><name><surname>Karmiloff-Smith</surname><given-names>A</given-names></name><name><surname>Metcalfe</surname><given-names>K</given-names></name><name><surname>Murphy</surname><given-names>KC</given-names></name><name><surname>Patton</surname><given-names>M</given-names></name><name><surname>Pober</surname><given-names>B</given-names></name><name><surname>Prescott</surname><given-names>K</given-names></name><name><surname>Scambler</surname><given-names>P</given-names></name><name><surname>Shaw</surname><given-names>A</given-names></name><name><surname>Smith</surname><given-names>AC</given-names></name><name><surname>Stevens</surname><given-names>AF</given-names></name><name><surname>Temple</surname><given-names>IK</given-names></name><name><surname>Hennekam</surname><given-names>R</given-names></name><name><surname>Tassabehji</surname><given-names>M</given-names></name></person-group><year>2005</year><article-title>Discriminating power of localized three-dimensional facial morphology</article-title><source>American Journal of Human Genetics</source><volume>77</volume><fpage>999</fpage><lpage>1010</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammond</surname><given-names>P</given-names></name><name><surname>Suttie</surname><given-names>M</given-names></name></person-group><year>2012</year><article-title>Large-scale objective phenotyping of 3D facial morphology</article-title><source>Human Mutation</source><volume>33</volume><fpage>817</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1002/humu.22054</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hart</surname><given-names>TC</given-names></name><name><surname>Hart</surname><given-names>PS</given-names></name></person-group><year>2009</year><article-title>Genetic studies of craniofacial anomalies: clinical implications and applications</article-title><source>Orthodontics &amp; Craniofacial Research</source><volume>12</volume><fpage>212</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1111/j.1601-6343.2009.01455.x</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennekam</surname><given-names>RC</given-names></name><name><surname>Biesecker</surname><given-names>LG</given-names></name></person-group><year>2012</year><article-title>Next-generation sequencing demands next-generation phenotyping</article-title><source>Human Mutation</source><volume>33</volume><fpage>884</fpage><lpage>886</lpage><pub-id pub-id-type="doi">10.1002/humu.22048</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennessy</surname><given-names>RJ</given-names></name><name><surname>Baldwin</surname><given-names>PA</given-names></name><name><surname>Browne</surname><given-names>DJ</given-names></name><name><surname>Kinsella</surname><given-names>A</given-names></name><name><surname>Waddington</surname><given-names>JL</given-names></name></person-group><year>2007</year><article-title>Three-dimensional laser surface imaging and geometric morphometrics resolve frontonasal dysmorphology in schizophrenia</article-title><source>Biological Psychiatry</source><volume>61</volume><fpage>1187</fpage><lpage>1194</lpage><pub-id pub-id-type="doi">10.1016/j.biopsych.2006.08.045</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennessy</surname><given-names>RJ</given-names></name><name><surname>Mclearie</surname><given-names>S</given-names></name><name><surname>Kinsella</surname><given-names>A</given-names></name><name><surname>Waddington</surname><given-names>JL</given-names></name></person-group><year>2006</year><article-title>Facial shape and asymmetry by three-dimensional laser surface scanning covary with cognition in a sexually dimorphic manner</article-title><source>The Journal of Neuropsychiatry and Clinical Neurosciences</source><volume>18</volume><fpage>73</fpage><lpage>80</lpage><pub-id pub-id-type="doi">10.1176/appi.neuropsych.18.1.73</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopper</surname><given-names>RA</given-names></name><name><surname>Kapadia</surname><given-names>H</given-names></name><name><surname>Morton</surname><given-names>T</given-names></name></person-group><year>2013</year><article-title>Normalizing facial ratios in apert syndrome patients with Le Fort II midface distraction and simultaneous zygomatic repositioning</article-title><source>Plastic and Reconstructive Surgery</source><volume>132</volume><fpage>129</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1097/PRS.0b013e318290fa8a</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleefstra</surname><given-names>T</given-names></name><name><surname>Wortmann</surname><given-names>SB</given-names></name><name><surname>Rodenburg</surname><given-names>RJ</given-names></name><name><surname>Bongers</surname><given-names>EM</given-names></name><name><surname>Hadzsiev</surname><given-names>K</given-names></name><name><surname>Noordam</surname><given-names>C</given-names></name><name><surname>Van Den Heuvel</surname><given-names>LP</given-names></name><name><surname>Nillesen</surname><given-names>WM</given-names></name><name><surname>Hollody</surname><given-names>K</given-names></name><name><surname>Gillessen-Kaesbach</surname><given-names>G</given-names></name><name><surname>Lammens</surname><given-names>M</given-names></name><name><surname>Smeitink</surname><given-names>JA</given-names></name><name><surname>Van der Burgt</surname><given-names>I</given-names></name><name><surname>Morava</surname><given-names>E</given-names></name></person-group><year>2011</year><article-title>Mitochondrial dysfunction and organic aciduria in five patients carrying mutations in the Ras-MAPK pathway</article-title><source>European Journal of Human Genetics</source><volume>19</volume><fpage>138</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1038/ejhg.2010.171</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kratz</surname><given-names>CP</given-names></name><name><surname>Zampino</surname><given-names>G</given-names></name><name><surname>Kriek</surname><given-names>M</given-names></name><name><surname>Kant</surname><given-names>SG</given-names></name><name><surname>Leoni</surname><given-names>C</given-names></name><name><surname>Pantaleoni</surname><given-names>F</given-names></name><name><surname>Oudesluys-Murphy</surname><given-names>AM</given-names></name><name><surname>Di Rocco</surname><given-names>C</given-names></name><name><surname>Kloska</surname><given-names>SP</given-names></name><name><surname>Tartaglia</surname><given-names>M</given-names></name><name><surname>Zenker</surname><given-names>M</given-names></name></person-group><year>2009</year><article-title>Craniosynostosis in patients with Noonan syndrome caused by germline KRAS mutations</article-title><source>American Journal of Medical Genetics Part A</source><volume>149A</volume><fpage>1036</fpage><lpage>1040</lpage><pub-id pub-id-type="doi">10.1002/ajmg.a.32786</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kruskal</surname><given-names>WH</given-names></name><name><surname>Wallis</surname><given-names>WA</given-names></name></person-group><year>1952</year><article-title>Use of ranks in one-Criterion variance analysis</article-title><source>Journal of the American Statistical Association</source><volume>47</volume><fpage>583</fpage><lpage>621</lpage><pub-id pub-id-type="doi">10.1080/01621459.1952.10483441</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lepri</surname><given-names>F</given-names></name><name><surname>De Luca</surname><given-names>A</given-names></name><name><surname>Stella</surname><given-names>L</given-names></name><name><surname>Rossi</surname><given-names>C</given-names></name><name><surname>Baldassarre</surname><given-names>G</given-names></name><name><surname>Pantaleoni</surname><given-names>F</given-names></name><name><surname>Cordeddu</surname><given-names>V</given-names></name><name><surname>Williams</surname><given-names>BJ</given-names></name><name><surname>Dentici</surname><given-names>ML</given-names></name><name><surname>Caputo</surname><given-names>V</given-names></name><name><surname>Venanzi</surname><given-names>S</given-names></name><name><surname>Bonaguro</surname><given-names>M</given-names></name><name><surname>Kavamura</surname><given-names>I</given-names></name><name><surname>Faienza</surname><given-names>MF</given-names></name><name><surname>Pilotta</surname><given-names>A</given-names></name><name><surname>Stanzial</surname><given-names>F</given-names></name><name><surname>Faravelli</surname><given-names>F</given-names></name><name><surname>Gabrielli</surname><given-names>O</given-names></name><name><surname>Marino</surname><given-names>B</given-names></name><name><surname>Neri</surname><given-names>G</given-names></name><name><surname>Silengo</surname><given-names>MC</given-names></name><name><surname>Ferrero</surname><given-names>GB</given-names></name><name><surname>Torrrente</surname><given-names>I</given-names></name><name><surname>Selicorni</surname><given-names>A</given-names></name><name><surname>Mazzanti</surname><given-names>L</given-names></name><name><surname>Digilio</surname><given-names>MC</given-names></name><name><surname>Zampino</surname><given-names>G</given-names></name><name><surname>Dallapiccola</surname><given-names>B</given-names></name><name><surname>Gelb</surname><given-names>BD</given-names></name><name><surname>Tartaglia</surname><given-names>M</given-names></name></person-group><year>2011</year><article-title>SOS1 mutations in Noonan syndrome: molecular spectrum, structural insights on pathogenic effects, and genotype-phenotype correlations</article-title><source>Human Mutation</source><volume>32</volume><fpage>760</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1002/humu.21492</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loos</surname><given-names>HS</given-names></name><name><surname>Wieczorek</surname><given-names>D</given-names></name><name><surname>Wurtz</surname><given-names>RP</given-names></name><name><surname>Von Der Malsburg</surname><given-names>C</given-names></name><name><surname>Horsthemke</surname><given-names>B</given-names></name></person-group><year>2003</year><article-title>Computer-based recognition of dysmorphic faces</article-title><source>European Journal of Human Genetics</source><volume>11</volume><fpage>555</fpage><lpage>560</lpage><pub-id pub-id-type="doi">10.1038/sj.ejhg.5200997</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makita</surname><given-names>Y</given-names></name><name><surname>Narumi</surname><given-names>Y</given-names></name><name><surname>Yoshida</surname><given-names>M</given-names></name><name><surname>Niihori</surname><given-names>T</given-names></name><name><surname>Kure</surname><given-names>S</given-names></name><name><surname>Fujieda</surname><given-names>K</given-names></name><name><surname>Matsubara</surname><given-names>Y</given-names></name><name><surname>Aoki</surname><given-names>Y</given-names></name></person-group><year>2007</year><article-title>Leukemia in Cardio-facio-cutaneous (CFC) syndrome: a patient with a germline mutation in BRAF proto-oncogene</article-title><source>Journal of Pediatric Hematology/oncology</source><volume>29</volume><fpage>287</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1097/MPH.0b013e3180547136</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nystrom</surname><given-names>AM</given-names></name><name><surname>Ekvall</surname><given-names>S</given-names></name><name><surname>Berglund</surname><given-names>E</given-names></name><name><surname>Bjorkqvist</surname><given-names>M</given-names></name><name><surname>Braathen</surname><given-names>G</given-names></name><name><surname>Duchen</surname><given-names>K</given-names></name><name><surname>Enell</surname><given-names>H</given-names></name><name><surname>Holmberg</surname><given-names>E</given-names></name><name><surname>Holmlund</surname><given-names>U</given-names></name><name><surname>Olsson-Engman</surname><given-names>M</given-names></name><name><surname>Anneren</surname><given-names>G</given-names></name><name><surname>Bondeson</surname><given-names>ML</given-names></name></person-group><year>2008</year><article-title>Noonan and cardio-facio-cutaneous syndromes: two clinically and genetically overlapping disorders</article-title><source>Journal of Medical Genetics</source><volume>45</volume><fpage>500</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1136/jmg.2008.057653</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><collab>Orphanet</collab></person-group><year>2013</year><article-title>Prevalence of rare diseases: Bibliographic data</article-title><person-group person-group-type="editor"><name><surname>KREMP</surname><given-names>O</given-names></name></person-group><source>Orphanet Report Series</source></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oti</surname><given-names>M</given-names></name><name><surname>Brunner</surname><given-names>HG</given-names></name></person-group><year>2007</year><article-title>The modular nature of genetic diseases</article-title><source>Clinical Genetics</source><volume>71</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1111/j.1399-0004.2006.00708.x</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramnath</surname><given-names>K</given-names></name><name><surname>Koterba</surname><given-names>S</given-names></name><name><surname>Xiao</surname><given-names>J</given-names></name><name><surname>Hu</surname><given-names>C</given-names></name><name><surname>Matthews</surname><given-names>I</given-names></name><name><surname>Baker</surname><given-names>S</given-names></name><name><surname>Cohn</surname><given-names>J</given-names></name><name><surname>Kanade</surname><given-names>T</given-names></name></person-group><year>2008</year><article-title>Multi-view AAM fitting and construction</article-title><source>International Journal of Computer Vision</source><volume>76</volume><fpage>183</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1007/s11263-007-0050-3</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rauen</surname><given-names>KA</given-names></name></person-group><year>1993</year><article-title>Cardiofaciocutaneous syndrome</article-title><person-group person-group-type="editor"><name><surname>Pagon</surname><given-names>RA</given-names></name><name><surname>Adam</surname><given-names>MP</given-names></name><name><surname>Bird</surname><given-names>TD</given-names></name><name><surname>Dolan</surname><given-names>CR</given-names></name><name><surname>Fong</surname><given-names>CT</given-names></name><name><surname>Stephens</surname><given-names>K</given-names></name></person-group><source>GeneReviews</source><publisher-loc>Seattle, WA</publisher-loc></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauen</surname><given-names>KA</given-names></name></person-group><year>2006</year><article-title>Distinguishing Costello versus cardio-facio-cutaneous syndrome: BRAF mutations in patients with a Costello phenotype</article-title><source>American Journal of Medical Genetics Part A</source><volume>140</volume><fpage>1681</fpage><lpage>1683</lpage><pub-id pub-id-type="doi">10.1002/ajmg.a.31315</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rauen</surname><given-names>KA</given-names></name></person-group><year>2007</year><article-title>HRAS and the Costello syndrome</article-title><source>Clinical Genetics</source><volume>71</volume><fpage>101</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1111/j.1399-0004.2007.00743.x</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimoin</surname><given-names>DL</given-names></name><name><surname>Hirschhorn</surname><given-names>K</given-names></name></person-group><year>2004</year><article-title>A history of medical genetics in pediatrics</article-title><source>Pediatric Research</source><volume>56</volume><fpage>150</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1203/01.PDR.0000129659.32875.84</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rossin</surname><given-names>EJ</given-names></name><name><surname>Lage</surname><given-names>K</given-names></name><name><surname>Raychaudhuri</surname><given-names>S</given-names></name><name><surname>Xavier</surname><given-names>RJ</given-names></name><name><surname>Tatar</surname><given-names>D</given-names></name><name><surname>Benita</surname><given-names>Y</given-names></name><name><surname>Cotsapas</surname><given-names>C</given-names></name><name><surname>Daly</surname><given-names>MJ</given-names></name></person-group><year>2011</year><article-title>Proteins encoded in genomic regions associated with immune-mediated disease physically interact and suggest underlying biology</article-title><source>PLOS Genetics</source><volume>7</volume><fpage>e1001273</fpage><pub-id pub-id-type="doi">10.1371/journal.pgen.1001273</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>AL</given-names></name><name><surname>Albrecht</surname><given-names>B</given-names></name><name><surname>Arici</surname><given-names>C</given-names></name><name><surname>Van der Burgt</surname><given-names>I</given-names></name><name><surname>Buske</surname><given-names>A</given-names></name><name><surname>Gillessen-Kaesbach</surname><given-names>G</given-names></name><name><surname>Heller</surname><given-names>R</given-names></name><name><surname>Horn</surname><given-names>D</given-names></name><name><surname>Hubner</surname><given-names>CA</given-names></name><name><surname>Korenke</surname><given-names>GC</given-names></name><name><surname>Konig</surname><given-names>R</given-names></name><name><surname>Kress</surname><given-names>W</given-names></name><name><surname>Kruger</surname><given-names>G</given-names></name><name><surname>Meinecke</surname><given-names>P</given-names></name><name><surname>Mucke</surname><given-names>J</given-names></name><name><surname>Plecko</surname><given-names>B</given-names></name><name><surname>Rossier</surname><given-names>E</given-names></name><name><surname>Schinzel</surname><given-names>A</given-names></name><name><surname>Schulze</surname><given-names>A</given-names></name><name><surname>Seemanova</surname><given-names>E</given-names></name><name><surname>Seidel</surname><given-names>H</given-names></name><name><surname>Spranger</surname><given-names>S</given-names></name><name><surname>Tuysuz</surname><given-names>B</given-names></name><name><surname>Uhrig</surname><given-names>S</given-names></name><name><surname>Wieczorek</surname><given-names>D</given-names></name><name><surname>Kutsche</surname><given-names>K</given-names></name><name><surname>Zenker</surname><given-names>M</given-names></name></person-group><year>2008</year><article-title>Mutation and phenotypic spectrum in patients with cardio-facio-cutaneous and Costello syndrome</article-title><source>Clinical Genetics</source><volume>73</volume><fpage>62</fpage><lpage>70</lpage><pub-id pub-id-type="doi">10.1111/j.1399-0004.2007.00931.x</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuurs-Hoeijmakers</surname><given-names>JH</given-names></name><name><surname>Oh</surname><given-names>EC</given-names></name><name><surname>Vissers</surname><given-names>LE</given-names></name><name><surname>Swinkels</surname><given-names>ME</given-names></name><name><surname>Gilissen</surname><given-names>C</given-names></name><name><surname>Willemsen</surname><given-names>MA</given-names></name><name><surname>Holvoet</surname><given-names>M</given-names></name><name><surname>Steehouwer</surname><given-names>M</given-names></name><name><surname>Veltman</surname><given-names>JA</given-names></name><name><surname>De Vries</surname><given-names>BB</given-names></name><name><surname>Van Bokhoven</surname><given-names>H</given-names></name><name><surname>De Brouwer</surname><given-names>AP</given-names></name><name><surname>Katsanis</surname><given-names>N</given-names></name><name><surname>Devriendt</surname><given-names>K</given-names></name><name><surname>Brunner</surname><given-names>HG</given-names></name></person-group><year>2012</year><article-title>Recurrent de novo mutations in PACS1 cause defective cranial-neural-crest migration and define a recognizable intellectual-disability syndrome</article-title><source>American Journal of Human Genetics</source><volume>91</volume><fpage>1122</fpage><lpage>1127</lpage><pub-id pub-id-type="doi">10.1016/j.ajhg.2012.10.013</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegel</surname><given-names>DH</given-names></name><name><surname>Mckenzie</surname><given-names>J</given-names></name><name><surname>Frieden</surname><given-names>IJ</given-names></name><name><surname>Rauen</surname><given-names>KA</given-names></name></person-group><year>2011</year><article-title>Dermatological findings in 61 mutation-positive individuals with cardiofaciocutaneous syndrome</article-title><source>The British Journal of Dermatology</source><volume>164</volume><fpage>521</fpage><lpage>529</lpage><pub-id pub-id-type="doi">10.1111/j.1365-2133.2010.10122.x</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Parkhi</surname><given-names>OM</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year>2013</year><article-title>Fisher Vector Faces in the Wild</article-title><source>British Machine Vision Conference</source></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sotos</surname><given-names>JG</given-names></name></person-group><year>2012</year><article-title>Abraham Lincoln's marfanoid mother: the earliest known case of multiple endocrine neoplasia type 2B?</article-title><source>Clinical Dysmorphology</source><volume>21</volume><fpage>131</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1097/MCD.0b013e328353ae0c</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suttie</surname><given-names>M</given-names></name><name><surname>Foroud</surname><given-names>T</given-names></name><name><surname>Wetherill</surname><given-names>L</given-names></name><name><surname>Jacobson</surname><given-names>JL</given-names></name><name><surname>Molteno</surname><given-names>CD</given-names></name><name><surname>Meintjes</surname><given-names>EM</given-names></name><name><surname>Hoyme</surname><given-names>HE</given-names></name><name><surname>Khaole</surname><given-names>N</given-names></name><name><surname>Robinson</surname><given-names>LK</given-names></name><name><surname>Riley</surname><given-names>EP</given-names></name><name><surname>Jacobson</surname><given-names>SW</given-names></name><name><surname>Hammond</surname><given-names>P</given-names></name></person-group><year>2013</year><article-title>Facial dysmorphism across the fetal alcohol spectrum</article-title><source>Pediatrics</source><volume>131</volume><fpage>e779</fpage><lpage>e788</lpage><pub-id pub-id-type="doi">10.1542/peds.2012-1371</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tidyman</surname><given-names>WE</given-names></name><name><surname>Rauen</surname><given-names>KA</given-names></name></person-group><year>2008</year><article-title>Noonan, Costello and cardio-facio-cutaneous syndromes: dysregulation of the Ras-MAPK pathway</article-title><source>Expert Reviews in Molecular Medicine</source><volume>10</volume><fpage>e37</fpage><pub-id pub-id-type="doi">10.1017/S1462399408000902</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twigg</surname><given-names>SR</given-names></name><name><surname>Vorgia</surname><given-names>E</given-names></name><name><surname>Mcgowan</surname><given-names>SJ</given-names></name><name><surname>Peraki</surname><given-names>I</given-names></name><name><surname>Fenwick</surname><given-names>AL</given-names></name><name><surname>Sharma</surname><given-names>VP</given-names></name><name><surname>Allegra</surname><given-names>M</given-names></name><name><surname>Zaragkoulias</surname><given-names>A</given-names></name><name><surname>Sadighi Akha</surname><given-names>E</given-names></name><name><surname>Knight</surname><given-names>SJ</given-names></name><name><surname>Lord</surname><given-names>H</given-names></name><name><surname>Lester</surname><given-names>T</given-names></name><name><surname>Izatt</surname><given-names>L</given-names></name><name><surname>Lampe</surname><given-names>AK</given-names></name><name><surname>Mohammed</surname><given-names>SN</given-names></name><name><surname>Stewart</surname><given-names>FJ</given-names></name><name><surname>Verloes</surname><given-names>A</given-names></name><name><surname>Wilson</surname><given-names>LC</given-names></name><name><surname>Healy</surname><given-names>C</given-names></name><name><surname>Sharpe</surname><given-names>PT</given-names></name><name><surname>Hammond</surname><given-names>P</given-names></name><name><surname>Hughes</surname><given-names>J</given-names></name><name><surname>Taylor</surname><given-names>S</given-names></name><name><surname>Johnson</surname><given-names>D</given-names></name><name><surname>Wall</surname><given-names>SA</given-names></name><name><surname>Mavrothalassitis</surname><given-names>G</given-names></name><name><surname>Wilkie</surname><given-names>AO</given-names></name></person-group><year>2013</year><article-title>Reduced dosage of ERF causes complex craniosynostosis in humans and mice and links ERK1/2 signaling to regulation of osteogenesis</article-title><source>Nature Genetics</source><volume>45</volume><fpage>308</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1038/ng.2539</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Viola</surname><given-names>P</given-names></name><name><surname>Jones</surname><given-names>M</given-names></name></person-group><year>2001</year><article-title>Rapid object detection using a boosted cascade of simple features. Computer Vision and Pattern Recognition, 2001. CVPR 2001. Proceedings of the 2001 IEEE Computer Society Conference on, 2001</article-title><comment>I-511-I-518 vol. 1</comment></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vollmar</surname><given-names>T</given-names></name><name><surname>Maus</surname><given-names>B</given-names></name><name><surname>Wurtz</surname><given-names>RP</given-names></name><name><surname>Gillessen-Kaesbach</surname><given-names>G</given-names></name><name><surname>Horsthemke</surname><given-names>B</given-names></name><name><surname>Wieczorek</surname><given-names>D</given-names></name><name><surname>Boehringer</surname><given-names>S</given-names></name></person-group><year>2008</year><article-title>Impact of geometry and viewing angle on classification accuracy of 2D based analysis of dysmorphic faces</article-title><source>European Journal of Medical Genetics</source><volume>51</volume><fpage>44</fpage><lpage>53</lpage><pub-id pub-id-type="doi">10.1016/j.ejmg.2007.10.002</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weinberger</surname><given-names>KQ</given-names></name><name><surname>Saul</surname><given-names>LK</given-names></name></person-group><year>2009</year><article-title>Distance metric learning for large margin nearest neighbor classification</article-title><source>Journal of Machine Learning Research</source><volume>10</volume><fpage>207</fpage><lpage>244</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weischenfeldt</surname><given-names>J</given-names></name><name><surname>Symmons</surname><given-names>O</given-names></name><name><surname>Spitz</surname><given-names>F</given-names></name><name><surname>Korbel</surname><given-names>JO</given-names></name></person-group><year>2013</year><article-title>Phenotypic impact of genomic structural variation: insights from and for human disease</article-title><source>Nature Reviews Genetics</source><volume>14</volume><fpage>125</fpage><lpage>138</lpage><pub-id pub-id-type="doi">10.1038/nrg3373</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wright</surname><given-names>EM</given-names></name><name><surname>Kerr</surname><given-names>B</given-names></name></person-group><year>2010</year><article-title>RAS-MAPK pathway disorders: important causes of congenital heart disease, feeding difficulties, developmental delay and short stature</article-title><source>Archives of Disease in Childhood</source><volume>95</volume><fpage>724</fpage><lpage>730</lpage><pub-id pub-id-type="doi">10.1136/adc.2009.160069</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zampino</surname><given-names>G</given-names></name><name><surname>Pantaleoni</surname><given-names>F</given-names></name><name><surname>Carta</surname><given-names>C</given-names></name><name><surname>Cobellis</surname><given-names>G</given-names></name><name><surname>Vasta</surname><given-names>I</given-names></name><name><surname>Neri</surname><given-names>C</given-names></name><name><surname>Pogna</surname><given-names>EA</given-names></name><name><surname>De Feo</surname><given-names>E</given-names></name><name><surname>Delogu</surname><given-names>A</given-names></name><name><surname>Sarkozy</surname><given-names>A</given-names></name><name><surname>Atzeri</surname><given-names>F</given-names></name><name><surname>Selicorni</surname><given-names>A</given-names></name><name><surname>Rauen</surname><given-names>KA</given-names></name><name><surname>Cytrynbaum</surname><given-names>CS</given-names></name><name><surname>Weksberg</surname><given-names>R</given-names></name><name><surname>Dallapiccola</surname><given-names>B</given-names></name><name><surname>Ballabio</surname><given-names>A</given-names></name><name><surname>Gelb</surname><given-names>BD</given-names></name><name><surname>Neri</surname><given-names>G</given-names></name><name><surname>Tartaglia</surname><given-names>M</given-names></name></person-group><year>2007</year><article-title>Diversity, parental germline origin, and phenotypic spectrum of de novo HRAS missense changes in Costello syndrome</article-title><source>Human Mutation</source><volume>28</volume><fpage>265</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1002/humu.20431</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenker</surname><given-names>M</given-names></name></person-group><year>2009</year><article-title>Genetic and pathogenetic aspects of Noonan syndrome and related disorders</article-title><source>Hormone Research</source><volume>72</volume><supplement>suppl 2</supplement><fpage>57</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1159/000243782</pub-id></element-citation></ref></ref-list></back><sub-article article-type="article-commentary" id="SA1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.02020.018</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Tollman</surname><given-names>Stephen</given-names></name><role>Reviewing editor</role><aff><institution>Wits University</institution>, <country>South Africa</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>eLife posts the editorial decision letter and author response on a selection of the published articles (subject to the approval of the authors). An edited version of the letter sent to the authors after peer review is shown, indicating the substantive concerns or comments; minor concerns are not usually shown. Reviewers have the opportunity to discuss the decision before the letter is sent (see <ext-link ext-link-type="uri" xlink:href="http://elifesciences.org/review-process">review process</ext-link>). Similarly, the author response typically shows only responses to the major concerns raised by the reviewers.</p></boxed-text><p>Thank you for sending your work entitled “Diagnostically-relevant facial gestalt information from ordinary photos” for consideration at <italic>eLife</italic>. Your article has been favorably evaluated by a Senior editor, Detlef Weigel, a Reviewing editor, and 2 reviewers, one of whom, Peter Claes, has agreed to reveal his identity.</p><p>The Reviewing editor and the reviewers discussed their comments before we reached this decision, and the Reviewing editor has assembled the following comments to help you prepare a revised submission.</p><p>With genome sequencing having become cheaper and cheaper over the past few years, the bottleneck in genetics is increasingly shifting to accurate, inexpensive phenotyping. Even for Mendelian genetic disorders, misdiagnosis rates are shockingly high. A sizeable fraction of genetic disorders is associated with craniofacial abnormalities, and the approach presented in this study greatly facilitates the acquisition and analysis of craniofacial information from simple photographs, as opposed to more complex 3D scans that are not necessarily suitable for every day medical diagnostics. The reviewers felt that the work was of high quality, and they had only some minor comments that need to be addressed in the revision.</p><p><italic>From Reviewer 1:</italic></p><p>The simple term “Face-Space” for the new representation presented by the authors is confusing. When doing a simple search on the internet, the concept of a Face-Space is well known as a space coding for facial shape variations due to general differences like differences in identity and not necessary clinical phenotype. In face recognition and perception, the term face-space is already widely used and does not reflect the same space as presented by the authors. I suggest defining the new representation differently to avoid confusion, for example Diagnostic Face Space.</p><p>In the Introduction the advantages over current studies using 3D images is slightly exaggerated. The single most important, BUT major advantage of the proposed approach is the easy access to ordinary images in contrast to expensive 3D acquisition systems. Rare genetic diseases are hard to find and collect, and through the use of ordinary 2D images one manages to acquire a substantial amount of samples across the world (which is typically required in AI based image strategies). In 3D, pose and illumination variations do not matter as such (which is well-documented in the literature of 2D versus 3D face recognition [1]) because pose differences can be normalized and syndromic diagnosis is mainly done on shape (and not texture) only. In contrast 2D images suffer a lot from pose and illumination differences, and the use of LMNN is a clever way of reducing their effect in clustering for diagnostic purposes as proposed by the authors (as also shown by <xref ref-type="bibr" rid="bib61">Weinberger and Saul 2009</xref> on 2D face recognition data). The easy access to 2D images by itself is a strong argument in favor of the proposed approach.</p><p>It is intriguing to see the generalization capabilities of the “Face-Space”. Trained on only 8 syndromes, it is capable of being applied to a much wider range of syndromes.</p><p>With regards to the SVM classification on the raw feature space, it is unclear to me, why this is performed. It seems to be a 2D implementation of a syndrome classification comparable to the 3D based classification done in the work of Hammond et al. The way I can read this in the context of the remainder of the paper is: In essence a traditional classification in the raw feature space using SVM increased from 94.4%, to 99.5% in “Face-Space” using kNN classification. However, I'm not sure if this is correct or not.</p><p>What is the stability of the main axes in “Face-Space”? In other words, do the distance metrics change a lot when using 8 different syndrome classes than the ones used for training in the manuscript? Did the authors experiment along these lines or not?</p><p>Finally the clustering improvement factor is novel and seems like an interesting metric proposed by the authors. Due to its novelty it is hard to give an interpretation to the metric and a more elaborated sample computation of the metric using the database composition as presented in the manuscript would be helpful in the methods section.</p><p><italic>From Reviewer 2:</italic></p><p>There are a couple of areas where the authors could add to the paper: One concerns the nature of the appearance model. We know that the faces show quite significant rigid pose variations. Would the model be improved by explicit modelling of the 3D variation? Perhaps an approach similar to that adopted by Iain Matthews and collaborators, building simultaneous 2D and 3D models might be worthwhile.</p><p>The other area relates to alternatives to the LMNN transform. Did the authors consider any other transforms? Do they rely on fundamentally different features of the data? Do they have significantly different performances?</p><p>I do not believe that either of these areas require additional computation – really just a short discussion to show the reader that there are alternative techniques within this area and choices have to be made about the computational and modelling techniques.</p></body></sub-article><sub-article article-type="reply" id="SA2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.02020.019</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>From Reviewer 1:</p><p><italic>The simple term “Face-Space” for the new representation presented by the authors is confusing. When doing a simple search on the internet, the concept of a Face-Space is well known as a space coding for facial shape variations due to general differences like differences in identity and not necessary clinical phenotype. In face recognition and perception, the term face-space is already widely used and does not reflect the same space as presented by the authors. I suggest defining the new representation differently to avoid confusion, for example Diagnostic Face Space</italic>.</p><p>We agree. The term “Face Space” has now been replaced throughout with the more specific term “Clinical Face Phenotype Space”. This better reflects the character and purpose of the algorithms.</p><p><italic>In the Introduction the advantages over current studies using 3D images is slightly exaggerated. The single most important, BUT major advantage of the proposed approach is the easy access to ordinary images in contrast to expensive 3D acquisition systems. Rare genetic diseases are hard to find and collect, and through the use of ordinary 2D images one manages to acquire a substantial amount of samples across the world (which is typically required in AI based image strategies). In 3D, pose and illumination variations do not matter as such (which is well-documented in the literature of 2D versus 3D face recognition [1]) because pose differences can be normalized and syndromic diagnosis is mainly done on shape (and not texture) only. In contrast 2D images suffer a lot from pose and illumination differences, and the use of LMNN is a clever way of reducing their effect in clustering for diagnostic purposes as proposed by the authors (as also shown by</italic> <xref ref-type="bibr" rid="bib61"><italic>Weinberger and Saul 2009</italic></xref> <italic>on 2D face recognition data). The easy access to 2D images by itself is a strong argument in favor of the proposed approach</italic>.</p><p>Comparisons to 3D imaging to the current approach, and previous work in 2D imaging to the current approach, are now separated into two sentences to provide greater clarity.</p><p>“While 3D imaging studies have shown high discriminatory power in terms of classification they have relied on specialized imaging equipment and patient cooperation. Previous work with 2D images has relied on manual annotation of images, controlling lighting, pose and expression to allow consistent analyses. These factors greatly limit the availability, and ultimately the potential widespread clinical utility of such approaches.”</p><p><italic>It is intriguing to see the generalization capabilities of the “Face-Space”. Trained on only 8 syndromes, it is capable of being applied to a much wider range of syndromes</italic>.</p><p><italic>With regards to the SVM classification on the raw feature space, it is unclear to me, why this is performed. It seems to be a 2D implementation of a syndrome classification comparable to the 3D based classification done in the work of Hammond et al. The way I can read this in the context of the remainder of the paper is: In essence a traditional classification in the raw feature space using SVM increased from 94.4%, to 99.5% in “Face-Space” using kNN classification. However, I'm not sure if this is correct or not</italic>.</p><p>The SVM work is now better contextualised to clarify the purpose of these investigations. The following sentences are now amended:</p><p>“We next sought to compare the syndrome relevant information content of the feature descriptors to previous studies (<xref ref-type="bibr" rid="bib27">Hammond, 2007</xref>, <xref ref-type="bibr" rid="bib28">Hammond et al., 2005</xref>, <xref ref-type="bibr" rid="bib11">Boehringer et al., 2006</xref>, <xref ref-type="bibr" rid="bib60">Vollmar et al., 2008</xref>). We found that classification analysis based on support vector machines provided similar accuracies to previous work, despite disparities in image variability (average classification accuracy 94.4%, see <xref ref-type="fig" rid="fig4s1">Figure 4–figure supplement 1</xref>, <xref ref-type="fig" rid="fig4s2">Figure 4–figure supplement 2</xref> and Methods).”</p><p><italic>What is the stability of the main axes in “Face-Space”? In other words</italic>, <italic>do the distance metrics change a lot when using 8 different syndrome classes than the ones used for training in the manuscript? Did the authors experiment along these lines or not?</italic></p><p>To address this, we performed a test using leave-one-out training of the Clinical Face Phenotype Space. However, beyond affirming the expected loss in fidelity in classifications within the resulting space, we were unable to find a suitable way to compare the compositions of the primary axes.</p><p><italic>Finally the clustering improvement factor is novel and seems like an interesting metric proposed by the authors. Due to its novelty it is hard to give an interpretation to the metric and a more elaborated sample computation of the metric using the database composition as presented in the manuscript would be helpful in the methods section</italic>.</p><p>We agree that the Clustering Improvement Factor (CIF) metric needed to be discussed in greater detail in the manuscript.</p><p>We have taken time to further validate and explain the CIF metric, and have improved this metric by better modelling of the background expectation. The consequence is that the average CIF for the 8 syndromes is 13.7 fold (note: later refined to 27.6-fold) over random chance. A CIF value of 1 is random chance (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Note that <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref> have been updated with the new CIF estimates.</p><p>To improve the explanation we have added a toy example of the CIF calculation as <xref ref-type="fig" rid="fig4s3">Figure 4–figure supplement 3</xref>.</p><p>The CIF explanation in the Methods section has been extended:</p><p>“Next we performed estimations of clustering of syndromes in face space. Initial tests using kNN classifiers showed that the classification accuracies were heavily dependent on spread and cardinality of the syndrome in the database. We went on to develop an estimate of search space reduction, hereafter referred to Clustering Improvement Factor (CIF), to determine the improvements in clustering in Clinical Face Phenotype Space controlling for the composition of the database (a simulated example is provided in <xref ref-type="fig" rid="fig4s3">Figure 4–figure supplement 3</xref>). We considered a syndrome with Np positive and Nn negative instances in the Clinical Face Phenotype Space. We defined the CIF as:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mtext>CIF</mml:mtext><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>expected rank </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mtext> of nearest positive match under random ranking</mml:mtext></mml:mrow><mml:mrow><mml:mtext>observed average rank </mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mtext> of nearest positive match</mml:mtext></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mtext>E</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mtext>O</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>with the average taken across all instances of the syndrome. O(r) was calculated from the observations in the Clinical Face Phenotype Space. To compute E(r), we used probability theory as follows.</p><p>Under a random ranking for a given positive query, the other Np-1 positive instances are each placed independently among the Nn negative instances, with a uniform discrete probability distribution. We defined the random variable Ni as the number of negative instances ranked higher than the first positive instance, so Ni takes integer values <inline-formula><mml:math id="inf7"><mml:mrow><mml:mn>0</mml:mn><mml:mo>≤</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>.</p><p>For a given positive query, the expected rank of the nearest positive match is the expected value of Ni+1, denoted by E(Ni)+1. To calculate E(Ni), we used the definition of expectation: <disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mtext>0</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>jPr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since Ni can only take non-negative integer values, for each possible value j between 0 and Nn,</p><p><disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>+</mml:mo><mml:mtext>1</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Substituting this in the formula for E(Ni), <disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mtext>0</mml:mtext></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>j</mml:mtext><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>+</mml:mo><mml:mtext>1</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Rewriting the sum, <disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mtext>jPr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>−</mml:mo><mml:mo>(</mml:mo><mml:mtext>j</mml:mtext><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For a given number j, Pr(Ni≥j) is the probability that all positive instances were placed after j negative instances. For any given individual positive instance, such placement has probability <inline-formula><mml:math id="inf8"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>. Since placement of all positive instances is independent, this gives <inline-formula><mml:math id="inf9"><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>p</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>Therefore, <inline-formula><mml:math id="inf10"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:mtext>Pr</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>≥</mml:mo><mml:mtext>j</mml:mtext><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>p</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula></p><p>Finally, this gives <inline-formula><mml:math id="inf11"><mml:mrow><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:mtext>r</mml:mtext><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>E</mml:mtext><mml:mo>(</mml:mo><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>i</mml:mtext></mml:msub><mml:mo>)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:munderover><mml:mstyle displaystyle="true"><mml:mo>∑</mml:mo></mml:mstyle><mml:mrow><mml:mtext>j</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub></mml:mrow></mml:munderover><mml:mrow><mml:msup><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mtext>j</mml:mtext><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>n</mml:mtext></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mtext>N</mml:mtext><mml:mtext>p</mml:mtext></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>.”</p><p>From Reviewer 2:</p><p><italic>There are a couple of areas where the authors could add to the paper:</italic></p><p><italic>One concerns the nature of the appearance model. We know that the faces show quite significant rigid pose variations. Would the model be improved by explicit modelling of the 3D variation? Perhaps an approach similar to that adopted by Iain Matthews and collaborators, building simultaneous 2D and 3D models might be worthwhile</italic>.</p><p>Please see response below.</p><p><italic>The other area relates to alternatives to the LMNN transform</italic>. <italic>Did the authors consider any other transforms? Do they rely on fundamentally different features of the data? Do they have significantly different performances?</italic></p><p>Please see response below.</p><p><italic>I do not believe that either of these areas require additional computation – really just a short discussion to show the reader that there are alternative techniques within this area and choices have to be made about the computational and modelling techniques</italic>.</p><p>Thank you for these comments. Indeed an explicit modelling of the 3D variation is an avenue we are pursuing in follow up research. We were not able to explore other metric learning alternatives within the scope of this pilot study but are pursuing this line of questioning in our current research. These points are now discussed briefly in the Discussion:</p><p>“Among the approaches that will be tested in future works are: increasing the number of feature points across the cranium, using profile images and taking advantage of multiple images of the same individual. Furthermore we will be exploring performing explicit modelling of the 3D variation for 2D images (<xref ref-type="bibr" rid="bib45">Ramnath et al., 2008</xref>), other types of feature descriptors, alternative metric learning and dimensionality reduction approaches (<xref ref-type="bibr" rid="bib54">Simonyan et al., 2013</xref>).”</p></body></sub-article></article>