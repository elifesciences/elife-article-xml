<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">83722</article-id><article-id pub-id-type="doi">10.7554/eLife.83722</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Interpersonal alignment of neural evidence accumulation to social exchange of confidence</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-296945"><name><surname>Esmaily</surname><given-names>Jamal</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5529-6732</contrib-id><email>jimi.esmaily@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-307571"><name><surname>Zabbah</surname><given-names>Sajjad</given-names></name><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-296950"><name><surname>Ebrahimpour</surname><given-names>Reza</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7013-8078</contrib-id><email>ebrahimpour@sharif.edu</email><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-162237"><name><surname>Bahrami</surname><given-names>Bahador</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0802-5328</contrib-id><email>bbahrami@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff8">8</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05591te55</institution-id><institution>Department of General Psychology and Education, Ludwig Maximillian University</institution></institution-wrap><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02nkz4493</institution-id><institution>Faculty of Computer Engineering, Shahid Rajaee Teacher Training University</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05591te55</institution-id><institution>Graduate School of Systemic Neurosciences, Ludwig Maximilian University Munich</institution></institution-wrap><addr-line><named-content content-type="city">Munich</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xreqs31</institution-id><institution>School of Cognitive Sciences, Institute for Research in Fundamental Sciences (IPM)</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02704qw51</institution-id><institution>Wellcome Centre for Human Neuroimaging, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Max Planck UCL Centre for Computational Psychiatry and Aging Research, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024c2fq17</institution-id><institution>Institute for Convergent Science and Technology, Sharif University of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff><aff id="aff8"><label>8</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pp7px91</institution-id><institution>Centre for Adaptive Rationality, Max Planck Institute for Human Development</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>21</day><month>12</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e83722</elocation-id><history><date date-type="received" iso-8601-date="2022-09-26"><day>26</day><month>09</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-11-09"><day>09</day><month>11</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-11-09"><day>09</day><month>11</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.11.08.515654"/></event></pub-history><permissions><copyright-statement>© 2023, Esmaily et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Esmaily et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-83722-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-83722-figures-v1.pdf"/><abstract><p>Private, subjective beliefs about uncertainty have been found to have idiosyncratic computational and neural substrates yet, humans share such beliefs seamlessly and cooperate successfully. Bringing together decision making under uncertainty and interpersonal alignment in communication, in a discovery plus pre-registered replication design, we examined the neuro-computational basis of the relationship between privately held and socially shared uncertainty. Examining confidence-speed-accuracy trade-off in uncertainty-ridden perceptual decisions under social vs isolated context, we found that shared (i.e. reported confidence) and subjective (inferred from pupillometry) uncertainty dynamically followed social information. An attractor neural network model incorporating social information as top-down additive input captured the observed behavior and demonstrated the emergence of social alignment in virtual dyadic simulations. Electroencephalography showed that social exchange of confidence modulated the neural signature of perceptual evidence accumulation in the central parietal cortex. Our findings offer a neural population model for interpersonal alignment of shared beliefs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>interpersonal alignment</kwd><kwd>decision making</kwd><kwd>uncertainty</kwd><kwd>neural attractor network</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>819040 - acronym: rid-O</award-id><principal-award-recipient><name><surname>Esmaily</surname><given-names>Jamal</given-names></name></principal-award-recipient></award-group><funding-statement>Open access funding provided by Max Planck Society. The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A multidisciplinary approach sheds light on the fundamental mechanisms underlying social belief communication under uncertainty.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>We communicate our confidence to others to share our beliefs about uncertainty with them. However, numerous studies have shown that even the same verbal or numerical expression of confidence can have very different meanings for different people in terms of the underlying uncertainty (<xref ref-type="bibr" rid="bib2">Ais et al., 2016</xref>; <xref ref-type="bibr" rid="bib53">Navajas et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Fleming et al., 2010</xref>). Similar inter-individual diversity has been found at the neural level (<xref ref-type="bibr" rid="bib22">Fleming et al., 2010</xref>; <xref ref-type="bibr" rid="bib67">Sinanaj et al., 2015</xref>; <xref ref-type="bibr" rid="bib8">Baird et al., 2013</xref>). Still, people manage to cooperate successfully in decision making under uncertainty (<xref ref-type="bibr" rid="bib6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="bib5">Austen-Smith and Banks, 1996</xref>). What computational and neuronal mechanisms enable people to converge to a <italic>shared meaning</italic> of their confidence expressions in interactive decision making despite the extensively documented neural and cognitive diversity? This question drives at the heart of recent efforts to understand the neurobiology of how people adapt their communication to their beliefs about their interaction partner (<xref ref-type="bibr" rid="bib73">Stolk et al., 2016</xref>). A number of studies have provided compelling empirical evidence of brain-to-brain coupling that could underlie adaptive communication of shared beliefs (<xref ref-type="bibr" rid="bib66">Silbert et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Honey et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Hasson et al., 2004</xref>; <xref ref-type="bibr" rid="bib19">Dikker et al., 2014</xref>; <xref ref-type="bibr" rid="bib41">Konvalinka et al., 2010</xref>). These works remain, to date, mostly observational in nature. Plausible neuro-computational mechanism(s) accounting for how interpersonal alignment of beliefs may arise from the firing patterns of decision-related neural populations in the human brain are still lacking (<xref ref-type="bibr" rid="bib31">Hasson and Frith, 2016</xref>; <xref ref-type="bibr" rid="bib82">Wheatley et al., 2019</xref>). Using a multidisciplinary approach, we addressed this question at behavioral, computational, and neurobiological levels.</p><p>By sharing their confidence with others, joint decision makers can surpass their respective individual performance by reducing uncertainty through interaction (<xref ref-type="bibr" rid="bib6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="bib68">Sorkin et al., 2001</xref>). Recent works showed that during dyadic decision making, interacting partners adjust to one another by matching their own average confidence to that of their partner (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>). Such confidence matching turns out to be a good strategy for maximizing joint accuracy under a range of naturalistic conditions, e.g., uncertainty about the partner’s reliability. However, at present there is no link connecting these socially observed emergent characteristics of confidence sharing with the elaborate frameworks that shape our understanding of confidence in decision making under uncertainty (<xref ref-type="bibr" rid="bib53">Navajas et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Fleming et al., 2010</xref>; <xref ref-type="bibr" rid="bib57">Pouget et al., 2016</xref>; <xref ref-type="bibr" rid="bib1">Adler and Ma, 2018</xref>; <xref ref-type="bibr" rid="bib3">Aitchison et al., 2015</xref>).</p><p>Theoretical work has shown that sequential sampling can, in principle, provide an optimal strategy for making the best of whatever uncertain, noisy evidence is available to the agent (<xref ref-type="bibr" rid="bib32">Heath, 1984</xref>). These models have had great success in explaining the relationship between decision reaction time (RT) and accuracy under a variety of conditions ranging from perceptual (<xref ref-type="bibr" rid="bib28">Hanks and Summerfield, 2017</xref>; <xref ref-type="bibr" rid="bib27">Gold and Shadlen, 2007</xref>) to value-based decisions (<xref ref-type="bibr" rid="bib62">Ruff and Fehr, 2014</xref>) guiding the search for the neuronal mechanisms of evidence accumulation to boundary in rodent and primate brains (<xref ref-type="bibr" rid="bib64">Schall, 2019</xref>). The relation between RT and accuracy, known as speed-accuracy trade-off, has been recently extended to a three-way relationship in which choice confidence is guided by <italic>both</italic> RT and probability (or frequency) of correct decision (<xref ref-type="bibr" rid="bib57">Pouget et al., 2016</xref>; <xref ref-type="bibr" rid="bib38">Kiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib78">Vickers, 1970</xref>). Critically, these studies have all focused on decision making in <italic>isolated individuals</italic> deciding privately (<xref ref-type="bibr" rid="bib82">Wheatley et al., 2019</xref>). Little is known about how these computational principles and neuronal mechanisms can give rise to socially shared beliefs about uncertainty.</p><p>To bridge this gap, we examined confidence-speed-accuracy trade-off in social vs isolated context in humans. We combined a canonical paradigm (i.e. dynamic random dot motion [RDM]) extensively employed in psychophysical and neuroscientific studies of speed-accuracy-confidence trade-off (<xref ref-type="bibr" rid="bib28">Hanks and Summerfield, 2017</xref>; <xref ref-type="bibr" rid="bib27">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib36">Kelly and O’Connell, 2013</xref>) with interactive dyadic social decision making (<xref ref-type="bibr" rid="bib6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>). We replicated the emergence of confidence matching and obtained pupillometry evidence for shared subjective beliefs in our social implementation of the random dot paradigm and we observed a novel pattern of confidence-speed-accuracy trade-off specifically under the social condition. We constructed a neural attractor model that captured this trade-off, reproduced confidence matching in virtual social simulations and made neural predictions about the coupling between neuronal evidence accumulation and social information exchange that were born out by the empirical data.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We used a discovery-and-replication design to investigate the computational and neurobiological substrates of confidence matching in two separate steps: 12 participants (4 female) were recruited in study 1 (discovery) and 15 (5 female, age: 28 (mean) ± Std (7)) in study 2 (replication, second study was pre-registered: <ext-link ext-link-type="uri" xlink:href="https://osf.io/5zces">https://osf.io/5zces</ext-link>). In each study, participants reported the direction of a random-dot motion stimulus and indicated their confidence (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) while EEG and eye tracking data were recorded, simultaneously. After an extensive training procedure (see Materials and methods for the recruitment), participants reached a stable behavioral (accuracy and RT) performance level. Then, two experimental sessions were conducted: first a private session (200 trials) in which participants performed the task alone; then a social session (800 trials for study 1 and 400 for study 2) in which they performed the task interactively together with a partner (implied to be another participant in a neighboring lab room).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experiment paradigm and behavioral results.</title><p>(<bold>a</bold>) Timeline of trials in isolated (top) and social (bottom) conditions. After stimulus presentation, subjects reported their decision and confidence simultaneously by clicking on 1 of the 12 vertical bars. In the social condition, decision and confidence of participant (white in the experiment, here black for illustration purpose) and partner (yellow) were color coded. (<bold>b</bold>) Confidence matching. Participants confidence against agent confidence show a significant relation in both studies (linear regression p&lt;0.001 for both studies). (<bold>c</bold>) Under social condition, when participants were paired with high (magenta) vs low (dark orange) confidence partner, accuracy (top panel) did not change (horizontal lines, 68% confidence interval of bootstrap test with 10,000 repetitions) but confidence (middle panel) and reaction time (RT) (bottom panel) were altered. Curves fitted to the accuracy data are Weibull cumulative distribution function. Error bars are standard error of the mean (SEM) across subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Accuracy and confidence of the computer generated partners (CGPs).</title><p>Confidence is plotted in blue and accuracy is plotted in red. (<bold>a</bold>) Study 1 – HAHC: high accuracy and high confidence. HALC: high accuracy and low confidence. LAHC: low accuracy and high confidence. LALC: low accuracy and low confidence. Error bars: standard deviation across trials. Top-right, bottom-left, and bottom-right are same as top-left but for HALC, LAHC, and LALC, respectively. (<bold>b</bold>) Study 2 – same as (<bold>a</bold>) but for 2 CGPs and across 15 partners generated for 15 participants. HCA: high confidence agent; LCA: low confidence agent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Statistical analysis of the confidence matching effect.</title><p>(<bold>a</bold>) Top: Permutation test. The empirically observed difference in mean confidence (red line) is significantly different from the distribution of the expected mean (black curve and dotted line) under null hypothesis (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>), random pairing, (p~0 for both studies). Bottom: Probability density function (green curve) over confidence matching index defined as (<inline-formula><mml:math id="inf2"><mml:mi mathvariant="bold-italic">Δ</mml:mi><mml:mi mathvariant="bold-italic">m</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi mathvariant="bold"> </mml:mi><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">s</mml:mi><mml:mi mathvariant="bold-italic">o</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi mathvariant="bold-italic">S</mml:mi><mml:mi mathvariant="bold-italic">u</mml:mi><mml:mi mathvariant="bold-italic">b</mml:mi><mml:mi mathvariant="bold-italic">j</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">c</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">C</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">a</mml:mi><mml:mi mathvariant="bold-italic">g</mml:mi><mml:mi mathvariant="bold-italic">e</mml:mi><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula>). Dots denote Δ<italic>m</italic> for each combination of 12 participants by 4 computer generated partners (CGPs). The mean confidence difference is significantly different from zero (study 1, p&lt;0.001, paired t-test t(47)=5.5, study 2: p=0.11 <italic>t</italic>(29)=1.59 but significantly different for HCA condition, p&lt;0.001 <italic>t</italic>(14)=4.02). (<bold>b</bold>) Same as (<bold>a</bold>) but for the second study in which we had 2 CGPs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig1-figsupp2-v1.tif"/></fig><fig id="fig1s3" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 3.</label><caption><title>Examination of the hypothesis that the partner’s confidence at trial <italic>t</italic> modulates the participant behavior at trial <italic>t</italic>+1.</title><p>Probability correct: First row, confidence: Second row and reaction time (RT): last row. Study 1: first column. Study 2: second column. We used a generalized linear mixed model (GLMM) similar to that used in <xref ref-type="fig" rid="fig1">Figure 1c</xref> and plotted the model’s coefficient estimating the impact of partner’s confidence in trial <italic>t</italic> on the dependent variable of interest in trial <italic>t</italic>+1. The effects of trial-by-trial variation of partner’s confidence on participant’s confidence and RT were significant and qualitatively similar to what was observed in <xref ref-type="fig" rid="fig1">Figure 1</xref>. See <xref ref-type="table" rid="table1">Table 1</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig1-figsupp3-v1.tif"/></fig><fig id="fig1s4" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 4.</label><caption><title>Summary of debriefing results of the second study.</title><p>(<bold>a</bold>) Participants felt that their partner was more confident when facing with a high confidence agent (HCA). This means our manipulation indeed worked. (<bold>b</bold>) Similar to (<bold>a</bold>) but for accuracy. Here, subject did not feel any difference between HCA and low confidence agent (LCA) accuracies. Error bars are SEM across participants.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig1-figsupp4-v1.tif"/></fig></fig-group><p>In every trial (<xref ref-type="fig" rid="fig1">Figure 1a</xref>), after fixation for 300 ms was confirmed by closed-loop real-time eye tracking, two choice-target points appeared at 10° eccentricity corresponding to the two possible motion directions (left and right). After a short random delay (200–500 ms, truncated exponential distribution), a dynamic RDM (see <xref ref-type="bibr" rid="bib65">Shadlen and Newsome, 2001</xref>) was centrally displayed for 500 ms in a virtual aperture (5° diameter). At the end of the motion sequence, the participant indicated the direction of motion and their confidence on a 6-point scale by a single mouse click. A horizontal line intersected at midpoint and marked by 12 rectangles (6 on each side) was displayed. Participants moved the mouse pointer – initially set at the midpoint – to indicate their decision (left vs right of midpoint) and confidence by clicking inside one of the rectangles. Further distance from the midpoint indicated more confidence. RT was calculated as the time between the onset of the motion stimulus sequence and the onset of deviation of the mouse pointer (see Materials and methods for more details) (<xref ref-type="bibr" rid="bib60">Resulaj et al., 2009</xref>) at the end of stimulus presentation.</p><p>In the isolated trials, the participant was then given visual feedback for accuracy (correct or wrong). In the social trials (<xref ref-type="fig" rid="fig1">Figure 1a</xref>, bottom panel), after the response, participants proceeded to the social stage. Here, the participants’ own choice and confidence as well as that of their partner were displayed coded by different colors (white for participants; yellow for partners). Joint decision was automatically arbitrated in favor of the decision with higher confidence. Finally, three distinct color-coded feedback messages (participant, partner, and joint decision) were displayed.</p><p>Participants were instructed to try to maximize the joint accuracy of their social decisions. In order to achieve joint benefit, confidence should be expressed such that the decision with higher probability of correct outcome dominates (<xref ref-type="bibr" rid="bib6">Bahrami et al., 2010</xref>). For this to happen, the participant needs to factor in the partner’s behavior and adjust her confidence accordingly. For example, if the participant believes that her decision is highly likely to be correct, her confidence should be expressed such that joint decision is dominated by the partner only if the probability that the partner’s decision is correct is even higher (and not, for example, if the partner expressed a high confidence habitually). This social modulation of one’s confidence in a perceptual decision comprises the core of our model of social communication of uncertainty.</p><p>Following from an earlier study (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>), for each block the participants were led to believe that they were paired with a new, anonymous human partner. In reality, in separate blocks, they were paired with four computer generated partners (henceforward, CGPs; see Materials and methods) constructed and tuned to parameters obtained from the participant’s own behavior in the isolated session: (1) high accuracy and high confidence (HAHC; i.e. this CGP’s decisions were more likely to be more confident as well as more accurate); (2) high accuracy and low confidence (HALC); (3) low accuracy and high confidence (LAHC); and (4) low accuracy and low confidence (LALC) (see Materials and methods for details). For study 2, we used two CGPs (HCA and LCA) while the agent accuracy was similar to those of participants (<xref ref-type="bibr" rid="bib11">Bang and Fleming, 2018</xref>) (Wilcoxon rank sum, p=0.37, <italic>df</italic> = 29, <italic>zval</italic> = 0.89). See <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for confidence and accuracy data of CGPs. Each participant completed 4 blocks of 200 trials cooperating with a different CGP in each block. Our questionnaire results also confirmed that our manipulation indeed worked (<xref ref-type="fig" rid="fig1s4">Figure 1—figure supplement 4</xref>) and more importantly none of the subject suspected their partners was an artificial one.</p><p>Having observed the confidence matching effect in both studies (<xref ref-type="fig" rid="fig1">Figure 1b</xref>), a permutation analysis confirmed that this effect did not arise trivially from mere pairing with any random partner (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>; <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>). The difference between the participant’s confidence and that of their partner was smaller in the social (vs isolated) condition (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>) consistent with the prediction that participants would match their average confidence to that of their partner in the social session (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>).</p><p>Having established the socially emergent phenomenon of confidence matching in the dynamic RDM paradigm, we then proceeded to examine choice speed, accuracy, and confidence under social conditions (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). We observed that when participants were paired with a high (vs low) confidence partner, there was no significant difference in accuracy between the social conditions (p=0.92, p=0.75 for study 1 and 2 respectively, generalized linear mixed model [GLMM], see Supplementary materials for details of the analysis [<xref ref-type="table" rid="table1">Table 1</xref>], <xref ref-type="fig" rid="fig1">Figure 1c</xref> top-left panel); confidence, however, was significantly higher (p&lt;0.001 for both studies, <xref ref-type="table" rid="table1">Table 1</xref>, <xref ref-type="fig" rid="fig1">Figure 1c</xref> middle panel) and RTs were significantly faster (p&lt;0.001 for both, <xref ref-type="table" rid="table1">Table 1</xref>, <xref ref-type="fig" rid="fig1">Figure 1c</xref> bottom panel) in the HCA vs LCA.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Details of statistical results in behavioral data (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Response</th><th align="left" valign="bottom">Regressors</th><th align="left" valign="bottom">Estimate</th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">t-Stat</th><th align="left" valign="bottom">p-Value</th><th align="left" valign="bottom">Total number</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="6"><bold>Study 1</bold></td><td align="left" valign="bottom" rowspan="2"><bold>Accuracy</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.007</td><td align="char" char="." valign="bottom">0.0006</td><td align="char" char="." valign="bottom">[0.006 0.008]</td><td align="char" char="." valign="bottom">11.57</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">–0.002</td><td align="char" char="." valign="bottom">0.021</td><td align="char" char="." valign="bottom">[–0.045 0.04]</td><td align="char" char="." valign="bottom">–0.1</td><td align="char" char="." valign="bottom">0.92</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>Confidence</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.0475</td><td align="char" char="." valign="bottom">0.0008</td><td align="char" char="." valign="bottom">[0.046 0.049]</td><td align="char" char="." valign="bottom">56.5</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">1.361</td><td align="char" char="." valign="bottom">0.03</td><td align="char" char="." valign="bottom">[1.31 1.42]</td><td align="char" char="." valign="bottom">46.4</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>RT</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">–0.005</td><td align="char" char="." valign="bottom">0.0001</td><td align="char" char="." valign="bottom">[–0.005 –0.004]</td><td align="char" char="." valign="bottom">–44.4</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">0.029</td><td align="char" char="." valign="bottom">0.004</td><td align="char" char="." valign="bottom">[–0.035 –0.021]</td><td align="char" char="." valign="bottom">7.85</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom" rowspan="6"><bold>Study 2</bold></td><td align="left" valign="bottom" rowspan="2"><bold>Accuracy</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.0209</td><td align="char" char="." valign="bottom">0.0016</td><td align="char" char="." valign="bottom">[0.017 0.024]</td><td align="char" char="." valign="bottom">13.23</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">–0.0092</td><td align="char" char="." valign="bottom">0.0296</td><td align="char" char="." valign="bottom">[–0.067 0.049]</td><td align="char" char="." valign="bottom">–0.31</td><td align="char" char="." valign="bottom">0.76</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>Confidence</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.1011</td><td align="char" char="." valign="bottom">0.1011</td><td align="char" char="." valign="bottom">[0.097 0.106]</td><td align="char" char="." valign="bottom">47.47</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">0.496</td><td align="char" char="." valign="bottom">0.037</td><td align="char" char="." valign="bottom">[0.42 0.56]</td><td align="char" char="." valign="bottom">13.32</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>RT</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">–0.009</td><td align="char" char="." valign="bottom">0.0003</td><td align="char" char="." valign="bottom">[–0.01 –0.008]</td><td align="char" char="." valign="bottom">–26.22</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">0.0363</td><td align="char" char="." valign="bottom">0.006</td><td align="char" char="." valign="bottom">[0.024 0.048]</td><td align="char" char="." valign="bottom">6.12</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr></tbody></table></table-wrap><p>This pattern of dissociations of speed and confidence from accuracy is non-trivial because the expectations of the standard sequential sampling models would be that a change in confidence should be reflected in change in accuracy (<xref ref-type="bibr" rid="bib57">Pouget et al., 2016</xref>; <xref ref-type="bibr" rid="bib63">Sanders et al., 2016</xref>). Many alternative mechanistic explanations are, in principle, possible. The rich literature on sequential sampling models in the random-dot paradigm permit articulating the components of such intuitive explanations as distinct computational models and comparing them by formal model comparison (see further below).</p><p>In order to assess the impact of social context on the participants’ level of subjective uncertainty and rule out two important alternative explanations of confidence matching, we next examined the pupil data. Several studies have recently established a link between state of uncertainty and baseline (i.e. non-luminance mediated) variations in pupil size (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="bib81">Wei and Wang, 2015</xref>; <xref ref-type="bibr" rid="bib51">Nassar et al., 2012</xref>; <xref ref-type="bibr" rid="bib20">Eldar et al., 2013</xref>; <xref ref-type="bibr" rid="bib50">Murphy et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Urai et al., 2017</xref>). If the impact of social context on confidence were truly reflective of a similar change in the participant’s belief about uncertainty, then we would expect the smaller pupil size when paired with high (HCA) vs low confidence agent (LCA) indicating lower subjective uncertainty. Alternatively, if confidence matching were principally due to pure imitation (<xref ref-type="bibr" rid="bib59">Rendell et al., 2011</xref>; <xref ref-type="bibr" rid="bib35">Iacoboni, 2009</xref>) or due to some form of social obligation in agreeing with others (e.g. normative conformity [<xref ref-type="bibr" rid="bib69">Stallen and Sanfey, 2015</xref>]) without any change in belief, we would expect the pupil size to remain unaffected by pairing condition under social context. We found that during the inter-trial interval (ITI), pupil size was larger in the blocks where participants were paired with LCA (vs HCA) (<xref ref-type="fig" rid="fig2">Figure 2</xref>, GLMM analysis, p&lt;0.01 and p&lt;0.001 for study 1 and 2 respectively, see Supplementary materials for details of the analysis; <xref ref-type="table" rid="table2">Table 2</xref>). We have added a time series analysis that demonstrates the temporal encoding of experimental conditions in the pupil signal during ITI (see <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). It is important to bear in mind that pupil dilation has been linked to other factors such as mental effort (<xref ref-type="bibr" rid="bib43">Lee and Daunizeau, 2021</xref>), level of surprise (<xref ref-type="bibr" rid="bib40">Kloosterman et al., 2015</xref>), and arousal level (<xref ref-type="bibr" rid="bib50">Murphy et al., 2014</xref>) as well. These caveats notwithstanding, the patterns of pupil dilation within the time period of ITI that are demonstrated and replicated here, are consistent with the hypothesis that participants’ subjective belief was shaped by interactions with differently confident partners. To support this conclusion further, we provide supplementary evidence linking the participant’s own confidence to pupil size (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Pupil size during inter-trial interval (ITI) under pairing conditions in the social context when participant was paired with a high (HCA) or low confidence (LCA) agent.</title><p>Normalized pupil diameter aligned to start of ITI period (<italic>t</italic>=0). Vertical dashed lines show average ITI duration. The shaded areas are one standard deviation of ITI period in each condition. Inset shows grand average (mean) pupil size during ITI under the two social conditions. Error bars are 95% confidence interval across trials. (**) indicates p&lt;0.01 and (***) shows p&lt;0.001. In the interest of clarity, signals were smoothed using an averaging filter.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Pupil size correlates with participant’s own confidence in the isolated condition.</title><p>(<bold>a</bold>) In study 1, when confidence was lowest (i.e. rated 1) pupil size was larger (orange curve) than highest confidence (rated 6, magenta curve). The shaded area indicates the average inter-trial interval starting from 0. (<bold>b</bold>) Study 2. (<bold>c</bold>) Regression betas from analysis of the impact of confidence on the pupil data in the isolated blocks. The generalized linear mixed model (GLMM) was: PupilData = b0+b1*Accuracy+b2*confidence+b3*Coherence. b2, the highlighted bar, shows the effect of confidence while the effect of coherence and accuracy are regressed out. PupilData is the average pupil size across the ITI period. Error bars are 95% CI. (<bold>d</bold>) same as (<bold>c</bold>) but for study (2).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Time series analysis of pupil size during inter-trial interval.</title><p>For each study, we employed a generalized linear mixed model (GLMM) Pupil(t)=b0+b1*socialCondition where socialCondition is high confidence agent (HCA) = 1 or low confidence agent (LCA) = 2. The time course of coefficient b1 is plotted with the corresponding SEs. We discretized time into non-overlapping bins of 100 ms width and averaged the data points falling within each bin. (<bold>a</bold>) Study 1. (<bold>b</bold>) Study 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig2-figsupp2-v1.tif"/></fig></fig-group><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Details of statistical results in pupil data (<xref ref-type="fig" rid="fig2">Figure 2</xref>).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Response</th><th align="left" valign="bottom">Regressors</th><th align="left" valign="bottom">Estimate</th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">t-Stat</th><th align="left" valign="bottom">p-Value</th><th align="left" valign="bottom">Total number</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Study 1</bold></td><td align="left" valign="bottom"><bold>Pupil</bold></td><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">–0.038</td><td align="char" char="." valign="bottom">0.011</td><td align="char" char="." valign="bottom">[–0.06 –0.01]</td><td align="char" char="." valign="bottom">–3.30</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">8390</td></tr><tr><td align="left" valign="bottom"><bold>Study 2</bold></td><td align="left" valign="bottom"><bold>Pupil</bold></td><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">–0.066</td><td align="char" char="." valign="bottom">0.015</td><td align="char" char="." valign="bottom">[–0.09 –0.04]</td><td align="char" char="." valign="bottom">–4.37</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">5842</td></tr></tbody></table></table-wrap><p>To arbitrate between alternative explanations and develop a neural hypothesis for the impact of social context on decision speed and confidence, we constructed a neural attractor model (<xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref>), a variant from the family of sequential sampling models of choice under uncertainty (<xref ref-type="bibr" rid="bib14">Bogacz et al., 2006</xref>). Briefly, in this model, noisy sensory evidence was sequentially accumulated by two competing mechanisms (red and blue in <xref ref-type="fig" rid="fig3">Figure 3a</xref> left) that raced toward a common pre-defined decision boundary (<xref ref-type="fig" rid="fig3">Figure 3a</xref> right) while mutually inhibiting each other. Choice was made as soon as one mechanism hits the boundary. This model has accounted for numerous observations of perceptual and value-based decision-making behavior and their underlying neuronal substrates in human (<xref ref-type="bibr" rid="bib34">Hunt et al., 2012</xref>) and non-human primate (<xref ref-type="bibr" rid="bib81">Wei and Wang, 2015</xref>) brain. Following previous works (<xref ref-type="bibr" rid="bib81">Wei and Wang, 2015</xref>; <xref ref-type="bibr" rid="bib9">Balsdon et al., 2020</xref>; <xref ref-type="bibr" rid="bib61">Rolls et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Atiya et al., 2019</xref>) we defined model confidence as the time-averaged difference between the activity of the winning and losing accumulators (corresponding to the shaded gray area between the two accumulator traces in <xref ref-type="fig" rid="fig3">Figure 3a</xref> right, for the model simulation see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>) during the period of stimulus presentation (from 0 to 500 ms). Importantly, this definition of confidence is consistent with recent findings that computations of confidence continue <italic>after</italic> a decision has been made as long as sensory evidence is available (<xref ref-type="bibr" rid="bib62">Ruff and Fehr, 2014</xref>; <xref ref-type="bibr" rid="bib9">Balsdon et al., 2020</xref>; <xref ref-type="bibr" rid="bib77">van Kempen et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Moran et al., 2015</xref>). We also demonstrate that our results do not depend on this specific formulation and also replicate with another alternative method (<xref ref-type="bibr" rid="bib79">Vickers, 1979</xref>)(see <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Neural attractor model.</title><p>(<bold>a</bold>) Left: A common top-down (<italic>W<sub>x</sub></italic>) current drives both populations, each selective for a different choice alternative. Right: A schematic illustration of the impact of a positive top-down drive on accumulator dynamics. Confidence corresponds to the shaded area between winning (blue) and losing (red) accumulators. Solid lines and dark gray shade: positive top-down drive; dashed lines and light gray shade: zero top-down drive. With positive top-down current, the winner hits the bound earlier (<italic>t</italic>1 vs <italic>t</italic>2) and the surface area between the competing accumulator traces is larger (dark vs light gray). (<bold>b</bold>) Systematic examination of the impact of <italic>W<sub>x</sub></italic> on model behavior. Left panel: Accuracy does not depend on the top-down current but confidence (middle) and reaction time (RT) (right) change accordingly. Colors indicate different levels of top-down current. Each curve is the average of 10,000 simulations of the model given the top-down current. (<bold>c</bold>) Dynamic coupling in simulated dyadic interaction. Virtual dyads were constructed by feeding one model’s confidence in previous trial to the other model as top-down drive and vice versa. (<bold>d</bold>) Left: Unconnected virtual dyad members (<italic>W<sub>x</sub></italic> = 0) simulate the isolated condition. Right: When the virtual dyad members are connected with top-down drive proportional to one another’s confidence in previous trial, dyad members’ confidence converge over time. In the isolated condition, confidence matching is not observed even though the pair receive the exact same sequence of stimuli. Shadowed areas of the confidence interval 95% resulted from 50 parallel simulations and curves were smoothed by an averaging filter for clearer illustration. The correlation with coherence has been removed from the confidence values via residual analysis (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> confidence values).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Confidence matching without removing the correlation with the shared stimulus coherence.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>The effect of top-down current on the attractor network.</title><p>The results of model simulations with a specific value of top-down current (<italic>W<sub>x</sub></italic> = 0.003). This plot shows the average accumulated evidence of the model in 1000 repetitions with (solid lines) and without (dashed lines) top-down current for a 0.5 s duration of stimulus with 6.4% coherency. Importantly, the network was shut down after stimuli offset, receiving only the noise terms (<xref ref-type="disp-formula" rid="equ7 equ8">Equations 7; 8</xref>) for 2 s (i.e. more than reaction times [RTs] of 99% of trials). The dot-dashed black line indicates the decision threshold. Inset plots show how the accuracy (ACC), absolute difference of firing rates of two populations (In-Out), and decision time (Dec. Time) changed in the presence (black bars) or absence (white bars) of the top-down current. (***) indicates p&lt;0.001, paired t-test between runs; n.s. denotes that there is no significant difference between conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Model performance regarding different confidence representations.</title><p>(<bold>a</bold>) Confidence representation based on <xref ref-type="disp-formula" rid="equ15">Equation 15</xref>. (<bold>b</bold>) Same as (a) but here confidence is calculated as the absolute difference of the winner and loser signal but only at the end of the stimulus duration (500 ms). Each curve is a simulation of 10,000 trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Model comparison.</title><p>We fit each model to the data from high confidence agent (HCA) and low confidence agent (LCA) blocks of each subject (<italic>N</italic>=15 (subjects) * 2 (blocks)=30). (<bold>a</bold>) Pie chart indicates the distribution of best (winning) model for the subjects and conditions. (<bold>b</bold>) Model comparison for each subjects (panels: subject number) in HCA condition (<italic>N</italic>=15). (<bold>c</bold>) Same as (<bold>b</bold>) but for LCA.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Model vs data.</title><p>(<bold>a–c</bold>) Correspondence between behavioral data (black circles, 40 trials per coherence level in the isolated session) and the model fits (red curves, simulation with 1000 trials per coherence level) plotted for each participant (<italic>n</italic>=15). Error bars are SEM across trials. (<bold>d</bold>) The average behavior across participants was closely captured by the average of model simulations in panel (<bold>a–c</bold>). Here, error bars are SEM across participants. Shaded area is the SEM across model simulations correspondent to each subject.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp5-v1.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 6.</label><caption><title>The speed of confidence matching.</title><p>(<bold>a</bold>) Empirical data depicting the time course of confidence matching. The difference between the subject’s confidence and that of the agent (<italic>y</italic>-axis) are averaged within a three-trial time window and plotted. To empirically examine the speed of confidence matching we have plotted the empirically observed timeline of confidence matching in the two studies. Here, the absolute difference between the confidence of the agent and that of the subject is plotted against the trial number which indicates time. Observing the curves suggests that confidence matching starts quickly and then slows down as indicated by our simulations. The empirical data is, naturally, noisier but the results do indicate that most of the matching happens at the very beginning. Shaded areas are SEM across participants. Signals were smoothed for illustration purposes. (<bold>b–d</bold>) Simulations of the modified model where the top-down current was modulated by time. Higher values of the time constant <inline-formula><mml:math id="inf3"><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:math></inline-formula> decrease the speed of matching. (<bold>b</bold>) We applied a different version of top-down current, in which the amount of top-down current is dependent to trial number ( <inline-formula><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi mathvariant="bold-italic"> </mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold-italic">i</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:math></inline-formula> where <italic>i</italic> is the trial number and <inline-formula><mml:math id="inf5"><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:math></inline-formula> is the speed parameter. <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">W</mml:mi><mml:mi mathvariant="bold-italic">x</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the asymptote value: dashed line). Lower values of <inline-formula><mml:math id="inf7"><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:math></inline-formula> indicates faster matching (<bold>c</bold>) rather than higher values of <inline-formula><mml:math id="inf8"><mml:mi mathvariant="bold-italic">τ</mml:mi></mml:math></inline-formula> (<bold>d</bold>) that indicates slower matching.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp6-v1.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 7.</label><caption><title>Model falsification.</title><p>(<bold>a</bold>) Top-down model. We simulated two versions of the model (2000 trials per coherence level) in which only top-down current was different between conditions (TD in high confidence agent [HCA] is higher than low confidence agent [LCA]). The model could show effects similar to behavior (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). (<bold>b</bold>) Same as (<bold>a</bold>) but for the bound model. This model could only reproduce the reaction time (RT) effect but not accuracy and confidence effect. (<bold>c</bold>) Same as (<bold>a</bold>) but the non-decision time (NDT) model. This model could show the no difference effect in accuracy, yet the confidence also does not change which is not in line with behavioral data (<xref ref-type="fig" rid="fig1">Figure 1c</xref>, main text). Moreover, the RT also changes in way that is stimulus independent that is also not similar to the behavioral pattern. (<bold>d</bold>) Same as (<bold>a</bold>) but for the Mu (gain) model. The model fails to reproduce accuracy effect. Moreover, the difference of confidence observed in this model is not stimulus/coherence dependent (unlike TD simulations (<bold>a</bold>) and behavioral data <xref ref-type="fig" rid="fig1">Figure 1c</xref>). See also <ext-link ext-link-type="uri" xlink:href="https://github.com/Jimmy-2016/ConfMatchEEG/tree/main/test_alternative_models">https://github.com/Jimmy-2016/ConfMatchEEG/tree/main/test_alternative_models</ext-link>, ( <xref ref-type="bibr" rid="bib21">Esmaily, 2023</xref>; <xref ref-type="bibr" rid="bib47">MathWorks Inc, 2023</xref>) for trying different values of parameters.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp7-v1.tif"/></fig><fig id="fig3s8" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 8.</label><caption><title>Model predictions for confidence matching are not sensitive to linearity assumptions.</title><p>(<bold>a</bold>) Replication of <xref ref-type="fig" rid="fig3">Figure 3b</xref> of the main text in which two models are coupled linearly (see <xref ref-type="disp-formula" rid="equ18">Equation 18</xref>) and show confidence matching. (<bold>b</bold>) Same as (<bold>a</bold>) but with quadratic coupling. In order to show the robustness of our findings to dropping the linearity assumption, we conducted another analysis in which the two instances of the model interact via a quadratic relationship (<italic>W</italic><sub><italic>x</italic></sub> = <italic>α</italic> * Conf2). Here, <italic>α</italic> was set to –0.001 and 0.006 for high confident and low confident models respectively, while every other parameter as well as the random seed were frozen. The results are very similar to the linear model (panel (<bold>a</bold>) and also <xref ref-type="fig" rid="fig3">Figure 3d</xref>). Indeed, our model is only sensitive to <italic>W</italic><sub><italic>x</italic></sub> as we shown in <xref ref-type="fig" rid="fig3">Figure 3b</xref>. As long as this parameter changes in response to the partner’s confidence, the exact form of the relationship i.e., linear or quadratic, with the partner’s confidence is not important.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig3-figsupp8-v1.tif"/></fig></fig-group><p>Earlier works that demonstrated the relationship between decision uncertainty and pupil-related, global arousal state in the brain (<xref ref-type="bibr" rid="bib50">Murphy et al., 2014</xref>; <xref ref-type="bibr" rid="bib75">Urai et al., 2017</xref>) guided our modeling hypothesis. We modeled the social context as a global, top-down additive input (<xref ref-type="fig" rid="fig3">Figure 3a</xref>; <italic>W<sub>x</sub></italic>) in the attractor model. This input drove both accumulator mechanisms equally and positively. The impact of this global top-down input is illustrated in <xref ref-type="fig" rid="fig3">Figure 3a</xref> right: with a positive top-down drive (<italic>W</italic><sub><italic>x</italic></sub>&gt;0), the winner (thick blue) and the loser (thick red) traces both rise faster compared to zero top-down drive (dotted lines). The model’s counterintuitive feature is that the surface area between the winning and losing accumulator is larger in the case of positive (dark gray shading) versus zero (light gray shading) top-down input. Model simulations show that when 0&lt;<italic>W<sub>x</sub></italic>, this difference in surface area leads to faster RTs and higher confidence but does not change accuracy because it does not affect the decision boundary. These simulation results are consistent with our behavioral findings comparing HCA vs LCA conditions (<xref ref-type="fig" rid="fig1">Figure 1c</xref>).</p><p>We formally compared our model to three alternative, plausible models of how social context may affect the decision process. Without loss of generality, we used data from study 2 to fit the model. The first model hypothesized that partner’s confidence dynamically modulated the decision bound (<xref ref-type="bibr" rid="bib9">Balsdon et al., 2020</xref>) (parameter <italic>B</italic> in <xref ref-type="disp-formula" rid="equ21">Equation 21</xref>). In this model, the partner’s higher confidence reduced the threshold for what counted as adequate evidence, producing the faster RTs under HCA (<xref ref-type="fig" rid="fig1">Figure 1</xref>.c). The second model proposed that partner’s confidence changed non-decision time (NDT) (<xref ref-type="bibr" rid="bib70">Stine et al., 2020</xref>; <xref ref-type="disp-formula" rid="equ22">Equation 22</xref>). Here, pairing with high confidence partner would not have any impact on perceptual processing but instead, non-specifically decrease RTs across all coherence levels without affecting accuracy. Finally, in the third model, the stimulus-independent perceptual gain (<xref ref-type="bibr" rid="bib20">Eldar et al., 2013</xref>; <xref ref-type="bibr" rid="bib44">Li et al., 2018</xref>) parameter of input current (parameter <italic>μ</italic><sub>0</sub> in <xref ref-type="disp-formula" rid="equ23">Equation 23</xref>) was modulated by partner confidence. Here, higher partner confidence increased the perceptual gain (as if increasing the volume of the radio) leading to increased confidence and decreased RT (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) and would be consistent with the pupillometry results. In each model, in the social condition, the parameter of interest was linearly modulated by the confidence of the partner in the previous trial. Importantly, in <xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>, we show that empirically, such trial-by-trial dependence is observed in confidence and RTs data in both study 1 and 2. Formal model comparison showed that our top-down additive current model was superior to all three alternatives (see <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>).</p><p>Having shown that a common top-down drive can qualitatively reproduce the impact of social context on speed-accuracy-confidence and quantitatively excel other alternatives in fitting the observed behavior, we then used the winning model to simulate our interactive social experiment virtually (<xref ref-type="fig" rid="fig3">Figure 3c</xref>). We simulated one decision maker with high confidence (subject 1 in <xref ref-type="fig" rid="fig3">Figure 3d</xref>) and another one with low confidence (subject 2). To simulate subject 1, we slightly increased the excitatory and the inhibitory weights. The opposite was done to simulate subject 2 (see Materials and methods for details). We then paired the two simulated agents by feeding the confidence of each virtual agent (from trial <italic>t–</italic>1) (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>) as top-down input to the other virtual agent (in trial <italic>t</italic>).</p><p>Using this virtual social experiment, we simulated the dyadic exchanges of confidence in the course of our experiment and drew a prediction that could be directly tested against the empirical behavioral data. Without any fine-tuning of parameters or any other intervention, confidence matching emerged spontaneously when two virtual agents with very different confidence levels in isolated condition (<xref ref-type="fig" rid="fig3">Figure 3d</xref> left) were paired with each other as a dyad (<xref ref-type="fig" rid="fig3">Figure 3d</xref> right). Importantly, the model could be adapted to show different speed of matching as well (see <xref ref-type="fig" rid="fig3s6">Figure 3—figure supplement 6</xref>). However, for simplicity we presented the simplest case in the main text.</p><p>To identify the neural correlates of interpersonal alignment of belief about uncertainty, we note that previous works using non-invasive electrophysiological recordings in humans engaged in motion discrimination (<xref ref-type="bibr" rid="bib74">Twomey et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Stolk et al., 2013</xref>) have identified the signature, accumulate-to-bound neural activity characteristic of evidence accumulation in the sequential sampling process. Specifically, these findings show a centropareital positivity (CPP) component in the event-related potential that rises with sensory evidence accumulation across time. The exact correspondence between the neural CPP and elements of the sequential sampling process are not yet clear (<xref ref-type="bibr" rid="bib54">O’Connell et al., 2018</xref>). For example, CPP could have resulted from the spatial superposition of the electrical activity of both accumulators or be the neural activity corresponding to the difference in accumulated evidence. These caveats notwithstanding, consistent with the previous literature, we found that in the isolated condition, our data replicated those earlier findings: <xref ref-type="fig" rid="fig4">Figure 4a</xref> shows a clear CPP event-related potential whose slope of rise was strongly modulated by motion coherence (GLMM, p&lt;0.001 and p=0.01 for study 1 and 2 receptively, see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1d</xref> and <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> for more details). Importantly, we have added the response-locked analysis of the CPP signals (see <xref ref-type="fig" rid="fig4s4">Figure 4—figure supplement 4</xref>). We do see that the response-locked CPP waveforms converge to one another for high vs low coherence trials at the moment of the response.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Coupling of neural evidence accumulation to social exchange of information.</title><p>(<bold>a</bold>) Centroparietal positivity (CPP) component in the isolated condition: event-related potentials are time-locked to stimulus onset, binned for high and low levels of coherency (for study 1, low: 3.2%, 6.4%, 12.8%; high: 25.6% and 51.2%; for study 2 (<bold>d</bold>), low: 1.6%, 3.2%, 6.4%; high: 12.8%, 25.6%) and grand averaged across centropatrial electrodes (see Materials and methods). Inset shows the topographic distribution of the EEG signal averaged across the time window indicated by the gray area. (<bold>b</bold>) CPP under social condition. Conventions the same as panel (a). (<bold>c</bold>) A generalized linear mixed model (GLMM) model showed the significant relation of centroparietal signals to levels of coherency and social condition (high confidence agent [HCA] vs low confidence agent [LCA]). Error bars are 95% confidence interval over the model’s coefficient estimates. Signals were smoothed by an averaging filter; shaded areas are SEM across trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Electrode placement in each study.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Relation of EEG signals from centropartial area of the brain to coherence levels and social conditions.</title><p>Top-left, ramping activities of the signals (calculated by a linear regression of signals amplitudes and the time windows of 0–500 ms) is modulated by coherence levels (generalized linear mixed model [GLMM] study 1: p&lt;0.001, study 2 (top-right): p=0.01). Bottom-left: relation of signals slope and social condition (high confidence agent [HCA] vs low confidence agent [LCA]) based on different coherence levels. HCA shows a steeper slope (see <xref ref-type="fig" rid="fig4">Figure 4</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1c</xref> for more details). Right column is the same as left column but for study 2.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Simulated slope of the accumulator activity in our computational model in low confidence agent (LCA) and high confidence agent (HCA) conditions.</title><p>(<bold>a</bold>) Slope of the winning accumulator (time window: 0–500 ms; shaded area, insets) at each coherence level for LCA and HCA condition. (<bold>b</bold>) Same as panel (<bold>a</bold>) but here for the <italic>difference</italic> in accumulator activities. Error bars are SEM across trials (n=3000, for each model).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig4-figsupp3-v1.tif"/></fig><fig id="fig4s4" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 4.</label><caption><title>Response-locked EEG signal separated for high vs low coherence levels.</title><p>(<bold>a</bold>) As expected from previous studies (<xref ref-type="bibr" rid="bib36">Kelly and O’Connell, 2013</xref>; <xref ref-type="bibr" rid="bib45">Loughnane et al., 2018</xref>; <xref ref-type="bibr" rid="bib54">O’Connell et al., 2018</xref>; <xref ref-type="bibr" rid="bib76">Vafaei Shooshtari et al., 2019</xref>) centropareital positivity (CPP) signals to high vs low coherence stimulus converge to one another around the moment of response (dashed line, Wilcoxon rank sum test p=0.35). (<bold>b</bold>) Study 2. It is worth noting that previous studies that examined response-locked CPP employed reaction time (or long duration) tasks with variable stimulus duration. In our study, however, stimulus duration was short and fixed. Our data therefore provide a new addition to this literature confirming response-locked CPP do not depend on termination of stimulus by response.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig4-figsupp4-v1.tif"/></fig><fig id="fig4s5" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 5.</label><caption><title>Power calculation (Monte Carlo simulation) for EEG slope effect (<xref ref-type="fig" rid="fig4">Figure 4</xref> in the main manuscript).</title><p>Our power calculator suggests we need 17 participants. EEG slope effect was the only effect that was not statistically significant in the first study.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-fig4-figsupp5-v1.tif"/></fig></fig-group><p>Our model hypothesized that under social condition, a top-down drive – determined by the partner’s communicated confidence in the previous trial – would modulate the rate of evidence accumulation (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). We tested if the CPP slope were larger <italic>within</italic> every given coherence bin when the participant was paired with an HCA (vs LCA). Indeed, the data demonstrated a larger slope of CPP rise under HCA vs LCA (<xref ref-type="fig" rid="fig4">Figure 4c</xref>, study 1 for the social condition p=0.15 but for the second study p&lt;0.01, see <xref ref-type="table" rid="table3 table4">Tables 3 and 4</xref> for more details). These findings demonstrate that interpersonal alignment of confidence is associated with a modulation of neural evidence accumulation – as quantified by CPP – by the social exchange of information (also see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>). It is important to note a caveat here before moving forward. These data show that both CPP and confidence are different between the HCA and LCA conditions. However, due to the nature of our experimental design, it would be premature to conclude from them that CPP <italic>contributes causally to</italic> the alignment of subjectively held beliefs or behaviorally expressed confidence. Put together with the behavioral confidence matching (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) and the pupil data (<xref ref-type="fig" rid="fig2">Figure 2</xref>) our findings suggest that some such neural-social coupling could be the underlying basis for the construction of a shared belief about uncertainty.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Details of statistical results in EEG data (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Response</th><th align="left" valign="bottom">Regressors</th><th align="left" valign="bottom">Estimate</th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">t-Stat</th><th align="left" valign="bottom">p-Value</th><th align="left" valign="bottom">Total number</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="2"><bold>Study 1</bold></td><td align="left" valign="bottom" rowspan="2"><bold>EEG slope</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.62</td><td align="char" char="." valign="bottom">0.065</td><td align="char" char="." valign="bottom">[0.49. 074]</td><td align="char" char="." valign="bottom">9.64</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6492</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">0.2</td><td align="char" char="." valign="bottom">0.14</td><td align="char" char="." valign="bottom">[-0.07 0.49]</td><td align="char" char="." valign="bottom">1.42</td><td align="char" char="." valign="bottom">0.15</td><td align="char" char="." valign="bottom">6492</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>Study 2</bold></td><td align="left" valign="bottom" rowspan="2"><bold>EEG slope</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.8</td><td align="char" char="." valign="bottom">0.29</td><td align="char" char="." valign="bottom">[0.24 1.37]</td><td align="char" char="." valign="bottom">2.8</td><td align="char" char="." valign="bottom">&lt;0.01</td><td align="char" char="." valign="bottom">5367</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="char" char="." valign="bottom">1.52</td><td align="char" char="." valign="bottom">0.63</td><td align="char" char="." valign="bottom">[0.27 2.77]</td><td align="char" char="." valign="bottom">2.39</td><td align="char" char="." valign="bottom">0.017</td><td align="char" char="." valign="bottom">5367</td></tr></tbody></table></table-wrap><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Details of statistical results in EEG data (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref> top row).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Response</th><th align="left" valign="bottom">Regressors</th><th align="left" valign="bottom">Estimate</th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">t-Stat</th><th align="left" valign="bottom">p-Value</th><th align="left" valign="bottom">Total number</th></tr></thead><tbody><tr><td align="left" valign="bottom"><bold>Study 1</bold></td><td align="left" valign="bottom"><bold>EEG slope</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.02</td><td align="char" char="." valign="bottom">0.005</td><td align="char" char="." valign="bottom">[0.01 0.03]</td><td align="char" char="." valign="bottom">4.48</td><td align="char" char="." valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">1523</td></tr><tr><td align="left" valign="bottom"><bold>Study 2</bold></td><td align="left" valign="bottom"><bold>EEG slope</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="char" char="." valign="bottom">0.06</td><td align="char" char="." valign="bottom">0.02</td><td align="char" char="." valign="bottom">[0.01 0.11]</td><td align="char" char="." valign="bottom">2.54</td><td align="char" char="." valign="bottom">&lt;0.01</td><td align="char" char="." valign="bottom">2822</td></tr></tbody></table></table-wrap></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We brought together two so-far-unrelated research directions: confidence in decision making under uncertainty and interpersonal alignment in communication. Our approach offers solutions to important current problems in each.</p><p>For decision science, we provide a model-based, theoretically grounded neural mechanism for going from individual, idiosyncratic representations of uncertainty (<xref ref-type="bibr" rid="bib53">Navajas et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Fleming et al., 2010</xref>) to socially transmitted confidence expressions (<xref ref-type="bibr" rid="bib6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>) that are seamlessly shared and allow for successful cooperation. The social-to-neuronal coupling mechanism that we borrowed from the communication literature (<xref ref-type="bibr" rid="bib31">Hasson and Frith, 2016</xref>; <xref ref-type="bibr" rid="bib82">Wheatley et al., 2019</xref>) is crucial in this new understanding of the neuronal basis of relationship between subjectively private and socially shared uncertainty.</p><p>For communication science, by examining perceptual decision making under uncertainty in social context, we created a laboratory model in which the goal of communication was to arrive at a shared belief about uncertainty (rather than creating a look-up table for the meaning of actions [<xref ref-type="bibr" rid="bib73">Stolk et al., 2016</xref>; <xref ref-type="bibr" rid="bib66">Silbert et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Honey et al., 2012</xref>]). In this way, we could employ the extensive theoretical, behavioral, and neurobiological body of knowledge in decision science (<xref ref-type="bibr" rid="bib57">Pouget et al., 2016</xref>; <xref ref-type="bibr" rid="bib1">Adler and Ma, 2018</xref>; <xref ref-type="bibr" rid="bib3">Aitchison et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Hanks and Summerfield, 2017</xref>; <xref ref-type="bibr" rid="bib27">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib62">Ruff and Fehr, 2014</xref>; <xref ref-type="bibr" rid="bib64">Schall, 2019</xref>; <xref ref-type="bibr" rid="bib38">Kiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib36">Kelly and O’Connell, 2013</xref>; <xref ref-type="bibr" rid="bib65">Shadlen and Newsome, 2001</xref>; <xref ref-type="bibr" rid="bib60">Resulaj et al., 2009</xref>; <xref ref-type="bibr" rid="bib63">Sanders et al., 2016</xref>; <xref ref-type="bibr" rid="bib81">Wei and Wang, 2015</xref>; <xref ref-type="bibr" rid="bib20">Eldar et al., 2013</xref>; <xref ref-type="bibr" rid="bib75">Urai et al., 2017</xref>; <xref ref-type="bibr" rid="bib84">Yeung and Summerfield, 2012</xref>; <xref ref-type="bibr" rid="bib23">Fleming and Daw, 2017</xref>; <xref ref-type="bibr" rid="bib37">Kiani and Shadlen, 2009</xref>) to construct a mechanistic neural hypothesis for interpersonal alignment.</p><p>Over the past few years, the efforts to understand the ‘brain in interaction’ have picked up momentum (<xref ref-type="bibr" rid="bib82">Wheatley et al., 2019</xref>; <xref ref-type="bibr" rid="bib25">Frith and Frith, 1999</xref>). A consensus emerging from these works is that, at a conceptual level, successful interpersonal alignment entails the mutual construction of a shared cognitive space between brains (<xref ref-type="bibr" rid="bib72">Stolk et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Wheatley et al., 2019</xref>; <xref ref-type="bibr" rid="bib24">Friston and Frith, 2015</xref>). This would allow interacting brains to adjust their internal dynamics to converge on shared beliefs and meanings (<xref ref-type="bibr" rid="bib31">Hasson and Frith, 2016</xref>; <xref ref-type="bibr" rid="bib26">Gallotti and Frith, 2013</xref>). To identify the neurobiological substrates of such shared cognitive space, brain-to-brain interactions need to be described in terms of information flow, i.e., the impact that interacting partners have on one another’s brain dynamics (<xref ref-type="bibr" rid="bib82">Wheatley et al., 2019</xref>).</p><p>The evidence for such information flow has predominantly consisted of demonstrations of alignment of brain-to-brain activity (i.e. synchrony at macroscopic level, e.g. fMRI BOLD signal) when people process the same (simple or complex) sensory input (<xref ref-type="bibr" rid="bib33">Honey et al., 2012</xref>; <xref ref-type="bibr" rid="bib29">Hasson et al., 2004</xref>; <xref ref-type="bibr" rid="bib16">Breveglieri et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Mukamel et al., 2005</xref>; <xref ref-type="bibr" rid="bib30">Hasson and Honey, 2012</xref>) or engage in complimentary communicative (<xref ref-type="bibr" rid="bib66">Silbert et al., 2014</xref>) roles to achieve a common goal. More recently, dynamic coupling (rather than synchrony) has been suggested as a more general description of the nature of brain-to-brain interaction (<xref ref-type="bibr" rid="bib31">Hasson and Frith, 2016</xref>). Going beyond the intuitive notions of synchrony and coupling, to our knowledge, no computational framework – grounded in the principles of neural computing – has been offered that could propose a plausible quantitative mechanism for these empirical observations of brain-to-brain coupling.</p><p>Combining four different methodologies, the work presented here undertook this task. Behaviorally, our participants engaged in social perceptual decision making under various levels of sensory and social uncertainty (<xref ref-type="bibr" rid="bib6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>). Emergence of confidence matching (<xref ref-type="fig" rid="fig1">Figure 1b</xref>) showed that participants coordinated their decision confidence with their social partner. Pupil data (<xref ref-type="fig" rid="fig2">Figure 2</xref>) suggested that participant’s belief about uncertainty was indeed shaped by the social coordination. A dissociation (<xref ref-type="fig" rid="fig1">Figure 1c</xref>) of decision speed and confidence from accuracy was reported that depended on the social context. This trade-off, as well as the emergence of confidence matching, was successfully captured by a neural attractor model (<xref ref-type="fig" rid="fig3">Figure 3</xref>) in which two competing neural populations of evidence accumulators – each tuned to one choice alternative – were driven by a common top-down drive determined by social information. This model drew predictions for behavior (<xref ref-type="fig" rid="fig3">Figure 3d</xref>) and neuronal activity (<xref ref-type="fig" rid="fig4">Figure 4</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplements 1</xref>–<xref ref-type="fig" rid="fig4s5">5</xref>) that were born out by the data. Social exchange of information modulated the neural signature of evidence accumulation in the parietal cortex.</p><p>Although numerous previous works have employed sequential sampling models to explain choice confidence, the overwhelming majority (<xref ref-type="bibr" rid="bib57">Pouget et al., 2016</xref>; <xref ref-type="bibr" rid="bib3">Aitchison et al., 2015</xref>; <xref ref-type="bibr" rid="bib28">Hanks and Summerfield, 2017</xref>; <xref ref-type="bibr" rid="bib27">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib62">Ruff and Fehr, 2014</xref>; <xref ref-type="bibr" rid="bib64">Schall, 2019</xref>; <xref ref-type="bibr" rid="bib38">Kiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Sanders et al., 2016</xref>; <xref ref-type="bibr" rid="bib37">Kiani and Shadlen, 2009</xref>; <xref ref-type="bibr" rid="bib42">Krajbich and Rangel, 2011</xref>) have opted for the drift diffusion family of models. Neural attractor models have so far been rarely used to understand confidence (<xref ref-type="bibr" rid="bib61">Rolls et al., 2010</xref>; <xref ref-type="bibr" rid="bib4">Atiya et al., 2019</xref>; <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>). Our attractor model is a reduced version (<xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref>) of the original biophysical neural circuit model for motion discrimination (<xref ref-type="bibr" rid="bib80">Wang, 2002</xref>). The specific affordances of attractor models allowed us to implement social context as a sustained, tonic top-down feedback to both accumulator mechanisms. More importantly, we were able to simulate social interactive decision making by virtually pairing any given two instances of the model (one for each member of a dyad) with each other: the confidence produced by each in a given trial served as top-down drive for the other in the next trial. Remarkably, a shared cognitive space about uncertainty (i.e. confidence matching) emerged spontaneously from this simulated pairing without us having to tweak any model parameters.</p><p>At a conceptual level, deconstructing the social communication of confidence into a comprehension and a production process (<xref ref-type="bibr" rid="bib66">Silbert et al., 2014</xref>) is helpful. Comprehension process refers to how socially communicated confidence is incorporated in the recipient brain and affects their decision making. Production process refers to how the recipient’s own decision confidence is constructed to be, in turn, socially expressed. It is tempting to attribute the CPP neural activity in the parietal cortex to the production process. Comprehension process, in turn, could be the top-down feedback from prefrontal brain areas previously implicated in confidence and metacognition (<xref ref-type="bibr" rid="bib22">Fleming et al., 2010</xref>; <xref ref-type="bibr" rid="bib23">Fleming and Daw, 2017</xref>; <xref ref-type="bibr" rid="bib18">De Martino et al., 2017</xref>) to the parietal cortex. However, we believe that our neural attractor model in particular and the empirical findings do not lend themselves easily to this conceptual simplification. For example, the evidence accumulation process can be a part of the production (because confidence emerges from the integrated difference between accumulators) as well as the comprehension process (because the rate of accumulation is modulated by the received social information). As useful as it is, the comprehension/production dichotomy’s limited scope should be recognized. Instead, armed with the quantitative framework of neural attractor models (for each individual) and interactive virtual pairing (to simulate dyads), future studies can now go beyond the comprehension/production dichotomy and examine the neuronal basis of interpersonal alignment with a model that have a strong footing in biophysical realities of neural computation.</p><p>Several limitations apply to our study. We chose different sets of coherence levels for the discovery (experiment 1) and replication (experiment 2). This choice was made deliberately. In experiment 1 we included a very high coherence (51%) level to optimize the experimental design for demonstrating the CPP component in the EEG signal. In experiment 2, we employed peri-threshold coherence levels in order to focus on behavior around the perceptual threshold to strengthen the model fitting and model comparison. This trade-off created some marginal differences in the observed effect sizes in the neural data across the two studies. The general findings were in good agreement.</p><p>The main strength of our work was to put together many ingredients (behavioral data, pupil and EEG signals, computational analysis) to build a picture of how the confidence of a partner, in the context of joint decision making, would influence our own decision process and confidence evaluations. Many of the effects that we describe here are well described already in the literature but putting them all together in a coherent framework remains a challenge. For example, our study did not directly examine neural alignment between interaction partners. We measured the EEG signal one participant at a time. The participant interacted with an alleged (experimenter-controlled) partner in any given trial. Our experimental design, however, permitted strict experimental control and allowed us to examine the participants’ social behavior (i.e. choices and confidence), pupil response, and brain dynamics as they achieved interpersonal alignment with the partner. Moreover, while the hypotheses raised by our neural attractor model did examine the nature of brain dynamics involved in evidence accumulation under social context, testing these hypotheses did not require hyper-scanning of two participants at the same time. We look forward to future studies that use the behavioral and computational paradigm described here to examine brain-to-brain neural alignment using hyper-scanning.</p><p>We have interpreted our findings to indicate that social information, i.e., partner’s confidence, impacts the participants’ beliefs about uncertainty. It is important to underscore here that, similar to real life, there are other sources of uncertainty in our experimental setup that could affect the participants’ belief. For example, under joint conditions, the group choice is determined through the comparison of the choices and confidences of the partners. As a result, the participant has a more complex task of matching their response not only with their perceptual experience but also coordinating it with the partner to achieve the best possible outcome. For the same reason, there is greater outcome uncertainty under joint vs individual conditions. Of course, these other sources of uncertainty are conceptually related to communicated confidence, but our experimental design aimed to remove them, as much as possible, by comparing the impact of social information under high vs low confidence of the partner.</p><p>Our study brings together questions from two distinct fields of neuroscience: perceptual decision making and social neuroscience. Each of these two fields have their own traditions and practical common sense. Typically, studies in perceptual decision making employ a small number of extensively trained participants (approximately 6–10 individuals). Social neuroscience studies, on the other hand, recruit larger samples (often more than 20 participants) without extensive training protocols. We therefore needed to strike a balance in this trade-off between number of participants and number of data points (e.g. trials) obtained from each participant. Note, for example, that each of our participants underwent around 4000 training trials. Importantly, our initial study (<italic>N</italic>=12) yielded robust results that showed the hypothesized effects nearly completely, supporting the adequacy of our power estimate. However, we decided to replicate the findings in a new sample with <italic>N</italic>=15 participants to enhance the reliability of our findings and examine our hypothesis in a stringent discovery-replication design. In <xref ref-type="fig" rid="fig4s5">Figure 4—figure supplement 5</xref>, we provide the results of a power analysis that we applied on the data from study 1 (i.e. the discovery phase). These results demonstrate that the sample size of study 2 (i.e. replication) was adequate when conditioned on the results from study 1.</p><p>Finally, one natural limitation of our experimental setup is that the situation being studied is very specific to the design choices made by the experimenters. These choices were made in order to operationalize the problem of social interaction within the psychophysics laboratory. For example, the joint decisions were not an agreement between partners (<xref ref-type="bibr" rid="bib6">Bahrami et al., 2010</xref>; <xref ref-type="bibr" rid="bib7">Bahrami et al., 2012</xref>). Instead, following a number of previous works (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="bib12">Bang et al., 2020</xref>), joint decisions were automatically assigned to the most confident choice. In addition, partner’s confidence and choice were random variables drawn from a distribution prespecified by the experimenter and therefore, by design, unresponsive to the participant’s behavior. In this sense, one may argue that the interaction partner’s behavior was not ‘natural’ since they did not react to the participant’s confidence communications (note however that the partner’s response times and accuracy were not entirely random but matched carefully to the participant’s behavior prerecorded in the individual session). How much of the findings are specific to these experimental setting and whether the behavior observed here would transfer to other real-life settings is an open question. For example, it is plausible that participants may show some behavioral reaction to the response time variations since there is some evidence indicating that for binary choices like here, response times also systematically communicate uncertainty to others (<xref ref-type="bibr" rid="bib55">Patel et al., 2012</xref>). Future studies could examine the degree to which the results might be paradigm-specific.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Participants</title><p>A total of 27 participants (12 in experiment 1 and 15 in experiment 2; 10 females; average age: 24 years; all naïve to the purpose of the experiment) were recruited for a two-session experiment – isolated and social session. All subjects reported normal or corrected-to-normal vision. The participants did several training sessions in order to become familiar with the procedure and reach a consistent pre-defined level of sensitivity (see Materials and methods for more details).</p></sec><sec id="s4-2"><title>Recruitment</title><p>Participants volunteered to take part in the experiment in return for course credit for study 1. For study 2, a payment of 80,000 Toman equivalent to 2.5€ per session was made to each participant. On the experiment day, participants were first given the task instructions. Written informed consent was then obtained. The experiments were approved by the local Ethics Committee at Shaheed Rajaei University’s Department of computer engineering.</p></sec><sec id="s4-3"><title>Task design</title><p>In the isolated session, each trial started with a red fixation point in the center of the screen (diameter 0.3°). Having fixated for 300 ms (in study 1, for a few subjects with eye monitoring difficulty this period shortened), two choice-target points appeared at 10° eccentricity corresponding to the two possible motion directions (left and right) (<xref ref-type="fig" rid="fig1">Figure 1</xref>). After a short random delay (200–500 ms, truncated exponential distribution), a dynamic RDM stimulus was displayed for 500 ms in a virtual aperture (5° diameter) centered on the initial fixation point. These motion stimuli have been described in detail elsewhere (<xref ref-type="bibr" rid="bib65">Shadlen and Newsome, 2001</xref>). At the end of the motion stimulus a response panel (see <xref ref-type="fig" rid="fig1">Figure 1a</xref>) was displayed on the screen. This response panel consisted of a horizontal line extending from left to the right end of the display, centered on the fixation cross. On each side of the horizontal line, six vertical rectangles were displayed side by side (<xref ref-type="fig" rid="fig1">Figure 1a</xref>) corresponding to six confidence levels for each decision alternative. The participants reported the direction of the RDM stimulus and simultaneously expressed their decision and confidence using the mouse.</p><p>The rectangles on the right and left of the midpoint corresponded to the right and left choices, respectively. By clicking on the rectangles further the midpoint participants indicated higher confidence. In this way, participant indicated their confidence and choice simultaneously (<xref ref-type="bibr" rid="bib38">Kiani et al., 2014</xref>; <xref ref-type="bibr" rid="bib46">Mahmoodi et al., 2015</xref>) For experiment 1, response time was defined as the moment that the marker deviated (more than one pixel) from the center of the screen. However, in order to rule out the effect of unintentional movements, for the second study we increased this threshold to one degree of visual angle. The participants were informed about their accuracy by a visual feedback presented in the center of the screen for 1 s (correct or wrong).</p><p>In the social session, the participants were told they were paired with an anonymous partner. In fact, they were paired with a CGP tailored to the participant’s own behavior in their isolated session. The participants did not know about this arrangement. Stimulus presentation and private response phase were identical to the isolated session. After the private response, the participants were presented with a social panel right (<xref ref-type="fig" rid="fig1">Figure 1</xref>). In this panel, the participant’s own response (choice and confidence) were presented together with that of their partner for 1 s. The participant and the partner responses were color-coded (white for participants; yellow for partners). Joint decision was determined by the choice of the more confident person and displayed in green. Then, three distinct color-coded feedbacks were provided.</p><p>In both isolated and social sessions, the participants were seated in an adjustable chair in a semi-dark room with chin and forehead supported in front of a CRT display monitor (first study: 17 inches; PF790; refresh rate, 85 Hz; screen 164 resolution, 1024×768; viewing distance, 57 cm, second study: 21 inches; Asus VG248; refresh rate, 75 Hz; screen resolution, 1024×768; viewing distance, 60 cm). All the code was written in PsychToolbox (<xref ref-type="bibr" rid="bib15">Brainard, 1997</xref>; <xref ref-type="bibr" rid="bib39">Kleiner et al., 2007</xref>; <xref ref-type="bibr" rid="bib56">Pelli, 1997</xref>).</p></sec><sec id="s4-4"><title>Training procedure</title><p>Each participant went through several training sessions (on average 4) to be trained on RDM task. They first trained in a response-free (i.e. RT) version of the RDM task in which motion stimulus was discontinued as soon as the participant responded. They were told to decide about the motion direction of dots as fast and accurately as possible (<xref ref-type="bibr" rid="bib38">Kiani et al., 2014</xref>). Once they reached a <italic>stable</italic> kevel of accuracy and RT, they proceeded to the main experiment. Before participating in the main experiment, they performed another 20–50 trials of warm-up. Here, the stimulus duration was fixed and responses included confidence report. For the social sessions, participants were told that in every block of 200 trials, they would be paired with a different person, seated in another room, with whom they would collaborate. They were also instructed about the joint decision scheme and were reminded that the objective in the social task was to maximize collective accuracy. Data from training and warm-up trials were included in the main analysis.</p></sec><sec id="s4-5"><title>Procedure</title><p>Each participant performed both the isolated and the social task. In the isolated session, they did one block containing 200 trials. Acquired data were employed to construct four computer partners for the first study and two partners for the second study. We used the procedure introduced in a previous works to generate CGPs (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>; <xref ref-type="bibr" rid="bib13">Bang et al., 2022</xref>). In the first study, the four partners were distinguished by their level of average accuracy and overall confidence: HAHC, HALC, LAHC, and finally LALC. For the second study partners only differed in confidence: HCA and LCA. Each participant performed one block of 200 trials for each of the paired partners – 800 overall for study 1 and 400 overall for study 2.</p><p>In the social session, participants were told to try to maximize the joint decision success (<xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>). They were told that their payment bonus depended on by their joint accuracy (<xref ref-type="bibr" rid="bib12">Bang et al., 2020</xref>). While performing the behavioral task, EEG signals and pupil data were also recorded.</p></sec><sec id="s4-6"><title>Computer generated partner</title><p>In study 1, following <xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>, four partners were generated for each participant tuned to the participant’s own behavioral data in the isolated session. Briefly, we created four simulated partners by varying their mean accuracy (high or low) and mean confidence (high or low). First, in the isolated session, the participant’s sensory noise (<italic>σ</italic>) and a set of thresholds that determined the distribution of their confidence responses were calculated (see Materials and methods also). Simulated partner’s accuracy was either high (0.3×<italic>σ</italic>) or low (1.2×<italic>σ</italic>). Mean confidence of simulated partners were also set according to the participant’s own data. For low confidence simulated partner, average confidence was set to the average of participant’s confidence in the low coherence (3.2% and 6.4%) trials. For the high confidence simulated partners, mean confidence was set to the average confidence of the participant in the high coherence (25.6% and 51.2%) trials. RTs were chosen randomly by sampling from a uniform random distribution (from 0.5 to 2 s). Thus, in some trials the participant needed to wait for the partner’s response.</p><p>Having thus determined the parameters of the simulated partners, we then generated the sequence of trial-by-trial responses of a given partner using the procedure introduced by <xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>. To produce the trial-by-trial responses of a given partner, we first generated a sequence of coherence levels with given directions (+ for rightward and – for leftward directions). Then we created a sequence of random values (sensory evidence), drawn from a Gaussian distribution with mean of coherence levels and variance of σ (sensory noise). Then, via applying the set of thresholds taken from the participant’s data in isolated condition, we mapped the sequence of random values into trial-by-trial responses to generate a partner with a given confidence mean. Finally, to simulate lapses of attention and response errors, we randomly selected a response (from a uniform distribution over 1–6) on 5% of the trials (see <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> for the accuracy and confidence of the generated partners).</p><p>For study 2, we used the same procedure as study 1 and simulated two partners. These partners’ accuracy was similar to the participant but each had a different confidence means (high confidence and low confidence partners). Therefore, we kept the <italic>σ</italic> constant and only change the confidence. For low confidence simulated partner, average confidence was set to the average of participant’s confidence in the low coherence (1,6%, 3.2%, and 6.4%) trials. For the high confidence simulated partners, mean confidence was set to the average confidence of the participant in the high coherence (12.8% and 25.6%) trials.</p></sec><sec id="s4-7"><title>Signal detection theory model for isolated sessions</title><p>In study 1 and 2, we simulated 4 and 2 artificial partners, respectively. We followed the procedure described by <xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref>. Briefly, working with the data from the isolated session, the sensory noise (<italic>σ</italic>) and response thresholds (<italic>θ</italic>) for each participant were calculated using a signal detection theory model. In this model, the level of sensory noise (<italic>σ</italic>) determines the participant’s sensitivity and a set of 11 thresholds determines the participant’s response distribution, which indicate both decision (via its sign) and confidence within the same distribution (see below).</p><p>On each trial, the sensory evidence, <italic>x</italic>, is sampled from a Gaussian distribution, <italic>x</italic> ∈<italic>N</italic>(<italic>s</italic>, <italic>σ</italic><sup>2</sup>). The mean, <italic>s</italic>, is the motion coherence level and is drawn uniformly from the set <italic>s</italic>∈<italic>S</italic> = {−0.512, −0.256, −0.128, −0.064, −0.032, 0.032, 0.064, 0.128, 0.256, 0.512} (for the second study <italic>S</italic> = {−0.256, −0.128, −0.064, −0.032, –0.016, 0.016, 0.032, 0.064, 0.128, 0.256}). The sign of <italic>s</italic> indicates the correct direction of motion (right = positive) and its absolute value indicates the motion coherency. The standard deviation, <italic>σ</italic>, describes the level of sensory noise and is the same for all stimuli. We assumed that the internal estimate of sensory evidence (<italic>z</italic>) is equal to the raw sensory evidence (<italic>x</italic>). If <italic>z</italic> is largely positive, it denotes high probability of choosing right direction and vice versa for largely negative values.</p><p>To determine the participant’s sensitivity and the response thresholds, first, we calculated the distribution of responses (<italic>r</italic>, ranging from –6 to 6, where the participant’s confidence was (<italic>c</italic> = |<italic>r</italic>|), and her decision was determined by the sign of <italic>r</italic>). <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> shows the response distribution.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="center center" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">z</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">z</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mn>6</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mtext> </mml:mtext><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mn>2</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mi mathvariant="normal">z</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="normal">z</mml:mi><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd><mml:mtd><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Using <italic>θ</italic> and <italic>σ</italic>, we mapped <italic>z</italic> to participants response (<italic>r</italic>). We found thresholds <italic>θ</italic><sub><italic>i</italic></sub> over <italic>S</italic> where <italic>i</italic> = −6,–5, −4,–3, −2, –1, 1, 2, 3, 4, 5 such that:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≤</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:munder><mml:mi>p</mml:mi><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>10</mml:mn></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mi>ϵ</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:munder><mml:mrow><mml:mrow><mml:mo>Ф</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where Φ is the Gaussian cumulative density function. For each stimulus, <italic>s</italic>∈<italic>S</italic>, the predicted response distribution, <inline-formula><mml:math id="inf9"><mml:mi>p</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:mfenced></mml:math></inline-formula>, calculated by <italic>S</italic>3:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>i</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing=".2em" columnspacing="1em" displaystyle="false"><mml:mtr><mml:mtd><mml:mtext> </mml:mtext><mml:mrow><mml:mo>Ф</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:mo>Ф</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mo>Ф</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mn>6</mml:mn><mml:mo>&lt;</mml:mo><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mrow><mml:mo>Ф</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mi>σ</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>From here, the model’s accuracy could be calculated by <italic>S</italic>4:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mo>.</mml:mo><mml:mi>s</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mi>S</mml:mi><mml:mo>.</mml:mo><mml:mi>s</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>.</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mn>10</mml:mn></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Given participant’s accuracy, we could find a set of <italic>θ</italic> and <italic>σ</italic>.</p></sec><sec id="s4-8"><title>Confidence estimation</title><p>Once we had determined <italic>θ</italic> and <italic>σ</italic>, we could produce a confidence landscape with a specific mean. In order to generate one high confidence and another low confidence partner, we needed to alter mean confidence by modifying the <italic>θ</italic>. There could be an infinite number of confidence distribution with the desired mean. We were interested in the maximum entropy distribution that satisfied two constraints: mean confidence should be specified, and the distribution must sum to 1. Using Lagrange multiplier (<italic>λ</italic>) the response distribution was calculated as:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>with <italic>λ</italic> chosen by solving the constraint<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:munderover><mml:mi>j</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:munderover><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mi>λ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We transformed confidence distributions (1–6) to response distributions (−6 to –1 and 1–6) by assuming symmetry around 0. <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref> shows the accuracy and confidence of generated agents.</p></sec><sec id="s4-9"><title>Computational model</title><p>We employed a previously described attractor network model (<xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref>) which is itself the reduced version of an earlier one (<xref ref-type="bibr" rid="bib80">Wang, 2002</xref>) inspired by the mean field theory. The model consists of two units simulating the average firing rates of two neural populations involved in information accumulation during perceptual decisions (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). When the network is given inputs proportional to stimulus coherence levels, a competition breaks out between two alternative units. This race would continue until firing rates of one of the two units reaches the high-firing-rate attractor state at which point the alternative favored by the unit is chosen. The details of this model have been comprehensively described elsewhere (<xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref>).</p><p>Each unit was selective to one choice (<xref ref-type="disp-formula" rid="equ7 equ8">Equations 7; 8</xref>) and received an input as follows:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <italic>J<sub>N</sub></italic><sub>11</sub> and <italic>J<sub>N</sub></italic><sub>22</sub> indicated the excitatory recurrent connection of each population and <italic>J<sub>N</sub></italic><sub>12</sub> and <italic>J<sub>N</sub></italic><sub>21</sub> showed the mutual inhibitory connection values. For the simulation in <xref ref-type="fig" rid="fig3">Figure 3b</xref> we set the recurrent connections to 0.3157 nA and inhibitory ones to 0.0646 nA. <italic>I</italic><sub>0</sub> indicated the effective external input which was set to 32.55 nA. <italic>I<sub>noise1</sub></italic>/<italic>I<sub>noise2</sub></italic> stood for the internal noise in each population unit. This zero mean Gaussian white noise was generated based on the time constant of 2 ms and standard deviation of 0.02 nA. <italic>I</italic><sub>1</sub>/<italic>I</italic><sub>2</sub> indicated the input currents proportional to the motion coherence level such that:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>.</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>c</mml:mi><mml:mn>100</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mo>.</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mfrac><mml:mi>c</mml:mi><mml:mn>100</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf10"><mml:msub><mml:mrow><mml:mi>J</mml:mi></mml:mrow><mml:mrow><mml:mi>A</mml:mi><mml:mo>.</mml:mo><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was the average synaptic coupling from the external source and set to 0.0002243 (nA Hz<sup>–1</sup>), <italic>c</italic> was coherence level and <inline-formula><mml:math id="inf11"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , a.k.a. perceptual gain, was the input value when the coherence was zero (set to 45.8 Hz).</p><p><italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub> were variables representing the synaptic current of either population and were proportional to the number of active NMDA receptors. Whenever the main text refers to accumulated evidence, we refer to <italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub> variables. Dynamics of these variables were as follows:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>γ</mml:mi><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf12"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> , the NMDA receptor delay time constant, was set to 100 ms, <inline-formula><mml:math id="inf13"><mml:mi>γ</mml:mi></mml:math></inline-formula> set to 0.641 and the time step, <inline-formula><mml:math id="inf14"><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:math></inline-formula>, was set to 0.5 ms. Dynamical <xref ref-type="disp-formula" rid="equ11 equ12">Equations 11; 12</xref> were solved using forward Euler method (<xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref>). (<italic>H</italic>), the generated firing rates of either populations, was calculated by:<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>d</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <italic>a</italic>, <italic>b</italic>, and <italic>d</italic> were set to 270 Hz nA<sup>–1</sup>, 108 Hz, and 0.154 s, respectively. These constants indicated the input-output relationship of a neural population.</p><p>The model’s choice in each trial was defined as the accumulated evidence of either population that first touched a threshold, and the decision time was defined as the time when the threshold was touched. Notably, the decision threshold was set to <italic>S<sub>threshold</sub></italic> = 0.32. Moreover, the confidence was defined as the area between two accumulators (<italic>S</italic><sub>1</sub> and <italic>S</italic><sub>2</sub> in <xref ref-type="disp-formula" rid="equ11 equ12">Equations 11; 12</xref>), in the time span of 0–500 ms, which was defined as:<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>500</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which was normalized by following logistic function (<xref ref-type="bibr" rid="bib81">Wei and Wang, 2015</xref>):<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mi>N</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>i</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mi>a</mml:mi><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where the values of <inline-formula><mml:math id="inf15"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> , <italic>a</italic>, <italic>k,</italic> and <inline-formula><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> were set to 1.32, –0.99, 5.9, and 0.16 respectively for model on entire trials of subjects in isolated sessions; <italic>confidence</italic> is calculated in <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> in time period of [0–500]ms.</p><p>In line with previous studies, we calculated the absolute difference between accumulators (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>; <xref ref-type="bibr" rid="bib81">Wei and Wang, 2015</xref>; <xref ref-type="bibr" rid="bib61">Rolls et al., 2010</xref>). In this formulation, confidence is calculated from model activity during the stimulus duration (<xref ref-type="bibr" rid="bib4">Atiya et al., 2019</xref>). Notably, in our confidence definition, we integrated the accumulators’ difference even when the winning accumulator hit the threshold (post-decision period) (<xref ref-type="bibr" rid="bib9">Balsdon et al., 2020</xref>; <xref ref-type="bibr" rid="bib52">Navajas et al., 2016</xref>; <xref ref-type="bibr" rid="bib85">Yu et al., 2015</xref>). This formulation of confidence provided a successful fit to subjects’ behaviors (<xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>). To demonstrate that our key findings do not depend on this specific formulation, we implemented another alternative method (<xref ref-type="bibr" rid="bib79">Vickers, 1979</xref>) and showed qualitatively similar results (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>) are obtained.</p><p>We calibrated the model to the data from the isolated condition to identify the best fitting parameters that would describe the participants’ behavior in isolation. In this procedure decision threshold, inhibitory and excitatory connections, NDT (set 0.27 s) and <inline-formula><mml:math id="inf17"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> were considered as the model variables (see <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1h</xref> for parameter values).</p><p>In order to explain the role of social context on participant’s behavior, we added a new input current to the model. Importantly we kept all other parameters of the model identical to the best fit to the participants’ behavior in the isolated situation:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula><disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>22</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>N</mml:mi><mml:mn>21</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mtext> </mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>In order to evaluate the effect of <italic>W<sub>x</sub></italic> on the RT, accuracy, and confidence, we simulated the model while systematically varying the values of <italic>W<sub>x</sub></italic> (<xref ref-type="fig" rid="fig3">Figure 3b</xref>).</p><p>Having established the qualitative relevance of <italic>W</italic><sub><italic>x</italic></sub> in providing a computational hypothesis for the impact of social context, then we defined <italic>W</italic><sub><italic>x</italic></sub> proportional to the confidence of partner as follows:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mtext> </mml:mtext><mml:mi>α</mml:mi><mml:mo>.</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf18"><mml:mi>t</mml:mi></mml:math></inline-formula> was the trial number. The model inputs were identical to isolated situation expect for the top-down current of <inline-formula><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> which indicates the social input where <inline-formula><mml:math id="inf20"><mml:mi>α</mml:mi></mml:math></inline-formula> was a normalization factor (or coupling coefficient) and <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>t</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo><mml:mi> </mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> indicates the partner’s confidence in the previous trial. Thus, we added a social input based on the linear combination of the partner’s confidence in the previous trial. Importantly the model performance is not sensitive to linearity assumptions (see <xref ref-type="fig" rid="fig3s8">Figure 3—figure supplement 8</xref>). Notably, the behavioral effect reported in the main script is also evident respect to the confidence of the agent in the previous trial (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref> and <xref ref-type="table" rid="table5">Table 5</xref>).</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Details of statistical results for the impact of previous trial (<xref ref-type="fig" rid="fig1s3">Figure 1—figure supplement 3</xref>).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Response</th><th align="left" valign="bottom">Regressors</th><th align="left" valign="bottom">Estimate</th><th align="left" valign="bottom">SE</th><th align="left" valign="bottom">CI</th><th align="left" valign="bottom">t-Stat</th><th align="left" valign="bottom">p-Value</th><th align="left" valign="bottom">Total number</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="6"><bold>Study 1</bold></td><td align="left" valign="bottom" rowspan="2"><bold>Accuracy</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="left" valign="bottom">0.007</td><td align="left" valign="bottom">0.0006</td><td align="char" char="." valign="bottom">[0.006 0.008]</td><td align="left" valign="bottom">11.58</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom"><bold>Conf (<italic>t</italic>–1)</bold></td><td align="left" valign="bottom">–0.0017</td><td align="left" valign="bottom">0.005</td><td align="char" char="." valign="bottom">[–0.01 0.01]</td><td align="left" valign="bottom">–0.28</td><td align="left" valign="bottom">0.77</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>Confidence</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="left" valign="bottom">0.047</td><td align="left" valign="bottom">0.001</td><td align="char" char="." valign="bottom">[0.045, 0.049]</td><td align="left" valign="bottom">54.7</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom"><bold>Conf (<italic>t</italic>–1)</bold></td><td align="left" valign="bottom">0.32</td><td align="left" valign="bottom">0.008</td><td align="char" char="." valign="bottom">[0.3 0.33]</td><td align="left" valign="bottom">38.31</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>RT</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="left" valign="bottom">–0.005</td><td align="left" valign="bottom">0.0001</td><td align="char" char="." valign="bottom">[–0.0048 0.0044]</td><td align="left" valign="bottom">–44.36</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom"><bold>Conf (<italic>t</italic>–1)</bold></td><td align="left" valign="bottom">–0.0055</td><td align="left" valign="bottom">0.001</td><td align="char" char="." valign="bottom">[–0.007 –0.003]</td><td align="left" valign="bottom">–5.44</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">9600</td></tr><tr><td align="left" valign="bottom" rowspan="6"><bold>Study 2</bold></td><td align="left" valign="bottom" rowspan="2"><bold>Accuracy</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">0.002</td><td align="char" char="." valign="bottom">[0.02 0.024]</td><td align="left" valign="bottom">13.23</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom"><bold>Conf (<italic>t</italic>–1)</bold></td><td align="left" valign="bottom">0.003</td><td align="left" valign="bottom">0.008</td><td align="char" char="." valign="bottom">[–0.012 0.018]</td><td align="left" valign="bottom">0.37</td><td align="left" valign="bottom">0.7</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>Confidence</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="left" valign="bottom">0.1</td><td align="left" valign="bottom">0.002</td><td align="char" char="." valign="bottom">[0.097 0.0106]</td><td align="left" valign="bottom">47.2</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom"><bold>Conf (<italic>t</italic>–1)</bold></td><td align="left" valign="bottom">0.09</td><td align="left" valign="bottom">0.01</td><td align="char" char="." valign="bottom">[0.07 0.11]</td><td align="left" valign="bottom">8.6</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom" rowspan="2"><bold>RT</bold><break/><bold>(HC vs LC)</bold></td><td align="left" valign="bottom"><bold>Coherency</bold></td><td align="left" valign="bottom">–0.009</td><td align="left" valign="bottom">0.0003</td><td align="char" char="." valign="bottom">[–0.001 –0.008]</td><td align="left" valign="bottom">–26.2</td><td align="left" valign="bottom">&lt;0.001</td><td align="char" char="." valign="bottom">6000</td></tr><tr><td align="left" valign="bottom"><bold>Condition</bold></td><td align="left" valign="bottom">0.005</td><td align="left" valign="bottom">0.001</td><td align="char" char="." valign="bottom">[0.001 0.008]</td><td align="left" valign="bottom">2.98</td><td align="left" valign="bottom">&lt;0.01</td><td align="char" char="." valign="bottom">6000</td></tr></tbody></table></table-wrap><p>For simulations reported in <xref ref-type="fig" rid="fig3">Figure 3d</xref>, we created high and low confident models by altering the inhibitory and excitatory connections of the original model. For the high confident model, excitatory and inhibitory connections were set to 0.3392 and 0.0699. For the low confident model excitatory and inhibitory connections were set to 0.3163 and 0.0652 respectively. For the simulation of social interaction (<xref ref-type="fig" rid="fig4">Figure 4f</xref>), we coupled two instances of the model using <xref ref-type="disp-formula" rid="equ20">Equation 20</xref> with <inline-formula><mml:math id="inf22"><mml:mi>α</mml:mi></mml:math></inline-formula> set to –0.0008 and 0.005 for high confident and low confident models, respectively. We ran the parallel simulations 50 times and reported the average results.</p><p>In order to remove the effect of coherence levels from models’ confidence, we measured the residuals of models’ confidence after regressing out the impact of coherence. Using this simple regression model:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mi mathvariant="normal">M</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where <italic>Coh</italic> is the motion coherence level and <inline-formula><mml:math id="inf23"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> is the error term, we removed the information explainable by motion coherence levels from confidence data as following. Confidence residuals were therefore:<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">R</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">d</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mrow></mml:math></disp-formula></p><p>All the simulations of model in the text – and parameters reported in the method – are related to the model calibrated on the collapsed data of all subjects (<italic>n</italic>=3000 for isolated sessions of study 2).</p></sec><sec id="s4-10"><title>Alternative formulations for confidence in the computational model</title><p>In our main model, confidence is formalized by <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>. We calculated the integral of difference between the losing and the winning accumulator during the stimulus presentation. This value would then be fed into a logistic function (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>) to produce the final confidence reported by the model (<xref ref-type="fig" rid="fig3">Figure 3b</xref> middle panel). To demonstrate the generality of our findings, we used another alternative (but similar) formulation in the previous literature for confidence representation. In <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3</xref>, we compare the resulting ‘raw’ confidence values (i.e. confidence values before they are fed to <xref ref-type="disp-formula" rid="equ15">Equation 15</xref>).</p><p>Alternative formulations for confidence are:</p><list list-type="order"><list-item><p>For comparison we plot our main formulation (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>) in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3a</xref>.</p></list-item><list-item><p>By calculating the difference between winning and losing accumulator at the END of stimulus duration (<xref ref-type="bibr" rid="bib52">Navajas et al., 2016</xref>; <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3b</xref>, we call this End method).</p></list-item></list><p>Our simulations showed that our formulation (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3a</xref>) shows an expected modulation to top-down currents. <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3b</xref> also shows a similar pattern which indicates our results are not different from End method. Therefore, our computational results could be generalized to different confidence representation methods.</p></sec><sec id="s4-11"><title>Model comparison</title><p>For model comparison, we used the fitted parameters from the isolated session (study 2 only without loss of generality). The model parameters for the isolated condition were extracted for each participants in their own respective isolated session (<italic>n</italic>=3000 across all participants). Then we compared all ‘alternative’ models with a ‘single free parameter’ to determine the model with the best account to behavioral data in social sessions (<italic>n</italic>=6000 across all participants). We considered three alternative models for the comparison. Note that in all models <italic>a</italic> is the normalization factor and the free parameter.</p></sec><sec id="s4-12"><title>Bound model</title><p>We hypothesized that partner’s confidence modulates the participant’s decision boundary according to:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mi>B</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p><italic>B</italic> determines the threshold applied on the solution of the <xref ref-type="disp-formula" rid="equ11 equ12">Equations 11; 12</xref> (see Materials and methods). <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the threshold in the isolated model. In this model, in social condition the bound depends on the value of the agent’s confidence in the previous trial. Note that the optimum value of <italic>a</italic>, normalization or coupling factor, is most likely to be negative since it generates lower RTs in social vs isolated situation.</p></sec><sec id="s4-13"><title>NDT model</title><p>We hypothesized that NDT would be modulated by confidence of agent in the previous trial. Here,<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p><inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mi>D</mml:mi><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> was the NDT fitted on the isolated data. Similarly, the optimum <italic>a</italic> was expected to be negative.</p></sec><sec id="s4-14"><title>Gain model</title><p>We hypothesized that social information modulated the perceptual gain defined as:<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:msub><mml:mn>0</mml:mn><mml:mrow><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <italic>μ</italic><sub>0</sub> denotes the input value of the model when motion coherence is zero (<xref ref-type="disp-formula" rid="equ9 equ10">Equations 9; 10</xref>, Materials and methods) and <inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> was calculated based on isolated data. If <italic>a</italic> is positive, then <italic>μ</italic><sub>0</sub> would be greater under social condition vs isolated condition, which in turn generates lower RTs and higher confidence.</p><p>In order to incorporate the accuracy, RT, and confidence in model comparison, we calculated the RT distribution of trials in each of the 12 confidence levels, 6 for left decision (−6 to –1) and 6 for right decision (1–6). The RT in each level was further divided into two categories (<xref ref-type="bibr" rid="bib58">Ratcliff and McKoon, 2008</xref>) (less than 700 ms and larger than 700 ms). We tried to maximize the likelihood of behavioral RT distribution in each response level (confidence and choice) given the model structure and parameters. The probability matrix was defined as follows:<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mi>P</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>700</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mi>T</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>700</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mo>−</mml:mo><mml:mn>6</mml:mn><mml:mo>≤</mml:mo><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></disp-formula></p><p>where <italic>i</italic> is confidence levels ranging from –6 to 6. Note, the probability was calculated based on all trials in our behavioral data set (6000 trials). The model’s probability matrix was also calculated in a similar manner. Hence, we derived a probability matrix of 12 response levels and 2 RT bins. The likelihood function was defined as follows:<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mi>J</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>P</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>P</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>h</mml:mi><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>P</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>C</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:munderover><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:munderover><mml:mi>J</mml:mi><mml:mi>o</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi><mml:mi>P</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Since we used similar parameters for the models (all models had one free parameter, <italic>a</italic>) we could directly compare cost values corresponding to each model. The model with the lowest cost is the preferred model; the parameters were found via MATLAB <italic>fmincon</italic> function. As is often the case, there was some variability across participants (see <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). To strengthen the conclusions about model comparison, we also provide evidence from a model falsification exercise that we performed. We simulated the models between two different social conditions (HCA and LCA) to see which model could, in theory, follow the behavioral pattern (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). Indeed, we attempted to numerically <italic>falsify</italic> the alternative models. <xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7</xref> shows the alternative model fails to reproduce the effect observed in <xref ref-type="fig" rid="fig1">Figure 1c</xref>.</p></sec><sec id="s4-15"><title>Eye monitoring and pupilometery</title><p>In both studies, the eye movements were recorded by an EyeLink 1000 (SR- Research) device with a sampling rate of 1000 Hz which was controlled by a dedicated host PC. The device was set in a desktop and pupil-corneal reflection mode while data from the left eye was recorded. At the beginning of each block, for most subjects, the system was recalibrated and then validated by 9-point schema presented on the screen. One subject was showed a 3-point schema due to the repetitive calibration difficulty. Having reached a detection error of less than 0.5°, the participants were led to the main task. Acquired eye data for pupil size were used for further analysis. Data of one subject in the first study was removed from further analysis due to storage failure.</p><p>Pupil data were divided into separate epochs and data from ITI were selected for analysis. ITI interval was defined as the time between offset of trial (<italic>t</italic>) feedback screen and stimulus presentation of trial (<italic>t</italic>+1). Then, blinks and jitters were detected and removed using linear interpolation. Values of pupil size before and after the blink were used for this interpolation. Data was also mid-pass filtered using Butterworth filter (second order, [0.01, 6] Hz) (<xref ref-type="bibr" rid="bib77">van Kempen et al., 2019</xref>). The pupil data was z-scored and then was baseline corrected by removing the average of signal in the period of [–1000 0] ms interval (before ITI onset). Importantly, trials with ITI &gt;3 s were excluded from analysis (365 out of 8800 for study 1 and 128 out 6000 for study 2; also see <xref ref-type="table" rid="table6">Table 6</xref> and Selection criteria for data analysis in Supplementary materials).</p><table-wrap id="table6" position="float"><label>Table 6.</label><caption><title>The rate of trial rejection of eye tracking (only data of social) and EEG data (visual inspection) per participant.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Participants</th><th align="left" valign="bottom">Eye tracking rejection % (social)</th><th align="left" valign="bottom">EEG trial rejection % (visual)</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="12"><bold>Study 1 (Discovery)</bold></td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">12.25</td><td align="char" char="." valign="bottom">4.6</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">12.87</td><td align="char" char="." valign="bottom">31.1</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">0.5</td><td align="char" char="." valign="bottom">22.1</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">14.8</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">1.37</td><td align="char" char="." valign="bottom">34.4</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">4.6</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="char" char="." valign="bottom">7.75</td><td align="char" char="." valign="bottom">8.8</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="char" char="." valign="bottom">0.37</td><td align="char" char="." valign="bottom">24.4</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="char" char="." valign="bottom">6.37</td><td align="char" char="." valign="bottom">7.6</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">46</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="char" char="." valign="bottom">0.12</td><td align="left" valign="bottom">NA</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="left" valign="bottom">NA</td><td align="left" valign="bottom">NA</td></tr><tr><td align="left" valign="bottom" rowspan="15"><bold>Study 2 (Replication)</bold></td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="char" char="." valign="bottom">2</td><td align="char" char="." valign="bottom">1.25</td><td align="char" char="." valign="bottom">1</td></tr><tr><td align="char" char="." valign="bottom">3</td><td align="char" char="." valign="bottom">5.75</td><td align="char" char="." valign="bottom">8.5</td></tr><tr><td align="char" char="." valign="bottom">4</td><td align="char" char="." valign="bottom">0.5</td><td align="char" char="." valign="bottom">3</td></tr><tr><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">16</td></tr><tr><td align="char" char="." valign="bottom">6</td><td align="char" char="." valign="bottom">1.5</td><td align="char" char="." valign="bottom">2.5</td></tr><tr><td align="char" char="." valign="bottom">7</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0.5</td></tr><tr><td align="char" char="." valign="bottom">8</td><td align="char" char="." valign="bottom">1.5</td><td align="char" char="." valign="bottom">9</td></tr><tr><td align="char" char="." valign="bottom">9</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">2</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">4</td></tr><tr><td align="char" char="." valign="bottom">11</td><td align="char" char="." valign="bottom">1</td><td align="char" char="." valign="bottom">7.5</td></tr><tr><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">0.5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">13</td><td align="char" char="." valign="bottom">0.75</td><td align="char" char="." valign="bottom">10.5</td></tr><tr><td align="char" char="." valign="bottom">14</td><td align="char" char="." valign="bottom">2.5</td><td align="char" char="." valign="bottom">12</td></tr><tr><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">14.75</td><td align="char" char="." valign="bottom">4.5</td></tr></tbody></table></table-wrap></sec><sec id="s4-16"><title>EEG signal recording and preprocessing</title><p>For the first study, a 32-channel eWave32 amplifier was used for recording which followed the 10–10 convention of electrode placement on the scalp (for the locations of the electrodes, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>; right mastoid as the reference). The amplifier, produced by ScienceBeam (<ext-link ext-link-type="uri" xlink:href="http://www.sciencebeam.com/">http://www.sciencebeam.com/</ext-link>), provided a 1 K sampling rate (<xref ref-type="bibr" rid="bib76">Vafaei Shooshtari et al., 2019</xref>). For the second study we used a 64-channel amplifier produced by LIV team (<ext-link ext-link-type="uri" xlink:href="http://lliivv.com/en/">http://lliivv.com/en/</ext-link>) with 250 Hz sampling rate (see the electrode placement in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>Raw data were analyzed using EEGLAB software (<xref ref-type="bibr" rid="bib17">Delorme and Makeig, 2004</xref>). First, data were notch filtered in the range of 45–55 Hz in order to remove the line noise. Using an FIR filter in the range of 0.1–100 Hz, high-frequency noise was also removed from data. Artifacts were removed by visual inspection using information from independent component analysis. Noisy trials were also removed by avisual inspection. Noisy channels were interpolated using EEGLAB software. The signals were divided into distinct epochs aligned to stimulus presentation ranging from 100 ms pre-stimulus onset until 500 ms post-stimulus offset. After preprocessing, EEG data in the designated epochs that had higher (lower) values than 200 (–200) μV were excluded from analysis (see <xref ref-type="table" rid="table6">Table 6</xref> and Materials and methods for detailed data analysis) (<xref ref-type="bibr" rid="bib36">Kelly and O’Connell, 2013</xref>). We used CP1, CP2, Cz, and Pz electrodes for further analysis. In the first study, EEG recording was not possible in two participants due to unresolvable impedance calibration problems in multiple channels.</p></sec><sec id="s4-17"><title>Relation of CPP to coherence and social condition</title><p>Activities of centroparietal area of the brain is shown to be modulated with coherence level. Here, we showed that CPP activities are statistically related to the coherence levels (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, top-row) in both studies. Furthermore, we tested how much this relationship is dependent to social condition (HCA, LCA, <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>, bottom-row). Our analysis showed that the slope (respect to coherence levels) is different in HCA vs LCA (also see <xref ref-type="table" rid="table6">Table 6</xref>). Notably, this effect is in line with our neural model prediction (see <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref>, next section).</p></sec><sec id="s4-18"><title>Selection criteria for data analysis</title><p>The data included in both studies could be classified into three main categories: behavioral, eye tracking, and EEG. For the behavioral analysis, data from all participants were included. In study 1, eye tracking data from one participant was lost due to storage failure. For pupil analysis, we excluded the trials with ITI longer than 3 s (~4% of trials in study 1 and ~2% for study 2).</p><p>We also analyzed brain data of participants in both studies. For the ERP analysis, we excluded trials with an absolute amplitude greater than 200 microvolts (overall less than 1% for both trials) as this data was deemed as outlier. Moreover, noisy trials and ICA components (around 5% of components in study 2) were rejected by visual inspection. Noisy electrodes were also interpolated (~8% of electrodes in study 2); see <xref ref-type="table" rid="table6">Table 6</xref> for more details. In study 1, EEG data from two participants were lost due to a technical failure. All data (behavioral, eye tracking, and EEG) for study 2 were properly stored, saved, and made available at <ext-link ext-link-type="uri" xlink:href="https://github.com/JimmyEsmaily/ConfMatch">https://github.com/JimmyEsmaily/ConfMatch</ext-link> (copy archived at <xref ref-type="bibr" rid="bib21">Esmaily, 2023</xref>; <xref ref-type="bibr" rid="bib47">MathWorks Inc, 2023</xref>).</p></sec><sec id="s4-19"><title>Statistical analysis</title><p>For hypothesis testing, we employed a number of GLMM. Unless otherwise stated, in our mixed models, participant was considered as random intercept. Details of each model is described in <xref ref-type="table" rid="table1 table2 table3 table4 table5 table6">Tables 1–6</xref> in the Supplementary materials. This approach enabled us to separate the effects of coherency and partner confidence. For RT and confidence, we assumed that the data is normality distributed. For the accuracy data we assumed the distribution is Poisson. We used a maximum likelihood method for fitting. All p-values reported in the text were drawn from the GLMM method, unless stated otherwise. For completeness, for each analysis we have added interaction terms as well (see <xref ref-type="table" rid="table7 table8">Tables 7 and 8</xref>).</p><table-wrap id="table7" position="float"><label>Table 7.</label><caption><title>Generalized linear mixed model (GLMM) including interaction terms (p-values are reported).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">Response</th><th align="left" valign="bottom">Coherence</th><th align="left" valign="bottom">Condition (LC vs HC)</th><th align="left" valign="bottom">Condition* coherence</th></tr></thead><tbody><tr><td align="left" valign="bottom" rowspan="5"><bold>Study 1</bold></td><td align="left" valign="bottom"><bold>Accuracy</bold></td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p=0.92</td><td align="left" valign="bottom">p=0.96</td></tr><tr><td align="left" valign="bottom"><bold>Confidence</bold></td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p&lt;0.001</td></tr><tr><td align="left" valign="bottom"><bold>RT</bold></td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p&lt;0.05</td></tr><tr><td align="left" valign="bottom"><bold>Pupil</bold></td><td align="left" valign="bottom">p=0.43</td><td align="left" valign="bottom">p=0.20</td><td align="left" valign="bottom">p=0.31</td></tr><tr><td align="left" valign="bottom"><bold>EEG slope</bold></td><td align="left" valign="bottom">p&lt;0.01</td><td align="left" valign="bottom">p=0.15</td><td align="left" valign="bottom">p=0.91</td></tr><tr><td align="left" valign="bottom" rowspan="5"><bold>Study 2</bold></td><td align="left" valign="bottom"><bold>Accuracy</bold></td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p=0.75</td><td align="left" valign="bottom">p=0.87</td></tr><tr><td align="left" valign="bottom"><bold>Confidence</bold></td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p&lt;0.001</td></tr><tr><td align="left" valign="bottom"><bold>RT</bold></td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p&lt;0.001</td><td align="left" valign="bottom">p=0.34</td></tr><tr><td align="left" valign="bottom"><bold>Pupil</bold></td><td align="left" valign="bottom">p=0.35</td><td align="left" valign="bottom">p=0.06</td><td align="left" valign="bottom">p=0.17</td></tr><tr><td align="left" valign="bottom"><bold>EEG slope</bold></td><td align="left" valign="bottom">p=0.62</td><td align="left" valign="bottom">p&lt;0.05</td><td align="left" valign="bottom">p=0.68</td></tr></tbody></table></table-wrap><table-wrap id="table8" position="float"><label>Table 8.</label><caption><title>Attractor model’s parameters.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Parameter</th><th align="left" valign="bottom">Parameter value</th><th align="left" valign="bottom">Reference, remarks</th></tr></thead><tbody><tr><td align="left" valign="bottom"><italic>JN,ii</italic></td><td align="left" valign="bottom">0.3157 nA</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom"><italic>JN,ij</italic></td><td align="left" valign="bottom">0.0646 nA</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom"><italic>µ</italic><sub>0</sub></td><td align="left" valign="bottom">45.8 Hz</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom">NDT</td><td align="left" valign="bottom">0.27 s</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom">Bound</td><td align="left" valign="bottom">0.32 nA</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom"><italic>a</italic> (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>)</td><td align="left" valign="bottom">–0.99</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom"><italic>b</italic><sub>0</sub> (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>)</td><td align="left" valign="bottom">1.32</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom"><italic>b</italic><sub>1</sub> (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>)</td><td align="left" valign="bottom">–0.165</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom"><italic>k</italic> (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>)</td><td align="left" valign="bottom">5.9</td><td align="left" valign="bottom">Calibrated based on pool of isolated data, also fitted on individual subjects’ data</td></tr><tr><td align="left" valign="bottom"><italic>I</italic><sub>0</sub></td><td align="left" valign="bottom">0.3255 nA</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom"><italic>J<sub>A.ext</sub></italic></td><td align="left" valign="bottom">0.00022 nA Hz<sup>–1</sup></td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom"><italic>τ<sub>s</sub></italic></td><td align="left" valign="bottom">0.1 s</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom"><italic>dt</italic></td><td align="left" valign="bottom">0.0005 s</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom"><italic>a</italic> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>)</td><td align="left" valign="bottom">270 (V nC)<sup>–1</sup></td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom"><italic>b</italic> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>)</td><td align="left" valign="bottom">108 Hz</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom"><italic>d</italic> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>)</td><td align="left" valign="bottom">0.154 s</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom"><italic>γ</italic></td><td align="left" valign="bottom">0.641</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom">Noise_std</td><td align="left" valign="bottom">0.025</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr><tr><td align="left" valign="bottom">I_noise</td><td align="left" valign="bottom">0.02</td><td align="left" valign="bottom">From <xref ref-type="bibr" rid="bib80">Wang, 2002</xref>; <xref ref-type="bibr" rid="bib83">Wong and Wang, 2006</xref></td></tr></tbody></table></table-wrap></sec><sec id="s4-20"><title>Permutation test to confirm confidence matching</title><p>A key null hypothesis (<inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>ϑ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϑ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the measure of interest: confidence matching) that we ruled out was that confidence matching was forced by the experimental design limitations and, therefore, would be observed in any random pairing of participants within our joint decision making setup. To reject this hypothesis, we performed a permutation test following <xref ref-type="bibr" rid="bib10">Bang et al., 2017</xref> (see their Supplementary Figure 3 for further details). For each participant and corresponding CGP pair, we defined |<italic>c</italic><sub>1</sub>–<italic>c</italic><sub>2</sub>| where <italic>c<sub>i</sub></italic> is the average confidence of participant <italic>i</italic> in a given pair. We then estimated the null distribution for this variable by randomly re-pairing the participant with other participants and computing the mean confidence matching for each such re-paired set (total number of sets 1000). In <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> (bottom row), the red line shows the empirically observed mean of confidence matching in our data. The null distribution is shown in black. Proportion of values from the null distribution that were less than the empirical mean was <italic>P</italic>~0.</p><p>In addition, we defined an index for measuring the confidence matching (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>, first row): <inline-formula><mml:math id="inf29"><mml:mi>Δ</mml:mi><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi mathvariant="normal"> </mml:mi><mml:mfenced open="|" close="|" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>S</mml:mi><mml:mi>u</mml:mi><mml:mi>b</mml:mi><mml:mi>j</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> . The larger the <inline-formula><mml:math id="inf30"><mml:mi>Δ</mml:mi><mml:mi>m</mml:mi></mml:math></inline-formula> the higher is the confidence matching. Although we did not observe a significant effect of <inline-formula><mml:math id="inf31"><mml:mi>Δ</mml:mi><mml:mi>m</mml:mi></mml:math></inline-formula>, we showed that this index is significantly different from zero in the HCA condition.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Software, Supervision, Funding acquisition, Validation, Investigation, Methodology, Project administration, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Investigation, Visualization, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Both experiments were approved by the local ethics committee at Faculty of computer engineering at Shahid Rajeie and also Iran University in Tehran, Iran (ethics application approval date and/or number: 5769). Written informed consent was obtained from all participants. The consent form was in the local Farsi language and did not include a &quot;consent to publish&quot; because data were anonymised, individual identity information was completely removed from the them and none of the experimental hypotheses involved the exact identification of the individual data from any participant. Participants received a fixed monetary compensation for their contribution.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-83722-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>This file contains supplementary tables that contains details of statistical analysis.</title></caption><media xlink:href="elife-83722-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Data that supports the findings of the study can be found here: <ext-link ext-link-type="uri" xlink:href="https://osf.io/v7fqz/">https://osf.io/v7fqz/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Bahrami</surname><given-names>B</given-names></name><name><surname>Esmaily</surname><given-names>J</given-names></name></person-group><source>Open Science Framework</source><year iso-8601-date="2020">2020</year><data-title>Neurobiology of Confidence Matching</data-title><pub-id pub-id-type="accession" xlink:href="https://osf.io/v7fqz/">v7fqz</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>JE and BB were supported by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation programme (819040 – acronym: rid-O). BB was supported by the NOMIS foundation and Templeton Religion Trust.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adler</surname><given-names>WT</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Limitations of proposed signatures of Bayesian confidence</article-title><source>Neural Computation</source><volume>30</volume><fpage>3327</fpage><lpage>3354</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01141</pub-id><pub-id pub-id-type="pmid">30314423</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ais</surname><given-names>J</given-names></name><name><surname>Zylberberg</surname><given-names>A</given-names></name><name><surname>Barttfeld</surname><given-names>P</given-names></name><name><surname>Sigman</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Individual consistency in the accuracy and distribution of confidence judgments</article-title><source>Cognition</source><volume>146</volume><fpage>377</fpage><lpage>386</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2015.10.006</pub-id><pub-id pub-id-type="pmid">26513356</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Bahrami</surname><given-names>B</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Doubly Bayesian analysis of confidence in perceptual decision-making</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004519</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004519</pub-id><pub-id pub-id-type="pmid">26517475</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atiya</surname><given-names>NAA</given-names></name><name><surname>Rañó</surname><given-names>I</given-names></name><name><surname>Prasad</surname><given-names>G</given-names></name><name><surname>Wong-Lin</surname><given-names>KF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A neural circuit model of decision uncertainty and change-of-mind</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>2287</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-10316-8</pub-id><pub-id pub-id-type="pmid">31123260</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Austen-Smith</surname><given-names>D</given-names></name><name><surname>Banks</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Information aggregation, rationality, and the condorcet Jury Theorem</article-title><source>American Political Science Review</source><volume>90</volume><fpage>34</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.2307/2082796</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahrami</surname><given-names>B</given-names></name><name><surname>Olsen</surname><given-names>K</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Optimally Interacting Minds</article-title><source>Science</source><volume>329</volume><fpage>1081</fpage><lpage>1085</lpage><pub-id pub-id-type="doi">10.1126/science.1185718</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahrami</surname><given-names>B</given-names></name><name><surname>Olsen</surname><given-names>K</given-names></name><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>What failure in collective decision-making tells us about metacognition</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>367</volume><fpage>1350</fpage><lpage>1365</lpage><pub-id pub-id-type="doi">10.1098/rstb.2011.0420</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baird</surname><given-names>B</given-names></name><name><surname>Smallwood</surname><given-names>J</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name><name><surname>Margulies</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Medial and lateral networks in anterior prefrontal cortex support metacognitive ability for memory and perception</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>16657</fpage><lpage>16665</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0786-13.2013</pub-id><pub-id pub-id-type="pmid">24133268</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balsdon</surname><given-names>T</given-names></name><name><surname>Wyart</surname><given-names>V</given-names></name><name><surname>Mamassian</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Confidence controls perceptual evidence accumulation</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>1753</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-15561-w</pub-id><pub-id pub-id-type="pmid">32273500</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Aitchison</surname><given-names>L</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Herce Castanon</surname><given-names>S</given-names></name><name><surname>Rafiee</surname><given-names>B</given-names></name><name><surname>Mahmoodi</surname><given-names>A</given-names></name><name><surname>Lau</surname><given-names>JYF</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Bahrami</surname><given-names>B</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Confidence matching in group decision-making</article-title><source>Nature Human Behaviour</source><volume>1</volume><elocation-id>0117</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-017-0117</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Distinct encoding of decision confidence in human medial prefrontal cortex</article-title><source>PNAS</source><volume>115</volume><fpage>6082</fpage><lpage>6087</lpage><pub-id pub-id-type="doi">10.1073/pnas.1800795115</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Ershadmanesh</surname><given-names>S</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Private-public mappings in human prefrontal cortex</article-title><source>eLife</source><volume>9</volume><elocation-id>e56477</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56477</pub-id><pub-id pub-id-type="pmid">32701449</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Neurocomputational mechanisms of confidence in self and others</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>4238</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-31674-w</pub-id><pub-id pub-id-type="pmid">35869044</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>E</given-names></name><name><surname>Moehlis</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: A formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The Psychophysics Toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Breveglieri</surname><given-names>R</given-names></name><name><surname>Galletti</surname><given-names>C</given-names></name><name><surname>Dal Bò</surname><given-names>G</given-names></name><name><surname>Hadjidimitrakis</surname><given-names>K</given-names></name><name><surname>Fattori</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Multiple aspects of neural activity during reaching preparation in the medial posterior parietal area V6A</article-title><source>Journal of Cognitive Neuroscience</source><volume>26</volume><fpage>878</fpage><lpage>895</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_00510</pub-id><pub-id pub-id-type="pmid">24168224</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Delorme</surname><given-names>A</given-names></name><name><surname>Makeig</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>EEGLAB: an open source toolbox for analysis of single-trial EEG dynamics including independent component analysis</article-title><source>Journal of Neuroscience Methods</source><volume>134</volume><fpage>9</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2003.10.009</pub-id><pub-id pub-id-type="pmid">15102499</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Martino</surname><given-names>B</given-names></name><name><surname>Bobadilla-Suarez</surname><given-names>S</given-names></name><name><surname>Nouguchi</surname><given-names>T</given-names></name><name><surname>Sharot</surname><given-names>T</given-names></name><name><surname>Love</surname><given-names>BC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Social information is integrated into value and confidence judgments according to its reliability</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6066</fpage><lpage>6074</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3880-16.2017</pub-id><pub-id pub-id-type="pmid">28566360</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dikker</surname><given-names>S</given-names></name><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Zevin</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>On the same wavelength: predictable language enhances speaker-listener brain-to-brain synchrony in posterior superior temporal gyrus</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>6267</fpage><lpage>6272</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3796-13.2014</pub-id><pub-id pub-id-type="pmid">24790197</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname><given-names>E</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The effects of neural gain on attention and learning</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1146</fpage><lpage>1153</lpage><pub-id pub-id-type="doi">10.1038/nn.3428</pub-id><pub-id pub-id-type="pmid">23770566</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Esmaily</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Confmatch</data-title><version designator="https://archive.softwareheritage.org/swh:1:dir:a3de79d590798c99a8dadd948ab6f2eb268304c3;origin=https://github.com/JimmyEsmaily/ConfMatch;visit=swh:1:snp:1dfaeff8fcb5056fbcd830194ce0dc527324a064;anchor=swh:1:rev:2adfb563eb24c137213d7163754d9e146fa42c50">https://archive.softwareheritage.org/swh:1:dir:a3de79d590798c99a8dadd948ab6f2eb268304c3;origin=https://github.com/JimmyEsmaily/ConfMatch;visit=swh:1:snp:1dfaeff8fcb5056fbcd830194ce0dc527324a064;anchor=swh:1:rev:2adfb563eb24c137213d7163754d9e146fa42c50</version><source>Software Heritage</source></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>SM</given-names></name><name><surname>Weil</surname><given-names>RS</given-names></name><name><surname>Nagy</surname><given-names>Z</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Relating introspective accuracy to individual differences in Brain Structure</article-title><source>Science</source><volume>329</volume><fpage>1541</fpage><lpage>1543</lpage><pub-id pub-id-type="doi">10.1126/science.1191883</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fleming</surname><given-names>SM</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Self-evaluation of decision-making: A general Bayesian framework for metacognitive computation</article-title><source>Psychological Review</source><volume>124</volume><fpage>91</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1037/rev0000045</pub-id><pub-id pub-id-type="pmid">28004960</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>K</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A Duet for one</article-title><source>Consciousness and Cognition</source><volume>36</volume><fpage>390</fpage><lpage>405</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2014.12.003</pub-id><pub-id pub-id-type="pmid">25563935</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Frith</surname><given-names>U</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Interacting Minds--A Biological Basis</article-title><source>Science</source><volume>286</volume><fpage>1692</fpage><lpage>1695</lpage><pub-id pub-id-type="doi">10.1126/science.286.5445.1692</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallotti</surname><given-names>M</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Social cognition in the we-mode</article-title><source>Trends in Cognitive Sciences</source><volume>17</volume><fpage>160</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2013.02.002</pub-id><pub-id pub-id-type="pmid">23499335</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Perceptual decision making in Rodents, Monkeys, and Humans</article-title><source>Neuron</source><volume>93</volume><fpage>15</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.003</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Nir</surname><given-names>Y</given-names></name><name><surname>Levy</surname><given-names>I</given-names></name><name><surname>Fuhrmann</surname><given-names>G</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Intersubject synchronization of cortical activity during natural vision</article-title><source>Science</source><volume>303</volume><fpage>1634</fpage><lpage>1640</lpage><pub-id pub-id-type="doi">10.1126/science.1089506</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Future trends in Neuroimaging: Neural processes as expressed within real-life contexts</article-title><source>NeuroImage</source><volume>62</volume><fpage>1272</fpage><lpage>1278</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2012.02.004</pub-id><pub-id pub-id-type="pmid">22348879</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mirroring and beyond: coupled dynamics as a generalized framework for modelling social interactions</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>371</volume><elocation-id>20150366</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2015.0366</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heath</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Random-walk and accumulator models of psychophysical discrimination: A critical evaluation</article-title><source>Perception</source><volume>13</volume><fpage>57</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1068/p130057</pub-id><pub-id pub-id-type="pmid">6473053</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Thesen</surname><given-names>T</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Carlson</surname><given-names>CE</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Doyle</surname><given-names>WK</given-names></name><name><surname>Rubin</surname><given-names>N</given-names></name><name><surname>Heeger</surname><given-names>DJ</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Slow cortical dynamics and the accumulation of information over long timescales</article-title><source>Neuron</source><volume>76</volume><fpage>423</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.08.011</pub-id><pub-id pub-id-type="pmid">23083743</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name><name><surname>Soltani</surname><given-names>A</given-names></name><name><surname>Woolrich</surname><given-names>MW</given-names></name><name><surname>Rushworth</surname><given-names>MFS</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mechanisms underlying cortical activity during value-guided choice</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>470</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1038/nn.3017</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iacoboni</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Imitation, empathy, and mirror neurons</article-title><source>Annual Review of Psychology</source><volume>60</volume><fpage>653</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1146/annurev.psych.60.110707.163604</pub-id><pub-id pub-id-type="pmid">18793090</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Internal and external influences on the rate of sensory evidence accumulation in the human brain</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>19434</fpage><lpage>19441</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3355-13.2013</pub-id><pub-id pub-id-type="pmid">24336710</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of confidence associated with a decision by Neurons in the Parietal Cortex</article-title><source>Science</source><volume>324</volume><fpage>759</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1126/science.1169405</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Corthell</surname><given-names>L</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Choice certainty is informed by both evidence and decision time</article-title><source>Neuron</source><volume>84</volume><fpage>1329</fpage><lpage>1342</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.12.015</pub-id><pub-id pub-id-type="pmid">25521381</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleiner</surname><given-names>M</given-names></name><name><surname>Brainard</surname><given-names>D</given-names></name><name><surname>Pelli</surname><given-names>D</given-names></name><name><surname>Ingling</surname><given-names>A</given-names></name><name><surname>Murray</surname><given-names>R</given-names></name><name><surname>Broussard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>What’s new in psychtoolbox-3</article-title><source>Perception</source><volume>36</volume><fpage>1</fpage><lpage>16</lpage></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kloosterman</surname><given-names>NA</given-names></name><name><surname>Meindertsma</surname><given-names>T</given-names></name><name><surname>van Loon</surname><given-names>AM</given-names></name><name><surname>Lamme</surname><given-names>VAF</given-names></name><name><surname>Bonneh</surname><given-names>YS</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Pupil size tracks perceptual content and surprise</article-title><source>The European Journal of Neuroscience</source><volume>41</volume><fpage>1068</fpage><lpage>1078</lpage><pub-id pub-id-type="doi">10.1111/ejn.12859</pub-id><pub-id pub-id-type="pmid">25754528</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Konvalinka</surname><given-names>I</given-names></name><name><surname>Vuust</surname><given-names>P</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Follow you, Follow me: continuous mutual prediction and adaptation in Joint Tapping</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>63</volume><fpage>2220</fpage><lpage>2230</lpage><pub-id pub-id-type="doi">10.1080/17470218.2010.497843</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krajbich</surname><given-names>I</given-names></name><name><surname>Rangel</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multialternative drift-diffusion model predicts the relationship between visual fixations and choice in value-based decisions</article-title><source>PNAS</source><volume>108</volume><fpage>13852</fpage><lpage>13857</lpage><pub-id pub-id-type="doi">10.1073/pnas.1101328108</pub-id><pub-id pub-id-type="pmid">21808009</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>DG</given-names></name><name><surname>Daunizeau</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Trading mental effort for confidence in the metacognitive control of value-based decision-making</article-title><source>eLife</source><volume>10</volume><elocation-id>e63282</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63282</pub-id><pub-id pub-id-type="pmid">33900198</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>V</given-names></name><name><surname>Michael</surname><given-names>E</given-names></name><name><surname>Balaguer</surname><given-names>J</given-names></name><name><surname>Herce Castañón</surname><given-names>S</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gain control explains the effect of distraction in human perceptual, cognitive, and economic decision making</article-title><source>PNAS</source><volume>115</volume><fpage>E8825</fpage><lpage>E8834</lpage><pub-id pub-id-type="doi">10.1073/pnas.1805224115</pub-id><pub-id pub-id-type="pmid">30166448</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Loughnane</surname><given-names>GM</given-names></name><name><surname>Newman</surname><given-names>DP</given-names></name><name><surname>Tamang</surname><given-names>S</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Antagonistic interactions between Microsaccades and evidence accumulation processes during decision formation</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>2163</fpage><lpage>2176</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2340-17.2018</pub-id><pub-id pub-id-type="pmid">29371320</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahmoodi</surname><given-names>A</given-names></name><name><surname>Bang</surname><given-names>D</given-names></name><name><surname>Olsen</surname><given-names>K</given-names></name><name><surname>Zhao</surname><given-names>YA</given-names></name><name><surname>Shi</surname><given-names>Z</given-names></name><name><surname>Broberg</surname><given-names>K</given-names></name><name><surname>Safavi</surname><given-names>S</given-names></name><name><surname>Han</surname><given-names>S</given-names></name><name><surname>Nili Ahmadabadi</surname><given-names>M</given-names></name><name><surname>Frith</surname><given-names>CD</given-names></name><name><surname>Roepstorff</surname><given-names>A</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Bahrami</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Equality bias impairs collective decision-making across cultures</article-title><source>PNAS</source><volume>112</volume><fpage>3835</fpage><lpage>3840</lpage><pub-id pub-id-type="doi">10.1073/pnas.1421692112</pub-id><pub-id pub-id-type="pmid">25775532</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="software"><person-group person-group-type="author"><collab>MathWorks Inc</collab></person-group><year iso-8601-date="2023">2023</year><data-title>Data analysis was performed using MATLAB</data-title><version designator="R2016a">R2016a</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/products/matlab/data">https://www.mathworks.com/products/matlab/data</ext-link></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Teodorescu</surname><given-names>AR</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Post choice information integration as a causal determinant of confidence: Novel data and a computational account</article-title><source>Cognitive Psychology</source><volume>78</volume><fpage>99</fpage><lpage>147</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2015.01.002</pub-id><pub-id pub-id-type="pmid">25868113</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mukamel</surname><given-names>R</given-names></name><name><surname>Gelbard</surname><given-names>H</given-names></name><name><surname>Arieli</surname><given-names>A</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name><name><surname>Fried</surname><given-names>I</given-names></name><name><surname>Malach</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Coupling between Neuronal Firing, Field Potentials, and fMRI in Human Auditory Cortex</article-title><source>Science</source><volume>309</volume><fpage>951</fpage><lpage>954</lpage><pub-id pub-id-type="doi">10.1126/science.1110913</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>PR</given-names></name><name><surname>Vandekerckhove</surname><given-names>J</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil-linked arousal determines variability in perceptual decision making</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003854</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003854</pub-id><pub-id pub-id-type="pmid">25232732</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname><given-names>MR</given-names></name><name><surname>Rumsey</surname><given-names>KM</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Parikh</surname><given-names>K</given-names></name><name><surname>Heasly</surname><given-names>B</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Rational regulation of learning dynamics by pupil-linked arousal systems</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1040</fpage><lpage>1046</lpage><pub-id pub-id-type="doi">10.1038/nn.3130</pub-id><pub-id pub-id-type="pmid">22660479</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navajas</surname><given-names>J</given-names></name><name><surname>Bahrami</surname><given-names>B</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Post-decisional accounts of biases in confidence</article-title><source>Current Opinion in Behavioral Sciences</source><volume>11</volume><fpage>55</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2016.05.005</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Navajas</surname><given-names>J</given-names></name><name><surname>Hindocha</surname><given-names>C</given-names></name><name><surname>Foda</surname><given-names>H</given-names></name><name><surname>Keramati</surname><given-names>M</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name><name><surname>Bahrami</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The idiosyncratic nature of confidence</article-title><source>Nature Human Behaviour</source><volume>1</volume><fpage>810</fpage><lpage>818</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0215-1</pub-id><pub-id pub-id-type="pmid">29152591</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Connell</surname><given-names>RG</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Wong-Lin</surname><given-names>KF</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bridging Neural and computational viewpoints on perceptual Decision-Making</article-title><source>Trends in Neurosciences</source><volume>41</volume><fpage>838</fpage><lpage>852</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.06.005</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patel</surname><given-names>D</given-names></name><name><surname>Fleming</surname><given-names>SM</given-names></name><name><surname>Kilner</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Inferring subjective states through the observation of actions</article-title><source>Proceedings of the Royal Society B</source><volume>279</volume><fpage>4853</fpage><lpage>4860</lpage><pub-id pub-id-type="doi">10.1098/rspb.2012.1847</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pelli</surname><given-names>DG</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The VideoToolbox software for visual psychophysics: transforming numbers into movies</article-title><source>Spatial Vision</source><volume>10</volume><fpage>437</fpage><lpage>442</lpage><pub-id pub-id-type="pmid">9176953</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Confidence and certainty: distinct probabilistic quantities for different goals</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>366</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1038/nn.4240</pub-id><pub-id pub-id-type="pmid">26906503</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>McKoon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Drift Diffusion Decision Model: Theory and data</article-title><source>Neural Computation</source><volume>20</volume><fpage>873</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1016/j.biotechadv.2011.08.021.Secreted</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rendell</surname><given-names>L</given-names></name><name><surname>Fogarty</surname><given-names>L</given-names></name><name><surname>Hoppitt</surname><given-names>WJE</given-names></name><name><surname>Morgan</surname><given-names>TJH</given-names></name><name><surname>Webster</surname><given-names>MM</given-names></name><name><surname>Laland</surname><given-names>KN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cognitive culture: theoretical and empirical insights into social learning strategies</article-title><source>Trends in Cognitive Sciences</source><volume>15</volume><fpage>68</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.12.002</pub-id><pub-id pub-id-type="pmid">21215677</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Resulaj</surname><given-names>A</given-names></name><name><surname>Kiani</surname><given-names>R</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Changes of mind in decision-making</article-title><source>Nature</source><volume>461</volume><fpage>263</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nature08275</pub-id><pub-id pub-id-type="pmid">19693010</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>Grabenhorst</surname><given-names>F</given-names></name><name><surname>Deco</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decision-Making, Errors, and Confidence in the Brain</article-title><source>Journal of Neurophysiology</source><volume>104</volume><fpage>2359</fpage><lpage>2374</lpage><pub-id pub-id-type="doi">10.1152/jn.00571.2010</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruff</surname><given-names>CC</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neurobiology of rewards and values in social decision making</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>549</fpage><lpage>562</lpage><pub-id pub-id-type="doi">10.1038/nrn3776</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sanders</surname><given-names>JI</given-names></name><name><surname>Hangya</surname><given-names>B</given-names></name><name><surname>Kepecs</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Signatures of a Statistical Computation in the Human Sense of Confidence</article-title><source>Neuron</source><volume>90</volume><fpage>499</fpage><lpage>506</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.03.025</pub-id><pub-id pub-id-type="pmid">27151640</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schall</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Accumulators, Neurons, and Response Time</article-title><source>Trends in Neurosciences</source><volume>42</volume><fpage>848</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2019.10.001</pub-id><pub-id pub-id-type="pmid">31704180</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Neural basis of a perceptual decision in the parietal cortex (area LIP) of the rhesus monkey</article-title><source>Journal of Neurophysiology</source><volume>86</volume><fpage>1916</fpage><lpage>1936</lpage><pub-id pub-id-type="doi">10.1152/jn.2001.86.4.1916</pub-id><pub-id pub-id-type="pmid">11600651</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silbert</surname><given-names>LJ</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Simony</surname><given-names>E</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Coupled neural systems underlie the production and comprehension of naturalistic narrative speech</article-title><source>PNAS</source><volume>111</volume><fpage>E4687</fpage><lpage>E4696</lpage><pub-id pub-id-type="doi">10.1073/pnas.1323812111</pub-id><pub-id pub-id-type="pmid">25267658</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sinanaj</surname><given-names>I</given-names></name><name><surname>Cojan</surname><given-names>Y</given-names></name><name><surname>Vuilleumier</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inter-individual variability in metacognitive ability for visuomotor performance and underlying brain structures</article-title><source>Consciousness and Cognition</source><volume>36</volume><fpage>327</fpage><lpage>337</lpage><pub-id pub-id-type="doi">10.1016/j.concog.2015.07.012</pub-id><pub-id pub-id-type="pmid">26241023</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sorkin</surname><given-names>RD</given-names></name><name><surname>Hays</surname><given-names>CJ</given-names></name><name><surname>West</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Signal-detection analysis of group decision making</article-title><source>Psychological Review</source><volume>108</volume><fpage>183</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.108.1.183</pub-id><pub-id pub-id-type="pmid">11212627</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stallen</surname><given-names>M</given-names></name><name><surname>Sanfey</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The neuroscience of social conformity: implications for fundamental and applied research</article-title><source>Frontiers in Neuroscience</source><volume>9</volume><elocation-id>337</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2015.00337</pub-id><pub-id pub-id-type="pmid">26441509</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stine</surname><given-names>GM</given-names></name><name><surname>Zylberberg</surname><given-names>A</given-names></name><name><surname>Ditterich</surname><given-names>J</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Differentiating between integration and non-integration strategies in perceptual decision making</article-title><source>eLife</source><volume>9</volume><elocation-id>e55365</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.55365</pub-id><pub-id pub-id-type="pmid">32338595</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>Verhagen</surname><given-names>L</given-names></name><name><surname>Schoffelen</surname><given-names>J-M</given-names></name><name><surname>Oostenveld</surname><given-names>R</given-names></name><name><surname>Blokpoel</surname><given-names>M</given-names></name><name><surname>Hagoort</surname><given-names>P</given-names></name><name><surname>van Rooij</surname><given-names>I</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural mechanisms of communicative innovation</article-title><source>PNAS</source><volume>110</volume><fpage>14574</fpage><lpage>14579</lpage><pub-id pub-id-type="doi">10.1073/pnas.1303170110</pub-id><pub-id pub-id-type="pmid">23959895</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>D’Imperio</surname><given-names>D</given-names></name><name><surname>di Pellegrino</surname><given-names>G</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Altered communicative decisions following ventromedial prefrontal lesions</article-title><source>Current Biology</source><volume>25</volume><fpage>1469</fpage><lpage>1474</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.03.057</pub-id><pub-id pub-id-type="pmid">25913408</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stolk</surname><given-names>A</given-names></name><name><surname>Verhagen</surname><given-names>L</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Conceptual Alignment: How Brains Achieve Mutual Understanding</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>180</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.11.007</pub-id><pub-id pub-id-type="pmid">26792458</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Twomey</surname><given-names>DM</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Abstract and Effector-Selective Decision Signals Exhibit Qualitatively Distinct Dynamics before Delayed Perceptual Reports</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>7346</fpage><lpage>7352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4162-15.2016</pub-id><pub-id pub-id-type="pmid">27413146</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Braun</surname><given-names>A</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pupil-linked arousal is driven by decision uncertainty and alters serial choice bias</article-title><source>Nature Communications</source><volume>8</volume><elocation-id>14637</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms14637</pub-id><pub-id pub-id-type="pmid">28256514</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vafaei Shooshtari</surname><given-names>S</given-names></name><name><surname>Esmaily Sadrabadi</surname><given-names>J</given-names></name><name><surname>Azizi</surname><given-names>Z</given-names></name><name><surname>Ebrahimpour</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Confidence Representation of Perceptual Decision by EEG and Eye Data in a Random Dot Motion Task</article-title><source>Neuroscience</source><volume>406</volume><fpage>510</fpage><lpage>527</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2019.03.031</pub-id><pub-id pub-id-type="pmid">30904664</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Kempen</surname><given-names>J</given-names></name><name><surname>Loughnane</surname><given-names>GM</given-names></name><name><surname>Newman</surname><given-names>DP</given-names></name><name><surname>Kelly</surname><given-names>SP</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>O’Connell</surname><given-names>RG</given-names></name><name><surname>Bellgrove</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Behavioural and neural signatures of perceptual decision-making are modulated by pupil-linked arousal</article-title><source>eLife</source><volume>8</volume><elocation-id>e42541</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.42541</pub-id><pub-id pub-id-type="pmid">30882347</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vickers</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1970">1970</year><article-title>Evidence for an accumulator model of psychophysical discrimination</article-title><source>Ergonomics</source><volume>13</volume><fpage>37</fpage><lpage>58</lpage><pub-id pub-id-type="doi">10.1080/00140137008931117</pub-id><pub-id pub-id-type="pmid">5416868</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vickers</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1979">1979</year><source>Decision Processes in Visual Perception</source><publisher-name>ACADEMIC PRESS INC</publisher-name></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wei</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Confidence estimation as a stochastic process in a neurodynamical system of decision making</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>99</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1152/jn.00793.2014</pub-id><pub-id pub-id-type="pmid">25948870</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wheatley</surname><given-names>T</given-names></name><name><surname>Boncz</surname><given-names>A</given-names></name><name><surname>Toni</surname><given-names>I</given-names></name><name><surname>Stolk</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Beyond the Isolated Brain: The Promise and Challenge of Interacting Minds</article-title><source>Neuron</source><volume>103</volume><fpage>186</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.009</pub-id><pub-id pub-id-type="pmid">31319048</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname><given-names>K-F</given-names></name><name><surname>Wang</surname><given-names>X-J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>1314</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3733-05.2006</pub-id><pub-id pub-id-type="pmid">16436619</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeung</surname><given-names>N</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Metacognition in human decision-making: confidence and error monitoring</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>367</volume><fpage>1310</fpage><lpage>1321</lpage><pub-id pub-id-type="doi">10.1098/rstb.2011.0416</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>S</given-names></name><name><surname>Pleskac</surname><given-names>TJ</given-names></name><name><surname>Zeigenfuse</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamics of postdecisional processing of confidence</article-title><source>Journal of Experimental Psychology. General</source><volume>144</volume><fpage>489</fpage><lpage>510</lpage><pub-id pub-id-type="doi">10.1037/xge0000062</pub-id><pub-id pub-id-type="pmid">25844627</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83722.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.11.08.515654" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.11.08.515654"/></front-stub><body><p>This important study examines how humans use information about the confidence of collaborators to guide their own perceptual decision making and confidence judgements. The study addresses this question with a combination of psychophysics, electrophysiological modeling, and computational modelling that provides a compelling validation of a computational framework that can be used to derive and test theory-based predictions about how collaborators use communication to align their confidence and thereby optimize their collective performance.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83722.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>O'Connell</surname><given-names>Redmond G</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02tyrky19</institution-id><institution>Trinity College Dublin</institution></institution-wrap><country>Ireland</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.11.08.515654">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.11.08.515654v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Interpersonal alignment of neural evidence accumulation to social exchange of confidence&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Michael Frank as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Both reviewers agree that your paper has significant merit and represents a potentially important advance for the field. They also do highlight a number of areas where the description of methods needs to be clarified as well as control analyses that would need to be conducted in order to verify the reported results. The reviewers also highlight several areas where the authors' claims should be tempered and/or discussed in more detail.</p><p>1) Both reviewers query the extent to which the paradigm was representative of real-world cooperative decisions. Here, the decision was not agreed upon but rather assigned to the most confident observer. Examination of the degree to which the results might be paradigm-specific is warranted. Relatedly, there are several areas where the claims of the authors would need to be tempered or else additional analyses provided. Reviewer 1 highlights that the authors imply that causal links between pupil diameter, CPP, and confidence have been demonstrated when this is not the case. Analyses establishing such links should be conducted or else the authors' conclusions should be amended.</p><p>2) Both reviewers indicate that additional control analyses should be conducted to verify the connectivity results.</p><p>3) Further analyses need to be conducted in order to establish that the reported ERP signal really is a CPP. Response-locked ERP waveforms should be shown and analysed in order to verify that the observed signal bears the known functional properties of the CPP.</p><p>4) Further detail is required regarding the methods used for analysing the pupil data and for calibrating the stimuli to allow meaningful pupillometry. In addition, Reviewer 2 suggests employing a timeseries-based statistical framework rather than relying on averaging over arbitrary timeframes.</p><p>5) Both reviewers point to insufficient detail being provided regarding the model fitting procedures to allow for replication. Were all parameter values fit by the model or were some of them fixed according to the authors' own criteria?</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Abstract: The authors write that their model &quot;spontaneously demonstrated the emergence of social alignment&quot;. I don't think that this is correct, social alignment was not spontaneous, as a key aspect was introduced specifically for this purpose (using the confidence of one agent as a top-down drive for the decision process of the other agent).</p><p>Introduction: I find that the general use of citations should be improved. Many references are only vaguely related to the content they are supposed to support, and there is a bias towards citing studies that have been published recently and in high-impact journals, at the expense of more relevant ones.</p><p>Results:</p><p>Please report the method and results of the debriefing questionnaire.</p><p>Figure 1b would be easier to understand if the x- and y- axes were swapped, as the participant's confidence is an outcome whereas the agent's confidence is controlled by the experimenter if I understand it correctly.</p><p>In Figure 1 supplement 3, the authors write &quot;The results did not change compared to figure 1c indicating the previous trials confidence impacts the behaviors in the upcoming trial regardless of experiment conditions (HCA, LCA)&quot;. This is not clear to me. Did they compare the effects reported in this figure between the two experimental conditions?</p><p>The authors interpret the larger pupil size (in blocks where participants are paired with a low-confidence agent) as reflecting participants' lower confidence. The evidence supporting this interpretation is far from clear. As noted by the authors, there are several alternative possibilities (arousal for instance). The authors might present more direct evidence linking confidence to pupil size in their own data, e.g. by examining this relation on a trial-by-trial basis, within each block or after the main effect of block is removed from the variables.</p><p>(Top of page 8) The 'intuitive description of the impact of global top-down input' does not provide a clear intuition to the reader. When both traces rise faster, why is there an increase in confidence? why does the difference between the two traces also increase in this case?</p><p>In the empirical and simulated data, it would be important to test the interactions between factors (coherence and partner's confidence, isolated vs. social) for completeness. In addition, the full specification of the GLMMs should be reported in the methods.</p><p>In their modelling work, the authors consider several candidate models in which a specific parameter is affected by the confidence of the partner in the previous trial. Then Figure 3d suggests that in simulations of the 'best' model, confidence matching occurs within the first 5 to 10 trials. When the initial trials are discarded, can we still observe confidence matching? It would be also important to compare this very fast convergence to that occurring in the real data. As the confidence of the partner is mostly modulated across different blocks of trials, it is unclear whether there is really a trial-by-trial adjustment beyond the first few initial trials within each block.</p><p>The model comparison (figure 3 —figure supplement 4) indicates that statistical comparisons between different models were done by using 20 initial points for each model. I don't think that this is relevant. Model fits should be estimated for each participant, and the fit quality can be then compared across participants. Figure 3 – supplement 5 is also not very informative. Individual data and fits, in the format of Figure 1c would be more relevant to examine the quality of the data and model fits.</p><p>The authors write: &quot;These findings are the first neurobiological demonstration of interpersonal alignment by coupling of neural evidence accumulation to social exchange of information&quot;. It is not clear that the CPP contributes to the alignment of confidence. The authors have shown that both CPP and confidence are different between the HCA and LCA conditions. They have not shown that CPP and confidence are actually connected, nor that the change in one variable mediates the change in the other.</p><p>The finding of a greater information flow from the prefrontal to centro-parietal cortex in the HCA condition is potentially interesting, but not so convincing in its current presentation. It would be helpful to run control analyses in order to ensure that a flow in the opposite direction is not as likely and that the result would also not be present with a different region of interest instead of PFC. It would also be important to examine this same quantity in the non-social condition, in order to better qualify the results in the social condition.</p><p>In this section, it's also unclear how this increase in connectivity for HCA contributes to the CPP. Theoretical arguments and empirical data should be provided to address this.</p><p>Finally, I find the writing often unclear, difficult to follow and often trying to oversell the findings. I understand that this is subjective, but I suppose that simpler sentences and shorter expressions would make it easier for the reader (e.g. avoiding word bundles such as &quot;socially observed emergent characteristics of confidence sharing&quot;, or &quot;causally necessary neural substrate&quot;). I find that clarity and concision would help the reader understand the true extent of the contribution of the study.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>To address the weaknesses, it would help if the authors could:</p><p>1) Bolster sample sizes.</p><p>2) Provide a clearer definition of the types of uncertainties that are associated with communicated low confidence, and a discussion of which of these trigger the observed effects.</p><p>3) Describe the pupil analysis in much more detail, in particular how they calibrated the stimuli to allow interpretability of baseline signals and how they selected the specific traces in each trial's ITI so that they are not contaminated by stimuli.</p><p>4) Employ a more convincing time-series-based statistical framework for the analyses of the pupil data that does not rely on crude averaging over arbitrary timeframes, while correcting for multiple comparisons and autocorrelation.</p><p>5) Provide response-locked analyses of the EEG signals to establish that they correspond to the decision-linked CPPs as reported in the literature.</p><p>6) Provide much more information and justification for the selection of the signals that are interpreted as reflecting the top-down drive from the prefrontal cortex, and either also provide source-localization methods or any other type of evidence for the prefrontal origins of these signals. Also please display where in sensor space these signals are taken from (even that is missing).</p><p>7) Describe for every single parameter value that is reported whether it was produced by fitting the model and how exactly this was done, whether it was manually adjusted to produce a desired pattern, or whether it was set to a fixed value based on some clearly defined criteria. The aim is that others can replicate this work and use this model in the future, so this information is crucial!</p><p>8) Provide robustness analyses that show that the assumptions about linear modulation of parameters by confidence and the offset of the accumulation at the end of the stimulation period are justified.</p><p>9) Provide some more discussion about the unnatural properties of the fake interaction partners in this experiment, and to what degree this limits the interpretability of the findings and their generalizability to other contexts. Ideally, the authors would already show in a new sample and setup that the model can apply to real interactions, but that may be too much to ask for a single paper.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.83722.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Both reviewers agree that your paper has significant merit and represents a potentially important advance for the field. They also do highlight a number of areas where the description of methods needs to be clarified as well as control analyses that would need to be conducted in order to verify the reported results. The reviewers also highlight several areas where the authors' claims should be tempered and/or discussed in more detail.</p><p>1) Both reviewers query the extent to which the paradigm was representative of real-world cooperative decisions. Here, the decision was not agreed upon but rather assigned to the most confident observer. Examination of the degree to which the results might be paradigm-specific is warranted. Relatedly, there are several areas where the claims of the authors would need to be tempered or else additional analyses provided. Reviewer 1 highlights that the authors imply that causal links between pupil diameter, CPP, and confidence have been demonstrated when this is not the case. Analyses establishing such links should be conducted or else the authors' conclusions should be amended.</p></disp-quote><p>In the discussion, the revised manuscript addresses the limitations of our design and discusses how our reduced version of interactive decision making differs from realworld joint decisions. Moreover, we have removed the implication of causal link between confidence, CPP and pupil diameter.</p><disp-quote content-type="editor-comment"><p>2) Both reviewers indicate that additional control analyses should be conducted to verify the connectivity results.</p></disp-quote><p>Thanks to reviewers’ comments, we have noted the caveat in our connectivity analysis. Having done a number of careful control analyses, we have decided to remove this analysis from the manuscript.</p><disp-quote content-type="editor-comment"><p>3) Further analyses need to be conducted in order to establish that the reported ERP signal really is a CPP. Response-locked ERP waveforms should be shown and analysed in order to verify that the observed signal bears the known functional properties of the CPP.</p></disp-quote><p>We have now addressed the concern about CPP signals. The revised results include response-locked ERP waveforms that bear the functional properties of CPP.</p><disp-quote content-type="editor-comment"><p>4) Further detail is required regarding the methods used for analysing the pupil data and for calibrating the stimuli to allow meaningful pupillometry. In addition, Reviewer 2 suggests employing a timeseries-based statistical framework rather than relying on averaging over arbitrary timeframes.</p></disp-quote><p>A number of modifications and additions have been made to the manuscript methods regarding the pupil data collection and analysis. We provide the necessary evidence for calibration of the pupil response to the stimulus luminance. In addition, we have reported the requested timeseries-based analysis for pupil data.</p><disp-quote content-type="editor-comment"><p>5) Both reviewers point to insufficient detail being provided regarding the model fitting procedures to allow for replication. Were all parameter values fit by the model or were some of them fixed according to the authors' own criteria?</p></disp-quote><p>We have now provided the detailed procedures of model fitting and model falsification. This includes a dedicated table indicating the procedure used (as well as the citation justifying it) for determining the value of each of the model parameters.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Abstract: The authors write that their model &quot;spontaneously demonstrated the emergence of social alignment&quot;. I don't think that this is correct, social alignment was not spontaneous, as a key aspect was introduced specifically for this purpose (using the confidence of one agent as a top-down drive for the decision process of the other agent).</p></disp-quote><p>We have now modified the abstract and removed the word “spontaneously”. In Lines 51-52 of the manuscript, the abstract now reads:</p><p>“An attractor neural network model incorporating social information as top-down additive input captured the observed behaviour and demonstrated the emergence of social alignment in virtual dyadic simulations.”</p><disp-quote content-type="editor-comment"><p>Introduction: I find that the general use of citations should be improved. Many references are only vaguely related to the content they are supposed to support, and there is a bias towards citing studies that have been published recently and in high-impact journals, at the expense of more relevant ones.</p></disp-quote><p>We have taken the reviewer’s opinion into account and revised our introduction. In doing so we have tried to trim and refine the selection of the cited papers and put more weight on earlier evidence that goes beyond the past few recent years. As indicated by the comment here, we tried to strike a tradeoff between a wider historical bracket and a narrower, more direct thematic relevance.</p><disp-quote content-type="editor-comment"><p>Results:</p><p>Please report the method and results of the debriefing questionnaire.</p></disp-quote><p>The summary of the debriefing results is now included in as a supplementary figure (Figure 1—figure supplement 4). Importantly, none of the participants suspect their partner is a computer agent.</p><disp-quote content-type="editor-comment"><p>Figure 1b would be easier to understand if the x- and y- axes were swapped, as the participant's confidence is an outcome whereas the agent's confidence is controlled by the experimenter if I understand it correctly.</p></disp-quote><p>We have now swapped the axes in Figure 1b of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>In Figure 1 supplement 3, the authors write &quot;The results did not change compared to figure 1c indicating the previous trials confidence impacts the behaviors in the upcoming trial regardless of experiment conditions (HCA, LCA)&quot;. This is not clear to me. Did they compare the effects reported in this figure between the two experimental conditions?</p></disp-quote><p>We apologize for the confusion. In Figure 1c we illustrate the psychometric data (probability correct, confidence and RT) and experimental condition (high- or low confidence partner) is indicated by separate lines of different colours. In the corresponding analysis in the main text (lines 167-175) we compared each of the three aspects of behaviour across the two conditions. We found significant differences between the two conditions for confidence and RT. This analysis treated the condition as a block-design variable.</p><p>In the figure 1—figure supplement 3, we examine whether the participant’s behaviour (again probability correct, confidence and RT) in any given trial, is affected by the partner’s confidence in the immediate previous trial. As a result, in this analysis, the trial-by-trial variation in partner’s confidence is the key independent variable of interest. Here too, we the results showed a significant effect for confidence and RT but not for probability correct.</p><p>To be clear, the two analyses are two variations for testing the same hypothesis and they are not directly, formally compared with one another. The results of the two analyses are consistent with each other. We have now clarified this issue in the caption of Figure1-Supplement Figure 3.</p><disp-quote content-type="editor-comment"><p>The authors interpret the larger pupil size (in blocks where participants are paired with a low-confidence agent) as reflecting participants' lower confidence. The evidence supporting this interpretation is far from clear. As noted by the authors, there are several alternative possibilities (arousal for instance). The authors might present more direct evidence linking confidence to pupil size in their own data, e.g. by examining this relation on a trial-by-trial basis, within each block or after the main effect of block is removed from the variables.</p></disp-quote><p>To address this comment, we have now added a supplementary figure to Figure 2 where we provide supplementary evidence linking the participant’s own confidence to pupil size after partialing out the contribution of other factors. We employed a time-series analysis and examined the hypothesis that changes in pupil size are correlated with the participant’s own confidence in the isolated condition (also see figure 2—figure supplement 2). We found that confidence is encoded in the pupil size during inter-trial interval (ITI). Our findings are consistent with previous works (Urai et al., 2017) who showed that post-choice pupil signal varies with subjective confidence in perceptual decision making.</p><disp-quote content-type="editor-comment"><p>(Top of page 8) The 'intuitive description of the impact of global top-down input' does not provide a clear intuition to the reader. When both traces rise faster, why is there an increase in confidence? why does the difference between the two traces also increase in this case?</p></disp-quote><p>We have now clarified this point in the main text as follows (lines 255-265):</p><p>“We modeled the social context as a global, top-down additive input (Figure 3a; <italic>Wx</italic>) in the attractor model. This input drove both accumulator mechanisms equally and positively. The impact of this global top-down input is illustrated in Figure 3a right: with a positive top-down drive (Wx&gt;0), the winner (thick blue) and the loser (thick red) traces both rise faster compared to zero top-down drive (dotted lines). The model’s counterintuitive feature is that the surface area between the winning and losing accumulator is larger in the case of positive (dark grey shading) versus zero (light grey shading) top-down input. Model simulations show that when <italic>Wx</italic> &gt; 0, this difference in surface area leads to faster RTs and higher confidence but does not change accuracy because it does not affect the decision boundary. These simulation results are consistent with our behavioral findings comparing HCA vs LCA conditions (Figure 1c).”</p><disp-quote content-type="editor-comment"><p>In the empirical and simulated data, it would be important to test the interactions between factors (coherence and partner's confidence, isolated vs. social) for completeness. In addition, the full specification of the GLMMs should be reported in the methods.</p></disp-quote><p>We have updated this GLMM analysis and included the interaction terms. The results of this analysis are now reported in table Supplementary File 1g.</p><disp-quote content-type="editor-comment"><p>In their modelling work, the authors consider several candidate models in which a specific parameter is affected by the confidence of the partner in the previous trial. Then Figure 3d suggests that in simulations of the 'best' model, confidence matching occurs within the first 5 to 10 trials. When the initial trials are discarded, can we still observe confidence matching? It would be also important to compare this very fast convergence to that occurring in the real data. As the confidence of the partner is mostly modulated across different blocks of trials, it is unclear whether there is really a trial-by-trial adjustment beyond the first few initial trials within each block.</p></disp-quote><p>We are grateful to the reviewer for this idea. It is possible to answer this question both empirically and computationally. To empirically examine the speed of confidence matching, as requested here, in Figure 3-figure supplement 6, in the top box we have plotted the empirically observed timeline of confidence matching in the two studies. Here the absolute difference between the confidence of the agent and that of the subject is plotted against the trial number which indicates time. Observing the curves suggests that confidence matching starts quickly and then slows down as indicated by our simulations. The empirical data is, naturally, more noisy but the results do indicate that most of the matching happens at the very beginning. These empirical results come with the caveat that in our experiment, only one side of each pair, i.e., the participant behaviour was dynamically responsive and the partner’s responses were (by design) unaffected by the participant’s behaviour. Nonetheless, it is still possible to observe the trace of confidence matching in these data.</p><p>To answer the question at the level of the model, we note that our model in its basic form, does not flexibly adjust the speed of matching since we do not have any parameters to modify the amount of adjustment by the top down input. However, this could be achieved with a simple modification by adding a time constant to the top-down current. In this modified formulation, the amount of top-down current depends on the trial number such that <italic>W<sub>x</sub></italic>(<italic>t</italic>) = <italic>W<sub>x</sub></italic><sub>0</sub>(<italic>i</italic>/(<italic>i</italic> + τ)) where <italic>i</italic> is the trial number, τ is the time constant and <italic>W<sub>x</sub></italic><sub>0</sub> is the asymptotic value indicated by the dashed line in the panel. As can be observed in panels b-c, confidence matching is faster with lower values of τ.</p><p>We have now added the response to this comment to figure 3—figure supplement figure 6.</p><disp-quote content-type="editor-comment"><p>The model comparison (figure 3 —figure supplement 4) indicates that statistical comparisons between different models were done by using 20 initial points for each model. I don't think that this is relevant. Model fits should be estimated for each participant, and the fit quality can be then compared across participants. Figure 3 – supplement 5 is also not very informative. Individual data and fits, in the format of Figure 1c would be more relevant to examine the quality of the data and model fits.</p></disp-quote><p>We have now incorporated the requested changes into our fitting procedure and the reporting of the fitting results. In the revised manuscript, we report the following procedure:</p><p>First, we fitted the model to each single participant data, estimating a separate set of parameters for the Isolated Condition session and another one for the data from each of the Social Conditions i.e., LCA and HCA. The fit quality to different models was compared across participants as the reviewer suggested. As is often the case, there was some variability across participants (see the Pie chart figure 3—figure supplement 4).</p><p>To strengthen the conclusions about model comparison, we also provide evidence from a model falsification exercise that we performed.</p><p>Finally, with respect to the reviewer’s concern about visualizations of model fits to behavioral data, we have now modified figure 3—figure supplement 5 to the same conventions as Figure 1c.</p><p>To compare rival models, in addition to examining how well they fair in fitting to the empirical data, it is important and useful to examine if any of the models can be falsified in the light of the observed data. To do so, rather than fitting the models to the data from the social condition, here we first fit each candidate model to the data from the Isolated condition and extract the model parameters. Then we simulate the social condition (using the model-specific fitted parameters) to see if the simulation can qualitatively reproduce the pattern of empirical data (i.e., reaction time, accuracy, and confidence) observed empirically in the social condition. To recap, we compare our proposed model (top down input, TD) and three alternative models (Bound, NDT, Mu). For the TD model we have:</p><p><italic>TD<sub>t</sub></italic> = <italic>TD<sub>Isolated</sub></italic> + <italic>a</italic>(<italic>AgentConf<sub>t</sub></italic><sub>–1</sub>) (R2)</p><p>Where <italic>TD<sub>isolated</sub></italic>=0 and <italic>AgentConf</italic> is the confidence of the agent, <italic>t</italic> is the trial time and <italic>a</italic> is the coupling parameter. Similarly, we defined the model Bound, NDT and Mu, respectively, as follows:</p><p><italic>Bound<sub>t</sub></italic> = <italic>Bound<sub>Isolated</sub></italic> + <italic>a</italic>(<italic>AgentConf<sub>t</sub></italic><sub>–1</sub>) (R3)</p><p><italic>NDT<sub>t</sub></italic> = <italic>NDT<sub>Isolated</sub></italic> + <italic>a</italic>(<italic>AgentConf<sub>t</sub></italic><sub>–1</sub>) (R4)</p><p><italic>Mu<sub>t</sub></italic> = <italic>Mu<sub>Isolated</sub></italic> + <italic>a</italic>(<italic>AgentConf<sub>t</sub></italic><sub>–1</sub>) (R5)</p><p>We simulated each of these models under different values of <italic>a</italic>. We used 0.003 and 0 for HCA (magenta) and LCA (orange) respectively. Similarly, we respectively used sets of (-0.15=<italic>a_HCA</italic>, 0=<italic>a_LCA</italic>), (-0.15, 0), (0.15, 0) as <italic>a</italic> for Bound, NDT, and Mu models. Then we simulated each model under these parameters 2000 times for each coherence level (form 0 to 25%). Then, we plotted the average of each model under each condition as shown Figure 3 – figure supplement 7.</p><p>Importantly, these simulations are not dependent on a specific parameter value. We have included the scripts for these model simulations here for readers to try different values for themselves https://github.com/Jimmy- 2016/ConfMatchEEG/tree/main/test_alternative_models. This repository is also listed in the manuscript.</p><p>Here, we also provide the intuition for why the alternative models fail to reproduce the observed behavioral effect:</p><p>Bound Model: Changing the decision bound would indeed change RT dramatically since the amount of evidence the model needs to decide would be different. However, a change in decision bound changes the accuracy as well. The higher the bound, the higher the accuracy. Most importantly, in our formulation, confidence does not depend on bound. We made this decision because there are many studies that have argued that confidence is related to post-decision evidence (Balsdon et al., 2020; Moran et al., 2015; Navajas et al., 2016; Yu et al., 2015). Therefore, changing the bound does not change the confidence.</p><p>NDT Model: Non-decision time is stimulus independent as can be seen in simulation results (Figure 3—figure supplement 7. C right panel). Therefore, we expect that increasing non-decision time would not have any effect on accuracy and confidence. This is what we observe in our simulations (Figure 3—figure supplement 7. C, left and middle). Importantly, this pattern was not what we have observed in behavioral data.</p><p>Mu Model: By increasing the Mu, the model becomes more sensitive to the evidence. This means it would accumulate the evidence more efficiently (faster and with more accuracy). Therefore by increasing the Mu, accuracy and confidence increase and RT decreases (Figure 3—figure supplement 7. D) which is inconsistent with our empirical observations.</p><disp-quote content-type="editor-comment"><p>The authors write: &quot;These findings are the first neurobiological demonstration of interpersonal alignment by coupling of neural evidence accumulation to social exchange of information&quot;. It is not clear that the CPP contributes to the alignment of confidence. The authors have shown that both CPP and confidence are different between the HCA and LCA conditions. They have not shown that CPP and confidence are actually connected, nor that the change in one variable mediates the change in the other.</p></disp-quote><p>We have now followed the reviewer’s more cautious approach and modified the quoted section (line 501-510) as follows:</p><p>“These findings demonstrate that interpersonal alignment of confidence is associated with a modulation of neural evidence accumulation - as quantified by CPP - by the social exchange of information (also see figure 4-figure supplement 3). It is important to note a caveat here before moving forward. These data show that both CPP and confidence are different between the HCA and LCA conditions. However, due to the nature of our experimental design, it would be premature to conclude from them that CPP <italic>contributes causally to</italic> the alignment of subjectively held beliefs and/or behaviourally expressed confidence. Put together with the behavioral confidence matching (Figure 1b) and the pupil data (Figure 2) our findings suggest that some such neural-social coupling could be the underlying basis for the construction of a shared belief about uncertainty.”</p><disp-quote content-type="editor-comment"><p>The finding of a greater information flow from the prefrontal to centro-parietal cortex in the HCA condition is potentially interesting, but not so convincing in its current presentation. It would be helpful to run control analyses in order to ensure that a flow in the opposite direction is not as likely and that the result would also not be present with a different region of interest instead of PFC. It would also be important to examine this same quantity in the non-social condition, in order to better qualify the results in the social condition.</p></disp-quote><p>We are grateful to both reviewers for their notes of caution regarding the connectivity analysis. After having performed the control analyses suggested here, we have decided to remove this connectivity analysis from the manuscript. Most importantly, our analysis did not yield a convincing result when we swapped the PFC with a control region of interest. We have therefore come to the conclusion that it is best to leave the connectivity analysis out of the paper.</p><disp-quote content-type="editor-comment"><p>In this section, it's also unclear how this increase in connectivity for HCA contributes to the CPP. Theoretical arguments and empirical data should be provided to address this.</p></disp-quote><p>Following from removing the connectivity analysis, we have also removed the corresponding speculation about the role of increased connectivity under HCA condition.</p><disp-quote content-type="editor-comment"><p>Finally, I find the writing often unclear, difficult to follow and often trying to oversell the findings. I understand that this is subjective, but I suppose that simpler sentences and shorter expressions would make it easier for the reader (e.g. avoiding word bundles such as &quot;socially observed emergent characteristics of confidence sharing&quot;, or &quot;causally necessary neural substrate&quot;). I find that clarity and concision would help the reader understand the true extent of the contribution of the study.</p></disp-quote><p>In the revised manuscript, we have taken the reviewer’s advice on board and attempted to use simpler and clearer language and avoid overly exaggerated claims.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>To address the weaknesses, it would help if the authors could:</p><p>1) Bolster sample sizes.</p></disp-quote><p>Our study brings together questions from two distinct fields of neuroscience: perceptual decision making and social neuroscience. Each of these two fields have their own traditions and practical common sense. Typically, studies in perceptual decision making employ a small number of extensively trained participants (approximately 6 to 10 individuals). Social neuroscience studies, on the other hand, recruit larger samples (often more than 20 participants) without extensive training protocols. We therefore needed to strike a balance in this trade-off between number of participants and number of data points (e.g. trials) obtained from each participant. Note, for example, that each of our participants underwent around 4000 training trials. Strikingly, our initial study (N=12) yielded robust results that showed the hypothesized effects nearly completely, supporting the adequacy of our power estimate. However, we decided to replicate the findings because, like the reviewer, we believe in the importance of adequate sampling. We increased our sample size to N=15 participants to enhance the reliability of our findings. However, we acknowledge the limitation of generalizing to larger samples, which we have now discussed in our revised manuscript and included a cautionary note regarding further generalizations.</p><p>To complement our results and add a measure of their reliability, here we provide the results of a power analysis that we applied on the data from study 1 (i.e. the discovery phase). These results demonstrate that the sample size of study 2 (i.e. replication) was adequate when conditioned on the results from study 1. The results showed that N=13 would be an adequate sample size for 80% power for behavoural and eye-tracking measurements. Power analysis for the EEG measurements indicated that we needed N=17. Combining these power analyses. Our sample size of N=15 for Study 2 was therefore reasonably justified.</p><p>We have now added a section to the discussion (Lines 790-805) that communicates these issues as follows:</p><p>“Our study brings together questions from two distinct fields of neuroscience: perceptual decision making and social neuroscience. Each of these two fields have their own traditions and practical common sense. Typically, studies in perceptual decision making employ a small number of extensively trained participants (approximately 6 to 10 individuals). Social neuroscience studies, on the other hand, recruit larger samples (often more than 20 participants) without extensive training protocols. We therefore needed to strike a balance in this trade-off between number of participants and number of data points (e.g. trials) obtained from each participant. Note, for example, that each of our participants underwent around 4000 training trials. Importantly, our initial study (N=12) yielded robust results that showed the hypothesized effects nearly completely, supporting the adequacy of our power estimate.</p><p>However, we decided to replicate the findings in a new sample with N=15 participants to enhance the reliability of our findings and examine our hypothesis in a stringent discovery-replication design. In Figure 4-figure supplement 5, we provide the results of a power analysis that we applied on the data from study 1 (i.e. the discovery phase). These results demonstrate that the sample size of study 2 (i.e. replication) was adequate when conditioned on the results from study 1.”</p><p>We conducted Monte Carlo simulations to determine the sample size required to achieve sufficient statistical power (80%) (Szucs and Ioannidis, 2017). In these simulations, we utilized the data from study 1. Within each sample size (N, x-axis), we randomly selected N participants from our 12 participants in study 1. We employed the with-replacement sampling method. Subsequently, we applied the same GLMM model used in the main text to assess the dependency of EEG signal slopes on social conditions (HCA vs LCA). To obtain an accurate estimate, we repeated the random sampling process 1000 times for each given sample size (N). Consequently, for a given sample size, we performed 1000 statistical tests using these randomly generated datasets. The proportion of statistically significant tests among these 1000 tests represents the statistical power (y-axis). We gradually increased the sample size until achieving an 80% power threshold, as illustrated in the figure. The number indicated by the red circle on the x axis of this graph represents the designated sample size.</p><disp-quote content-type="editor-comment"><p>2) Provide a clearer definition of the types of uncertainties that are associated with communicated low confidence, and a discussion of which of these trigger the observed effects.</p></disp-quote><p>We appreciate the reviewer’s advice to remain cautious about the possible sources of uncertainty in our experiment. In the Discussion (lines 790-801) we have now added the following paragraph.</p><p>“We have interpreted our findings to indicate that social information, i.e. partner’s confidence, impacts the participants beliefs about uncertainty. It is important to underscore here that, similar to real life, there are other sources of uncertainty in our experimental setup that could affect the participants' belief. For example, under joint conditions, the group choice is determined through the comparison of the choices and confidences of the partners. As a result, the participant has a more complex task of matching their response not only with their perceptual experience but also coordinating it with the partner to achieve the best possible outcome. For the same reason, there is greater outcome uncertainty under joint vs individual conditions. Of course, these other sources of uncertainty are conceptually related to communicated confidence but our experimental design aimed to remove them, as much as possible, by comparing the impact of social information under high vs low confidence of the partner.”</p><p>In addition to the above, we would like to clarify one point here with specific respect to the comment. Note that the computer-generated partner’s accuracy was identical under high and low confidence. In addition, our behavioral findings did not show any difference in accuracy under HCA and LCA conditions. As a consequence, the argument that “the trial outcome is determined by the joint performance of both partners, which is normally reduced for low-confidence partners” is not valid because the low-confidence partner’s performance is identical to that of the high-confidence partner. It is possible, of course, that we have misunderstood the reviewer’s point here and we would be happy to discuss this further if necessary.</p><p>In <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> we provide the calibration plot for our eye tracking setup, describing the relationship between pupil size and display luminance. Luminance of the random dot motion stimuli (ie white dots on black background) was Cd/m<sup>2</sup> and, importantly, identical across the two critical social conditions. We hope that this additional detail satisfies the reviewer’s concern. For the purpose of brevity, we have decided against adding this part to the manuscript and supplementary material.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Calibration plot for the experimental setup.</title><p>Average pupil size (arbitrary units from eyelink device) is plotted against display luminance. The plot is obtained by presenting the participant with uniform full screen displays with 10 different luminance levels covering the entire range of the monitor RGB values (0 to 255) whose luminance was separately measured with a photometer. Each display lasted 10 seconds. Error bars are standard deviation between sessions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-83722-sa2-fig1-v1.tif"/></fig><p>Moreover, while the authors state that the traces were normalized to a value of 0 at the start of the ITI period, the data displayed in Figure 2 do not show this normalization but different non-zero values. Are these data not normalized, or was a different procedure used?</p><p>Finally, the authors analyze the pupil signal averaged across a wide temporal ITI interval that may contain stimulus-locked responses (there is not enough information in the manuscript to clearly determine which temporal interval was chosen and averaged across, and how it was made sure that this signal was not contaminated by stimulus effects).</p><disp-quote content-type="editor-comment"><p>3) Describe the pupil analysis in much more detail, in particular how they calibrated the stimuli to allow interpretability of baseline signals and how they selected the specific traces in each trial's ITI so that they are not contaminated by stimuli.</p></disp-quote><p>We have now added the following details to the Methods section in line 1106-1135.</p><p>“In both studies, the Eye movements were recorded by an EyeLink 1000 (SR- Research) device with a sampling rate of 1000Hz which was controlled by a dedicated host PC. The device was set in a desktop and pupil-corneal reflection mode while data from the left eye was recorded. At the beginning of each block, the system was recalibrated and then validated by 9-point schema presented on the screen. For one subject was, a 3-point schema was used due to repetitive calibration difficulty. Having reached a detection error of less than 0.5°, the participants proceeded to the main task. Acquired eye data for pupil size were used for further analysis. Data of one subject in the first study was removed from further analysis due to storage failure.</p><p>Pupil data were divided into separate epochs and data from Inter-Trials Interval (ITI) were selected for analysis. ITI interval was defined as the time between offset of trial (t) feedback screen and stimulus presentation of trial (t+1). Then, blinks and jitters were detected and removed using linear interpolation. Values of pupil size before and after the blink were used for this interpolation. Data was also mid-pass filtered using a Butterworth filter (second order, [0.01, 6] Hz)[50]. The pupil data was z-scored and then was baseline corrected by removing the average of signal in the period of [-1000 0] ms interval (before ITI onset). For the statistical analysis (GLMM) in Figure 2, we used the average of the pupil signal in the ITI period. Therefore, no pupil value is contaminated by the upcoming stimuli. Importantly, trials with ITI&gt;3s were excluded from analysis (365 out of 8800 for study 1 and 128 out 6000 for study 2. Also see Supplementary File 1f and Selection criteria for data analysis in Supplementary Materials)”</p><disp-quote content-type="editor-comment"><p>4) Employ a more convincing time-series-based statistical framework for the analyses of the pupil data that does not rely on crude averaging over arbitrary timeframes, while correcting for multiple comparisons and autocorrelation.</p></disp-quote><p>In the revised manuscript, we have added a time series analysis that demonstrates the temporal encoding of experimental conditions in the pupil signal. We employed a general linear mixed model (GLMM) using the model Pupil(t) = b0 + b1*socialCondition</p><p>where social conditions are HCA=1 and LCA=2. The GLMM was applied to the time window from 0 to 2 seconds after the onset of ITI. We have plotted the time course of b1 (error bar = standard error obtained from the GLMM). We discretized time into nonoverlapping bins of 100ms width and averaged the data points falling within each bin.</p><p>This analysis is now added to the manuscript in figure 2—figure supplement 2.</p><disp-quote content-type="editor-comment"><p>5) Provide response-locked analyses of the EEG signals to establish that they correspond to the decision-linked CPPs as reported in the literature.</p></disp-quote><p>We are grateful to the reviewer for this helpful suggestion. We have now added the response-locked analysis of the CPP signals (see figure 4- supplement figure 4). As expected by the reviewer, we do see that the response-locked CPP waveforms converge to one another for high vs low coherence trials at the moment of the response. This pattern is more clearly seen in experiment 2. It is worth noting that previous studies that examined response-locked CPP employed reaction time (or long duration) tasks with variable stimulus duration. In our study, however, stimulus duration was fixed. Our data therefore provide a new addition to this literature confirming response locked CPP do not depend on termination of stimulus by response. We have now added the information to the manuscript.</p><disp-quote content-type="editor-comment"><p>6) Provide much more information and justification for the selection of the signals that are interpreted as reflecting the top-down drive from the prefrontal cortex, and either also provide source-localization methods or any other type of evidence for the prefrontal origins of these signals. Also please display where in sensor space these signals are taken from (even that is missing).</p></disp-quote><p>We are grateful to both reviewers for their notes of caution regarding the connectivity analysis. After having performed the control analyses suggested by reviewer 1 and the checks requested here, we have decided to remove this connectivity analysis from the manuscript.</p><disp-quote content-type="editor-comment"><p>7) Describe for every single parameter value that is reported whether it was produced by fitting the model and how exactly this was done, whether it was manually adjusted to produce a desired pattern, or whether it was set to a fixed value based on some clearly defined criteria. The aim is that others can replicate this work and use this model in the future, so this information is crucial!</p></disp-quote><p>In Supplementary material Supplementary File 1h, we have now described, for every single parameter that is reported, whether they adjusted (manually or fitted) or fixed. In each case, the relevant reference is also reported.</p><p>Please note that since we use the same model as (Wong, 2006), we kept most of the parameters the same as the original study. The following 5 parameters were modified</p><p>1. Decision threshold</p><p>2. Mu0 which is the gain</p><p>3. Recurrent connection (JN11 and JN22)</p><p>4. Inhibitory connection (JN12 and JN21) 5. Non-decision time.</p><p>Moreover, in order to map the difference between S1 and S2 activity to the confidence landscape (from 0 to 1), we used a mapping function as described in (Wei and Wang, 2015) and described as the following equation:</p><p><italic>Normalized Conf</italic> = <italic>b</italic>1 + <italic>a</italic>/exp(<italic>K</italic> * <italic>confdiff</italic> – <italic>b</italic>0) (R5)</p><p>Where <italic>confdiff</italic> is the sum of the difference between losing and winning accumulators during the stimulus presentation (0-500) and <italic>b<sub>1</sub></italic>, <italic>a</italic>, <italic>K</italic>, <italic>b<sub>0</sub></italic> are free parameters. We have now reported this table and the accompanying explanations in the Supplementary material Supplementary File 1h.</p><disp-quote content-type="editor-comment"><p>8) Provide robustness analyses that show that the assumptions about linear modulation of parameters by confidence and the offset of the accumulation at the end of the stimulation period are justified.</p></disp-quote><p>We have now added the following further analysis and showed that our model is indeed not sensitive to linearity assumption (see figure 3—figure supplement 8).</p><disp-quote content-type="editor-comment"><p>9) Provide some more discussion about the unnatural properties of the fake interaction partners in this experiment, and to what degree this limits the interpretability of the findings and their generalizability to other contexts. Ideally, the authors would already show in a new sample and setup that the model can apply to real interactions, but that may be too much to ask for a single paper.</p></disp-quote><p>A similar concern was expressed by reviewer 1. We have now acknowledged this caveat in the discussion in line 485 to 504. The final paragraph of the discussion now reads as follows:</p><p>“Finally, one natural limitation of our experimental setup is that the situation being studied is very specific to the design choices made by the experimenters. These choices were made in order to operationalize the problem of social interaction within the psychophysics laboratory. For example, the joint decisions were not an agreement between partners (Bahrami et al., 2010, 2012). Instead, following a number of previous works (Bang et al., 2017, 2020) joint decisions were automatically assigned to the most confident choice. In addition, partner’s confidence and choice were random variables drawn from a distribution prespecified by the experimenter and therefore, by design, unresponsive to the participant’s behaviour. In this sense, one may argue that the interaction partner’s behaviour was not “natural” since they did not react to the participant's confidence communications (note however that the partner’s response times and accuracy were not entirely random but matched carefully to the participant’s behavior prerecorded in the individual session). How much of the findings are specific to these experimental setting and whether the behavior observed here would transfer to other real-life settings is an open question. For example, it is plausible that participants may show some behavioral reaction to the response time variations since there is some evidence indicating that for binary choices like here, response times also systematically communicate uncertainty to others (Patel et al., 2012). Future studies could examine the degree to which the results might be paradigm-specific.”</p><p>References cited in the rebuttal:</p><p>Bahrami, B., Olsen, K., Bang, D., Roepstorff, A., Rees, G., and Frith, C. (2012). What failure in collective decision-making tells us about metacognition. <italic>Philosophical Transactions of the Royal Society B: Biological Sciences</italic>, <italic>367</italic>(1594), 1350–1365. https://doi.org/10.1098/rstb.2011.0420</p><p>Bahrami, B., Olsen, K., Latham, P. E., Roepstorff, A., Rees, G., and Frith, C. D. (2010).</p><p>Optimally interacting minds. <italic>Science</italic>, <italic>329</italic>(5995), 1081–1085. https://doi.org/10.1126/science.1185718</p><p>Balsdon, T., Wyart, V., and Mamassian, P. (2020). Confidence controls perceptual evidence accumulation. <italic>Nature Communications</italic>, <italic>11</italic>(1753), 1–11. https://doi.org/10.1038/s41467-020-15561-w</p><p>Bang, D., Aitchison, L., Moran, R., Herce Castanon, S., Rafiee, B., Mahmoodi, A., Lau, J. Y. F., Latham, P. E., Bahrami, B., and Summerfield, C. (2017). Confidence matching in group decision-making. <italic>Nature Human Behaviour</italic>, <italic>1</italic>(6), 0117. https://doi.org/10.1038/s41562-017-0117</p><p>Bang, D., Ershadmanesh, S., Nili, H., and Fleming, S. M. (2020). Private–public mappings in human prefrontal cortex. <italic>eLife</italic>, <italic>9</italic>, 1–25. https://doi.org/10.7554/<italic>eLife</italic>.56477</p><p>Kelly, S. P., and O’Connell, R. G. (2013). Internal and External Influences on the Rate of Sensory Evidence Accumulation in the Human Brain. <italic>Journal of Neuroscience</italic>, <italic>33</italic>(50),</p><p>19434–19441. https://doi.org/10.1523/JNEUROSCI.3355-13.2013</p><p>Loughnane, G. M., Newman, D. P., Tamang, S., Kelly, S. P., and O’Connell, R. G. (2018). Antagonistic Interactions Between Microsaccades and Evidence Accumulation</p><p>Processes During Decision Formation. <italic>The Journal of Neuroscience</italic>, <italic>38</italic>(9), 2163–</p><p>2176. https://doi.org/10.1523/JNEUROSCI.2340-17.2018</p><p>Moran, R., Teodorescu, A. R., and Usher, M. (2015). Post choice information integration as a causal determinant of confidence: Novel data and a computational account. <italic>Cognitive Psychology</italic>, <italic>78</italic>, 99–147. https://doi.org/10.1016/j.cogpsych.2015.01.002</p><p>Navajas, J., Bahrami, B., and Latham, P. E. (2016). Post-decisional accounts of biases in confidence. <italic>Current Opinion in Behavioral Sciences</italic>, <italic>11</italic>, 55–60. https://doi.org/10.1016/j.cobeha.2016.05.005</p><p>O’Connell, R. G., Dockree, P. M., and Kelly, S. P. (2012). A supramodal accumulation-tobound signal that determines perceptual decisions in humans. <italic>Nature Neuroscience</italic>,</p><p><italic>15</italic>(12), 1729–1735. https://doi.org/10.1038/nn.3248</p><p>O’Connell, R. G., Shadlen, M. N., Wong-Lin, K., and Kelly, S. P. (2018). Bridging Neural and Computational Viewpoints on Perceptual Decision-Making. <italic>Trends in Neurosciences</italic>,</p><p><italic>41</italic>(11), 838–852. https://doi.org/10.1016/j.tins.2018.06.005</p><p>Pan, J., Klímová, M., McGuire, J. T., and Ling, S. (2022). Arousal-based pupil modulation is dictated by luminance. <italic>Scientific Reports</italic>, <italic>12</italic>(1), 1390. https://doi.org/10.1038/s41598022-05280-1</p><p>Patel, D., Fleming, S. M., and Kilner, J. M. (2012). Inferring subjective states through the observation of actions. <italic>Proceedings of the Royal Society B: Biological Sciences</italic>,</p><p><italic>279</italic>(1748), 4853–4860. https://doi.org/10.1098/rspb.2012.1847</p><p>Szucs, D., and Ioannidis, J. P. A. (2017). Empirical assessment of published effect sizes and power in the recent cognitive neuroscience and psychology literature. <italic>PLOS Biology</italic>,</p><p><italic>15</italic>(3), e2000797-. https://doi.org/10.1371/journal.pbio.2000797</p><p>Urai, A. E., Braun, A., and Donner, T. H. (2017). Pupil-linked arousal is driven by decision uncertainty and alters serial choice bias. <italic>Nature Communications</italic>, <italic>8</italic>. https://doi.org/10.1038/ncomms14637</p><p>Vafaei Shooshtari, S., Esmaily Sadrabadi, J., Azizi, Z., and Ebrahimpour, R. (2019). Confidence Representation of Perceptual Decision by EEG and Eye Data in a Random Dot Motion Task. <italic>Neuroscience</italic>, <italic>406</italic>(March), 510–527. https://doi.org/10.1016/j.neuroscience.2019.03.031</p><p>Wang, X. J. (2002). Probabilistic decision making by slow reverrberation in cortical circuits. <italic>Neuron</italic>, <italic>36</italic>, 955–968.</p><p>Wei, Z., and Wang, X.-J. (2015). Confidence estimation as a stochastic process in a neurodynamical system of decision making. <italic>Journal of Neurophysiology</italic>, <italic>114</italic>(1), 99– 113. https://doi.org/10.1152/jn.00793.2014</p><p>Wong, K.-F. (2006). A Recurrent Network Mechanism of Time Integration in Perceptual Decisions. <italic>Journal of Neuroscience</italic>, <italic>26</italic>(4), 1314–1328. https://doi.org/10.1523/JNEUROSCI.3733-05.2006</p><p>Yu, S., Pleskac, T. J., and Zeigenfuse, M. D. (2015). Dynamics of postdecisional processing of confidence. <italic>Journal of Experimental Psychology: General</italic>, <italic>144</italic>(2), 489–510. https://doi.org/10.1037/xge0000062</p></body></sub-article></article>