<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">66463</article-id><article-id pub-id-type="doi">10.7554/eLife.66463</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A functional model of adult dentate gyrus neurogenesis</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-221681"><name><surname>Gozel</surname><given-names>Olivia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2223-4097</contrib-id><email>gozel@uchicago.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">†</xref></contrib><contrib contrib-type="author" id="author-3010"><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>School of Life Sciences and School of Computer and Communication Sciences, Ecole Polytechnique Fédérale de Lausanne</institution><addr-line><named-content content-type="city">Lausanne</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution>Departments of Neurobiology and Statistics, University of Chicago</institution><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Grossman Center for Quantitative Biology and Human Behavior, University of Chicago</institution><addr-line><named-content content-type="city">Chicago</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution>Salk Institute for Biological Studies</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Huguenard</surname><given-names>John R</given-names></name><role>Senior Editor</role><aff><institution>Stanford University School of Medicine</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>†</label><p>Departments of Neurobiology and Statistics, University of Chicago, Chicago, United States</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>17</day><month>06</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e66463</elocation-id><history><date date-type="received" iso-8601-date="2021-01-12"><day>12</day><month>01</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-06-16"><day>16</day><month>06</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Gozel and Gerstner</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Gozel and Gerstner</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-66463-v2.pdf"/><abstract><p>In adult dentate gyrus neurogenesis, the link between maturation of newborn neurons and their function, such as behavioral pattern separation, has remained puzzling. By analyzing a theoretical model, we show that the switch from excitation to inhibition of the GABAergic input onto maturing newborn cells is crucial for their proper functional integration. When the GABAergic input is excitatory, cooperativity drives the growth of synapses such that newborn cells become sensitive to stimuli similar to those that activate mature cells. When GABAergic input switches to inhibitory, competition pushes the configuration of synapses onto newborn cells toward stimuli that are different from previously stored ones. This enables the maturing newborn cells to code for concepts that are novel, yet similar to familiar ones. Our theory of newborn cell maturation explains both how adult-born dentate granule cells integrate into the preexisting network and why they promote separation of similar but not distinct patterns.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>adult neurogenesis</kwd><kwd>dentate gyrus</kwd><kwd>unsupervised learning</kwd><kwd>synaptic plasticity</kwd><kwd>competitive network</kwd><kwd>pattern separation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Swiss National Science Foundation</institution></institution-wrap></funding-source><award-id>200020 184615</award-id><principal-award-recipient><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010661</institution-id><institution>Horizon 2020 Framework Programme</institution></institution-wrap></funding-source><award-id>785907</award-id><principal-award-recipient><name><surname>Gerstner</surname><given-names>Wulfram</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The GABA input switching from excitatory to inhibitory during maturation of adult-born dentate granule cells is crucial for their integration into the preexisting circuit.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>In the adult mammalian brain, neurogenesis, the production of new neurons, is restricted to a few brain areas, such as the olfactory bulb and the dentate gyrus (<xref ref-type="bibr" rid="bib31">Deng et al., 2010</xref>). The dentate gyrus is a major entry point of input from cortex, primarily entorhinal cortex (EC), to the hippocampus (<xref ref-type="bibr" rid="bib8">Amaral et al., 2007</xref>), which is believed to be a substrate of learning and memory (<xref ref-type="bibr" rid="bib52">Jarrard, 1993</xref>). Adult-born cells in dentate gyrus mostly develop into dentate granule cells (DGCs), the main excitatory cells that project to area CA3 of hippocampus (<xref ref-type="bibr" rid="bib31">Deng et al., 2010</xref>).</p><p>The properties of rodent adult-born DGCs change as a function of their maturation stage, until they become indistinguishable from other mature DGCs at approximately 8 weeks (<xref ref-type="bibr" rid="bib31">Deng et al., 2010</xref>; <xref ref-type="bibr" rid="bib54">Johnston et al., 2016</xref>; <xref ref-type="fig" rid="fig1">Figure 1a</xref>). Many of them die before they fully mature (<xref ref-type="bibr" rid="bib29">Dayer et al., 2003</xref>). Their survival is experience dependent and relies on NMDA receptor activation (<xref ref-type="bibr" rid="bib86">Tashiro et al., 2006</xref>). Initially, newborn DGCs have enhanced excitability (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib62">Li et al., 2017</xref>) and stronger synaptic plasticity than mature DGCs, reflected by a larger long-term potentiation (LTP) amplitude and a lower threshold for induction of LTP (<xref ref-type="bibr" rid="bib93">Wang et al., 2000</xref>; <xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib41">Ge et al., 2007</xref>). Furthermore, after 4 weeks of maturation adult-born DGCs have only weak connections to interneurons, while at 7 weeks of age, their activity causes indirect inhibition of mature DGCs (<xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Network model and pretraining.</title><p>(<bold>a</bold>) Integration of an adult-born DGC (blue) as a function of time: GABAergic synaptic input (red) switches from excitatory (+) to inhibitory (−); strong connections to interneurons develop only later; glutamatergic synaptic input (black), interneuron (red). (<bold>b</bold>) Network structure. EC neurons (black, rate <italic>x</italic><sub><italic>j</italic></sub>) are fully connected with weights <inline-formula><mml:math id="inf1"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> to DGCs (blue, rate <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>). The feedforward weight vector <inline-formula><mml:math id="inf3"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> onto neuron <inline-formula><mml:math id="inf4"><mml:mi>i</mml:mi></mml:math></inline-formula> is depicted in black. DGCs and interneurons (red, rate <inline-formula><mml:math id="inf5"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:math></inline-formula>) are mutually connected with probability <inline-formula><mml:math id="inf6"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and weights <inline-formula><mml:math id="inf8"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, respectively. Connections with a triangular (round) end are glutamatergic (GABAergic). (<bold>c</bold>) Given presynaptic activity <inline-formula><mml:math id="inf10"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, the weight update <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is shown as a function of the firing rate <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the postsynaptic DGC with LTD for <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula> and LTP for <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. (<bold>d</bold>) Center of mass for three ensembles of patterns from the MNIST data set, visualized as 12 × 12 pixel patterns. The two-dimensional arrangements and colors are for visualization only. (<bold>e</bold>) One hundred receptive fields, each defined as the set of feedforward weights, are represented in a two-dimensional organization. After pretraining with patterns from MNIST digits 3 and 4, 79 DGCs have receptive fields corresponding to threes and fours of different writing styles, while 21 remain unselective (highlighted by red frames).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Dentate gyrus network.</title><p>(<bold>a</bold>) The dentate gyrus circuitry is complex, with two main types of excitatory cells: dentate granule cells (DGCs, the principal cells) and Mossy cells, as well as many different types of inhibitory cells, including somatostatin-positive (SST+) cells (magenta), cells expressing cholecystokinin (CCK) and vasoactive intestinal polypeptide (VIP) (orange), and parvalbumin-positive (PV+) cells (green). This tentative schematic of the main aspects of known circuitry neglects the anatomical location of the cells (e.g. granular layer, molecular layer, hilus) and simplifies cell types and connections. For references, see Introduction of main text. HIL: hilar interneurons, HIPP: hilar-perforant-path-associated interneurons, EC: entorhinal cortex. (<bold>b</bold>) Simplification of the network that we use in our model implementation. EC input does not project to inhibitory neurons in our model, but it is known that it provides feedforward inhibition to DGCs through PV+ cells. We model this by normalizing the input patterns. Lateral inhibition in our model corresponds to the experimentally observed feedback inhibition from HIPP cells.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig1-figsupp1-v2.tif"/></fig></fig-group><p>Newborn DGCs receive no direct connections from mature DGCs (<xref ref-type="bibr" rid="bib32">Deshpande et al., 2013</xref>; <xref ref-type="bibr" rid="bib7">Alvarez et al., 2016</xref>) (yet see <xref ref-type="bibr" rid="bib92">Vivar et al., 2012</xref>), but are indirectly activated via interneurons (<xref ref-type="bibr" rid="bib7">Alvarez et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Heigele et al., 2016</xref>). At about 3 weeks after birth, the γ-aminobutyric acid (GABAergic) input from interneurons to adult-born DGCs switches from excitatory in the early phase to inhibitory in the late phase of maturation (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>; <xref ref-type="bibr" rid="bib31">Deng et al., 2010</xref>) (‘GABA-switch’, <xref ref-type="fig" rid="fig1">Figure 1a</xref>). Analogous to a similar transition during embryonic and early postnatal stages (<xref ref-type="bibr" rid="bib94">Wang and Kriegstein, 2011</xref>), the GABA-switch is caused by a change in the expression profile of chloride cotransporters. In the early phase of maturation, newborn cells express the <inline-formula><mml:math id="inf15"><mml:mrow><mml:msup><mml:mtext>Na</mml:mtext><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>-K</mml:mtext><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mtext>-</mml:mtext><mml:mo>⁢</mml:mo><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>Cl</mml:mtext><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> cotransporter NKCC1, which leads to a high intracellular chloride concentration. Hence, the GABA reversal potential is higher than the resting potential (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>; <xref ref-type="bibr" rid="bib47">Heigele et al., 2016</xref>), and GABAergic inputs lead to Cl<sup>−</sup> ions outflow through the <inline-formula><mml:math id="inf16"><mml:msub><mml:mtext>GABA</mml:mtext><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> ionic receptors, which results in depolarization of the newborn cell (<xref ref-type="bibr" rid="bib15">Ben-Ari, 2002</xref>; <xref ref-type="bibr" rid="bib73">Owens and Kriegstein, 2002</xref>). In the late phase of maturation, expression of the <inline-formula><mml:math id="inf17"><mml:mrow><mml:msup><mml:mtext>K</mml:mtext><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>-Cl</mml:mtext><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>-coupled cotransporter KCC2 kicks in, which lowers the intracellular chloride concentration of the newborn cell to levels similar to those of mature cells, leading to a hyperpolarization of the cell membrane due to Cl<sup>−</sup> inflow upon GABAergic stimulation (<xref ref-type="bibr" rid="bib15">Ben-Ari, 2002</xref>; <xref ref-type="bibr" rid="bib73">Owens and Kriegstein, 2002</xref>). The transition from depolarizing (excitatory) to hyperpolarizing (inhibitory) effects of GABA is referred to as the ‘GABA-switch’. It has been shown that GABAergic inputs are crucial for the integration of newborn DGCs into the preexisting circuit (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>; <xref ref-type="bibr" rid="bib21">Chancey et al., 2013</xref>; <xref ref-type="bibr" rid="bib7">Alvarez et al., 2016</xref>; <xref ref-type="bibr" rid="bib47">Heigele et al., 2016</xref>).</p><p>The mammalian dentate gyrus contains – just like hippocampus in general – a myriad of inhibitory cell types (<xref ref-type="bibr" rid="bib36">Freund and Buzsáki, 1996</xref>; <xref ref-type="bibr" rid="bib84">Somogyi and Klausberger, 2005</xref>; <xref ref-type="bibr" rid="bib57">Klausberger and Somogyi, 2008</xref>), including basket cells, chandelier cells, and hilar cells (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). Basket cells can be subdivided in two categories: some express cholecystokinin (CCK) and vasoactive intestinal polypeptide (VIP), while the others express parvalbumin (PV) and are fast-spiking (<xref ref-type="bibr" rid="bib36">Freund and Buzsáki, 1996</xref>; <xref ref-type="bibr" rid="bib8">Amaral et al., 2007</xref>). Chandelier cells also express PV (<xref ref-type="bibr" rid="bib36">Freund and Buzsáki, 1996</xref>). Overall, it has been estimated that PV is expressed in 15–21% of all dentate GABAergic cells (<xref ref-type="bibr" rid="bib36">Freund and Buzsáki, 1996</xref>) and in 20–25% of the GABAergic neurons in the granule cell layer (<xref ref-type="bibr" rid="bib50">Houser, 2007</xref>). Amongst the GABAergic hilar cells, 55% express somatostatin (SST) (<xref ref-type="bibr" rid="bib50">Houser, 2007</xref>) and somatostatin-positive interneurons (SST-INs) represent about 16% of the GABAergic neurons in the dentate gyrus as a whole (<xref ref-type="bibr" rid="bib36">Freund and Buzsáki, 1996</xref>). While axons of hilar interneurons stay in the hilus and provide perisomatic inhibition onto dentate GABAergic cells, axons of hilar-perforant-path-associated interneurons (HIPP) extend to the molecular layer and provide dendritic inhibition onto both DGCs and interneurons (<xref ref-type="bibr" rid="bib100">Yuan et al., 2017</xref>). HIPP axons generate lots of synaptic terminals and extend as far as 3.5 mm along the septotemporal axis of the dentate gyrus (<xref ref-type="bibr" rid="bib8">Amaral et al., 2007</xref>). PV-expressing interneurons (PV-INs) and SST-INs both target adult-born DGCs early (after 2–3 weeks) in their maturation (<xref ref-type="bibr" rid="bib44">Groisman et al., 2020</xref>). PV-INs provide both feedforward inhibition and feedback inhibition (also called lateral inhibition) to the DGCs (<xref ref-type="bibr" rid="bib44">Groisman et al., 2020</xref>). In general, SST-INs provide lateral, but not feedforward, inhibition onto DGCs (<xref ref-type="bibr" rid="bib85">Stefanelli et al., 2016</xref>; <xref ref-type="bibr" rid="bib44">Groisman et al., 2020</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p><p>Adult-born DGCs are preferentially reactivated by stimuli similar to the ones they experienced during their early phase of maturation, up to 3 weeks after cell birth (<xref ref-type="bibr" rid="bib87">Tashiro et al., 2007</xref>). Even though the amount of newly generated cells per month is rather low (3–6% of the total DGCs population [<xref ref-type="bibr" rid="bib91">van Praag et al., 1999</xref>; <xref ref-type="bibr" rid="bib17">Cameron and McKay, 2001</xref>]), adult-born DGCs are critical for behavioral pattern separation (<xref ref-type="bibr" rid="bib24">Clelland et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Sahay et al., 2011a</xref>; <xref ref-type="bibr" rid="bib53">Jessberger et al., 2009</xref>), in particular in tasks where similar stimuli or contexts have to be discriminated (<xref ref-type="bibr" rid="bib24">Clelland et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Sahay et al., 2011a</xref>). However, the functional role of adult-born DGCs is controversial (<xref ref-type="bibr" rid="bib79">Sahay et al., 2011b</xref>; <xref ref-type="bibr" rid="bib3">Aimone et al., 2011</xref>). One view is that newborn DGCs contribute to pattern separation through a modulatory role (<xref ref-type="bibr" rid="bib79">Sahay et al., 2011b</xref>). Another view suggests that newborn DGCs act as encoding units that become sensitive to features of the environment which they encounter during a critical window of maturation (<xref ref-type="bibr" rid="bib55">Kee et al., 2007</xref>; <xref ref-type="bibr" rid="bib87">Tashiro et al., 2007</xref>). Some authors have even challenged the role of newborn DGCs in pattern separation in the classical sense and have proposed a pattern integration effect instead (<xref ref-type="bibr" rid="bib3">Aimone et al., 2011</xref>), while others suggest a dynamical (<xref ref-type="bibr" rid="bib6">Aljadeff et al., 2015</xref>; <xref ref-type="bibr" rid="bib82">Shani-Narkiss et al., 2020</xref>) or forgetting (<xref ref-type="bibr" rid="bib4">Akers et al., 2014</xref>) role for newborn DGCs. Within that broader controversy, we ask two specific questions: First, why are GABAergic inputs crucial for the integration of newborn DGCs into the preexisting circuit? And second, why are newborn DGCs particularly important in tasks where similar stimuli or contexts have to be discriminated?</p><p>To address these questions, we present a model of how newborn DGCs integrate into the preexisting circuit. In contrast to earlier models where synaptic input connections onto newborn cells were assumed to be strong enough to drive them (<xref ref-type="bibr" rid="bib19">Chambers et al., 2004</xref>; <xref ref-type="bibr" rid="bib14">Becker, 2005</xref>; <xref ref-type="bibr" rid="bib26">Crick and Miranker, 2006</xref>; <xref ref-type="bibr" rid="bib97">Wiskott et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Chambers and Conroy, 2007</xref>; <xref ref-type="bibr" rid="bib2">Aimone et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Appleby and Wiskott, 2009</xref>; <xref ref-type="bibr" rid="bib95">Weisz and Argibay, 2009</xref>; <xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Finnegan and Becker, 2015</xref>; <xref ref-type="bibr" rid="bib30">DeCostanzo et al., 2018</xref>), our model uses an unsupervised biologically plausible Hebbian learning rule that makes synaptic connections between EC and newborn DGCs either disappear or grow from small values at birth to values that eventually enable feedforward input from EC to drive DGCs. Contrary to previous modeling studies, our plasticity model does not require an artificial renormalization of synaptic connection weights since model weights are naturally bounded by the synaptic plasticity rule. We show that learning with a biologically plausible plasticity rule is possible thanks to the GABA-switch, which has been overlooked in previous modeling studies. Specifically, the growth of synaptic weights from small values is supported in our model by the excitatory action of GABA, whereas, after the switch, specialization of newborn cells arises from competition between DGCs, triggered by the inhibitory action of GABA. Furthermore, our theory of adult-born DGCs integration yields a transparent explanation of why newborn cells favor pattern separation of similar stimuli, but do not impact pattern separation of distinct stimuli.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We model a small patch of cells within dentate gyrus as a recurrent network of 100 DGCs and 25 GABAergic interneurons, omitting the Mossy cells for the sake of simplicity (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The modeled interneurons correspond to SST-INs from the HIPP category, as they are the providers of feedback inhibition to DGCs through dendritic projections (<xref ref-type="bibr" rid="bib85">Stefanelli et al., 2016</xref>; <xref ref-type="bibr" rid="bib100">Yuan et al., 2017</xref>; <xref ref-type="bibr" rid="bib44">Groisman et al., 2020</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The activity of a DGC with index <inline-formula><mml:math id="inf18"><mml:mi>i</mml:mi></mml:math></inline-formula> and an interneuron with index <inline-formula><mml:math id="inf19"><mml:mi>k</mml:mi></mml:math></inline-formula> is described by their continuous firing rates <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf21"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:math></inline-formula>, respectively. Firing rates are modeled by neuronal frequency–current curves that vanish for weak input and increase if the total input into a neuron is larger than a firing threshold. Since newborn DGCs exhibit enhanced excitability early in maturation (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib62">Li et al., 2017</xref>), the firing threshold of model neurons increases during maturation from a lower to a higher value (Materials and methods). Connectivity in a localized patch of dentate neurons is high: DGCs densely project to GABAergic interneurons (<xref ref-type="bibr" rid="bib1">Acsády et al., 1998</xref>), and SST-INs heavily project to cells in their neighborhood (<xref ref-type="bibr" rid="bib8">Amaral et al., 2007</xref>). Hence, in the recurrent network model, each model DGC projects to, and receives input from, a given interneuron with probability 0.9. The exact percentage of GABAergic neurons (or SST-INs) in the dentate gyrus as a whole is not known, but has been estimated at about 10% and only a fraction of these are SST-INs (<xref ref-type="bibr" rid="bib36">Freund and Buzsáki, 1996</xref>). The number of inhibitory neurons in our model network might therefore seem too high. However, our results are robust to substantial changes in the number of inhibitory neurons (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>).</p><p>Each of the 100 model DGCs receives input from a set of 144 model EC cells (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). In the rat, the number of DGCs has been estimated to be about 10<sup>6</sup>, while the number of EC input cells is estimated to be about 2 · 10<sup>5</sup> (<xref ref-type="bibr" rid="bib9">Andersen et al., 2007</xref>), yielding an expansion factor from EC to dentate gyrus of about 5. Theoretical analysis suggests that the expansion of the number of neurons enhances decorrelation of the representation of input patterns (<xref ref-type="bibr" rid="bib68">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib5">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib69">Marr, 1971</xref>; <xref ref-type="bibr" rid="bib76">Rolls and Treves, 1998</xref>) and promotes pattern separation (<xref ref-type="bibr" rid="bib12">Babadi and Sompolinsky, 2014</xref>). Our standard network model does not reflect this expansion because we want to highlight the particular ability of adult neurogenesis in combination with the GABA-switch to decorrelate input patterns independently of specific choices of the network architecture. However, we show later that an enlarged network with an expansion from 144 model EC cells to 700 model DGCs (similar to the anatomical expansion factor) yields similar results.</p><p>At birth, a DGC with index <inline-formula><mml:math id="inf22"><mml:mi>i</mml:mi></mml:math></inline-formula> does not receive synaptic glutamatergic input yet. Hence, the connection from any model EC cell with index <inline-formula><mml:math id="inf23"><mml:mi>j</mml:mi></mml:math></inline-formula> is initialized at <inline-formula><mml:math id="inf24"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The growth or decay of the synaptic strength <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the connection from <inline-formula><mml:math id="inf26"><mml:mi>j</mml:mi></mml:math></inline-formula> to <inline-formula><mml:math id="inf27"><mml:mi>i</mml:mi></mml:math></inline-formula> is controlled by a Hebbian plasticity rule (<xref ref-type="fig" rid="fig1">Figure 1c</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>η</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>x</italic><sub><italic>j</italic></sub> is the firing rate of the presynaptic EC neuron, η (‘learning rate’) is the susceptibility of a cell to synaptic plasticity, and <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>,</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:math></inline-formula> are positive parameters (Materials and methods, <xref ref-type="table" rid="table1">Table 1</xref>). The first term on the right-hand side of <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> describes LTP whenever the presynaptic neuron is active (<inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula> and the postsynaptic firing <inline-formula><mml:math id="inf30"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is above a threshold θ; the second term on the right-hand side of <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> describes long-term depression (LTD) whenever the presynaptic neuron is active and the postsynaptic firing rate is positive but below the threshold θ; LTD stops if the synaptic weight is zero. Such a combination of LTP and LTD is consistent with experimental data (<xref ref-type="bibr" rid="bib11">Artola et al., 1990</xref>; <xref ref-type="bibr" rid="bib83">Sjöström et al., 2001</xref>) as shown in earlier rate-based (<xref ref-type="bibr" rid="bib16">Bienenstock et al., 1982</xref>) or spike-based (<xref ref-type="bibr" rid="bib75">Pfister and Gerstner, 2006</xref>) plasticity models. The third term on the right-hand side of <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> implements heterosynaptic plasticity (<xref ref-type="bibr" rid="bib23">Chistiakova et al., 2014</xref>; <xref ref-type="bibr" rid="bib101">Zenke and Gerstner, 2017</xref>): whenever strong presynaptic input arriving at synapses <inline-formula><mml:math id="inf31"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> drives the firing of postsynaptic neuron <inline-formula><mml:math id="inf32"><mml:mi>i</mml:mi></mml:math></inline-formula> at a rate above θ, the weight of a synapse <inline-formula><mml:math id="inf33"><mml:mi>j</mml:mi></mml:math></inline-formula> is downregulated if synapse <inline-formula><mml:math id="inf34"><mml:mi>j</mml:mi></mml:math></inline-formula> does not receive any input, while the weights of synapses <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math></inline-formula> are simultaneously increased due to the first term (<xref ref-type="bibr" rid="bib65">Lynch et al., 1977</xref>). Importantly, the threshold condition for the third term (postsynaptic rate above θ) is the same as that for induction of LTP in the first term so that if some synapses are potentiated, silent synapses are depressed. In the model, heterosynaptic interaction between synapses is induced since information about postsynaptic activity is shared across synapses. This could be achieved in biological neurons via backpropagating action potentials or similar depolarization of the postsynaptic membrane potential at several synaptic locations; alternatively, heterosynaptic crosstalk could be implemented by signaling molecules. Note that since our neuron model is a point neuron, all synapses are neighbors of each other. In our model, the ‘heterosynaptic’ term has a negative sign which ensures that the weights cannot grow without bounds (Materials and methods). In this sense, the third term has a ‘homeostatic’ function (<xref ref-type="bibr" rid="bib101">Zenke and Gerstner, 2017</xref>), yet acts on a time scale faster than experimentally observed homeostatic synaptic plasticity (<xref ref-type="bibr" rid="bib89">Turrigiano et al., 1998</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameters for the simulations.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th colspan="2">Biologically plausible network</th><th colspan="2">Simplified network</th></tr></thead><tbody><tr><td rowspan="2">Network</td><td><inline-formula><mml:math id="inf36"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>144</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf37"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf39"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td colspan="2"><inline-formula><mml:math id="inf40"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figures 1</xref>–<xref ref-type="fig" rid="fig4">4</xref>)</td><td/><td/></tr><tr><td/><td colspan="2"><inline-formula><mml:math id="inf41"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>700</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>–<xref ref-type="fig" rid="fig4s2">2</xref>)</td><td/><td/></tr><tr><td rowspan="2">Connectivity</td><td><inline-formula><mml:math id="inf42"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf44"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula></td><td/></tr><tr><td><inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf46"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula></td><td/><td/></tr><tr><td rowspan="2">Dynamics</td><td><inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ms</td><td><inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mtext>inh</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ms</td><td><inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> ms</td><td/></tr><tr><td><inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf51"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula></td><td/><td/></tr><tr><td rowspan="3">Plasticity</td><td><inline-formula><mml:math id="inf52"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf53"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf54"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.03</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf55"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td><inline-formula><mml:math id="inf56"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf57"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf58"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>1.65</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.15</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td><inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>9.85</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula></td><td/></tr><tr><td rowspan="2">Numerical simulations</td><td><inline-formula><mml:math id="inf63"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> ms</td><td><inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf65"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> ms</td><td><inline-formula><mml:math id="inf66"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula></td></tr><tr><td><inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula></td><td/><td/><td/></tr></tbody></table></table-wrap><p>We ask whether such a biologically plausible plasticity rule enables adult-born DGCs to be integrated in an existing network of mature cells. To address this question, we exploit two observations (<xref ref-type="fig" rid="fig1">Figure 1a</xref>): first, the effect of interneurons onto newborn DGCs exhibits a GABA-switch from excitatory to inhibitory after about three weeks of maturation (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>; <xref ref-type="bibr" rid="bib31">Deng et al., 2010</xref>) and, second, newborn DGCs receive input from interneurons early in their maturation (before the third week), but project back to interneurons only later (<xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>). For simplicity, no plasticity rule was implemented within the dentate gyrus: connections between newborn DGCs and inhibitory cells are either absent or present with a fixed value (see below). However, before integration of adult-born DGCs can be addressed, an adult-stage network where mature cells already store some memories has to be constructed.</p><sec id="s2-1"><title>Mature neurons represent prototypical input patterns</title><p>In an adult-stage network, some mature cells already have a functional role. Hence, we start with a network that already has strong random EC-to-DGC connection weights (Materials and methods). We then pretrain our network of 100 DGCs using the same learning rule (<xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>, with identical learning rate η for all DGCs) that we will use later for the integration of newborn cells. For the stimulation of EC cells, we apply patterns representing thousands of handwritten digits in different writing styles from MNIST, a standard data set in artificial intelligence (<xref ref-type="bibr" rid="bib59">Lecun et al., 1998</xref>). Even though we do not expect EC neurons to show a two-dimensional arrangement, the use of two-dimensional patterns provides a simple way to visualize the activity of all 144 EC neurons in our model (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). We implicitly model feedforward inhibition from PV-INs (<xref ref-type="bibr" rid="bib44">Groisman et al., 2020</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>) by normalizing input patterns so that all inputs have the same amplitude (Materials and methods). Below, we present results for a representative combination of three digits (digits 3, 4, and 5), but other combinations of digits have also been tested (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><p>After pretraining with patterns from digits 3 and 4 in a variety of writing styles, we examine the receptive field of each DGC. Each receptive field, consisting of the connections from all 144 EC neurons onto one DGC, is characterized by its spatial structure (i.e. the pattern of connection weights) and its total strength (i.e. the efficiency of the optimal stimulus to drive the cell). We observe that out of the 100 DGCs, some have developed spatial receptive fields that correspond to different writing styles of digit 3, others receptive fields that correspond to variants of digit 4 (<xref ref-type="fig" rid="fig1">Figure 1e</xref>).</p><p>Behavioral discrimination has been shown to be correlated with classification accuracy based on DGC population activity (<xref ref-type="bibr" rid="bib98">Woods et al., 2020</xref>). Hence, to quantify the representation quality, we compute classification performance by a linear classifier that is driven by the activity of our 100 DGC model cells (Materials and methods). At the end of pretraining, the classification performance for patterns of digits 3 and 4 from a distinct test set not used during pretraining is high: 99.25% (classification performance on digit 3: 98.71%; digit 4: 99.80%), indicating that nearly all input patterns of the two digits are well represented by the network of mature DGCs. The median classification performance for 10 random combinations of two groups of pretrained digits is 98.54%, the 25th percentile 97.26%, and the 75th percentile 99.5% (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p><p>A detailed mathematical analysis (Materials and methods) shows that heterosynaptic plasticity in <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> ensures that the total strength of the receptive field of each selective DGC converges to a stable value which is similar for selective DGCs confirming the homeostatic function of heterosynaptic plasticity (<xref ref-type="bibr" rid="bib101">Zenke and Gerstner, 2017</xref>). As a consequence, synaptic weights are intrinsically bounded without the need to impose hard bounds on the weight dynamics. Moreover, we find that the spatial structure of the receptive field represents the weighted average of all those input patterns for which that DGC is responsive. The mathematical analysis also shows that those DGCs that do not develop selectivity have weak synaptic connections and a very low total strength of the receptive field.</p><p>After convergence of synaptic weights during pretraining, selective DGCs are considered mature cells. Mature cells are less plastic than newborn cells (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib41">Ge et al., 2007</xref>). So in the following, unless specified otherwise, we set <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> for mature cells (feedforward connection weights from EC to mature cells remain therefore fixed). A scenario where mature cells retain synaptic plasticity is also investigated (see Robustness of the model and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). Some DGCs did not develop any strong weight patterns during pretraining and exhibit unselective receptive fields (highlighted in red in <xref ref-type="fig" rid="fig1">Figure 1e</xref>). We classify these as unresponsive units.</p></sec><sec id="s2-2"><title>Newborn neurons become selective for novel patterns during maturation</title><p>In our main neurogenesis model, we replace unresponsive model units by plastic newborn DGCs (<inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>η</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>), which receive lateral GABAergic input but do not receive feedforward input yet (all weights from EC are set to zero). The replacement of unresponsive neurons reflects the fact that unresponsive units have weak synaptic connections and, experimentally, a lack of NMDA receptor activation has been shown to be deleterious for the survival of newborn DGCs (<xref ref-type="bibr" rid="bib86">Tashiro et al., 2006</xref>). To mimic exposure of an animal to a novel set of stimuli, we now add input patterns from digit 5 to the set of presented stimuli, which was previously limited to patterns of digits 3 and 4. The novel patterns from digit 5 are randomly interspersed into the sequence of patterns from digits 3 and 4; in other words, the presentation sequence was not optimized with a specific goal in mind.</p><p>We postulate that functional integration of newborn DGCs requires the two-step maturation process caused by the GABA-switch from excitation to inhibition. Since excitatory GABAergic input potentially increases correlated activity within the dentate gyrus network, we predict that newborn DGCs respond to familiar stimuli during the early phase of maturation, but not during the late phase, when inhibitory GABAergic input leads to competition.</p><p>To test this hypothesis, our model newborn DGCs go through two maturation phases (Materials and methods). The early phase of maturation is cooperative because, for each pattern presentation, activated mature DGCs indirectly excite the newborn DGCs via GABAergic interneurons. We assume that in natural settings, the activation of <inline-formula><mml:math id="inf70"><mml:msub><mml:mtext>GABA</mml:mtext><mml:mi>A</mml:mi></mml:msub></mml:math></inline-formula> receptors is low enough that the mean membrane potential remains below the chloride reversal potential at which shunting inhibition would be induced (<xref ref-type="bibr" rid="bib47">Heigele et al., 2016</xref>). In this regime, the net effect of synaptic activity is hence excitatory. This lateral activation of newborn DGCs drives the growth of their receptive fields in a direction similar to those of the currently active mature DGCs. Consistent with our hypothesis we find that, at the end of the early phase of maturation, newborn DGCs show a receptive field corresponding to a mixture of several input patterns (<xref ref-type="fig" rid="fig2">Figure 2a</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Newborn DGCs become selective for novel patterns during maturation.</title><p>(<bold>a</bold>) Unselective neurons are replaced by newborn DGCs, which learn their feedforward weights while patterns from digits 3, 4, and 5 are presented. At the end of the early phase of maturation, the receptive fields of all newborn DGCs (red frames) show mixed selectivity. (<bold>b</bold>) At the end of the late phase of maturation, newborn DGCs are selective for patterns from the novel digit 5, with different writing styles. (<bold>c, d</bold>) Distribution of the percentage of model DGCs (mean with 10th and 90th percentiles) in each firing rate bin at the end of the early (<bold>c</bold>) and late (<bold>d</bold>) phase of maturation. Statistics calculated across MNIST patterns (‘3’s, ‘4’s, ‘5’s). Percentages are per subpopulation (mature and newborn). Note that neurons with firing rate &lt; 1 Hz for one pattern may fire at medium or high rate for another pattern. (<bold>e</bold>) The L2-norm of the feedforward weight vector onto newborn DGCs (mean ± SEM) increases as a function of maturation indicating growth of synapses and receptive field strength. Horizontal axis: time = 1 indicates end of early (top) or late (bottom) phase (two epochs per phase, <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.0005</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>f</bold>) Percentage of newborn DGCs activated (firing rate &gt; 1 Hz) by a stimulus averaged over test patterns of digits 3, 4, and 5 as a function of maturation. (<bold>g</bold>) At the end of the late phase of maturation, three different patterns of digit 5 applied to EC neurons (top) cause different firing rate patterns of the 100 DGCs arranged in a matrix of 10-by-10 cells (middle). DGCs with a receptive field (see <bold>b</bold>) similar to a presented EC activation pattern respond more strongly than the others. Bottom: Firing rates of the DGCs with indices sorted from highest to lowest firing rate in response to the first pattern. All three patterns shown come from the testing set, and are correctly classified using our readout network.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Activity of 100 model DGCs in response to different patterns.</title><p>At the end of pretraining, three different patterns of digit 4 applied to EC neurons (top) cause different firing rate patterns of the 100 DGCs arranged in a matrix of 10-by-10 cells (middle). DGCs with a receptive field (left: 10-by-10 grid of receptive fields) similar to a presented EC activation pattern respond more strongly than the others. Bottom: Firing rates of the DGCs with indices sorted from highest to lowest firing rate in response to the first pattern. All three patterns shown come from the testing set and are correctly classified using our readout network.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Receptive fields of DGCs.</title><p>(<bold>a</bold>) Several novel digits can be learned simultaneously. After pretraining as in <xref ref-type="fig" rid="fig1">Figure 1e</xref>, unresponsive neurons are replaced by newborn DGCs. When patterns from digits 3, 4, 5, and 6 are presented in random order, newborn DGCs exhibit after maturation receptive fields with selectivity for the novel digits 5 and 6. (<bold>b</bold>) Several novel digits can be learned sequentially. After pretraining with digits 3 and 4, 10 randomly selected unresponsive neurons are replaced by newborn DGCs. Patterns from digits 3, 4, and 5 are presented in random order, while newborn DGCs mature and develop selectivity for the novel digit 5, with different writing styles. Later, the eleven remaining unresponsive neurons of the network are replaced by newborn DGCs. When patterns from the novel digit 6 are presented intermingled with patterns from digits 3, 4, and 5, the newborn DGCs develop selectivity for digit 6.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig2-figsupp2-v2.tif"/></fig></fig-group><p>In the late phase of maturation, model newborn DGCs receive inhibitory GABAergic input from interneurons, similar to the input received by mature DGCs. Given that at the end of the early phase, newborn DGCs have receptive fields similar to those of mature DGCs, lateral inhibition induces competition with mature DGCs for activation during presentation of patterns from the novel digit. Because model newborn DGCs start their late phase of maturation with a higher excitability (lower threshold) compared to mature DGCs, consistent with observed enhanced excitability of newborn cells (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib62">Li et al., 2017</xref>), the activation of newborn DGCs is facilitated for those input patterns for which no mature DGC has preexisting selectivity. Therefore, in the late phase of maturation, competition drives the synaptic weights of most newborn DGCs toward receptive fields corresponding to different subcategories of the ensemble of input patterns of the novel digit 5 (<xref ref-type="fig" rid="fig2">Figure 2b</xref>).</p><p>The total strength of the receptive field of a given DGC can be characterized by the sum of the squared synaptic weights of all feedfoward projections onto the cell (i.e. the square of the L2-norm). During maturation, the L2-norm of the feedforward weights onto newborn DGCs increases (<xref ref-type="fig" rid="fig2">Figure 2e</xref>) indicating an increase in total glutamatergic innervation, e.g., through an increase in the number and size of spines (<xref ref-type="bibr" rid="bib102">Zhao et al., 2006</xref>). Nevertheless, the distribution of firing rates of newborn DGCs is shifted to lower values at the end of the late phase compared to the end of the early phase of maturation (<xref ref-type="fig" rid="fig2">Figure 2c,d</xref>), consistent with in vivo calcium imaging recordings showing that newborn DGCs are more active than mature DGCs (<xref ref-type="bibr" rid="bib27">Danielson et al., 2016</xref>).</p><p>We emphasize that upon presentation of a pattern of a given digit, only those DGCs with a receptive field similar to the specific writing style of the presented pattern become strongly active, others fire at a medium firing rate, yet others at a low rate (<xref ref-type="fig" rid="fig2">Figure 2g</xref>). As a consequence, the firing rate of a particular newborn DGC at the end of its maturation to a pattern from digit 5 is strongly modulated by the specific choice of stimulation pattern within the class of ‘5’s. Analogous results are obtained for patterns from pretrained digits 3 and 4 (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Hence, the ensemble of DGCs is effectively performing pattern separation <italic>within</italic> each digit class as opposed to a simple ternary classification task. The selectivity of newborn DGCs develops during maturation. Indeed, during the late, competitive, phase, the percentage of active newborn DGCs decreases, both upon presentation of familiar patterns (digits 3 and 4), as well as upon presentation of novel patterns (digit 5) (<xref ref-type="fig" rid="fig2">Figure 2f</xref>). This reflects the development of the selectivity of our model newborn DGCs from broad to narrow tuning, consistent with experimental observations (<xref ref-type="bibr" rid="bib67">Marín-Burgin et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Danielson et al., 2016</xref>).</p><p>If two novel ensembles of digits (instead of a single one) are introduced during maturation of newborn DGCs, we observe that some newborn DGCs become selective for one of the novel digits, while others become selective for the other novel digit (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). This was expected, since we have found earlier that DGCs are becoming selective for different prototype writing styles even <italic>within</italic> a digit category; hence introducing several additional digit categories of novel patterns simply increases the prototype diversity. Therefore, newborn DGCs can ultimately promote separation of several novel overarching categories of patterns, no matter if they are learned simultaneously or sequentially (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>).</p></sec><sec id="s2-3"><title>Adult-born neurons promote better discrimination</title><p>As above, we compute classification performance of our model network as a surrogate for behavioral discrimination (<xref ref-type="bibr" rid="bib98">Woods et al., 2020</xref>). At the end of the late phase of maturation of newborn DGCs, we obtain an overall classification performance of 94.56% for the three ensembles of digits (classification performance for digit 3: 90.50%; digit 4: 98.17%; digit 5: 95.18%). Confusion matrices show that although novel patterns are not well classified at the end of the early phase of maturation (<xref ref-type="fig" rid="fig3">Figure 3e</xref>), they are as well classified as pretrained patterns at the end of the late phase of maturation (<xref ref-type="fig" rid="fig3">Figure 3f</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The GABA-switch guides learning of novel representations.</title><p>(<bold>a</bold>) Pretraining on digits 3, 4, and 5 simultaneously without neurogenesis (control 1). Patterns from digits 3, 4, and 5 are presented to the network while all DGCs learn their feedforward weights. After pretraining, 79 DGCs have receptive fields corresponding to the three learned digits, while 21 remain unselective (as in <xref ref-type="fig" rid="fig1">Figure 1e</xref>). (<bold>b</bold>) Sequential training without neurogenesis (control 2). After pretraining as in <xref ref-type="fig" rid="fig1">Figure 1e</xref>, the unresponsive neurons stay plastic, but they fail to become selective for digit 5 when patterns from digits 3, 4, and 5 are presented in random order. (<bold>c</bold>) Sequential training without neurogenesis but all DGCs stay plastic (control 3). Some of the DGCs previously responding to patterns from digits 3 or 4 become selective for digit 5. (<bold>d–f</bold>) Confusion matrices. Classification performance in percent (using a linear classifier as readout network) for control 1 (<bold>d</bold>) and for the main model at the end of the early (<bold>e</bold>) and late (<bold>f</bold>) phase; <xref ref-type="fig" rid="fig2">Figure 2a,b</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig3-v2.tif"/></fig><p>We compare this performance with that of a network where all three digit ensembles are directly simultaneously pretrained starting from random weights (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, control 1). In this case, the overall classification performance is 92.09% (classification performance for digit 3: 86.83%; digit 4: 98.78%; digit 5: 90.70%). The confusion matrix shows that all three digits are decently classified, but with an overall lower performance (<xref ref-type="fig" rid="fig3">Figure 3d</xref>). Across 10 simulation experiments, classification performance is significantly higher when a novel ensemble of patterns is learned sequentially by newborn DGCs (<italic>P</italic><sub>2</sub>; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>), than if all patterns are learned simultaneously (<italic>P</italic><sub>1</sub>; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). Indeed, the distribution of <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> for the 10 simulation experiments has a mean which is significantly different from zero (Wilcoxon signed rank test: p-val = 0.0020, Wilcoxon signed rank = 55; one-way t-test: p-val = 0.0269, t-stat = 2.6401, df = 9; <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec><sec id="s2-4"><title>The GABA-switch guides learning of novel representations</title><p>To assess whether maturation of newborn DGCs promotes learning of a novel ensemble of digit patterns, we compare our results with two control models without neurogenesis (controls 2 and 3).</p><p>In control 2, similar to the neurogenesis case, the feedforward weights and thresholds of mature DGCs are fixed (learning rate <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) after pretraining with patterns from digits 3 and 4, while the thresholds and weights of all unresponsive neurons remain plastic (<inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>η</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) upon introduction of patterns from the novel digit 5. The only differences to the model with neurogenesis are that unresponsive neurons: (1) keep their feedforward weights (i.e. no reinitialization to zero values) and (2) keep the same connections from and to inhibitory neurons. In this case, we find that the previously unresponsive DGCs do not become selective for the novel digit 5, no matter during how many epochs patterns are presented (we went up to 100 epochs) (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, control 2). Therefore, if patterns from digit 5 are presented to the network, the model fails to discriminate them from the previously learned digits 3 and 4: the overall classification performance is 81.69% (classification performance for digit 3: 85.94%; digit 4: 97.56%; digit 5: 59.42%). This result suggests that integration of newborn DGCs is beneficial for sequential learning of novel patterns.</p><p>In control 3, all DGCs keep plastic feedforward weights (learning rate <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>η</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) after pretraining and introduction of the novel digit 5, no matter if they became selective or not for the pretrained digits 3 and 4. We observe that in the case where all neurons are plastic, learning of the novel digit induces a change in selectivity of mature neurons. Several DGCs switch their selectivity to become sensitive to the novel digit (<xref ref-type="fig" rid="fig3">Figure 3c</xref>), while none of the previously unresponsive units becomes selective for presented patterns (compare with <xref ref-type="fig" rid="fig1">Figure 1e</xref>). In contrast to the model with neurogenesis, we observe a drop in classification performance to 90.92% (classification performance for digit 3: 85.45%; digit 4: 98.37%; digit 5: 88.90%). We find that the classification performance for digit 3 is the one which decreases the most. This is due to the fact that many DGCs previously selective for digit 3 modified their weights to become selective for digit 5. Importantly, the more novel patterns are introduced, the more overwriting of previously stored memories occurs. Hence, if all DGCs remain plastic, discrimination between a novel pattern and a familiar pattern stored long ago is impaired.</p></sec><sec id="s2-5"><title>Maturation of newborn neurons shapes the representation of novel patterns</title><p>Since each input pattern stimulates slightly different, yet overlapping, subsets of the 100 model DGCs in a sparse code such that about 20 DGCs respond to each pattern (<xref ref-type="fig" rid="fig2">Figure 2g</xref>), there is no simple one-to-one assignment between neurons and patterns. In order to visualize the activity patterns of the ensemble of DGCs, we perform dimensionality reduction. We construct a two-dimensional space using the activity patterns of the network at the end of the late phase of maturation of newborn DGCs trained with ‘3’s, ‘4’s and ‘5’s. One axis connects the center of mass (in the 100-dimensional activity space) of all DGC responses to ‘3’s with all responses to ‘5’s (arbitrarily called ‘axis 1’) and the other axis those from ‘4’s to ‘5’s (arbitrarily called ‘axis 2’). We then project the activity of the 100 model DGCs upon presentation of MNIST testing patterns onto those two axes, both at the end of the early and late phase of maturation of newborn DGCs (Materials and methods). Each two-dimensional projection is illustrated by a dot whose color corresponds to the digit class of the presented input pattern (blue for digit 3, green for digit 4, red for digit 5). Different input patterns within the same digit class cause different activation patterns of the DGCs, as depicted by extended clouds of dots of the same color (<xref ref-type="fig" rid="fig4">Figure 4a,b</xref>). Interestingly, an example pattern of a ‘5’ that is visually similar to a ‘4’ (characterized by the green cross) yields a DGC representation that lies closer to other ‘4’s (green cloud of dots) than to typical ‘5’s (red cloud of dots) (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). Noteworthy the separation of the representation of ‘5’s from ‘3’s and ‘4’s is better at end of the late phase (<xref ref-type="fig" rid="fig4">Figure 4b</xref>) when compared to the end of the early phase of maturation (<xref ref-type="fig" rid="fig4">Figure 4a</xref>). For instance, even though the pattern ‘5’ corresponding to the orange cross is represented close to representations of ‘4’s at the end of the early phase of maturation (green cloud of dots, <xref ref-type="fig" rid="fig4">Figure 4a</xref>), it is represented far from any ‘3’s and ‘4’s at the end of maturation (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). The expansion of the representation of ‘5’s into a previously empty subspace evolves as a function of time during the late phase of maturation (<xref ref-type="fig" rid="fig4">Figure 4d</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Novel patterns expand the representation into a previously empty subspace.</title><p>(<bold>a</bold>) Left: The DGC activity responses at the end of the early phase of maturation of newborn DGCs are projected on discriminatory axes. Each point corresponds to the representation of one input pattern. Color indicates digit 3 (blue), 4 (green), and 5 (red). Right: Firing rate profiles of three example patterns (highlighted by crosses on the left) are sorted from high to low for the pattern represented by the orange cross (inset: zoom of firing rates of DGCs with low activity). (<bold>b</bold>) Same as (a), but at the end of the late phase of maturation of newborn DGCs. Note that the red dots around the orange cross have moved into a different subspace. (<bold>c</bold>) Example patterns of digit 5 corresponding to the symbols in (<bold>a</bold>) and (<bold>b</bold>). All three are accurately classified by our readout network. (<bold>d</bold>) Evolution of the mean (± SEM) of the projection of the activity upon presentation of all test patterns of digit 5.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Receptive fields of the DGCs in a larger network with <inline-formula><mml:math id="inf76"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>G</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">700</mml:mn></mml:mrow></mml:math></inline-formula> (all other parameters unchanged).</title><p>After pretraining with digits 3 and 4, all 275 unresponsive DGCs (highlighted by the red/magenta squares) are replaced by newborn DGCs. Newborn DGCs follow a two-step maturation process while patterns from digits 3, 4, and 5 are presented to the network. At the end of maturation, most newborn DGCs represent different prototypes of the novel digit 5. Two of them (highlighted in magenta) became selective for digit 4.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Receptive fields of the DGCs in a larger network with <inline-formula><mml:math id="inf77"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>G</mml:mi><mml:mo mathvariant="bold">⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo mathvariant="normal">=</mml:mo><mml:mn mathvariant="normal">700</mml:mn></mml:mrow></mml:math></inline-formula> (all other parameters unchanged), when only a fraction of unresponsive units are replaced by newborn DGCs.</title><p>Out of the 275 unresponsive DGCs after pretraining with digits 3 and 4 (highlighted by the red/magenta squares), 119 are replaced by newborn DGCs (highlighted by the magenta squares), to mimic the experimental observation that only a fraction of DGCs are newborn DGCs. Newborn DGCs follow a two-step maturation process while patterns from digits 3, 4 and 5 are presented to the network. At the end of maturation, newborn DGCs represent different prototypes or features of the novel digit 5. The remaining unresponsive units (highlighted by the red squares) are available to be replaced later by newborn DGCs so as to learn further tasks.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig4-figsupp2-v2.tif"/></fig></fig-group></sec><sec id="s2-6"><title>Robustness of the model</title><p>Our results are robust to changes in network architecture. As mentioned earlier, neither the exact number of GABAergic neurons (<xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>), nor that of DGCs is critical. Indeed, a larger network with 700 DGCs, thus mimicking the anatomically observed expansion factor of about 5 between EC and dentate gyrus (all other parameters unchanged), yields similar results (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>).</p><p>In the network with 700 DGCs, 275 cells remain unresponsive after pretraining with digits 3 and 4. In line with our earlier approach in the network with 100 DGCs, we can algoritmically replace all unresponsive neurons with newborn DGCs before patterns of digit 5 are added. Upon maturation, newborn DGC receptive fields provide a detailed representation of the prototypes of the novel digit 5 (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) and good classification performance is obtained (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Interestingly, due to the randomness of the recurrent connections, some newborn DGCs become selective for particular prototypes of the familiar (pretrained) digits 3 and 4 that are not already extensively represented by the network (see newborn DGCs selective for digit 4 highlighted by magenta squares in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p>As an alternative to replacing all unresponsive cells simultaneously, we can also replace only a fraction of them by newborn cells so as to simulate a continuous turn-over of cells. For example, if 119 of the 275 unresponsive cells are replaced by newborn DGCs before the start of presentations of digit 5, then these 119 cells become selective for different writing styles and generic features of the novel digit 5 (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>) and allow a good classification performance of all three digits. On the other hand, replacing only 35 of the 275 unresponsive cells is not sufficient (<xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). In an even bigger network with more than 144 EC cells and more than 700 DGCs, we could choose to replace 1% of the total DGC population per week by newborn cells, consistent with biology (<xref ref-type="bibr" rid="bib91">van Praag et al., 1999</xref>; <xref ref-type="bibr" rid="bib17">Cameron and McKay, 2001</xref>). Importantly, if only a small fraction of unresponsive cells are replaced at a given moment, other unresponsive cells remain available to be replaced later by newborn DGCs that are then ready to learn new stimuli.</p><p>Interestingly, the timing of the introduction of the novel stimulus is important. In our main neurogenesis model with 100 DGCs, we introduce the novel digit 5 at the beginning of the early phase of maturation, which consists of one epoch of MNIST training patterns (all patterns are presented once). If the novel digit is only introduced in the middle of the early phase (half epoch), it cannot be properly learned (classification performance for digit 5: 46.52%). However, if introduced after three-eights or one-quarter of the early phase, the novel digit can be picked out (classification performance for digit 5: 93.61% and 94.17%, respectively). We thus observe an increase in performance the earlier the novel digit is introduced after cell birth (classification performance for digit 5 was 95.18% when introduced at the beginning of the early phase of maturation). Therefore, our model predicts that a novel stimulus has to be introduced early enough with respect to newborn DGC maturation to be well discriminated and that the accuracy of discrimination is better the earlier it is introduced.</p><p>This could lead to an online scenario of our model, where adult-born DGCs are produced every day and different classes of novel patterns are introduced at different timepoints. To understand whether newborn DGCs in their early and late phase of maturation would interfere, two aspects should be kept in mind. First, since model newborn DGCs in the early phase of maturation do not project to other neurons yet, they do not influence the circuit and thus do not affect maturation of other newborn DGCs. Second, since model newborn DGCs in the late phase of maturation project to GABAergic neurons in the dentate gyrus, they will, just like mature cells, indirectly activate newborn DGCs that are in their early phase of maturation. As a result, early phase newborn DGCs will develop receptive fields that represent an average of all the stimuli that excite the mature and late phase newborn DGCs, which indirectly activate them. The ultimate selectivity of newborn DGCs is determined after the GABA-switch, when competition sets in, which makes those cells that have recently switched most sensitive to aspects of the input patterns that are not yet well represented by other cells. Therefore, in an online scenario, different model newborn DGCs would become selective for different novel patterns according to both their maturation stage with respect to presentation of the novel patterns, and the selectivity of mature and late phase newborn DGCs which indirectly activate them.</p><p>Finally, in our neurogenesis model, we have set the learning rate of mature DGCs to zero despite the observation that mature DGCs retain some plasticity (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib41">Ge et al., 2007</xref>). We therefore studied a variant of the model in which mature DGCs also exhibit plasticity. First, we used our main model with 100 DGCs and 21 newborn DGCs. The implementation was identical, except that the learning rate of the mature DGCs was kept at a nonzero value during the maturation of the 21 newborn DGCs. We do not observe a large change in classification performance, even if the learning rate of the mature cells is the same as that of newborn cells (<xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). Second, we used our extended network with 700 DGCs to be able to investigate the effect of plastic mature DGCs while having a proportion of newborn cells matching experiments. We find that with 35 newborn DGCs (corresponding to the experimentally reported fraction of about 5%), plastic mature DGCs (with a learning rate half of that of newborn cells) improve classification performance (<xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). This is due to the fact that several of the mature DGCs (that were previously selective for ‘3’s or ‘4’s) become selective for prototypes of the novel digit 5. Consequently, more than the 35 newborn DGCs specialize for digit 5, so that digit 5 is eventually represented better by the network with mature cell plasticity than the standard network where plasticity is limited to newborn cells. Note that those mature DGCs that had earlier specialized on writing styles of digit 3 or 4 similar to a digit 5 are most likely to retune their selectivity. If the novel inputs were very distinct from the pretrained familiar inputs, mature DGCs would be unlikely to develop selectivity for the novel inputs.</p></sec><sec id="s2-7"><title>Newborn DGCs become selective for similar novel patterns</title><p>To investigate whether our theory for integration of newborn DGCs can explain why adult dentate gyrus neurogenesis promotes discrimination of similar stimuli, but does not affect discrimination of distinct patterns (<xref ref-type="bibr" rid="bib24">Clelland et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Sahay et al., 2011a</xref>), we use a simplified competitive winner-take-all network (Materials and methods). It contains only as many DGCs as trained clusters, and the GABAergic inhibitory neurons are implicitly modeled through direct DGC-to-DGC inhibitory connections. DGCs are either silent or active (binary activity state, while in the detailed network DGCs had continuous firing rates). The synaptic plasticity rule is however the same as for the detailed network, with different parameter values (Materials and methods). We also construct an artificial data set (<xref ref-type="fig" rid="fig5">Figure 5a,b</xref>) that allows us to control the similarity <inline-formula><mml:math id="inf78"><mml:mi>s</mml:mi></mml:math></inline-formula> of pairs of clusters (Materials and methods). The MNIST data set is not appropriate to distinguish similar from dissimilar patterns, because all digit clusters are similar and highly overlapping, reflected by a high within cluster dispersion (e.g. across the set of all ‘3’) compared to the separation between clusters (e.g. typical ‘3’ versus typical ‘5’).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>A newborn DGC becomes selective for similar but not distinct novel stimuli.</title><p>(<bold>a</bold>) Center of mass of clusters <inline-formula><mml:math id="inf79"><mml:mi>k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf80"><mml:mi>l</mml:mi></mml:math></inline-formula> of an artificial data set (<inline-formula><mml:math id="inf81"><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf82"><mml:msub><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula>, respectively, separated by angle Ω) are represented by arrows that point to the surface of a hypersphere. Dots represent individual patterns. (<bold>b</bold>) Center of mass of three clusters of the artificial data set, visualized as 16 × 8 pixel patterns. The two-dimensional arrangements and colors are for visualization only. (<bold>c, d</bold>) Example input patterns (activity of 16 × 8 input neurons) from clusters 1 and 2 for similar clusters (c, <inline-formula><mml:math id="inf83"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>), and distinct clusters (d, <inline-formula><mml:math id="inf84"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>). Below: dots correspond to patterns, crosses indicate the input patterns shown (schematic). (<bold>e, f</bold>) After pretraining with patterns from two clusters, the receptive fields (set of synaptic weights onto neurons 1 and 2) exhibit the center of mass of each cluster of input patterns (blue and green crosses). (<bold>g, h</bold>) Novel stimuli from cluster 3 (orange dots) are added. If the clusters are similar, the receptive field of the newborn DGC (red cross) moves toward the center of mass of the three clusters during its early phase of maturation (<bold>g</bold>), and if the clusters are distinct toward the center of mass of the two pretrained clusters (<bold>h</bold>). (<bold>i, j</bold>) Receptive field after the late phase of maturation for the case of similar (<bold>i</bold>) or distinct (<bold>j</bold>) clusters. (<bold>k, l</bold>) For comparison, the center of mass of all patterns of the blue and green clusters (left column) and of the blue, green, and orange clusters (right column) for the case of similar (<bold>k</bold>) or distinct (<bold>l</bold>) clusters. Color scale: input firing rate <inline-formula><mml:math id="inf85"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> or weight <inline-formula><mml:math id="inf86"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> normalized to <inline-formula><mml:math id="inf87"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>=</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig5-v2.tif"/></fig><p>After a pretraining period, a first mature DGC responds to patterns of cluster 1 and a second mature DGC to those of cluster 2 (<xref ref-type="fig" rid="fig5">Figure 5e,f</xref>). We then fix the feedforward weights of those two DGCs and introduce a newborn DGC in the network. Thereafter, we present patterns from three clusters (the two pretrained ones, as well as a novel one), while the plastic feedforward weights of the newborn DGC are the only ones that are updated. We observe that the newborn DGC ultimately becomes selective for the novel cluster if it is similar (<inline-formula><mml:math id="inf88"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>) to the two pretrained clusters (<xref ref-type="fig" rid="fig5">Figure 5i</xref>), but not if it is distinct (<inline-formula><mml:math id="inf89"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig5">Figure 5j</xref>). The selectivity develops in two phases. In the early phase of maturation of the newborn model cell, a pattern from the novel cluster that is similar to one of the pretrained clusters activates the mature DGC that has a receptive field closest to the novel pattern. The activated mature DGC drives the newborn DGC via lateral excitatory GABAergic connections to a firing rate where LTP is triggered at active synapses onto the newborn DGC. LTP also happens when a pattern from one of the pretrained clusters is presented. Thus, synaptic plasticity leads to a receptive field that reflects the average of all stimuli from all three clusters (<xref ref-type="fig" rid="fig5">Figure 5g</xref>).</p><p>To summarize our findings in a more mathematical language, we characterize the receptive field of the newborn cell by the vector of its feedforward weights. Analogous to the notion of a firing rate vector that represents the set of firing rates of an ensemble of neurons, the feedforward weight vector represents the set of weights of all synapses projecting onto a given neuron (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). In the early phase of maturation, for similar clusters, the feedforward weight vector onto the newborn DGC grows in the direction of the center of mass of all three clusters (the two pretrained ones and the novel one), because for each pattern presentation, be it a novel pattern or a familiar one, one of the mature DGCs becomes active and stimulates the newborn cell (compare <xref ref-type="fig" rid="fig5">Figure 5g</xref> and <xref ref-type="fig" rid="fig5">Figure 5k</xref>). However, if the novel cluster has a low similarity to pretrained clusters, patterns from the novel cluster do not activate any of the mature DGCs. Therefore, the receptive field of the newborn cell reflects the average of stimuli from the two pretrained clusters only (compare <xref ref-type="fig" rid="fig5">Figure 5h</xref> and <xref ref-type="fig" rid="fig5">Figure 5l</xref>).</p><p>As a result of the different orientation of the feedforward weight vector onto the newborn DGC at the end of the early phase of maturation, two different situations arise in the late phase of maturation, when lateral GABAergic connections are inhibitory. If the novel cluster is similar to the pretrained clusters, the weight vector onto the newborn DGC at the end of the early phase of maturation lies at the center of mass of all the patterns across the three clusters. Thus, it is closer to the novel cluster than the weight vector onto either of the mature DGCs (<xref ref-type="fig" rid="fig5">Figure 5g</xref>). So if a novel pattern is presented, the newborn DGC wins the competition between the three DGCs, and its feedforward weight vector moves toward the center of mass of the novel cluster (<xref ref-type="fig" rid="fig5">Figure 5i</xref>). By contrast, if the novel cluster is distinct, the weight vector onto the newborn DGC at the end of the early phase of maturation is located at the center of mass of the two pretrained clusters (<xref ref-type="fig" rid="fig5">Figure 5h</xref>). If a novel pattern is presented, no output unit is activated since their receptive fields are not similar enough to the input pattern. Therefore, the newborn DGC always stays silent and does not update its feedforward weights (<xref ref-type="fig" rid="fig5">Figure 5j</xref>). These results are consistent with studies that have suggested that dentate gyrus is only involved in the discrimination of similar stimuli, but not distinct stimuli (<xref ref-type="bibr" rid="bib43">Gilbert et al., 2001</xref>; <xref ref-type="bibr" rid="bib51">Hunsaker and Kesner, 2008</xref>). For discrimination of distinct stimuli, another pathway might be used, such as the direct EC to CA3 connection (<xref ref-type="bibr" rid="bib99">Yeckel and Berger, 1990</xref>; <xref ref-type="bibr" rid="bib38">Fyhn et al., 2007</xref>).</p><p>In conclusion, our model suggests that adult dentate gyrus neurogenesis promotes discrimination of similar patterns because newborn DGCs can ultimately become selective for novel stimuli, which are similar to already learned stimuli. On the other hand, newborn DGCs fail to represent novel distinct stimuli, precisely because they are too distinct from other stimuli already represented by the network. Presentation of novel distinct stimuli in the late phase of maturation therefore does not induce synaptic plasticity of the newborn DGC feedforward weight vector toward the novel stimuli. In the simplified network, the transition between similar and distinct can be determined analytically (Materials and methods). This analysis clarifies the importance of the switch from cooperative dynamics (excitatory interactions) in the early phase to competitive dynamics (inhibitory interactions) in the late phase of maturation.</p></sec><sec id="s2-8"><title>Upon successful integration the receptive field of a newborn DGC represents an average of novel stimuli</title><p>With the simplified model network, it is possible to analytically compute the maximal strength of the DGC receptive field via the L2-norm of the feedforward weight vector onto the newborn DGC (Materials and methods). In addition, the angle between the center of mass of the novel patterns and the feedforward weight vector onto the adult-born DGC can also be analytically computed (Materials and methods). To illustrate the analytical results and characterize the evolution of the receptive field of the newborn DGC, we thus examine the angle φ of the feedforward weight vector with the center of mass of the novel cluster (i.e. the average of the novel stimuli), as a function of maturation time (<xref ref-type="fig" rid="fig6">Figure 6b,c</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Maturation dynamics for similar patterns.</title><p>(<bold>a</bold>) Schematics of the unit hypersphere with three clusters of patterns (colored dots) and three scaled feedforward weight vectors (colored arrows). After pretraining, the blue and green weight vectors point to the center of mass of the corresponding clusters. Patterns from the novel cluster (orange points) are presented only later to the network. During the early phase of maturation, the newborn DGC grows its vector of feedforward weights (red arrow) in the direction of the subspace of patterns which indirectly activate the newborn cell (dark grey star: center of mass of the presented patterns, located below the part of the sphere surface highlighted in grey). (<bold>b</bold>) During the late phase of maturation, the red vector turns toward the novel cluster. The symbol φ indicates the angle between the center of mass of the novel cluster and the feedforward weight vector onto the newborn cell. (<bold>c</bold>) The angle φ decreases in the late phase of maturation of the newborn DGC if the novel cluster is similar to the previously stored clusters. Its final average value of <inline-formula><mml:math id="inf90"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>≈</mml:mo><mml:msup><mml:mn>0.4</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> is caused by the jitter of the weight vector around the center of mass of the novel cluster.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Evolution of the feedforward weight vector onto the newborn DGC.</title><p>(<bold>a–d</bold>) The total synaptic strength <inline-formula><mml:math id="inf91"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> of the weight vector onto the newborn DGC (top row), and of its angular separation φ with the center of mass of the novel cluster (bottom row), as a function of the number of pattern presentations. (<bold>a, b</bold>) The three clusters are similar (<inline-formula><mml:math id="inf92"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>c, d</bold>) The three clusters are distinct (<inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>). Early phase of maturation (<bold>a, c</bold>), late phase of maturation (<bold>b, d</bold>). The red line shows the mean value of the synaptic strength of the mature DGCs. (<bold>e, f</bold>) Schematic drawing for the analytical computations. The L2-norm of the weight vector <inline-formula><mml:math id="inf94"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> onto the newborn DGC at the end of the early phase of maturation, and its angle φ with the center of mass of the novel cluster, for (<bold>e</bold>) similar clusters (<inline-formula><mml:math id="inf95"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>) and (<bold>f</bold>) distinct clusters (<inline-formula><mml:math id="inf96"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>). The sphere has a radius <inline-formula><mml:math id="inf97"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula>. The centers of mass of the first two clusters (represented by the two mature DGCs) projected onto the hypersphere are represented by the blue and green dots. The red dot represents the projection of the center of mass of the novel cluster, <inline-formula><mml:math id="inf98"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula>, onto the hypersphere.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-66463-fig6-figsupp1-v2.tif"/></fig></fig-group><p>In the early phase of maturation, the feedforward weight vector onto the newborn DGC grows, while its angle with the center of mass of the novel cluster stays constant (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). In the late phase of maturation, the angle φ between the center of mass of the novel cluster and the feedforward weight vector onto the newborn DGC decreases in the case of similar patterns (<xref ref-type="fig" rid="fig6">Figure 6c</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), but not in the case of distinct patterns (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), indicating that the newborn DGC becomes selective for the novel cluster for similar but not for distinct patterns.</p><p>The analysis of the simplified model thus leads to a geometric picture that helps us to understand how the similarity of patterns influences the evolution of the receptive field of the newborn DGC before and after the switch from excitation to inhibition of the GABAergic input. For novel patterns that are similar to known patterns, the receptive field of a newborn DGC at the end of maturation represents the average of novel stimuli.</p></sec><sec id="s2-9"><title>The cooperative phase of maturation promotes pattern separation for any dimensionality of input data</title><p>Despite the fact that input patterns in our model represent the activity of 144 or 128 model EC cells, the effective dimensionality of the input data was significantly below 100 because the clusters for different input classes were rather concentrated around their respective center of mass. We define the effective input dimensionality as the participation ratio (<xref ref-type="bibr" rid="bib70">Mazzucato et al., 2016</xref>; <xref ref-type="bibr" rid="bib64">Litwin-Kumar et al., 2017</xref>) (Materials and methods). Using this definition, the input data of both the MNIST 12 × 12 patterns from digits 3, 4, and 5 and the seven clusters of the handmade dataset for similar patterns (<inline-formula><mml:math id="inf99"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>) are relatively low-dimensional (<inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>19</mml:mn></mml:mrow></mml:math></inline-formula> of a maximum of 144, and <inline-formula><mml:math id="inf101"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math></inline-formula> of a maximum of 128, respectively). We emphasize that in both cases the spread of the input data around the cluster center implies that the effective dimensionality is larger than the number of clusters. In natural settings, we expect the input data to have even higher dimension. Therefore, here we investigate the effect of dimensionality of the input data on our neurogenesis model by increasing the spread around the cluster centers.</p><p>We use our simplified network model and create similar artificial datasets (<inline-formula><mml:math id="inf102"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>) with different values for the concentration parameter κ (Materials and methods). The smaller the κ, the broader the distributions around their center of mass; hence, the larger the overlap of patterns generated from different cluster distributions. Therefore, we can increase the effective dimensionality of the input by decreasing the concentration parameter κ. First, as expected from our analytical analysis (Materials and methods), we find that the broader the cluster distributions the smaller the length of the feedforward weight vector onto newborn DGCs (from just below 1.5 with <inline-formula><mml:math id="inf103"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> to about 1.35 with <inline-formula><mml:math id="inf104"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>6</mml:mn><mml:mo>⋅</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>). Second, we examine the ability of the simplified network to discriminate input patterns coming from input spaces with different dimensionalities. To do so, we compare our neurogenesis model (Neuro.) with a random initialization model (RandInitL.). In both cases, two DGCs are pretrained with patterns from two clusters, as above. Then we fix the weights of the two mature DGCs and introduce patterns from a third cluster as well as a newborn DGC. For the neurogenesis case, after maturation of the newborn DGC we fix its weights (while for the random initialization model we keep them plastic) upon introduction of patterns from a fourth cluster as well as another newborn DGC, and so on until the network contains seven DGCs and patterns from the full dataset of seven clusters have been presented. We compare our neurogenesis model, where each newborn DGC starts with zero weights and undergo a two-phase maturation (one epoch per phase), with a random initialization model where each newborn DGC is directly fully integrated into the circuit and whose feedforward weight vector is randomly initialized with a length of 0.1 (RandInitL.) and is then learned for two epochs.</p><p>Since clusters can be highly overlapping, we assess discrimination performance by computing the reconstruction error at the end of training. Reconstruction error is evaluated analogously to classification error, except that the readout layer has the task of an autoencoder: it contains as many readout units as there are input units. Reconstruction error is the mean squared distance between the input vector and the reconstructed output vector based on testing patterns. We observe that for any dimensionality of the input space, even as high as 97-dimensional, the neurogenesis model performs better (has a lower total reconstruction error) than the random initialization model (<xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>). Indeed, in the neurogenesis case newborn DGCs grow their feedforward weights (from zero) in the direction of presented input patterns in their early cooperative phase of maturation and can later become selective for novel patterns during the competitive phase. In contrast, since the random initialization model has no early cooperative phase, the newborn DGC weight vector does not grow unless an input pattern is by chance well aligned with its randomly initialized weight vector (which is unlikely in a high-dimensional input space). We get similar results for a larger initialization of the synaptic weights (e.g. the length of the weight vector at birth is set to 1, results not shown). Importantly, in high input dimensions, the advantage of a larger weight vector length at birth in the random initialization model is overridden by the capability of newborn DGCs to grow their weight vector in the appropriate direction during their early cooperative phase of maturation. Finally, we note that even if the length of the feedforward weight vector onto newborn DGCs is set to 1.5 (RandInitH., <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>), which is the upper bound according to our analytical results (Materials and methods), the random initialization model performs worse than the neurogenesis model for low up to relatively high-dimensional input spaces (<inline-formula><mml:math id="inf105"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>83</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>) despite its advantage in the competition conferred by the longer weight vector. It is only when input clusters are extremely broad and overlapping that the random initialization model performs similarly to the neurogenesis model (<inline-formula><mml:math id="inf106"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>90</mml:mn><mml:mo>,</mml:mo><mml:mn>97</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>). In other words, a random initialization at full length of weight vectors works well if input data is homogeneously distributed on the positive quadrant of the unit sphere but fails if the input data is clustered in a few directions. Moreover, random initialization requires that synaptic weights are large from the start which is biologically not plausible. In summary, the two-phase neurogenesis model is advantageous because the feedforward weights onto newborn cells can start at arbitrarily small values; their growth is, during the cooperative phase, guided to occur in a direction that is relevant for the task at hand; the final competitive phase eventually enables specialization onto novel inputs.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>While experimental studies, such as manipulating the ratio of NKCC1 to KCC2, suggest that the switch from excitation to inhibition of the GABAergic input onto adult-born DGCs is crucial for their integration into the preexisting circuit (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>; <xref ref-type="bibr" rid="bib7">Alvarez et al., 2016</xref>) and that adult dentate gyrus neurogenesis promotes pattern separation (<xref ref-type="bibr" rid="bib24">Clelland et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Sahay et al., 2011a</xref>; <xref ref-type="bibr" rid="bib53">Jessberger et al., 2009</xref>), the link between channel properties and behavior has remained puzzling (<xref ref-type="bibr" rid="bib79">Sahay et al., 2011b</xref>; <xref ref-type="bibr" rid="bib3">Aimone et al., 2011</xref>). Our modeling work shows that the GABA-switch enables newborn DGCs to become selective for novel stimuli, which are similar to familiar, already-stored, representations, consistent with the experimentally observed function of pattern separation (<xref ref-type="bibr" rid="bib24">Clelland et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Sahay et al., 2011a</xref>; <xref ref-type="bibr" rid="bib53">Jessberger et al., 2009</xref>).</p><p>Previous modeling studies already suggested that newborn DGCs integrate novel inputs into the representation in dentate gyrus (<xref ref-type="bibr" rid="bib19">Chambers et al., 2004</xref>; <xref ref-type="bibr" rid="bib14">Becker, 2005</xref>; <xref ref-type="bibr" rid="bib26">Crick and Miranker, 2006</xref>; <xref ref-type="bibr" rid="bib97">Wiskott et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Chambers and Conroy, 2007</xref>; <xref ref-type="bibr" rid="bib10">Appleby and Wiskott, 2009</xref>; <xref ref-type="bibr" rid="bib2">Aimone et al., 2009</xref>; <xref ref-type="bibr" rid="bib95">Weisz and Argibay, 2009</xref>; <xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Finnegan and Becker, 2015</xref>; <xref ref-type="bibr" rid="bib30">DeCostanzo et al., 2018</xref>). However, our work differs from them in four important aspects. First of all, we implement an unsupervised biologically plausible plasticity rule, while many studies used supervised algorithmic learning rules (<xref ref-type="bibr" rid="bib19">Chambers et al., 2004</xref>; <xref ref-type="bibr" rid="bib14">Becker, 2005</xref>; <xref ref-type="bibr" rid="bib20">Chambers and Conroy, 2007</xref>; <xref ref-type="bibr" rid="bib95">Weisz and Argibay, 2009</xref>; <xref ref-type="bibr" rid="bib35">Finnegan and Becker, 2015</xref>; <xref ref-type="bibr" rid="bib30">DeCostanzo et al., 2018</xref>). Second, as we model the formerly neglected GABA-switch, the connection weights from EC to newborn DGCs are grown from small values through cooperativity in the early phase of maturation. This integration step was mostly bypassed in earlier models by initialization of the connectivity weights toward newborn DGCs to random, yet fully grown values (<xref ref-type="bibr" rid="bib26">Crick and Miranker, 2006</xref>; <xref ref-type="bibr" rid="bib2">Aimone et al., 2009</xref>; <xref ref-type="bibr" rid="bib95">Weisz and Argibay, 2009</xref>; <xref ref-type="bibr" rid="bib35">Finnegan and Becker, 2015</xref>). Third, as the dentate gyrus network is commonly modeled as a competitive network, weight normalization is crucial. In our framework, competition occurs during the late phase of maturation. Previous modeling works either applied algorithmic weight normalization or hard bounds on the weights at each iteration step (<xref ref-type="bibr" rid="bib26">Crick and Miranker, 2006</xref>; <xref ref-type="bibr" rid="bib2">Aimone et al., 2009</xref>; <xref ref-type="bibr" rid="bib95">Weisz and Argibay, 2009</xref>; <xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Finnegan and Becker, 2015</xref>). Instead, our plasticity rule includes heterosynaptic plasticity, which intrinsically softly bounds connectivity weights by a homeostatic effect. Finally, although some earlier computational models of adult dentate gyrus neurogenesis could explain the pattern separation abilities of newborn cells, separation was obtained independently of the similarity between the stimuli. Contrarily to experimental data, no distinction was made between similar and distinct patterns (<xref ref-type="bibr" rid="bib19">Chambers et al., 2004</xref>; <xref ref-type="bibr" rid="bib14">Becker, 2005</xref>; <xref ref-type="bibr" rid="bib26">Crick and Miranker, 2006</xref>; <xref ref-type="bibr" rid="bib97">Wiskott et al., 2006</xref>; <xref ref-type="bibr" rid="bib20">Chambers and Conroy, 2007</xref>; <xref ref-type="bibr" rid="bib2">Aimone et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Appleby and Wiskott, 2009</xref>; <xref ref-type="bibr" rid="bib96">Weisz and Argibay, 2012</xref>; <xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>; <xref ref-type="bibr" rid="bib35">Finnegan and Becker, 2015</xref>; <xref ref-type="bibr" rid="bib30">DeCostanzo et al., 2018</xref>). To our knowledge, we present the first model that can explain both (1) how adult-born DGCs integrate into the preexisting network and (2) why they promote pattern separation of similar stimuli and not distinct stimuli.</p><p>Our work emphasizes why a two-phase maturation of newborn DGCs is beneficial for proper integration in the preexisting network. From a computational perspective, the early phase of maturation, when GABAergic inputs onto newborn DGCs are excitatory, corresponds to cooperative unsupervised learning. Therefore, the synapses grow in the direction of patterns that indirectly activate the newborn DGCs via GABAergic interneurons (<xref ref-type="fig" rid="fig6">Figure 6a</xref>). At the end of the early phase of maturation, the receptive field of a newborn DGC represents the center of mass of all input patterns that led to its (indirect) activation. In the late phase of maturation, GABAergic inputs onto newborn DGCs become inhibitory, so that lateral interactions change from cooperation to competition, causing a shift of the receptive fields of the newborn DGCs toward novel features (<xref ref-type="fig" rid="fig6">Figure 6b</xref>). At the end of maturation, newborn DGCs are thus selective for novel inputs. This integration mechanism is in agreement with the experimental observation that newborn DGCs are broadly tuned early in maturation, yet highly selective at the end of maturation (<xref ref-type="bibr" rid="bib67">Marín-Burgin et al., 2012</xref>; <xref ref-type="bibr" rid="bib27">Danielson et al., 2016</xref>). Loosely speaking, the cooperative phase of excitatory GABAergic input promotes the growth of the synaptic weights coarsely in the relevant direction, whereas the competitive phase of inhibitory GABAergic input helps to specialize on detailed, but potentially important differences between patterns.</p><p>In the context of theories of unsupervised learning, the switch of lateral GABAergic input to newborn DGCs from excitatory to inhibitory provides a biological solution to the ‘problem of unresponsive units’ (<xref ref-type="bibr" rid="bib49">Hertz et al., 1991</xref>). Unsupervised competitive learning has been used to perform clustering of input patterns into a few categories (<xref ref-type="bibr" rid="bib77">Rumelhart and Zipser, 1985</xref>; <xref ref-type="bibr" rid="bib46">Grossberg, 1987</xref>; <xref ref-type="bibr" rid="bib58">Kohonen, 1989</xref>; <xref ref-type="bibr" rid="bib49">Hertz et al., 1991</xref>; <xref ref-type="bibr" rid="bib34">Du, 2010</xref>). Ideally, after learning of the feedforward weights between an input layer and a competitive network, input patterns that are distinct from each other activate different neuron assemblies of the competitive network. After convergence of competitive Hebbian learning, the vector of feedforward weights onto a given neuron points to the center of mass of the cluster of input patterns for which it is selective (<xref ref-type="bibr" rid="bib58">Kohonen, 1989</xref>; <xref ref-type="bibr" rid="bib49">Hertz et al., 1991</xref>). Yet, if the synaptic weights are randomly initialized, it is possible that the set of feedforward weights onto some neurons of the competitive network point in a direction ‘quasi-orthogonal’ (Materials and methods) to the subspace of the presented input patterns. Therefore, those neurons, called ‘unresponsive units’, will never get active during pattern presentation. Different learning strategies have been developed in the field of artificial neural networks to avoid this problem (<xref ref-type="bibr" rid="bib45">Grossberg, 1976</xref>; <xref ref-type="bibr" rid="bib16">Bienenstock et al., 1982</xref>; <xref ref-type="bibr" rid="bib77">Rumelhart and Zipser, 1985</xref>; <xref ref-type="bibr" rid="bib46">Grossberg, 1987</xref>; <xref ref-type="bibr" rid="bib33">DeSieno, 1988</xref>; <xref ref-type="bibr" rid="bib58">Kohonen, 1989</xref>; <xref ref-type="bibr" rid="bib49">Hertz et al., 1991</xref>; <xref ref-type="bibr" rid="bib34">Du, 2010</xref>). However, most of these algorithmic approaches lack a biological interpretation. In our model, weak synapses onto newborn DGCs form spontaneously after neuronal birth. The excitatory GABAergic input in the early phase of maturation drives the growth of the synaptic weights in the direction of the subspace of presented patterns that succeed in activating some of the mature DGCs. Hence, the early cooperative phase of maturation can be seen as a smart initialization of the synaptic weights onto newborn DGCs, close enough to novel patterns so as to become selective for them in the late competitive phase of maturation. However, the cooperative phase is helpful only if the novel patterns are similar to the input statistics defined by the set of known (familiar) patterns.</p><p>Our results are in line with the classic view that dentate gyrus is responsible for decorrelation of inputs (<xref ref-type="bibr" rid="bib68">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib5">Albus, 1971</xref>; <xref ref-type="bibr" rid="bib69">Marr, 1971</xref>; <xref ref-type="bibr" rid="bib76">Rolls and Treves, 1998</xref>), a necessary step for differential storage of similar memories in CA3, and with the observation that dentate gyrus lesions impair discrimination of similar but not distinct stimuli (<xref ref-type="bibr" rid="bib43">Gilbert et al., 2001</xref>; <xref ref-type="bibr" rid="bib51">Hunsaker and Kesner, 2008</xref>). To discriminate distinct stimuli, another pathway might be involved, such as the direct EC to CA3 connection (<xref ref-type="bibr" rid="bib99">Yeckel and Berger, 1990</xref>; <xref ref-type="bibr" rid="bib38">Fyhn et al., 2007</xref>).</p><p>The parallel of neurogenesis in dentate gyrus and olfactory bulb suggests that similar mechanisms could be at work in both areas. Yet, even though adult olfactory bulb neurogenesis seems to have a similar functional role to adult dentate gyrus neurogenesis (<xref ref-type="bibr" rid="bib79">Sahay et al., 2011b</xref>), follow a similar integration sequence and undergo a GABA-switch from excitatory to inhibitory, the circuits are different in several aspects. First, while newborn neurons in dentate gyrus are excitatory, newborn cells in the olfactory bulb are inhibitory. Second, the newborn olfactory cells start firing action potentials only once they are well integrated (<xref ref-type="bibr" rid="bib18">Carleton et al., 2003</xref>). Therefore, in view of a transfer of results to the olfactory bulb, it would be interesting to adjust our model of adult dentate gyrus neurogenesis accordingly. For example, a voltage-based synaptic plasticity rule could be used to account for subthreshold plasticity mechanisms (<xref ref-type="bibr" rid="bib25">Clopath et al., 2010</xref>).</p><p>Our model of transition from an early cooperative phase to a late competitive phase makes specific predictions, at the behavioral and cellular level. In our model, the early cooperative phase of maturation can only drive the growth of synaptic weights onto newborn cells if they are indirectly activated by mature DGCs through GABAergic input, which has an excitatory effect due to the high NKCC1/KCC2 ratio early in maturation. Therefore, our model predicts that NKCC1-knockout mice would be impaired in discriminating similar contexts or objects because newborn cells stay silent due to lack of indirect activation. The feedforward weight vector onto newborn DGCs could not grow in the early phase and newborn DGCs could not become selective for novel inputs. Therefore, our model predicts that since newborn DGCs are poorly integrated into the preexisting circuit, they are unlikely to survive. If, however, in the same paradigm newborn cells are activated by light-induced or electrical stimulation, we predict that they become selective to novel patterns. Thus discrimination abilities would be restored and newborn DGCs are likely to survive. Analogously, we predict that using inducible NKCC1-knockout mice, animals would gradually be impaired in discrimination tasks after induced knockout and reach a stable maximum impairment about 3 weeks after the start of induced knockout.</p><p>Experimental observations support the importance of the switch from early excitation to late inhibition of the GABAergic input onto newborn DGCs. An absence of early excitation using NKCC1-knockout mice has been shown to strongly affect synapse formation and dendritic development in vivo (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>). Conversely, a reduction in inhibition in the dentate gyrus through decrease in KCC2 expression has been associated with epileptic activity (<xref ref-type="bibr" rid="bib74">Pathak et al., 2007</xref>; <xref ref-type="bibr" rid="bib13">Barmashenko et al., 2011</xref>). An analogous switch of the GABAergic input has been observed during development, and its proper timing has been shown to be crucial for sensorimotor gating and cognition (<xref ref-type="bibr" rid="bib94">Wang and Kriegstein, 2011</xref>; <xref ref-type="bibr" rid="bib37">Furukawa et al., 2017</xref>). In addition to early excitation and late inhibition, our theory also critically depends on the time scale of the switching process. In our model, the switch makes an instantaneous transition between early and late phase of maturation. Several experimental results have suggested that the switch is indeed sharp and occurs within a single day, both during development (<xref ref-type="bibr" rid="bib56">Khazipov et al., 2004</xref>; <xref ref-type="bibr" rid="bib90">Tyzio et al., 2007</xref>; <xref ref-type="bibr" rid="bib60">Leonzino et al., 2016</xref>) and adult dentate gyrus neurogenesis (<xref ref-type="bibr" rid="bib47">Heigele et al., 2016</xref>). Furthermore, in hippocampal cell cultures, expression of KCC2 is upregulated by GABAergic activity but not affected by glutamatergic activity (<xref ref-type="bibr" rid="bib39">Ganguly et al., 2001</xref>). A similar process during adult dentate gyrus neurogenesis would increase the number of newborn DGCs available for representing novel features by advancing the timing of their switch. In this way, instead of a few thousands of newborn DGCs ready to switch (3–6% of the whole population [<xref ref-type="bibr" rid="bib91">van Praag et al., 1999</xref>; <xref ref-type="bibr" rid="bib17">Cameron and McKay, 2001</xref>], divided by 30 days), a larger fraction of newborn DGCs would be made available for coding, if appropriate stimulation occurs. Finally, while neurotransmitter switching has been observed following sustained stimulation for hours to days (<xref ref-type="bibr" rid="bib63">Li et al., 2020</xref>), it is still unclear if it has the same functional role as the GABA-switch in our model. In particular, it remains an open question if neurotransmitter switching promotes the integration of neurons in the same way as our model GABA-switch does in the context of adult dentate gyrus neurogenesis.</p><p>To conclude, our theory for integration of adult-born DGCs suggests that newborn cells have a coding – rather than a modulatory – role during dentate gyrus pattern separation function. Our theory highlights the importance of GABAergic input in adult dentate gyrus neurogenesis and links the switch from excitation to inhibition to the integration of newborn DGCs into the preexisting circuit. Finally, it illustrates how Hebbian plasticity of EC to DGC synapses along with the switch make newborn cells suitable to promote pattern separation of similar but not distinct stimuli, a long-standing mystery in the field of adult dentate gyrus neurogenesis (<xref ref-type="bibr" rid="bib79">Sahay et al., 2011b</xref>; <xref ref-type="bibr" rid="bib3">Aimone et al., 2011</xref>).</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Network architecture and neuronal dynamics</title><p>DGCs are the principal cells of the dentate gyrus. They mainly receive excitatory projections from the EC through the perforant path and GABAergic inputs from local interneurons, as well as excitatory input from Mossy cells. They project to CA3 pyramidal cells and inhibitory neurons, as well as local Mossy cells (<xref ref-type="bibr" rid="bib1">Acsády et al., 1998</xref>; <xref ref-type="bibr" rid="bib48">Henze et al., 2002</xref>; <xref ref-type="bibr" rid="bib8">Amaral et al., 2007</xref>; <xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>; <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). In our model, we omit Mossy cells for simplicity and describe the dentate gyrus as a competitive circuit consisting of <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> DGCs and <inline-formula><mml:math id="inf108"><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:math></inline-formula> GABAergic interneurons (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). The activity of <inline-formula><mml:math id="inf109"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> neurons in EC represents an input pattern <inline-formula><mml:math id="inf110"><mml:mrow><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Because the perforant path also induces strong feedforward inhibition in the dentate gyrus (<xref ref-type="bibr" rid="bib61">Li et al., 2013</xref>), we assume that the effective EC activity is normalized, such that <inline-formula><mml:math id="inf111"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for any input pattern <inline-formula><mml:math id="inf112"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). We use <inline-formula><mml:math id="inf113"><mml:mi>P</mml:mi></mml:math></inline-formula> different input patterns <inline-formula><mml:math id="inf114"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf115"><mml:mrow><mml:mn>1</mml:mn><mml:mo>⩽</mml:mo><mml:mi>μ</mml:mi><mml:mo>⩽</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula> in the simulations of the model.</p><p>In our network, model EC neurons have excitatory all-to-all connections to the DGCs. In rodent hippocampus, spiking mature DGCs activate interneurons in dentate gyrus, which in turn inhibit other mature DGCs (<xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>; <xref ref-type="bibr" rid="bib7">Alvarez et al., 2016</xref>). In our model, the DGCs are thus recurrently connected with inhibitory neurons (<xref ref-type="fig" rid="fig1">Figure 1b</xref>). Connections from DGCs to interneurons exist in our model with probability <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and have a weight <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Similarly, connections from interneurons to DGCs occur with probability <inline-formula><mml:math id="inf118"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and have a weight <inline-formula><mml:math id="inf119"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. All parameters are reported in <xref ref-type="table" rid="table1">Table 1</xref> (Biologically plausible network).</p><p>Before an input pattern is presented, all rates of model DGCs are initialized to zero. We assume that the DGCs have a frequency–current curve that is given by a rectified hyperbolic tangent (<xref ref-type="bibr" rid="bib28">Dayan and Abbott, 2001</xref>), which is similar to the frequency–current curve of spiking neuron models with refractoriness (<xref ref-type="bibr" rid="bib42">Gerstner et al., 2014</xref>). Moreover, we exploit the equivalence of two common firing rate equations (<xref ref-type="bibr" rid="bib72">Miller and Fumarola, 2012</xref>) and let the firing rate <inline-formula><mml:math id="inf120"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of DGC <inline-formula><mml:math id="inf121"><mml:mi>i</mml:mi></mml:math></inline-formula> upon stimulation with input pattern <inline-formula><mml:math id="inf122"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> evolve according to:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>tanh</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mi>L</mml:mi></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf123"><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>.</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> denotes rectification: <inline-formula><mml:math id="inf124"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf125"><mml:mrow><mml:mi>a</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and zero otherwise. Here, <italic>b</italic><sub><italic>i</italic></sub> is a firing threshold, <inline-formula><mml:math id="inf126"><mml:mi>L</mml:mi></mml:math></inline-formula> is the smoothness parameter of the frequency–current curve (<inline-formula><mml:math id="inf127"><mml:msup><mml:mi>L</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is the slope of the frequency–current curve at the firing threshold), and <inline-formula><mml:math id="inf128"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> the total input to cell <inline-formula><mml:math id="inf129"><mml:mi>i</mml:mi></mml:math></inline-formula>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:munderover><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <italic>x</italic><sub><italic>j</italic></sub> the activity of EC input neuron <inline-formula><mml:math id="inf130"><mml:mi>j</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf131"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⩾</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> the feedforward weight from EC input neuron <inline-formula><mml:math id="inf132"><mml:mi>j</mml:mi></mml:math></inline-formula> to DGC <inline-formula><mml:math id="inf133"><mml:mi>i</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf134"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> the weight from inhibitory neuron <inline-formula><mml:math id="inf135"><mml:mi>k</mml:mi></mml:math></inline-formula> to DGC <inline-formula><mml:math id="inf136"><mml:mi>i</mml:mi></mml:math></inline-formula>. The sum runs over all inhibitory neurons, but the weights are set to <inline-formula><mml:math id="inf137"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> if the connection is absent. The firing rate <inline-formula><mml:math id="inf138"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is unit-free and normalized to a maximum of 1, which we interpret as a firing rate of 10 Hz. We take the synaptic weights as unit-less parameters such that <inline-formula><mml:math id="inf139"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is also unit-free.</p><p>The firing rate <inline-formula><mml:math id="inf140"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:math></inline-formula> of inhibitory neuron <inline-formula><mml:math id="inf141"><mml:mi>k</mml:mi></mml:math></inline-formula>, is defined as:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mtext>inh</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf142"><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> a parameter which relates to the desired ensemble sparsity, and <inline-formula><mml:math id="inf143"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:math></inline-formula> the total input toward interneuron <inline-formula><mml:math id="inf144"><mml:mi>k</mml:mi></mml:math></inline-formula>, given as:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf145"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> the weight from DGC <inline-formula><mml:math id="inf146"><mml:mi>i</mml:mi></mml:math></inline-formula> to inhibitory neuron <inline-formula><mml:math id="inf147"><mml:mi>k</mml:mi></mml:math></inline-formula>. (We set <inline-formula><mml:math id="inf148"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> if the connection is absent.) The feedback from inhibitory neurons ensures a sparse activity of model DGCs for each pattern. With <inline-formula><mml:math id="inf149"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>*</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula> we find that more than 70% of model DGCs are silent (firing rate &lt; 1 Hz [<xref ref-type="bibr" rid="bib81">Senzai and Buzsáki, 2017</xref>]) when an input pattern is presented, and less than 10% are highly active (firing rate &gt; 1 Hz) (<xref ref-type="fig" rid="fig2">Figure 2c,d</xref>), consistent with the experimentally observed sparse activity in dentate gyrus (<xref ref-type="bibr" rid="bib22">Chawla et al., 2005</xref>).</p></sec><sec id="s4-2"><title>Plasticity rule</title><p>Projections from EC onto newborn DGCs exhibit Hebbian plasticity (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib41">Ge et al., 2007</xref>; <xref ref-type="bibr" rid="bib71">McHugh et al., 2007</xref>). Therefore, in our model, the connections from EC neurons to DGCs are plastic, following a Hebbian learning rule that exhibits LTD or LTP depending on the firing rate <inline-formula><mml:math id="inf150"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the postsynaptic cell (<xref ref-type="bibr" rid="bib16">Bienenstock et al., 1982</xref>; <xref ref-type="bibr" rid="bib11">Artola et al., 1990</xref>; <xref ref-type="bibr" rid="bib83">Sjöström et al., 2001</xref>; <xref ref-type="bibr" rid="bib75">Pfister and Gerstner, 2006</xref>). Input patterns, <inline-formula><mml:math id="inf151"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf152"><mml:mrow><mml:mn>1</mml:mn><mml:mo>⩽</mml:mo><mml:mi>μ</mml:mi><mml:mo>⩽</mml:mo><mml:mi>P</mml:mi></mml:mrow></mml:math></inline-formula>, are presented in random order. For each input pattern, we let the firing rate converge for a time <inline-formula><mml:math id="inf153"><mml:mi>T</mml:mi></mml:math></inline-formula> where <inline-formula><mml:math id="inf154"><mml:mi>T</mml:mi></mml:math></inline-formula> was chosen long enough to achieve convergence to a precision of 10<sup>−6</sup>. After <inline-formula><mml:math id="inf155"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> presentations (i.e. at time <inline-formula><mml:math id="inf156"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula>), the weight vector has value <inline-formula><mml:math id="inf157"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>. We then present the next pattern and update at time <inline-formula><mml:math id="inf158"><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf159"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), according to the following plasticity rule (<xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>, written here for convenience):<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>η</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <italic>x</italic><sub><italic>j</italic></sub> is the firing rate of presynaptic EC input neuron <inline-formula><mml:math id="inf160"><mml:mi>j</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf161"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> the firing rate of postsynaptic DGC <inline-formula><mml:math id="inf162"><mml:mi>i</mml:mi></mml:math></inline-formula>, η the learning rate, θ marks the transition from LTD to LTP, and the relative strength α, γ of LTP and LTD depend on θ via <inline-formula><mml:math id="inf163"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mi>θ</mml:mi><mml:mn>3</mml:mn></mml:msup></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf164"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. The values of the parameters <inline-formula><mml:math id="inf165"><mml:msub><mml:mi>α</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf166"><mml:msub><mml:mi>γ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, β, and θ are given in <xref ref-type="table" rid="table1">Table 1</xref> (Biologically plausible network). The weights are hard-bounded from below at 0, i.e., if <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> leads to a new weight smaller than zero, <inline-formula><mml:math id="inf167"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is set to zero. The first two terms of <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> are a variation of the BCM rule (<xref ref-type="bibr" rid="bib16">Bienenstock et al., 1982</xref>). The third term implements heterosynaptic plasticity (<xref ref-type="bibr" rid="bib23">Chistiakova et al., 2014</xref>; <xref ref-type="bibr" rid="bib101">Zenke and Gerstner, 2017</xref>) with three important features: first, heterosynaptic plasticity has a negative sign and therefore leads to synaptic depression; second, heterosynaptic plasticity sets in above a threshold (<inline-formula><mml:math id="inf168"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>) that is the same threshold as that for LTP, so that if LTP occurs at some synapses LTD is induced at other synapses; third, above threshold the dependence upon the postsynaptic firing rate <inline-formula><mml:math id="inf169"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is supra-linear. The interaction of the three different terms in the plasticity rule has several consequences. Because the first two terms of the plasticity rule are Hebbian (‘homosynaptic’) and proportional to the presynaptic activity <italic>x</italic><sub><italic>j</italic></sub>, the active DGCs (<inline-formula><mml:math id="inf170"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>) update their feedforward weights in direction of the input pattern <inline-formula><mml:math id="inf171"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>. Moreover, whenever LTP occurs at some synapses, all weights onto neuron <inline-formula><mml:math id="inf172"><mml:mi>i</mml:mi></mml:math></inline-formula> are downregulated heterosynaptically by an amount that increases supra-linearly with the postsynaptic rate <inline-formula><mml:math id="inf173"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, implicitly controlling the length of the weight vector (see below) similar to synaptic homeostasis (<xref ref-type="bibr" rid="bib89">Turrigiano et al., 1998</xref>) but on a rapid time scale (<xref ref-type="bibr" rid="bib101">Zenke and Gerstner, 2017</xref>). Analogous to learning in a competitive network (<xref ref-type="bibr" rid="bib58">Kohonen, 1989</xref>; <xref ref-type="bibr" rid="bib49">Hertz et al., 1991</xref>), the vector of feedforward weights onto active DGCs will move toward the center of mass of the cluster of patterns they are selective for, as we will discuss now.</p><p>For a given input pattern <inline-formula><mml:math id="inf174"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>, there are three fixed points for the postsynaptic firing rate: <inline-formula><mml:math id="inf175"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf176"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf177"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> (the negative root is omitted because <inline-formula><mml:math id="inf178"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⩾</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> due to <xref ref-type="disp-formula" rid="equ2">Equation (2)</xref>). For <inline-formula><mml:math id="inf179"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>, there is LTD, so the weights move toward zero: <inline-formula><mml:math id="inf180"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, while for <inline-formula><mml:math id="inf181"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>, there is LTP, so the weights move toward <inline-formula><mml:math id="inf182"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>→</mml:mo><mml:mfrac><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>x</mml:mi><mml:mi>j</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig1">Figure 1c</xref>). The value of <inline-formula><mml:math id="inf183"><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is defined implicitly by the network <xref ref-type="disp-formula" rid="equ2 equ3 equ4 equ5">Equations (2–5)</xref>. If a pattern <inline-formula><mml:math id="inf184"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> is presented only for a short time these fixed points are not reached during a single pattern presentation.</p><sec id="s4-2-1"><title>Winners, losers, and quasi-orthogonal inputs</title><p>We define the winners as the DGCs that become strongly active (<inline-formula><mml:math id="inf185"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>) during presentation of an input pattern. Since the input patterns are normalized to have an L2-norm of 1 (<inline-formula><mml:math id="inf186"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> by construction), and the L2-norm of the feedforward weight vectors is bounded (see Section Direction and length of the weight vector), the winning units are the ones whose weight vectors <inline-formula><mml:math id="inf187"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> (row of the feedforward connectivity matrix) align best with the current input pattern <inline-formula><mml:math id="inf188"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>.</p><p>We emphasize that all synaptic weights and all presynaptic firing rates <inline-formula><mml:math id="inf189"><mml:msub><mml:mi>ν</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> are non-negative: <inline-formula><mml:math id="inf190"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⩾</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf191"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⩾</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Thus, both the weight vectors and the vectors of input firing rates live in the positive quadrant. The angle between an input pattern <inline-formula><mml:math id="inf192"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> and the weight vector <inline-formula><mml:math id="inf193"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of neuron <inline-formula><mml:math id="inf194"><mml:mi>i</mml:mi></mml:math></inline-formula> can be at most ninety degrees. We say that an input pattern <inline-formula><mml:math id="inf195"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> is ‘quasi-orthogonal’ to a weight vector <inline-formula><mml:math id="inf196"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> if, in the stationary state, the input is not sufficient to activate neuron <inline-formula><mml:math id="inf197"><mml:mi>i</mml:mi></mml:math></inline-formula>, i.e., <inline-formula><mml:math id="inf198"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:msubsup><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>I</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mi>I</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. If an input pattern <inline-formula><mml:math id="inf199"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> is quasi-orthogonal to a weight vector <inline-formula><mml:math id="inf200"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, then neuron <inline-formula><mml:math id="inf201"><mml:mi>i</mml:mi></mml:math></inline-formula> does not fire in response to <inline-formula><mml:math id="inf202"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula> after the stimulus has been applied for a long enough time. Note that for a case without inhibitory neurons and with <inline-formula><mml:math id="inf203"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we recover the standard orthogonality condition, but for finite <inline-formula><mml:math id="inf204"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> quasi-orthogonality corresponds to angles larger than some reference angle.</p></sec><sec id="s4-2-2"><title>Direction and length of the weight vector</title><p>Let us denote the ensemble of patterns for which neuron <inline-formula><mml:math id="inf205"><mml:mi>i</mml:mi></mml:math></inline-formula> is a winner by <inline-formula><mml:math id="inf206"><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> and call this the set of winning patterns (<inline-formula><mml:math id="inf207"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>). Suppose that neuron <inline-formula><mml:math id="inf208"><mml:mi>i</mml:mi></mml:math></inline-formula> is quasi-orthogonal to all other patterns, so that for all <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∉</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, we have <inline-formula><mml:math id="inf210"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Then the feedforward weight vector of neuron <inline-formula><mml:math id="inf211"><mml:mi>i</mml:mi></mml:math></inline-formula> converges in expectation to:<disp-formula id="equ7"><label>(6)</label><mml:math id="m7"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf212"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf213"><mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mn>3</mml:mn></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>. Hence <inline-formula><mml:math id="inf214"><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> is a weighted average over all winning patterns.</p><p>The squared length of the feedforward weight vector can be computed by multiplying <xref ref-type="disp-formula" rid="equ7">Equation (6)</xref> with <inline-formula><mml:math id="inf215"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:msup><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mfrac></mml:mstyle></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Since input patterns have length one, the scalar product on the right-hand side can be rewritten as <inline-formula><mml:math id="inf216"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where α is the angle between the weight vector and pattern <inline-formula><mml:math id="inf217"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>. Division by <inline-formula><mml:math id="inf218"><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow></mml:math></inline-formula> yields the L2-norm of the feedforward weight vector:<disp-formula id="equ9"><label>(8)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mfrac></mml:mpadded></mml:mrow></mml:mrow></mml:math></disp-formula>where the averages run, as before, over all winning patterns.</p><p>Let us now derive bounds for <inline-formula><mml:math id="inf219"><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow></mml:math></inline-formula>. First, since <inline-formula><mml:math id="inf220"><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⩽</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> we have <inline-formula><mml:math id="inf221"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>⩽</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. Second, since for all winning patterns <inline-formula><mml:math id="inf222"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&gt;</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula>, where θ is the LTP threshold, we have <inline-formula><mml:math id="inf223"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>⩾</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. Thus the length of the weight vector is finite and bounded by:<disp-formula id="equ10"><label>(9)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⩽</mml:mo><mml:mrow><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mfrac></mml:mrow><mml:mo>⩽</mml:mo><mml:mrow><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>It is possible to make the second bound tighter if we find the winning pattern with the smallest firing rate <inline-formula><mml:math id="inf224"><mml:msub><mml:mi>ν</mml:mi><mml:mtext>min</mml:mtext></mml:msub></mml:math></inline-formula> such that <inline-formula><mml:math id="inf225"><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⩾</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:msub><mml:mi>ν</mml:mi><mml:mtext>min</mml:mtext></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ11"><label>(10)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⩽</mml:mo><mml:mrow><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mtext>min</mml:mtext></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The bound is reached if neuron <inline-formula><mml:math id="inf226"><mml:mi>i</mml:mi></mml:math></inline-formula> is winner for a single input pattern.</p><p>We can also derive a lower bound. For a pattern <inline-formula><mml:math id="inf227"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, let us write the firing rate of neuron <inline-formula><mml:math id="inf228"><mml:mi>i</mml:mi></mml:math></inline-formula> as <inline-formula><mml:math id="inf229"><mml:mrow><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> where <inline-formula><mml:math id="inf230"><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the mean firing rate of neuron <inline-formula><mml:math id="inf231"><mml:mi>i</mml:mi></mml:math></inline-formula> averaged across all winning patterns and <inline-formula><mml:math id="inf232"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. We assume that the absolute size of <inline-formula><mml:math id="inf233"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is small, i.e., <inline-formula><mml:math id="inf234"><mml:mrow><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>≪</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>. Linearization of <xref ref-type="disp-formula" rid="equ9">Equation (8)</xref> around <inline-formula><mml:math id="inf235"><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> yields:<disp-formula id="equ12"><label>(11)</label><mml:math id="m12"><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mstyle displaystyle="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>G</mml:mi><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Elementary geometric arguments for a neuron model with monotonically increasing frequency–current curve yield that the value of <inline-formula><mml:math id="inf236"><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:math></inline-formula> is positive (or zero) because an increase in the angle α lowers both the cosine and the firing rate, giving rise to a positive correlation. Since we are interested in a lower bound, we can therefore drop the term proportional to <inline-formula><mml:math id="inf237"><mml:msubsup><mml:mi>G</mml:mi><mml:mn>1</mml:mn><mml:mo>′</mml:mo></mml:msubsup></mml:math></inline-formula> and evaluate the ratio <inline-formula><mml:math id="inf238"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow></mml:math></inline-formula> to find:<disp-formula id="equ13"><label>(12)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⩾</mml:mo><mml:mrow><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msub></mml:mrow><mml:mo>⩾</mml:mo><mml:mrow><mml:mfrac><mml:mi>γ</mml:mi><mml:mi>β</mml:mi></mml:mfrac><mml:mo>⁢</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>max</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf239"><mml:msub><mml:mi>ν</mml:mi><mml:mi>max</mml:mi></mml:msub></mml:math></inline-formula> is the maximal firing rate of a DGC and <inline-formula><mml:math id="inf240"><mml:mrow><mml:mover accent="true"><mml:mi>α</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:msub><mml:mi mathvariant="normal">C</mml:mi><mml:mi mathvariant="normal">i</mml:mi></mml:msub></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the angle of the winning pattern that has the largest angle with the weight vector. The first bound is tight and is reached if neuron <inline-formula><mml:math id="inf241"><mml:mi>i</mml:mi></mml:math></inline-formula> is winner for only two patterns.</p><p>To summarize we find that the length of the weight vector remains bounded in a narrow range. Hence, for a reasonable distribution of input patterns and weight vectors, the value of <inline-formula><mml:math id="inf242"><mml:mrow><mml:mo fence="true">||</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>w</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mo fence="true">||</mml:mo></mml:mrow></mml:math></inline-formula> is similar for different neurons <inline-formula><mml:math id="inf243"><mml:mi>i</mml:mi></mml:math></inline-formula>, so that the weight vector will have, after convergence, similar lengths for all DGCs that are winners for at least one pattern. In our simulations with the MNIST data set, we find that the length of feedforward weight vectors lies in the range between 9.3 and 11.1 across all responsive neurons with a mean value close to 10; <xref ref-type="fig" rid="fig2">Figure 2e</xref>.</p></sec><sec id="s4-2-3"><title>Early maturation phase</title><p>During the early phase of maturation, the GABAergic input onto a newborn DGC with index <inline-formula><mml:math id="inf244"><mml:mi>l</mml:mi></mml:math></inline-formula> has an excitatory effect. In the model, it is implemented as follows: <inline-formula><mml:math id="inf245"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf246"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for any interneuron <inline-formula><mml:math id="inf247"><mml:mi>k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf248"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise (no connection). Since newborn cells do not project yet onto inhibitory neurons (<xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>), we have <inline-formula><mml:math id="inf249"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Newborn DGCs are known to have enhanced excitability (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib62">Li et al., 2017</xref>), so their threshold is kept at <inline-formula><mml:math id="inf250"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+2.8pt"><mml:mn>0</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>∀</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Because the newborn model DGCs receive lateral excitation via interneurons and their thresholds are zero during the early phase of maturation, the lateral excitatory GABAergic input is always sufficient to activate them. Hence, if the firing rate of a newborn DGC exceeds the LTP threshold θ, the feedforward weights grow toward the presented input pattern, <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>.</p><p>Presentation of all patterns of the data set once (one epoch) is sufficient to reach convergence of the feedforward weights onto newborn DGCs. We define the end of the first epoch as the end of the early phase, i.e., simulation of one epoch of the model corresponds to about 3 weeks of biological time.</p></sec><sec id="s4-2-4"><title>Late maturation phase</title><p>During the late phase of maturation (starting at about 3 weeks [<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>]), the GABAergic input onto newborn DGCs switches from excitatory to inhibitory. In terms of our model, it means that all existing <inline-formula><mml:math id="inf251"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> connections switch their sign to <inline-formula><mml:math id="inf252"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. Furthermore, since newborn DGCs develop lateral connections to inhibitory neurons in the late maturation phase (<xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>), we set <inline-formula><mml:math id="inf253"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf254"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf255"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi><mml:mo>⁢</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise. The thresholds of newborn DGCs are updated after presentation of pattern μ at time <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi>n</mml:mi><mml:mo>⋅</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf257"><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>b</mml:mi><mml:mi>l</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>) according to <inline-formula><mml:math id="inf258"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mi>b</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>ν</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf259"><mml:msub><mml:mi>ν</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> is a reference rate and <inline-formula><mml:math id="inf260"><mml:msub><mml:mi>η</mml:mi><mml:mi>b</mml:mi></mml:msub></mml:math></inline-formula> a learning rate, to mimic the decrease of excitability as newborn DGCs mature (<xref ref-type="table" rid="table1">Table 1</xref>, Biologically plausible network). Therefore, the distribution of firing rates of newborn DGCs is shifted to the left (toward lower firing rates) at the end of the late phase of maturation compared to the early phase of maturation (<xref ref-type="fig" rid="fig2">Figure 2c,d</xref>). A sufficient condition for a newborn DGC to win the competition upon presentation of patterns of the novel cluster is that the scalar product between a pattern of the novel cluster and the feedforward weight vector onto the newborn DGC is larger than the scalar product between the pattern of the novel cluster and the feedforward weight vector onto any of the mature DGCs. Analogous to the early phase of maturation, presentation of all patterns of the data set once (one epoch) is sufficient to reach convergence of the feedforward weights onto newborn DGCs. We therefore consider that the late phase of maturation has been finished after one epoch.</p></sec></sec><sec id="s4-3"><title>Input patterns</title><p>Two different sets of input patterns are used. Both data sets have a number <inline-formula><mml:math id="inf261"><mml:mi>K</mml:mi></mml:math></inline-formula> of clusters and several thousands of patterns per cluster. As a first data set, we use the MNIST 12 × 12 patterns (<xref ref-type="bibr" rid="bib59">Lecun et al., 1998</xref>) (<inline-formula><mml:math id="inf262"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>144</mml:mn></mml:mrow></mml:math></inline-formula>), normalized such that the L2-norm of each pattern is equal to 1. Normalization of inputs (be it implemented algorithmically as done here or by explicit inhibitory feedback) ensures that, once weight growth due to synaptic plasticity has ended and weights have stabilized, the overall strength of input onto DGCs is approximately identical for all cells (see Section Direction and length of the weight vector). Equalized lengths of weight vectors are, in turn, an important feature of classic soft or hard competitive networks (<xref ref-type="bibr" rid="bib58">Kohonen, 1989</xref>; <xref ref-type="bibr" rid="bib49">Hertz et al., 1991</xref>). The training set contains approximately 6000 patterns per digit, while the testing set contains about 1000 patterns per digit (<xref ref-type="fig" rid="fig1">Figure 1d</xref>). Both training patterns and test patterns contain a large variety of different writing styles indicating that the clusters of input patterns for each class are broadly distributed around their center of mass.</p><p>As a second data set, we use handmade artificial patterns designed such that the distance between the centers of any two clusters, or in other words their pairwise similarity, is the same. All clusters lie on the positive quadrant of the surface of a hypersphere of dimension <inline-formula><mml:math id="inf263"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. The cluster centers are Walsh patterns shifted along the diagonal (<xref ref-type="fig" rid="fig5">Figure 5b</xref>):<disp-formula id="equ14"><label>(13)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:mtd><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>with <inline-formula><mml:math id="inf264"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> a parameter that determines the spacing between clusters. <italic>c</italic><sub>0</sub> is a normalization factor to ensure that the center of mass of all clusters has an L2-norm of 1:<disp-formula id="equ15"><label>(14)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msqrt></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The number of input neurons <inline-formula><mml:math id="inf265"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is <inline-formula><mml:math id="inf266"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>2</mml:mn><mml:mi>K</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>. The scalar product, and hence the angle Ω, between the center of mass of any pair of clusters <inline-formula><mml:math id="inf267"><mml:mi>k</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf268"><mml:mi>l</mml:mi></mml:math></inline-formula> (<inline-formula><mml:math id="inf269"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:math></inline-formula>) is a function of ξ (<xref ref-type="fig" rid="fig5">Figure 5a</xref>):<disp-formula id="equ16"><label>(15)</label><mml:math id="m16"><mml:mrow><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>l</mml:mi></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We define the pairwise similarity <inline-formula><mml:math id="inf270"><mml:mi>s</mml:mi></mml:math></inline-formula> of two clusters as: <inline-formula><mml:math id="inf271"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ξ</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Highly similar clusters have a large <inline-formula><mml:math id="inf272"><mml:mi>s</mml:mi></mml:math></inline-formula> due to the small distance between their centers (hence a small ξ).</p><p>To make the artificial data set comparable to the MNIST 12 × 12 data set, we choose <inline-formula><mml:math id="inf273"><mml:mrow><mml:mi>K</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:math></inline-formula>, so <inline-formula><mml:math id="inf274"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula>, and we generate 6000 noisy patterns per cluster for the training set and 1000 other noisy patterns per cluster for the testing set. Since our noisy high-dimensional input patterns have to be symmetrically distributed around the centers of mass <inline-formula><mml:math id="inf275"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula>, yet lie on the hypersphere, we have to use an appropriate sampling method. The patterns <inline-formula><mml:math id="inf276"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> of a given cluster <inline-formula><mml:math id="inf277"><mml:mi>k</mml:mi></mml:math></inline-formula> with center of mass <inline-formula><mml:math id="inf278"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> are thus sampled from a Von Mises–Fisher distribution (<xref ref-type="bibr" rid="bib66">Mardia and Jupp, 2009</xref>):<disp-formula id="equ17"><label>(16)</label><mml:math id="m17"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>ζ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf279"><mml:mover accent="true"><mml:mi>ζ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> an L2-normalized vector taken in the space orthogonal to <inline-formula><mml:math id="inf280"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula>. The vector <inline-formula><mml:math id="inf281"><mml:mover accent="true"><mml:mi>ζ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula> is obtained by performing the singular-value decomposition of <inline-formula><mml:math id="inf282"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> (<inline-formula><mml:math id="inf283"><mml:mrow><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Σ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:mrow></mml:math></inline-formula>) and multiplying the matrix <inline-formula><mml:math id="inf284"><mml:mi>U</mml:mi></mml:math></inline-formula> (after removing its first column), which corresponds to the left-singular vectors in the orthogonal space to <inline-formula><mml:math id="inf285"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula>, with a vector whose elements are drawn from the standard normal distribution. Then the L2-norm of the obtained pattern is set to 1, so that it lies on the surface of the hypersphere. A rejection sampling scheme is used to obtain <inline-formula><mml:math id="inf286"><mml:mi>a</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib66">Mardia and Jupp, 2009</xref>). The sample <inline-formula><mml:math id="inf287"><mml:mi>a</mml:mi></mml:math></inline-formula> is kept if <inline-formula><mml:math id="inf288"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>ln</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>⩾</mml:mo><mml:mrow><mml:mtext>ln</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>u</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with κ a concentration parameter, <inline-formula><mml:math id="inf289"><mml:mrow><mml:mi>ψ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf290"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ψ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>ln</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mi>ψ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf291"><mml:mi>u</mml:mi></mml:math></inline-formula> drawn from a uniform distribution <inline-formula><mml:math id="inf292"><mml:mrow><mml:mi>u</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf293"><mml:mrow><mml:mi>a</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf294"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msqrt><mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>κ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>κ</mml:mi></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf295"><mml:mi>z</mml:mi></mml:math></inline-formula> drawn from a beta distribution <inline-formula><mml:math id="inf296"><mml:mrow><mml:mi>z</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℬ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>The concentration parameter κ characterizes the spread of the distribution around the center <inline-formula><mml:math id="inf297"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula>. In the limit where <inline-formula><mml:math id="inf298"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, sampling from the Von Mises–Fisher distribution becomes equivalent to sampling uniformly on the surface of the hypersphere, so the clusters become highly overlapping. In dimension <inline-formula><mml:math id="inf299"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>128</mml:mn></mml:mrow></mml:math></inline-formula>, if <inline-formula><mml:math id="inf300"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>&gt;</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>3</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>, the probability of overlap between clusters is negligible. We use a value <inline-formula><mml:math id="inf301"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-4"><title>Classification performance (readout network)</title><p>It has been observed that classification performance based on DGC population activity is a good proxy for behavioral discrimination (<xref ref-type="bibr" rid="bib98">Woods et al., 2020</xref>). Hence, to evaluate whether the newborn DGCs contribute to the function of the dentate gyrus network, we study classification performance. Once the feedforward weights have been adjusted upon presentation of many input patterns from the training set (Section Plasticity rule), we keep them fixed and determine classification on the test set using artificial readout units (RO).</p><p>To do so, the readout weights (<inline-formula><mml:math id="inf302"><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> from model DGC <inline-formula><mml:math id="inf303"><mml:mi>i</mml:mi></mml:math></inline-formula> to readout unit <inline-formula><mml:math id="inf304"><mml:mi>k</mml:mi></mml:math></inline-formula>) are initialized at random values drawn from a uniform distribution: <inline-formula><mml:math id="inf305"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:msubsup><mml:mo>∼</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mi class="ltx_font_mathcaligraphic">𝒰</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, with <inline-formula><mml:math id="inf306"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:math></inline-formula>. The number of readout units, <inline-formula><mml:math id="inf307"><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, corresponds to the number of learned classes. To adjust the readout weights, all patterns of the training data set that belong to the learned classes are presented one after the other. For each pattern <inline-formula><mml:math id="inf308"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>, we let the firing rate of the DGCs converge (values at convergence: <inline-formula><mml:math id="inf309"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:math></inline-formula>). The activity of a readout unit <inline-formula><mml:math id="inf310"><mml:mi>k</mml:mi></mml:math></inline-formula> is given by:<disp-formula id="equ18"><label>(17)</label><mml:math id="m18"><mml:mrow><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>As we aim to assess the performance of the network of DGCs, the readout weights are adjusted by an artificial supervised learning rule. The loss function, which corresponds to the difference between the activity of the readout units and a one-hot representation of the corresponding pattern label (<xref ref-type="bibr" rid="bib49">Hertz et al., 1991</xref>),<disp-formula id="equ19"><label>(18)</label><mml:math id="m19"><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="true"><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow></mml:msub></mml:munderover></mml:mstyle><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mi>k</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf311"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>k</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:math></inline-formula> the element <inline-formula><mml:math id="inf312"><mml:mi>k</mml:mi></mml:math></inline-formula> of a one-hot representation of the correct label of pattern <inline-formula><mml:math id="inf313"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>, is minimized by stochastic gradient descent:<disp-formula id="equ20"><label>(19)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msubsup><mml:mi>L</mml:mi><mml:mi>k</mml:mi><mml:mi>μ</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>g</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>i</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The readout units have a rectified hyperbolic tangent frequency-current curve: <inline-formula><mml:math id="inf314"><mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext>tanh</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, whose derivative is: <inline-formula><mml:math id="inf315"><mml:mrow><mml:mrow><mml:msup><mml:mi>g</mml:mi><mml:mo>′</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mtext>tanh</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We learn the weights of the readout units over 100 epochs of presentations of all training patterns with <inline-formula><mml:math id="inf316"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, which is sufficient to reach convergence.</p><p>Thereafter, the readout weights are fixed. Each test set pattern belonging to one of the learned classes is presented once, and the firing rates of the DGCs are let to converge. Finally, the activity of the readout units <inline-formula><mml:math id="inf317"><mml:msubsup><mml:mi>ν</mml:mi><mml:mi>k</mml:mi><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mi>O</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is computed and compared to the correct label <inline-formula><mml:math id="inf318"><mml:msubsup><mml:mi>L</mml:mi><mml:mi>k</mml:mi><mml:mi>μ</mml:mi></mml:msubsup></mml:math></inline-formula> of the presented pattern. If the readout unit with the highest activity value is the one that represents the class of the presented input pattern, the pattern is said to be correctly classified. Classification performance is given by the number of correctly classified patterns divided by the total number of test patterns of the learned classes.</p></sec><sec id="s4-5"><title>Control cases</title><p>In our standard setting, patterns from a third digit are presented to a network that has previously only seen patterns from two digits. The question is whether neurogenesis helps when adding the third digit. We use several control cases to compare with the neurogenesis case. In the first control case, all three digits are learned in parallel (<xref ref-type="fig" rid="fig3">Figure 3a</xref>, control 1). In the two other control cases, we either keep all feedforward connections toward the DGCs plastic (<xref ref-type="fig" rid="fig3">Figure 3c</xref>, control 3) or fix the feedforward connections for all selective DGCs but keep unselective neurons plastic (as in the neurogenesis case) (<xref ref-type="fig" rid="fig3">Figure 3b</xref>, control 2). However, in both instances, the DGCs do not mature in the two-step process induced by the GABA-switch that is part of our model of neurogenesis.</p></sec><sec id="s4-6"><title>Pretraining with two digits</title><p>As we are interested by neurogenesis at the adult stage, we pretrain the network with patterns from two digits, such that it already stores some memories before neurogenesis takes place. To do so, we randomly initialize the weights from EC neurons to DGCs: they are drawn from a uniform distribution (<inline-formula><mml:math id="inf319"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>). The L2-norm of the feedforward weight vector onto each DGC is then normalized to 1, to ensure fair competition between DGCs during learning. Then we present all patterns from digits 3 and 4 in random order, as many times as needed for convergence of the weights. During each pattern presentation the firing rates of the DGCs are computed (Section Network architecture and neuronal dynamics) and their feedforward weights are updated according to our plasticity rule (Section Plasticity rule). We find that we need approximately 40 epochs for convergence of the weights and use 80 epochs to make sure that all weights are stable. At the end of pretraining, our network is considered to correspond to an adult stage, because some DGCs are selective for prototypes of the pretrained digits (<xref ref-type="fig" rid="fig1">Figure 1e</xref>).</p></sec><sec id="s4-7"><title>Projection on pairwise discriminatory axes</title><p>To assess how separability of the DGC activation patterns develops during the late phase of maturation of newborn DGCs, we project the population activity onto axes which are optimized for pairwise discrimination (patterns from digit 3 versus patterns from digit 5, 4 versus 5, and 3 versus 4). Those axes are determined using Fisher linear discriminant analysis, as explained below.</p><p>We determine the vector of DGC firing rates, <inline-formula><mml:math id="inf320"><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, at the end of the late phase of maturation of newborn DGCs upon presentation of each pattern, <inline-formula><mml:math id="inf321"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, from digits 3, 4, and 5 of the training MNIST dataset. The mean activity in response to all training patterns μ from digit <inline-formula><mml:math id="inf322"><mml:mi>m</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf323"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msub><mml:mi>N</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>∈</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, is computed for each of the three digits (<inline-formula><mml:math id="inf324"><mml:msub><mml:mi>N</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is the number of training patterns of digit <inline-formula><mml:math id="inf325"><mml:mi>m</mml:mi></mml:math></inline-formula>). The pairwise Fisher linear discriminant is defined as the linear function <inline-formula><mml:math id="inf326"><mml:mrow><mml:msup><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mover accent="true"><mml:mi>ν</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula> that maximizes the distance between the means of the projected activity in response to two digits (e.g. <inline-formula><mml:math id="inf327"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf328"><mml:mi>n</mml:mi></mml:math></inline-formula>), while normalizing for within-digit variability. The objective function to maximize is thus given as:<disp-formula id="equ21"><label>(20)</label><mml:math id="m21"><mml:mrow><mml:mrow><mml:mi>J</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>w</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>T</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula>with <inline-formula><mml:math id="inf329"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>m</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>μ</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>T</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> the between-digit scatter matrix, and <inline-formula><mml:math id="inf330"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mi>W</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> the within-digit scatter matrix (<inline-formula><mml:math id="inf331"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>m</mml:mi></mml:msub></mml:math></inline-formula> is the covariance matrix of the DGC activity in response to pattern of digit <inline-formula><mml:math id="inf332"><mml:mi>m</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf333"><mml:msub><mml:mi mathvariant="normal">Σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> is the covariance matrix of the DGC activity in response to pattern of digit <inline-formula><mml:math id="inf334"><mml:mi>n</mml:mi></mml:math></inline-formula>). It can be shown that the direction of the optimal discriminatory axis between digit <inline-formula><mml:math id="inf335"><mml:mi>m</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf336"><mml:mi>n</mml:mi></mml:math></inline-formula> is given by the eigenvector of <inline-formula><mml:math id="inf337"><mml:mrow><mml:msubsup><mml:mi>S</mml:mi><mml:mi>W</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:msub><mml:mi>S</mml:mi><mml:mi>B</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> with the corresponding largest eigenvalue.</p><p>We arbitrarily set ‘axis 1’ as the optimal discriminatory axis between digit 3 and digit 5, ‘axis 2’ as the optimal discriminatory axis between digit 4 and digit 5, and ‘axis 3’ as the optimal discriminatory axis between digit 3 and digit 4. For each of the three discriminatory axes, we define its origin (i.e. projection value of 0) as the location of the average projection of all training patterns of the three digits on the corresponding axis. <xref ref-type="fig" rid="fig4">Figure 4</xref> represents the projections of DGC activity upon presentation of testing patterns at the end of the early and late phase of maturation of newborn DGCs onto the above-defined axes.</p></sec><sec id="s4-8"><title>Statistics</title><p>In the main text, we present a representative example with three digits from the MNIST data set (3, 4, and 5). It is selected from a set of 10 random combinations of three different digits. For each combination, one network is pretrained with two digits for 80 epochs. Then the third digit is added and neurogenesis takes place (one epoch of early phase of maturation, and one epoch of late phase of maturation). Furthermore, another network is pretrained directly with the three digits for 80 epochs. Classification performance is reported for all combinations (<xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>).</p></sec><sec id="s4-9"><title>Simplified rate network</title><p>We use a toy network and the artificial data set to determine whether our theory of integration of newborn DGCs can explain why adult dentate gyrus neurogenesis helps for the discrimination of similar, but not for distinct patterns.</p><p>The rate network described above is simplified as follows. We use <inline-formula><mml:math id="inf338"><mml:mi>K</mml:mi></mml:math></inline-formula> DGCs for <inline-formula><mml:math id="inf339"><mml:mi>K</mml:mi></mml:math></inline-formula> clusters. Their firing rate <inline-formula><mml:math id="inf340"><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is given by:<disp-formula id="equ22"><label>(21)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>m</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi class="ltx_font_mathcaligraphic">ℋ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf341"><mml:mi class="ltx_font_mathcaligraphic">ℋ</mml:mi></mml:math></inline-formula> is the Heaviside step function. As before, <italic>b</italic><sub><italic>i</italic></sub> is the threshold, and <inline-formula><mml:math id="inf342"><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> the total input toward neuron <inline-formula><mml:math id="inf343"><mml:mi>i</mml:mi></mml:math></inline-formula>:<disp-formula id="equ23"><label>(22)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>G</mml:mi><mml:mo>⁢</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msub></mml:munderover><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ν</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>with <italic>x</italic><sub><italic>j</italic></sub> the input of presynaptic EC neuron <inline-formula><mml:math id="inf344"><mml:mi>j</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf345"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> the feedforward weight between EC neuron <inline-formula><mml:math id="inf346"><mml:mi>j</mml:mi></mml:math></inline-formula> and DGC <inline-formula><mml:math id="inf347"><mml:mi>i</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf348"><mml:msub><mml:mi>ν</mml:mi><mml:mi>k</mml:mi></mml:msub></mml:math></inline-formula> the firing rate of DGC <inline-formula><mml:math id="inf349"><mml:mi>k</mml:mi></mml:math></inline-formula>. Inhibitory neurons are modeled implicitly: each DGC directly connects to all other DGCs via inhibitory recurrent connections of value <inline-formula><mml:math id="inf350"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. During presentation of pattern <inline-formula><mml:math id="inf351"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>μ</mml:mi></mml:msup></mml:math></inline-formula>, the firing rates of the DGCs evolve according to <xref ref-type="disp-formula" rid="equ22">Equation (21)</xref>. After convergence, the feedforward weights are updated: <inline-formula><mml:math id="inf352"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The synaptic plasticity rule is the same as before, see <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>, but with the parameters reported in <xref ref-type="table" rid="table1">Table 1</xref> (Simple network). They are different from those of the biologically plausible network because we now aim for a single winning neuron for each cluster. Note that for an LTP threshold <inline-formula><mml:math id="inf353"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> all active DGCs update their feedforward weights because of the Heaviside function for the firing rate (<xref ref-type="disp-formula" rid="equ22">Equation 21</xref>).</p><p>Assuming a single winner <inline-formula><mml:math id="inf354"><mml:msup><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:math></inline-formula> for each pattern presentation, the input (<xref ref-type="disp-formula" rid="equ23">Equation 22</xref>) to the winner is:<disp-formula id="equ24"><label>(23)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:msup><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:msup><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>while the input to the losers is:<disp-formula id="equ25"><label>(24)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Therefore, two conditions need to be satisfied for a solution with a single winner:<disp-formula id="equ26"><label>(25)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:msup><mml:mi>i</mml:mi><mml:mo>*</mml:mo></mml:msup></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>for the winner to actually be active, and: <disp-formula id="equ27"><label>(26)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:mrow><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula>to prevent non-winners to become active. The value of <italic>b</italic><sub><italic>i</italic></sub> in the model is lower in the early phase than in the late phase of maturation to mimic enhanced excitability (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib62">Li et al., 2017</xref>).</p><sec id="s4-9-1"><title>Similar versus distinct patterns with the artificial data set</title><p>Using the artificial data set with <inline-formula><mml:math id="inf355"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ14">Equation 13</xref>), the scalar product between the centers of mass of two different clusters, given by <xref ref-type="disp-formula" rid="equ16">Equation (15)</xref>, satisfies: <inline-formula><mml:math id="inf356"><mml:mrow><mml:mn>0.5</mml:mn><mml:mo>⩽</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⩽</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. This corresponds to <inline-formula><mml:math id="inf357"><mml:mrow><mml:msup><mml:mn>0</mml:mn><mml:mo>∘</mml:mo></mml:msup><mml:mo>⩽</mml:mo><mml:mi mathvariant="normal">Ω</mml:mi><mml:mo>⩽</mml:mo><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mn>60</mml:mn><mml:mo>∘</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>.</p><p>After stimulation with a pattern <inline-formula><mml:math id="inf358"><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:math></inline-formula>, it takes some time before the firing rates of the DGCs converge. We call two patterns ‘similar’ if they activate, at least initially, the same output unit, while we consider two patterns as ‘distinct’ if they do not activate the same output unit, not even initially. We now show that, with a large concentration parameter κ, patterns of different clusters are similar if <inline-formula><mml:math id="inf359"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>&lt;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo></mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula> and distinct if <inline-formula><mml:math id="inf360"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>&gt;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo></mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p><p>We first consider a DGC <inline-formula><mml:math id="inf361"><mml:mi>i</mml:mi></mml:math></inline-formula> whose feedforward weight vector has converged toward the center of mass of cluster <inline-formula><mml:math id="inf362"><mml:mi>k</mml:mi></mml:math></inline-formula>. If an input pattern <inline-formula><mml:math id="inf363"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> from cluster <inline-formula><mml:math id="inf364"><mml:mi>k</mml:mi></mml:math></inline-formula> is presented, it will receive the following initial input:<disp-formula id="equ28"><label>(27)</label><mml:math id="m28"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϑ</mml:mi><mml:mtext>kk</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϑ</mml:mi><mml:mtext>kk</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf365"><mml:msub><mml:mi>ϑ</mml:mi><mml:mtext>kk</mml:mtext></mml:msub></mml:math></inline-formula> is the angle between the pattern <inline-formula><mml:math id="inf366"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> and the center of mass <inline-formula><mml:math id="inf367"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msup></mml:math></inline-formula> of the cluster to which it belongs. The larger the concentration parameter κ for the generation of the artificial data set, the smaller the dispersion of the clusters, and thus the larger <inline-formula><mml:math id="inf368"><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϑ</mml:mi><mml:mtext>kk</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. If instead, an input pattern from cluster <inline-formula><mml:math id="inf369"><mml:mi>l</mml:mi></mml:math></inline-formula> is presented, that same DGC will receive a lower initial input:<disp-formula id="equ29"><label>(28)</label><mml:math id="m29"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>l</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ϑ</mml:mi><mml:mtext>kl</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>The approximation holds for a small dispersion of the clusters (large concentration parameter κ). We note that there is no subtraction of the recurrent input yet because output units are initialized with zero firing rate before each pattern presentation. By definition, similar patterns stimulate (initially) the same DGCs. A DGC can be active for two clusters only if its threshold is:<disp-formula id="equ30"><label>(29)</label><mml:math id="m30"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Therefore, with a high concentration parameter κ, patterns of different clusters are similar if <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>&lt;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo></mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>, while patterns of different clusters are distinct if <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>&gt;</mml:mo><mml:msqrt><mml:mrow><mml:mfrac><mml:mrow><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true" maxsize="142%" minsize="142%">||</mml:mo></mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>.</p></sec><sec id="s4-9-2"><title>Parameter choice</title><p>The upper bound of the expected L2-norm of the feedforward weight vector toward the DGCs at convergence can be computed, see <xref ref-type="disp-formula" rid="equ11">Equation (10)</xref>. With the parameters in <xref ref-type="table" rid="table1">Table 1</xref> (Simple network), the value is <inline-formula><mml:math id="inf372"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>⩽</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula>. Moreover, the input patterns for each cluster are highly concentrated; hence, their angle with the center of mass of the cluster they belong to is close to 0, so we have <inline-formula><mml:math id="inf373"><mml:mrow><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula>. Therefore, at convergence, a DGC selective for a given cluster <inline-formula><mml:math id="inf374"><mml:mi>k</mml:mi></mml:math></inline-formula> receives an input <inline-formula><mml:math id="inf375"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msup><mml:mi>i</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>≈</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> upon presentation of input patterns <inline-formula><mml:math id="inf376"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> belonging to cluster <inline-formula><mml:math id="inf377"><mml:mi>k</mml:mi></mml:math></inline-formula>. We choose <inline-formula><mml:math id="inf378"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1.2</mml:mn></mml:mrow></mml:math></inline-formula> to satisfy <xref ref-type="disp-formula" rid="equ26">Equation (25)</xref>. Given <italic>b</italic><sub><italic>i</italic></sub> the threshold value <inline-formula><mml:math id="inf379"><mml:msub><mml:mi>ξ</mml:mi><mml:mtext>thresh</mml:mtext></mml:msub></mml:math></inline-formula> for which two clusters are similar (and above which two clusters are distinct) can be determined by <xref ref-type="disp-formula" rid="equ30">Equation (29)</xref> : <inline-formula><mml:math id="inf380"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mtext>thresh</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. We created a handmade data set with <inline-formula><mml:math id="inf381"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula> for the case of similar clusters (therefore with similarity <inline-formula><mml:math id="inf382"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>), and a handmade data set with <inline-formula><mml:math id="inf383"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula> for the distinct case (hence with similarity <inline-formula><mml:math id="inf384"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>).</p><p>Let us suppose that the weights of DGC <inline-formula><mml:math id="inf385"><mml:mi>i</mml:mi></mml:math></inline-formula> have converged and made this cell respond to patterns of cluster <inline-formula><mml:math id="inf386"><mml:mi>i</mml:mi></mml:math></inline-formula>. If another DGC <inline-formula><mml:math id="inf387"><mml:mi>k</mml:mi></mml:math></inline-formula> of the network is selective for cluster <inline-formula><mml:math id="inf388"><mml:mi>k</mml:mi></mml:math></inline-formula>, cell <inline-formula><mml:math id="inf389"><mml:mi>i</mml:mi></mml:math></inline-formula> gets the input <inline-formula><mml:math id="inf390"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub><mml:mo>≈</mml:mo><mml:mfrac><mml:mn>1.5</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mtext>rec</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> upon presentation of input patterns <inline-formula><mml:math id="inf391"><mml:msup><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:msup></mml:math></inline-formula> belonging to cluster <inline-formula><mml:math id="inf392"><mml:mrow><mml:mi>k</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow></mml:math></inline-formula>. Hence, to satisfy <xref ref-type="disp-formula" rid="equ27">Equation (26)</xref>, we need <inline-formula><mml:math id="inf393"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub><mml:mo>&lt;</mml:mo><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1.5</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.24</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>. We set <inline-formula><mml:math id="inf394"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1.2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Furthermore, a newborn DGC is born with a null feedforward weight vector so that at birth, its input consists only of the indirect excitatory input from mature DGCs, which vanishes if all DGCs are quiescent and takes a value <inline-formula><mml:math id="inf395"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:mrow><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> if a mature DGC responds to the input. For the feedforward weight vector to grow, the newborn cell <inline-formula><mml:math id="inf396"><mml:mi>i</mml:mi></mml:math></inline-formula> needs to be active. This could be achieved through spontaneous activity that could be implemented by setting the intrinsic firing threshold at birth to a value <inline-formula><mml:math id="inf397"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mtext>birth</mml:mtext></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>. In this case, a difference between similar and distinct patterns is not expected. Alternatively, activity of newborn cells can be achieved in the absence of spontaneous activity under the condition <inline-formula><mml:math id="inf398"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mtext>birth</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>. For the simulations with the toy model, we set <inline-formula><mml:math id="inf399"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mtext>birth</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula>, which leads to weight growth in newborn cells for similar, but not distinct patterns.</p></sec><sec id="s4-9-3"><title>Neurogenesis with the artificial data set</title><p>To save computation time, we initialize the feedforward weight vectors of two mature DGCs at two training patterns randomly chosen from the first two clusters, normalized such that they have an L2-norm of 1.5. We then present patterns from clusters 1 and 2 and let the feedforward weights evolve according to <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> until they reach convergence.</p><p>We thereafter fix the feedforward weights onto the two mature cells and introduce a novel cluster of patterns as well as a newborn DGC in the network. The sequence of presentation of patterns from the three clusters (a novel one and two pretrained ones) is random. The newborn DGC is born with a null feedforward weight vector, and its maturation follows the same rules as before (plastic feedforward weights). In the early phase, GABAergic input has an excitatory effect (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>) and the newborn DGC does not inhibit the mature DGCs (<xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>). This is modeled by setting <inline-formula><mml:math id="inf400"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> for the connections from mature to newborn DGC, and <inline-formula><mml:math id="inf401"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for the connections from newborn to mature DGCs. The threshold of the newborn DGC starts at <inline-formula><mml:math id="inf402"><mml:mrow><mml:msub><mml:mi>b</mml:mi><mml:mtext>birth</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:math></inline-formula> at birth, mimicking enhanced excitability (<xref ref-type="bibr" rid="bib80">Schmidt-Hieber et al., 2004</xref>; <xref ref-type="bibr" rid="bib62">Li et al., 2017</xref>), and increases linearly up to 1.2 (same threshold as that of mature DGCs) over 12,000 pattern presentations, reflecting loss of excitability with maturation. The exact time window is not critical. In the late phase of maturation of the newborn DGC, GABAergic input switches to inhibitory (<xref ref-type="bibr" rid="bib40">Ge et al., 2006</xref>), and the newborn DGC recruits feedback inhibition onto mature DGCs (<xref ref-type="bibr" rid="bib88">Temprana et al., 2015</xref>). It is modeled by switching the sign of the connection from mature to newborn DGC: <inline-formula><mml:math id="inf403"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> and establishing connections from newborn to mature DGCs: <inline-formula><mml:math id="inf404"><mml:mrow><mml:msubsup><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mtext>rec</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula>. Each of the 6000 patterns is presented once during the early phase of maturation and once during the late phase of maturation.</p><p>The above paradigm is run separately for each of the two handmade data sets: the one where clusters are similar (<inline-formula><mml:math id="inf405"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>) and the one where clusters are distinct (<inline-formula><mml:math id="inf406"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-9-4"><title>Analytical computation of the L2-norm and angle</title><p>We consider the case where two mature DGCs have learned their synaptic connections, such that the first mature DGC with feedforward weight vector <inline-formula><mml:math id="inf407"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> is selective for cluster 1 with normalized center of mass <inline-formula><mml:math id="inf408"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msup></mml:math></inline-formula>, and the second mature DGC with feedforward weight vector <inline-formula><mml:math id="inf409"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> is selective for cluster 2 with normalized center of mass <inline-formula><mml:math id="inf410"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>. After convergence, we have <inline-formula><mml:math id="inf411"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf412"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf413"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>k</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is the expected L2-norm of the feedforward weight vector onto mature DGC <inline-formula><mml:math id="inf414"><mml:mi>k</mml:mi></mml:math></inline-formula> that is selective for pretrained cluster <inline-formula><mml:math id="inf415"><mml:mi>k</mml:mi></mml:math></inline-formula>. In addition, the upper bound for the L2-norm of the weight vectors of the mature DGCs can be determined <inline-formula><mml:math id="inf416"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⩽</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:math></inline-formula>. In our case, we obtain <inline-formula><mml:math id="inf417"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1.49</mml:mn></mml:mrow></mml:math></inline-formula> because of the dispersion of the patterns around their center of mass; hence, we will use this value for the numerical computations below.</p><p>We represent the feedforward weight vector <inline-formula><mml:math id="inf418"><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> onto a newborn DGC as an arrow of length <inline-formula><mml:math id="inf419"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). We compute analytically its L2-norm at the end of the early phase of maturation of the newborn DGC, as well as its angle φ with the center of mass of the novel cluster <inline-formula><mml:math id="inf420"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msup></mml:math></inline-formula>, to confirm the results obtained numerically (<xref ref-type="fig" rid="fig6">Figure 6</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>).</p><p>In the early phase of maturation, the feedforward weight vector onto the newborn DGC grows. The norm stabilizes at a higher value in the case of similar patterns (<inline-formula><mml:math id="inf421"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>) than in the case of distinct patterns (<inline-formula><mml:math id="inf422"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). It is due to the fact that the center of mass of three <italic>similar</italic> clusters lies closer to the surface of the sphere than the center of mass of two <italic>distinct</italic> clusters (see below). In the late phase of maturation, for similar clusters we observe a slight increase of the L2-norm of the feedforward weight vector onto the newborn DGC concomitantly with the decrease of angle with the center of mass of the novel cluster (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), because the center of mass of the novel cluster lies closer to the surface of the sphere than the center of mass of the three clusters.</p></sec><sec id="s4-9-5"><title>Similar clusters</title><p>The angle between the center of mass of any pair of similar clusters (<inline-formula><mml:math id="inf423"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf424"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>) is given by <xref ref-type="disp-formula" rid="equ16">Equation (15)</xref>:<disp-formula id="equ31"><label>(30)</label><mml:math id="m31"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>arccos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mn>0.2</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Half the distance between the projections of the center of mass of any pair of two similar clusters on a concentric sphere with radius <inline-formula><mml:math id="inf425"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is given by (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>):<disp-formula id="equ32"><label>(31)</label><mml:math id="m32"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The triangle that connects the centers of masses of the three clusters is equilateral, and <inline-formula><mml:math id="inf426"><mml:mi>y</mml:mi></mml:math></inline-formula> separates one of its angle in two equal parts (<inline-formula><mml:math id="inf427"><mml:mrow><mml:mi>π</mml:mi><mml:mo>/</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:math></inline-formula> [rad] each). So the length <inline-formula><mml:math id="inf428"><mml:mi>y</mml:mi></mml:math></inline-formula> can be calculated:<disp-formula id="equ33"><label>(32)</label><mml:math id="m33"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>6</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Using Pythagoras formula, we can thus determine the expected L2-norm <inline-formula><mml:math id="inf429"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> of the feedforward weight vector onto the newborn DGC at the end of the early phase of maturation:<disp-formula id="equ34"><label>(33)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>and finally its angle with the center of mass of the novel cluster:<disp-formula id="equ35"><label>(34)</label><mml:math id="m35"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>arccos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>The numerical values are as follows: <inline-formula><mml:math id="inf430"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1.47</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf431"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>≈</mml:mo><mml:mn>9.21</mml:mn><mml:mrow><mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which correspond to the values on <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>.</p></sec><sec id="s4-9-6"><title>Distinct clusters</title><p>In the case of distinct patterns (<inline-formula><mml:math id="inf432"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf433"><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:math></inline-formula>), the angle between the center of mass of any pair of clusters is given by <xref ref-type="disp-formula" rid="equ16">Equation (15)</xref>:<disp-formula id="equ36"><label>(35)</label><mml:math id="m36"><mml:mrow><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>arccos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mn>0.8</mml:mn><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>&gt;</mml:mo><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>S</mml:mi></mml:msub></mml:mrow></mml:math></disp-formula></p><p>We can directly compute the expected L2-norm of the feedforward weight vector onto the newborn DGC at the end of the early phase of maturation (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>):<disp-formula id="equ37"><label>(36)</label><mml:math id="m37"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>cos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We can then calculate the length <inline-formula><mml:math id="inf434"><mml:mi>z</mml:mi></mml:math></inline-formula> between the projection of the center of mass of one of the two pretrained clusters on a concentric sphere with radius <inline-formula><mml:math id="inf435"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> and the feedforward weight vector onto the newborn DGC:<disp-formula id="equ38"><label>(37)</label><mml:math id="m38"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi>sin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi mathvariant="normal">Ω</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>Analogous to the similar case, we observe that <inline-formula><mml:math id="inf436"><mml:mi>y</mml:mi></mml:math></inline-formula> separates one angle of the equilateral triangle connecting the projections of the center of mass of the clusters on the sphere in two equal parts, consequently:<disp-formula id="equ39"><label>(38)</label><mml:math id="m39"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>z</mml:mi><mml:mrow><mml:mi>tan</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>π</mml:mi><mml:mn>6</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Finally, the angle between the center of mass of the novel cluster and the feedforward weight vector onto the newborn DGC at the end of the early phase of maturation is:<disp-formula id="equ40"><label>(39)</label><mml:math id="m40"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>arccos</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>y</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>We obtain the following approximate values: <inline-formula><mml:math id="inf437"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mo fence="true">||</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>w</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mi>i</mml:mi></mml:msub><mml:mo fence="true">||</mml:mo></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>≈</mml:mo><mml:mn>1.34</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf438"><mml:mrow><mml:mi>φ</mml:mi><mml:mo>≈</mml:mo><mml:mn>47.2</mml:mn><mml:mrow><mml:msup><mml:mo stretchy="false">[</mml:mo><mml:mo>∘</mml:mo></mml:msup><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, which correspond to the values on <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>. The angle φ is smaller in the similar case than in the distinct case, hence the norm is larger in the similar case, as observed in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>.</p></sec></sec><sec id="s4-10"><title>Effective dimensionality and participation ratio</title><p>The effective dimensionality of the input is measured as the participation ratio (PR) defined as <inline-formula><mml:math id="inf439"><mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>⁢</mml:mo><mml:mi>R</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mtext>Tr</mml:mtext></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>C</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf440"><mml:mi>C</mml:mi></mml:math></inline-formula> is the covariance matrix of the input patterns, and <inline-formula><mml:math id="inf441"><mml:mrow><mml:mtext>Tr</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the trace of matrix <inline-formula><mml:math id="inf442"><mml:mi>C</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib70">Mazzucato et al., 2016</xref>; <xref ref-type="bibr" rid="bib64">Litwin-Kumar et al., 2017</xref>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Josef Bischofberger and Laurenz Wiskott for great discussions and useful remarks, as well as Paul Miller and an anonymous reviewer for constructive comments and suggestions. This research was supported by the Swiss National Science Foundation (no. 200020 184615) and by the European Union Horizon 2020 Framework Program under grant agreement no. 785907 (HumanBrain Project, SGA2).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Funding acquisition, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Classification performance for random combinations of digits.</title><p>The classification performance (<italic>P</italic><sub>0</sub>, <italic>P</italic><sub>1</sub>, <italic>P</italic><sub>2</sub>) is defined as the percentage of correctly classified patterns on the test set. The numbers m n + q (first column) indicate that MNIST digits m and n are used for pretraining (second column); m, n and q are used for pretraining (third column); or m and n are used for pretraining, and patterns from digit q added after neurogenesis (fourth column). <inline-formula><mml:math id="inf443"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (last column) is used for evaluating the contribution of neurogenesis to classification performance.</p></caption><media mime-subtype="x-tex" mimetype="application" xlink:href="elife-66463-supp1-v2.tex"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Comparison of networks with different numbers of inhibitory neurons.</title><p>The number of excitatory neurons is <inline-formula><mml:math id="inf444"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> for all three networks, and there are <inline-formula><mml:math id="inf445"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> inhibitory neurons. The case with <inline-formula><mml:math id="inf446"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the one presented in the main text. All other network parameters are unchanged (including <inline-formula><mml:math id="inf447"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mo>∗</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). Each network is pretrained once with digits 3 and 4. The percentage of active neurons (firing rate &gt; 1 Hz) for each testing pattern of the corresponding digit is given (mean ± standard deviation), as well as the classification performance over all testing patterns from the trained digits.</p></caption><media mime-subtype="x-tex" mimetype="application" xlink:href="elife-66463-supp2-v2.tex"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Network with 700 DGCs (expansion factor from EC to dentate gyrus of about 5) compared to the case with <inline-formula><mml:math id="inf448"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> as in the main text.</title><p>All other network parameters are unchanged. Each network is pretrained with digits 3 and 4. Note that only a subset of neurons responsive to digit 3 (or 4) get active (firing rate &gt; 1 Hz) for a given pattern 3 (or 4). Classification performance is evaluated over all test patterns from the trained digits. Top: after pretraining; bottom: late phase, after adding patterns from digit ‘5’. Either all unresponsive cells (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), or only a fraction of these (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>), have been replaced by newborn model cells. For the network with 700 DGCs, about 16-18% of DGCs are activated upon presentation of a digit 3 or 4 or 5 (about 112-126 model DGCs). If 119 newborn DGCs are plastic during presentation of the novel digit 5 (middle column), these can become selective for prototypes of digit 5 (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>) yielding a good classification performance while keeping 156 unresponsive DGCs available for future tasks. If only 35 newborn DGCs are available, classification performance is lower (right column).</p></caption><media mime-subtype="x-tex" mimetype="application" xlink:href="elife-66463-supp3-v2.tex"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Classification performance with plastic mature DGCs.</title><p>Top: Using the main neurogenesis network with <inline-formula><mml:math id="inf449"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> DGCs, we keep the learning rate of newborn DGCs at <inline-formula><mml:math id="inf450"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, but now set the learning rate of mature DGCs to nonzero values (<inline-formula><mml:math id="inf451"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mtext>mature</mml:mtext></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) throughout maturation of newborn DGCs. This enables us to vary the level of remaining plasticity in mature DGCs. The number of newborn DGCs that undergo neurogenesis (<inline-formula><mml:math id="inf452"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>newborn</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) is the same as in the main text. Overall classification performance for digits 3, 4, and 5 (<inline-formula><mml:math id="inf453"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) is computed at the end of the late phase of maturation of newborn DGCs, as well as the classification performance for digit 3 (<italic>P</italic><sub>3</sub>), digit 4 (<italic>P</italic><sub>4</sub>) and digit 5 (<italic>P</italic><sub>5</sub>). Bottom: Same with the extended neurogenesis network with <inline-formula><mml:math id="inf454"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>D</mml:mi><mml:mi>G</mml:mi><mml:mi>C</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>700</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The number of newborn DGCs is either set to <inline-formula><mml:math id="inf455"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>newborn</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>119</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (corresponding to 17% of newborn DGCs), or <inline-formula><mml:math id="inf456"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mtext>newborn</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>35</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (corresponding to 5% of newborn DGCs). The results with <inline-formula><mml:math id="inf457"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mtext>mature</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> from the main text are repeated here for convenience.</p></caption><media mime-subtype="x-tex" mimetype="application" xlink:href="elife-66463-supp4-v2.tex"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Comparison of the neurogenesis model and the random initialization model for different input dimensionalities.</title><p>The simplified model with <inline-formula><mml:math id="inf458"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (similar input clusters) is used. Pretraining with two clusters and subsequent learning of a novel cluster 3 (Neuro.) was performed in the same way as reported in the main text. After full maturation of the newborn DGC (two epochs), its weights were fixed, and patterns of a novel cluster 4 were introduced as well as another newborn DGC, and so on until all seven clusters were learned. Reconstruction errors were computed at the end of learning of all seven clusters, and compared with two cases where newborn DGCs do not undergo a two-phase maturation during their 2 epochs of learning, always stay plastic, and are born with a randomly initialized feedforward weight vector: one where the L2-norm of the weight vector starts at a low value of 0.1 (RandInitL.), and one where the L2-norm starts at 1.5, which is the upper bound for the length of the weight vector (RandInitH.). We compare the reconstruction error between the neurogenesis model and the random initialization models for different values of the effective input dimensionality (PR), which depends on the concentration parameter (κ) used when creating the artificial dataset. The results with the dataset used in the main text (<inline-formula><mml:math id="inf459"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, PR =11) are reported here for comparison.</p></caption><media mime-subtype="x-tex" mimetype="application" xlink:href="elife-66463-supp5-v2.tex"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-66463-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Simulation and plotting scripts can be found at: <ext-link ext-link-type="uri" xlink:href="https://github.com/ogozel/NeurogenesisModel">https://github.com/ogozel/NeurogenesisModel</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:e46f2dfc10c21d69ac057f31c5800f46644b004a">https://archive.softwareheritage.org/swh:1:rev:e46f2dfc10c21d69ac057f31c5800f46644b004a</ext-link>).</p><p>The following previously published dataset was used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Cortes</surname><given-names>C</given-names></name><name><surname>Burges</surname><given-names>CJC</given-names></name></person-group><year iso-8601-date="1999">1999</year><data-title>The MNIST database of handwritten digits</data-title><source>THE MNIST DATABASE</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="http://yann.lecun.com/exdb/mnist/">yann.lecun.com/exdb/mnist/</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Acsády</surname> <given-names>L</given-names></name><name><surname>Kamondi</surname> <given-names>A</given-names></name><name><surname>Sík</surname> <given-names>A</given-names></name><name><surname>Freund</surname> <given-names>T</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>GABAergic cells are the major postsynaptic targets of mossy fibers in the rat Hippocampus</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>3386</fpage><lpage>3403</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-09-03386.1998</pub-id><pub-id pub-id-type="pmid">9547246</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aimone</surname> <given-names>JB</given-names></name><name><surname>Wiles</surname> <given-names>J</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Computational influence of adult neurogenesis on memory encoding</article-title><source>Neuron</source><volume>61</volume><fpage>187</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.11.026</pub-id><pub-id pub-id-type="pmid">19186162</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aimone</surname> <given-names>JB</given-names></name><name><surname>Deng</surname> <given-names>W</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Resolving new memories: a critical look at the Dentate Gyrus, adult neurogenesis, and pattern separation</article-title><source>Neuron</source><volume>70</volume><fpage>589</fpage><lpage>596</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.010</pub-id><pub-id pub-id-type="pmid">21609818</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Akers</surname> <given-names>KG</given-names></name><name><surname>Martinez-Canabal</surname> <given-names>A</given-names></name><name><surname>Restivo</surname> <given-names>L</given-names></name><name><surname>Yiu</surname> <given-names>AP</given-names></name><name><surname>De Cristofaro</surname> <given-names>A</given-names></name><name><surname>Hsiang</surname> <given-names>HL</given-names></name><name><surname>Wheeler</surname> <given-names>AL</given-names></name><name><surname>Guskjolen</surname> <given-names>A</given-names></name><name><surname>Niibori</surname> <given-names>Y</given-names></name><name><surname>Shoji</surname> <given-names>H</given-names></name><name><surname>Ohira</surname> <given-names>K</given-names></name><name><surname>Richards</surname> <given-names>BA</given-names></name><name><surname>Miyakawa</surname> <given-names>T</given-names></name><name><surname>Josselyn</surname> <given-names>SA</given-names></name><name><surname>Frankland</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Hippocampal neurogenesis regulates forgetting during adulthood and infancy</article-title><source>Science</source><volume>344</volume><fpage>598</fpage><lpage>602</lpage><pub-id pub-id-type="doi">10.1126/science.1248903</pub-id><pub-id pub-id-type="pmid">24812394</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albus</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>A theory of cerebellar function</article-title><source>Mathematical Biosciences</source><volume>10</volume><fpage>25</fpage><lpage>61</lpage><pub-id pub-id-type="doi">10.1016/0025-5564(71)90051-4</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aljadeff</surname> <given-names>J</given-names></name><name><surname>Stern</surname> <given-names>M</given-names></name><name><surname>Sharpee</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Transition to chaos in random networks with cell-type-specific connectivity</article-title><source>Physical Review Letters</source><volume>114</volume><elocation-id>088101</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevLett.114.088101</pub-id><pub-id pub-id-type="pmid">25768781</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alvarez</surname> <given-names>DD</given-names></name><name><surname>Giacomini</surname> <given-names>D</given-names></name><name><surname>Yang</surname> <given-names>SM</given-names></name><name><surname>Trinchero</surname> <given-names>MF</given-names></name><name><surname>Temprana</surname> <given-names>SG</given-names></name><name><surname>Büttner</surname> <given-names>KA</given-names></name><name><surname>Beltramone</surname> <given-names>N</given-names></name><name><surname>Schinder</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A disynaptic feedback network activated by experience promotes the integration of new granule cells</article-title><source>Science</source><volume>354</volume><fpage>459</fpage><lpage>465</lpage><pub-id pub-id-type="doi">10.1126/science.aaf2156</pub-id><pub-id pub-id-type="pmid">27789840</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amaral</surname> <given-names>DG</given-names></name><name><surname>Scharfman</surname> <given-names>HE</given-names></name><name><surname>Lavenex</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The dentate gyrus: fundamental neuroanatomical organization (dentate gyrus for dummies)</article-title><source>Progress in Brain Research</source><volume>163</volume><fpage>3</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(07)63001-5</pub-id><pub-id pub-id-type="pmid">17765709</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Andersen</surname> <given-names>P</given-names></name><name><surname>Morris</surname> <given-names>R</given-names></name><name><surname>Amaral</surname> <given-names>D</given-names></name><name><surname>Bliss</surname> <given-names>T</given-names></name><name><surname>O’Keefe</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>The Hippocampus Book</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195100273.001.0001</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Appleby</surname> <given-names>PA</given-names></name><name><surname>Wiskott</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Additive neurogenesis as a strategy for avoiding interference in a sparsely-coding dentate gyrus</article-title><source>Network: Computation in Neural Systems</source><volume>20</volume><fpage>137</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1080/09548980902993156</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Artola</surname> <given-names>A</given-names></name><name><surname>Bröcher</surname> <given-names>S</given-names></name><name><surname>Singer</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Different voltage-dependent thresholds for inducing long-term depression and long-term potentiation in slices of rat visual cortex</article-title><source>Nature</source><volume>347</volume><fpage>69</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1038/347069a0</pub-id><pub-id pub-id-type="pmid">1975639</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babadi</surname> <given-names>B</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparseness and expansion in sensory representations</article-title><source>Neuron</source><volume>83</volume><fpage>1213</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.035</pub-id><pub-id pub-id-type="pmid">25155954</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barmashenko</surname> <given-names>G</given-names></name><name><surname>Hefft</surname> <given-names>S</given-names></name><name><surname>Aertsen</surname> <given-names>A</given-names></name><name><surname>Kirschstein</surname> <given-names>T</given-names></name><name><surname>Köhling</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Positive shifts of the GABAA receptor reversal potential due to altered chloride homeostasis is widespread after status epilepticus</article-title><source>Epilepsia</source><volume>52</volume><fpage>1570</fpage><lpage>1578</lpage><pub-id pub-id-type="doi">10.1111/j.1528-1167.2011.03247.x</pub-id><pub-id pub-id-type="pmid">21899534</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Becker</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A computational principle for hippocampal learning and neurogenesis</article-title><source>Hippocampus</source><volume>15</volume><fpage>722</fpage><lpage>738</lpage><pub-id pub-id-type="doi">10.1002/hipo.20095</pub-id><pub-id pub-id-type="pmid">15986407</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Ari</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Excitatory actions of gaba during development: the nature of the nurture</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>728</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1038/nrn920</pub-id><pub-id pub-id-type="pmid">12209121</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bienenstock</surname> <given-names>EL</given-names></name><name><surname>Cooper</surname> <given-names>LN</given-names></name><name><surname>Munro</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex</article-title><source>The Journal of Neuroscience</source><volume>2</volume><fpage>32</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.02-01-00032.1982</pub-id><pub-id pub-id-type="pmid">7054394</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cameron</surname> <given-names>HA</given-names></name><name><surname>McKay</surname> <given-names>RD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Adult neurogenesis produces a large pool of new granule cells in the dentate gyrus</article-title><source>The Journal of Comparative Neurology</source><volume>435</volume><fpage>406</fpage><lpage>417</lpage><pub-id pub-id-type="doi">10.1002/cne.1040</pub-id><pub-id pub-id-type="pmid">11406822</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carleton</surname> <given-names>A</given-names></name><name><surname>Petreanu</surname> <given-names>LT</given-names></name><name><surname>Lansford</surname> <given-names>R</given-names></name><name><surname>Alvarez-Buylla</surname> <given-names>A</given-names></name><name><surname>Lledo</surname> <given-names>PM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Becoming a new neuron in the adult olfactory bulb</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>507</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1038/nn1048</pub-id><pub-id pub-id-type="pmid">12704391</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname> <given-names>RA</given-names></name><name><surname>Potenza</surname> <given-names>MN</given-names></name><name><surname>Hoffman</surname> <given-names>RE</given-names></name><name><surname>Miranker</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Simulated apoptosis/neurogenesis regulates learning and memory capabilities of adaptive neural networks</article-title><source>Neuropsychopharmacology</source><volume>29</volume><fpage>747</fpage><lpage>758</lpage><pub-id pub-id-type="doi">10.1038/sj.npp.1300358</pub-id><pub-id pub-id-type="pmid">14702022</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chambers</surname> <given-names>RA</given-names></name><name><surname>Conroy</surname> <given-names>SK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Network modeling of adult neurogenesis: shifting rates of neuronal turnover optimally gears network learning according to novelty gradient</article-title><source>Journal of Cognitive Neuroscience</source><volume>19</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1162/jocn.2007.19.1.1</pub-id><pub-id pub-id-type="pmid">17214558</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chancey</surname> <given-names>JH</given-names></name><name><surname>Adlaf</surname> <given-names>EW</given-names></name><name><surname>Sapp</surname> <given-names>MC</given-names></name><name><surname>Pugh</surname> <given-names>PC</given-names></name><name><surname>Wadiche</surname> <given-names>JI</given-names></name><name><surname>Overstreet-Wadiche</surname> <given-names>LS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>GABA depolarization is required for experience-dependent synapse unsilencing in adult-born neurons</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>6614</fpage><lpage>6622</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0781-13.2013</pub-id><pub-id pub-id-type="pmid">23575858</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chawla</surname> <given-names>MK</given-names></name><name><surname>Guzowski</surname> <given-names>JF</given-names></name><name><surname>Ramirez-Amaya</surname> <given-names>V</given-names></name><name><surname>Lipa</surname> <given-names>P</given-names></name><name><surname>Hoffman</surname> <given-names>KL</given-names></name><name><surname>Marriott</surname> <given-names>LK</given-names></name><name><surname>Worley</surname> <given-names>PF</given-names></name><name><surname>McNaughton</surname> <given-names>BL</given-names></name><name><surname>Barnes</surname> <given-names>CA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Sparse, environmentally selective expression of Arc RNA in the upper blade of the rodent fascia dentata by brief spatial experience</article-title><source>Hippocampus</source><volume>15</volume><fpage>579</fpage><lpage>586</lpage><pub-id pub-id-type="doi">10.1002/hipo.20091</pub-id><pub-id pub-id-type="pmid">15920719</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chistiakova</surname> <given-names>M</given-names></name><name><surname>Bannon</surname> <given-names>NM</given-names></name><name><surname>Bazhenov</surname> <given-names>M</given-names></name><name><surname>Volgushev</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Heterosynaptic plasticity: multiple mechanisms and multiple roles</article-title><source>The Neuroscientist : A Review Journal Bringing Neurobiology, Neurology and Psychiatry</source><volume>20</volume><fpage>483</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1177/1073858414529829</pub-id><pub-id pub-id-type="pmid">24727248</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clelland</surname> <given-names>CD</given-names></name><name><surname>Choi</surname> <given-names>M</given-names></name><name><surname>Romberg</surname> <given-names>C</given-names></name><name><surname>Clemenson</surname> <given-names>GD</given-names></name><name><surname>Fragniere</surname> <given-names>A</given-names></name><name><surname>Tyers</surname> <given-names>P</given-names></name><name><surname>Jessberger</surname> <given-names>S</given-names></name><name><surname>Saksida</surname> <given-names>LM</given-names></name><name><surname>Barker</surname> <given-names>RA</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name><name><surname>Bussey</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A functional role for adult hippocampal neurogenesis in spatial pattern separation</article-title><source>Science</source><volume>325</volume><fpage>210</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1126/science.1173215</pub-id><pub-id pub-id-type="pmid">19590004</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clopath</surname> <given-names>C</given-names></name><name><surname>Büsing</surname> <given-names>L</given-names></name><name><surname>Vasilaki</surname> <given-names>E</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Connectivity reflects coding: a model of voltage-based STDP with homeostasis</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>344</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1038/nn.2479</pub-id><pub-id pub-id-type="pmid">20098420</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crick</surname> <given-names>C</given-names></name><name><surname>Miranker</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Apoptosis, neurogenesis, and information content in Hebbian networks</article-title><source>Biological Cybernetics</source><volume>94</volume><fpage>9</fpage><lpage>19</lpage><pub-id pub-id-type="doi">10.1007/s00422-005-0026-8</pub-id><pub-id pub-id-type="pmid">16372165</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danielson</surname> <given-names>NB</given-names></name><name><surname>Kaifosh</surname> <given-names>P</given-names></name><name><surname>Zaremba</surname> <given-names>JD</given-names></name><name><surname>Lovett-Barron</surname> <given-names>M</given-names></name><name><surname>Tsai</surname> <given-names>J</given-names></name><name><surname>Denny</surname> <given-names>CA</given-names></name><name><surname>Balough</surname> <given-names>EM</given-names></name><name><surname>Goldberg</surname> <given-names>AR</given-names></name><name><surname>Drew</surname> <given-names>LJ</given-names></name><name><surname>Hen</surname> <given-names>R</given-names></name><name><surname>Losonczy</surname> <given-names>A</given-names></name><name><surname>Kheirbek</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Distinct contribution of adult-born hippocampal granule cells to context encoding</article-title><source>Neuron</source><volume>90</volume><fpage>101</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.019</pub-id><pub-id pub-id-type="pmid">26971949</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Theoretical Neuroscience</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dayer</surname> <given-names>AG</given-names></name><name><surname>Ford</surname> <given-names>AA</given-names></name><name><surname>Cleaver</surname> <given-names>KM</given-names></name><name><surname>Yassaee</surname> <given-names>M</given-names></name><name><surname>Cameron</surname> <given-names>HA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Short-term and long-term survival of new neurons in the rat dentate gyrus</article-title><source>The Journal of Comparative Neurology</source><volume>460</volume><fpage>563</fpage><lpage>572</lpage><pub-id pub-id-type="doi">10.1002/cne.10675</pub-id><pub-id pub-id-type="pmid">12717714</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DeCostanzo</surname> <given-names>AJ</given-names></name><name><surname>Fung</surname> <given-names>CCA</given-names></name><name><surname>Fukai</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Hippocampal neurogenesis reduces the dimensionality of sparsely coded representations to enhance memory encoding</article-title><source>Frontiers in Computational Neuroscience</source><volume>12</volume><elocation-id>99</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2018.00099</pub-id><pub-id pub-id-type="pmid">30666194</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deng</surname> <given-names>W</given-names></name><name><surname>Aimone</surname> <given-names>JB</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>New neurons and new memories: how does adult hippocampal neurogenesis affect learning and memory?</article-title><source>Nature Reviews Neuroscience</source><volume>11</volume><fpage>339</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1038/nrn2822</pub-id><pub-id pub-id-type="pmid">20354534</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deshpande</surname> <given-names>A</given-names></name><name><surname>Bergami</surname> <given-names>M</given-names></name><name><surname>Ghanem</surname> <given-names>A</given-names></name><name><surname>Conzelmann</surname> <given-names>KK</given-names></name><name><surname>Lepier</surname> <given-names>A</given-names></name><name><surname>Götz</surname> <given-names>M</given-names></name><name><surname>Berninger</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Retrograde monosynaptic tracing reveals the temporal evolution of inputs onto new neurons in the adult dentate gyrus and olfactory bulb</article-title><source>PNAS</source><volume>110</volume><fpage>E1152</fpage><lpage>E1161</lpage><pub-id pub-id-type="doi">10.1073/pnas.1218991110</pub-id><pub-id pub-id-type="pmid">23487772</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>DeSieno</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Adding a conscience to competitive learning</article-title><conf-name>IEEE International Conference on Neural Networks</conf-name><fpage>117</fpage><lpage>124</lpage><pub-id pub-id-type="doi">10.1109/ICNN.1988.23839</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Du</surname> <given-names>KL</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Clustering: a neural network approach</article-title><source>Neural Networks</source><volume>23</volume><fpage>89</fpage><lpage>107</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2009.08.007</pub-id><pub-id pub-id-type="pmid">19758784</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Finnegan</surname> <given-names>R</given-names></name><name><surname>Becker</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neurogenesis paradoxically decreases both pattern separation and memory interference</article-title><source>Frontiers in Systems Neuroscience</source><volume>9</volume><elocation-id>136</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2015.00136</pub-id><pub-id pub-id-type="pmid">26500511</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname> <given-names>TF</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Interneurons of the hippocampus</article-title><source>Hippocampus</source><volume>6</volume><fpage>347</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1996)6:4&lt;347::AID-HIPO1&gt;3.0.CO;2-I</pub-id><pub-id pub-id-type="pmid">8915675</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Furukawa</surname> <given-names>M</given-names></name><name><surname>Tsukahara</surname> <given-names>T</given-names></name><name><surname>Tomita</surname> <given-names>K</given-names></name><name><surname>Iwai</surname> <given-names>H</given-names></name><name><surname>Sonomura</surname> <given-names>T</given-names></name><name><surname>Miyawaki</surname> <given-names>S</given-names></name><name><surname>Sato</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neonatal maternal separation delays the GABA excitatory-to-inhibitory functional switch by inhibiting KCC2 expression</article-title><source>Biochemical and Biophysical Research Communications</source><volume>493</volume><fpage>1243</fpage><lpage>1249</lpage><pub-id pub-id-type="doi">10.1016/j.bbrc.2017.09.143</pub-id><pub-id pub-id-type="pmid">28962859</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fyhn</surname> <given-names>M</given-names></name><name><surname>Hafting</surname> <given-names>T</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name><name><surname>Moser</surname> <given-names>EI</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Hippocampal remapping and grid realignment in entorhinal cortex</article-title><source>Nature</source><volume>446</volume><fpage>190</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1038/nature05601</pub-id><pub-id pub-id-type="pmid">17322902</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ganguly</surname> <given-names>K</given-names></name><name><surname>Schinder</surname> <given-names>AF</given-names></name><name><surname>Wong</surname> <given-names>ST</given-names></name><name><surname>Poo</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>GABA itself promotes the developmental switch of neuronal GABAergic responses from excitation to inhibition</article-title><source>Cell</source><volume>105</volume><fpage>521</fpage><lpage>532</lpage><pub-id pub-id-type="doi">10.1016/S0092-8674(01)00341-5</pub-id><pub-id pub-id-type="pmid">11371348</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ge</surname> <given-names>S</given-names></name><name><surname>Goh</surname> <given-names>EL</given-names></name><name><surname>Sailor</surname> <given-names>KA</given-names></name><name><surname>Kitabatake</surname> <given-names>Y</given-names></name><name><surname>Ming</surname> <given-names>GL</given-names></name><name><surname>Song</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>GABA regulates synaptic integration of newly generated neurons in the adult brain</article-title><source>Nature</source><volume>439</volume><fpage>589</fpage><lpage>593</lpage><pub-id pub-id-type="doi">10.1038/nature04404</pub-id><pub-id pub-id-type="pmid">16341203</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ge</surname> <given-names>S</given-names></name><name><surname>Yang</surname> <given-names>CH</given-names></name><name><surname>Hsu</surname> <given-names>KS</given-names></name><name><surname>Ming</surname> <given-names>GL</given-names></name><name><surname>Song</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A critical period for enhanced synaptic plasticity in newly generated neurons of the adult brain</article-title><source>Neuron</source><volume>54</volume><fpage>559</fpage><lpage>566</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.05.002</pub-id><pub-id pub-id-type="pmid">17521569</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gerstner</surname> <given-names>W</given-names></name><name><surname>Kistler</surname> <given-names>W</given-names></name><name><surname>Naud</surname> <given-names>R</given-names></name><name><surname>Paninski</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Neuronal Dynamics: From Single Neurons to Networks and Models of Cognition</source><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9781107447615</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname> <given-names>PE</given-names></name><name><surname>Kesner</surname> <given-names>RP</given-names></name><name><surname>Lee</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dissociating hippocampal subregions: double dissociation between dentate gyrus and CA1</article-title><source>Hippocampus</source><volume>11</volume><fpage>626</fpage><lpage>636</lpage><pub-id pub-id-type="doi">10.1002/hipo.1077</pub-id><pub-id pub-id-type="pmid">11811656</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Groisman</surname> <given-names>AI</given-names></name><name><surname>Yang</surname> <given-names>SM</given-names></name><name><surname>Schinder</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Differential coupling of adult-born granule cells to parvalbumin and somatostatin interneurons</article-title><source>Cell Reports</source><volume>30</volume><fpage>202</fpage><lpage>214</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2019.12.005</pub-id><pub-id pub-id-type="pmid">31914387</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grossberg</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Adaptive pattern classification and universal recoding II: feedback, expectation, olfaction, illusions</article-title><source>Biological Cybernetics</source><volume>23</volume><fpage>187</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1007/BF00340335</pub-id><pub-id pub-id-type="pmid">963125</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grossberg</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><source>The Adaptive Brain I: Cognition, Learning, Reinforcement, and Rhythm</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Heigele</surname> <given-names>S</given-names></name><name><surname>Sultan</surname> <given-names>S</given-names></name><name><surname>Toni</surname> <given-names>N</given-names></name><name><surname>Bischofberger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Bidirectional GABAergic control of action potential firing in newborn hippocampal granule cells</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>263</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1038/nn.4218</pub-id><pub-id pub-id-type="pmid">26752162</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henze</surname> <given-names>DA</given-names></name><name><surname>Wittner</surname> <given-names>L</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Single granule cells reliably discharge targets in the hippocampal CA3 network in vivo</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>790</fpage><lpage>795</lpage><pub-id pub-id-type="doi">10.1038/nn887</pub-id><pub-id pub-id-type="pmid">12118256</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hertz</surname> <given-names>J</given-names></name><name><surname>Krogh</surname> <given-names>A</given-names></name><name><surname>Palmer</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>Introduction to the Theory of Neural Computation</source><publisher-name>Addison-Wesley</publisher-name></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Houser</surname> <given-names>CR</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Interneurons of the dentate gyrus: an overview of cell types, terminal fields and neurochemical identity</article-title><source>Progress in Brain Research</source><volume>163</volume><fpage>217</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/S0079-6123(07)63013-1</pub-id><pub-id pub-id-type="pmid">17765721</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunsaker</surname> <given-names>MR</given-names></name><name><surname>Kesner</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Evaluating the differential roles of the dorsal dentate gyrus, dorsal CA3, and dorsal CA1 during a temporal ordering for spatial locations task</article-title><source>Hippocampus</source><volume>18</volume><fpage>955</fpage><lpage>964</lpage><pub-id pub-id-type="doi">10.1002/hipo.20455</pub-id><pub-id pub-id-type="pmid">18493930</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jarrard</surname> <given-names>LE</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>On the role of the hippocampus in learning and memory in the rat</article-title><source>Behavioral and Neural Biology</source><volume>60</volume><fpage>9</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1016/0163-1047(93)90664-4</pub-id><pub-id pub-id-type="pmid">8216164</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jessberger</surname> <given-names>S</given-names></name><name><surname>Clark</surname> <given-names>RE</given-names></name><name><surname>Broadbent</surname> <given-names>NJ</given-names></name><name><surname>Clemenson</surname> <given-names>GD</given-names></name><name><surname>Consiglio</surname> <given-names>A</given-names></name><name><surname>Lie</surname> <given-names>DC</given-names></name><name><surname>Squire</surname> <given-names>LR</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dentate gyrus-specific knockdown of adult neurogenesis impairs spatial and object recognition memory in adult rats</article-title><source>Learning &amp; Memory</source><volume>16</volume><fpage>147</fpage><lpage>154</lpage><pub-id pub-id-type="doi">10.1101/lm.1172609</pub-id><pub-id pub-id-type="pmid">19181621</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname> <given-names>ST</given-names></name><name><surname>Shtrahman</surname> <given-names>M</given-names></name><name><surname>Parylak</surname> <given-names>S</given-names></name><name><surname>Gonçalves</surname> <given-names>JT</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Paradox of pattern separation and adult neurogenesis: a dual role for new neurons balancing memory resolution and robustness</article-title><source>Neurobiology of Learning and Memory</source><volume>129</volume><fpage>60</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2015.10.013</pub-id><pub-id pub-id-type="pmid">26549627</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kee</surname> <given-names>N</given-names></name><name><surname>Teixeira</surname> <given-names>CM</given-names></name><name><surname>Wang</surname> <given-names>AH</given-names></name><name><surname>Frankland</surname> <given-names>PW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Preferential incorporation of adult-generated granule cells into spatial memory networks in the dentate gyrus</article-title><source>Nature Neuroscience</source><volume>10</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/nn1847</pub-id><pub-id pub-id-type="pmid">17277773</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khazipov</surname> <given-names>R</given-names></name><name><surname>Khalilov</surname> <given-names>I</given-names></name><name><surname>Tyzio</surname> <given-names>R</given-names></name><name><surname>Morozova</surname> <given-names>E</given-names></name><name><surname>Ben-Ari</surname> <given-names>Y</given-names></name><name><surname>Holmes</surname> <given-names>GL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Developmental changes in GABAergic actions and seizure susceptibility in the rat hippocampus</article-title><source>European Journal of Neuroscience</source><volume>19</volume><fpage>590</fpage><lpage>600</lpage><pub-id pub-id-type="doi">10.1111/j.0953-816X.2003.03152.x</pub-id><pub-id pub-id-type="pmid">14984409</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klausberger</surname> <given-names>T</given-names></name><name><surname>Somogyi</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neuronal diversity and temporal dynamics: the unity of hippocampal circuit operations</article-title><source>Science</source><volume>321</volume><fpage>53</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1126/science.1149381</pub-id><pub-id pub-id-type="pmid">18599766</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kohonen</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="1989">1989</year><source>Self-Organization and Associative Memory</source><publisher-name>Springer-Verlag</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-88163-3</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lecun</surname> <given-names>Y</given-names></name><name><surname>Bottou</surname> <given-names>L</given-names></name><name><surname>Bengio</surname> <given-names>Y</given-names></name><name><surname>Haffner</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Gradient-based learning applied to document recognition</article-title><source>Proceedings of the IEEE</source><volume>86</volume><fpage>2278</fpage><lpage>2324</lpage><pub-id pub-id-type="doi">10.1109/5.726791</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leonzino</surname> <given-names>M</given-names></name><name><surname>Busnelli</surname> <given-names>M</given-names></name><name><surname>Antonucci</surname> <given-names>F</given-names></name><name><surname>Verderio</surname> <given-names>C</given-names></name><name><surname>Mazzanti</surname> <given-names>M</given-names></name><name><surname>Chini</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The timing of the excitatory-to-inhibitory GABA switch is regulated by the oxytocin receptor via KCC2</article-title><source>Cell Reports</source><volume>15</volume><fpage>96</fpage><lpage>103</lpage><pub-id pub-id-type="doi">10.1016/j.celrep.2016.03.013</pub-id><pub-id pub-id-type="pmid">27052180</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>Y</given-names></name><name><surname>Stam</surname> <given-names>FJ</given-names></name><name><surname>Aimone</surname> <given-names>JB</given-names></name><name><surname>Goulding</surname> <given-names>M</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Molecular layer perforant path-associated cells contribute to feed-forward inhibition in the adult dentate gyrus</article-title><source>PNAS</source><volume>110</volume><fpage>9106</fpage><lpage>9111</lpage><pub-id pub-id-type="doi">10.1073/pnas.1306912110</pub-id><pub-id pub-id-type="pmid">23671081</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>L</given-names></name><name><surname>Sultan</surname> <given-names>S</given-names></name><name><surname>Heigele</surname> <given-names>S</given-names></name><name><surname>Schmidt-Salzmann</surname> <given-names>C</given-names></name><name><surname>Toni</surname> <given-names>N</given-names></name><name><surname>Bischofberger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Silent synapses generate sparse and orthogonal action potential firing in adult-born hippocampal granule cells</article-title><source>eLife</source><volume>6</volume><elocation-id>e23612</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.23612</pub-id><pub-id pub-id-type="pmid">28826488</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>HQ</given-names></name><name><surname>Pratelli</surname> <given-names>M</given-names></name><name><surname>Godavarthi</surname> <given-names>S</given-names></name><name><surname>Zambetti</surname> <given-names>S</given-names></name><name><surname>Spitzer</surname> <given-names>NC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Decoding neurotransmitter switching: the road forward</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>4078</fpage><lpage>4089</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0005-20.2020</pub-id><pub-id pub-id-type="pmid">32434858</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname> <given-names>A</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Axel</surname> <given-names>R</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Optimal degrees of synaptic connectivity</article-title><source>Neuron</source><volume>93</volume><fpage>1153</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.01.030</pub-id><pub-id pub-id-type="pmid">28215558</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lynch</surname> <given-names>GS</given-names></name><name><surname>Dunwiddie</surname> <given-names>T</given-names></name><name><surname>Gribkoff</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Heterosynaptic depression: a postsynaptic correlate of long-term potentiation</article-title><source>Nature</source><volume>266</volume><fpage>737</fpage><lpage>739</lpage><pub-id pub-id-type="doi">10.1038/266737a0</pub-id><pub-id pub-id-type="pmid">195211</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mardia</surname> <given-names>KV</given-names></name><name><surname>Jupp</surname> <given-names>PE</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Directional Statistics</source><publisher-name>John Wiley &amp; Sons</publisher-name><pub-id pub-id-type="doi">10.1002/9780470316979</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marín-Burgin</surname> <given-names>A</given-names></name><name><surname>Mongiat</surname> <given-names>LA</given-names></name><name><surname>Pardi</surname> <given-names>MB</given-names></name><name><surname>Schinder</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Unique processing during a period of high excitation/inhibition balance in adult-born neurons</article-title><source>Science</source><volume>335</volume><fpage>1238</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.1126/science.1214956</pub-id><pub-id pub-id-type="pmid">22282476</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>A theory of cerebellar cortex</article-title><source>The Journal of Physiology</source><volume>202</volume><fpage>437</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1969.sp008820</pub-id><pub-id pub-id-type="pmid">5784296</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Simple memory: a theory for archicortex</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>262</volume><fpage>23</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1098/rstb.1971.0078</pub-id><pub-id pub-id-type="pmid">4399412</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzucato</surname> <given-names>L</given-names></name><name><surname>Fontanini</surname> <given-names>A</given-names></name><name><surname>La Camera</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stimuli reduce the dimensionality of cortical activity</article-title><source>Frontiers in Systems Neuroscience</source><volume>10</volume><elocation-id>11</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2016.00011</pub-id><pub-id pub-id-type="pmid">26924968</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McHugh</surname> <given-names>TJ</given-names></name><name><surname>Jones</surname> <given-names>MW</given-names></name><name><surname>Quinn</surname> <given-names>JJ</given-names></name><name><surname>Balthasar</surname> <given-names>N</given-names></name><name><surname>Coppari</surname> <given-names>R</given-names></name><name><surname>Elmquist</surname> <given-names>JK</given-names></name><name><surname>Lowell</surname> <given-names>BB</given-names></name><name><surname>Fanselow</surname> <given-names>MS</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name><name><surname>Tonegawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Dentate gyrus NMDA receptors mediate rapid pattern separation in the hippocampal network</article-title><source>Science</source><volume>317</volume><fpage>94</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1126/science.1140263</pub-id><pub-id pub-id-type="pmid">17556551</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>KD</given-names></name><name><surname>Fumarola</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mathematical equivalence of two common forms of firing rate models of neural networks</article-title><source>Neural Computation</source><volume>24</volume><fpage>25</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1162/NECO_a_00221</pub-id><pub-id pub-id-type="pmid">22023194</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Owens</surname> <given-names>DF</given-names></name><name><surname>Kriegstein</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Is there more to GABA than synaptic inhibition?</article-title><source>Nature Reviews Neuroscience</source><volume>3</volume><fpage>715</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1038/nrn919</pub-id><pub-id pub-id-type="pmid">12209120</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pathak</surname> <given-names>HR</given-names></name><name><surname>Weissinger</surname> <given-names>F</given-names></name><name><surname>Terunuma</surname> <given-names>M</given-names></name><name><surname>Carlson</surname> <given-names>GC</given-names></name><name><surname>Hsu</surname> <given-names>FC</given-names></name><name><surname>Moss</surname> <given-names>SJ</given-names></name><name><surname>Coulter</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Disrupted dentate granule cell chloride regulation enhances synaptic excitability during development of temporal lobe epilepsy</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>14012</fpage><lpage>14022</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4390-07.2007</pub-id><pub-id pub-id-type="pmid">18094240</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pfister</surname> <given-names>JP</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Triplets of spikes in a model of spike timing-dependent plasticity</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>9673</fpage><lpage>9682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1425-06.2006</pub-id><pub-id pub-id-type="pmid">16988038</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rolls</surname> <given-names>ET</given-names></name><name><surname>Treves</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Neural Networks and Brain Function</source><publisher-loc>Oxford</publisher-loc><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780198524328.001.0001</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rumelhart</surname> <given-names>DE</given-names></name><name><surname>Zipser</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Feature discovery by competitive learning</article-title><source>Cognitive Science</source><volume>9</volume><fpage>75</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1207/s15516709cog0901_5</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahay</surname> <given-names>A</given-names></name><name><surname>Scobie</surname> <given-names>KN</given-names></name><name><surname>Hill</surname> <given-names>AS</given-names></name><name><surname>O'Carroll</surname> <given-names>CM</given-names></name><name><surname>Kheirbek</surname> <given-names>MA</given-names></name><name><surname>Burghardt</surname> <given-names>NS</given-names></name><name><surname>Fenton</surname> <given-names>AA</given-names></name><name><surname>Dranovsky</surname> <given-names>A</given-names></name><name><surname>Hen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011a</year><article-title>Increasing adult hippocampal neurogenesis is sufficient to improve pattern separation</article-title><source>Nature</source><volume>472</volume><fpage>466</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1038/nature09817</pub-id><pub-id pub-id-type="pmid">21460835</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sahay</surname> <given-names>A</given-names></name><name><surname>Wilson</surname> <given-names>DA</given-names></name><name><surname>Hen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011b</year><article-title>Pattern separation: a common function for new neurons in hippocampus and olfactory bulb</article-title><source>Neuron</source><volume>70</volume><fpage>582</fpage><lpage>588</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.05.012</pub-id><pub-id pub-id-type="pmid">21609817</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt-Hieber</surname> <given-names>C</given-names></name><name><surname>Jonas</surname> <given-names>P</given-names></name><name><surname>Bischofberger</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Enhanced synaptic plasticity in newly generated granule cells of the adult hippocampus</article-title><source>Nature</source><volume>429</volume><fpage>184</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1038/nature02553</pub-id><pub-id pub-id-type="pmid">15107864</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Senzai</surname> <given-names>Y</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Physiological properties and behavioral correlates of hippocampal granule cells and mossy cells</article-title><source>Neuron</source><volume>93</volume><fpage>691</fpage><lpage>704</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.12.011</pub-id><pub-id pub-id-type="pmid">28132824</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shani-Narkiss</surname> <given-names>H</given-names></name><name><surname>Vinograd</surname> <given-names>A</given-names></name><name><surname>Landau</surname> <given-names>ID</given-names></name><name><surname>Tasaka</surname> <given-names>G</given-names></name><name><surname>Yayon</surname> <given-names>N</given-names></name><name><surname>Terletsky</surname> <given-names>S</given-names></name><name><surname>Groysman</surname> <given-names>M</given-names></name><name><surname>Maor</surname> <given-names>I</given-names></name><name><surname>Sompolinsky</surname> <given-names>H</given-names></name><name><surname>Mizrahi</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Young adult-born neurons improve odor coding by mitral cells</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>5867</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-19472-8</pub-id><pub-id pub-id-type="pmid">33203831</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname> <given-names>PJ</given-names></name><name><surname>Turrigiano</surname> <given-names>GG</given-names></name><name><surname>Nelson</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title><source>Neuron</source><volume>32</volume><fpage>1149</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00542-6</pub-id><pub-id pub-id-type="pmid">11754844</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Somogyi</surname> <given-names>P</given-names></name><name><surname>Klausberger</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Defined types of cortical interneurone structure space and spike timing in the hippocampus</article-title><source>The Journal of Physiology</source><volume>562</volume><fpage>9</fpage><lpage>26</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.2004.078915</pub-id><pub-id pub-id-type="pmid">15539390</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stefanelli</surname> <given-names>T</given-names></name><name><surname>Bertollini</surname> <given-names>C</given-names></name><name><surname>Lüscher</surname> <given-names>C</given-names></name><name><surname>Muller</surname> <given-names>D</given-names></name><name><surname>Mendez</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Hippocampal somatostatin interneurons control the size of neuronal memory ensembles</article-title><source>Neuron</source><volume>89</volume><fpage>1074</fpage><lpage>1085</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.024</pub-id><pub-id pub-id-type="pmid">26875623</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tashiro</surname> <given-names>A</given-names></name><name><surname>Sandler</surname> <given-names>VM</given-names></name><name><surname>Toni</surname> <given-names>N</given-names></name><name><surname>Zhao</surname> <given-names>C</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>NMDA-receptor-mediated, cell-specific integration of new neurons in adult dentate gyrus</article-title><source>Nature</source><volume>442</volume><fpage>929</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nature05028</pub-id><pub-id pub-id-type="pmid">16906136</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tashiro</surname> <given-names>A</given-names></name><name><surname>Makino</surname> <given-names>H</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Experience-specific functional modification of the dentate gyrus through adult neurogenesis: a critical period during an immature stage</article-title><source>Journal of Neuroscience</source><volume>27</volume><fpage>3252</fpage><lpage>3259</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4941-06.2007</pub-id><pub-id pub-id-type="pmid">17376985</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Temprana</surname> <given-names>SG</given-names></name><name><surname>Mongiat</surname> <given-names>LA</given-names></name><name><surname>Yang</surname> <given-names>SM</given-names></name><name><surname>Trinchero</surname> <given-names>MF</given-names></name><name><surname>Alvarez</surname> <given-names>DD</given-names></name><name><surname>Kropff</surname> <given-names>E</given-names></name><name><surname>Giacomini</surname> <given-names>D</given-names></name><name><surname>Beltramone</surname> <given-names>N</given-names></name><name><surname>Lanuza</surname> <given-names>GM</given-names></name><name><surname>Schinder</surname> <given-names>AF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Delayed coupling to feedback inhibition during a critical period for the integration of adult-born granule cells</article-title><source>Neuron</source><volume>85</volume><fpage>116</fpage><lpage>130</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.11.023</pub-id><pub-id pub-id-type="pmid">25533485</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turrigiano</surname> <given-names>GG</given-names></name><name><surname>Leslie</surname> <given-names>KR</given-names></name><name><surname>Desai</surname> <given-names>NS</given-names></name><name><surname>Rutherford</surname> <given-names>LC</given-names></name><name><surname>Nelson</surname> <given-names>SB</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Activity-dependent scaling of quantal amplitude in neocortical neurons</article-title><source>Nature</source><volume>391</volume><fpage>892</fpage><lpage>896</lpage><pub-id pub-id-type="doi">10.1038/36103</pub-id><pub-id pub-id-type="pmid">9495341</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tyzio</surname> <given-names>R</given-names></name><name><surname>Holmes</surname> <given-names>GL</given-names></name><name><surname>Ben-Ari</surname> <given-names>Y</given-names></name><name><surname>Khazipov</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Timing of the developmental switch in GABA<sub>A</sub> mediated signaling from excitation to inhibition in CA3 rat hippocampus using gramicidin perforated patch and extracellular recordings</article-title><source>Epilepsia</source><volume>48</volume><fpage>96</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1111/j.1528-1167.2007.01295.x</pub-id><pub-id pub-id-type="pmid">17910587</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Praag</surname> <given-names>H</given-names></name><name><surname>Kempermann</surname> <given-names>G</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Running increases cell proliferation and neurogenesis in the adult mouse dentate gyrus</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>266</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1038/6368</pub-id><pub-id pub-id-type="pmid">10195220</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vivar</surname> <given-names>C</given-names></name><name><surname>Potter</surname> <given-names>MC</given-names></name><name><surname>Choi</surname> <given-names>J</given-names></name><name><surname>Lee</surname> <given-names>JY</given-names></name><name><surname>Stringer</surname> <given-names>TP</given-names></name><name><surname>Callaway</surname> <given-names>EM</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name><name><surname>Suh</surname> <given-names>H</given-names></name><name><surname>van Praag</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Monosynaptic inputs to new neurons in the dentate gyrus</article-title><source>Nature Communications</source><volume>3</volume><elocation-id>1107</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms2101</pub-id><pub-id pub-id-type="pmid">23033083</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>Y</given-names></name><name><surname>Chik</surname> <given-names>DT</given-names></name><name><surname>Wang</surname> <given-names>ZD</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Coherence resonance and noise-induced synchronization in globally coupled Hodgkin-Huxley neurons</article-title><source>Physical Review E</source><volume>61</volume><fpage>740</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1103/PhysRevE.61.740</pub-id><pub-id pub-id-type="pmid">11046318</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>DD</given-names></name><name><surname>Kriegstein</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Blocking early GABA depolarization with bumetanide results in permanent alterations in cortical circuits and sensorimotor gating deficits</article-title><source>Cerebral Cortex</source><volume>21</volume><fpage>574</fpage><lpage>587</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhq124</pub-id><pub-id pub-id-type="pmid">20624842</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weisz</surname> <given-names>VI</given-names></name><name><surname>Argibay</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>A putative role for neurogenesis in neuro-computational terms: inferences from a hippocampal model</article-title><source>Cognition</source><volume>112</volume><fpage>229</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2009.05.001</pub-id><pub-id pub-id-type="pmid">19481201</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weisz</surname> <given-names>VI</given-names></name><name><surname>Argibay</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neurogenesis interferes with the retrieval of remote memories: forgetting in neurocomputational terms</article-title><source>Cognition</source><volume>125</volume><fpage>13</fpage><lpage>25</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2012.07.002</pub-id><pub-id pub-id-type="pmid">22841299</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiskott</surname> <given-names>L</given-names></name><name><surname>Rasch</surname> <given-names>MJ</given-names></name><name><surname>Kempermann</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A functional hypothesis for adult hippocampal neurogenesis: avoidance of catastrophic interference in the dentate gyrus</article-title><source>Hippocampus</source><volume>16</volume><fpage>329</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1002/hipo.20167</pub-id><pub-id pub-id-type="pmid">16435309</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Woods</surname> <given-names>NI</given-names></name><name><surname>Stefanini</surname> <given-names>F</given-names></name><name><surname>Apodaca-Montano</surname> <given-names>DL</given-names></name><name><surname>Tan</surname> <given-names>IMC</given-names></name><name><surname>Biane</surname> <given-names>JS</given-names></name><name><surname>Kheirbek</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The dentate gyrus classifies cortical representations of learned stimuli</article-title><source>Neuron</source><volume>107</volume><fpage>173</fpage><lpage>184</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.04.002</pub-id><pub-id pub-id-type="pmid">32359400</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeckel</surname> <given-names>MF</given-names></name><name><surname>Berger</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Feedforward excitation of the hippocampus by afferents from the entorhinal cortex: redefinition of the role of the trisynaptic pathway</article-title><source>PNAS</source><volume>87</volume><fpage>5832</fpage><lpage>5836</lpage><pub-id pub-id-type="doi">10.1073/pnas.87.15.5832</pub-id><pub-id pub-id-type="pmid">2377621</pub-id></element-citation></ref><ref id="bib100"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname> <given-names>M</given-names></name><name><surname>Meyer</surname> <given-names>T</given-names></name><name><surname>Benkowitz</surname> <given-names>C</given-names></name><name><surname>Savanthrapadian</surname> <given-names>S</given-names></name><name><surname>Ansel-Bollepalli</surname> <given-names>L</given-names></name><name><surname>Foggetti</surname> <given-names>A</given-names></name><name><surname>Wulff</surname> <given-names>P</given-names></name><name><surname>Alcami</surname> <given-names>P</given-names></name><name><surname>Elgueta</surname> <given-names>C</given-names></name><name><surname>Bartos</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Somatostatin-positive interneurons in the dentate gyrus of mice provide local- and long-range septal synaptic inhibition</article-title><source>eLife</source><volume>6</volume><elocation-id>e21105</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.21105</pub-id><pub-id pub-id-type="pmid">28368242</pub-id></element-citation></ref><ref id="bib101"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname> <given-names>F</given-names></name><name><surname>Gerstner</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Hebbian plasticity requires compensatory processes on multiple timescales</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>372</volume><elocation-id>20160259</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2016.0259</pub-id><pub-id pub-id-type="pmid">28093557</pub-id></element-citation></ref><ref id="bib102"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname> <given-names>C</given-names></name><name><surname>Teng</surname> <given-names>EM</given-names></name><name><surname>Summers</surname> <given-names>RG</given-names></name><name><surname>Ming</surname> <given-names>GL</given-names></name><name><surname>Gage</surname> <given-names>FH</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Distinct morphological stages of dentate granule neuron maturation in the adult mouse hippocampus</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>3</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3648-05.2006</pub-id><pub-id pub-id-type="pmid">16399667</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66463.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Sharpee</surname><given-names>Tatyana O</given-names></name><role>Reviewing Editor</role><aff><institution>Salk Institute for Biological Studies</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Miller</surname><given-names>Paul</given-names> </name><role>Reviewer</role><aff><institution>Brandeis University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This paper demonstrates through theoretical modelling how the switch from excitatory to inhibitory signaling occurring in new born neurons can aid the integration of new neurons into the existing neural circuit. The modelling analysis also analyzes how this can aid the temporal integration of relevant memories.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A functional model of adult dentate gyrus neurogenesis&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and John Huguenard as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Paul Miller (Reviewer #1).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions:</p><p>1. Provide analysis of the model where mature neurons also exhibit plasticity but at reduced levels.</p><p>2. Examine how network behaves when inputs have different statistics.</p><p><italic>Reviewer #1:</italic></p><p>The authors propose a role for newborn cells in the dentate gyrus that relies upon their input from interneurons being initially excitatory before switching to inhibitory as the cells mature. The computational modeling and accompanying analyses show how, when receiving only excitatory input, the newborn cells become responsive to stimuli similar to those that already cause high responses in other cells, but then, following the developmental switch such that they receive inhibitory input, those cells then gain responses to novel, but similar, inputs. In a simplified model, the authors are able to quantify the criterion of &quot;sufficient similarity&quot;, such that if the novel inputs are not similar enough to the original ones the newborn cells to not gain responses to them. The authors demonstrate that such only when newborn cells are incorporated into the network and respond to the novel stimuli, can those stimuli be categorized by the network, as necessary for them to be recognized.</p><p>A major achievement of the paper is to identify a role in information processing for the developmental shift in reversal potential of chloride ions. Such a role is well supported by the results in the paper.</p><p>As in all modeling papers, some choices must be made so as to simplify the system to render it tractable and its behavior understandable. Some of the choices could be better justified or discussed, as highlighted below.</p><p>The normalization of inputs such that the L2-norm is fixed seems rather unusual, and is not clearly the actual impact of feedforward inhibition. It would be nice to know whether this feature of the input vector is important. Would it matter if the L1-norm were used for normalization, or if normalization were not precise? Perhaps a comment on the importance of this could be made, as well as a justification for the choice.</p><p>Throughout the manuscript, the authors employ a homeostatic term in the postsynaptic firing rate. The term is called &quot;heterosynaptic&quot; by the authors, but strictly it is not, since it does not depend on other presynaptic inputs. Rather, the plasticity rule effects &quot;firing rate homeostasis&quot; and is implemented in a manner similarly to Renart, Song, and Wang, (2003). I think this should be mentioned at a minimum and perhaps entirely renamed throughout the manuscript.</p><p>The authors consider the ability of the network to discriminate novel patterns as the newborn cells gain responses to the novel patterns. I assume that the formation of new responses arises when the novel patterns are presented randomly amidst the set of previously learned patterns. It would be valuable to see if there are prediction of any differences in behavior if the novel patterns were presented alone, or if there are any impacts of different manners of interspersing learned patterns with novel patterns.</p><p>Lines 68-91 contain a lot of details of the circuitry, many of which are not included in the model. It would be helpful to have a figure showing the full circuit based on the information written (which is rather hard to take in through one reading) and beside it to include a figure of the model circuit so the reader can easily see what is being simplified and omitted.</p><p>Lines 118-119: I think you should mention here or in the Discussion that you have selected a specific set of synapses to undergo plasticity-that is, if I understand correctly, you have ignored any plasticity with the Dentate Gyrus.</p><p>Line 167-8, Equation 1: This equation appears to be different from that of Equation6 in the methods. In particular the &quot;HET&quot; function depends on postsynaptic rate-cubed, not just the difference between rate and threshold as suggested here. Why not just write the exact equation and indicate/describe the behavior of each term?</p><p>Line 260-261: The terminology is a bit confusing, as activation is not clearly a &quot;change in membrane potential&quot; but a change in firing rate, so has different units to the reversal potential. Especially as the membrane potential must be venturing above threshold to produce some spiking activity. Perhaps the criterion is equivalent to &quot;the activity is low enough that the mean membrane potential remains below the reversal potential of the chloride channels&quot;?</p><p>Line 272-3: The statement about the switch in excitability here assumes we already know it, though it is described in the methods much later. Perhaps this sort of issue is inevitable in journals where methods are placed after results, but it would be better if the order were reversed!</p><p>Line 320: I see no justification for a one-way t-test. I think they should be two-way unless it is only possible a priori for a change in one direction.</p><p>Line 338: The mention of fixing one set of inputs arises out of nowhere without justification – though that justification comes later as this is just one of two controls. I think it would be better with the order reversed. Or, at least when it is first mentioned here, please be clear why this was chosen, as – I assume – the feedforward weights to selective cells are not fixed in the main set of results. If feedforward weights to selective cells are fixed in the earlier sections, then it should be clearly mentioned, as I did not notice it.</p><p>Line 364: Following the previous comment, this line suggests that feedforward weights are not fixed in your primary results with good discrimination. Please clarify if this statement is constrained to the networks without neurogenesis.</p><p><italic>Reviewer #2:</italic></p><p>Gozel and Gerstner investigated the functional role of adult neurogenesis in the dentate gyrus using simulations and mathematical analysis of a computational model. The novelty of the paper compared to numerous previous studies in the field is the inclusion of the GABAergic switch from excitation to inhibition of new neurons during the maturation process. So far this has been overlooked in the computational literature. G&amp;G propose an elegant and potentially interesting idea for how the two phase maturation process could be functionally beneficial for an animal tasked with discriminating stimuli, and would be the first to recapitulate the experimental finding of adult neurogenesis contributing to pattern separation of similar but not distinct stimuli.</p><p>However, my assessment is that the current model simulations and analysis are not sufficient to support for the claims made in the paper. Furthermore, the main experimental finding that can be understood based on this modeling work is emergence pattern separation for similar but not distinct stimuli. While interesting, this is rather technical, and may depend quite strongly on the details of the model.</p><p>1. The input stimuli from the MNIST dataset presented to the network are low dimensional to a very good approximation (&quot;3&quot;, &quot;4&quot;, &quot;5&quot;), in contrast to the type of stimuli a real network would be presented with which are expected to be high-dimensional.</p><p>1.1. In the model analyzed, the narrowness of the distribution of synaptic weight vector norms is important for network stability. This narrow distribution could at least in part be inherited from the low dimensionality of stimuli (all &quot;3's&quot; have large overlap with the &quot;average 3&quot;). If the overlaps of different stimuli are broadly distributed, so will the distribution of how many input patterns each neuron is a &quot;winner&quot; in. It is important to test this stability in more realistic stimulus ensembles, perhaps by controlling the width of the overlap distribution using the binary model the authors present towards the end of the paper.</p><p>1.2. The authors claim that synapses of newborn DGCs starting the maturation process from 0 is important for solving the problem of unresponsive neurons. The reason is that during this phase the synaptic weight vector becomes aligned with a specific direction of the input space. It is possible that unresponsive neurons are stuck in a local minimum (like the case with no neurogenesis) precisely because stimuli (and overlaps) are narrowly distributed around the mean. If stimuli are more broadly distributed (~higher dimension), the basins of attraction are expected to be more numerous and more shallow. Therefore one may expect the problem of the system getting stuck in a local minimum to be far less severe in this case, and for &quot;control 2&quot; networks to learn well.</p><p>2. Setting no plasticity (eta = 0) for mature cells is a very strong assumption. Some protocols (e.g., TBS<sub>2</sub> in Schmidt-Hieber, 2004; and others in Ge 2007) lead to ~2 fold increase in plasticity in young vs. mature neurons. Since mature neurons significantly outnumber young neurons, the effect of plasticity in mature neurons cannot be neglected altogether, especially since the paper's main focus is on the integration of newborn neurons into the circuit. Given the actual degree of synaptic plasticity in mature neurons (according to the papers that the authors themselves cite) I expect the behavior of the authors model to be much closer to &quot;control 3&quot;. To support their claims, I think the authors should show that their network compares favorably to control 3 even if DGCs remain plastic throughout (but to a lesser extent). In this scenario I expect the fraction of neurons that are new at any given time to be much more important than the current model, since the mature part of the network is fixed. Therefore this fraction should also be matched to experiments.</p><p>3. It is not clear to me how the two phase maturation process of DGCs would be affected in a scenario where at any given point some DGCs are in the excitatory phase of GABA and others are in the inhibitory phase. This would be expected if there is a continuous stream of new neurons. Would the plasticity of the neurons in the inhibitory phase not interfere with aligning the activity to similar stimuli due to plasticity of neurons in the excitatory phase? If there is interference, would the authors then predict that neurogenesis occurs in waves (i.e., some kind of global signal would coordinate transition from phase 1 to 2 across synapses)?</p><p>Is there evidence supporting that?</p><p>It seems to me that the calculation in the section &quot;Analytical computation of the L2-norm and angle&quot;--at least in principle--be extended to estimate the interference: the competition due to plasticity of neurons in the inhibitory phase increases the angle phi, and thus slows down the alignment of the weights due to plasticity of neurons in the excitatory phase.</p><p>107, Review of functional role of DGCs.</p><p>Aljadeff et al., 2015,</p><p>Shani-Narkiss et al., 2020</p><p>suggest a dynamical role for new neurons.</p><p>312, It would be interesting if the advantage of adding newborn neurons stimulated with &quot;5&quot; to a network pretrained with &quot;3&quot; and &quot;4&quot; over a network pretrained with &quot;3&quot;, &quot;4&quot;, and &quot;5&quot; would persist if some amount of plasticity remains in mature neurons (Figure 3d).</p><p>614, It would be good to discuss the possibility that neurotransmitter switch (without neurogenesis) has the same functional role as GABA switch in the current model. See e.g., Li et al., (2020) J Neuroscience.</p><p>Furthermore, can this model teach us anything about neurogenesis in the olfactory bulb? Is there an E to I switch there too?</p><p>725, Miller and Fumarola may not be the right reference to cite here. This specific nonlinearity (rectified tanh) is not standard and is not included in that paper.</p><p>778, Definition of quasi orthogonal is not clear. The inhibitory rates can have fluctuations and temporal dynamics of their own even if the network is assumed to be silent when each stimulus is presented. Therefore inputs might be quasi-orthogonal at one time but not at another. If in this is used just to qualitatively understand the network behavior, this somewhat sloppy definition is ok, but I think this caveat should be mentioned to avoid confusion.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.66463.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential Revisions:</p><p>1. Provide analysis of the model where mature neurons also exhibit plasticity but at reduced levels.</p></disp-quote><p>We have included a new paragraph in the Results section “Robustness of the model” (lines 473-495) and a Supplementary File 4 to address this point.</p><disp-quote content-type="editor-comment"><p>2. Examine how network behaves when inputs have different statistics.</p></disp-quote><p>We have included a new Results section “The cooperative phase of maturation promotes pattern separation for any dimensionality of input data” (lines 601-678) as well as a new Supplementary File 5 to address this point (and a Method section “Effective dimensionality and participation ratio”, lines 1349-1353, to define our dimensionality measure).</p><disp-quote content-type="editor-comment"><p>Reviewer #1:</p><p>The authors propose a role for newborn cells in the dentate gyrus that relies upon their input from interneurons being initially excitatory before switching to inhibitory as the cells mature. The computational modeling and accompanying analyses show how, when receiving only excitatory input, the newborn cells become responsive to stimuli similar to those that already cause high responses in other cells, but then, following the developmental switch such that they receive inhibitory input, those cells then gain responses to novel, but similar, inputs. In a simplified model, the authors are able to quantify the criterion of &quot;sufficient similarity&quot;, such that if the novel inputs are not similar enough to the original ones the newborn cells to not gain responses to them. The authors demonstrate that such only when newborn cells are incorporated into the network and respond to the novel stimuli, can those stimuli be categorized by the network, as necessary for them to be recognized.</p><p>A major achievement of the paper is to identify a role in information processing for the developmental shift in reversal potential of chloride ions. Such a role is well supported by the results in the paper.</p><p>As in all modeling papers, some choices must be made so as to simplify the system to render it tractable and its behavior understandable. Some of the choices could be better justified or discussed, as highlighted below.</p><p>The normalization of inputs such that the L2-norm is fixed seems rather unusual, and is not clearly the actual impact of feedforward inhibition. It would be nice to know whether this feature of the input vector is important. Would it matter if the L1-norm were used for normalization, or if normalization were not precise? Perhaps a comment on the importance of this could be made, as well as a justification for the choice.</p></disp-quote><p>We thank the reviewer for raising an important point. In our model, it is merely a practical simplification to consider input patterns which all have an L2-norm of 1. Indeed, it ensures that the upper bound of the L2-norm of the feedforward weight vectors onto newborn DGCs is fixed and identical for all newborn DGCs (see Methods, section “Direction and length of the weight vector”).</p><p>In competitive networks, it is in general important that the length of the feedforward weight vectors onto different DGCs is similar, otherwise the cells with longer weight vectors would have an unfair advantage and thus have higher probability to win the competition. Since the input is normalized and the weight vector converges to the center of inputs for which it becomes active, it follows that all the final weights vectors are expected to be of similar length. Indeed, in our model, at the end of maturation, newborn DGCs do not get identical weight vector lengths, though the distribution is rather narrow. For unprecise L2-normalization of the inputs, we expect that if the imprecision is uniform over the input space, then our neurogenesis model should still do fine, because the weight vector length of each newborn DGC would depend on input pattern lengths that have the same statistics. However, if the input space has some regions of input patterns that have much higher L2-norm than other regions of input patterns, then we expect the clusters with higher norm to be well represented by model newborn DGCs, while clusters with low L2-norms would be poorly represented, therefore discrimination of input patterns would decrease.</p><p>If the L1-norm was used for input normalization, it would yield weight vectors with slightly variable L2-norms non-uniformly distributed in the input space. Without loss of generality, we can consider 2-dimensional inputs for a visual explanation. If the L1-norm of input patterns is fixed to a value R, then inputs whose direction is horizontal (arrowhead at (R,0)) or vertical (arrowhead at (0,R)) will have larger L2-norm (R) than inputs whose direction is diagonal to the xy-plane (arrowhead at (R/2,R/2), so L2-norm=sqrt(½)*R). Therefore, difference between L1 and L2 normalization will show up when one compares diagonally oriented input vectors with inputs aligned with one of the axes.</p><p>We added an explanation for the reason for our L2-normalization implementation choice in lines 1033-1039. It reads:</p><p>“Normalization of inputs (be it implemented algorithmically as done here or by explicit inhibitory feedback) ensures that, once weight growth due to synaptic plasticity has ended and weights have stabilized, the overall strength of input onto DGCs is approximately identical for all cells (see Section Direction and length of the weight vector). Equalized lengths of weight vectors are, in turn, an important feature of classic soft or hard competitive networks (Kohonen, 1989; Hertz et al., 1991).”</p><disp-quote content-type="editor-comment"><p>Throughout the manuscript, the authors employ a homeostatic term in the postsynaptic firing rate. The term is called &quot;heterosynaptic&quot; by the authors, but strictly it is not, since it does not depend on other presynaptic inputs. Rather, the plasticity rule effects &quot;firing rate homeostasis&quot; and is implemented in a manner similarly to Renart, Song, and Wang (2003). I think this should be mentioned at a minimum and perhaps entirely renamed throughout the manuscript.</p></disp-quote><p>In our manuscript, we used the definition of “heterosynaptic” that can be found in Chistiakova et al., (2014) and Zenke and Gerstner, (2017). It is a homeostatic mechanism but differs from standard experimentally observed synaptic scaling mechanisms (Turrigiano et al., 1998) mainly by the fact that it occurs on a much shorter timescale: “Homeostatic plasticity differs from the heterosynaptic plasticity […] in two important aspects. First, homeostatic plasticity requires nonspecific dramatic changes of neuronal activity over prolonged periods, which are unlikely to happen during everyday life and learning. Second, it operates on a very long time scale, hours and days, and thus cannot counteract runaway dynamics induced within seconds and minutes by Hebbian-type learning rules.” (Chistiakova et al., 2014). In Renart et al., (2003), they model a “homeostatic” mechanism: it has a long characteristic timescale. However, their actual implementation differs: “since what matters for the steady state of the very slow scaling process is the integrated activity of each cell across different stimuli, we have replaced this temporal average by a spatial average carried out over several network simulations run in parallel” (Renart et al., 2003).</p><p>In this terminology, “homeostatic” and “heterosynaptic” mechanisms both depend on postsynaptic activity but are independent of presynaptic activity (the “heterosynaptic” term in our synaptic plasticity rule does not depend on the presynaptic firing rate x<sub>j</sub>). Since they do not depend on the identity of the presynaptic neuron, heterosynaptic plasticity affects several synapses in parallel, hence the terminology. For example, a strong presynaptic input at synapse j may cause the postsynaptic neuron to fire at a very high rate. The heterosynaptic term then lowers the strength of other synapses k nonequal j, independent of the presynaptic activity. On the other hand, “homosynaptic” terms are Hebbian: they depend on pre- and postsynaptic activity.</p><p>As a personal aside, I (Wulfram) would add that we switched to the term heterosynaptic in my lab after the work of Friedemann Zenke. Whenever I gave a talk and mentioned homeostatic plasticity, all participants immediately had in mind the beautiful work of Turrigiano – but this work considers changes on the time scale of 24 hours. However, Zenke showed that you cannot stabilize firing rates in a recurrent network if the homeostatic mechanism works on the time scale of several hours. We had a meeting at the Royal Academy of Sciences in London some years ago where homeostatic mechanisms were debated controversially. I would like to pull out of the controversy by talking about heterosynaptic plasticity that contributes to firing rate homeostatis. Importantly, the heterosynaptic plasticity that we use is in the same equation (and on the same time scale) as the Hebbian plasticity. Two review papers of Friedemann Zenke give the main arguments for this terminology.</p><p>To make this issue clearer to the reader and avoid potential confusion of terminology, we now say in the text always heterosynaptic plasticity inducing rapid homeostatic weight stabilization, for example:</p><p>– lines 185-192: “The third term on the right-hand-side of equation (1) implements heterosynaptic plasticity (Chistiakova et al., 2014; Zenke and Gerstner, 2017): whenever the postsynaptic neuron fires at a rate above theta, all weights are downregulated independent of presynaptic activity. It ensures that the weights cannot grow without bounds (Methods). In this sense, the third term has a 'homeostatic' function (Zenke and Gerstner, 2017), yet acts on a time scale faster than experimentally observed homeostatic synaptic plasticity (Turrigiano et al., 1998).”</p><p>– lines 240-244: “A detailed mathematical analysis (Methods) shows that heterosynaptic plasticity in equation (1) ensures that the total strength of the receptive field of each selective DGC converges to a stable value which is similar for selective DGCs confirming the homeostatic function of heterosynaptic plasticity (Zenke and Gerstner, 2017).”</p><p>– lines 921-925: “Moreover, all weights onto neuron i are downregulated heterosynaptically by an amount that increases supra-linearly with the postsynaptic rate nu<sub>i</sub>, implicitly controlling the length of the weight vector (see below) similar to synaptic homeostasis (Turrigiano et al., 1998) but on a rapid time scale (Zenke and Gerstner, 2017).”</p><disp-quote content-type="editor-comment"><p>The authors consider the ability of the network to discriminate novel patterns as the newborn cells gain responses to the novel patterns. I assume that the formation of new responses arises when the novel patterns are presented randomly amidst the set of previously learned patterns. It would be valuable to see if there are prediction of any differences in behavior if the novel patterns were presented alone, or if there are any impacts of different manners of interspersing learned patterns with novel patterns.</p></disp-quote><p>It is correct that in our implementation, novel patterns are presented randomly amidst the set of previously learned patterns. But an aspect that is even more important (as shown with our simplified network model) is whether novel patterns are, or are not, similar enough to familiar patterns. Let us discuss four situations:</p><p>First, if similar novel patterns are presented alone during the early phase of maturation, we expect that the feedforward weight vector onto newborn DGCs will grow directly in the direction of the center of mass of the novel patterns. Indeed, if novel patterns are similar to familiar patterns, they will activate mature DGCs which will indirectly activate the newborn DGCs. Thus, whether novel patterns similar to familiar patterns are presented alone or amidst familiar patterns ultimately makes no difference at the end of maturation.</p><p>Second, in the late phase of maturation we expect the feedforward weight vector onto newborn DGCs to represent different prototypes of the novel similar patterns, no matter if only novel patterns are presented or if both novel and familiar patterns are presented. (However, if only familiar patterns are presented during the late competitive phase of maturation, newborn DGCs will never win the competition (because they were selective for novel patterns at the end of the early phase), hence stay silent and not update their weights.)</p><p>Third, we show that the timing of introduction of the novel patterns is important (lines 455-472). Introduction of similar novel patterns must be early enough, otherwise feedforward weight vectors onto newborn DGCs grow in a direction which is too far from novel patterns. However, if novel patterns are introduced towards the end of the early phase but are not interspersed with familiar patterns, we expect that a shorter period is sufficient for changing the direction of the weight vector to a direction which is close enough from novel patterns to be able to become selective for them in the late phase of maturation.</p><p>Fourth, we expect the L2-norm of the feedforward weight vector onto newborn DGCs to grow faster in the early phase if only familiar patterns are presented initially.</p><p>In summary, a blocked presentation of novel patterns that are similar to familiar patterns timed towards the end of the early phase is actually helpful – and an interspersed presentation is actually a scenario which yields slower learning. The advantage of the interspersed presentation is that the timing does not need to be optimized, which is the reason for our implementation choice.</p><p>We now specify more clearly our implementation and its rationale (lines 267-271):</p><p>“To mimic exposure of an animal to a novel set of stimuli, we now add input patterns from digit 5 to the set of presented stimuli, which was previously limited to patterns of digits 3 and 4. The novel patterns from digit 5 are randomly interspersed into the sequence of patterns from digits 3 and 4; in other words, the presentation sequence was not optimized with a specific goal in mind.”</p><disp-quote content-type="editor-comment"><p>Lines 68-91 contain a lot of details of the circuitry, many of which are not included in the model. It would be helpful to have a figure showing the full circuit based on the information written (which is rather hard to take in through one reading) and beside it to include a figure of the model circuit so the reader can easily see what is being simplified and omitted.</p></disp-quote><p>Thanks for the suggestion. We added a new Figure 1 —figure supplement 1 and refer to it in the main text where we write about the circuitry (lines 71, 92, 141, 216, 854, 860).</p><disp-quote content-type="editor-comment"><p>Lines 118-119: I think you should mention here or in the Discussion that you have selected a specific set of synapses to undergo plasticity-that is, if I understand correctly, you have ignored any plasticity with the Dentate Gyrus.</p></disp-quote><p>Thank you for pointing out the need for more clarity. In our model, the only synapses that are plastic and follow our plasticity rule are those between EC and DGCs. There is no plasticity rule involved within the dentate gyrus: connections are absent or present (with fixed values). However, the connections between newborn DGCs (E) and inhibitory neurons (I) within dentate gyrus still changes as a function of maturation: from no E-to-I and fixed positive I-to-E connections in the early phase to fixed positive E-to-I and fixed negative I-to-E connections in the late phase.</p><p>To clarify this important point, we now specify that it is the synaptic connections between EC and newborn DGCs which are plastic, and write in lines 120-123 (Introduction):</p><p>“[…] our model uses an unsupervised biologically plausible Hebbian learning rule that makes synaptic connections between EC and newborn DGCs either disappear or grow from small values at birth to values that eventually enable feedforward input from EC to drive DGCs.”</p><p>Furthermore, we explicitly mention the lack of plasticity at other synapses within the dentate gyrus in lines 200-202 (Results):</p><p>“For simplicity, no plasticity rule was implemented within the dentate gyrus: connections between newborn DGCs and inhibitory cells are either absent or present with a fixed value (see below).”</p><disp-quote content-type="editor-comment"><p>Line 167-8, Equation 1: This equation appears to be different from that of Equation6 in the methods. In particular the &quot;HET&quot; function depends on postsynaptic rate-cubed, not just the difference between rate and threshold as suggested here. Why not just write the exact equation and indicate/describe the behavior of each term?</p></disp-quote><p>We agree that our “simplified” expression ended up being more confusing than self-explanatory. As suggested, we wrote the exact equation in the Results (line 174-175).</p><disp-quote content-type="editor-comment"><p>Line 260-261: The terminology is a bit confusing, as activation is not clearly a &quot;change in membrane potential&quot; but a change in firing rate, so has different units to the reversal potential. Especially as the membrane potential must be venturing above threshold to produce some spiking activity. Perhaps the criterion is equivalent to &quot;the activity is low enough that the mean membrane potential remains below the reversal potential of the chloride channels&quot;?</p></disp-quote><p>Thanks for pointing out the inaccuracy of our terminology, and for the reformulation suggestion. We modified lines 281-285 as follows:</p><p>“We assume that in natural settings, the activation of GABA<sub>A</sub> receptors is low enough that the mean membrane potential remains below the chloride reversal potential at which shunting inhibition would be induced (Heigele et al., 2016). In this regime, the net effect of synaptic activity is hence excitatory.”</p><disp-quote content-type="editor-comment"><p>Line 272-3: The statement about the switch in excitability here assumes we already know it, though it is described in the methods much later. Perhaps this sort of issue is inevitable in journals where methods are placed after results, but it would be better if the order were reversed!</p></disp-quote><p>Even though we mentioned the change in excitability in the Introduction (lines 38-39), we indeed did not say that we were including it in our model. We hope that we solved this issue by now mentioning that we do model the change in excitability of newborn DGCs by modifying their firing threshold early in the Results section, lines 143-148:</p><p>“Firing rates are modeled by neuronal frequency-current curves that vanish for weak input and increase if the total input into a neuron is larger than a firing threshold. Since newborn DGCs exhibit enhanced excitability early in maturation (Schmidt-Hieber et al., 2004; Li et al., 2017), the firing threshold of model neurons increases during maturation from a lower to a higher value (Methods).”</p><disp-quote content-type="editor-comment"><p>Line 320: I see no justification for a one-way t-test. I think they should be two-way unless it is only possible a priori for a change in one direction.</p></disp-quote><p>Thanks for your careful reading. It is true that the change in classification performance could be in both directions. However, we were only interested by a potential difference from a zero change: we did not compare the two distributions themselves (a two-way t-test would test if the mean of one distribution is different from the mean of the other distribution). We clarify this in the new version (lines 353-360):</p><p>“Across ten simulation experiments, classification performance is significantly higher when a novel ensemble of patterns is learned sequentially by newborn DGCs (P<sub>2</sub>; Supplementary File 1), than if all patterns are learned simultaneously (P<sub>1</sub>; Supplementary File 1). Indeed, the distribution of P<sub>2</sub>-P<sub>1</sub> for the ten simulation experiments has a mean which is significantly different from zero (Wilcoxon signed rank test: p-val = 0.0020, Wilcoxon signed rank = 55; one-way t-test: p-val = 0.0269, t-stat = 2.6401, df = 9; Supplementary File 1).”</p><disp-quote content-type="editor-comment"><p>Line 338: The mention of fixing one set of inputs arises out of nowhere without justification – though that justification comes later as this is just one of two controls. I think it would be better with the order reversed. Or, at least when it is first mentioned here, please be clear why this was chosen, as – I assume – the feedforward weights to selective cells are not fixed in the main set of results. If feedforward weights to selective cells are fixed in the earlier sections, then it should be clearly mentioned, as I did not notice it.</p></disp-quote><p>Thanks for your comment. In addition to the next comment, it made us realize that our text from section “Mature neurons represent prototypical input patterns” to section “The GABA-switch guides learning of novel representations” was not completely clear and the flow was sometimes maybe misleading. Therefore, we slightly reorganized the text, and added some details on the implementation. Here are the main changes we made:</p><p>– We added a sentence to mention that pretraining starts from random (nonzero) weights, and that all DGCs have the same learning rate during pretraining, lines 206-210: “Hence we start with a network that already has strong random EC-to-DGC connection weights (Methods). We then pretrain our network of 100 DGCs using the same learning rule (equation (1), with identical learning rate eta for all DGCs) that we will use later for the integration of newborn cells.”</p><p>– We moved the information about setting the learning rate of selective cells to zero (which was previously at the beginning of section “Newborn neurons become selective for novel patterns during maturation”) to the end of section “Mature neurons represent prototypical input patterns” and explicitly state that mature cells are not plastic in our main implementation (but we included results for when they are), now lines 250-256: “After convergence of synaptic weights during pretraining, selective DGCs are considered mature cells. Mature cells are less plastic than newborn cells (Schmidt-Hieber et al., 2004; Ge et al., 2007). So in the following, unless specified otherwise, we set eta=0 in equation (1) for mature cells (feedforward connection weights from EC to mature cells remain therefore fixed). A scenario where mature cells retain synaptic plasticity is also investigated (see Robustness of the model and Supplementary File 4).”</p><p>– We state that newborn DGCs in the main neurogenesis model start with zero feedforward weights, lines 261-263: “In our main neurogenesis model, we replace unresponsive model units by plastic newborn DGCs (eta &gt; 0 in equation (1)) which receive lateral GABAergic input but do not receive feedforward input yet (all weights from EC are set to zero).”</p><p>– In lines 348-350, we allude that “pretraining with three digits” (our control 1) is implemented in the exact same way as “pretraining with 2 digits”, except that patterns from three digits (instead of two) are presented: “We compare this performance with that of a network where all three digit ensembles are directly simultaneously pretrained starting from random weights (Figure 3a, control1).”</p><p>– We moved the paragraph about introducing two novel digits (previously in lines 322-331) to the end of the previous section “Newborn neurons become selective for novel patterns during maturation” (now lines 329-338). We believe that the flow is better in this way, as this paragraph talks about a variant of the main neurogenesis results (where two novel digits instead of one are introduced during maturation of the newborn cells), and not about the pretraining control (as it may have previously suggested).</p><p>We slightly modified section “The GABA-switch guides learning of novel representations” (now lines 361-394) to more clearly contrast controls 2 and 3 with the main neurogenesis model in terms of learning rate of the selective cells and the connectivity of the unresponsive cells.</p><disp-quote content-type="editor-comment"><p>Line 364: Following the previous comment, this line suggests that feedforward weights are not fixed in your primary results with good discrimination. Please clarify if this statement is constrained to the networks without neurogenesis.</p></disp-quote><p>We hope that with the reorganization of the text as explained in the previous point, this statement (now at lines 392-393) is now clear. Specifically, this statement applies to control 3: all DGCs keep plastic feedforward weights. However, unresponsive units at the end of pretraining are not replaced by newborn DGCs which undergo maturation. Upon presentation of novel patterns, those unresponsive units have low probability to become selective, because their feedforward weight vector has a low norm, and they probably point outside of the space of presented inputs (otherwise they would have become selective earlier). Therefore, novel patterns are learned by cells that were already selective for (similar) familiar patterns. On the other hand, in the main neurogenesis model, newborn DGCs (previously unresponsive units) learn the novel patterns, while the selectivity of mature DGCs is not overwritten since they are not plastic.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Gozel and Gerstner investigated the functional role of adult neurogenesis in the dentate gyrus using simulations and mathematical analysis of a computational model. The novelty of the paper compared to numerous previous studies in the field is the inclusion of the GABAergic switch from excitation to inhibition of new neurons during the maturation process. So far this has been overlooked in the computational literature. G&amp;G propose an elegant and potentially interesting idea for how the two phase maturation process could be functionally beneficial for an animal tasked with discriminating stimuli, and would be the first to recapitulate the experimental finding of adult neurogenesis contributing to pattern separation of similar but not distinct stimuli.</p><p>However, my assessment is that the current model simulations and analysis are not sufficient to support for the claims made in the paper. Furthermore, the main experimental finding that can be understood based on this modeling work is emergence pattern separation for similar but not distinct stimuli. While interesting, this is rather technical, and may depend quite strongly on the details of the model.</p><p>1. The input stimuli from the MNIST dataset presented to the network are low dimensional to a very good approximation (&quot;3&quot;, &quot;4&quot;, &quot;5&quot;), in contrast to the type of stimuli a real network would be presented with which are expected to be high-dimensional.</p><p>1.1. In the model analyzed, the narrowness of the distribution of synaptic weight vector norms is important for network stability. This narrow distribution could at least in part be inherited from the low dimensionality of stimuli (all &quot;3's&quot; have large overlap with the &quot;average 3&quot;). If the overlaps of different stimuli are broadly distributed, so will the distribution of how many input patterns each neuron is a &quot;winner&quot; in. It is important to test this stability in more realistic stimulus ensembles, perhaps by controlling the width of the overlap distribution using the binary model the authors present towards the end of the paper.</p><p>1.2. The authors claim that synapses of newborn DGCs starting the maturation process from 0 is important for solving the problem of unresponsive neurons. The reason is that during this phase the synaptic weight vector becomes aligned with a specific direction of the input space. It is possible that unresponsive neurons are stuck in a local minimum (like the case with no neurogenesis) precisely because stimuli (and overlaps) are narrowly distributed around the mean. If stimuli are more broadly distributed (~higher dimension), the basins of attraction are expected to be more numerous and more shallow. Therefore one may expect the problem of the system getting stuck in a local minimum to be far less severe in this case, and for &quot;control 2&quot; networks to learn well.</p></disp-quote><p>We thank the reviewer for this important suggestion. Our datasets are indeed rather low-dimensional. To investigate how our network behaves when the input space is higher dimensional, as suggested, we used our simple network and we created new handmade datasets with higher dimensionality. Our dimensionality measure is now defined in a new Method section “Effective dimensionality and participation ratio” (lines 1349-1353). We included those results in Supplementary File 5 and in a new Results section “The cooperative phase of maturation promotes pattern separation for any dimensionality of input data” (lines 601-678):</p><p>“Despite the fact that input patterns in our model represent the activity of 144 or 128 model EC cells, the effective dimensionality of the input data was significantly below 100 because the clusters for different input classes were rather concentrated around their respective center of mass. We define the effective input dimensionality as the participation ratio (Mazzucato et al., 2016; Litwin-Kumar et al., 2017) (Methods). Using this definition, the input data of both the MNIST 12x12 patterns from digits 3, 4 and 5 and the seven clusters of the handmade dataset for similar patterns (s=0.8) are relatively low-dimensional (PR=19 out of a maximum of 144, and PR=11 out of a maximum of 128, respectively). We emphasize that in both cases the spread of the input data around the cluster center implies that the effective dimensionality is larger than the number of clusters. In natural settings, we expect the input data to have even higher dimension. Therefore, here we investigate the effect of dimensionality of the input data on our neurogenesis model by increasing the spread around the cluster centers.</p><p>We use our simplified network model and create similar artificial datasets (s=0.8) with different values for the concentration parameter kappa (Methods). The smaller the kappa, the broader the distributions around their center of mass, hence the larger the overlap of patterns generated from different cluster distributions. Therefore, we can increase the effective dimensionality of the input by decreasing the concentration parameter kappa. First, as expected from our analytical analysis (Methods), we find that the broader the cluster distributions the smaller the length of the feedforward weight vector onto newborn DGCs (from just below 1.5 with kappa = 10<sup>4</sup> to about 1.35 with kappa = 6 * 10<sup>2</sup>). Second, we examine the ability of the simplified network to discriminate input patterns coming from input spaces with different dimensionalities. To do so, we compare our neurogenesis model (Neuro.) with a random initialization model (RandInitL.). In both cases, two DGCs are pretrained with patterns from two clusters, as above. Then we fix the weights of the two mature DGCs and introduce patterns from a third cluster as well as a newborn DGC. For the neurogenesis case, after maturation of the newborn DGC we fix its weights (while for the random initialization model we keep them plastic) upon introduction of patterns from a fourth cluster as well as another newborn DGC, and so on until the network contains seven DGCs and patterns from the full dataset of seven clusters have been presented. We compare our neurogenesis model, where each newborn DGC starts with zero weights and undergo a two-phase maturation (1 epoch per phase), with a random initialization model where each newborn DGC is directly fully integrated into the circuit and whose feedforward weight vector is randomly initialized with a length of 0.1 (RandInitL.) and is then learned for 2 epochs.</p><p>Since clusters can be highly overlapping, we assess discrimination performance by computing the reconstruction error at the end of training. Reconstruction error is evaluated analogously to classification error, except that the readout layer has the task of an autoencoder: it contains as many readout units as there are input units. Reconstruction error is the mean squared distance between the input vector and the reconstructed output vector based on testing patterns. We observe that for any dimensionality of the input space, even as high as 97-dimensional, the neurogenesis model performs better (has a lower total reconstruction error) than the random initialization model (Supplementary File 5). Indeed, in the neurogenesis case newborn DGCs grow their feedforward weights (from zero) in the direction of presented input patterns in their early cooperative phase of maturation and can later become selective for novel patterns during the competitive phase. In contrast, since the random initialization model has no early cooperative phase, the newborn DGC weight vector does not grow unless an input pattern is by chance well aligned with its randomly initialized weight vector (which is unlikely in a high dimensional input space). We get similar results for a larger initialization of the synaptic weights (e.g., the length of the weight vector at birth is set to 1, results not shown). Importantly, in high input dimensions, the advantage of a larger weight vector length at birth in the random initialization model is overridden by the capability of newborn DGCs to grow their weight vector in the appropriate direction during their early cooperative phase of maturation. Finally, we note that even if the length of the feedforward weight vector onto newborn DGCs is set to 1.5 (RandInitH., Supplementary File 5), which is the upper bound according to our analytical results (Methods), the random initialization model performs worse than the neurogenesis model for low up to relatively high-dimensional input spaces (PR=83, Supplementary File 5) despite its advantage in the competition conferred by the longer weight vector. It is only when input clusters are extremely broad and overlapping that the random initialization model performs similarly to the neurogenesis model (PR=90,97, Supplementary File 5). In other words, a random initialization at full length of weight vectors works well if input data is homogeneously distributed on the positive quadrant of the unit sphere but fails if the input data is clustered in a few directions. Moreover, random initialization requires that synaptic weights are large from the start which is biologically not plausible. In summary, the two-phase neurogenesis model is advantageous because the feedforward weights onto newborn cells can start at arbitrarily small values; their growth is, during the cooperative phase, guided to occur in a direction that is relevant for the task at hand; the final competitive phase eventually enables specialization onto novel inputs.”</p><p>Reply to point 1.1: The narrowness of the distribution of synaptic weight vector norms is indeed important. If the norm of a feedforward weight vector onto a particular DGC would be much larger than the norm of all other feedforward weight vectors onto other DGCs, then that particular DGC would be a winner of the competition upon presentation of more heterogeneous input patterns. In other words, that particular neuron would have much broader tuning, while other DGCs would have narrower tuning. However, our model ensures that the L2-norm of the feedforward weight vectors onto newborn DGCs at the end of their maturation reaches a value between a lower bound and an upper bound (see Methods, “Direction and length of the weight vector”). It implies that if input patterns are highly concentrated around a center of mass (for example in our standard handmade artificial dataset where the concentration parameter kappa=10<sup>4</sup>) the L2-norm will end up being higher than if patterns are broadly distributed around their center of mass (for example cases where kappa &lt; 10<sup>4</sup>). The only way for L2-norms to being widely different is if input patterns are heterogeneously distributed: for example if there is a high concentration around a center of mass (CM1) and a low concentration around another (CM2) then the DGC selective for CM1 will have a larger L2-norm than the DGC selective for CM2. We do not expect this to be a problem if the patterns from cluster 2 are far enough from the patterns of cluster 1 in input space. Furthermore, we note that if patterns from cluster 2 are distributed so broadly that they overlap with patterns from concentrated cluster 1, then: (1) those patterns will be classified by the network as belonging to cluster 1, even though they were initially generated from a “broad cluster 2” distribution; and (2) since those patterns now belong to cluster 1, they do not activate the DGC selective for cluster 2 anymore, hence the distribution of patterns that activate DGC2 becomes narrower and the L2-norm of DGC2 increases (and thus gets closer to the one of DGC1).</p><p>Reply to point 1.2: Our new results (see section “The cooperative phase of maturation promotes pattern separation for any dimensionality of input data”) shed light on this point. Briefly, our neurogenesis model still performs better than a random initialization model (aka “control 2”) even for relatively high input space dimensionalities, because the early cooperative phase acts as a smart initialization for the growth of the feedforward weight vector onto the newborn DGC in the appropriate direction. The higher the input space dimensionality, the more advantageous this smart initialization, as there is low probability that a randomly initialized feedforward weight vector onto a newborn cell is sufficiently well aligned with input patterns to become selective for them. It is only when classes are extremely broad and overlapping and that the feedforward weight vector starts with a large norm that the random initialization model performs as well as our neurogenesis model. These results agree with the experimental observation that adult dentate gyrus neurogenesis helps for the discrimination of similar patterns, but not distinct patterns. Similar patterns are close to each other's in input space, therefore smaller deeper basins of attraction are needed to discriminate them. On the other hand, distinct patterns are far from each other's in input space, hence larger and shallower basins of attraction are sufficient to discriminate between them.</p><disp-quote content-type="editor-comment"><p>2. Setting no plasticity (eta = 0) for mature cells is a very strong assumption. Some protocols (e.g., TBS<sub>2</sub> in Schmidt-Hieber 2004; and others in Ge 2007) lead to ~2 fold increase in plasticity in young vs. mature neurons. Since mature neurons significantly outnumber young neurons, the effect of plasticity in mature neurons cannot be neglected altogether, especially since the paper's main focus is on the integration of newborn neurons into the circuit. Given the actual degree of synaptic plasticity in mature neurons (according to the papers that the authors themselves cite) I expect the behavior of the authors model to be much closer to &quot;control 3&quot;. To support their claims, I think the authors should show that their network compares favorably to control 3 even if DGCs remain plastic throughout (but to a lesser extent). In this scenario I expect the fraction of neurons that are new at any given time to be much more important than the current model, since the mature part of the network is fixed. Therefore this fraction should also be matched to experiments.</p></disp-quote><p>The reviewer raises an interesting point. We addressed this concern with new simulations with the main and extended networks. We added a new Supplementary File 4, and report the new results in section “Robustness of the model” (lines 473-495):</p><p>“Finally, in our neurogenesis model, we have set the learning rate of mature DGCs to zero despite the observation that mature DGCs retain some plasticity (Schmidt-Hieber et al., 2004; Ge et al., 2007). We therefore studied a variant of the model in which mature DGCs also exhibit plasticity. First, we used our main model with 100 DGCs and 21 newborn DGCs. The implementation was identical, except that the learning rate of the mature DGCs was kept at a nonzero value during the maturation of the 21 newborn DGCs. We do not observe a large change in classification performance, even if the learning rate of the mature cells is the same as that of newborn cells (Supplementary File 4). Second, we used our extended network with 700 DGCs to be able to investigate the effect of plastic mature DGCs while having a proportion of newborn cells matching experiments. We find that with 35 newborn DGCs (corresponding to the experimentally reported fraction of about 5%), plastic mature DGCs (with a learning rate half of that of newborn cells) improve classification performance (Supplementary File 4). This is due to the fact that several of the mature DGCs (that were previously selective for '3's or '4's) become selective for prototypes of the novel digit 5. Consequently, more than the 35 newborn DGCs specialize for digit 5, so that digit 5 is eventually represented better by the network with mature cell plasticity than the standard network where plasticity is limited to newborn cells. Note that those mature DGCs that had earlier specialized on writing styles of digits 3 or 4 similar to a digit 5 are most likely to retune their selectivity. If the novel inputs were very distinct from the pretrained familiar inputs, mature DGCs would be unlikely to develop selectivity for the novel inputs.”</p><disp-quote content-type="editor-comment"><p>3. It is not clear to me how the two phase maturation process of DGCs would be affected in a scenario where at any given point some DGCs are in the excitatory phase of GABA and others are in the inhibitory phase. This would be expected if there is a continuous stream of new neurons. Would the plasticity of the neurons in the inhibitory phase not interfere with aligning the activity to similar stimuli due to plasticity of neurons in the excitatory phase? If there is interference, would the authors then predict that neurogenesis occurs in waves (i.e., some kind of global signal would coordinate transition from phase 1 to 2 across synapses)?</p><p>Is there evidence supporting that?</p><p>It seems to me that the calculation in the section &quot;Analytical computation of the L2-norm and angle&quot;--at least in principle--be extended to estimate the interference: the competition due to plasticity of neurons in the inhibitory phase increases the angle phi, and thus slows down the alignment of the weights due to plasticity of neurons in the excitatory phase.</p></disp-quote><p>It is a good point that in a true online scenario where newborn DGCs are born continuously, some of them would be in their early phase (excitatory phase of GABA) while others would be in their late phase (inhibitory phase of GABA). It is indeed interesting to determine if this would lead to interference in the proper maturation of any of these newborn DGCs. We tackle this question in two ways. (1) Do newborn DGCs in the late (GABA inhibitory) phase interfere with the ability of newborn DGCs in the early (GABA excitatory) phase to become selective for familiar inputs? (2) Do newborn DGCs in the early (GABA excitatory) phase interfere with the ability of newborn DGCs in the late (GABA inhibitory) phase to become selective for novel patterns?</p><p>1) In our model, newborn DGCs in the early phase of maturation (=earlyDGCs) receive indirect GABAergic excitation (through inhibitory neurons) from surrounding mature DGCs and from newborn DGCs which are in their late phase of maturation (=lateDGCs). Throughout their late phase of maturation, lateDGCs will become more and more selective for novel input patterns. Therefore, as they go through their late phase of maturation, they will push the configuration of the feedforward weight vector onto the earlyDGCs from towards the center of mass of input patterns that are well represented by mature DGCs to the center of mass of all input patterns (i.e. also the ones that were introduced during the early phase of maturation of lateDGCs). We therefore do not expect lateDGCs to interfere with the ability of earlyDGCs to become selective for familiar inputs. Rather, at the end of their maturation, earlyDGCs will either ultimately become selective for novel patterns that are similar to the ones for which mature DGCs are selective (aka different prototypes of familiar inputs), or similar to the ones for which lateDGCs eventually became selective. The alignment itself will depend on the stage of lateDGCs that indirectly activate the earlyDGCs.</p><p>2) In our model, newborn DGCs in the early phase of maturation (=earlyDGCs) do not project yet to inhibitory neurons (or any neurons). Therefore, they do not affect the circuit, and the activity of newborn DGCs in the late phase of maturation (=lateDGCs) is independent of the activity of earlyDGCs. Therefore, we do not expect a “slowing down of the alignment of the weights due to plasticity of neurons in the excitatory phase”.</p><p>Accordingly, our model does not predict that neurogenesis occurs in waves, and we are not aware of experimental evidence suggesting it.</p><disp-quote content-type="editor-comment"><p>107, Review of functional role of DGCs.</p><p>Aljadeff at al., 2015,</p><p>Shani-Narkiss et al., 2020</p><p>suggest a dynamical role for new neurons.</p></disp-quote><p>Thanks, they are now included in lines 108-109.</p><disp-quote content-type="editor-comment"><p>312, It would be interesting if the advantage of adding newborn neurons stimulated with &quot;5&quot; to a network pretrained with &quot;3&quot; and &quot;4&quot; over a network pretrained with &quot;3&quot;, &quot;4&quot;, and &quot;5&quot; would persist if some amount of plasticity remains in mature neurons (Figure 3d).</p></disp-quote><p>We hope that we answered this point in the “main concern 2” and related further analyses. Plastic mature DGCs do not affect classification performance of our neurogenesis model. Rather, it does improve it in some cases. Therefore, it is closer to the main model in terms of performance, and still better than simultaneous pretraining of digits 3, 4 and 5 (our “control 1”).</p><disp-quote content-type="editor-comment"><p>614, It would be good to discuss the possibility that neurotransmitter switch (without neurogenesis) has the same functional role as GABA switch in the current model. See e.g., Li et al., (2020) J Neuroscience.</p><p>Furthermore, can this model teach us anything about neurogenesis in the olfactory bulb? Is there an E to I switch there too?</p></disp-quote><p>We now touch upon a potential link between neurotransmitter switching and the GABA switch in our model in lines 832-837:</p><p>“Finally, while neurotransmitter switching has been observed following sustained stimulation for hours to days (Li et al., 2020), it is still unclear if it has the same functional role as the GABA switch in our model. In particular, it remains an open question if neurotransmitter switching promotes the integration of neurons in the same way as our model GABA switch does in the context of adult dentate gyrus neurogenesis.”</p><p>The suitability of our model to investigate adult olfactory bulb neurogenesis is now considered in the Discussion (lines 779-790):</p><p>“The parallel of neurogenesis in dentate gyrus and olfactory bulb suggests that similar mechanisms could be at work in both areas. Yet, even though adult olfactory bulb neurogenesis seems to have a similar functional role to adult dentate gyrus neurogenesis (Sahay et al., 2011b), follow a similar integration sequence and undergo a GABA switch from excitatory to inhibitory, the circuits are different in several aspects. First, while newborn neurons in dentate gyrus are excitatory, newborn cells in the olfactory bulb are inhibitory. Second, the newborn olfactory cells start firing action potentials only once they are well integrated (Carleton et al., 2003). Therefore, in view of a transfer of results to the olfactory bulb, it would be interesting to adjust our model of adult dentate gyrus neurogenesis accordingly. For example, a voltage-based synaptic plasticity rule could be used to account for subthreshold plasticity mechanisms (Clopath et al., 2010).”</p><disp-quote content-type="editor-comment"><p>725, Miller and Fumarola may not be the right reference to cite here. This specific nonlinearity (rectified tanh) is not standard and is not included in that paper.</p></disp-quote><p>Thanks for pointing out that our formulation was too dense here. In fact, there are two different aspects.</p><p>i) In Miller and Fumarola, (2012), they show the mathematical equivalence of two expressions commonly used for rate models: a “voltage equation” (their equation (1)) and a “firing rate equation” (their equation (2)). The function ‘f’ in their firing rate equation can be any nonlinear function. Therefore, our rate equation (2) corresponds to their equation (2) with a particular choice for the nonlinear function ‘f’: the rectified hyperbolic tangent.</p><p>ii) The rectified hyperbolic tangent function is a choice that “combines a hard threshold with a saturation” (equation (2.11) in the “Theoretical Neuroscience” book from Dayan and Abbott). Through rectification, negative firing rates, which are not biologically plausible, are avoided. Another way to avoid negative firing rate without rectification of the input would be to use ½ + (½)*tanh(input). The main difference is that the firing rate increases more slowly from 0 in that case, while it increases linearly with our rectified tanh. The rectified tanh is also closer to the f-I curve of spiking neuron models such as the leaky integrate-and-fire model with absolute refractory period.</p><p>To clarify these issues, in the new version of the paper we now write (lines 871-876):</p><disp-quote content-type="editor-comment"><p>“We assume that the DGCs have a frequency-current curve that is given by a rectified hyperbolic tangent (Dayan and Abbott, 2001) which is similar to the frequency-current curve of spiking neuron models with refractoriness (Gerstner et al., 2014). Moreover, we exploit the equivalence of two common firing rate equations (Miller and Fumarola, 2012) and let the firing rate nu<sub>i</sub> of DGC i upon stimulation with input pattern <inline-formula><mml:math id="inf460"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> evolve according to: […]”</p><p>778, Definition of quasi orthogonal is not clear. The inhibitory rates can have fluctuations and temporal dynamics of their own even if the network is assumed to be silent when each stimulus is presented. Therefore inputs might be quasi-orthogonal at one time but not at another. If in this is used just to qualitatively understand the network behavior, this somewhat sloppy definition is ok, but I think this caveat should be mentioned to avoid confusion.</p></disp-quote><p>Thanks for pointing this out. We now clarify that the definition is applied to the stationary state (i.e., after the transient); note that the model is noise-free and expected to be non-chaotic so that, apart from an initial transient, fluctuations cannot appear with stationary input. We now write (lines 943-954):</p><p>“We emphasize that all synaptic weights, and all presynaptic firing rates nu<sub>j</sub> are non-negative: w<sub>ij</sub> ≥ 0 and nu<sub>j</sub> ≥ 0. […] Note that for a case without inhibitory neurons and with b<sub>i</sub> -&gt; 0, we recover the standard orthogonality condition, but for finite b<sub>I &gt;</sub> 0 quasi-orthogonality corresponds to angles larger than some reference angle.”</p></body></sub-article></article>