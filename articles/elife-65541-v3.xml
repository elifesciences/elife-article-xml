<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">65541</article-id><article-id pub-id-type="doi">10.7554/eLife.65541</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Creating and controlling visual environments using BonVision</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-201764"><name><surname>Lopes</surname><given-names>Gonçalo</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0731-4945</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-218917"><name><surname>Farrell</surname><given-names>Karolina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0707-2838</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-218918"><name><surname>Horrocks</surname><given-names>Edward AB</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4019-5351</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-218919"><name><surname>Lee</surname><given-names>Chi-Yu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6440-3050</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-183142"><name><surname>Morimoto</surname><given-names>Mai M</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9654-3960</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-210980"><name><surname>Muzzu</surname><given-names>Tomaso</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0018-8416</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-206288"><name><surname>Papanikolaou</surname><given-names>Amalia</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0048-6560</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-218920"><name><surname>Rodrigues</surname><given-names>Fabio R</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-4848-7167</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con8"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-218921"><name><surname>Wheatcroft</surname><given-names>Thomas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7990-3744</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con9"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" id="author-218922"><name><surname>Zucca</surname><given-names>Stefano</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8891-1225</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con10"/><xref ref-type="fn" rid="conf2"/><xref ref-type="fn" rid="fn1">‡</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-107594"><name><surname>Solomon</surname><given-names>Samuel G</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5321-0288</contrib-id><email>s.solomon@ucl.ac.uk</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con11"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-140683"><name><surname>Saleem</surname><given-names>Aman B</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7100-1954</contrib-id><email>aman.saleem@ucl.ac.uk</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund8"/><xref ref-type="other" rid="fund5"/><xref ref-type="other" rid="fund7"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con12"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution>NeuroGEARS Ltd.</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>UCL Institute of Behavioural Neuroscience, Department of Experimental Psychology, University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Senior Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="other" id="fn1"><label>‡</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn2"><label>§</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn3"><label>#</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn4"><label>¶</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn5"><label>**</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn6"><label>††</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn7"><label>‡‡</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn8"><label>§§</label><p>These authors are listed alphabetically</p></fn><fn fn-type="other" id="fn9"><label>##</label><p>These authors are listed alphabetically</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>21</day><month>04</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e65541</elocation-id><history><date date-type="received" iso-8601-date="2020-12-07"><day>07</day><month>12</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-04-20"><day>20</day><month>04</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Lopes et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Lopes et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-65541-v3.pdf"/><abstract><p>Real-time rendering of closed-loop visual environments is important for next-generation understanding of brain function and behaviour, but is often prohibitively difficult for non-experts to implement and is limited to few laboratories worldwide. We developed BonVision as an easy-to-use open-source software for the display of virtual or augmented reality, as well as standard visual stimuli. BonVision has been tested on humans and mice, and is capable of supporting new experimental designs in other animal models of vision. As the architecture is based on the open-source Bonsai graphical programming language, BonVision benefits from native integration with experimental hardware. BonVision therefore enables easy implementation of closed-loop experiments, including real-time interaction with deep neural networks, and communication with behavioural and physiological measurement and manipulation devices.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>spatial vision</kwd><kwd>navigation</kwd><kwd>virtual reality</kwd><kwd>augmented reality</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Mouse</kwd><kwd>Rat</kwd><kwd>Zebrafish</kwd></kwd-group><funding-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>200501/Z/16/A</award-id><principal-award-recipient><name><surname>Saleem</surname><given-names>Aman B</given-names></name></principal-award-recipient></award-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship (200501)</award-id><principal-award-recipient><name><surname>Saleem</surname><given-names>Aman B</given-names></name></principal-award-recipient></award-group><award-group id="fund8"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>Sir Henry Dale Fellowship (200501)</award-id><principal-award-recipient><name><surname>Saleem</surname><given-names>Aman B</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>R023808</award-id><principal-award-recipient><name><surname>Solomon</surname><given-names>Samuel G</given-names></name><name><surname>Saleem</surname><given-names>Aman B</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004343</institution-id><institution>Stavros Niarchos Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Solomon</surname><given-names>Samuel G</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000268</institution-id><institution>Biotechnology and Biological Sciences Research Council</institution></institution-wrap></funding-source><award-id>R004765</award-id><principal-award-recipient><name><surname>Solomon</surname><given-names>Samuel G</given-names></name><name><surname>Saleem</surname><given-names>Aman B</given-names></name></principal-award-recipient></award-group><award-group id="fund7"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004412</institution-id><institution>Human Frontier Science Program</institution></institution-wrap></funding-source><award-id>RGY0076/2018</award-id><principal-award-recipient><name><surname>Saleem</surname><given-names>Aman B</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>BonVision is an open-source software for real-time rendering and control of 2D and 3D visual environments that also enables easy replication of experiments.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Understanding behaviour and its underlying neural mechanisms calls for the ability to construct and control environments that immerse animals, including humans, in complex naturalistic environments that are responsive to their actions. Gaming-driven advances in computation and graphical rendering have driven the development of immersive closed-loop visual environments, but these new platforms are not readily amenable to traditional research paradigms. For example, they do not specify an image in egocentric units (of visual angle), sacrifice precise control of a visual display, and lack transparent interaction with external hardware.</p><p>Most vision research has been performed in non-immersive environments with standard two-dimensional visual stimuli, such as gratings or dot stimuli, using established platforms including PsychToolbox (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>) or PsychoPy (<xref ref-type="bibr" rid="bib21">Peirce, 2007</xref>; <xref ref-type="bibr" rid="bib22">Peirce, 2008</xref>). Pioneering efforts to bring gaming-driven advances to neuroscience research have provided new platforms for closed-loop visual stimulus generation: STYTRA (<xref ref-type="bibr" rid="bib29">Štih et al., 2019</xref>) provides 2D visual stimuli for larval zebrafish in python, ratCAVE (<xref ref-type="bibr" rid="bib8">Del Grosso and Sirota, 2019</xref>) is a specialised augmented reality system for rodents in python, FreemoVR (<xref ref-type="bibr" rid="bib30">Stowers et al., 2017</xref>) provides virtual reality in Ubuntu/Linux, and ViRMEn (<xref ref-type="bibr" rid="bib2">Aronov and Tank, 2014</xref>) provides virtual reality in Matlab. However, these new platforms lack the generalised frameworks needed to specify or present standard visual stimuli.</p><p>Our initial motivation was to create a visual display software with three key features. First, an integrated, standardised platform that could rapidly switch between traditional visual stimuli (such as grating patterns) and immersive virtual reality. Second, the ability to replicate experimental workflows across different physical configurations (e.g. when moving from one to two computer monitors, or from flat-screen to spherical projection). Third, the ability for rapid and efficient interfacing with external hardware (needed for experimentation) without needing to develop complex multi-threaded routines. We wanted to provide these advances in a way that made it easier for users to construct and run closed-loop experimental designs. In closed-loop experiments, stimuli are ideally conditioned by asynchronous inputs, such as those provided by multiple independent behavioural and neurophysiological measurement devices. Most existing platforms require the development of multi-threaded routines to run experimental paradigms (e.g. control brain stimulation, or sample from recording devices) without compromising the rendering of visual scenes. Implementing such multi-thread routines is complex. We therefore chose to develop a visual presentation framework within the Bonsai programming language (<xref ref-type="bibr" rid="bib14">Lopes et al., 2015</xref>). Bonsai is a graphical, high-performance, and event-based language that is widely used in neuroscience experiments and is already capable of real-time interfacing with most types of external hardware. Bonsai is specifically designed for flexible and high-performance composition of data streams and external events, and is therefore able to monitor and connect multiple sensor and effector systems in parallel, making it easier to implement closed-loop experimental designs.</p><p>We developed BonVision, an open-source software package that can generate and display well-defined visual stimuli in 2D and 3D environments. BonVision exploits Bonsai’s ability to run OpenGL commands on the graphics card through the Bonsai.Shaders package. BonVision further extends Bonsai by providing pre-built GPU shaders and resources for stimuli used in vision research, including movies, along with an accessible, modular interface for composing stimuli and designing experiments. The definition of stimuli in BonVision is independent of the display hardware, allowing for easy replication of workflows across different experimental configurations. Additional unique features include the ability to automatically detect and define the relationship between the observer and the display from a photograph of the experimental apparatus, and to use the outputs of real-time inference methods to determine the position and pose of an observer online, thereby generating augmented reality environments.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To provide a framework that allowed both traditional visual presentation and immersive virtual reality, we needed to bring these very different ways of defining the visual scene into the same architecture. We achieved this by mapping the 2D retino-centric coordinate frame (i.e. degrees of the visual field) to the surface of a 3D sphere using the Mercator projection (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). The resulting sphere could therefore be rendered onto displays in the same way as any other 3D environment. We then used ‘cube mapping’ to specify the 360° projection of 3D environments onto arbitrary viewpoints around an experimental observer (human or animal; <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Using this process, a display device becomes a window into the virtual environment, where each pixel on the display specifies a vector from the observer through that window. The vector links pixels on the display to pixels in the ‘cube map’, thereby rendering the corresponding portion of the visual field onto the display.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>BonVision's adaptable display and render configurations.</title><p>(<bold>A</bold>) Illustration of how two-dimensional textures are generated in BonVision using Mercator projection for sphere mapping, with elevation as latitude and azimuth as longitude. The red dot indicates the position of the observer. (<bold>B</bold>) Three-dimensional objects were placed at the appropriate positions and the visual environment was rendered using cube-mapping. (<bold>C–E</bold>) Examples of the same two stimuli, a checkerboard + grating (middle row) or four three-dimensional objects (bottom row), displayed in different experimental configurations (top row): two angled LCD monitors (<bold>C</bold>), a head-mounted display (<bold>D</bold>), and demi-spherical dome (<bold>E</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig1-v3.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Mapping stimuli onto displays in various positions.</title><p>(<bold>A</bold>) Checkerboard stimulus being rendered. (<bold>B</bold>) Projection of the stimulus onto a sphere using Mercator projection. (<bold>C</bold>) Example display positions (dA–dF) and (<bold>D</bold>) corresponding rendered images. Red dot in <bold>C</bold> indicates the observer position.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig1-figsupp1-v3.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Modular structure of workflow and example workflows.</title><p>(<bold>A</bold>) Description of the modules in BonVision workflows that generate stimuli. Every BonVision stimuli includes a module that creates and initialises the render window, shown in ‘BonVision window and resources’. This defines the window parameters in <italic>Create Window</italic> (such as background colour, screen index, VSync), and loads predefined (<italic>BonVision Resources</italic>) and user defined textures (<italic>Texture Resources</italic>, not shown), and 3D meshes (<italic>Mesh Resources</italic>). This is followed by the modules: ‘Drawing region’, where the visual space covered by the stimuli is defined, which can be the complete visual space, 360° × 360°. ‘Draw stimuli’ and ‘Define scene’ are where the stimulus is defined, ‘Map Stimuli’, which maps the stimuli into the 3D environment, and ‘Define display’, where the display devices are defined. (<bold>B and C</bold>) Modules that define the checkerboard +grating stimulus (<bold>B</bold>) shown in the middle row of <xref ref-type="fig" rid="fig1">Figure 1</xref> and 3D world (<bold>C</bold>) with five objects shown in the bottom row of <xref ref-type="fig" rid="fig1">Figure 1</xref>. The display device is defined separately and either display can be appended at the end of the workflow. This separation of the display device allows for replication between experimental configurations. (<bold>D</bold>) The variants of the modules used to display stimuli on a head-mounted display. The empty region under ‘Define scene’ would be filled by the corresponding nodes in <bold>B</bold> and <bold>C</bold>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig1-figsupp2-v3.tif"/></fig></fig-group><p>Our approach has the advantage that the visual stimulus is defined irrespectively of display hardware, allowing us to independently define each experimental apparatus without changing the preceding specification of the visual scene, or the experimental design (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplements 1</xref> and <xref ref-type="fig" rid="fig1s2">2</xref>). Consequently, BonVision makes it easy to replicate visual environments and experimental designs on various display devices, including multiple monitors, curved projection surfaces, and head-mounted displays (<xref ref-type="fig" rid="fig1">Figure 1C–E</xref>). To facilitate easy and rapid porting between different experimental apparatus, BonVision features a fast semi-automated display calibration. A photograph of the experimental setup with fiducial markers (<xref ref-type="bibr" rid="bib10">Garrido-Jurado et al., 2014</xref>) measures the 3D position and orientation of each display relative to the observer (<xref ref-type="fig" rid="fig2">Figure 2</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). BonVision’s inbuilt image processing algorithms then estimate the position and orientation of each marker to fully specify the display environment.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Automated calibration of display position.</title><p>(<bold>A</bold>) Schematic showing the position of two hypothetical displays of different sizes, at different distances and orientation relative to the observer (red dot). (<bold>B</bold>) How a checkerboard of the same visual angle would appear on each of the two displays. (<bold>C</bold>) Example of automatic calibration of display position. Standard markers are presented on the display, or in the environment, to allow automated detection of the position and orientation of both the display and the observer. These positions and orientations are indicated by the superimposed red cubes as calculated by BonVision. (<bold>D</bold>) How the checkerboard would appear on the display when rendered, taking into account the precise position of the display. (<bold>E and F</bold>) Same as (<bold>C and D</bold>), but for another pair of display and observer positions. The automated calibration was based on the images shown in <bold>C</bold> and <bold>E</bold>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Automated workflow to calibrate display position.</title><p>The automated calibration is carried out by taking advantage of ArUco markers (<xref ref-type="bibr" rid="bib8">Del Grosso and Sirota, 2019</xref>) that can be used to calculate the 3D position of a surface. (<bold>Ai</bold>) We use one marker on the display and one placed in the position of the observer. We then use a picture of the display and observer position taken by a calibrated camera. This is an example where we used a mobile phone camera for calibration. (<bold>Aii</bold>) The detected 3D positions of the screen and the observer, as calculated by BonVision. (<bold>Aiii</bold>) A checkerboard image and a small superimposed patch of grating, rendered based on the precise position of the display. (<bold>B and C</bold>) same as <bold>A and C</bold> for different screen and observer positions: with the screen tilted towards the animal (<bold>B</bold>), or the observer shifted to the right of the screen (<bold>C</bold>). The automated calibration was based on the images shown in <bold>Ai</bold>, <bold>Bi</bold>, and <bold>Ci</bold>, which in this case were taken using a mobile phone camera.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig2-figsupp1-v3.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Automated gamma-calibration of visual displays.</title><p>BonVision monitored a photodiode (Photodiode v2.1, <ext-link ext-link-type="uri" xlink:href="https://www.cf-hw.org/harp/behavior">https://www.cf-hw.org/harp/behavior</ext-link>) through a HARP microprocessor to measure the light output of the monitor (Dell Latitude 7480). The red, green, and blue channels of the display were sent the same values (i.e. grey scale). (<bold>A</bold>) Gamma calibration. The input to the display channels was modulated by a linear ramp (range 0–255). Without calibration the monitor output (arbitrary units) increased exponentially (blue line). The measurement was then used to construct an intermediate look-up table that corrected the values sent to the display. Following calibration, the display intensity is close to linear (red line). Inset at top: schematic of the experimental configuration. (<bold>B</bold>) Similar to <bold>A</bold>, but showing the intensity profile of a drifting sinusoidal grating. Measurements before calibration resemble an exponentiated sinusoid (blue dotted line). Measurements after calibration resemble a regular sinusoid (red dotted line).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig2-figsupp2-v3.tif"/></fig></fig-group><p>Virtual reality environments are easy to generate in BonVision. BonVision has a library of standard pre-defined 3D structures (including planes, spheres, and cubes), and environments can be defined by specifying the position and scale of the structures, and the textures rendered on them (e.g. <xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref> and Figure 5F). BonVision also has the ability to import standard format 3D design files created elsewhere in order to generate more complex environments (file formats listed in Materials and methods). This allows users to leverage existing 3D drawing platforms (including open source platform ‘Blender’: <ext-link ext-link-type="uri" xlink:href="https://www.blender.org/">https://www.blender.org/</ext-link>) to construct complex virtual scenes (see Appendix 1).</p><p>BonVision can define the relationship between the display and the observer in real-time. This makes it easy to generate augmented reality environments, where what is rendered on a display depends on the position of an observer (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). For example, when a mouse navigates through an arena surrounded by displays, BonVision enables closed-loop, position-dependent updating of those displays. Bonsai can track markers to determine the position of the observer, but it also has turn-key capacity for real-time live pose estimation techniques – using deep neural networks (<xref ref-type="bibr" rid="bib16">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib13">Kane et al., 2020</xref>) – to keep track of the observer’s movements. This allows users to generate and present interactive visual environments (simulation in <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref> and <xref ref-type="fig" rid="fig3">Figure 3B and C</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Using BonVision to generate an augmented reality environment.</title><p>(<bold>A</bold>) Illustration of how the image on a fixed display needs to adapt as an observer (red dot) moves around an environment. The displays simulate windows from a box into a virtual world outside. (<bold>B</bold>) The virtual scene (from: <ext-link ext-link-type="uri" xlink:href="http://scmapdb.com/wad">http://scmapdb.com/wad</ext-link>:skybox-skies) that was used to generate the example images and <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref> offline. (<bold>C</bold>) Real-time simulation of scene rendering in augmented reality. We show two snapshots of the simulated scene rendering, which is also shown in <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>. In each case the inset image shows the actual video images, of a mouse exploring an arena, that were used to determine the viewpoint of an observer in the simulation. The mouse’s head position was inferred (at a rate of 40 frames/s) by a network trained using DeepLabCut (<xref ref-type="bibr" rid="bib2">Aronov and Tank, 2014</xref>). The top image shows an instance when the animal was on the left of the arena (head position indicated by the red dot in the main panel) and the lower image shows an instance when it was on the right of the arena.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig3-v3.tif"/></fig><media id="fig3video1" mime-subtype="mp4" mimetype="video" xlink:href="elife-65541-fig3-video1.mp4"><label>Figure 3—video 1.</label><caption><title>Augmented reality simulation using BonVision.</title><p>This video is an example of a deep neural network, trained with DeepLabCut, being used to estimate the position of a mouse’s head in an environment in real-time, and updating a virtual scene presented on the monitors based on this estimated position. The first few seconds of the video display the online tracking of specific features (nose, head, and base of tail) while an animal is moving around (shown as a red dot) in a three-port box (as in <xref ref-type="bibr" rid="bib28">Soares et al., 2016</xref>). Subsequently the inset shows the original video of the animal’s movements, which the simulation is based on. The rest of the video image shows how a green field landscape (source: <ext-link ext-link-type="uri" xlink:href="http://scmapdb.com/wad">http://scmapdb.com/wad</ext-link>:skybox-skies) outside the box would be rendered on three simulated displays within the box (one placed on each of the three oblique walls). These three displays simulate windows onto the world beyond the box. The position of the animal was updated by DeepLabCut at 40 frames/s, and the simulation was rendered at the same rate.</p></caption></media></fig-group><p>BonVision is capable of rendering visual environments near the limits of the hardware (<xref ref-type="fig" rid="fig4">Figure 4</xref>). This is possible because Bonsai is based on a just-in-time compiler architecture such that there is little computational overhead. BonVision accumulates a list of the commands to OpenGL as the programme makes them. To optimise rendering performance, the priority of these commands is ordered according to that defined in the Shaders component of the <italic>LoadResources</italic> node (which the user can manipulate for high-performance environments). These ordered calls are then executed when the frame is rendered. To benchmark the responsiveness of BonVision in closed-loop experiments, we measured the delay (latency) between an external event and the presentation of a visual stimulus. We first measured the closed-loop latency for BonVision when a monitor was refreshed at a rate of 60 Hz (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We found delays averaged 2.11 ± 0.78 frames (35.26 ± 13.07 ms). This latency was slightly shorter than that achieved by PsychToolbox (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>) on the same laptop (2.44 ± 0.59 frames, 40.73 ± 9.8 ms; Welch’s t-test, p&lt;10<sup>−80</sup>, n = 1000). The overall latency of BonVision was mainly constrained by the refresh rate of the display device, such that higher frame rate displays yielded lower latency (60 Hz: 35.26 ± 13.07 ms; 90 Hz: 28.45 ± 7.22 ms; 144 Hz: 18.49 ± 10.1 ms; <xref ref-type="fig" rid="fig4">Figure 4A</xref>). That is, the number of frames between the external event and stimulus presentation was similar across frame rate (60 Hz: 2.11 ± 0.78 frames; 90 Hz: 2.56 ± 0.65 frames; 144 Hz: 2.66 ± 1.45 frames; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). We used two additional methods to benchmark visual display performance relative to other frameworks (we did not try to optimise code fragments for each framework) (<xref ref-type="fig" rid="fig4">Figure 4B and C</xref>). BonVision was able to render up to 576 independent elements and up to eight overlapping textures at 60 Hz without missing (‘dropping’) frames, broadly matching PsychoPy (<xref ref-type="bibr" rid="bib21">Peirce, 2007</xref>; <xref ref-type="bibr" rid="bib22">Peirce, 2008</xref>) and Psychtoolbox (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>). BonVision’s performance was similar at different frame rates – at standard frame rate (60 Hz) and at 144 Hz (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). BonVision achieved slightly fewer overlapping textures than PsychoPy, as BonVision does not currently have the option to trade-off the resolution of a texture and its mask for performance. BonVision also supports video playback, either by preloading the video or by streaming it from the disk. The streaming mode, which utilises real-time file I/O and decompression, is capable of displaying both standard definition (SD: 480 p) and full HD (HD: 1080 p) at 60 Hz on a standard computer (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). At higher rates, performance is impaired for Full HD videos, but is improved by buffering, and fully restored by preloading the video onto memory (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). We benchmarked BonVision on a standard Windows OS laptop, but BonVision is now also capable of running on Linux.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Closed-loop latency and performance benchmarks.</title><p>(<bold>A</bold>) Latency between sending a command (virtual key press) and updating the display (measured using a photodiode). (<bold>A.i and A.ii</bold>) Latency depended on the frame rate of the display, updating stimuli with a delay of one to three frames. (<bold>A.iii and A.iv</bold>). (<bold>B and C</bold>) Benchmarked performance of BonVision with respect to Psychtoolbox and PsychoPy. (<bold>B</bold>) When using non-overlapping textures BonVision and Psychtoolbox could present 576 independent textures without dropping frames, while PsychoPy could present 16. (<bold>C</bold>) When using overlapping textures PsychoPy could present 16 textures, while BonVision and Psychtoolbox could present eight textures without dropping frames. (<bold>D</bold>) Benchmarks for movie playback. BonVision is capable of displaying standard definition (480 p) and high definition (1080 p) movies at 60 frames/s on a laptop computer with a standard CPU and graphics card. We measured display rate when fully pre-loading the movie into memory (blue), or when streaming from disk (with no buffer: orange; 1-frame buffer: green; 2-frame buffer: red; 4-frame buffer: purple). When asked to display at rates higher than the monitor refresh rate (&gt;60 frames/s), the 480 p video played at the maximum frame rate of 60fps in all conditions, while the 1080 p video reached the maximum rate when pre-loaded. Using a buffer slightly improved performance. A black square at the bottom right of the screen in <bold>A–C</bold> is the position of a flickering rectangle, which switches between black and white at every screen refresh. The luminance in this square is detected by a photodiode and used to measure the actual frame flip times.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>BonVision performance benchmarks at high frame rate.</title><p>(<bold>A</bold>) When using non-overlapping textures BonVision was able to render 576 independent textures without dropping frames at 60 Hz. At 144 Hz BonVision was able to 256 non-overlapping textures, with no dropped frames, and seldom dropped frames with 576 textures. BonVision was unable to render 1024 or more textures at the requested frame rate. (<bold>B</bold>) When using overlapping textures BonVision was able to render 64 independent textures without dropping frames at 60 Hz. At 144 Hz BonVision was able to render 32 textures, with no dropped frames. Note that these tests were performed on a computer with better hardware specification than that used in <xref ref-type="fig" rid="fig4">Figure 4</xref>, which led to improved performance on the benchmarks at 60 Hz. A black square at the bottom right of the screen in <bold>A and B</bold> is the position of a flickering rectangle, which switches between black and white at every screen refresh. The luminance in this square is detected by a photodiode and used to measure the actual frame flip times.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig4-figsupp1-v3.tif"/></fig></fig-group><p>To confirm that the rendering speed and timing accuracy of BonVision are sufficient to support neurophysiological experiments, which need high timing precision, we mapped the receptive fields of neurons early in the visual pathway (<xref ref-type="bibr" rid="bib31">Yeh et al., 2009</xref>), in the mouse primary visual cortex and superior colliculus. The stimulus (‘sparse noise’) consisted of small black or white squares briefly (0.1 s) presented at random locations (<xref ref-type="fig" rid="fig5">Figure 5A</xref>). This stimulus, which is commonly used to measure receptive fields of visual neurons, is sensitive to the timing accuracy of the visual stimulus, meaning that errors in timing would prevent the identification of receptive fields. In our experiments using BonVision, we were able to recover receptive fields from electrophysiological measurements - both in the superior colliculus and primary visual cortex of awake mice (<xref ref-type="fig" rid="fig5">Figure 5B and C</xref>) – demonstrating that BonVision meets the timing requirements for visual neurophysiology. The receptive fields show in <xref ref-type="fig" rid="fig5">Figure 5C</xref> were generated using timing signals obtained directly from the stimulus display (via a photodiode). BonVision’s independent logging of stimulus presentation timing was also sufficient to capture the receptive field (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Illustration of BonVision across a range of vision research experiments.</title><p>(<bold>A</bold>) Sparse noise stimulus, generated with BonVision, is rendered onto a demi-spherical screen. (<sc><bold>B </bold></sc>and<sc><bold> C</bold></sc>) Receptive field maps from recordings of local field potential in the superior colliculus (<bold>B</bold>), and spiking activity in the primary visual cortex (<bold>C</bold>) of mouse. (<bold>D</bold>) Two cubes were presented at different depths in a virtual environment through a head-mounted display to human subjects. Subjects had to report which cube was larger: left or right. (<bold>E</bold>) Subjects predominantly reported the larger object correctly, with a slight bias to report that the object in front was bigger. (<bold>F</bold>) BonVision was used to generate a closed-loop virtual platform that a mouse could explore (top: schematic of platform). Mice naturally tended to run faster along the platform, and in later sessions developed a speed profile, where they slowed down as they approached the end of the platform (virtual cliff). (<bold>G</bold>) The speed of the animal at the start of the platform and at the end of the platform as a function training. (<bold>H</bold>) BonVision was used to present visual stimuli overhead while an animal was free to explore an environment (which included a refuge). The stimulus was a small dot (5° diameter) moving across the projected surface over several seconds. (<bold>I</bold>) The cumulative probability of Freeze and Flight behaviour across time in response to moving dot presented overhead.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>BonVision timing logs are sufficient to support receptive field mapping of spiking activity.</title><p>Top row in each case shows the receptive field identified using the timing information provided by a photodiode that monitored a small square on the stimulus display that was obscured from the animal. Bottom row in each case shows the receptive field identified by using the timing logged by BonVision during the stimulus presentation (a separate timing system was used to align the clocks between the computer hosting BonVision and the Open EPhys recording device). (<bold>A</bold>) Average OFF and ON receptive field maps for 33 simultaneously recorded units in a single recording session. (<bold>B</bold>) Individual OFF receptive field maps for three representative units in the same session.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-65541-fig5-figsupp1-v3.tif"/></fig></fig-group><p>To assess the ability of BonVision to control virtual reality environments we first tested its ability to present stimuli to human observers on a head-mounted display (<xref ref-type="bibr" rid="bib25">Scarfe and Glennerster, 2015</xref>). BonVision uses positional information (obtained from the head-mounted display) to update the view of the world that needs to be provided to each eye, and returns two appropriately rendered images. On each trial, we asked observers to identify the larger of two non-overlapping cubes that were placed at different virtual depths (<xref ref-type="fig" rid="fig5">Figure 5D and E</xref>). The display was updated in closed-loop to allow observers to alter their viewpoint by moving their head. Distinguishing objects of the same retinal size required observers to use depth-dependent cues (<xref ref-type="bibr" rid="bib24">Rolland et al., 1995</xref>), and we found that all observers were able to identify which cube was larger (<xref ref-type="fig" rid="fig5">Figure 5E</xref>).</p><p>We next asked if BonVision was capable of supporting other visual display environments that are increasingly common in the study of animal behaviour. We first projected a simple environment onto a dome that surrounded a head-fixed mouse (as shown in <xref ref-type="fig" rid="fig1">Figure 1E</xref>). The mouse was free to run on a treadmill, and the treadmill’s movements were used to update the mouse’s position on a virtual platform (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). Not only did mouse locomotion speed increase with repeated exposure, but the animals modulated their speed depending on their location in the platform (<xref ref-type="fig" rid="fig5">Figure 5F and G</xref>). BonVision is therefore capable of generating virtual reality environments which both elicit and are responsive to animal behaviour. BonVision was also able to produce instinctive avoidance behaviours in freely moving mice (<xref ref-type="fig" rid="fig5">Figure 5H and I</xref>). We displayed a small black dot slowly sweeping across the overhead visual field. Visual stimuli presented in BonVision primarily elicited a freezing response, which similar experiments have previously described (<xref ref-type="bibr" rid="bib7">De Franceschi et al., 2016</xref>; <xref ref-type="fig" rid="fig5">Figure 5I</xref>). Together these results show that BonVision provides sufficient rendering performance to support human and animal visual behaviour.</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>BonVision is a single software package to support experimental designs that require visual display, including virtual and augmented reality environments. BonVision is easy and fast to implement, cross-platform and open source, providing versatility and reproducibility.</p><p>BonVision makes it easier to address several barriers to reproducibility in visual experiments. First, BonVision is able to replicate and deliver visual stimuli on very different experimental apparatus. This is possible because BonVision’s architecture separates specification of the display and the visual environment. Second, BonVision includes a library of workflows and operators to standardise and ease the construction of new stimuli and virtual environments. For example, it has established protocols for defining display positions (<xref ref-type="fig" rid="fig3">Figure 3</xref>), mesh-mapping of curved displays (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), and automatic linearisation of display luminance (<xref ref-type="fig" rid="fig4">Figure 4</xref>), as well as a library of examples for experiments commonly used in visual neuroscience. In addition, the modular structure of BonVision enables the development and exchange of custom nodes for generating new visual stimuli or functionality without the need to construct the complete experimental paradigm. Third, BonVision is based on Bonsai (<xref ref-type="bibr" rid="bib14">Lopes et al., 2015</xref>), which has a large user base and an active developer community, and is now a standard tool for open-source neuroscience research. BonVision naturally integrates Bonsai’s established packages in the multiple domains important for modern neuroscience, which are widely used in applications including real-time video processing (<xref ref-type="bibr" rid="bib32">Zacarias et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Buccino et al., 2018</xref>), optogenetics (<xref ref-type="bibr" rid="bib32">Zacarias et al., 2018</xref>; <xref ref-type="bibr" rid="bib5">Buccino et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Moreira et al., 2019</xref>), fibre photometry (<xref ref-type="bibr" rid="bib28">Soares et al., 2016</xref>; <xref ref-type="bibr" rid="bib11">Hrvatin et al., 2020</xref>), electrophysiology (including specific packages for Open Ephys <xref ref-type="bibr" rid="bib27">Siegle et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Neto et al., 2016</xref> and high-density silicon probes <xref ref-type="bibr" rid="bib12">Jun et al., 2017</xref>; <xref ref-type="bibr" rid="bib9">Dimitriadis, 2018</xref>), and calcium imaging (e.g. UCLA miniscope <xref ref-type="bibr" rid="bib1">Aharoni et al., 2019</xref>; <xref ref-type="bibr" rid="bib6">Cai et al., 2016</xref>). Bonsai requires researchers to get accustomed to its graphical interface and event-based framework. However, it subsequently reduces the time required to learn real-time programming, and the time to build new interfaces with external devices (see Appendix 1). Moreover, since Bonsai workflows can be called via the command line, BonVision can also be integrated into pre-existing, specialised frameworks in established laboratories.</p><p>In summary, BonVision can generate complex 3D environments and retinotopically defined 2D visual stimuli within the same framework. Existing platforms used for vision research, including PsychToolbox (<xref ref-type="bibr" rid="bib4">Brainard, 1997</xref>), PsychoPy (<xref ref-type="bibr" rid="bib21">Peirce, 2007</xref>; <xref ref-type="bibr" rid="bib22">Peirce, 2008</xref>), STYTRA (<xref ref-type="bibr" rid="bib29">Štih et al., 2019</xref>), or RigBox (<xref ref-type="bibr" rid="bib3">Bhagat et al., 2020</xref>), focus on well-defined 2D stimuli. Similarly, gaming-driven software, including FreemoVR (<xref ref-type="bibr" rid="bib30">Stowers et al., 2017</xref>), ratCAVE (<xref ref-type="bibr" rid="bib8">Del Grosso and Sirota, 2019</xref>), and ViRMEn (<xref ref-type="bibr" rid="bib2">Aronov and Tank, 2014</xref>), are oriented towards generating virtual reality environments. BonVision combines the advantages of both these approaches in a single framework (Appendix 1), while bringing the unique capacity to automatically calibrate the display environment, and use deep neural networks to provide real-time control of virtual environments. Experiments in BonVision can be rapidly prototyped and easily replicated across different display configurations. Being free, open-source, and portable, BonVision is a state-of-the-art tool for visual display that is accessible to the wider community.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Benchmarking</title><p>We performed benchmarking to measure latencies and skipped (‘dropped’) frames. For benchmarks at 60 Hz refresh rate, we used a standard laptop with the following configuration: Dell Latitude 7480, Intel Core i7-6600U Processor Base with Integrated HD Graphics 520 (Dual Core, 2.6 GHz), 16 GB RAM. For higher refresh rates we used a gaming laptop ASUS ROG Zephyrus GX501GI, with an Intel Core i7-8750H (six cores, 2.20 GHz), 16 GB RAM, equipped with a NVIDIA GeForce GTX 1080. The gaming laptop's built-in display refreshes at 144 Hz, and for measuring latencies at 90 Hz we connected it to a Vive Pro SteamVR head-mounted display (90 Hz refresh rate). All tests were run on Windows 10 Pro 64-bit.</p><p>To measure the time from input detection to display update, as well as dropped frames detection, we used open-source HARP devices from Champalimaud Research Scientific Hardware Platform, using the Bonsai.HARP package. Specifically we used the HARP Behavior device (a lost latency DAQ; <ext-link ext-link-type="uri" xlink:href="https://www.cf-hw.org/harp/behavior">https://www.cf-hw.org/harp/behavior</ext-link>), to synchronise all measurements with the extensions: ‘Photodiode v2.1’ to measure the change of the stimulus on the screen, and ‘Mice poke simple v1.2’ as the nose poke device to externally trigger changes. To filter out the infrared noise generated from an internal LED sensor inside the Vive Pro HMD, we positioned an infrared cut-off filter between the internal headset optics and the photodiode. Typically, the minimal latency for any update is two frames: one which is needed for the VSync, and one is the delay introduced by the OS. Display hardware can add further delays if they include additional buffering. Benchmarks for video playback were carried out using a trailer from the Durian Open Movie Project ( copyright Blender Foundation | <ext-link ext-link-type="uri" xlink:href="http://durian.blender.org/">durian.blender.org</ext-link>).</p><p>All benchmark programmes and data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/bonvision/benchmarks">https://github.com/bonvision/benchmarks</ext-link>.</p></sec><sec id="s4-2"><title>File formats</title><p>We tested the display of images and videos using the image and video benchmark workflows. We confirmed the ability to use the following image formats: PNG, JPG, BMP, TIFF, and GIF. Movie display relies on the FFmpeg library (<ext-link ext-link-type="uri" xlink:href="https://ffmpeg.org/">https://ffmpeg.org/</ext-link>), an industry standard, and we confirmed ability to use the following containers: AVI, MP4, OGG, OGV, and WMV; in conjunction with standard codecs: H264, MPEG4, MPEG2, DIVX. Importing 3D models and complex scenes relies on the Open Asset Importer Library (Assimp | <ext-link ext-link-type="uri" xlink:href="http://assimp.org/">http://assimp.org/</ext-link>). We confirmed the ability to import and render 3D models and scenes from the following formats: OBJ, Blender.</p></sec><sec id="s4-3"><title>Animal experiments</title><p>All experiments were performed in accordance with the Animals (Scientific Procedures) Act 1986 (United Kingdom) and Home Office (United Kingdom) approved project and personal licenses. The experiments were approved by the University College London Animal Welfare Ethical Review Board under Project License 70/8637. The mice (C57BL6 wild-type) were group-housed with a maximum of five to a cage, under a 12 hr light/dark cycle. All behavioural and electrophysiological recordings were carried out during the dark phase of the cycle.</p></sec><sec id="s4-4"><title>Innate defensive behaviour</title><p>Mice (five male, C57BL6, 8 weeks old) were placed in a 40 cm square arena. A dark refuge placed outside the arena could be accessed through a 10 cm door in one wall. A DLP projector (Optoma GT760) illuminated a screen 35 cm above the arena with a grey background (80 candela/m<sup>2</sup>). When the mouse was near the centre of the arena, a 2.5 cm black dot appeared on one side of the projection screen and translated smoothly to the opposite side over 3.3 s. Ten trials were conducted over 5 days and the animal was allowed to explore the environment for 5–10 min before the onset of each trial.</p><p>Mouse movements were recorded with a near infrared camera (Blackfly S, BFS-U3-13Y3M-C, sampling rate: 60 Hz) positioned over the arena. An infrared LED was used to align video and stimulus. Freezing was defined as a drop in the animal speed below 2 cm/s that lasted more than 0.1 s; flight responses as an increase in the animal running speed above 40 cm/s (<xref ref-type="bibr" rid="bib7">De Franceschi et al., 2016</xref>). Responses were only considered if they occurred within 3.5 s from stimulus onset.</p></sec><sec id="s4-5"><title>Surgery</title><p>Mice were implanted with a custom-built stainless-steel metal plate on the skull under isoflurane anaesthesia. A ~1 mm craniotomy was performed either over the primary visual cortex (2 mm lateral and 0.5 mm anterior from lambda) or superior colliculus (0.5 mm lateral and 0.2 mm anterior from lambda). Mice were allowed to recover for 4–24 hr before the first recording session.</p><p>We used a virtual reality apparatus similar to those used in previous studies (<xref ref-type="bibr" rid="bib26">Schmidt-Hieber and Häusser, 2013</xref>; <xref ref-type="bibr" rid="bib18">Muzzu et al., 2018</xref>). Briefly, mice were head-fixed above a polystyrene wheel with a radius of 10 cm. Mice were positioned in the geometric centre of a truncated spherical screen onto which we projected the visual stimulus. The visual stimulus was centred at +60° azimuth and +30° elevation and had a span of 120° azimuth and 120° elevation.</p></sec><sec id="s4-6"><title>Virtual reality behaviour</title><p>Five male, 8-week-old, C57BL6 mice were used for this experiment. One week after the surgery, mice were placed on a treadmill and habituated to the virtual reality (VR) environment by progressively increasing the number of time spent head fixed: from ~15 min to 2 hr. Mice spontaneously ran on the treadmill, moving through the VR in absence of reward. The VR environment was a 100 cm long platform with a patterned texture that animals ran over for multiple trials. Each trial started with an animal at the start of the platform and ended when it reached the end, or if 60 s had elapsed. At the end of a trial, there was a 2 s grey interval before the start of the next trial.</p></sec><sec id="s4-7"><title>Neural recordings</title><p>To record neural activity, we used multi-electrode array probes with two shanks and 32 channels (ASSY-37 E-1, Cambridge Neurotech Ltd., Cambridge, UK). Electrophysiology data was acquired with an Open Ephys acquisition board connected to a different computer from that used to generate the visual stimulus.</p><p>The electrophysiological data from each session was processed using Kilosort 1 or Kilosort 2 (<xref ref-type="bibr" rid="bib20">Pachitariu et al., 2016</xref>). We synchronised spike times with behavioural data by aligning the signal of a photodiode that detected the visual stimuli transitions (PDA25K2, Thorlabs, Inc, USA). We sampled the firing rate at 60 Hz, and then smoothed it with a 300 ms Gaussian filter. We calculated receptive fields as the average firing rate or local field potential elicited by the appearance of a stimulus in each location (custom routines in MATLAB).</p></sec><sec id="s4-8"><title>Augmented reality for mice</title><p>The mouse behaviour videos were acquired by Bruno Cruz from the lab of Joe Paton at the Champalimaud Centre for the Unknown, using methods similar to <xref ref-type="bibr" rid="bib28">Soares et al., 2016</xref>. A <italic>ResNet-50</italic> network was trained using DeepLabCut (<xref ref-type="bibr" rid="bib16">Mathis et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Kane et al., 2020</xref>). We simulated a visual environment in which a virtual scene was presented beyond the arena, and updated the scenes on three walls of the arena. This simulated how the view changed as the animal moved through the environment. The position of the animal was updated from the video file at a rate of 40 frames/s on a gaming laptop: ASUS ROG Zephyrus GX501GI, with an Intel Core i7-8750H (six cores, 2.20 GHz), 16 GB RAM, equipped with a NVIDIA GeForce GTX 1080, using a 512 × 512 video. The performance can be improved using a lower pixel resolution for video capture, and we were able to achieve up to 80 frames/s without a noticeable decrease in tracking accuracy using this strategy. Further enhancements can be achieved using a MobileNetV2 network (<xref ref-type="bibr" rid="bib13">Kane et al., 2020</xref>). The position inference from the deep neural network and the BonVision visual stimulus rendering were run on the same machine.</p></sec><sec id="s4-9"><title>Human psychophysics</title><p>All procedures were approved by the Experimental Psychology Ethics Committee at University College London (Ethics Application EP/2019/002). We obtained informed consent and consent to publish from all participants. Four male participants were tested for this experiment. The experiments were run on a gaming laptop (described above) connected to a Vive Pro SteamVR head-mounted display (90 Hz refresh rate). BonVision is compatible with different headsets (e.g. Oculus Rift, HTC Vive). BonVision receives the projection matrix (perspective projection of world display) and the view matrix (position of eye in the world) for each eye from the head set. BonVision uses these matrices to generate two textures, one for the left eye and one for the right eye. Standard onboard computations on the headset provide additional non-linear transformations that account for the relationship between the eye and the display (such as lens distortion effects).</p></sec><sec id="s4-10"><title>Code availability</title><p>BonVision is an open-source software package available to use under the MIT license. It can be downloaded through the Bonsai (<ext-link ext-link-type="uri" xlink:href="http://bonsai-rx.org/">bonsai-rx.org</ext-link>) package manager, and the source code is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/bonvision/BonVision">github.com/bonvision/BonVision</ext-link>. All benchmark programmes and data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/bonvision/benchmarks">https://github.com/bonvision/benchmarks</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:740c31ae5db0ab594034257c92049782e76c31ba;origin=https://github.com/bonvision/benchmarks;visit=swh:1:snp:35f43afee358f16550c9c97b4c4a261a01a11f6d;anchor=swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639">swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639</ext-link>, <xref ref-type="bibr" rid="bib15">Lopes, 2021</xref>). Installation instructions, demos, and learning tools are available at: <ext-link ext-link-type="uri" xlink:href="https://bonvision.github.io/">bonvision.github.io/</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We are profoundly thankful to Bruno Cruz and Joe Paton for sharing their videos of mouse behaviour. This work was supported by a Wellcome Enrichment award: Open Research (200501/Z/16/A), Sir Henry Dale Fellowship from the Wellcome Trust and Royal Society (200501), Human Science Frontiers Program grant (RGY0076/2018) to ABS, an International Collaboration Award (with Adam Kohn) from the Stavros Niarchos Foundation/Research to Prevent Blindness to SGS, Medical Research Council grant (R023808), Biotechnology and Biological Sciences Research Council grant (R004765) to SGS and ABS.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>Gonçalo Lopes is affiliated with NeuroGEARS Ltd. The author has no financial interests to declare.</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Validation, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Validation, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Validation</p></fn><fn fn-type="con" id="con5"><p>Validation, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con6"><p>Validation, Investigation, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con7"><p>Validation, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con8"><p>Validation, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con9"><p>Validation, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con10"><p>Validation, Investigation, Writing - review and editing</p></fn><fn fn-type="con" id="con11"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con12"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Investigation, Visualization, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: All procedures were approved by the Experimental Psychology Ethics Committee at University College London (Ethics Application EP/2019/002). We obtained informed consent, and consent to publish from all participants.</p></fn><fn fn-type="other"><p>Animal experimentation: All experiments were performed in accordance with the Animals (Scientific Procedures) Act 1986 (United Kingdom) and Home Office (United Kingdom) approved project and personal licenses. The experiments were approved by the University College London Animal Welfare Ethical Review Board under Project License 70/8637.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-65541-transrepform-v3.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>BonVision is an open-source software package available to use under the MIT license. It can be downloaded through the Bonsai (<ext-link ext-link-type="uri" xlink:href="https://bonsai-rx.org">https://bonsai-rx.org</ext-link>) package manager, and the source code is available at: <ext-link ext-link-type="uri" xlink:href="https://github.com/bonvision/BonVision">https://github.com/bonvision/BonVision</ext-link>. All benchmark programs and data are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/bonvision/benchmarks">https://github.com/bonvision/benchmarks</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639">https://archive.softwareheritage.org/swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639</ext-link>). Installation instructions, demos and learning tools are available at: <ext-link ext-link-type="uri" xlink:href="https://bonvision.github.io/">https://bonvision.github.io/</ext-link>.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aharoni</surname> <given-names>D</given-names></name><name><surname>Khakh</surname> <given-names>BS</given-names></name><name><surname>Silva</surname> <given-names>AJ</given-names></name><name><surname>Golshani</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>All the light that we can see: a new era in miniaturized microscopy</article-title><source>Nature Methods</source><volume>16</volume><fpage>11</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0266-x</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aronov</surname> <given-names>D</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Engagement of neural circuits underlying 2D spatial navigation in a rodent virtual reality system</article-title><source>Neuron</source><volume>84</volume><fpage>442</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.08.042</pub-id><pub-id pub-id-type="pmid">25374363</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhagat</surname> <given-names>J</given-names></name><name><surname>Wells</surname> <given-names>MJ</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Burgess</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Rigbox: an Open-Source toolbox for probing neurons and behavior</article-title><source>Eneuro</source><volume>7</volume><elocation-id>ENEURO.0406-19.2020</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0406-19.2020</pub-id><pub-id pub-id-type="pmid">32493756</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brainard</surname> <given-names>DH</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The psychophysics toolbox</article-title><source>Spatial Vision</source><volume>10</volume><fpage>433</fpage><lpage>436</lpage><pub-id pub-id-type="doi">10.1163/156856897X00357</pub-id><pub-id pub-id-type="pmid">9176952</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buccino</surname> <given-names>AP</given-names></name><name><surname>Lepperød</surname> <given-names>ME</given-names></name><name><surname>Dragly</surname> <given-names>SA</given-names></name><name><surname>Häfliger</surname> <given-names>P</given-names></name><name><surname>Fyhn</surname> <given-names>M</given-names></name><name><surname>Hafting</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Open source modules for tracking animal behavior and closed-loop stimulation based on open ephys and bonsai</article-title><source>Journal of Neural Engineering</source><volume>15</volume><elocation-id>055002</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aacf45</pub-id><pub-id pub-id-type="pmid">29946057</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cai</surname> <given-names>DJ</given-names></name><name><surname>Aharoni</surname> <given-names>D</given-names></name><name><surname>Shuman</surname> <given-names>T</given-names></name><name><surname>Shobe</surname> <given-names>J</given-names></name><name><surname>Biane</surname> <given-names>J</given-names></name><name><surname>Song</surname> <given-names>W</given-names></name><name><surname>Wei</surname> <given-names>B</given-names></name><name><surname>Veshkini</surname> <given-names>M</given-names></name><name><surname>La-Vu</surname> <given-names>M</given-names></name><name><surname>Lou</surname> <given-names>J</given-names></name><name><surname>Flores</surname> <given-names>SE</given-names></name><name><surname>Kim</surname> <given-names>I</given-names></name><name><surname>Sano</surname> <given-names>Y</given-names></name><name><surname>Zhou</surname> <given-names>M</given-names></name><name><surname>Baumgaertel</surname> <given-names>K</given-names></name><name><surname>Lavi</surname> <given-names>A</given-names></name><name><surname>Kamata</surname> <given-names>M</given-names></name><name><surname>Tuszynski</surname> <given-names>M</given-names></name><name><surname>Mayford</surname> <given-names>M</given-names></name><name><surname>Golshani</surname> <given-names>P</given-names></name><name><surname>Silva</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A shared neural ensemble links distinct contextual memories encoded close in time</article-title><source>Nature</source><volume>534</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1038/nature17955</pub-id><pub-id pub-id-type="pmid">27251287</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Franceschi</surname> <given-names>G</given-names></name><name><surname>Vivattanasarn</surname> <given-names>T</given-names></name><name><surname>Saleem</surname> <given-names>AB</given-names></name><name><surname>Solomon</surname> <given-names>SG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Vision guides selection of freeze or flight defense strategies in mice</article-title><source>Current Biology</source><volume>26</volume><fpage>2150</fpage><lpage>2154</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.06.006</pub-id><pub-id pub-id-type="pmid">27498569</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Del Grosso</surname> <given-names>NA</given-names></name><name><surname>Sirota</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ratcave: a 3D graphics Python package for cognitive psychology experiments</article-title><source>Behavior Research Methods</source><volume>51</volume><fpage>2085</fpage><lpage>2093</lpage><pub-id pub-id-type="doi">10.3758/s13428-019-01245-x</pub-id><pub-id pub-id-type="pmid">31062192</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dimitriadis</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Why not record from every channel with a CMOS scanning probe?</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/275818</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garrido-Jurado</surname> <given-names>S</given-names></name><name><surname>Muñoz-Salinas</surname> <given-names>R</given-names></name><name><surname>Madrid-Cuevas</surname> <given-names>FJ</given-names></name><name><surname>Marín-Jiménez</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Automatic generation and detection of highly reliable fiducial markers under occlusion</article-title><source>Pattern Recognition</source><volume>47</volume><fpage>2280</fpage><lpage>2292</lpage><pub-id pub-id-type="doi">10.1016/j.patcog.2014.01.005</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hrvatin</surname> <given-names>S</given-names></name><name><surname>Sun</surname> <given-names>S</given-names></name><name><surname>Wilcox</surname> <given-names>OF</given-names></name><name><surname>Yao</surname> <given-names>H</given-names></name><name><surname>Lavin-Peter</surname> <given-names>AJ</given-names></name><name><surname>Cicconet</surname> <given-names>M</given-names></name><name><surname>Assad</surname> <given-names>EG</given-names></name><name><surname>Palmer</surname> <given-names>ME</given-names></name><name><surname>Aronson</surname> <given-names>S</given-names></name><name><surname>Banks</surname> <given-names>AS</given-names></name><name><surname>Griffith</surname> <given-names>EC</given-names></name><name><surname>Greenberg</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neurons that regulate mouse torpor</article-title><source>Nature</source><volume>583</volume><fpage>115</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2387-5</pub-id><pub-id pub-id-type="pmid">32528180</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname> <given-names>JJ</given-names></name><name><surname>Steinmetz</surname> <given-names>NA</given-names></name><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>Denman</surname> <given-names>DJ</given-names></name><name><surname>Bauza</surname> <given-names>M</given-names></name><name><surname>Barbarits</surname> <given-names>B</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Anastassiou</surname> <given-names>CA</given-names></name><name><surname>Andrei</surname> <given-names>A</given-names></name><name><surname>Aydın</surname> <given-names>Ç</given-names></name><name><surname>Barbic</surname> <given-names>M</given-names></name><name><surname>Blanche</surname> <given-names>TJ</given-names></name><name><surname>Bonin</surname> <given-names>V</given-names></name><name><surname>Couto</surname> <given-names>J</given-names></name><name><surname>Dutta</surname> <given-names>B</given-names></name><name><surname>Gratiy</surname> <given-names>SL</given-names></name><name><surname>Gutnisky</surname> <given-names>DA</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name><name><surname>Karsh</surname> <given-names>B</given-names></name><name><surname>Ledochowitsch</surname> <given-names>P</given-names></name><name><surname>Lopez</surname> <given-names>CM</given-names></name><name><surname>Mitelut</surname> <given-names>C</given-names></name><name><surname>Musa</surname> <given-names>S</given-names></name><name><surname>Okun</surname> <given-names>M</given-names></name><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Putzeys</surname> <given-names>J</given-names></name><name><surname>Rich</surname> <given-names>PD</given-names></name><name><surname>Rossant</surname> <given-names>C</given-names></name><name><surname>Sun</surname> <given-names>WL</given-names></name><name><surname>Svoboda</surname> <given-names>K</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>KD</given-names></name><name><surname>Koch</surname> <given-names>C</given-names></name><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Harris</surname> <given-names>TD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Fully integrated silicon probes for high-density recording of neural activity</article-title><source>Nature</source><volume>551</volume><fpage>232</fpage><lpage>236</lpage><pub-id pub-id-type="doi">10.1038/nature24636</pub-id><pub-id pub-id-type="pmid">29120427</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kane</surname> <given-names>GA</given-names></name><name><surname>Lopes</surname> <given-names>G</given-names></name><name><surname>Saunders</surname> <given-names>JL</given-names></name><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Real-time, low-latency closed-loop feedback using markerless posture tracking</article-title><source>eLife</source><volume>9</volume><elocation-id>e61909</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.61909</pub-id><pub-id pub-id-type="pmid">33289631</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname> <given-names>G</given-names></name><name><surname>Bonacchi</surname> <given-names>N</given-names></name><name><surname>Frazão</surname> <given-names>J</given-names></name><name><surname>Neto</surname> <given-names>JP</given-names></name><name><surname>Atallah</surname> <given-names>BV</given-names></name><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Moreira</surname> <given-names>L</given-names></name><name><surname>Matias</surname> <given-names>S</given-names></name><name><surname>Itskov</surname> <given-names>PM</given-names></name><name><surname>Correia</surname> <given-names>PA</given-names></name><name><surname>Medina</surname> <given-names>RE</given-names></name><name><surname>Calcaterra</surname> <given-names>L</given-names></name><name><surname>Dreosti</surname> <given-names>E</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Lopes</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>BonVision Benchmarks</data-title><source>Software Heritage</source><version designator="swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639">swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639">https://archive.softwareheritage.org/swh:1:rev:7205c04aa8fcba1075e9c9991ac117bd25e92639</ext-link></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mathis</surname> <given-names>A</given-names></name><name><surname>Mamidanna</surname> <given-names>P</given-names></name><name><surname>Cury</surname> <given-names>KM</given-names></name><name><surname>Abe</surname> <given-names>T</given-names></name><name><surname>Murthy</surname> <given-names>VN</given-names></name><name><surname>Mathis</surname> <given-names>MW</given-names></name><name><surname>Bethge</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>DeepLabCut: markerless pose estimation of user-defined body parts with deep learning</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>1281</fpage><lpage>1289</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0209-y</pub-id><pub-id pub-id-type="pmid">30127430</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moreira</surname> <given-names>JM</given-names></name><name><surname>Itskov</surname> <given-names>PM</given-names></name><name><surname>Goldschmidt</surname> <given-names>D</given-names></name><name><surname>Baltazar</surname> <given-names>C</given-names></name><name><surname>Steck</surname> <given-names>K</given-names></name><name><surname>Tastekin</surname> <given-names>I</given-names></name><name><surname>Walker</surname> <given-names>SJ</given-names></name><name><surname>Ribeiro</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>optoPAD, a closed-loop optogenetics system to study the circuit basis of feeding behaviors</article-title><source>eLife</source><volume>8</volume><elocation-id>e43924</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43924</pub-id><pub-id pub-id-type="pmid">31226244</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Muzzu</surname> <given-names>T</given-names></name><name><surname>Mitolo</surname> <given-names>S</given-names></name><name><surname>Gava</surname> <given-names>GP</given-names></name><name><surname>Schultz</surname> <given-names>SR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Encoding of locomotion kinematics in the mouse cerebellum</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0203900</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0203900</pub-id><pub-id pub-id-type="pmid">30212563</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neto</surname> <given-names>JP</given-names></name><name><surname>Lopes</surname> <given-names>G</given-names></name><name><surname>Frazão</surname> <given-names>J</given-names></name><name><surname>Nogueira</surname> <given-names>J</given-names></name><name><surname>Lacerda</surname> <given-names>P</given-names></name><name><surname>Baião</surname> <given-names>P</given-names></name><name><surname>Aarts</surname> <given-names>A</given-names></name><name><surname>Andrei</surname> <given-names>A</given-names></name><name><surname>Musa</surname> <given-names>S</given-names></name><name><surname>Fortunato</surname> <given-names>E</given-names></name><name><surname>Barquinha</surname> <given-names>P</given-names></name><name><surname>Kampff</surname> <given-names>AR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Validating silicon polytrodes with paired juxtacellular recordings: method and dataset</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>892</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.1152/jn.00103.2016</pub-id><pub-id pub-id-type="pmid">27306671</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Pachitariu</surname> <given-names>M</given-names></name><name><surname>Steinmetz</surname> <given-names>M</given-names></name><name><surname>Kadir</surname> <given-names>M</given-names></name><name><surname>Carandini</surname> <given-names>M</given-names></name><name><surname>Harris</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Kilosort: realtime spike-sorting for extracellular electrophysiology with hundreds of channels</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/061481</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>PsychoPy--psychophysics software in python</article-title><source>Journal of Neuroscience Methods</source><volume>162</volume><fpage>8</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2006.11.017</pub-id><pub-id pub-id-type="pmid">17254636</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peirce</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Generating stimuli for neuroscience using PsychoPy</article-title><source>Frontiers in Neuroinformatics</source><volume>2</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.11.010.2008</pub-id><pub-id pub-id-type="pmid">19198666</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname> <given-names>TD</given-names></name><name><surname>Aldarondo</surname> <given-names>DE</given-names></name><name><surname>Willmore</surname> <given-names>L</given-names></name><name><surname>Kislin</surname> <given-names>M</given-names></name><name><surname>Wang</surname> <given-names>SS</given-names></name><name><surname>Murthy</surname> <given-names>M</given-names></name><name><surname>Shaevitz</surname> <given-names>JW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Fast animal pose estimation using deep neural networks</article-title><source>Nature Methods</source><volume>16</volume><fpage>117</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0234-5</pub-id><pub-id pub-id-type="pmid">30573820</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rolland</surname> <given-names>JP</given-names></name><name><surname>Gibson</surname> <given-names>W</given-names></name><name><surname>Ariely</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Towards quantifying depth and size perception in virtual environments</article-title><source>Presence: Teleoperators and Virtual Environments</source><volume>4</volume><fpage>24</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1162/pres.1995.4.1.24</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scarfe</surname> <given-names>P</given-names></name><name><surname>Glennerster</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Using high-fidelity virtual reality to study perception in freely moving observers</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>3</elocation-id><pub-id pub-id-type="doi">10.1167/15.9.3</pub-id><pub-id pub-id-type="pmid">26161632</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt-Hieber</surname> <given-names>C</given-names></name><name><surname>Häusser</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Cellular mechanisms of spatial navigation in the medial entorhinal cortex</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>325</fpage><lpage>331</lpage><pub-id pub-id-type="doi">10.1038/nn.3340</pub-id><pub-id pub-id-type="pmid">23396102</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Siegle</surname> <given-names>JH</given-names></name><name><surname>López</surname> <given-names>AC</given-names></name><name><surname>Patel</surname> <given-names>YA</given-names></name><name><surname>Abramov</surname> <given-names>K</given-names></name><name><surname>Ohayon</surname> <given-names>S</given-names></name><name><surname>Voigts</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Open ephys: an open-source, plugin-based platform for multichannel electrophysiology</article-title><source>Journal of Neural Engineering</source><volume>14</volume><elocation-id>045003</elocation-id><pub-id pub-id-type="doi">10.1088/1741-2552/aa5eea</pub-id><pub-id pub-id-type="pmid">28169219</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Soares</surname> <given-names>S</given-names></name><name><surname>Atallah</surname> <given-names>BV</given-names></name><name><surname>Paton</surname> <given-names>JJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Midbrain dopamine neurons control judgment of time</article-title><source>Science</source><volume>354</volume><fpage>1273</fpage><lpage>1277</lpage><pub-id pub-id-type="doi">10.1126/science.aah5234</pub-id><pub-id pub-id-type="pmid">27940870</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Štih</surname> <given-names>V</given-names></name><name><surname>Petrucco</surname> <given-names>L</given-names></name><name><surname>Kist</surname> <given-names>AM</given-names></name><name><surname>Portugues</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Stytra: an open-source, integrated system for stimulation, tracking and closed-loop behavioral experiments</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006699</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006699</pub-id><pub-id pub-id-type="pmid">30958870</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowers</surname> <given-names>JR</given-names></name><name><surname>Hofbauer</surname> <given-names>M</given-names></name><name><surname>Bastien</surname> <given-names>R</given-names></name><name><surname>Griessner</surname> <given-names>J</given-names></name><name><surname>Higgins</surname> <given-names>P</given-names></name><name><surname>Farooqui</surname> <given-names>S</given-names></name><name><surname>Fischer</surname> <given-names>RM</given-names></name><name><surname>Nowikovsky</surname> <given-names>K</given-names></name><name><surname>Haubensak</surname> <given-names>W</given-names></name><name><surname>Couzin</surname> <given-names>ID</given-names></name><name><surname>Tessmar-Raible</surname> <given-names>K</given-names></name><name><surname>Straw</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Virtual reality for freely moving animals</article-title><source>Nature Methods</source><volume>14</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id><pub-id pub-id-type="pmid">28825703</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yeh</surname> <given-names>CI</given-names></name><name><surname>Xing</surname> <given-names>D</given-names></name><name><surname>Williams</surname> <given-names>PE</given-names></name><name><surname>Shapley</surname> <given-names>RM</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Stimulus ensemble and cortical layer determine V1 spatial receptive fields</article-title><source>PNAS</source><volume>106</volume><fpage>14652</fpage><lpage>14657</lpage><pub-id pub-id-type="doi">10.1073/pnas.0907406106</pub-id><pub-id pub-id-type="pmid">19706551</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zacarias</surname> <given-names>R</given-names></name><name><surname>Namiki</surname> <given-names>S</given-names></name><name><surname>Card</surname> <given-names>GM</given-names></name><name><surname>Vasconcelos</surname> <given-names>ML</given-names></name><name><surname>Moita</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Speed dependent descending control of freezing behavior in <italic>Drosophila melanogaster</italic></article-title><source>Nature Communications</source><volume>9</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1038/s41467-018-05875-1</pub-id><pub-id pub-id-type="pmid">30209268</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Basic workflow structure</title><p>Each BonVision workflow starts by loading the basic Shaders library (this is Bonsai's implementation of OpenGL) and then creating a window in which stimuli are to be displayed. Bonsai is an event-based framework, so the visual stimulus generation and control are driven by events from the <italic>RenderFrame</italic> or <italic>UpdateFrame</italic> nodes, which are in turn activated when a screen refresh occurs. An event broadcast from the <italic>RenderFrame</italic> or <italic>UpdateFrame</italic> node then activates the cascade of nodes that load, generate, or update the different visual stimuli.</p></sec><sec id="s9" sec-type="appendix"><title>Closed-loop control</title><p>Parameters of stimuli can also be updated, asynchronously and in parallel, by other events. Parameters of any Bonsai node can be controlled by addressing the relevant property within that node – all parameters within a node can be made visible to the external caller of that node. This is particularly useful for generating closed loop stimuli where the value of these parameters can be linked to external IO devices (e.g. position sensors) that are easily accessible using established Bonsai drivers and packages. A major advantage of the Bonsai framework is that the visual stimulus generation does not need to pause to poll those I/O devices, and the values from those devices can be retrieved any time up to the rendering of the frame, creating opportunities for low-lag updating of the visual stimulus.</p></sec><sec id="s10" sec-type="appendix"><title>Considerations while using BonVision</title><sec id="s10-1"><title>Client control</title><p>Some experimental designs may rely on complex experimental control protocols that are already established in other software, or are challenging to implement in a reactive framework. For such applications, BonVision’s rendering platform can be used as a client to create and control calibrated visual stimuli. This can be implemented using Bonsai’s inbuilt IP communication protocols to interact with the independent controller software (e.g. Python or MATLAB). BonVision workflows can also be executed from the command-line using standard syntax, without opening the graphical interface of Bonsai.</p></sec><sec id="s10-2"><title>Mercator projection</title><p>A key motivation in developing BonVision was the ability to present 2D and 3D stimuli in the same framework. To enable this, we chose to project 2D stimuli onto a 3D sphere, using the Mercator projection. The Mercator projection, however, contracts longitude coordinates around the two poles, and the consequence is that 2D stimuli presented close to the poles are deformed without compensation. Experiments that require 2D-defined stimuli to be presented near the default poles therefore need particular care. There are a few options to overcome this limitation. One option is to rotate the sphere mapping so that the poles are shifted away from the desired stimulus location. A second option is to present the texture on a 3D object facing the observer. For example, to present a grating in a circular aperture, we could have the grating texture rendered on a disk presented in 3D, and the disk is placed in the appropriate position. Finally, the user can present stimuli via the <italic>NormalisedView</italic> node, which defines stimuli in screen pixel coordinates, and use manual calibrations and precomputations to ensure the stimuli are of the correct dimensions.</p></sec><sec id="s10-3"><title>Constructing 3D environments</title><p>There are many well-established software packages with graphical interfaces that are capable of creating 3D objects and scenes, and users are likely to have their preferred method. BonVision therefore focuses on providing easy importing of a wide variety of 3D model formats. BonVision offers three options for building 3D environments:</p><list list-type="order"><list-item><p>BonVision (limited capability). Inbuilt BonVision processes allow for the rendering of textures onto simple planar surfaces. The user defines the position and orientation of each plane in 3D space, and the texture that is to be drawn onto that plane, using the <italic>DrawTexturedModel</italic> node.</p></list-item><list-item><p>Import (load) 3D models of objects (including cubes, spheres, and more complex models). Common 3D models (such as those used in <xref ref-type="fig" rid="fig1">Figure 1</xref>) are often freely available online. Custom models can be generated using standard 3D software, including Blender and CAD programmes. The user defines the position of each object, and its dynamics, within BonVision, and can independently attach the desired texture(s) to each of the different faces of those objects using the <italic>DrawTexturedModel</italic> node.</p></list-item><list-item><p>Import a full 3D scene (with multiple objects and camera views). BonVision is able to interact with both individual objects and cameras defined within a 3D scene. A particular advantage of this method is that specialised software (e.g. Blender) provide convenient methods to construct and visualise scenes in advance; BonVision provides the calibrated display environment and capacity for interaction with the objects.</p></list-item></list><p>Once the 3D scene is created, the user can then control a camera (e.g. move or rotate) in the resultant virtual world. BonVision computes the effects of the camera movement (i.e. without any additional user code) to render what the camera should see onto a display device.</p></sec><sec id="s10-4"><title>Animation lags and timing logs</title><p>While BonVision expends substantial effort to eliminate interruptions to the presentation of a visual stimulus, these can occur, and solutions may be beyond the control of the experimenter. To avoid the potential accumulation of timing errors, the UpdateFrame node uses the current time to specify the current location in an animation sequence. The actual presentation time of each frame in an animation can be logged using the standard logging protocols in BonVision. The log can also include the user predefined or real-time updated parameters that were used to generate the corresponding stimulus frame.</p></sec><sec id="s10-5"><title>Customised nodes and new stimuli</title><p>Bonsai’s modular nature and simple integration with C# and Python scripting means BonVision can be extended by users. The BonVision package is almost entirely implemented using the Bonsai visual programming language, showcasing its power as a domain-specific language. Custom BonVision nodes can be easily created in the graphical framework, or using C# or Python scripting with user-defined inputs, outputs, properties and operations can be generated by users to create novel visual stimuli, define interactions between objects and enable visual environments which are arbitrarily responsive to experimental subjects.</p></sec><sec id="s10-6"><title>Physics engine</title><p>BonVision is able to calculate interactions between objects using the package Bonsai.Physics, including collisions, bouncing off surfaces, or deformations.</p></sec><sec id="s10-7"><title>Spatial calibration</title><p>BonVision provides automatic calibration protocols to define the position of display(s) relative to the observer. A single positional marker is sufficient for each flat display (illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>; a standard operating procedure is described on the website). An additional marker is placed in the position of the observer to provide the reference point.</p><p>When the observer’s position relative to the display varies (e.g. in the augmented reality example in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="video" rid="fig3video1">Figure 3—video 1</xref>), the easiest solution is to calibrate the position of the displays relative to a fixed point in the arena. The observer position is then calculated in real-time, and the vector from the observer to the reference point is added to the vector from the reference to the display. The resultant vector is the calibrated position of the display relative to the observer’s current position.</p><p>In the case of head-mounted displays (HMDs), BonVision takes advantage of the fact that HMD drivers can provide the calibrated transform matrices from the observer’s eye centre, using the <italic>HMDView</italic> node.</p><p>When the presentation surface is curved (e.g. projection onto a dome) a manual calibration step is required as in other frameworks. This calibration step is often referred to as mesh-mapping and involves the calculation of a transformation matrix that specifies the relationship between a (virtual) flat display and position on the projection surface. A standard operating procedure for calculating this mesh-map is described on the BonVision website.</p></sec><sec id="s10-8"><title>Performance optimisation</title><p>We recommend displaying stimuli through a single graphics card when possible. When multiple displays are used for visual stimulation, we recommend configuring them as a single extended display (as seen by the operating system). All our tests were performed under this configuration.</p><table-wrap id="app1table1" position="float"><label>Appendix 1—table 1.</label><caption><title>Features of visual display software.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Features</th><th align="center">BonVision</th><th align="center">PsychToolbox</th><th align="center">PsychoPy</th><th align="center">ViRMEn</th><th align="center">ratCAVE</th><th align="center">FreemoVR</th><th align="center">Unity</th></tr></thead><tbody><tr><td>Free and Open-source (FOSS)</td><td align="center">√√</td><td align="center">√#</td><td align="center">√√</td><td align="center">√#</td><td align="center">√</td><td align="center">√√</td><td align="center">√</td></tr><tr><td>Rendering of 3D environments</td><td align="center">√√</td><td align="center">√</td><td align="center">√</td><td align="center">√√</td><td align="center">√√</td><td align="center">√√</td><td align="center">√√</td></tr><tr><td>Dynamic rendering based on observer viewpoint</td><td align="center">√√</td><td align="center"/><td align="center"/><td align="center">√</td><td align="center">√√</td><td align="center">√√</td><td align="center">√</td></tr><tr><td>GUI for designing 3D scenes</td><td align="center"/><td align="center"/><td align="center"/><td align="center">√√</td><td align="center"/><td align="center"/><td align="center">√√</td></tr><tr><td>Import 3<sup>rd</sup> party 3D scenes</td><td align="center">√√</td><td align="center">√</td><td align="center">√</td><td align="center"/><td align="center"/><td align="center"/><td align="center">√√</td></tr><tr><td>Real-time interactive 3D scenes</td><td align="center">√√</td><td align="center">√</td><td align="center"/><td align="center">√√</td><td align="center">√√</td><td align="center">√√</td><td align="center">√√</td></tr><tr><td>Web-based deployment</td><td align="center"/><td align="center"/><td align="center">√√</td><td align="center"/><td align="center"/><td align="center"/><td align="center">√√</td></tr><tr><td>Interfacing with cameras, sensors and effectors</td><td align="center">√√</td><td align="center">√√</td><td align="center">~</td><td align="center">√√</td><td align="center"/><td align="center">~</td><td align="center">~</td></tr><tr><td>Real-time hardware control</td><td align="center">√√</td><td align="center">~</td><td align="center">~</td><td align="center">√</td><td align="center">√√</td><td align="center">√</td><td align="center">√</td></tr><tr><td>Traditional visual stimuli</td><td align="center">√√</td><td align="center">√√</td><td align="center">√√</td><td align="center"/><td align="center"/><td align="center"/><td align="center"/></tr><tr><td>Auto-calibration of display position and pose</td><td align="center">√√</td><td align="center"/><td align="center"/><td align="center"/><td align="center"/><td align="center"/><td align="center"/></tr><tr><td>Integration with deep learning pose estimation</td><td align="center">√√</td><td align="center"/><td align="center"/><td align="center"/><td align="center"/><td align="center"/><td align="center"/></tr></tbody></table><table-wrap-foot><fn><p>√√ easy and well-supported.</p><p>√ possible, not well-supported.</p></fn><fn><p>~ difficult to implement.</p><p># based on MATLAB (requires a license).</p></fn></table-wrap-foot></table-wrap></sec></sec><sec id="s11" sec-type="appendix"><title>Learning to use BonVision</title><p>We provide the following learning materials (which will continue to be updated):</p><list list-type="simple"><list-item><p>Tutorials and Documentation: <ext-link ext-link-type="uri" xlink:href="https://bonvision.github.io">https://bonvision.github.io</ext-link></p></list-item><list-item><p>Video tutorials: <ext-link ext-link-type="uri" xlink:href="https://www.youtube.com/channel/UCEg-3mfbvjIwbzDVvqYudAA">https://www.youtube.com/channel/UCEg-3mfbvjIwbzDVvqYudAA</ext-link></p></list-item><list-item><p>Demos and Examples: <ext-link ext-link-type="uri" xlink:href="https://github.com/bonvision/examples">https://github.com/bonvision/examples</ext-link></p></list-item><list-item><p>Community forum: <ext-link ext-link-type="uri" xlink:href="https://groups.google.com/forum/#!forum/bonsai-users">https://groups.google.com/forum/#!forum/bonsai-users</ext-link></p></list-item></list></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65541.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Baker</surname><given-names>Chris I</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute of Mental Health, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Newman</surname><given-names>Jonathan P</given-names></name><role>Reviewer</role><aff><institution>Massachusetts Institute of Technology</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Maia Chagas</surname><given-names>André</given-names> </name><role>Reviewer</role><aff><institution>University of Tübingen</institution><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Koay</surname><given-names>Sue Ann</given-names> </name><role>Reviewer</role><aff><institution>Princeton University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.03.09.983775">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.03.09.983775v2">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Increasingly, neuroscience experiments require immersive virtual environments that approximate natural sensory motor loops while permitting high-bandwidth measurements of brain activity. BonVision is an open-source graphics programming library that allows experimenters to quickly implement immersive 3D visual environments across display hardware and geometry with automated calibration and integration with hundreds of different neural recording technologies, behavioral apparatuses, etc. BonVision standardizes sharing complex, closed-loop visual tasks between labs with vastly different equipment and provides a concrete and easy way to do so.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Creating and controlling visual environments using BonVision&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Chris Baker as the Senior and Reviewing Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Jonathan P Newman (Reviewer #1); André Maia Chagas (Reviewer #2); Sue Ann Koay (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this letter to help you prepare a revised submission.</p><p>Essential revisions:</p><p>In general, the reviewers were very positive about the manuscript and appreciated the time and effort taken both to develop BonVision and write this manuscript. The major concerns reflect a desire from the reviewers to see more detail on specific points as well as clarification over some of the statements made.</p><p>In your revision please address the specific recommendations below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>General comment: There are two measures of performance that are not explored in the manuscript but may aid in describing BonVision's advantages over alternative software. The first is the improved performance and ease of use compared to alternatives in cases where the input used to drive visual stimuli consists of mixtures of asynchronous data sources (e.g. ephys and behavioral measurements together). This is something I imagine BonVision could do with less effort and greater computational efficiency than alternative software. The animal experiments provided are good benchmarks because they are common designs, but do not demonstrate BonVision's immense potential for easily creating visual tasks with complex IO and stimulus contingencies. The second is a measure of human effort required to implement an experiment using BonVision compared to imperative, text-based alternatives. I think both of these issues could be tackled in the discussion by expanding a bit on Lines 144-146: why are BonVision and Bonsai so good at data stream composition compared to alternatives, and why is a visual programming approach so appropriate for Bonsai/BonVision's target use cases?</p><p>General comment: Following up on my desire for a more detailed explanation of the operation of the Bonsai.Shaders library, most of its operators have obvious relations to traditional OpenGL programming operations. However, an explanation of how the traditionally global state machine (context) of OpenGL was mapped onto the Bonsai.Shaders nodes and how temporal order of OpenGL context option manipulation is enforced might be helpful for those wishing to understand the underlying mechanics of BonVision and create their own tools using the Shaders library.</p><p>Line 11: The use of the word &quot;timing&quot; is ambiguous to me. Are the authors referring to closed loop reaction times and determinism, hardware IO delays, the combination of samples from asynchronous data streams, or all of the above?</p><p>Lines 13 and 22: The authors correctly state that graphics programming requires advanced training. However, the use of Bonsai, a functional language that operates entirely on Observable Sequences, also requires quite a lot of training to use effectively. I do think the authors have a point here, and I agree Bonsai is tool worth learning, but I feel the main strength of using Bonsai is its (broadly defined) performance (speed, elegance when dealing with asynchronous data, ease and formality of experiment sharing, ease of rapid prototyping, etc) rather than its learning curve. This point is exacerbated by the lack of documentation (outside of the source code) for many Bonsai features.</p><p>Line 64: Adding a parenthetical link to the Blender website seems appropriate.</p><p>Line 97: The model species should be stated here.</p><p>Figure 4(C): There is a single instance of BonVision being outperformed by PsychoPy3 in the case of 16-layer texture blending at 60 FPS. Can the authors comment on why this might be (e.g. PsychoPy3's poor performance at low layer counts is due to some memory bottleneck?) and why this matters (or does not matter) practically in the context of BonVision's target uses?</p><p>Figure 4(A-C): The cartoons of display screens have little black boxes in the lower right corners and I'm not sure what they mean.</p><p>Figure 5(A): As mentioned previously, it seems that these are post-hoc temporally aligned receptive fields (RFs). Is it worth seeing what the RFs created without post-hoc photodiode-based alignment of stimuli onset look like so that we can see the effect of display presentation jitter (or lack thereof)? This would be a nice indication of the utility of the package for real-time stimulus shaping for system ID purposes where ground truth alignment might not be possible. This is made more relevant given BonVision's apparent larger latency jitter compared to PsychToolbox (Figure 4A).</p><p>Figure 5(D): Although useful, the size discrimination task probably does not cover all potential corner cases with this type of projection. I don't think more experiments need to be performed but a more thorough theoretical comparison with other methods, e.g. sphere mapping, might be useful to motivate the choice of cube mapping for rendering 3D objects, perhaps in the discussion.</p><p>Figure 5(I): The caption refers to the speed of the animal on the ordinate axis but that figure seems to display a psychometric curve for freezing or running behaviors over time from stimulus presentation.</p><p>Line 293 and 319-322: HARP is a small enough project that I feel some explanation of the project's intentions and capabilities and the Bonsai library used to acquire it from HARP hardware might be useful.</p><p>Line 384: &quot;OpenEphys&quot; should be changed to &quot;Open Ephys&quot; in the text and in reference 13 used to cite the Acquisition Board's use.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– Figures 1, 2, 3 and Supp1 – the indication of the observer in these figures is sometimes a black, and sometimes a red dot. This is not bad, but I think you could streamline your figures if on the first one you had a legend for what represents the observer (ie observer = red dot) and have the same pattern through the figures?</p><p>– Figure 2 – If I understand correctly, in panels C and E, the markers are read by an external camera, which I am supposing in this case are the laptop camera? If this is the case, could you please change these panels so that they explicitly show where the cameras are? Maybe adding the first top left panel from supp Figure 3 to Figure 2 and indicate from where the markers are read would solve this?</p><p>– Figure 5 – Panel I: the legend states &quot;The speed of the animal across different trials, aligned to the time of stimulus appearance.&quot; but the figure Y axis states Cumulative probability. I guess the legend needs updating? Also it is not clear to me how the cumulative probabilities of freeze and flight can sum up to more than one, as it seems to be the case from the figure? I am assuming that an animal either freezes of flees in this test? Maybe I missed something?</p><p>– In the results, lines 40 to 42, the authors describe how they have managed to have a single framework for both traditional visual presentation and immersive virtual reality. Namely they project 2D coordinate frame to a 3D sphere using Mercator projection. I would like to ask the authors to explain a bit how they deal with the distortions present in this type of projection. As far as I understand, this type of projection inflates the size of objects that are further away from the sphere midline (with increased intensity the further away)? Is this compensated for in the framework somehow? Would it make sense to offer users the option to choose different projections depending on their application?</p><p>– In line 62 &quot;BonVision also has the ability to import standard format 3D design files&quot; could the authors specify which file formats are accepted?</p><p>– When benchmarking BonVision (starting on line 73), the authors focus on 60Hz stimulus presentation using monitors with different capabilities. This is great, as it addresses main points for human, non-human primates and rodent experiments. I believe however that it would be great for the paper and the community in general if the authors could do some benchmarking with higher frame rates and contextualize BonVision for the use with other animal models, such as Fly, fish, etc. Given that there are a couple of papers describing visual stimulators that take care of the different wavelengths needed to stimulate the visual system of these animals, it seems to me that BonVision would be a great tool to create stimuli and environments for these stimulators and animal models.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I have a few presentation style points where I feel the text should be more careful not to come across as unintendedly too strong, or otherwise justification need to be provided to substantiate the claims. Most importantly, line 19 &quot;the ability for rapid and efficient interfacing with external hardware (needed for experimentation) without development of complex multi-threaded routines&quot; is a bit mysterious to me because I am unsure what these external hardware are that BonVision facilitates interfacing with. For example, experimenters do prefer multi-threaded routines where the other threads are used to trigger reward delivery, sensory stimuli of other modalities, or control neural stimulation or recording devices. This is in order to avoid blocking execution of the visual display software when these other functions are called. If BonVision provides a solution for these kinds of experiment interfacing requirements, I think they are definitely important enough to mention in the text. Otherwise, the sentence of line 19 needs some work in order to make it clear as to exactly which functionalities of BonVision are being referred to.</p><p>The other claims that stood out to me are as follows. In the abstract it is said that &quot;Real-time rendering… necessary for next-generation…&quot;, but I don't know if anybody can actually claim that any one method is necessary. In line 116, &quot;suggesting habituation to the virtual environment&quot;, the authors can also acknowledge that mice might simply be habituating to the rig (e.g. even if there was no visual display), since this does not seem to be a major claim that needs to be made. The virtual cliff effect (line 118) also seems very interesting, but the authors have not fully demonstrated that mice are not alternatively responding to a change in floor texture. It is also unclear to me why a gray floor (which looks to be equiluminant with the rest of the textured floor at least by guessing from Figure 5F) should be visually identified as a cliff, as opposed to, say, black. In order to make this claim about visual cliff identification especially without binocular vision, the authors would probably have to show experiments where the mice do not slow down at other floor changes (to white maybe?), but I'm unsure as to whether the data exists for this nor whether it is worth the effort. Overall I don't see a reason why the authors should attempt to claim that &quot;BonVision is capable of eliciting naturalistic behaviors in a virtual environment&quot;, since the naturalness of rodent behaviors in virtual environments is a topic of debate in some circles, independent of the software used to generate those environments. I figure it's better to stay away unless this is a fight that one desires to fight.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.65541.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>General comment: There are two measures of performance that are not explored in the manuscript but may aid in describing BonVision's advantages over alternative software. The first is the improved performance and ease of use compared to alternatives in cases where the input used to drive visual stimuli consists of mixtures of asynchronous data sources (e.g. ephys and behavioral measurements together). This is something I imagine BonVision could do with less effort and greater computational efficiency than alternative software. The animal experiments provided are good benchmarks because they are common designs, but do not demonstrate BonVision's immense potential for easily creating visual tasks with complex IO and stimulus contingencies. The second is a measure of human effort required to implement an experiment using BonVision compared to imperative, text-based alternatives. I think both of these issues could be tackled in the discussion by expanding a bit on Lines 144-146: why are BonVision and Bonsai so good at data stream composition compared to alternatives, and why is a visual programming approach so appropriate for Bonsai/BonVision's target use cases?</p></disp-quote><p>We agree and we have now revised the Introduction and Discussion to better make these points transparent (particularly around lines 44-55 and 235-239).</p><disp-quote content-type="editor-comment"><p>General comment: Following up on my desire for a more detailed explanation of the operation of the Bonsai.Shaders library, most of its operators have obvious relations to traditional OpenGL programming operations. However, an explanation of how the traditionally global state machine (context) of OpenGL was mapped onto the Bonsai.Shaders nodes and how temporal order of OpenGL context option manipulation is enforced might be helpful for those wishing to understand the underlying mechanics of BonVision and create their own tools using the Shaders library.</p></disp-quote><p>We thank the reviewer for prompting us. Generally, we now mention that we build on the Bonsai.Shaders package in new text (lines 58-59 and in Supplementary Details).</p><p>Specifically, related to the point related to the temporal order of OpenGL, we also include the text (in lines 133-135): “BonVision accumulates a list of the commands to OpenGL as the program makes them. To optimise rendering performance, the priority of these commands is ordered according to that defined in the Shaders component of the <italic>LoadResources</italic> node (which the user can manipulate for high-performance environments). These ordered calls are then executed when the frame is rendered.”</p><disp-quote content-type="editor-comment"><p>Line 11: The use of the word &quot;timing&quot; is ambiguous to me. Are the authors referring to closed loop reaction times and determinism, hardware IO delays, the combination of samples from asynchronous data streams, or all of the above?</p></disp-quote><p>Thank you for picking this up – the organisation of the first paragraph meant that the subject of this sentence was unclear, and we have now tried to make this paragraph clearer, including splitting it into two distinct points (lines 3-17). We hope these changes now address the reviewers point.</p><disp-quote content-type="editor-comment"><p>Lines 13 and 22: The authors correctly state that graphics programming requires advanced training. However, the use of Bonsai, a functional language that operates entirely on Observable Sequences, also requires quite a lot of training to use effectively. I do think the authors have a point here, and I agree Bonsai is tool worth learning, but I feel the main strength of using Bonsai is its (broadly defined) performance (speed, elegance when dealing with asynchronous data, ease and formality of experiment sharing, ease of rapid prototyping, etc) rather than its learning curve. This point is exacerbated by the lack of documentation (outside of the source code) for many Bonsai features.</p></disp-quote><p>We agree and have revised the Introduction (lines 42-55) and Discussion (lines 235-239) to make this clearer.</p><disp-quote content-type="editor-comment"><p>Line 64: Adding a parenthetical link to the Blender website seems appropriate.</p><p>Line 97: The model species should be stated here.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>Figure 4(C): There is a single instance of BonVision being outperformed by PsychoPy3 in the case of 16-layer texture blending at 60 FPS. Can the authors comment on why this might be (e.g. PsychoPy3's poor performance at low layer counts is due to some memory bottleneck?) and why this matters (or does not matter) practically in the context of BonVision's target uses?</p></disp-quote><p>In the conditions under which the benchmarking was performed, PsychoPy was able to present more overlapping stimuli compared to BonVision and PsychToolBox, because PsychoPy presented stimuli at a lower resolution compared to the other systems. We now indicate this in the main text of the manuscript (lines 150-151).</p><disp-quote content-type="editor-comment"><p>Figure 4(A-C): The cartoons of display screens have little black boxes in the lower right corners and I'm not sure what they mean.</p></disp-quote><p>The black square represents the position of a flickering square, the luminance of which is detected by a photodiode and used to measure frame display times. We have now updated the legend of Figure 4 to make this clear.</p><disp-quote content-type="editor-comment"><p>Figure 5(A): As mentioned previously, it seems that these are post-hoc temporally aligned receptive fields (RFs). Is it worth seeing what the RFs created without post-hoc photodiode-based alignment of stimuli onset look like so that we can see the effect of display presentation jitter (or lack thereof)? This would be a nice indication of the utility of the package for real-time stimulus shaping for system ID purposes where ground truth alignment might not be possible. This is made more relevant given BonVision's apparent larger latency jitter compared to PsychToolbox (Figure 4A).</p></disp-quote><p>We thank the reviewer for this suggestion. We now include receptive field maps calculated using the BonVision timing log in Figure5—figure supplement 1. Using the BonVision timing alone was also effective in identifying receptive fields.</p><disp-quote content-type="editor-comment"><p>Figure 5(D): Although useful, the size discrimination task probably does not cover all potential corner cases with this type of projection. I don't think more experiments need to be performed but a more thorough theoretical comparison with other methods, e.g. sphere mapping, might be useful to motivate the choice of cube mapping for rendering 3D objects, perhaps in the discussion.</p></disp-quote><p>We now clarify that we use the size discrimination task as a simple test of the ability of BonVision to run VR stimuli on a head-mounted display.</p><p>Although we considered the different mapping styles, we settled on cube mapping for 3D stimuli, as this is currently the standard for 3D rendering systems, and the most computationally efficient. We included a detailed discussion on the merits and issues with Mercatore projection for 2D stimuli in the new section “Appendix 1”.</p><disp-quote content-type="editor-comment"><p>Figure 5(I): The caption refers to the speed of the animal on the ordinate axis but that figure seems to display a psychometric curve for freezing or running behaviors over time from stimulus presentation.</p></disp-quote><p>Thank you for pointing this out, we have now corrected this.</p><disp-quote content-type="editor-comment"><p>Line 293 and 319-322: HARP is a small enough project that I feel some explanation of the project's intentions and capabilities and the Bonsai library used to acquire it from HARP hardware might be useful.</p></disp-quote><p>We have now added more information on the HARP sources, and why we have employed it here, including details of the Bonsai library needed to use the HARP device (lines 648-652). However, we are not core members of the HARP project and are wary of speaking for them on its intentions and other capabilities.</p><disp-quote content-type="editor-comment"><p>Line 384: &quot;OpenEphys&quot; should be changed to &quot;Open Ephys&quot; in the text and in reference 13 used to cite the Acquisition Board's use.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>– Figures 1, 2, 3 and Supp1 – the indication of the observer in these figures is sometimes a black, and sometimes a red dot. This is not bad, but I think you could streamline your figures if on the first one you had a legend for what represents the observer (ie observer = red dot) and have the same pattern through the figures?</p></disp-quote><p>Great suggestion, thank you. We have now changed all observers to red dots and indicated this in the legend.</p><disp-quote content-type="editor-comment"><p>– Figure 2 – If I understand correctly, in panels C and E, the markers are read by an external camera, which I am supposing in this case are the laptop camera? If this is the case, could you please change these panels so that they explicitly show where the cameras are? Maybe adding the first top left panel from supp Figure 3 to Figure 2 and indicate from where the markers are read would solve this?</p></disp-quote><p>We think that the reviewer had spotted that there are multiple cameras shown in the image, and we apologise for not spotting this ourselves. The calibration is performed using only the images shown (that is the camera that is taking the image is the one used for the calibration). We now make this clearer in the legend to Figure 2.</p><disp-quote content-type="editor-comment"><p>– Figure 5 – Panel I: the legend states &quot;The speed of the animal across different trials, aligned to the time of stimulus appearance.&quot; but the figure Y axis states Cumulative probability. I guess the legend needs updating? Also it is not clear to me how the cumulative probabilities of freeze and flight can sum up to more than one, as it seems to be the case from the figure? I am assuming that an animal either freezes of flees in this test? Maybe I missed something?</p></disp-quote><p>We thank the reviewer for highlighting this error. We have updated the legend.</p><disp-quote content-type="editor-comment"><p>– In the results, lines 40 to 42, the authors describe how they have managed to have a single framework for both traditional visual presentation and immersive virtual reality. Namely they project 2D coordinate frame to a 3D sphere using Mercator projection. I would like to ask the authors to explain a bit how they deal with the distortions present in this type of projection. As far as I understand, this type of projection inflates the size of objects that are further away from the sphere midline (with increased intensity the further away)? Is this compensated for in the framework somehow? Would it make sense to offer users the option to choose different projections depending on their application?</p></disp-quote><p>This is an excellent point. We have added a specific discussion related to the Mercator projection in the new section called Appendix, where we discuss the distortions and methods to work around them.</p><disp-quote content-type="editor-comment"><p>– In line 62 &quot;BonVision also has the ability to import standard format 3D design files&quot; could the authors specify which file formats are accepted?</p></disp-quote><p>We now link from the main text to the ‘File Formats’ section in Methods.</p><disp-quote content-type="editor-comment"><p>– When benchmarking BonVision (starting on line 73), the authors focus on 60Hz stimulus presentation using monitors with different capabilities. This is great, as it addresses main points for human, non-human primates and rodent experiments. I believe however that it would be great for the paper and the community in general if the authors could do some benchmarking with higher frame rates and contextualize BonVision for the use with other animal models, such as Fly, fish, etc. Given that there are a couple of papers describing visual stimulators that take care of the different wavelengths needed to stimulate the visual system of these animals, it seems to me that BonVision would be a great tool to create stimuli and environments for these stimulators and animal models.</p></disp-quote><p>We have added a new Figure 4—figure supplement 1, in which we show the results of the non-overlapping textures benchmark for BonVision at 144 Hz refresh. Comparison with the same data obtained at 60 Hz shows little deterioration in performance. These new data supplement the extant tests in Figure 4A, where we tested the closed-loop latency at these higher frame rates.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I have a few presentation style points where I feel the text should be more careful not to come across as unintendedly too strong, or otherwise justification need to be provided to substantiate the claims. Most importantly, line 19 &quot;the ability for rapid and efficient interfacing with external hardware (needed for experimentation) without development of complex multi-threaded routines&quot; is a bit mysterious to me because I am unsure what these external hardware are that BonVision facilitates interfacing with. For example, experimenters do prefer multi-threaded routines where the other threads are used to trigger reward delivery, sensory stimuli of other modalities, or control neural stimulation or recording devices. This is in order to avoid blocking execution of the visual display software when these other functions are called. If BonVision provides a solution for these kinds of experiment interfacing requirements, I think they are definitely important enough to mention in the text. Otherwise, the sentence of line 19 needs some work in order to make it clear as to exactly which functionalities of BonVision are being referred to.</p></disp-quote><p>We agree and have now revised the Introduction (lines 42-55) to make these points clearer.</p><disp-quote content-type="editor-comment"><p>The other claims that stood out to me are as follows. In the abstract it is said that &quot;Real-time rendering… necessary for next-generation…&quot;, but I don't know if anybody can actually claim that any one method is necessary.</p></disp-quote><p>We have changed the text to say ‘important’ rather than ‘necessary’.</p><disp-quote content-type="editor-comment"><p>In line 116, &quot;suggesting habituation to the virtual environment&quot;, the authors can also acknowledge that mice might simply be habituating to the rig (e.g. even if there was no visual display), since this does not seem to be a major claim that needs to be made. The virtual cliff effect (line 118) also seems very interesting, but the authors have not fully demonstrated that mice are not alternatively responding to a change in floor texture. It is also unclear to me why a gray floor (which looks to be equiluminant with the rest of the textured floor at least by guessing from Figure 5F) should be visually identified as a cliff, as opposed to, say, black. In order to make this claim about visual cliff identification especially without binocular vision, the authors would probably have to show experiments where the mice do not slow down at other floor changes (to white maybe?), but I'm unsure as to whether the data exists for this nor whether it is worth the effort. Overall I don't see a reason why the authors should attempt to claim that &quot;BonVision is capable of eliciting naturalistic behaviors in a virtual environment&quot;, since the naturalness of rodent behaviors in virtual environments is a topic of debate in some circles, independent of the software used to generate those environments. I figure it's better to stay away unless this is a fight that one desires to fight.</p></disp-quote><p>We agree that there are heated debates around these issues in the field, and that this is not the place to have those discussions. We have changed the relevant sentence to read (lines 204-205): “BonVision is therefore capable of generating virtual reality environments which both elicit, and are responsive to animal behaviour.”</p></body></sub-article></article>