<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">54313</article-id><article-id pub-id-type="doi">10.7554/eLife.54313</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Stable and dynamic representations of value in the prefrontal cortex</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-167803"><name><surname>Enel</surname><given-names>Pierre</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8983-6223</contrib-id><email>pierre.enel@mssm.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-13779"><name><surname>Wallis</surname><given-names>Joni D</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><contrib contrib-type="author" id="author-88065"><name><surname>Rich</surname><given-names>Erin L</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7153-6027</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund6"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><xref ref-type="other" rid="dataset1"/></contrib><aff id="aff1"><label>1</label><institution>Nash Family Neuroscience Department, Icahn School of Medicine at Mount Sinai</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Friedman Brain Institute, Icahn School of Medicine at Mount Sinai</institution><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Helen Wills Neuroscience Institute, University of California at Berkeley</institution><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution>Department of Psychology, University of California at Berkeley</institution><addr-line><named-content content-type="city">Berkeley</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse, National Institutes of Health</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Wassum</surname><given-names>Kate M</given-names></name><role>Senior Editor</role><aff><institution>University of California, Los Angeles</institution><country>United States</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>07</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e54313</elocation-id><history><date date-type="received" iso-8601-date="2019-12-10"><day>10</day><month>12</month><year>2019</year></date><date date-type="accepted" iso-8601-date="2020-07-06"><day>06</day><month>07</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Enel et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Enel et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-54313-v3.pdf"/><abstract><p>Optimal decision-making requires that stimulus-value associations are kept up to date by constantly comparing the expected value of a stimulus with its experienced outcome. To do this, value information must be held in mind when a stimulus and outcome are separated in time. However, little is known about the neural mechanisms of working memory (WM) for value. Contradicting theories have suggested WM requires either persistent or transient neuronal activity, with stable or dynamic representations, respectively. To test these hypotheses, we recorded neuronal activity in the orbitofrontal and anterior cingulate cortex of two monkeys performing a valuation task. We found that features of all hypotheses were simultaneously present in prefrontal activity, and no single hypothesis was exclusively supported. Instead, mixed dynamics supported robust, time invariant value representations while also encoding the information in a temporally specific manner. We suggest that this hybrid coding is a critical mechanism supporting flexible cognitive abilities.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>value</kwd><kwd>working memory</kwd><kwd>prefrontal cortex</kwd><kwd>orbitofrontal cortex</kwd><kwd>decision-making</kwd><kwd>neural coding</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01-MH121448</award-id><principal-award-recipient><name><surname>Wallis</surname><given-names>Joni D</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01-MH097990</award-id><principal-award-recipient><name><surname>Wallis</surname><given-names>Joni D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100005197</institution-id><institution>Hilda and Preston Davis Foundation</institution></institution-wrap></funding-source><award-id>Postdoctoral fellowship</award-id><principal-award-recipient><name><surname>Rich</surname><given-names>Erin L</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000026</institution-id><institution>National Institute on Drug Abuse</institution></institution-wrap></funding-source><award-id>K08-DA039051</award-id><principal-award-recipient><name><surname>Rich</surname><given-names>Erin L</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01-MH117763</award-id><principal-award-recipient><name><surname>Wallis</surname><given-names>Joni D</given-names></name></principal-award-recipient></award-group><award-group id="fund6"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001391</institution-id><institution>Whitehall Foundation Research Grant</institution></institution-wrap></funding-source><award-id>Research Grant</award-id><principal-award-recipient><name><surname>Rich</surname><given-names>Erin L</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The prefrontal cortex encodes both stable and dynamic representations of expected value, providing mechanisms to support robust as well as flexible access to value information during temporal delays.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Theories of value-based decision-making suggest that the brain computes values for different choice options in order to compare items with different qualities on a common scale (<xref ref-type="bibr" rid="bib46">Padoa-Schioppa, 2011</xref>). This attribution of a value relies on the association between an option and its outcome, and is learned by comparing the expected value of an option to the actual outcome experienced with it. However, when the presentation of an option is separated in time from its outcome, expected values must be temporarily held in memory for such comparisons to be possible.</p><p>The prefrontal cortex (PFC) plays a key role in working memory (WM), the temporary maintenance of information across such temporal gaps. While the lateral PFC is particularly involved in WM relating to cognitive information (<xref ref-type="bibr" rid="bib13">Constantinidis and Klingberg, 2016</xref>), it is less clear how value expectations are maintained across delays. The content model of WM postulates that each prefrontal region maintains and manipulates the type of information that it is specialized to process (<xref ref-type="bibr" rid="bib23">Goldman-Rakic, 1996</xref>; <xref ref-type="bibr" rid="bib30">Lara et al., 2009</xref>). From this view, maintaining an accurate estimation of expected values may rely more on those regions of PFC involved in learning and decision-making, such as the orbitofrontal cortex (OFC) or anterior cingulate cortex (ACC). OFC and ACC unit activity is known to reflect expected values associated with stimuli or actions (<xref ref-type="bibr" rid="bib57">Rushworth and Behrens, 2008</xref>; <xref ref-type="bibr" rid="bib28">Kennerley et al., 2009</xref>; <xref ref-type="bibr" rid="bib1">Amiez et al., 2006</xref>), but little is known about the dynamics in either region that could bridge delays between outcomes and their predictive cues.</p><p>Here, we aimed to assess how value information is maintained across task delays in OFC and ACC. Neural mechanisms that maintain other domains of information in WM have been widely studied, but remain a subject of debate (<xref ref-type="bibr" rid="bib12">Constantinidis et al., 2018</xref>; <xref ref-type="bibr" rid="bib33">Lundqvist et al., 2018</xref>). One theory suggests that persistent and stable neuronal activity maintains WM representations across time (<xref ref-type="bibr" rid="bib65">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib56">Riley and Constantinidis, 2015</xref>; <xref ref-type="bibr" rid="bib12">Constantinidis et al., 2018</xref>). This hypothesis stems from the observation of consistent activity patterns in the PFC of monkeys while they wait during a delay to execute an action (<xref ref-type="bibr" rid="bib19">Funahashi et al., 1989</xref>; <xref ref-type="bibr" rid="bib22">Fuster, 1973</xref>). However, in various WM tasks, little persistent activity is found in the lateral lPFC of macaque monkeys (<xref ref-type="bibr" rid="bib61">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib32">Lundqvist et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Rainer and Miller, 2002</xref>), yet the memoranda can be decoded throughout the delay (<xref ref-type="bibr" rid="bib6">Barak et al., 2010</xref>). This prompted the development of a second theory, suggesting that WM involves dynamic representations and activity-silent synaptic encoding (<xref ref-type="bibr" rid="bib62">Stokes et al., 2017</xref>; <xref ref-type="bibr" rid="bib41">Miller et al., 2018</xref>). Yet another perspective postulates that WM could be implemented by the sequential activation of single neurons passing on information to bridge a delay (<xref ref-type="bibr" rid="bib52">Rajan et al., 2016</xref>; <xref ref-type="bibr" rid="bib24">Harvey et al., 2012</xref>).</p><p>Given these multiple perspectives, we investigated the nature of value representation in the OFC and ACC in a value-based decision-making task in which monkeys were presented a reward predicting cue associated with a reward delivered after a delay. While delay activity encoded value, attempts at categorizing activity as persistent or transient, stable or dynamic, failed to exclusively support either view. Instead, both characteristics could be found in single unit and population activities, such that a purely stable or dynamic representation of value could be extracted. We hypothesize that these mixed dynamics occurring in the same neural population serve unique purposes. Stable representations allow downstream regions to read out memoranda with the same decoding scheme irrespective of the delay duration, while dynamic representations encode temporal information necessary to anticipate events and prepare behaviors. From this view, a rich mixed dynamical regime could supply underlying mechanisms that support flexible cognitive abilities.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavior</title><p>Two monkeys (subjects M and N) performed a value-based decision-making task in which visual stimuli were associated with juice rewards (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Images predicted an amount and type of reward, which was either primary in the form of juice at the end of the trial, or secondary as a bar that increased proportionally to the value associated with the stimulus (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). The bar, always present on the screen, represented an amount of juice that was received when it was cashed in every four trials. A total of eight pictures comprised the set of stimuli associated with reward, with four different values matched between primary (juice) and secondary (bar) rewards in terms of the monkeys’ preferences (<xref ref-type="bibr" rid="bib53">Rich and Wallis, 2016</xref>). After the presentation of the pictures, monkeys were required to move a joystick either left or right depending on another visual cue in order to obtain the predicted reward. Joystick direction was unrelated to the reward-predicting picture, but if the incorrect response was made no reward was delivered.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Value-based decision making task.</title><p>(<bold>A</bold>) Monkeys initiated a trial by fixating a point in the center of the screen. A reward predicting cue appeared that the subject was required to fixate for 450 ms. After a 1500 ms delay, one of two possible images instructed the monkey to move a joystick right or left. Contingent on a correct joystick response, monkeys received a reward either in the form of juice (primary) or an increase of a reward bar (secondary), which was constantly displayed on screen (blue bar in figure). Note that the presentation of the reward predicting cue and the delivery of the reward are separated in time by more than 2 s. (<bold>B</bold>) A total of eight reward predicting pictures covered the combinations of four possible values, and two reward types. (<bold>C</bold>) Probability of choosing a juice option in choice trials for every pair of cues in which one predicts a juice reward and the other a bar reward.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig1-v3.tif"/></fig><p>Monkeys were presented with choice trials (~20% of trials), in which the animals could choose between two stimuli presented simultaneously on the screen. ChoicesSelections were made by fixating the stimulus of their choice, and the associated reward was delivered at the end of the trial. Both monkeys learned to perform the task optimally by selecting the stimulus associated with higher values in more than 90% of the trials (<xref ref-type="bibr" rid="bib53">Rich and Wallis, 2016</xref>). A logistic regression model predicting the cues chosen by the animals with variables value and type showed a much lower coefficient for type, and its p-values were many orders of magnitude larger, confirming that the type of reward had little influence on the choice of animals compared to value (<xref ref-type="fig" rid="fig1">Figure 1C</xref>, coefficients with [p-value of t-test] for monkeys M and N respectively; type left/right: .41 [2.53e-3]/-.51 [1.75e-4] and −0.61 [2.6e-3]/0.32 [0.11], value left/right: 1.77 [6.1e-117]/−1.95 [3.02e-136] and 2.66 [1.59e-94]/−2.46 [2.27e-90]).</p><p>To assess the dynamics of value representations, we focused on the single cue trials only. Here, reaction times in the unrelated joystick task decreased as the value of the stimulus increased, providing evidence that monkeys maintained an expectation of the upcoming reward across the intervening time interval (<xref ref-type="bibr" rid="bib53">Rich and Wallis, 2016</xref>, <xref ref-type="fig" rid="fig1">Figure 1B</xref>). Reaction times were also unaffected by the type of reward anticipated (linear regression predicting reaction time as a function of value and type in single cue trials : coefficients for value [p-value of t-test] for monkeys M and N, respectively; type: −21 [.08] and 14.5 [.32], value: −63.9 [1.73e-31] and −65.7 [2.66e-23]). Therefore, we focus mainly on the representation of value during the period between the onset of the reward predicting cue and reward delivery, which lasted ~3 s, and included a ~2.4 s delay after the cue offset.</p><p>Single and multi units were recorded in the orbitofrontal (OFC) and anterior cingulate cortex (ACC) of the two subjects while they performed the decision-making task across 24 and 20 sessions (monkey M and N, respectively).</p><p>Units from all recording sessions and both monkeys were pooled together to create 2 populations of 798 and 315 neurons recorded from the OFC and ACC, respectively (<xref ref-type="table" rid="table1">Table 1</xref>). Basic unit and decoding results did not differ across monkeys (except for encoding and decoding of reward type in ACC, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1D and E</xref>) so data were pooled.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Number of recorded units with an average activity per session above 1 Hz.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th align="center" colspan="2">Single</th><th align="center" colspan="2">Multi</th><th>Total</th></tr><tr><th>Monkey</th><th>M</th><th>N</th><th>M</th><th>N</th><th/></tr></thead><tbody><tr><td>OFC</td><td>259</td><td>192</td><td>170</td><td>177</td><td>798</td></tr><tr><td>ACC</td><td>114</td><td>72</td><td>81</td><td>48</td><td>315</td></tr></tbody></table></table-wrap></sec><sec id="s2-2"><title>Encoding and decoding of value in unit activity</title><p>To assess the encoding of task variables by the neurons in these two regions, we regressed the firing rate of single and multi units on variables value, type and their interaction value x type and tested the contribution of these variables with an ANOVA. We considered value as a categorical variable, as some units activated in a non-linear fashion with respect to value, for example by responding only to a specific value, and could not be modeled with a simple linear regression (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Note that these activity profiles are not consistent with traditional and bprime;value coding′, in which firing rates monotonically change with value, but they nonetheless encode value information. To demonstrate that a non-negligible proportion of units encoded value non-linearly, and that these units do not respond simply to the identity of one reward cue, we found the number of neurons significant for value in an ANOVA (activity ~ C(value) + type + C(value):type), that were not significant for an interaction between value and type with the same ANOVA, and not significant for value with a linear regression model (activity ~ value + type + value:type). Approximately 13% of units in both OFC and ACC fit these criteria.</p><p>Similar to previous reports (<xref ref-type="bibr" rid="bib54">Rich and Wallis, 2017</xref>), the vast majority of OFC and ACC units encoded value at some point after the reward predicting cue and before the reward (73.2% for OFC and 77.8% for ACC, <xref ref-type="fig" rid="fig2">Figure 2B</xref>). The proportion of neurons encoding value peaked after the presentation of the reward predicting cue and steadily decreased until the reward (C, <xref ref-type="fig" rid="fig2">Figure 2A</xref>). The type of reward was only weakly encoded compared to value, with less than half the proportion of units encoding it at any time (29.8% for OFC and 37.1% for ACC, <xref ref-type="fig" rid="fig2">Figure 2B</xref>). No difference between ACC and OFC was found in the proportion of units encoding value at any point during the epoch of interest, but encoding of type was higher in ACC mostly driven by the data of one monkey (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, χ<sup>2</sup> test of independence, value: p=0.13 and type: p=0.022). The proportion of units encoding value was higher in ACC during the delay (<xref ref-type="fig" rid="fig2">Figure 2A</xref>); however, this difference was also mostly driven by the data from one monkey and is not significant in the other subject (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). In accordance with previous findings (<xref ref-type="bibr" rid="bib29">Kennerley and Wallis, 2009</xref>), the relative onset of value encoding was earlier in the OFC compared to ACC (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Latencies in the OFC were significantly shorter for both value and type (Kruskal-Wallis on latencies value:[stat = 19.4, p=1.05*10<sup>−5</sup>] and type:[stat = 7.93, p=4.85*10<sup>−3</sup>]). This result was replicated in each monkey’s data independently (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Encoding and decoding of task variables.</title><p>(<bold>A</bold>) Percentage of units encoding the different variables at each point in time in both OFC and ACC. Significant difference in value encoding neurons with χ<sup>2</sup> test is indicated by thick black lines at the bottom (p≤0.01). (<bold>B</bold>) Percentage of units encoding the task variables at any point in time from the onset of cue presentation to the delivery of the reward. (<bold>C</bold> and <bold>D</bold>) Average decoding accuracy of value and type, respectively, across five randomly generated population data sets, with a ridge regression classifier. Significant decoding is shown by the thick portions of line and corresponds to the time bins where the aggregated p-value of 1000-permutation tests for each of the five data sets was lower than 0.01. The dashed red line is the average of 200 sub-samples of OFC data with the same number of units as in ACC for comparison. (<bold>E</bold> and <bold>F</bold>) Kernel density estimate of encoding latencies in units for value and type, respectively. OFC latencies are significantly shorter (see main text).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig2-v3.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Encoding and decoding of value and type for each monkey.</title><p>In each plot, OFC and ACC are represented with red and blue respectively, and monkey M and N with plain and dashed lines. (<bold>A</bold>) Percentage of units encoding value at each point in time. Significant difference in value encoding neurons with χ<sup>2</sup> test is indicated by thick black/grey lines at the bottom (p≤0.01) for monkey M/N. (<bold>B</bold>) Average value decoding accuracy across five randomly generated population data sets, with a ridge regression classifier. Significant decoding is shown by the thick portions of line and corresponds to the time bins where the aggregated p-value of 1000-permutation tests for each of the five data sets was lower than 0.01. (<bold>C</bold>) Kernel density estimate of unit value encoding latencies. <bold>D</bold>, <bold>E</bold> and <bold>F</bold> correspond to <bold>A</bold>, <bold>B</bold> and <bold>C</bold> for variable type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig2-figsupp1-v3.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Example neurons with nonlinear encoding of value.</title><p>Average firing rate of six example neurons showing a non-linear relationship with value. Green and black curves represent averaged z-scored firing rate for juice and bar trials, respectively. Error bars represent the standard error of the mean. These example neurons are significant for value with an ANOVA, not significant for the interaction of value and type with the same ANOVA and not significant for value in a linear regression.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig2-figsupp2-v3.tif"/></fig></fig-group><p>Population decoding of value with a linear ridge classifier elicited a peak in accuracy similar to the peak in the proportion of encoding units after the presentation of the delay, followed by a stable significant decoding until reward (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, significance threshold set to p&lt;0.01 with 1000 permutations, replicated in each monkey, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). A similar result was observed for decoding type (<xref ref-type="fig" rid="fig2">Figure 2C</xref>), although decoding accuracy rapidly dropped and reached non significance around the joystick task. The first significant bin decoding value and type was 100 ms and 150 ms earlier respectively in OFC compared to ACC, and peak decoding was earlier and higher in OFC compared to ACC (for value/type: 93.5%/91.5% in OFC at 250 ms/350 ms, 77.6%/75.6% in ACC at 450 ms/450 ms).</p><p>Both regions had similar proportions of value encoding after the cue presentation; however, decoding accuracy was more than 10% higher in OFC, which can be partly explained by a higher unit count in the OFC population. To fairly compare decoding accuracy between the two regions, we randomly sub-sampled the OFC population 200 times to match the number of ACC units. The average accuracy of the sub-sampled populations was still slightly higher in OFC than ACC, indicating that the higher number of units in OFC alone cannot explain the difference in accuracy. The sensitivity index (also known as d’) averaged across pairs of values in all neurons was higher in OFC (0.26) than in ACC (0.22), suggesting that values might be easier to separate in OFC activity compared to ACC, leading to higher decoding accuracy.</p><p>Together, these results show that value is strongly encoded in these prefrontal regions and can be significantly decoded throughout the delay. Because of the lower incidence of reward type encoding and lack of continuous type decoding throughout the delay, the rest of the study focused more on the representation of value than type.</p></sec><sec id="s2-3"><title>Persistent versus sequential encoding</title><p>The total number of neurons encoding value during the delay was much higher than the number encoding at any particular point in time, indicating that value was represented by different neurons throughout the delay. Given this, we next sought to understand the contribution of individual units to the population dynamics. A common method for doing this is sorting neuron responses. For example, to demonstrate that a neural population uses a sequential activation scheme, sorting the neurons by their peak activation typically displays a tiling of the delay (<xref ref-type="bibr" rid="bib50">Pastalkova et al., 2008</xref>). To ensure that sequential encoding of value is a robust feature of OFC and ACC activity, we split the data into an equal number of training and testing trials. We sorted the units based on the time of peak value encoding during the delay with the training half, and found that both OFC and ACC value coding tiled the delay, such that different subsets of neurons were most selective at different times (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). We then sorted the testing data according to the peak-encoding bin of the training data to assess its consistency (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), and found that the tiling of the delay is also present in the testing data, although somewhat less clear. In the period restricted to the delay itself, peak encoding times did not follow a straight line, such that the proportion of encoding neurons decreased during the delay (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). This suggests that encoding epochs are not uniformily distributed in time, consistent with a recent model of sequential encoding (<xref ref-type="bibr" rid="bib52">Rajan et al., 2016</xref>). The peak encoding bins of training and testing data sets were also significantly correlated, which is expected with sequential encoding. Thus, our results partially support the hypothesis of a sequential representation of value in both areas, consistent with conclusions drawn from previous studies (<xref ref-type="bibr" rid="bib50">Pastalkova et al., 2008</xref>), although the mere firing rate of units is often used (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A D</xref> for firing rates). The same analyses of variable type yielded similar results (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Tiling of the delay by value encoding units.</title><p>(<bold>A</bold>) Tiling of individual units according to their peak encoding of value across the delay in both OFC and ACC with the training half of the trials. Colors represent the negative log of the encoding p-value (ANOVA, F test on value). The measure is bounded between 2 and 5 for visualization and corresponds to p-values ranging from 0.01 to 10<sup>−5</sup>. (<bold>B</bold>) Tiling with the testing half of the trials, with units sorted according to the training half of the trials, to show consistency in sequential encoding. Note that only units that had significant encoding in both the training and testing dataset were included, hence the lower number of units. (<bold>C</bold>) Scatter plot of the peak encoding bin for training versus testing half of the data. Graphs show the Spearman correlation coefficient, R, with associated p-value, p, and trendline.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig3-v3.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Tiling and spanning of delay with unit firing rate.</title><p>Activity of neurons was averaged across trials and normalized so that activity is scaled between 0 and 1. (<bold>A</bold>) Tiling of the delay by OFC activity and surrounding epochs. (<bold>B</bold>) Clustering of OFC activity in two clusters with K-means clustering. Left graph shows the activity of each unit sorted by cluster. In the right graph, each curve is the averaged activity across all the units within a cluster. (<bold>C</bold>) Same as B with six clusters (determined with elbow method) instead of 2. <bold>D</bold>, <bold>E</bold> and <bold>F</bold> are identical to <bold>A</bold>, <bold>B</bold> and <bold>C</bold> with ACC data instead of OFC. Events occurring at times indicated by 0 on the x-axis are: reward cue on, joystick instruction cue on, and reward delivery.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig3-figsupp1-v3.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Tiling of the delay by reward type encoding units.</title><p>Same as main figure for reward type. Panels <bold>A, B </bold>and <bold>C</bold> correspond to the panels A, B and C in the main figure but for reward type instead of reward value.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig3-figsupp2-v3.tif"/></fig></fig-group><p>However, sorting the neurons by the proportion of the delay during which they encode value offers a different picture. <xref ref-type="fig" rid="fig4">Figure 4</xref> shows the same splitting procedure applied to encoding duration (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> for variable type). While some units had time-limited encoding of value, others continuously encoded value throughout the delay, supporting the notion that there is a stable representation of value in the population. Similar results were obtained by clustering the average firing rates of neurons into two clusters (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B E</xref>). One cluster had increased firing rates for the duration of the delay, consistent with the persistent activity hypothesis. However, separating neurons into six clusters uncovered more diverse profiles of activity that nonetheless followed basic principles: phasic activity followed task events with stable or monotonically evolving activity between events (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C F</xref>). Thus, depending on the sorting method employed, the dynamic and sequential or stable and persistent hypothesis could be defended. Overall, encoding by most units covered a relatively small portion of the delay. A reverse cumulative distribution of the number of units encoding value showed a sharp decrease in the fraction of the delay covered (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). Only 6.9% and 10.2% of units encoded value for more than half the delay in OFC and ACC, respectively, while less than 1% of neurons encoded type for that same duration (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1D</xref>). Further, 80% coverage of the delay was found in roughly 1% of the units (OFC: 0.75%, ACC: 1.59%). The distribution of delay coverage by encoding units did not differ between OFC and ACC for the encoding of value, but there was a tendency for higher type encoding duration in ACC (Kolmogorov-Smirnov test, value: [stat = 0.079, p=0.90] and type:[stat = 0.21, p=0.025]).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Spanning of the delay by value encoding units.</title><p>(<bold>A</bold>) Units sorted by the duration of encoding of value with the training half of the trials. Colors represent the negative log of the encoding p-value (ANOVA, F test on value). The measure is bounded between 2 and 5 for visualization and corresponds to p-values ranging from 0.01 to 10<sup>−5</sup>. (<bold>B</bold>) Encoding durations of the testing half of the trials sorted by the duration the training trials. (<bold>C</bold>) Scatter plot of encoding duration for training versus testing half of the data. Graphs show the Spearman correlation coefficient, R, with associated p-value, p, and trendline. (<bold>D</bold>) Reverse cumulative distribution of units encoding value as a function of the fraction of the delay covered. This graph shows the proportion of units encoding value for at least the fraction of the delay indicated on the x-axis (no significant difference between regions, see main text).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig4-v3.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Spanning of the delay by reward type encoding units.</title><p>Same as main figure for reward type. Panels <bold>A, B, C </bold>and <bold>D</bold> correspond to the panels A, B, C and D in the main figure but for reward type instead of reward value.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig4-figsupp1-v3.tif"/></fig></fig-group></sec><sec id="s2-4"><title>Population dynamics</title><p>Decoding analyses allow us to assess how well value information can be extracted from a population as a whole, but standard approaches that find unique solutions to optimally decode value at each point in time say little about the dynamics of a representation. In addition, our results show that value encoding among units can paint contrasting pictures at the level of population representation, that is, as both dynamic and sequential or stable and persistent. Therefore, to explore representational dynamics across time, we used cross-temporal decoding (CTD), in which a decoder is trained and tested at every possible pair of time bins in the delay (<xref ref-type="bibr" rid="bib61">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib4">Astrand et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Stoll et al., 2016</xref>; <xref ref-type="bibr" rid="bib39">Meyers et al., 2008</xref>). In this context, the classification accuracy is a proxy for the similarity in value representation between any two time points. We can then produce an accuracy matrix consisting of each pair of training/testing time bins. When the training and testing times are identical, we obtain a classical decoding procedure as shown in <xref ref-type="fig" rid="fig2">Figure 2C</xref>, which corresponds to the diagonal in the CTD accuracy matrix.</p><p>Using this method on the OFC and ACC populations, value representation dynamics were not clearly dynamic or stable (<xref ref-type="fig" rid="fig5">Figure 5A</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1A</xref> for variable type). OFC evidenced a somewhat stable representation from 500 ms after the reward predicting cue (around cue offset) until the response cue, but the representation remained dynamic before and after this period, as the accuracy was mostly confined to the diagonal. ACC accuracy was lower overall but showed similar patterns. While significant decoding appeared to spread from reward cue to after the response cue, accuracy was not homogeneous and remained strongest along the diagonal, indicating a rather dynamic representation of value.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Accuracy of cross-temporal decoding (CTD) of value with different methods.</title><p>Training and testing of a ridge classifier at different time bins across the delay. CTD was applied to (<bold>A</bold>) original data, (<bold>B</bold>) a stable subspace obtained from the combination of a value subspace and an ensemble method where units are iteratively removed to obtain maximum accuracy, and (<bold>C</bold>) on a dynamic ensemble where units were iteratively removed from the ensemble to maximize a dynamic score. Thick white lines indicate junctions between the three successive epochs (cue presentation, joystick task and reward delivery). The thin dashed white lines indicate the reference event of each epoch (reward predicting cue onset, response instruction, reward). The decoding accuracy corresponds to the average of five randomly generated pseudo populations and non-significant accuracy has been greyed out for clarity. The p-value threshold is 0.01 and p-values were obtained by aggregating the p-values of each dataset for a given training/testing time bin pair (see Methods section for more details; rwd = reward, resp. cue = response cue). The stable and locality scores are displayed on each panel (s = stability score, l = locality score). Note that significant decoding before the presentation of the cue is due to smoothing.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig5-v3.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Cross-temporal decoding of reward type.</title><p>Accuracy of cross-temporal decoding (CTD) of type with different methods. Same as main figure for reward type.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig5-figsupp1-v3.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Cross-temporal decoding of value with ensemble and subspace methods independently.</title><p>Cross-temporal decoding without (<bold>A, C</bold>) or with (<bold>B, D</bold>) a value subspace, and with the full population (<bold>A, B</bold>) or with the best ensemble (<bold>C, D</bold>). Thick white lines indicate junctions between the three successive epochs (cue presentation, joystick task and reward delivery). The thin dashed white lines indicate the reference event of each epoch (reward predicting cue onset, response instruction, reward). The contour curve indicates areas with a p-value lower than 0.01 with a 2000 fold permutation test. (rwd = reward, resp. cue = response cue, s = stability score).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig5-figsupp2-v3.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Cross-temporal decoding of value for each monkey independently.</title><p>Cross-temporal decoding accuracy of value with different methods for each monkey. Each row correspond to a different combination of monkey and region, as indicated at the left of each row. Since results are similar across rows, they are largely independent of monkey and region.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig5-figsupp3-v3.tif"/></fig></fig-group><p>This dynamic pattern was also reflected in the population trajectory speed obtained by calculating the distance between two successive time bins in the neural space spanned by the estimated firing rate of each unit with a shorter smoothing window for more temporally accurate firing rate (50 ms SD Gaussian kernel, <xref ref-type="fig" rid="fig6">Figure 6</xref>). The reward predicting cue triggered an increase in speed that was followed by a period of low speed, indicative of relatively stable population activity, until the joystick instruction cue triggered another period of rapidly changing activity. Thus, strongly dynamic bouts of activity occur with changes in task stimuli, and in the absence of such changes, delay activity becomes quite stable.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Trajectory speed and regression of time.</title><p>(<bold>A</bold>) Speed (rate of change) of the average activity trajectory in the neural space defined by the full population of neurons, in OFC (left) and ACC (right). Note that the difference in scale between OFC and ACC (y-axis) is due to the difference in the number of units in each population. (<bold>B</bold>) Regression of the time bin from population activity with a simple linear regression. The black line and shaded area represent the average and standard deviation of leave-one-out cross-validation results. The dashed line corresponds to ground truth. For more temporally accurate results, the firing rate of each unit was estimated with a 50 ms standard deviation Gaussian kernel instead of the 100 ms used in the other analyses.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig6-v3.tif"/></fig></sec><sec id="s2-5"><title>Stable representation of value</title><p>The basic CTD method relies on a decoder that weighs on the most discriminating features (i.e. neurons) at a particular point in time. If a sub-population strongly encodes value at one time, the decoder will attribute stronger weights to these units. However, if this sub-population changes representations or ceases to encode value, the decoder accuracy will drop. Our results suggest such temporally local representations change across time, but this could simply be due to inhomogeneous representations of value at the population level. That is, the strongest representation at one time point may not generalize well to other time points. This would not exclude the possibility that a stable representation of value exists within the population, because the classifier is not optimized to find a decoding plane that is consistent throughout the delay.</p><p>To determine whether a linear combination of neural activities can be used to extract a stable representation of value, we combined two methods to optimize CTD. The first is a subspace method inspired by <xref ref-type="bibr" rid="bib43">Murray et al., 2017</xref>, in which data are projected into a subspace that maximizes value representation while lowering the influence of temporal dynamics (see Materials and methods). This was applied to data from the onset of cue presentation to the delivery of reward. A three-dimensional subspace was obtained, and data projected onto this subspace was used to decode value, using CTD in the same manner as was done for the full population. Our cross-validation approach ensured that both train and test data were not used to generate the subspace, as it would introduce a bias in decoding results (see Materials and methods).</p><p>Repeating the CTD procedure with data projected onto the subspace demonstrates that more stable value representations can be extracted from the same population data (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2B</xref>). Although significant decoding using CTD did not fully span the cue to reward period in OFC, higher accuracies were more widespread. ACC accuracy, which also spanned a larger portion of the delay, covered nearly all of the epoch between cue and reward using the subspace approach.</p><p>Although a higher unit count is expected to produce higher decoding performances, in the case of CTD better performances might be obtained by pre-selecting the neurons that most participate in the stable decoding over time. For this purpose, we derived an ensemble method inspired by <xref ref-type="bibr" rid="bib5">Backen et al., 2018</xref> that iteratively selects units based on their contribution to a stability score. This measure was defined as the averaged accuracy matrix of the CTD restricted to the delay period. Essentially, we started by removing each unit independently to find the n-1 ensemble that most increased the stability score, then removed a second unit to find the best n-2 ensemble and so on until only one unit was left, and then selected the ensemble that maximized decoding accuracy. This procedure markedly improved stable decoding across time, with significant decoding spanning all time pairs after the reward predicting cues (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2C</xref>). The ensembles that best optimized the CTD accuracy in both regions contained only a fraction of the units from the full population (average of five randomly generated data sets: 107 out of 798 units for OFC and 71 out of 315 units for ACC). Greater numbers of units did not improve performance, likely because many of these modified their value encoding over time and did not provide a stable signal.</p><p>While the subspace and best ensemble approaches both found somewhat stable value representations, the most stability with highest accuracies was obtained with a combination of these methods. The subspace procedure was applied to each ensemble selected by the iterative ensemble method. The result is a strikingly stable CTD in both regions for the entire duration of the delay, and beyond reward delivery in ACC (<xref ref-type="fig" rid="fig5">Figure 5B</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1B</xref> for variable type). This demonstrates that when CTD is applied to a neural population, the representation may appear to be partially dynamic, yet it is possible to find a subspace of the population activity that defines a completely stable representation across time. In addition, combining subspace and ensemble procedures is a promising method to extract extremely stable representations of task variables. These results were replicated with data from each monkey independently (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>).</p></sec><sec id="s2-6"><title>Dynamic representation of value</title><p>Since we found that stable value representations could be extracted with appropriate methods, it is logical that dynamic representations can also be identified. To do this, we first defined a 'locality’ score to quantify how temporally local a representation is, based on the decoding accuracy over time. We fit a Gaussian curve to the decoding accuracy obtained from training at a given time bin and testing on all time bins. The height of the Gaussian divided by its standard deviation defined a score of temporal locality. The measure was defined as the average of the scores computed from the accuracy curves obtained across all training time bins. A dynamic representation of value was obtained by selecting the neurons with the ensemble method described above that maximized this measure. The resulting CTD accuracy displays the main feature of a strongly local and dynamic representation of value, where higher accuracy is confined to the diagonal (<xref ref-type="fig" rid="fig5">Figure 5C</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1C</xref> for variable type). With the original full population, value representation in the OFC was rather stable in the middle of the delay, but the dynamic ensemble shrunk the higher accuracy block to constrain it to the diagonal. This result demonstrates that a dynamic subspace encoding local value representations can be extracted from the same neural population that encodes value in a very stable way. Together, these results support the idea that a basic CTD applied to a neural population yields a limited view of mixed neural dynamics.</p><p>Since the dynamic and stable ensembles yielded opposite dynamics, we expected limited overlap between them. Indeed, the overlap in units was found to be at chance levels for all ensemble instances obtained by cross-validation and data sets in both regions (25 instances from five-fold cross-validation and five data sets; χ<sup>2</sup> test of independence with p≤0.01 and multi-test false discovery rate correction).</p><p>To further demonstrate that the activity in both of these areas is dynamic and supports the encoding of time, we regressed the index of time bins from 500 ms before the reward predicting cue onset until 500 ms after reward delivery with a simple linear regression from the neural population activity (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Indeed, time could be predicted accurately, supporting the notion of a robust representation of time in the neural population activity.</p></sec><sec id="s2-7"><title>Relationship between value encoding and decoding dynamics</title><p>To explore the relationship between single unit encoding and population dynamics, we correlated different encoding measures with their contribution to the value subspace and stable ensemble (Spearman correlation, see Materials and methods). We defined the encoding strength as the average of the negative log p-value of each unit across the delay, and encoding duration as the fraction of the delay when the unit significantly encoded value (p-value &lt; 0.01). First we correlated each of these measures with the unsigned value subspace weights, since a stronger weight reflected a higher influence on the subspace irrespective of its sign (unsigned weights were averaged across five randomly generated population data sets). Both measures were highly correlated with subspace weights (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, left and middle) indicating a clear link between encoding strength/duration and contributions to the value subspace. However, units might also change their pattern of value encoding during the delay. To quantify this, we defined an additional stability measure that includes the duration, strength and stability of value representation by penalizing cases where the difference in average firing between a pair of values changes sign (see Materials and methods for details). This measure was more strongly correlated with the value subspace weights (<xref ref-type="fig" rid="fig7">Figure 7A</xref>, right graph). Overall, these measures describe the qualities of activity that promote selection to the value subspace.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Correlation between encoding and decoding measures.</title><p>(<bold>A</bold>) Correlations between encoding measures and subspace weights. (<bold>B</bold>) Correlations between encoding measures and CTD accuracy contribution. (<bold>C</bold>) Z-scored firing rate of two example neurons negatively contributing to the stable ensemble (1 and 3) and one neuron contributing positively (2). Individual data points corresponding to these neurons are circled in B. (<bold>D</bold>) Correlations between stability measure and locality measure contribution. All figures show Spearman correlations. sqrt = square root; negative contributions were transformed with a symmetrical function around the origin based on square root: <inline-formula><mml:math id="inf1"><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>*</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:math></inline-formula>. Transformations applied to the data had no effect on the correlations which are rank based (see Materials and methods).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-fig7-v3.tif"/></fig><p>To explore the contribution of each unit to the stable ensemble, we defined an accuracy contribution score based on the change in mean accuracy across all time bin pairs in the CTD elicited by removing a unit from the ensemble (this measure is an average across cross-validation and population data sets). This score was correlated with encoding strength, duration and stability score in both regions (<xref ref-type="fig" rid="fig7">Figure 7B</xref>, top row). A strongly encoding unit might encode value for a limited time and/or reverse its encoding (e.g. <xref ref-type="fig" rid="fig7">Figure 7C</xref> neuron 1), and a unit covering a large part of the delay might also change over time and not provide a stable representation. Conversely, units with stable encoding contribute more to the ensemble (<xref ref-type="fig" rid="fig7">Figure 7C</xref> neuron 2). However, since decoding relies on the interaction between many neurons, these simple encoding measures of individual units cannot fully explain all of the ensemble contributions. For example, neuron three in <xref ref-type="fig" rid="fig7">Figure 7C</xref> strongly encodes value for the whole delay, but contributes negatively to the stable ensemble.</p><p>Finally, contributions to the dynamic ensembles were not correlated with the stability measure (<xref ref-type="fig" rid="fig7">Figure 7D</xref>) as expected. However, these measures were neither anti-correlated which indicates that the contribution to a dynamic ensemble is not explained merely by an absence of stable encoding.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we shed light on the complex dynamics of value representations in prefrontal cortex. While value representations have been found in the activity of ACC and OFC (for example, <xref ref-type="bibr" rid="bib57">Rushworth and Behrens, 2008</xref>), little is known about their dynamics across delays. One recent study found that OFC neurons that exhibit slow temporal integration also encode value more strongly and do so from the time of a choice until the receipt of reward (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2016</xref>). This suggested the presence of a subset of OFC neurons with stable encoding dynamics, consistent with the present results. Here, we expand on this to show that targeted methods elicit seemingly opposite results that do not lend themselves to straightforward interpretations with respect to the current theoretical frameworks common in the WM literature. Unit encoding shows features of both persistent and sequential activity, depending on which feature the analysis method is designed to extract. Similarly, targeted methods can extract either a stable subspace or a temporally local representation at the population level. These results, along with recent studies of the dynamic nature of delay activity in the prefrontal cortex (<xref ref-type="bibr" rid="bib59">Spaak et al., 2017</xref>; <xref ref-type="bibr" rid="bib43">Murray et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Meyers, 2018</xref>), argue in favor of a more nuanced view than the pure persistent/stable, activity silent/dynamic or sequential activity hypotheses.</p><p>Here, we propose that mixed dynamical regimes, with both time and temporally insensitive information concurrently encoded in the same neural population, can provide a richer substrate to face varying task demands. On one hand, the main function of WM-related processes is to temporarily maintain information not available in the sensory environment so it can be manipulated or retrieved later. A stable neural representation is a robust means of holding such information online and is particularly useful when information must be extracted at an unspecified time. This is because a target population can reliably read out an unchanging representation whenever it is required. Conversely, representing relevant information in a time-sensitive manner is also critical, since most behaviors are organized in time, whether they are internally generated or aligned to external events (<xref ref-type="bibr" rid="bib40">Meyers, 2018</xref>; <xref ref-type="bibr" rid="bib14">Cueva et al., 2019</xref>). For example, our task includes a recurring sequence of events that the subjects had learned. By representing time, they can anticipate the next event and prepare to either process a task relevant stimulus or produce a motor output. Such preparatory processes can help optimize task performance and maximize the amount of reward the subject can receive.</p><p>These results present the coding of value in two dynamical regimes, either fully stable or fully dynamic. However, these are the representations that were specifically targeted by the present analysis methods, and it is possible that these dynamics are the extremes of a continuous spectrum of dynamical regimes that can be extracted by any downstream population, depending on the computational task it performs. Indeed, it has been shown that the time constant of neurons encoding value in prefrontal region is diverse (<xref ref-type="bibr" rid="bib9">Cavanagh et al., 2016</xref>), supporting the hypothesis of a multitude of dynamical regimes. Similar to the idea that mixed selectivity provides a universal mix of task variables from which the representations most relevant to the task can be extracted (<xref ref-type="bibr" rid="bib55">Rigotti et al., 2013</xref>), a multitude of dynamical regimes can provide a broad range of dynamics from which some can be selected for their relevance to the task (<xref ref-type="bibr" rid="bib8">Bernacchia et al., 2011</xref>).</p><p>Beyond temporal dynamics, our data set also demonstrates notable heterogeneity in the representations of outcomes within these prefrontal networks. While value signals were robust in both OFC and ACC, consistent with previous reports of strong abstract value coding in these regions (<xref ref-type="bibr" rid="bib47">Padoa-Schioppa and Assad, 2006</xref>; <xref ref-type="bibr" rid="bib57">Rushworth and Behrens, 2008</xref>), there were also weaker representations of the anticipated reward type. This is consistent with findings in human and rodent OFC reporting value signals that are specific to a particular type of outcome (<xref ref-type="bibr" rid="bib25">Howard et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Stalnaker et al., 2014</xref>). Interestingly, this variable was barely encoded and decoded in ACC in one of the monkeys, suggesting that different information representation strategies were adopted by each subject. In addition, we provide evidence that neurons that encode value with linear changes in firing rate coexist with a smaller population of neurons that exhibit non-linear value encoding, some of which are non-monotonic, for example firing preferentially to value 3. Many internally generated continuous variables, such as time, space, and numerosity, are represented with both monotonic and non-monotonic activities in different brain regions (<xref ref-type="bibr" rid="bib44">Nieder and Dehaene, 2009</xref>; <xref ref-type="bibr" rid="bib16">Eichenbaum, 2014</xref>; <xref ref-type="bibr" rid="bib69">Wittmann, 2013</xref>; <xref ref-type="bibr" rid="bib42">Moser and Moser, 2008</xref>; <xref ref-type="bibr" rid="bib20">Funahashi, 2013</xref>; <xref ref-type="bibr" rid="bib15">Dehaene and Brannon, 2011</xref>) and our results demonstrate that value representations follow a similar format. Such nonlinear coding may enhance population-level signals (as it is the case with nonlinear mixed selectivity <xref ref-type="bibr" rid="bib21">Fusi et al., 2016</xref>), and complements the rich representations of time and outcome type that are likely critical to accurately predicting multiple facets of an upcoming reward.</p><p>The debate about persistent versus dynamic representation in WM tasks has relied heavily on modeling experiments. Neural network models of persistent activity have historically been implemented as fixed point attractors when the remembered information comes from a discrete number of items (<xref ref-type="bibr" rid="bib2">Amit, 1995</xref>; <xref ref-type="bibr" rid="bib11">Compte et al., 2000</xref>) or bump/line attractors in the case of parametric WM (<xref ref-type="bibr" rid="bib37">Machens et al., 2005</xref>; <xref ref-type="bibr" rid="bib58">Seung, 1996</xref>). Another neural network framework, with random recurrent connections referred to as reservoir computing, subsequently demonstrated that short term memory is readily obtained from generic networks in which different inputs (stimuli) elicit distinct trajectories thereby encoding stimuli for a short duration with highly dynamic signatures (<xref ref-type="bibr" rid="bib34">Maass et al., 2002</xref>; <xref ref-type="bibr" rid="bib27">Jaeger, 2001</xref>). While these dynamic properties appear irreconcilable, they are not necessarily exclusive and can be combined in a single network, as demonstrated by more recent models (<xref ref-type="bibr" rid="bib35">Maass et al., 2007</xref>; <xref ref-type="bibr" rid="bib49">Pascanu and Jaeger, 2011</xref>; <xref ref-type="bibr" rid="bib17">Enel et al., 2016</xref>). Such networks can provide mixed dynamics similar to those observed in the present task, suggesting that they are better models for short-term value maintenance in OFC and ACC. Recent models also provide theoretical explanations for the degree of persistent versus dynamical activity. Specifically, some emerging theories suggest that this trade-off is a function of task requirements instead of the network architecture (<xref ref-type="bibr" rid="bib38">Masse et al., 2019</xref>; <xref ref-type="bibr" rid="bib45">Orhan and Ma, 2019</xref>). In addition, little is known about the effect of training on WM representations in experimental subjects such as monkeys. The degree of stable versus dynamic representations might vary depending on whether the subject is over-trained on the task or on particular stimuli in the task, such that over training might shift weak and dynamical activities toward more stable and persistent ones (<xref ref-type="bibr" rid="bib7">Barak et al., 2013</xref>).</p><p>Among the dynamic representations of value, we found some evidence for sequential organization within our neural populations. Sequential activity is the successive, short-duration activation of individual neurons, a specific case of dynamical activity that follows a regular pattern. These patterns are thought to either pass information to bridge a delay or represent the passage of time in the absence of external stimuli (<xref ref-type="bibr" rid="bib50">Pastalkova et al., 2008</xref>; <xref ref-type="bibr" rid="bib52">Rajan et al., 2016</xref>). Sequential unit activity has also been associated with a variety of functions, including the temporal segmentation of memories in the hippocampus (<xref ref-type="bibr" rid="bib36">MacDonald et al., 2011</xref>; <xref ref-type="bibr" rid="bib16">Eichenbaum, 2014</xref>) and WM in parietal cortex and the rat medial PFC (<xref ref-type="bibr" rid="bib24">Harvey et al., 2012</xref>; <xref ref-type="bibr" rid="bib18">Fujisawa et al., 2008</xref>). To the best of our knowledge, this type of sequential encoding has not been reported in prefrontal regions of primates performing WM tasks, potentially because it has not been the focus of analyses to date. Consistent with the heterogeneous nature of our neural population, only some neurons showed evidence of reliable sequencing, while other neurons′ activation and encoding was not short and uniform. Sequencing appeared most prominent early in the delay, and may coincide with the brief, highly dynamical epoch immediately after the presentation of a stimulus to be remembered that has been found in this and other studies. This dynamic period is typically followed by a more stable representation until the end of the delay (<xref ref-type="bibr" rid="bib61">Stokes et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Murray et al., 2017</xref>; <xref ref-type="bibr" rid="bib6">Barak et al., 2010</xref>; <xref ref-type="bibr" rid="bib67">Wasmuht et al., 2018</xref>; <xref ref-type="bibr" rid="bib10">Cavanagh et al., 2018</xref>). It has been proposed that the dynamical epoch following the memorandum presentation is the result of stimulus processing, which in some tasks corresponds to the transformation of the stimulus into task-relevant information (<xref ref-type="bibr" rid="bib61">Stokes et al., 2013</xref>), and our results suggest that this process may involve more reliable sequencing among neurons. More generally, it appears that task relevant inputs to a brain region trigger local processing that is reflected in transiently increased and dynamic activity as was demonstrated by a reservoir model mimicking ACC activity dynamics (<xref ref-type="bibr" rid="bib17">Enel et al., 2016</xref>).</p><p>One challenge for attractor networks modeling WM tasks has been maintaining stable representations in the presence of distractors or intervening task demands. In the current study, monkeys had to perform a joystick sub-task before receiving reward, and CTD with the original neural populations yielded stable value representations until the presentation of the joystick instruction cue. Similar dynamics have been observed when a distractor perturbs the representation of a spatial cue held in WM (<xref ref-type="bibr" rid="bib48">Parthasarathy et al., 2017</xref>; <xref ref-type="bibr" rid="bib10">Cavanagh et al., 2018</xref>). The initially stable representation might be the result of unperturbed network activity that persists until a task relevant input (the response instruction cue) impacts the networks′ dynamics. However, here we show that analyses designed to identify stable representations can, indeed, extract a subspace of the same population activity that was stable across the intervening joystick task. Interestingly, value encoding and basic decoding remained largely unaffected during the intervening task, unlike results previously reported in a dual task experiment (<xref ref-type="bibr" rid="bib68">Watanabe and Funahashi, 2014</xref>). Here, when monkeys solved attention and memory tasks concurrently, selectivity for components of either task diminished at the level of single neurons and population activity. Similarly, intervening stimuli or distractors presented during the delay of a WM task disrupt the representation of memoranda (<xref ref-type="bibr" rid="bib31">Lebedev et al., 2004</xref>; <xref ref-type="bibr" rid="bib66">Warden and Miller, 2007</xref>; <xref ref-type="bibr" rid="bib26">Jacob and Nieder, 2014</xref>). The discrepancy between these and our results might lie in the non-overlapping nature of value information and joystick task. In our case, the task and stimuli were independent, whereas in these other studies, a stimulus of the same nature as the memoranda (e.g. a visual stimulus) was presented, perhaps triggering a conflicting representation.</p><p>A notable conclusion from this study is that OFC and ACC have similar dynamics that represent value throughout a delay. Despite this overall similarity, a few key differences were found. First, value was encoded less than 100 ms earlier in OFC compared to ACC. Second, value was more easily decoded in OFC immediately after the reward predicting cue, while a higher proportion of ACC neurons encoded value during the delay. These results argue in favor of the notion that value information is represented and processed earlier in the OFC than in ACC (<xref ref-type="bibr" rid="bib29">Kennerley and Wallis, 2009</xref>). Nonetheless, the mixing of dynamic regimes appears quite similar between these two areas, so that both encode time sensitive and insensitive representations of value. Therefore, the underlying neural mechanisms responsible for encoding value over time are likely similar in OFC and ACC.</p><p>Overall, using analysis methods designed to extract different signal dynamics, we found that all co-exist to some extent within the same neuronal populations that represent the same task-related variable. Rich representations of expected values are critical for adaptive learning, and the mixed dynamics reported here could play an important role in conveying both general and temporally specific value expectations to areas involved in optimizing goal-directed behavior.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Behavior and neurophysiology</title><p>The behavioral task has been previously described in <xref ref-type="bibr" rid="bib53">Rich and Wallis, 2016</xref>; <xref ref-type="bibr" rid="bib54">Rich and Wallis, 2017</xref>. Two head-fixed male Rhesus macaque monkeys (Macaca mulatta), aged 7 and 9 years, weighing 14 and 9 kg at the time of recording, were trained to perform a value-based decision making task in which they chose between visual stimuli associated with rewards. In the present study we focused on single cue trials only. All procedures were in accord with the National Institute of Health guidelines and recommendations of the University of California at Berkeley Animal Care and Use Committee. Subjects sat in a primate chair, viewed a computer screen and manipulated a bidirectional joystick. Task presentation and reward contingencies were controlled using MonkeyLogic software (<xref ref-type="bibr" rid="bib3">Asaad and Eskandar, 2008</xref>), and eye movements were tracked with an infra-red camera (ISCAN, Woburn, MA).</p><p>A total of eight pictures (~2° × 3° of visual angle) comprised the set of stimuli associated with rewards. Pictures were selected randomly from this set on each trial. Four pictures predicted the delivery of juice reward (0.05, 0.10, 0.18, 0.30 ml), and four predicted that the length of a reward bar always present on the screen would increase by a set increment. Subjects were previously trained to associate the length of the bar to a proportional amount of juice obtained every four trials. Associations between cue and reward were probabilistic, so that on four out of seven trials the type and amount of reward was consistent with the cue. On one out of seven trials, the type of reward was different, on one out of seven trials, the amount was randomly picked to be different, and on one out of seven trials, both the amount and type differed from what the cue predicted.</p><p>Single cue trials were randomly interleaved with choice trials. Because the different reward values for primary (juice) and secondary (bar) outcomes were titrated, monkeys almost always chose the target associated with a higher value irrespective of the type of reward (<xref ref-type="bibr" rid="bib53">Rich and Wallis, 2016</xref>).</p><p>During the delay between the reward predicting cue and reward delivery, monkeys were required to move a joystick either left or right depending on a visual cue, which we refer to as response instruction to avoid confusion with the reward predicting cue. Reward delivery was contingent on a correct joystick answer, and reaction times were inversely correlated with the size of the expected reward (<xref ref-type="bibr" rid="bib53">Rich and Wallis, 2016</xref>).</p><p>Electrodes were lowered at the beginning of each session in the OFC (areas 11 and 13) and dorsal bank of the ACC sulcus (area 24). Recorded units were not screened for selectivity, but those with average activity lower than 1 Hz across the session were excluded. Further details on behavior and recording methods can be found in previous publications (<xref ref-type="bibr" rid="bib53">Rich and Wallis, 2016</xref>; <xref ref-type="bibr" rid="bib54">Rich and Wallis, 2017</xref>).</p><p>Firing rates of each unit were estimated with a 100ms-SD Gaussian kernel convolution over the spike train of entire recording sessions. The resulting firing rate was divided into epochs around the reward cue, the response (joystick) instruction cue and reward delivery, then binned into 100 ms bins every 25 ms for encoding and every 50 ms for decoding methods (to reduce the number of computations, this includes cross-temporal decoding), so that there were 40 and 20 bins for 1 s of activity, respectively. Single and multi units from both monkeys were pooled together into a pseudo population for each region as the analyses presented in this paper did not produce different results between the two subjects (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). Note that significant decoding and encoding of value and type started before or on the presentation of reward predicting cue because of the smoothing method described above.</p></sec><sec id="s4-2"><title>Unit encoding</title><p>The firing rate of units was modeled with a linear regression with value, type of reward and their interaction as variables in the epochs described above. Value was defined as a categorical variable and analyzed with ANOVAs as the firing rate profile of single neurons did not always follow a linear trend with value (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Individual F tests for each variable were applied on each time bin, and a bin of activity was considered to encode a variable if the p-value associated with the test was part of at least seven consecutive time bins (~175 ms with 25 ms steps between bins) where the p-value was lower than 0.01. Encoding strength was represented as the negative log of the F test p-value.</p><p>For both the sequential encoding and encoding duration analyses, trials were randomly split in half and the above encoding analysis was applied independently to each half during the delay activity, that is, from reward predicting cue offset to reward delivery. We only kept the neurons that had at least seven bins in a row with an encoding p-value less than 0.01 in both training and testing sets, hence the lower number of neurons in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig4">Figure 4</xref>. Peak encoding and encoding duration were obtained for each half of the data independently and compared across with Spearman correlations to test for their robustness.</p></sec><sec id="s4-3"><title>Decoding</title><p>Neurons across sessions and monkeys were pooled into a single population for each of OFC and ACC. Trials were randomly drawn from each session to match the number of trials in the population data. Because the choice of trials had an influence on the decoding accuracy, we repeated that draw five times to build five different data sets (Appendix 1, step [a]). The results of every population analysis presented in this paper is the average of the five randomly generated data sets. All reported accuracies of all decoding methods, including cross-temporal decoding, correspond to the average of a five-fold cross-validation on each of the five data sets, to ensure generalizability of the results (Appendix 1, step [b]).</p><p>Population representations of value were assessed with a ridge regression classifier (linear regression with <italic>L</italic><sub>2</sub>'ridge’, or 'Tikhonov’, regularization), which is a robust and efficient classifier as compared to SVM or logistic regression, which require costly computations. A one-versus-one approach was used to classify the four different classes of 'value', which is equivalent to six binary classifiers. The regularization parameter was optimized with a nested cross-validation within the outer five-fold cross-validation mentioned above by exploring every power of ten between −2 and 5 with a 0.5 step with the training data for each population or ensemble to maximize the average performance of the classifier trained and tested on each time point between cue onset and reward (Appendix 1, step [c]).</p><p>The significance of decoding accuracy was determined with permutation testing. The value label of trials was shuffled 1000 times and compared to the original data. The p-value was calculated as the number of permutations with a higher accuracy than the original data divided by 1000. p-Values across the five data sets were aggregated with an averaging method that does not require the p-values to be independent. The aggregated p-value was derived as two times the average of the five p-values from the five data sets (<xref ref-type="bibr" rid="bib64">Vovk and Wang, 2012</xref>).</p><p>For cross-temporal decoding, we trained and tested the decoder on all possible combinations of time bin pairs. Accuracy along the diagonal represents training and testing at the same time point, while points away from the diagonal correspond to more temporally distant activities. Importantly, the testing data always correspond to out-of-sample trials so that the decoder was not trained and tested on the same trials, regardless of whether training and testing data points were from the same or different time bins.</p></sec><sec id="s4-4"><title>Subspace</title><p>To extract a stable value representation, we implemented the following method inspired by <xref ref-type="bibr" rid="bib43">Murray et al., 2017</xref>. The firing rate activity of each neuron in a population was averaged over time from cue onset to reward delivery, and then averaged across the trials where the cue predicted the same value. The resulting matrix had four rows corresponding to the four values and n columns corresponding to the number of neurons in the population. A principal component analysis (PCA) was performed on this matrix and allowed us to project the neural population activity of each trial at every time bin onto a three dimensional subspace (four values minus one dimension). This method finds the dimensions that capture the highest variance related to value while discarding temporal information by averaging across time.</p><p>When decoding with the subspace, we used a nested cross-validation approach to ensure that the subspace was derived with data independent from the training and testing of the decoder to avoid inflated accuracy due to trial-wise noise (Appendix 1, step [e]). In other words, the training data was split into two sets of trials, one to define the subspace, the other to train the decoder which was then tested on the remaining trials. The assignment of the split data to either subspace or decoder training was then swapped and the decoder tested on the remaining data. As a result, this procedure elicited two matrices of accuracy for each fold of the inner cross-validation (Appendix 1, step [d]) and were averaged.</p><p>Permutation testing with the subspace shuffled only the labels of the training trials to more closely reproduce the propensity of the subspace trained with original data to elicit higher than chance accuracy.</p></sec><sec id="s4-5"><title>Ensembles</title><p>Testing all the possible combinations of units to find the ensemble that best decodes value is computationally infeasible. To remedy this <xref ref-type="bibr" rid="bib5">Backen et al., 2018</xref> proposed a method that finds an ensemble that substantially optimizes decoding accuracy with a subset of the full population in a computationally feasible way, although it does not guarantee the selected ensemble is the single best. This method begins by screening each unit individually and selecting the most discriminating, then successively screens the remainder and adds the unit that most increases accuracy. Effectively, this method sorts the units in the order of highest contribution to decoding accuracy. However, decoding relies on the interaction of unit activities and by successively adding units, that method does not take into account all possible interactions. For example in a three neuron population, neuron 2 and 3 together might yield a higher decoding accuracy than any other pair of neurons, but neuron 1 alone yields the best accuracy and might be selected first, dismissing the possibility that neuron 2 and 3 are selected as the best 2-neuron ensemble. To partially circumvent this, we used the opposite method: successively removing units. As with the adding of units, we iteratively removed the units that maximized the decoding. This method yielded better results than adding the units.</p><p>In combination with the subspace method, stable ensembles were optimized with this method to find the ensembles that maximized the averaged cross-temporal decoding accuracy across all data points from cue onset to the reward delivery. This corresponds to averaging across all the points of the accuracy matrices represented in the figures except the points before cue and after reward. Once the best ensembles for each case (e.g. with or without subspace) were determined, cross-temporal decoding was performed with data beyond this period to show the boundaries of decoding accuracy. The dynamic ensemble was searched with the same iterative neuron-selection method that optimized a measure of temporal 'locality’ of value representation as described in the main text.</p><p>For both stable and dynamic ensembles, the iterative ensemble optimization procedure involved a nested cross-validation procedure to ensure the independence of the testing and ensemble optimization trials (Appendix 1, step [d]). As a consequence, five ensemble searches were performed for each tested population and their corresponding testing accuracies were averaged.</p><p>To speed the computations associated with the stable ensemble search, a smaller number of bins was used by estimating the firing rate with non-overlapping 200 ms bins instead of the 100 ms bins with 25 ms steps used in the results. To achieve a similar result with the dynamic ensemble while keeping temporal precision of the data, one in five bins was included in the training while testing was performed on every bin.</p></sec><sec id="s4-6"><title>Unit and population measures</title><p>Three measures were defined to quantify the encoding of value in units. The encoding strength is the average of the negative log of the p-value across time during the delay:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mo>−</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>T</italic> is the total number of bins in the delay, and <inline-formula><mml:math id="inf2"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the p-value of a unit at time bin <italic>t</italic> obtained from the ANOVA F-test described in the unit encoding section above.</p><p>The encoding duration is the number of bins where the above p-value was lower than 0.01 divided by the total number of bins.</p><p>The stability measure was defined as follow:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>v</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:mrow><mml:mo>|</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>10</mml:mn><mml:mo>×</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>−</mml:mo><mml:mover><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>|</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf3"><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:math></inline-formula> is the average firing rate of a unit for value <italic>v</italic>, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the average over all the possible combinations of values <italic>v</italic> and <italic>w</italic> except <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow></mml:math></inline-formula> is the <inline-formula><mml:math id="inf7"><mml:mrow><mml:mo stretchy="false">]</mml:mo><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">[</mml:mo></mml:mrow></mml:math></inline-formula> bounded hyperbolic tangent function, <inline-formula><mml:math id="inf8"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:math></inline-formula> is the absolute value function and <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> is the pooled standard deviation of the firing rate of a unit across trials with value <italic>v</italic> and <italic>w</italic>:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>s</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf10"><mml:msubsup><mml:mi>s</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> is the variance of the firing rate for trials with value <italic>v</italic>. Note that the difference of firing rate averaged by value, <inline-formula><mml:math id="inf11"><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>¯</mml:mo></mml:mover><mml:mo>-</mml:mo><mml:mover accent="true"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:math></inline-formula>, is signed and the measure takes the average of this signed difference. If a unit reverses its relative encoding of two values, the sign of the average difference changes over time and averaging across time will be close to zero, reflecting the lack of stability of encoding of that unit.</p><p>Two measures were used to quantify the contribution of units to the value subspace and the ensemble. For the value subspace, we used the absolute value of the subspace weights, which corresponds to the eigenvector components obtained through a PCA of the time and trial averaged data. The contribution of a unit to the stable subspace through the ensemble method was estimated by calculating the difference in average CTD accuracy (across all time bin pairs) by removing that unit from the ensemble. The last unit removed from the ensemble was discarded. For visualization of this specific measure, a symmetric square root transformation was applied where the absolute value of negative values was used before reassigning them their sign:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>g</mml:mi><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msqrt><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The distribution of this measure across units was asymmetrical around the zero, so correlation analyses were applied individually on positive and negative contributions (see Main text).</p><p>A 'locality’ measure was defined to optimize the ensemble of neurons eliciting a local CTD accuracy of value during the delay. We fit a Gaussian curve to the accuracies obtained from training at one time bin and testing on all delay bins:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The offset value <italic>b</italic> was fixed to 0.25, the chance level, and the mean of the Gaussian µ was set to the time of the training. Only the scaling factor <italic>a</italic> and the standard deviation σ were optimized. The locality measure <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>m</mml:mi></mml:mrow></mml:math></inline-formula> for a given ensemble is as follow:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>T</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:munder><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:munder><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mn>.25</mml:mn></mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>t</italic><sub>1</sub> and <italic>t</italic><sub>2</sub> are the training time and testing time, respectively, <inline-formula><mml:math id="inf13"><mml:mrow><mml:msub><mml:mi>max</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>f</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the maximum of the Gaussian fitted on training at time <italic>t</italic><sub>1</sub> and testing on all <italic>t</italic><sub>2</sub> time bins, and <inline-formula><mml:math id="inf14"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:math></inline-formula> is the standard deviation of the Gaussian fitted at time <italic>t</italic><sub>1</sub>. The contribution of a unit to the dynamic ensemble was calculated as the difference in locality measure when that unit is added to the ensemble.</p></sec><sec id="s4-7"><title>Correlations</title><p>To avoid inflated results from non-normally distributed data, we used Spearman correlations. In each of the figures showing these correlations, transformations (log or square) were applied for visualization purpose only. Note that these transformations do not affect the Spearman correlation, as it is based on rank and these transformations are monotonic and so preserve the rank of observations.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Methodology, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Data curation, Supervision, Funding acquisition, Investigation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Animal experimentation: This study was performed in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the National Institutes of Health (Assurance Number A3084-01). All of the animals were handled according to approved institutional animal care and use committee (IACUC) protocols (Protocol Number R283) of the University of California at Berkeley. All surgery was performed under isoflurane anesthesia, and every effort was made to minimize suffering.</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-54313-transrepform-v3.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The neural recording data analyzed in this paper is available online at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.4j0zpc88b">https://doi.org/10.5061/dryad.4j0zpc88b</ext-link>. The python scripts to reproduce the cross-temporal decoding analysis with subspace and ensembles is available online at <ext-link ext-link-type="uri" xlink:href="https://github.com/p-enel/stable-and-dynamic-value">https://github.com/p-enel/stable-and-dynamic-value</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://github.com/elifesciences-publications/stable-and-dynamic-value">https://github.com/elifesciences-publications/stable-and-dynamic-value</ext-link>).</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Enel</surname><given-names>P</given-names></name><name><surname>Wallis</surname><given-names>JD</given-names></name><name><surname>Rich</surname><given-names>EL</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Single- and multi-unit firing rate of two macaque monkeys' OFC and ACC neurons in value based decision-making task</data-title><source>Dryad Digital Repository</source><pub-id assigning-authority="Dryad" pub-id-type="doi">10.5061/dryad.4j0zpc88b</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amiez</surname> <given-names>C</given-names></name><name><surname>Joseph</surname> <given-names>JP</given-names></name><name><surname>Procyk</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reward encoding in the monkey anterior cingulate <italic>Cortex</italic></article-title><source>Cerebral Cortex</source><volume>16</volume><fpage>1040</fpage><lpage>1055</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhj046</pub-id><pub-id pub-id-type="pmid">16207931</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The hebbian paradigm reintegrated: local reverberations as internal representations</article-title><source>Behavioral and Brain Sciences</source><volume>18</volume><fpage>617</fpage><lpage>626</lpage><pub-id pub-id-type="doi">10.1017/S0140525X00040164</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaad</surname> <given-names>WF</given-names></name><name><surname>Eskandar</surname> <given-names>EN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A flexible software tool for temporally-precise behavioral control in Matlab</article-title><source>Journal of Neuroscience Methods</source><volume>174</volume><fpage>245</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1016/j.jneumeth.2008.07.014</pub-id><pub-id pub-id-type="pmid">18706928</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Astrand</surname> <given-names>E</given-names></name><name><surname>Enel</surname> <given-names>P</given-names></name><name><surname>Ibos</surname> <given-names>G</given-names></name><name><surname>Dominey</surname> <given-names>PF</given-names></name><name><surname>Baraduc</surname> <given-names>P</given-names></name><name><surname>Ben Hamed</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Comparison of classifiers for decoding sensory and cognitive information from prefrontal neuronal populations</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e86314</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0086314</pub-id><pub-id pub-id-type="pmid">24466019</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Backen</surname> <given-names>T</given-names></name><name><surname>Treue</surname> <given-names>S</given-names></name><name><surname>Martinez-Trujillo</surname> <given-names>JC</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Encoding of spatial attention by primate prefrontal cortex neuronal ensembles</article-title><source>eNeuro</source><volume>5</volume><elocation-id>ENEURO.0372-16.2017</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0372-16.2017</pub-id><pub-id pub-id-type="pmid">29568798</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neuronal population coding of parametric working memory</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>9424</fpage><lpage>9430</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1875-10.2010</pub-id><pub-id pub-id-type="pmid">20631171</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Sussillo</surname> <given-names>D</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Tsodyks</surname> <given-names>M</given-names></name><name><surname>Abbott</surname> <given-names>LF</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>From fixed points to Chaos: three models of delayed discrimination</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>214</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2013.02.002</pub-id><pub-id pub-id-type="pmid">23438479</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernacchia</surname> <given-names>A</given-names></name><name><surname>Seo</surname> <given-names>H</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A reservoir of time constants for memory traces in cortical neurons</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>366</fpage><lpage>372</lpage><pub-id pub-id-type="doi">10.1038/nn.2752</pub-id><pub-id pub-id-type="pmid">21317906</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>SE</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Kennerley</surname> <given-names>SW</given-names></name><name><surname>Hunt</surname> <given-names>LT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Autocorrelation structure at rest predicts value correlates of single neurons during reward-guided choice</article-title><source>eLife</source><volume>5</volume><elocation-id>e18937</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.18937</pub-id><pub-id pub-id-type="pmid">27705742</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanagh</surname> <given-names>SE</given-names></name><name><surname>Towers</surname> <given-names>JP</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name><name><surname>Hunt</surname> <given-names>LT</given-names></name><name><surname>Kennerley</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reconciling persistent and dynamic hypotheses of working memory coding in prefrontal cortex</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3498</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05873-3</pub-id><pub-id pub-id-type="pmid">30158519</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Compte</surname> <given-names>A</given-names></name><name><surname>Brunel</surname> <given-names>N</given-names></name><name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Synaptic mechanisms and network dynamics underlying spatial working memory in a cortical network model</article-title><source>Cerebral Cortex</source><volume>10</volume><fpage>910</fpage><lpage>923</lpage><pub-id pub-id-type="doi">10.1093/cercor/10.9.910</pub-id><pub-id pub-id-type="pmid">10982751</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Funahashi</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Murray</surname> <given-names>JD</given-names></name><name><surname>Qi</surname> <given-names>XL</given-names></name><name><surname>Wang</surname> <given-names>M</given-names></name><name><surname>Arnsten</surname> <given-names>AFT</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Persistent spiking activity underlies working memory</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7020</fpage><lpage>7028</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2486-17.2018</pub-id><pub-id pub-id-type="pmid">30089641</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Klingberg</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The neuroscience of working memory capacity and training</article-title><source>Nature Reviews Neuroscience</source><volume>17</volume><fpage>438</fpage><lpage>449</lpage><pub-id pub-id-type="doi">10.1038/nrn.2016.43</pub-id><pub-id pub-id-type="pmid">27225070</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cueva</surname> <given-names>CJ</given-names></name><name><surname>Marcos</surname> <given-names>E</given-names></name><name><surname>Saez</surname> <given-names>A</given-names></name><name><surname>Genovesio</surname> <given-names>A</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Salzman</surname> <given-names>CD</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Low dimensional dynamics for working memory and time encoding</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/504936</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dehaene</surname> <given-names>S</given-names></name><name><surname>Brannon</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><source>Space Time and Number in the Brain: Searching for the Foundations of Mathematical Thought</source><publisher-name>Academic Press</publisher-name><pub-id pub-id-type="doi">10.1016/C2010-0-66570-9</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Time cells in the Hippocampus: a new dimension for mapping memories</article-title><source>Nature Reviews Neuroscience</source><volume>15</volume><fpage>732</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1038/nrn3827</pub-id><pub-id pub-id-type="pmid">25269553</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enel</surname> <given-names>P</given-names></name><name><surname>Procyk</surname> <given-names>E</given-names></name><name><surname>Quilodran</surname> <given-names>R</given-names></name><name><surname>Dominey</surname> <given-names>PF</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reservoir computing properties of neural dynamics in prefrontal cortex</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1004967</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004967</pub-id><pub-id pub-id-type="pmid">27286251</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fujisawa</surname> <given-names>S</given-names></name><name><surname>Amarasingham</surname> <given-names>A</given-names></name><name><surname>Harrison</surname> <given-names>MT</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Behavior-dependent short-term assembly dynamics in the medial prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>823</fpage><lpage>833</lpage><pub-id pub-id-type="doi">10.1038/nn.2134</pub-id><pub-id pub-id-type="pmid">18516033</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funahashi</surname> <given-names>S</given-names></name><name><surname>Bruce</surname> <given-names>CJ</given-names></name><name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Mnemonic coding of visual space in the monkey's dorsolateral prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>61</volume><fpage>331</fpage><lpage>349</lpage><pub-id pub-id-type="doi">10.1152/jn.1989.61.2.331</pub-id><pub-id pub-id-type="pmid">2918358</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Funahashi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Space representation in the prefrontal cortex</article-title><source>Progress in Neurobiology</source><volume>103</volume><fpage>131</fpage><lpage>155</lpage><pub-id pub-id-type="doi">10.1016/j.pneurobio.2012.04.002</pub-id><pub-id pub-id-type="pmid">22521602</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fusi</surname> <given-names>S</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Rigotti</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Why neurons mix: high dimensionality for higher cognition</article-title><source>Current Opinion in Neurobiology</source><volume>37</volume><fpage>66</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2016.01.010</pub-id><pub-id pub-id-type="pmid">26851755</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuster</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1973">1973</year><article-title>Unit activity in prefrontal cortex during delayed-response performance: neuronal correlates of transient memory</article-title><source>Journal of Neurophysiology</source><volume>36</volume><fpage>61</fpage><lpage>78</lpage><pub-id pub-id-type="doi">10.1152/jn.1973.36.1.61</pub-id><pub-id pub-id-type="pmid">4196203</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman-Rakic</surname> <given-names>PS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Regional and cellular fractionation of working memory</article-title><source>PNAS</source><volume>93</volume><fpage>13473</fpage><lpage>13480</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.24.13473</pub-id><pub-id pub-id-type="pmid">8942959</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Coen</surname> <given-names>P</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Choice-specific sequences in parietal cortex during a virtual-navigation decision task</article-title><source>Nature</source><volume>484</volume><fpage>62</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1038/nature10918</pub-id><pub-id pub-id-type="pmid">22419153</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howard</surname> <given-names>JD</given-names></name><name><surname>Gottfried</surname> <given-names>JA</given-names></name><name><surname>Tobler</surname> <given-names>PN</given-names></name><name><surname>Kahnt</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Identity-specific coding of future rewards in the human orbitofrontal cortex</article-title><source>PNAS</source><volume>112</volume><fpage>5195</fpage><lpage>5200</lpage><pub-id pub-id-type="doi">10.1073/pnas.1503550112</pub-id><pub-id pub-id-type="pmid">25848032</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname> <given-names>SN</given-names></name><name><surname>Nieder</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Complementary roles for primate frontal and parietal cortex in guarding working memory from distractor stimuli</article-title><source>Neuron</source><volume>83</volume><fpage>226</fpage><lpage>237</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.05.009</pub-id><pub-id pub-id-type="pmid">24991963</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Jaeger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>The&quot; Echo State&quot; Approach to Analysing and Training Recurrent Neural Networks</source><publisher-name>Technical Report GMD Report 148, German National Research Center for Information Technology</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennerley</surname> <given-names>SW</given-names></name><name><surname>Dahmubed</surname> <given-names>AF</given-names></name><name><surname>Lara</surname> <given-names>AH</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Neurons in the frontal lobe encode the value of multiple decision variables</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1162</fpage><lpage>1178</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21100</pub-id><pub-id pub-id-type="pmid">18752411</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kennerley</surname> <given-names>SW</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Encoding of reward and space during a working memory task in the orbitofrontal cortex and anterior cingulate sulcus</article-title><source>Journal of Neurophysiology</source><volume>102</volume><fpage>3352</fpage><lpage>3364</lpage><pub-id pub-id-type="doi">10.1152/jn.00273.2009</pub-id><pub-id pub-id-type="pmid">19776363</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lara</surname> <given-names>AH</given-names></name><name><surname>Kennerley</surname> <given-names>SW</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Encoding of gustatory working memory by orbitofrontal neurons</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>765</fpage><lpage>774</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4637-08.2009</pub-id><pub-id pub-id-type="pmid">19158302</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lebedev</surname> <given-names>MA</given-names></name><name><surname>Messinger</surname> <given-names>A</given-names></name><name><surname>Kralik</surname> <given-names>JD</given-names></name><name><surname>Wise</surname> <given-names>SP</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Representation of attended versus remembered locations in prefrontal cortex</article-title><source>PLOS Biology</source><volume>2</volume><elocation-id>e365</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0020365</pub-id><pub-id pub-id-type="pmid">15510225</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundqvist</surname> <given-names>M</given-names></name><name><surname>Rose</surname> <given-names>J</given-names></name><name><surname>Herman</surname> <given-names>P</given-names></name><name><surname>Brincat</surname> <given-names>SL</given-names></name><name><surname>Buschman</surname> <given-names>TJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Gamma and beta bursts underlie working memory</article-title><source>Neuron</source><volume>90</volume><fpage>152</fpage><lpage>164</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.028</pub-id><pub-id pub-id-type="pmid">26996084</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lundqvist</surname> <given-names>M</given-names></name><name><surname>Herman</surname> <given-names>P</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Working memory: delay activity, yes! persistent activity? maybe not</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7013</fpage><lpage>7019</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2485-17.2018</pub-id><pub-id pub-id-type="pmid">30089640</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname> <given-names>W</given-names></name><name><surname>Natschläger</surname> <given-names>T</given-names></name><name><surname>Markram</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Real-time computing without stable states: a new framework for neural computation based on perturbations</article-title><source>Neural Computation</source><volume>14</volume><fpage>2531</fpage><lpage>2560</lpage><pub-id pub-id-type="doi">10.1162/089976602760407955</pub-id><pub-id pub-id-type="pmid">12433288</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maass</surname> <given-names>W</given-names></name><name><surname>Joshi</surname> <given-names>P</given-names></name><name><surname>Sontag</surname> <given-names>ED</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Computational aspects of feedback in neural circuits</article-title><source>PLOS Computational Biology</source><volume>3</volume><elocation-id>e165</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0020165</pub-id><pub-id pub-id-type="pmid">17238280</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDonald</surname> <given-names>CJ</given-names></name><name><surname>Lepage</surname> <given-names>KQ</given-names></name><name><surname>Eden</surname> <given-names>UT</given-names></name><name><surname>Eichenbaum</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Hippocampal &quot;time cells&quot; bridge the gap in memory for discontiguous events</article-title><source>Neuron</source><volume>71</volume><fpage>737</fpage><lpage>749</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.07.012</pub-id><pub-id pub-id-type="pmid">21867888</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Machens</surname> <given-names>CK</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Flexible control of mutual inhibition: a neural model of two-interval discrimination</article-title><source>Science</source><volume>307</volume><fpage>1121</fpage><lpage>1124</lpage><pub-id pub-id-type="doi">10.1126/science.1104171</pub-id><pub-id pub-id-type="pmid">15718474</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Masse</surname> <given-names>NY</given-names></name><name><surname>Yang</surname> <given-names>GR</given-names></name><name><surname>Song</surname> <given-names>HF</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Circuit mechanisms for the maintenance and manipulation of information in working memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1159</fpage><lpage>1167</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0414-3</pub-id><pub-id pub-id-type="pmid">31182866</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname> <given-names>EM</given-names></name><name><surname>Freedman</surname> <given-names>DJ</given-names></name><name><surname>Kreiman</surname> <given-names>G</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Poggio</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dynamic population coding of category information in inferior temporal and prefrontal cortex</article-title><source>Journal of Neurophysiology</source><volume>100</volume><fpage>1407</fpage><lpage>1419</lpage><pub-id pub-id-type="doi">10.1152/jn.90248.2008</pub-id><pub-id pub-id-type="pmid">18562555</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyers</surname> <given-names>EM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dynamic population coding and its relationship to working memory</article-title><source>Journal of Neurophysiology</source><volume>120</volume><fpage>2260</fpage><lpage>2268</lpage><pub-id pub-id-type="doi">10.1152/jn.00225.2018</pub-id><pub-id pub-id-type="pmid">30207866</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Miller</surname> <given-names>KJ</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Value representations in orbitofrontal cortex drive learning, but not choice</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/245720v1.abstract</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname> <given-names>EI</given-names></name><name><surname>Moser</surname> <given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A metric for space</article-title><source>Hippocampus</source><volume>18</volume><fpage>1142</fpage><lpage>1156</lpage><pub-id pub-id-type="doi">10.1002/hipo.20483</pub-id><pub-id pub-id-type="pmid">19021254</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname> <given-names>JD</given-names></name><name><surname>Bernacchia</surname> <given-names>A</given-names></name><name><surname>Roy</surname> <given-names>NA</given-names></name><name><surname>Constantinidis</surname> <given-names>C</given-names></name><name><surname>Romo</surname> <given-names>R</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable population coding for working memory coexists with heterogeneous neural dynamics in prefrontal cortex</article-title><source>PNAS</source><volume>114</volume><fpage>394</fpage><lpage>399</lpage><pub-id pub-id-type="doi">10.1073/pnas.1619449114</pub-id><pub-id pub-id-type="pmid">28028221</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieder</surname> <given-names>A</given-names></name><name><surname>Dehaene</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representation of number in the brain</article-title><source>Annual Review of Neuroscience</source><volume>32</volume><fpage>185</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.051508.135550</pub-id><pub-id pub-id-type="pmid">19400715</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Orhan</surname> <given-names>AE</given-names></name><name><surname>Ma</surname> <given-names>WJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A diverse range of factors affect the nature of neural representations underlying short-term memory</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>275</fpage><lpage>283</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0314-y</pub-id><pub-id pub-id-type="pmid">30664767</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neurobiology of economic choice: a good-based model</article-title><source>Annual Review of Neuroscience</source><volume>34</volume><fpage>333</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-061010-113648</pub-id><pub-id pub-id-type="pmid">21456961</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Padoa-Schioppa</surname> <given-names>C</given-names></name><name><surname>Assad</surname> <given-names>JA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neurons in the orbitofrontal cortex encode economic value</article-title><source>Nature</source><volume>441</volume><fpage>223</fpage><lpage>226</lpage><pub-id pub-id-type="doi">10.1038/nature04676</pub-id><pub-id pub-id-type="pmid">16633341</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parthasarathy</surname> <given-names>A</given-names></name><name><surname>Herikstad</surname> <given-names>R</given-names></name><name><surname>Bong</surname> <given-names>JH</given-names></name><name><surname>Medina</surname> <given-names>FS</given-names></name><name><surname>Libedinsky</surname> <given-names>C</given-names></name><name><surname>Yen</surname> <given-names>SC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mixed selectivity morphs population codes in prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1770</fpage><lpage>1779</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0003-2</pub-id><pub-id pub-id-type="pmid">29184197</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pascanu</surname> <given-names>R</given-names></name><name><surname>Jaeger</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A neurodynamical model for working memory</article-title><source>Neural Networks</source><volume>24</volume><fpage>199</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2010.10.003</pub-id><pub-id pub-id-type="pmid">21036537</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pastalkova</surname> <given-names>E</given-names></name><name><surname>Itskov</surname> <given-names>V</given-names></name><name><surname>Amarasingham</surname> <given-names>A</given-names></name><name><surname>Buzsáki</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Internally generated cell assembly sequences in the rat Hippocampus</article-title><source>Science</source><volume>321</volume><fpage>1322</fpage><lpage>1327</lpage><pub-id pub-id-type="doi">10.1126/science.1159775</pub-id><pub-id pub-id-type="pmid">18772431</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rainer</surname> <given-names>G</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Timecourse of object-related neural activity in the primate prefrontal cortex during a short-term memory task</article-title><source>European Journal of Neuroscience</source><volume>15</volume><fpage>1244</fpage><lpage>1254</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2002.01958.x</pub-id><pub-id pub-id-type="pmid">11982635</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajan</surname> <given-names>K</given-names></name><name><surname>Harvey</surname> <given-names>CD</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Recurrent network models of sequence generation and memory</article-title><source>Neuron</source><volume>90</volume><fpage>128</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.02.009</pub-id><pub-id pub-id-type="pmid">26971945</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname> <given-names>EL</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding subjective decisions from orbitofrontal cortex</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>973</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1038/nn.4320</pub-id><pub-id pub-id-type="pmid">27273768</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rich</surname> <given-names>EL</given-names></name><name><surname>Wallis</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Spatiotemporal dynamics of information encoding revealed in orbitofrontal high-gamma</article-title><source>Nature Communications</source><volume>8</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/s41467-017-01253-5</pub-id><pub-id pub-id-type="pmid">29074960</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rigotti</surname> <given-names>M</given-names></name><name><surname>Barak</surname> <given-names>O</given-names></name><name><surname>Warden</surname> <given-names>MR</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Fusi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The importance of mixed selectivity in complex cognitive tasks</article-title><source>Nature</source><volume>497</volume><fpage>585</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1038/nature12160</pub-id><pub-id pub-id-type="pmid">23685452</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riley</surname> <given-names>MR</given-names></name><name><surname>Constantinidis</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Role of prefrontal persistent activity in working memory</article-title><source>Frontiers in Systems Neuroscience</source><volume>9</volume><elocation-id>181</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2015.00181</pub-id><pub-id pub-id-type="pmid">26778980</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rushworth</surname> <given-names>MF</given-names></name><name><surname>Behrens</surname> <given-names>TE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Choice, uncertainty and value in prefrontal and cingulate cortex</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>389</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1038/nn2066</pub-id><pub-id pub-id-type="pmid">18368045</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname> <given-names>HS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>How the brain keeps the eyes still</article-title><source>PNAS</source><volume>93</volume><fpage>13339</fpage><lpage>13344</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.23.13339</pub-id><pub-id pub-id-type="pmid">8917592</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Spaak</surname> <given-names>E</given-names></name><name><surname>Watanabe</surname> <given-names>K</given-names></name><name><surname>Funahashi</surname> <given-names>S</given-names></name><name><surname>Stokes</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Stable and dynamic coding for working memory in primate prefrontal cortex</article-title><source>The Journal of Neuroscience</source><volume>37</volume><fpage>6503</fpage><lpage>6516</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3364-16.2017</pub-id><pub-id pub-id-type="pmid">28559375</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stalnaker</surname> <given-names>TA</given-names></name><name><surname>Cooch</surname> <given-names>NK</given-names></name><name><surname>McDannald</surname> <given-names>MA</given-names></name><name><surname>Liu</surname> <given-names>TL</given-names></name><name><surname>Wied</surname> <given-names>H</given-names></name><name><surname>Schoenbaum</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Orbitofrontal neurons infer the value and identity of predicted outcomes</article-title><source>Nature Communications</source><volume>5</volume><elocation-id>3926</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms4926</pub-id><pub-id pub-id-type="pmid">24894805</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>MG</given-names></name><name><surname>Kusunoki</surname> <given-names>M</given-names></name><name><surname>Sigala</surname> <given-names>N</given-names></name><name><surname>Nili</surname> <given-names>H</given-names></name><name><surname>Gaffan</surname> <given-names>D</given-names></name><name><surname>Duncan</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dynamic coding for cognitive control in prefrontal cortex</article-title><source>Neuron</source><volume>78</volume><fpage>364</fpage><lpage>375</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.01.039</pub-id><pub-id pub-id-type="pmid">23562541</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stokes</surname> <given-names>MG</given-names></name><name><surname>Buschman</surname> <given-names>TJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><collab>Chichester</collab></person-group><year iso-8601-date="2017">2017</year><chapter-title>Dynamic Coding for Flexible Cognitive Control</chapter-title><person-group person-group-type="editor"><name><surname>Egner</surname> <given-names>T</given-names></name></person-group><source>The Wiley Handbook of Cognitive Control</source><publisher-loc>United Kingdom</publisher-loc><publisher-name>John Wiley &amp; Sons, Ltd</publisher-name><fpage>221</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1002/9781118920497.ch13</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stoll</surname> <given-names>FM</given-names></name><name><surname>Fontanier</surname> <given-names>V</given-names></name><name><surname>Procyk</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Specific frontal neural dynamics contribute to decisions to check</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11990</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11990</pub-id><pub-id pub-id-type="pmid">27319361</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Vovk</surname> <given-names>V</given-names></name><name><surname>Wang</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Combining p-values via averaging</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1212.4966">https://arxiv.org/abs/1212.4966</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/S0166-2236(00)01868-3</pub-id><pub-id pub-id-type="pmid">11476885</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Warden</surname> <given-names>MR</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The representation of multiple objects in prefrontal neuronal delay activity</article-title><source>Cerebral Cortex</source><volume>17</volume><fpage>i41</fpage><lpage>i50</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm070</pub-id><pub-id pub-id-type="pmid">17726003</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wasmuht</surname> <given-names>DF</given-names></name><name><surname>Spaak</surname> <given-names>E</given-names></name><name><surname>Buschman</surname> <given-names>TJ</given-names></name><name><surname>Miller</surname> <given-names>EK</given-names></name><name><surname>Stokes</surname> <given-names>MG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Intrinsic neuronal dynamics predict distinct functional roles during working memory</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>3499</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-05961-4</pub-id><pub-id pub-id-type="pmid">30158572</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Watanabe</surname> <given-names>K</given-names></name><name><surname>Funahashi</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural mechanisms of dual-task interference and cognitive capacity limitation in the prefrontal cortex</article-title><source>Nature Neuroscience</source><volume>17</volume><fpage>601</fpage><lpage>611</lpage><pub-id pub-id-type="doi">10.1038/nn.3667</pub-id><pub-id pub-id-type="pmid">24584049</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wittmann</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The inner sense of time: how the brain creates a representation of duration</article-title><source>Nature Reviews Neuroscience</source><volume>14</volume><fpage>217</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1038/nrn3452</pub-id><pub-id pub-id-type="pmid">23403747</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Data sets and cross-validation.</title><p>In this figure, data matrices are represented with rectangles where rows are trials and columns units. Data from different sessions were pooled together into pseudo-populations by randomly selecting trials five times so that trial condition was matched (<bold>a</bold>) and formed five data sets. The rest of the figure represents the specific procedure used to optimize stable ensembles combined with the subspace method, which is the most involved nested cross-validation of the data and was applied independently on each of the five data sets. The outer 5-fold cross-validation (<bold>b</bold>) ensured that ensembles were not optimized and tested on the same data. The training data in each of the 5 folds of the outer cross-validation was used independently to optimize five stable ensembles (25 total, five data sets times five cross-validation folds). These data were further split following a 5-fold inner cross-validation to train and test a decoder on an ensemble (d) and used in parallel to optimize the <italic>L</italic><sub>2</sub> normalization parameter of the ridge regression (<bold>c</bold>), with its own 5-fold cross-validation. To avoid defining the stable subspace and training the ridge decoder on the same data, the training trials from the inner cross-validation were further split in 2, one half for the subspace and the other for the data that were projected into the subspace and then used to train the decoder with the <italic>L</italic><sub>2</sub> parameter optimized in step (<bold>c</bold>), and then this split was inversed to allow all data to be either part of the subspace or the training. Note that steps (<bold>c</bold>), (<bold>d</bold>) and (<bold>e</bold>) were repeated for each ensemble tested, from size n-1, n-2,... to a single unit. The ensemble eliciting the most stable representation for each fold of the outer cross-validation was tested on its corresponding test trials of the outer cross-validation. The cross-validation procedure for other analyses was similar but had fewer steps, for example the dynamic ensemble procedure did not involve the subspace/training data split.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-app1-fig1-v3.tif"/></fig></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54313.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names></name><role>Reviewing Editor</role><aff><institution>National Institute on Drug Abuse, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Hunt</surname><given-names>Laurence Tudor</given-names></name><role>Reviewer</role><aff><institution>Oxford University</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Schoenbaum</surname><given-names>Geoffrey</given-names> </name><role>Reviewer</role><aff><institution>National Institute on Drug Abuse, National Institutes of Health</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>In this study, Rich and colleagues recorded neural activity from the orbitofrontal and anterior cingulate cortex of monkeys during a delayed response task in which cues were presented that signaled different value primary or secondary reward. Their goal was to explore the representation of reward value during working memory performance to contrast predictions of proposals regarding whether information is maintained in a stable or dynamic manner. Using a novel approach of constructing ensembles only of neurons tailored to one or the other criteria, they argue persuasively that value information is available to downstream areas from the output of both regions at both timescales. The results are convincing and represent an important new perspective on this important question specifically and more generally on how to think about what information is represented in neural activity.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Stable and dynamic representations of value in the prefrontal cortex&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, including Geoffrey Schoenbaum as the Reviewing Editor and Reviewer #3, and the evaluation has been overseen Kate Wassum as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Laurence Tudor Hunt (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>Summary:</p><p>In this study, Rich and colleagues recorded neural activity from the OFC and ACC of monkeys during a delayed response task in which cues were presented that signaled different value primary or secondary reward. Their goal was to explore the representation of reward value during working memory performance to contrast predictions of proposals regarding whether information is maintained in a stable or dynamic manner. Using a novel approach of constructing ensembles only of neurons tailored to one or the other criteria, they argue persuasively that value information is available to downstream areas from the output of both regions at both timescales. The results are convincing and represent an important new perspective on this important question specifically and more generally on how to think about what information is represented in neural activity.</p><p>Essential revisions:</p><p>The reviewers were each positive about the research and the paper, in the reviews and in the discussion. In discussion, there were four essential revisions noted below that are most important.</p><p>– The first is to clarify why decoding in Figure 5 is high prior to cue onset in some panels. This was commented on by two of the reviewers and must be addressed.</p><p>– The second is the issue of how neurons were selected and the issues of cross-validation and circularity raised by these same reviewers.</p><p>– Related to this is a third issue, which is whether or not neurons were included as value encoding and in the subsequent ensemble analyses if they had categorical or non-linear value correlates. The authors mention this was true, and on discussion it was felt that it was important to clarify this and ideally to show that the results either do or do not require their inclusion with more traditional linear correlates.</p><p>– And finally, fourth, all reviewers agreed that a parallel analysis considering type would be of interest. It is not critical what it shows, but it was felt this analysis could be done and might affect how some interpret these results.</p><p>Further details on these points can be found in the full reviews.</p><p><italic>Reviewer #1:</italic></p><p>This study tests how single and multi-unit recordings from ACC and OFC of non-human primates encode value information across delays. The authors find that both value and reward type information can be decoded from activity in both areas. In addition, subspace analyses for value information show that encoding is both stable and dynamic across time.</p><p>The nature of encoding across delays is a timely topic. Here the authors consider both the stability and the dynamics of this encoding over time. This is innovative and I think the results tell us a lot about the nature of neural coding in OFC/ACC. I have a few concerns that should be addressed.</p><p>1) I think it is a missed opportunity that the more interesting analyses are only applied to value but not reward type. Encoding of reward type has received considerable attention recently (e.g., Stalnaker et al., 2014; Howard et al., 2015), and here, decoding accuracy for value and type was nominally comparable, so it is unclear why this wasn't done here. I'd strongly encourage the authors to apply the analyses focusing on stability and dynamics of encoding to reward type as well.</p><p>2) I do not understand how it is possible that, as shown in Figure 5B (lower panel), the decoder trained on data from 500 ms before cue onset (that is before neurons encode any value) can decode value throughout the rest of the trial.</p><p>3) Related to the point above, it appears there are at least two potential sources of non-independence errors in the decoding analyses.</p><p>3.1) First, it appears the L2 parameter was selected based on all data and that &quot;the parameter eliciting the best decoding for each condition was used for all decoding results presented here&quot;. As described this analysis is circular and will inflate decoding accuracies. Instead the authors should use nested cross-validation, where the parameter is optimized (using cross-validation) within the training data. (That is, the data is split into 3 parts. Different classifiers with different levels of L2 are trained on part 1. These classifiers are then evaluated on part 2 and accuracies are computed. The value of L2 that results in the best accuracy (determined in part 2) is then used to train a classifier based on part 1 and 2, and the final accuracy is computed based on part 3. No parameter decisions are ever based on accuracy that is determined using part 3)</p><p>3.2) The second instance of non-independence may occur during the ensembles selection approach. This is not fully explained, but it appears that the ensemble selection was not based on nested cross-validation either. Again, a nested cross validation should be used such that the ensembles are selected within the training data using cross-validation, and the final accuracy matrices should be computed based on data that was never used in the selection procedure.</p><p><italic>Reviewer #2:</italic></p><p>This study provides further insight into the recent debate around 'stable' versus 'dynamic' representations of value in prefrontal cortex. In short, the authors find evidence supporting both types of value representation. The authors use a range of methods to look at this, but perhaps the most convincing analyses are shown in Figure 5, where different subsamples of the same neural data (different 'ensembles') can be used to create cross-temporal decoding patterns that either appear 'stable' or 'dynamic', with the original data containing a mixture of the two.</p><p>Overall I found the paper to provide good support for a hybrid of stable and dynamic coding schemes, and the data to be well analysed. However, there were several points in the paper that I thought could do with a bit more attention.</p><p>1) Figure 2A vs. 2C Why are the % of encoding neurons broadly similar between ACC and OFC, but the decoding accuracy different between the two regions shortly after cue onset? This is a bit of an outstanding question that the authors don't really address.</p><p>2) In Figure 3, the authors do the right thing of splitting their data into training/test data when sorting by peak latency. (Otherwise, the tiling can easily arise from random noise, e.g. https://twitter.com/steinmetzneuro/status/1150971584812920832). It is, however, equally important that they do this in Figure 4.</p><p>3) &quot;We considered value as a categorical variable, as some units activated in a non-linear fashion, for example by responding only to a specific value and could not be modeled with a simple linear regression&quot; – isn't it possible that these neurons are encoding picture identity rather than value? In order to show that there is non-linear coding of value, then presumably the prediction would be that this non-linear value coding would generalise across the primary and secondary reinforcers? This could be easily shown for Supplementary Figure 2 – the lines could be split into primary and secondary reinforcer, with the 'non-linear value' prediction meaning that these two lines should overlap.</p><p>4) Figure 5 – it looks like the ACC cross-temporal decoding in the stable subspace goes back in time to before the stimulus onset (i.e. the classifier trained from timepoints -500ms to 0ms pre-cue has above-chance decoding accuracy). Is there a reason for this?</p><p>5) I couldn't understand how the 'trajectory speed' in Figure 6A is calculated – is the 'distance' in neural space being computed on raw firing rates, or on classification accuracies? Some further detail on this would be helpful.</p><p>6) It would also be interesting to know whether the regression shown in Figure 6B is less successful in the stable ensemble than in the dynamic ensemble?</p><p>7) &quot;While value representations have been found in the activity of ACC and OFC e.g. [Rushworth and Behrens, 2008], this is, to our knowledge, the first study of their dynamics during a short delay.&quot; – It might be relevant to cite Cavanagh et al., 2016 here? That study used cross-temporal correlation to study how value coding persisted during the delay between the choice and outcome periods, albeit in the context of different subpopulations with different resting autocorrelations (see their Figure 5).</p><p><italic>Reviewer #3:</italic></p><p>In this study, Rich and colleagues recorded neural activity from the OFC and ACC of monkeys during a delayed response task in which cues were presented that signaled different value primary or secondary reward. Their goal was to explore the representation of reward value during working memory performance – across a delay period in the current task – to contrast predictions of proposals regarding how information regarding expected value would be maintained – whether it would be stable or dynamic. Using a novel approach of hand-picking neurons to compose ensembles tailored to one or the other criteria, they argue persuasively that value information is available to downstream areas at both timescales in both of these two prefrontal areas. The results are convincing I think and represent an important new perspective on this important question specifically and more generally on how to think about what information is represented in neural activity. So I am very positive generally about this paper. That said, I do have some things I would really like to see done.</p><p>1) I did not see any behavior. I think it is important to show the behavior in this paper in order to demonstrate that the monkeys do treat the cues as if they have these scalar values and further to show that they treat the cues predicting putatively similar amounts of primary and secondary reward similarly. The latter may be particularly important wrt to my second point below. This should be shown using choice behavior on the probe trials, but I also think it should be evident in reaction time or some other measure during the forced choice trials that comprise the neural data set.</p><p>2) I found the classification that something is encoding value to be confusing. The authors say that value was treated as a categorical variable since some neurons had idiosyncratic responses to cues not at the ends of the value ranges. Yet it seems to me that this begs the question of whether these neurons are signaling value. At a minimum, to count as a value neuron, a cell should show the same response across both trial types that have the same putative value. So a value neuron must either have a linear value correlate or it would need to fall into this special category… and even then, I think calling such a cell a &quot;value cell&quot; stretches the conventional definition of value. For instance, if a neuron fired the most to 2 and then significantly less to 1, 3, and 4, I would have trouble seeing this as a value cell by most definitions of value, even if it fired the same to 2 on both trial types. Given this, in my opinion, analyses such as those in Figures 2 and 5 that are the heart of the paper need to be redone without such &quot;non-linear&quot; value neurons as part of the populations. If the results only hold with those neurons included, then I think the interpretation of what is being represented statically and dynamically might be significantly affected for me.</p><p>3) Looking at Figure 2C and D, I do not understand the statement at the end of the subsection “Encoding and decoding of value in unit activity”, that because reward type is poorly encoded, the paper focuses on value. I think it is ok to focus on value because that is the question of interest. But this statement misrepresents the reason it seems to me. And really I'd like to see both explored either separately or as part of the same analysis. Both information is relevant to the broader question I think; primary versus secondary is as interesting a part of what is being maintained as simple value. In fact I'd say it is arguably more important, since it is important to me to be paid in dollars and not equivalent piles of marshmallows for example!?!</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.54313.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>The reviewers were each positive about the research and the paper, in the reviews and in the discussion. In discussion, there were four essential revisions noted below that are most important.</p><p>– The first is to clarify why decoding in Figure 5 is high prior to cue onset in some panels. This was commented on by two of the reviewers and must be addressed.</p></disp-quote><p>We thank the reviewers for identifying this problem. After some investigation, we uncovered several contributors to this result, which we describe below, all of which have been addressed in the revised manuscript. Given that this issue turned out to arise from some unexpected consequences of the analytic approaches, we include details of our revised methods in the manuscript along with a supplementary figure to inform the field.</p><p>The first problem we identified was that the decoder was trained with the same data that was used to define the value subspace, making it possible for trial-wise noise to bias the pre-cue activity once it was projected onto the subspace. To remedy this issue, we divided the training data in two halves; the first was used to define the subspace and the other to train the decoder. We then repeated the process using the first half for the decoder and the second half for the subspace so that all data was used in turn for subspace and training. We averaged across the two accuracy matrices to obtain the final result. Updated Materials and methods section:</p><p>“When decoding with the subspace, we used a nested cross-validation approach to ensure that the subspace was derived with data independent from the training and testing of the decoder to avoid inflated accuracy due to trial-wise noise (Appendix 1—figure 1E). […] As a result, this procedure elicited two matrices of accuracy for each fold of the inner cross-validation (Appendix 1—figure 1D) and were averaged.”</p><p>Because of this sensitivity to bias, we also cross-validated the selection of trials that were part of the analysis. That is, the neural population studied here is composed of several sessions and subjects pooled together, and trials were randomly selected from these to arrive at the same number of trials per condition. To create a non-biased decoding estimate, we averaged the accuracy matrices of 5 random selections of trials. Updated Materials and methods section:</p><p>“Neurons across sessions and monkeys were pooled into a single population for each of OFC and ACC. […] The results of every population analysis presented in this paper is the average of the 5 randomly generated data sets.”</p><p>Finally, we adapted permutation testing to the subspace method. We noticed that small fluctuations in the activity before the cue had the potential to change decoding accuracy when the decoder was trained on these data and tested after the cue. This indicated that the decoder was simply parsing normal variance in the neural activity, which should be addressed by appropriate permutation tests. To address this in the cross-temporal decoding with the subspace method, instead of shuffling all the trials, the subspace and testing trials were kept as is and only the training trials were shuffled. This method successfully reproduced the divergence from chance accuracy with the original data, and thus prevents spurious decoding before the cue to reach significance. Corresponding updated Materials and methods:</p><p>“Permutation testing with the subspace shuffled only the labels of the training trials to more closely reproduce the propensity of the subspace trained with original data to elicit higher than chance accuracy.”</p><p>The reviewers will still notice significant decoding a few ms before the cue in the cross-temporal decoding, but this is now merely due to smoothing windows, and is absent from bins that contain only pre-cue data. We kept the original longer smoothing windows, as these are more efficient for decoding in our experience, and elicit smoother accuracies making interpretations easier for the timescale of our investigations. This point is mentioned in the legend of the cross-temporal decoding figure:</p><p>“[…]. Note that significant decoding before the presentation of the cue is due to smoothing.”</p><p>The splitting of the data is illustrated in a new figure (Appendix 1—figure 1):</p><p>“Data sets and cross-validation. In this figure, data matrices are represented with rectangles where rows are trials and columns units. […] The cross-validation procedure for other analyses was similar but had fewer steps, e.g. dynamic ensemble procedure did not involve the subspace/training data split.”</p><disp-quote content-type="editor-comment"><p>– The second is the issue of how neurons were selected and the issues of cross-validation and circularity raised by these same reviewers.</p></disp-quote><p>Thank you for bringing this issue to our attention. We now select neurons to be part of an ensemble with a cross-validation method, so the final accuracy now reflects the average of a 5-fold cross-validation that resulted in 5 different ensembles and their corresponding accuracies. The final cross-temporal decoding method now includes several levels of nested cross-validation as described in the updated Materials and methods section:</p><p>“For both stable and dynamic ensembles, the iterative ensemble optimization procedure involved a nested cross-validation procedure to ensure the independence of the testing and ensemble optimization trials (Appendix 1—figure 1D). As a consequence, 5 ensemble searches were performed for each tested population and their corresponding testing accuracy were averaged.”</p><disp-quote content-type="editor-comment"><p>– Related to this is a third issue, which is whether or not neurons were included as value encoding and in the subsequent ensemble analyses if they had categorical or non-linear value correlates. The authors mention this was true, and on discussion it was felt that it was important to clarify this and ideally to show that the results either do or do not require their inclusion with more traditional linear correlates.</p></disp-quote><p>Indeed, our encoding and decoding methods consider value as a categorical variable and we recognize that this choice was not discussed in detail in the manuscript. We also agree that it is a departure from the traditional perspective that considers value coding to be a monotonic relationship between activities in single units and reward quantities, and now discuss this in a bit more detail by including the following text:</p><p>“We considered value as a categorical variable, as some units activated in a non-linear fashion with respect to value, for example by responding only to a specific value, and could not be modeled with a simple linear regression (Figure 2—figure supplement 2). […] Approximately 13% of units in both OFC and ACC fit these criteria.”</p><p>(Also relevant to this discussion is the response to reviewer 2’s comment below on distinguishing non-linear responses to value, versus responses to an individual picture.)</p><p>The larger issue raised by this comment, however, is whether the results that we report depend on the inclusion of non-linear coding neurons. To address this, we evaluated all neurons with a simple linear regression that included value, type and their interaction as independent variables and re-processed all the data with only linear neurons, which we defined as those that were significant for the value regressor for at least 7 bins in a row (~175ms) at any time from the onset of cue presentation to the onset of reward delivery. Note that a neuron that is classified as linear because of a significant value coefficient in a linear regression model at one time point, might not be significant at another time point with this test but will be with an ANOVA instead, so it is particularly difficult to cleanly parse populations of linear vs. non-linear neurons. However, this approach was designed to capture neurons with traditional monotonic value coding with methods typically used in the literature. Overall, all results were qualitatively similar when analyses were performed with only linear neurons defined in this fashion (see Author response images 1-3). This confirms that our results are not dependent on the presence of non-linear value coding neurons, and leaves unchanged our main interpretations.</p><fig id="sa2fig1"><label>Author response image 1.</label><caption><title>Figure 2, linear value neurons only.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-resp-fig1-v3.tif"/></fig><fig id="sa2fig2"><label>Author response image 2.</label><caption><title>Figures 3 and 4, linear value neurons only.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-resp-fig2-v3.tif"/></fig><fig id="sa2fig3"><label>Author response image 3.</label><caption><title>Figure 5, linear value neurons only.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-resp-fig3-v3.tif"/></fig><disp-quote content-type="editor-comment"><p>– And finally, fourth, all reviewers agreed that a parallel analysis considering type would be of interest. It is not critical what it shows, but it was felt this analysis could be done and might affect how some interpret these results.</p></disp-quote><p>All the main analyses (sequential and persistent encoding, and cross-temporal decoding) are now also done on “type” variable and are included in the paper: Figure 2 was revised (panel F was added), and Figures 3—figure supplement 1, Figure 4—figure supplement 1 and Figure 5—figure supplement 1 was added.</p><disp-quote content-type="editor-comment"><p>Further details on these points can be found in the full reviews.</p><p>Reviewer #1:</p><p>[…] The nature of encoding across delays is a timely topic. Here the authors consider both the stability and the dynamics of this encoding over time. This is innovative and I think the results tell us a lot about the nature of neural coding in OFC/ACC. I have a few concerns that should be addressed.</p><p>1) I think it is a missed opportunity that the more interesting analyses are only applied to value but not reward type. Encoding of reward type has received considerable attention recently (e.g., Stalnaker et al., 2014; Howard et al., 2015), and here, decoding accuracy for value and type was nominally comparable, so it is unclear why this wasn't done here. I'd strongly encourage the authors to apply the analyses focusing on stability and dynamics of encoding to reward type as well.</p></disp-quote><p>As noted in the combined comments section, we have now included analyses for reward type. In addition, we now include the following notes discussing the encoding of reward type:</p><p>“While value signals were robust in both OFC and ACC, consistent with previous reports of strong abstract value coding in these regions (Padoa-Schioppa, 2006, Rushworth, 2008), there were also weaker representations of the anticipated reward type. This is consistent with findings in human and rodent OFC reporting value signals that are specific to a particular type of outcome (Howard, 2015, Stalnaker, 2014).”</p><disp-quote content-type="editor-comment"><p>2) I do not understand how it is possible that, as shown in Figure 5B (lower panel), the decoder trained on data from 500 ms before cue onset (that is before neurons encode any value) can decode value throughout the rest of the trial.</p></disp-quote><p>Answered above in the combined comments section.</p><disp-quote content-type="editor-comment"><p>3) Related to the point above, it appears there are at least two potential sources of non-independence errors in the decoding analyses.</p><p>3.1) First, it appears the L2 parameter was selected based on all data and that &quot;the parameter eliciting the best decoding for each condition was used for all decoding results presented here&quot;. As described this analysis is circular and will inflate decoding accuracies. Instead the authors should use nested cross-validation, where the parameter is optimized (using cross-validation) within the training data. (That is, the data is split into 3 parts. Different classifiers with different levels of L2 are trained on part 1. These classifiers are then evaluated on part 2 and accuracies are computed. The value of L2 that results in the best accuracy (determined in part 2) is then used to train a classifier based on part 1 and 2, and the final accuracy is computed based on part 3. No parameter decisions are ever based on accuracy that is determined using part 3)</p></disp-quote><p>Indeed, the regularization parameter was not optimized in a fully independent way and we have now corrected this. The L2 parameter is now optimized on the training data only with a nested cross-validation. For ensemble optimization, this parameter was not optimized independently for each possible ensemble n that was explored but for the ensemble n+1 that was previously selected in the iterative search to limit the computations as this parameter search is computationally expensive, but because the n and n+1 datasets were almost identical, the potential loss in accuracy due to a suboptimal L2 parameter selection should be minimal. The Materials and methods section was updated as well to reflect this change:</p><p>“The regularization parameter was optimized with a nested cross-validation within the outer 5-fold cross-validation mentioned above by exploring every power of ten between -2 and 5 with a 0.5 step with the training data for each population or ensemble to maximize the average performance of the classifier trained and tested on each time point between cue onset and reward (Appendix 1—figure 1C).”</p><disp-quote content-type="editor-comment"><p>3.2) The second instance of non-independence may occur during the ensembles selection approach. This is not fully explained, but it appears that the ensemble selection was not based on nested cross-validation either. Again, a nested cross validation should be used such that the ensembles are selected within the training data using cross-validation, and the final accuracy matrices should be computed based on data that was never used in the selection procedure.</p></disp-quote><p>Answered above in the combined comments section.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>[…] Overall I found the paper to provide good support for a hybrid of stable and dynamic coding schemes, and the data to be well analysed. However, there were several points in the paper that I thought could do with a bit more attention.</p><p>1) Figure 2A vs. C Why are the % of encoding neurons broadly similar between ACC and OFC, but the decoding accuracy different between the two regions shortly after cue onset? This is a bit of an outstanding question that the authors don't really address.</p></disp-quote><p>This is indeed an interesting point that was not fully explored in the paper. Now that decoding is performed on 5 different data sets and the resulting accuracies averaged, the difference in decoding accuracy between ACC and OFC is smaller yet still observable. This difference is partly explained by a higher number of units in the OFC population, because subsets of OFC units matched to the number of ACC units elicit lower accuracy, as shown in Figure 2C and D. However, there is still a gap in accuracy even after accounting for the difference in population size, and this is likely due to the fact that the two analyses rest on different measures – either the proportion of neurons that meet criteria for significantly encoding value, or the quality of this value signal in the population. The difference between these measures suggests that there is a higher fidelity value signal in OFC at the population level, despite slightly more neurons in ACC meeting our encoding criteria.</p><p>To support this perspective, we looked closer at the encoding p-values in each region. While the distributions are indistinguishable (p-values of encoding value from ANOVA at peak encoding [325ms after cue], distributions compared with Kruskall-Wallis test [K=.62, pval=.43]), sensitivity indices (also known as d’) are higher on average in OFC than ACC (average across neurons and values at peak encoding: OFC=0.26, ACC=0.22). This supports the view that the same proportion of neurons encode value after the cue presentation, but the way value is encoded has an influence on the decoder. Thus, in this case, a single value obtained from a linear model, whether it is a linear regression or an ANOVA, has limited ability to describe the encoding of complex information like value.</p><p>A shortened version of this discussion has been added to the Results:</p><p>“Both regions had similar proportions of value encoding after the cue presentation, however decoding accuracy was more than 10% higher in OFC, which can be partly explained by a higher unit count in the OFC population. […] The sensitivity index averaged across pairs of values in all neurons was higher in OFC (0.26) than in ACC (0.22), suggesting that values might be easier to separate in OFC activity compared to ACC, leading to higher decoding accuracy.”</p><disp-quote content-type="editor-comment"><p>2) In Figure 3, the authors do the right thing of splitting their data into training/test data when sorting by peak latency. (Otherwise, the tiling can easily arise from random noise, e.g. https://twitter.com/steinmetzneuro/status/1150971584812920832). It is, however, equally important that they do this in Figure 4.</p></disp-quote><p>Thank you for pointing out this issue, we applied the cross-validation method to the encoding duration of single neuron and found that encoding duration is a robust feature as well. It is now reflected in the encoding durations in Figure 4 and the main text in the Results section:</p><p><italic>“</italic>However, sorting the neurons by the proportion of the delay during which they encode value offers a different picture. Figure 4 shows the same splitting procedure applied to encoding duration (Figure 4—figure supplement 1 for variable type).”</p><p>And in the Materials and methods section:</p><p>“For both the sequential encoding and encoding duration analyses, trials were randomly split in half and the above encoding analysis was applied independently on each half on the delay activity, i.e. from reward predicting cue offset to reward delivery.”</p><disp-quote content-type="editor-comment"><p>3) &quot;We considered value as a categorical variable, as some units activated in a non-linear fashion, for example by responding only to a specific value and could not be modeled with a simple linear regression&quot; – isn't it possible that these neurons are encoding picture identity rather than value? In order to show that there is non-linear coding of value, then presumably the prediction would be that this non-linear value coding would generalise across the primary and secondary reinforcers? This could be easily shown for Supplementary Figure 2 – the lines could be split into primary and secondary reinforcer, with the 'non-linear value' prediction meaning that these two lines should overlap.</p></disp-quote><p>As noted by the reviewer, this task is well constructed to determine whether non-linear neurons simply respond to one picture from the set, potentially encoding picture identity, or whether they respond to (e.g.) two pictures corresponding to one of the four possible values and therefore are code value in a non-linear fashion, regardless of picture or reward type. To address this, we followed the reviewer’s suggestion and Figure 2—figure supplement 2 now shows examples of non-linear neurons with activities separated by reward type showing non-linear value encoding that is similar for primary and secondary rewards. To explore this question quantitatively, we focused on activity at the peak of encoding (325 ms after cue presentation) and defined a unit as non-linear value coding if it met these 3 criteria:</p><p>– the unit <italic>was not</italic> significant for value with a linear regression described above in the response to combined comments (activity ~ value + type + value x type)</p><p>– the unit <italic>was</italic> significant for value with an ANOVA (activity ~ C(value) + type + value x type)</p><p>– the unit <italic>was not</italic> significant for the interaction of value and type with an ANOVA (activity ~ C(value) + type + value x type)</p><p>With this method we found that approximately <italic>13% of neurons</italic> in both OFC and ACC were non-linearly encoding value. Unfortunately, since there is only a single cue per value/type combination, it is impossible to further parse neurons that show an interaction in the above analyses into responses that are truly selective for picture identity versus those that encode the value of a specific outcome (i.e. the interaction of value and type), however we found with visual inspection of firing rate profiles that potential picture neurons encoding a single combination of value and type were very rare. These results have been added to the manuscript in the text identified in the combined comments section above.</p><disp-quote content-type="editor-comment"><p>4) Figure 5 – it looks like the ACC cross-temporal decoding in the stable subspace goes back in time to before the stimulus onset (i.e. the classifier trained from timepoints -500ms to 0ms pre-cue has above-chance decoding accuracy). Is there a reason for this?</p></disp-quote><p>Answered above in the combined comments section.</p><disp-quote content-type="editor-comment"><p>5) I couldn't understand how the 'trajectory speed' in Figure 6A is calculated – is the 'distance' in neural space being computed on raw firing rates, or on classification accuracies? Some further detail on this would be helpful.</p></disp-quote><p>The trajectory speed was calculated from the firing rate of individual units estimated with the same method used for the rest of the paper, albeit with a smaller smoothing window for more temporally accurate data. This was previously only explained in the legend of Figure 6, so to improve clarity we have now also indicated this in the main text as follows:</p><disp-quote content-type="editor-comment"><p>“This dynamic pattern was also reflected in the population trajectory speed obtained by calculating the distance between two successive time bins in the neural space spanned by the estimated firing rate of each unit with a shorter smoothing window for more temporally accurate firing rate (50 ms SD Gaussian kernel […]”</p><p>6) It would also be interesting to know whether the regression shown in Figure 6B is less successful in the stable ensemble than in the dynamic ensemble?</p></disp-quote><p>Figure 6B showed a regression of time bins against the predicted time bin based on neural activity in the dynamic ensemble. The reviewer’s suggestion is that this should better predict time than the stable ensemble, which we found was generally the case (<xref ref-type="fig" rid="sa2fig4">Author response image 4</xref>). The sum of squared errors was higher for the stable ensembles than for the dynamic ensemble, however the original population still predicted time better than either of these two ensembles (SSE: original population = 9018, dynamic ensemble = 26851, stable ensemble = 36309), suggesting the presence of additional temporal information that is not present in either reduced ensemble.</p><fig id="sa2fig4"><label>Author response image 4.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-54313-resp-fig4-v3.tif"/></fig><disp-quote content-type="editor-comment"><p>7) &quot;While value representations have been found in the activity of ACC and OFC e.g. [Rushworth and Behrens, 2008], this is, to our knowledge, the first study of their dynamics during a short delay.&quot; – It might be relevant to cite Cavanagh et al., 2016 here? That study used cross-temporal correlation to study how value coding persisted during the delay between the choice and outcome periods, albeit in the context of different subpopulations with different resting autocorrelations (see their Figure 5).</p></disp-quote><p>Thank you for this reference, this reference has been added to the Discussion in two sections:</p><p>“In this study, we shed light on the complex dynamics of value representations in prefrontal cortex. While value representations have been found in the activity of ACC and OFC e.g. [Rushworth, 2008a], little is known about their dynamics across delays. […] Here, we expand on this to show that targeted methods elicit seemingly opposite results…”</p><p>“These results present the coding of value in two extreme dynamical regimes, either fully stable or fully dynamic. […] Indeed, it has been shown that the time constant of neurons encoding value in prefrontal region is diverse [Cavanagh et al., 2016], supporting the hypothesis of multitude of dynamical regimes.”</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>[…] 1) I did not see any behavior. I think it is important to show the behavior in this paper in order to demonstrate that the monkeys do treat the cues as if they have these scalar values and further to show that they treat the cues predicting putatively similar amounts of primary and secondary reward similarly. The latter may be particularly important wrt to my second point below. This should be shown using choice behavior on the probe trials, but I also think it should be evident in reaction time or some other measure during the forced choice trials that comprise the neural data set.</p></disp-quote><p>In the previous manuscript, behavioral results were only briefly mentioned, and the reader was referred to a previous publication for further description. To enhance the current presentation, Figure 1 has now been extended (Figure 1C) to show the preference of the monkeys for higher valued cues with little influence of the type of reward. These results were quantified with a logistic regression model on choice trials, in which the monkeys chose between two cues. In addition, a linear regression model predicting the reaction time of monkeys in the joystick task shows a significant effect of value but not type. The following text has been added:</p><p>Figure 1C legend: “Probability of choosing a juice option in choice trials for every pair of cues in which one predicts a juice reward and the other a bar reward.”</p><p>“A logistic regression model predicting the cues chosen by the animals with variables value and type shows a much lower coefficient for type, and its p-values are many order of magnitude larger, confirming that the type of reward had little influence on the choice of animals compared to value (Figure 1C, coefficients with [p-value of t-test] for monkeys M and N respectively; type left/right:.41 [2.53e-3] / -.51 [1.75e-4] and -.61 [2.6e-3] /.32 [0.11], value left/right: 1.77 [6.1e-117] / -1.95 [3.02e-136] and 2.66 [1.59e-94] / -2.46 [2.27e-90]).”</p><p>“Reaction times were also unaffected by the type of reward anticipated (linear regression predicting reaction time as a function of value and type in single cue trials : coefficients for value [p-value of t-test] for monkeys M and N respectively; type: -21 [.08] and 14.5 [.32], value: -63.9 [1.73e-31] and -65.7 [2.66e-23]).”</p><disp-quote content-type="editor-comment"><p>2) I found the classification that something is encoding value to be confusing. The authors say that value was treated as a categorical variable since some neurons had idiosyncratic responses to cues not at the ends of the value ranges. Yet it seems to me that this begs the question of whether these neurons are signaling value. At a minimum, to count as a value neuron, a cell should show the same response across both trial types that have the same putative value. So a value neuron must either have a linear value correlate or it would need to fall into this special category… and even then, I think calling such a cell a &quot;value cell&quot; stretches the conventional definition of value. For instance, if a neuron fired the most to 2 and then significantly less to 1, 3, and 4, I would have trouble seeing this as a value cell by most definitions of value, even if it fired the same to 2 on both trial types. Given this, in my opinion, analyses such as those in Figure 2 and 5 that are the heart of the paper need to be redone without such &quot;non-linear&quot; value neurons as part of the populations. If the results only hold with those neurons included, then I think the interpretation of what is being represented statically and dynamically might be significantly affected for me.</p></disp-quote><p>Answered above in the combined comments section.</p><disp-quote content-type="editor-comment"><p>3) Looking at Figure 2C and D, I do not understand the statement at the end of the subsection “Encoding and decoding of value in unit activity”, that because reward type is poorly encoded, the paper focuses on value. I think it is ok to focus on value because that is the question of interest. But this statement misrepresents the reason it seems to me. And really I'd like to see both explored either separately or as part of the same analysis. Both information is relevant to the broader question I think; primary versus secondary is as interesting a part of what is being maintained as simple value. In fact I'd say it is arguably more important, since it is important to me to be paid in dollars and not equivalent piles of marshmallows for example!?!</p></disp-quote><p>Answered above in the combined comments section.</p></body></sub-article></article>