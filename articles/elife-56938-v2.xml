<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">56938</article-id><article-id pub-id-type="doi">10.7554/eLife.56938</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>A flexible framework for simulating and fitting generalized drift-diffusion models</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-158221"><name><surname>Shinn</surname><given-names>Maxwell</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7424-4230</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-166020"><name><surname>Lam</surname><given-names>Norman H</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5817-6680</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><xref ref-type="fn" rid="pa1">‡</xref></contrib><contrib contrib-type="author" corresp="yes" id="author-24355"><name><surname>Murray</surname><given-names>John D</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4115-8181</contrib-id><email>john.murray@yale.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Department of Psychiatry, Yale University</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Interdepartmental Neuroscience Program, Yale University</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Department of Physics, Yale University</institution><addr-line><named-content content-type="city">New Haven</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="present-address" id="pa1"><label>‡</label><p>Department of Brain and Cognitive Sciences, Massaschusetts Institute of Technology, Cambridge, United States</p></fn><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>04</day><month>08</month><year>2020</year></pub-date><pub-date pub-type="collection"><year>2020</year></pub-date><volume>9</volume><elocation-id>e56938</elocation-id><history><date date-type="received" iso-8601-date="2020-03-16"><day>16</day><month>03</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2020-08-03"><day>03</day><month>08</month><year>2020</year></date></history><permissions><copyright-statement>© 2020, Shinn et al</copyright-statement><copyright-year>2020</copyright-year><copyright-holder>Shinn et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-56938-v2.pdf"/><abstract><p>The drift-diffusion model (DDM) is an important decision-making model in cognitive neuroscience. However, innovations in model form have been limited by methodological challenges. Here, we introduce the generalized drift-diffusion model (GDDM) framework for building and fitting DDM extensions, and provide a software package which implements the framework. The GDDM framework augments traditional DDM parameters through arbitrary user-defined functions. Models are solved numerically by directly solving the Fokker-Planck equation using efficient numerical methods, yielding a 100-fold or greater speedup over standard methodology. This speed allows GDDMs to be fit to data using maximum likelihood on the full response time (RT) distribution. We demonstrate fitting of GDDMs within our framework to both animal and human datasets from perceptual decision-making tasks, with better accuracy and fewer parameters than several DDMs implemented using the latest methodology, to test hypothesized decision-making mechanisms. Overall, our framework will allow for decision-making model innovation and novel experimental designs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>decision making</kwd><kwd>model fitting</kwd><kwd>computational model</kwd><kwd>psychophysics</kwd><kwd>response time</kwd><kwd>drift-diffusion model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH112746</award-id><principal-award-recipient><name><surname>Murray</surname><given-names>John D</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010848</institution-id><institution>Gruber Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Shinn</surname><given-names>Maxwell</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000038</institution-id><institution>NSERC</institution></institution-wrap></funding-source><award-id>PGSD2-502866-2017</award-id><principal-award-recipient><name><surname>Lam</surname><given-names>Norman H</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Sophisticated decision-making mechanisms and complex experimental paradigms can be modeled, simulated, and fit to empirical response time data, using a flexible and efficient computational modeling framework.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The drift-diffusion model (DDM) is an important model in cognitive psychology and cognitive neuroscience, and is fundamental to our understanding of decision-making (<xref ref-type="bibr" rid="bib55">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib8">Bogacz et al., 2006</xref>). The DDM explains both choice and response time (RT) behavior across a wide range of tasks and species, and has proven to be an essential tool for studying the neural mechanisms of decision-making (<xref ref-type="bibr" rid="bib29">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib25">Forstmann et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Ratcliff et al., 2016</xref>). As a sequential sampling model for two-alternative forced-choice tasks, it assumes the subject obtains a continuous stream of evidence for each alternative throughout the course of an experimental trial. Evidence consists of some true underlying signal (drift) in addition to neural and environmental noise (diffusion). According to the DDM, once a decision variable tracking the difference in evidence between the two alternatives is sufficiently large, i.e. when the decision variable reaches some fixed upper or lower bound, a choice is made at the moment of bound crossing, thereby modeling for the decision both choice and RT. In its simplest form, the DDM includes four parameters: a drift rate, representing evidence; a diffusion constant or bound height representing noise or response caution; a starting position, representing side bias and often fixed at zero; and a non-decision time, representing afferent and efferent delays but external to the DDM process (<xref ref-type="bibr" rid="bib55">Ratcliff, 1978</xref>). These models can be fit to choice and response time data.</p><p>In order to test new hypotheses about decision-making processes, there is great interest in developing extensions to the DDM. For example, the three-parameter DDM is known to be sub-optimal with respect to maximizing reward rate in standard block-design tasks when evidence strength varies unpredictably (<xref ref-type="bibr" rid="bib43">Malhotra et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>). Therefore, it has been hypothesized that decision-making is governed by an urgency signal which increases the probability of response over the course of a trial, implemented as either time-varying evidence gain (<xref ref-type="bibr" rid="bib13">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib71">Thura et al., 2012</xref>; <xref ref-type="bibr" rid="bib48">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib15">Ditterich, 2006b</xref>; <xref ref-type="bibr" rid="bib17">Drugowitsch et al., 2014a</xref>) or collapsing integration bounds (<xref ref-type="bibr" rid="bib12">Churchland et al., 2008</xref>; <xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>; <xref ref-type="bibr" rid="bib15">Ditterich, 2006b</xref>; <xref ref-type="bibr" rid="bib16">Drugowitsch et al., 2012</xref>). Additionally, the DDM may be extended in order to model more diverse experimental paradigms. For example, evidence which changes in magnitude throughout a single trial violates the DDM’s assumption that drift rate is fixed for a single trial. Modeling such tasks requires DDM extensions to support time-varying drift rate. We will show that these and most other extensions can be encapsulated within a single generalized DDM (GDDM). The GDDM provides a common mathematical language for discussing DDM extensions, and as we will show, supports an efficient framework for simulating models and fitting them to data.</p><p>There are three main methods for obtaining response time distributions from the GDDM. First, analytical solutions have a fast execution time, but only exist for special cases of the GDDM. While efficient, most GDDMs do not permit analytical solutions, and thus analytical solutions are only common for the DDM. Several packages exist for analytically solving DDMs and a limited number of extensions (<xref ref-type="bibr" rid="bib88">Wiecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib83">Wagenmakers et al., 2007</xref>; <xref ref-type="bibr" rid="bib30">Grasman et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Vandekerckhove and Tuerlinckx, 2008</xref>; <xref ref-type="bibr" rid="bib81">Voss and Voss, 2007</xref>; <xref ref-type="bibr" rid="bib20">Drugowitsch, 2016</xref>; <xref ref-type="bibr" rid="bib70">Tavares et al., 2017</xref>; <xref ref-type="bibr" rid="bib46">Millner et al., 2018</xref>; <xref ref-type="bibr" rid="bib68">Srivastava et al., 2017</xref>). A second method, trial-wise trajectory simulation, fully supports GDDMs for flexibility in model design (<xref ref-type="bibr" rid="bib11">Chandrasekaran and Hawkins, 2019</xref>). However, this method is inefficient, and shows limited precision and computational efficiency. We will show that this in practice impacts the ability to fit data to RT distributions. To our knowledge, the only software package which currently exists for simulating GDDMs uses trial-wise trajectory simulation (<xref ref-type="bibr" rid="bib11">Chandrasekaran and Hawkins, 2019</xref>).</p><p>A third and better method is to iteratively propagate the distribution forward in time, which is achieved by solving the Fokker-Planck equation (<xref ref-type="bibr" rid="bib82">Voss and Voss, 2008</xref>) or using the method of <xref ref-type="bibr" rid="bib67">Smith, 2000</xref>. These methods allow the RT distribution of GDDMs to be estimated with better performance and lower error than trial-wise trajectory simulation. However, they are difficult to implement, which has thus far impeded their widespread use in cognitive psychology and neuroscience. Currently, GDDMs without known analytical solutions are best implemented using home-grown code (e.g. <xref ref-type="bibr" rid="bib14">Ditterich, 2006a</xref>; <xref ref-type="bibr" rid="bib93">Zylberberg et al., 2016</xref>; <xref ref-type="bibr" rid="bib34">Hawkins et al., 2015b</xref>; <xref ref-type="bibr" rid="bib10">Brunton et al., 2013</xref>), which may not utilize the latest methodology or, as emphasized by <xref ref-type="bibr" rid="bib60">Ratcliff and Tuerlinckx, 2002</xref>, could yield misleading results. Progress is limited by the ability to simulate and fit extensions to the DDM. A common open-source software package for efficiently solving GDDMs would help promote code sharing and reuse, and reduce the amount of time which must be spent implementing diffusion models. Researchers need a general, highly-efficient toolbox for building, solving, and fitting extended DDMs.</p><p>Here, we formally define and evaluate the GDDM framework for solving and fitting extensions to the DDM, and introduce the PyDDM (<ext-link ext-link-type="uri" xlink:href="https://github.com/murraylab/PyDDM">https://github.com/murraylab/PyDDM</ext-link>) software package as an implementation of this framework. We show that a GDDM with scientifically-interesting cognitive mechanisms can capture experimental data from a perceptual decision-making task in an animal dataset better than alternative models, while still providing an excellent fit to human data. Then, we show the execution time vs error tradeoff induced by different numerical methods, and demonstrate the GDDM RT distributions may be estimated with short runtime and low error. In particular, GDDMs may utilize the Crank-Nicolson and backward Euler methods to numerically solve the Fokker-Planck equation, which we will show to be highly efficient compared to alternative methods, and which permits fitting with full-distribution maximum likelihood. Solving the Fokker-Planck equation is analogous to simulating an infinite number of individual trials. The GDDM together with its associated fitting methodology comprise the GDDM framework. Finally, we compare our implementation of the GDDM framework to other software packages for diffusion modeling. In addition to being the only package to support the GDDM framework, we show PyDDM has a number of technical advantages, such as near-perfect parallel efficiency, a graphical user interface for exploring model forms, and a software verification system for ensuring reliable results. We hope that the GDDM framework will encourage innovative experimental designs and lower the barrier for experimenting with new model mechanisms.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>GDDM as a generalization of the DDM</title><p>The classic DDM is a four-parameter model which specifies fixed values for drift, bound height or noise level, starting position bias, and non-decision time (<xref ref-type="fig" rid="fig1">Figure 1</xref>). It assumes that evidence is constant throughout the trial, and that the integration process does not directly depend on time. This form is mathematically accessible, and has been used to model choice and RT data (<xref ref-type="bibr" rid="bib55">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib58">Ratcliff et al., 2016</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Generalized DDM.</title><p>The DDM has fixed, limited parameters, and requires a stream of constant evidence. The ‘full DDM’ expands on the DDM by allowing uniformly-distributed starting position or non-decision time and Gaussian-distributed drift rate. The GDDM is a generalization of the DDM framework which allows arbitrary distributions for starting position and non-decision time, as well as arbitrary functions (instead of distributions) for drift rate and collapsing bounds. It also permits experimental paradigms with dynamic evidence. Several examples of potential GDDM mechanisms are shown. In each case, total response time is the time to reach the diffusion bound plus the non-decision time.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig1-v2.tif"/></fig><p>In order to expand our knowledge of decision-making mechanisms, there is interest in using the DDM to model a variety of experimental paradigms. To accommodate different paradigms, the DDM itself must be extended. One key example is the case where a choice bias is induced experimentally via a change in prior probability or reward magnitude, and it is hypothesized that an offset starting position alone is insufficient to capture the behavioral effects. The DDM has been extended to accommodate an experimentally-induced bias by placing a constant offset on the drift rate (<xref ref-type="bibr" rid="bib47">Mulder et al., 2012</xref>; <xref ref-type="bibr" rid="bib56">Ratcliff, 1985</xref>; <xref ref-type="bibr" rid="bib3">Ashby, 1983</xref>).</p><p>There is a set of influential yet insufficient extensions to the DDM called the ‘full DDM’. This set of extensions was developed to explain two specific discrepancies between the DDM and data (<xref ref-type="bibr" rid="bib2">Anderson, 1960</xref>; <xref ref-type="bibr" rid="bib37">Laming, 1968</xref>; <xref ref-type="bibr" rid="bib6">Blurton et al., 2017</xref>). First, experimental data exhibited a difference in mean RT between correct and error trials which could not be captured by the classic DDM, so two parameters for across-trial variability were introduced to explain this difference: uniformly-distributed initial conditions to explain fast errors (<xref ref-type="bibr" rid="bib37">Laming, 1968</xref>) and Gaussian-distributed drift rate to explain slow errors (<xref ref-type="bibr" rid="bib55">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib59">Ratcliff and Rouder, 1998</xref>). Second, the classic DDM also had a sharper rise time than experimental data, so uniformly-distributed across-trial variability in non-decision time was introduced to capture this effect. A previous study validated predictions of these across-trial variability parameters (<xref ref-type="bibr" rid="bib84">Wagenmakers et al., 2008a</xref>). Compared to the classic DDM, the ‘full DDM’ improves the fit to data, but does not expand the range of experiments which may be performed. Likewise, it does not facilitate extenstions to test alternative cognitive mechansims.. A more versatile framework is therefore needed.</p><p>The GDDM framework provides a flexible means for developing extensions to the DDM (<xref ref-type="fig" rid="fig1">Figure 1</xref>) which can accommodate a wide range of experimental designs and hypotheses about decision-making. The GDDM allows for extensions in five key aspects. First, the drift rate and noise level may depend on time. This allows a range of new experimental paradigms to be modeled using the DDM, such as doubly-stochastic evidence (<xref ref-type="bibr" rid="bib93">Zylberberg et al., 2016</xref>), pulsed evidence (<xref ref-type="bibr" rid="bib36">Huk and Shadlen, 2005</xref>), discrete evidence (<xref ref-type="bibr" rid="bib13">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib10">Brunton et al., 2013</xref>; <xref ref-type="bibr" rid="bib53">Pinto et al., 2018</xref>), or changing evidence (<xref ref-type="bibr" rid="bib64">Shinn et al., 2020</xref>). Additionally, the time dependence of drift rate and noise allows a time-variant urgency signal which modulates the gain of evidence (<xref ref-type="bibr" rid="bib13">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib71">Thura et al., 2012</xref>; <xref ref-type="bibr" rid="bib48">Murphy et al., 2016</xref>; <xref ref-type="bibr" rid="bib17">Drugowitsch et al., 2014a</xref>). Second, drift rate and noise level may depend on position. This allows for model features such as unstable (<xref ref-type="bibr" rid="bib89">Wong and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib62">Roxin and Ledberg, 2008</xref>; <xref ref-type="bibr" rid="bib21">Erlich et al., 2015</xref>) and leaky (<xref ref-type="bibr" rid="bib89">Wong and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib52">Ossmy et al., 2013</xref>; <xref ref-type="bibr" rid="bib24">Farashahi et al., 2018</xref>) integration. Leaky integration with a short time constant is functionally similar to the urgency-gated model with a low pass filter (<xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>; <xref ref-type="bibr" rid="bib72">Thura and Cisek, 2014</xref>). Likewise, position-dependent variance may allow for multiplicative noise (<xref ref-type="bibr" rid="bib26">Freyer et al., 2012</xref>). Third, the bound may change over time, which allows collapsing bounds according to an arbitrary function (<xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>; <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>; <xref ref-type="bibr" rid="bib16">Drugowitsch et al., 2012</xref>), such as for an urgency signal. Fourth, models are parameterizable by an arbitrary dependence on fittable parameters or task conditions. These parameters may be shared between two related experimental tasks, such as an experimental and control task. It also allows alternative implementations of side bias (<xref ref-type="bibr" rid="bib31">Hanks et al., 2011</xref>) and condition-dependent nonlinearities (<xref ref-type="bibr" rid="bib64">Shinn et al., 2020</xref>). Fifth, across-trial parameter variability is supported within the GDDM framework for both starting point and non-decision time, but not drift rate.</p><p>Mathematically, these concepts are expressed using five functions, corresponding to the drift rate, noise, bound height, starting position, and post-factum modifications to the distribution (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Collectively, the GDDM framework allows complicated models to be expressed in a consistent manner, simplifying the way which we may think about complicated DDM extensions.</p></sec><sec id="s2-2"><title>GDDM mechanisms characterize empirical data</title><p>The cognitive mechanisms by which evidence is used to reach a decision is still an open question. One way to distinguish these mechanisms from one another is by fitting models of these cognitive mechanisms to empirical RT distributions. To demonstrate the flexibility of GDDMs, we constructed a GDDM including three plausible mechanisms which are difficult or impossible to implement in a standard DDM: collapsing bounds, leaky integration, and a parameterized nonlinear dependence of drift rate on stimulus coherence. As described below in more detail, we then tested the fit of a GDDM which includes these three mechanisms, compared to the DDM and the ‘full DDM’, in two psychophysical datasets from perceptual decision-making tasks: one from monkeys (<xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref>), and one from human subjects (<xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>). We found that these mechanisms allow the GDDM to provide a better fit to RTs with a more parsimonious model than the DDM or the ‘full DDM’ in the monkey dataset, suggesting that these mechanisms may be important to explaining the monkeys’ behavior. However, consistent with previous results (<xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>), the GDDM fit no better than the DDM or ‘full DDM’ in the human dataset, suggesting that different cognitive strategies may be used in each case.</p><p><xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref> trained two rhesus monkeys on a random dot motion discrimination task in which dots moved randomly but with varying strengths of coherent motion to the left or right (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). Motion coherence was randomly selected to be one of six conditions ranging from 0% to 51.2%. The monkeys were required to determine the direction of coherent motion and respond via saccade to flanking targets. Response time was recorded, which we fit with the standard DDM, ‘full DDM’, and GDDM.</p><p>We compared the GDDM to the ‘full DDM’ and to three different implementations of the DDM which differ in the methodology used to estimate the RT distribution. A detailed comparison of these three packages—PyDDM, HDDM, and EZ-Diffusion—as well other common packages is provided in the section ‘Software package for GDDM fitting’. The DDMs included terms for drift, non-decision time, and either bound height or noise magnitude. The DDM fit by PyDDM assumed drift rate was a linear combination of some baseline drift rate and coherence (<xref ref-type="disp-formula" rid="equ14">Equation 14</xref>), whereas the DDM fit by HDDM used separate drift rates for each coherence condition (<xref ref-type="disp-formula" rid="equ16">Equation 16</xref>), and EZ-Diffusion fit all parameters fully independently for each coherence condition (<xref ref-type="disp-formula" rid="equ17">Equation 17</xref>). We also fit a ‘full DDM’ model, which included across-trial variability in the drift rate, non-decision time, and starting position (<xref ref-type="disp-formula" rid="equ15">Equation 15</xref>). Finally, we fit a GDDM, which consisted of a typical DDM with no across-trial variability in the parameters, but which demonstrates three mechanisms that are difficult or impossible to model in a standard DDM framework: leaky integration, an exponentially collapsing bound, and a drift rate which is determined from coherence using a fittable nonlinearity (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>). Each of the two monkeys was fit using a separate set of parameters for all models.</p><p>Each model was fit to the data, using full-distribution maximum likelihood for PyDDM and HDDM, and an analytic expression for EZ-Diffusion (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). First we examine the fit of the model quantitatively. Bayesian information criterion (BIC) is a common way of quantifying model fit while penalizing for free parameters. According to BIC, the 6-parameter GDDM fit better than all other models (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). To determine whether this improved fit was due to the penalty BIC places on the number of parameters, we compared the models directly with likelihood. Again, we found that the GDDM fit this dataset better than the other models which have more parameters (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). Finally, we tested whether this improved fit is due to the fact that PyDDM and HDDM use full-distribution maximum likelihood as an objective function, whereas EZ-Diffusion does not. We found that the mean squared error (MSE) of the GDDM was also lower than all other models (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). These findings of a good fit by this GDDM, compared to the DDM and the ‘full DDM’, can be interpreted as evidence supporting the cognitive mechanisms included in the GDDM.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Fit of DDM, ‘full DDM’, and GDDM to <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref>.</title><p>Five models (see <bold>Materials and methods</bold>) were fit with three different software packages to data from monkey N. Model fits are compared according to BIC (<bold>a</bold>), negative log likelihood (<bold>b</bold>), and mean squared error (MSE) (<bold>c</bold>). The psychometric (<bold>d</bold>) and chronometric (<bold>e</bold>) functions represent data as dots and models as lines. The probability density function (PDF) (<bold>f</bold>) of each model is shown for 12.8% coherence, 6.4% coherence, 3.2% coherence, and 0% coherence. The data RT histogram is shown in gray, and black dots show the smoothed kernel density estimate probability distribution derived from the data (<xref ref-type="bibr" rid="bib63">Scott, 2015</xref>). Columns of the figure and the color of the line denote different models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Random dot motion task.</title><p>Two rhesus monkeys performed a perceptual decision-making task. Visual stimuli consisted of randomly placed dots on a screen which each moved independently to the left or right. The monkey was required to determine the majority direction of the dots. Task difficulty was varied by changing the fraction of dots which moved coherently in a single direction.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Fit of DDM and GDDM in monkey B.</title><p>Format similar to <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Fit of DDM and GDDM to <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>.</title><p>Format similar to <xref ref-type="fig" rid="fig2">Figure 2</xref>. For this dataset, from human subjects, this GDDM did not improve the fit compared to the DDM or ‘full DDM’.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig2-figsupp3-v2.tif"/></fig></fig-group><p>Additionally, we assessed the qualitative fit of the models to the data to gain insight into what aspects of the RT distributions the GDDM was better able to capture. All five models provided a good fit to empirical psychometric (<xref ref-type="fig" rid="fig2">Figure 2d</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2d</xref>) and chronometric (<xref ref-type="fig" rid="fig2">Figure 2e</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2e</xref>) functions. However, models varied substantially in their ability to fit the qualitative properties of the RT distribution. The standard DDMs fit with HDDM and PyDDM showed a faster increase in response rate for the RT distribution at all coherence levels, and hence underestimated the mode of the distribution (<xref ref-type="fig" rid="fig2">Figure 2f</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2f</xref>). The DDM fit with EZ-Diffusion overestimated the rate of increase in responses but did not begin to increase until the monkeys had made many responses, better matching the mode but creating an incorrect shape (<xref ref-type="fig" rid="fig2">Figure 2f</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2f</xref>). The ‘full DDM’ fit better than the DDMs, and managed to capture the general shape of the RT distribution at high coherences (<xref ref-type="fig" rid="fig2">Figure 2f</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2f</xref>), in addition to low coherences for one monkey (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). However, the GDDM fit better and with fewer parameters than the ‘full DDM’, matching the rate of increased responses and the overall distribution shape for all models under all conditions (<xref ref-type="fig" rid="fig2">Figure 2f</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2f</xref>).</p><p>The RT distributions produced by monkeys may not be representative of those produced by humans (e.g., <xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>), which show a characteristic skewed distribution. This skew is characteristic of the DDM without collapsing bounds, and highlights that a different strategy may be used, potentially driven by overtraining or task structure (<xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>). We therefore fit the same GDDM to a more representative dataset from humans performing a similar perceptual decision-making task. Specifically, from the publicly available dataset of <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>, which used a random dot motion discrimination task (similar to <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref>), we fit the same set of models to the trials with no feedback delay, which the authors had found were better fit with no or little collapse of decision bounds. Consistent with the findings of <xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref> and <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>, here we found that the 6-parameter GDDM provides no better fit than the 11-parameter ‘full DDM’ and 8-parameter DDM fit using HDDM (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3a–c</xref>). The GDDM, ‘full DDM’, and DDMs fit with PyDDM and HDDM are all able to provide good fits to the psychometric function, chronometric function, and RT distributions for human subjects (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3d–f</xref>). This lack of improved fit by the GDDM suggests that the human subjects employed a cognitive strategy closer to the standard DDM, and that the specific mechanisms of this GDDM (collapsing bounds, leaky integration, and input nonlinearity) are not important for explaining psychophysical behavior in this dataset.</p><p>For the example datasets considered above, the GDDM presented here improved the fit to the monkey data and provided a good fit to the human data. We caution that this result does not necessarily mean that this particular GDDM is appropriate for any particular future study. Models should not only provide a good fit to data, but also be parsimonious in form and represent a clear mechanistic interpretation (<xref ref-type="bibr" rid="bib77">Vandekerckhove et al., 2015</xref>). The examples here demonstrate the utility of GDDMs for quantitatively evaluating cognitive mechanisms of interest through fitting models to empirical data.</p></sec><sec id="s2-3"><title>Methods for estimating RT distributions in the DDM and GDDM</title><p>There are three broad classes of methods for estimating a GDDM’s RT distribution. First, analytical expressions can be derived using stochastic calculus and sequential analysis (<xref ref-type="bibr" rid="bib86">Wald, 1945</xref>; <xref ref-type="bibr" rid="bib2">Anderson, 1960</xref>). Analytical expressions were favored historically because they can be computed quickly by humans or computers. However, apart from the simplest form of the DDM and a few particular classes of extensions, analytical solutions often do not exist. While analytical expressions offer maximum performance and accuracy, deriving new analytical expressions is not a practical way to rapidly implement and test new GDDMs.</p><p>The second method, trial-wise trajectory simulation, fully supports GDDM simulation. This method simulates the decision variable trajectories of many individual trials to sample RTs without producing a probability distribution (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). Previous work has used trial-wise trajectories to simulate DDMs with no known analytical solution (e.g. <xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>). However, this flexibility comes at the cost of performance. As we show in section ‘Execution time vs error tradeoff’, trial-wise trajectory simulation is very slow and provides a coarse-grained representation of the model. Since this method does not produce a probability distribution, it is difficult to use efficient and robust methods such as full-distribution maximum likelihood for fitting data, though progress is ongoing in this area using likelihood-free methods such as through approximate Bayesian computation (<xref ref-type="bibr" rid="bib35">Holmes and Trueblood, 2018</xref>).</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Methods for estimating GDDM RT distributions.</title><p>(<bold>a</bold>) A schematic of trial-wise trajectory simulation methods. In a single trial, a subject’s decision process can be summarized by a stochastic decision variable (DV); simulating this single-trial decision variable many times results in a histogram approximation to the RT distribution. (<bold>b</bold>) A schematic of solving the Fokker-Planck equation. The full probability distribution of decision variables is solved at each time point, providing a numerical solutions with higher accuracy and reduced execution time.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig3-v2.tif"/></fig><p>By contrast, the third method may use the Fokker-Planck equation to numerically compute the first passage time distribution. The Fokker-Planck equation is a mathematical expression which reformulates the trial-by-trial diffusion process: rather than simulating single-trial decision variable trajectories, this method ‘simulates’ all possible paths at once by propagating the distribution of single-trial decision variable trajectories over time (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). Unlike analytical solutions, numerical solutions from the Fokker-Planck equation can be computed for all GDDMs. They also generate a probability density function for the RT distribution instead of a sample, allowing fitting with full-distribution likelihood methods. Several algorithms may be used to solve the DDM or GDDM using the Fokker-Planck equation, each with different advantages and disadvantages.</p><p>We consider three such algorithms: forward Euler, backward Euler, and Crank-Nicolson (see Appendix). These algorithms differ in how they propagate the probability distribution over time: forward Euler iteratively approximates the probability distribution of trajectory position at each timestep using the distribution at the previous timestep, while backward Euler and Crank-Nicolson iteratively solve systems of linear equations to approximate the probability distribution (<xref ref-type="bibr" rid="bib82">Voss and Voss, 2008</xref>). The three algorithms also differ in their conceptual and technical difficulty to implement, with forward Euler being easier than backward Euler or Crank-Nicolson. In section ‘Execution time vs error tradeoff’, we will show that the backward Euler and Crank-Nicolson algorithms provide higher accuracy for a given execution time—and a faster execution time for a given accuracy level—than the forward Euler algorithm and trial-wise trajectory simulation.</p></sec><sec id="s2-4"><title>Execution time vs error tradeoff</title><p>Next, we evaluated the effectiveness of different methodologies for estimating the RT distribution. Two general methodological considerations for all of these models are the precision with which the simulations are performed (accuracy), and the amount of time it takes the simulation to run on a computer (execution time). Accurate RT distributions are important to ensure the result faithfully represents the model, and fast execution times are important to increase the feasibility of fitting models to data. In general, for a given algorithm, there is a tradeoff between execution time and the accuracy of the solution obtained. One algorithm may be more efficient than another algorithm if it produces similar accuracy in a shorter amount of time, or better accuracy in the same amount of time.</p><p>We directly evaluate the execution time vs error tradeoff in trial-wise trajectory simulation and in solving the Fokker-Planck equation. To do so, we consider a standard DDM with constant drift, diffusion, and bounds (<xref ref-type="disp-formula" rid="equ18">Equation 18</xref>). While this is not itself a GDDM, such a model has an analytical solution to the reaction-time distribution (as a closed-form infinite sum) (<xref ref-type="bibr" rid="bib2">Anderson, 1960</xref>; <xref ref-type="bibr" rid="bib55">Ratcliff, 1978</xref>) which can be used as ground truth for computing the error in numerical estimates (see <bold>Materials and methods</bold>).</p><p>First, we test trial-wise trajectory simulation. We perform a parameter sweep across timesteps Δ<italic>t</italic> and sample sizes <italic>N</italic>. We simulate using the Euler-Maruyama method, which is standard in the DDM literature (e.g. <xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref>; <xref ref-type="bibr" rid="bib34">Hawkins et al., 2015b</xref>; <xref ref-type="bibr" rid="bib22">Evans et al., 2017</xref>; <xref ref-type="bibr" rid="bib32">Harty et al., 2017</xref>; <xref ref-type="bibr" rid="bib91">Zhou et al., 2009</xref>; <xref ref-type="bibr" rid="bib4">Atiya et al., 2020</xref>; <xref ref-type="bibr" rid="bib66">Simen et al., 2011</xref>; <xref ref-type="bibr" rid="bib50">Nguyen et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Lewis et al., 2014</xref>; <xref ref-type="bibr" rid="bib79">Verdonck et al., 2016</xref>) and for simple models provides identical truncation error but better execution time than more sophisticated methods such as stochastic fourth-order Runge-Kutta (<xref ref-type="bibr" rid="bib27">Gard, 1988</xref>; <xref ref-type="bibr" rid="bib9">Brown et al., 2006</xref>). Because decreasing Δ<italic>t</italic> for a fixed <italic>N</italic> increases the number of timebins and thus increases the variability across each timebin, we employed a smoothing procedure on the resulting simulated histogram to account for this. For each simulation, we smoothed the simulated RT histogram with all mean filters of size 1 to 500 points and chose the filter with the lowest error; since a filter of size one is equivalent to applying no smoothing, this procedure could only improve accuracy. In practice, selecting a filter this way would not be possible due to the lack of analytical ground truth for most GDDMs, and so this smoothing procedure artificially inflates the method’s performance compared to other methods, and effectively yields an upper bound on trial-wise trajectory performance. Execution time and error are shown separately for a range of numerical parameters (<xref ref-type="fig" rid="fig4">Figure 4a,b</xref>). As a proxy for the execution time vs error tradeoff, we also show the product of execution time and mean squared error for the parameter sweep (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Execution time vs error tradeoff.</title><p>A DDM with constant drift, noise, and bounds was estimated for different spatial (Δ<italic>x</italic>) and temporal (Δ<italic>t</italic>) resolutions. Execution time (<bold>a</bold>) and MSE (<bold>b</bold>) are shown across values of Δ<italic>x</italic> and Δ<italic>t</italic>. MSE was computed against the analytical DDM solution for the full distribution. Darker colors indicate a more favorable execution time vs error tradeoff. Gray color indicates numerical instability. (<bold>c</bold>) Relationships between execution time and error which approximately maximize this tradeoff are shown. Parametric expressions were empirically determined to approximately maximize accuracy vs error tradeoff (see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>), and are given by <inline-formula><mml:math id="inf1"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.43</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> for forward Euler and <inline-formula><mml:math id="inf2"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> for trial-wise trajectory simulation, backward Euler, and Crank-Nicolson. The lower left indicates low error and fast execution time whereas the upper right indicates slow execution time and high error.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Execution time vs MSE tradeoff across parameters.</title><p>As a proxy for the execution time vs error tradeoff, mean squared error (MSE) multiplied by execution time in seconds is shown for several values of Δ<italic>x</italic> and Δ<italic>t</italic> for each method. The lower left corner is the most accurate, and the upper right corner is the fastest. Darker colors indicate more favorable values.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Example estimated RT distributions with different numerical parameters.</title><p>(<bold>a</bold>) Example traces are shown for all four methods. These traces are compared to the analytic solution, shown as a dotted black line. (<bold>b</bold>) The execution time is plotted against the accuracy. Lines from <xref ref-type="fig" rid="fig4">Figure 4c</xref> are plotted in light colors for comparison. Note that not all points fall on these lines because the points don’t necessarily satisfy the constraints on Δ<italic>x</italic> and Δ<italic>t</italic> from <xref ref-type="fig" rid="fig4">Figure 4c</xref> and <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig4-figsupp2-v2.tif"/></fig></fig-group><p>In general, trial-wise trajectory simulation provides a suboptimal execution time vs error tradeoff. Using the product of MSE and execution time as a proxy measure of overall performance to illustrate this tradeoff, trial-wise trajectories perform most efficiently for highly imprecise models, i.e. those with a large timestep and a small number of trajectories simulated (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). To examine the execution times and accuracies which lead to this tradeoff, we fix <inline-formula><mml:math id="inf3"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula> and plot the execution time and accuracy across many different values of sample size <italic>N</italic> (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). We see that as execution time increases, accuracy does not increase by more than one order of magnitude. Examples of these traces are provided in <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>.</p><p>Next, we examine methods for solving the Fokker-Planck equation. We consider three different algorithms: forward Euler, backward Euler, and Crank-Nicolson (<xref ref-type="table" rid="table1">Table 1</xref>). The execution time vs accuracy tradeoff of these three algorithms are governed by two parameters: the timestep Δ<italic>t</italic>, and the granularity of decision variable discretization Δ<italic>x</italic>. Decreasing either of these two parameters slows down the execution time but increases accuracy. These three algorithms operate in a distinct but related manner. Forward Euler is the easiest algorithm to implement, however it is only numerically stable for values of Δ<italic>t</italic> which are very small relative to Δ<italic>x</italic>. Forward Euler is an approximation in the limit as <inline-formula><mml:math id="inf4"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and thus cannot be run for large Δ<italic>t</italic> step sizes relative to Δ<italic>x</italic> (see <bold>Materials and methods</bold>). Backward Euler solves this problem by providing a numerically stable solution across all values of Δ<italic>t</italic> and Δ<italic>x</italic>. While it provides a similar execution time and accuracy as forward Euler given Δ<italic>t</italic> and Δ<italic>x</italic>, the fact that it is numerically stable across all values of these parameters allows more efficient values of Δ<italic>t</italic> and Δ<italic>x</italic> to be chosen. Crank-Nicolson further improves the accuracy and execution time over backward Euler (<xref ref-type="table" rid="table1">Table 1</xref>). Like backward Euler, Crank-Nicolson does not have theoretical constraints on Δ<italic>x</italic> or Δ<italic>t</italic>, but unlike backward Euler, it may experience numerical instability for models with time-variant integration bounds. In practice, for each of these methods, the required maximum step size is constrained by how strongly varying the drift and noise are in space and time, where more varying drift would require a smaller step size. Convergence of the results can be evaluated by checking results with smaller step sizes.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Methods for estimating RT distributions, in order of priority within PyDDM.</title></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>Time accuracy</th><th>Grid accuracy</th><th>Requirements</th></tr></thead><tbody><tr><td>Analytical</td><td>Exact</td><td>Exact</td><td>Time- and position-independent drift rate, diffusion constant, and bounds (or linearly collapsing bounds); centered point source initial condition</td></tr><tr><td>Crank-Nicolson</td><td><inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf6"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>Time independent bounds</td></tr><tr><td>Backward Euler</td><td><inline-formula><mml:math id="inf7"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf8"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>None (fallback for all models)</td></tr><tr><td/><td/><td/><td/></tr><tr><td>Forward Euler</td><td><inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td><inline-formula><mml:math id="inf10"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula></td><td>Numerics <inline-formula><mml:math id="inf11"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>&lt;</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (not used by PyDDM)</td></tr><tr><td>Trial-wise trajectories</td><td>N/A</td><td>N/A</td><td>(not used by PyDDM)</td></tr></tbody></table></table-wrap><p>To evaluate the execution time vs error tradeoff, we perform a parameter sweep across values of Δ<italic>t</italic> and Δ<italic>x</italic> for these three algorithms and again compute the product of execution time and mean squared error as a proxy for execution time vs error tradeoff (<xref ref-type="fig" rid="fig4">Figure 4a,b</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Overall, the execution time vs error tradeoff is one to six orders of magnitude better for these algorithms than for trial-wise trajectory simulation. We find that, for backward Euler and Crank-Nicolson, the execution time vs error tradeoff in the model shown in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> is maximized approximately when <inline-formula><mml:math id="inf12"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. Forward Euler is unstable in this parameterization, with the best execution time vs error tradeoff when it is closest to its stability condition. We examine the execution time and error for parameterizations which satisfy these constraints, and compare them to trial-wise trajectory simulation (<xref ref-type="fig" rid="fig4">Figure 4c</xref>). We see that Crank-Nicolson is strictly better than other methods: for a given error, Crank-Nicolson offers a faster execution time, and for a given execution time, Crank-Nicolson offers lower error. However, since Crank-Nicolson may not be appropriate for models with time-variant boundaries, the second best method is also of interest. We see that, for a given execution time, backward Euler is better than forward Euler by two orders of magnitude, and better than trial-wise trajectory simulations by up to five orders of magnitude. Therefore, solving the Fokker-Planck equation using Crank-Nicolson and backward Euler represents a key advantage over the forward Euler and trial-wise trajectory simulation methods.</p></sec><sec id="s2-5"><title>Software package for GDDM fitting</title><sec id="s2-5-1"><title>Overview of our software package</title><p>We developed a software package to encapsulate the methodological framework we have presented. Our package, PyDDM, allows the user to build models, simulate models, and fit them to data using the efficient GDDM framework methodology described previously. The structure and workflow of the package were designed to mirror the GDDM framework (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). PyDDM selects the most efficient approach to solve a GDDM by automatically determining whether the model can be solved analytically and whether it uses time-varying bounds (<xref ref-type="table" rid="table1">Table 1</xref>). PyDDM is also built to emphasize the modularity of the GDDM framework. Components of models may be readily reused in other models, and to promote this, we additionally provide an online database of models (the ‘PyDDM Cookbook’ <ext-link ext-link-type="uri" xlink:href="https://pyddm.readthedocs.io/en/latest/cookbook/index.html">https://pyddm.readthedocs.io/en/latest/cookbook/index.html</ext-link>) to promote code sharing and replication. Users are encouraged to submit their models to this database.</p><p>Furthermore, PyDDM is designed to accommodate the arbitrary flexibility of GDDMs. For reliable parameters estimation, it supports differential evolution (<xref ref-type="bibr" rid="bib69">Storn and Price, 1997</xref>) as a global optimization method in addition to local search methods such as the Nelder-Mead simplex algorithm (<xref ref-type="bibr" rid="bib49">Nelder and Mead, 1965</xref>). Fits are performed using full-distribution maximum likelihood on the full probability distribution in order to use all available data to estimate parameters. Likelihood can be overly sensitive to outliers or ‘contaminants’ (<xref ref-type="bibr" rid="bib60">Ratcliff and Tuerlinckx, 2002</xref>), and this problem has been solved elsewhere by using a mixture model of the DDM process with a uniform distribution in a fixed ratio (<xref ref-type="bibr" rid="bib88">Wiecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib85">Wagenmakers et al., 2008b</xref>; <xref ref-type="bibr" rid="bib81">Voss and Voss, 2007</xref>). PyDDM builds on this methodology by allowing a parameterizable mixture model of any distribution and a fittable ratio. Because some models can be difficult to understand conceptually, PyDDM also includes a GUI for visualizing the impact of different model parameters (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), analogous to a GDDM-compatible version of the tool by <xref ref-type="bibr" rid="bib1">Alexandrowicz, 2020</xref>.</p></sec><sec id="s2-5-2"><title>Comparison to other software packages</title><p>Besides PyDDM, several software packages are available to simulate DDMs, and one package to simulate GDDMs. We compare these in detail in <xref ref-type="table" rid="table2">Table 2</xref>, to emphasize a few key points of comparison.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Comparison of existing DDM packages.</title><p>PyDDM is compared to HDDM (<xref ref-type="bibr" rid="bib88">Wiecki et al., 2013</xref>), EZ-Diffusion (<xref ref-type="bibr" rid="bib83">Wagenmakers et al., 2007</xref>), CHaRTr (<xref ref-type="bibr" rid="bib11">Chandrasekaran and Hawkins, 2019</xref>), Diffusion Model Analysis Toolbox (DMAT) (<xref ref-type="bibr" rid="bib30">Grasman et al., 2009</xref>; <xref ref-type="bibr" rid="bib78">Vandekerckhove and Tuerlinckx, 2008</xref>), and fast-dm (<xref ref-type="bibr" rid="bib81">Voss and Voss, 2007</xref>; <xref ref-type="bibr" rid="bib82">Voss and Voss, 2008</xref>; <xref ref-type="bibr" rid="bib80">Voss et al., 2015</xref>). Red indicates limited flexibility, yellow indicates moderate flexibility, and green indicates maximal flexibility. For the solver, these colors indicate minimal, moderate, and maximal efficiency.</p></caption><table frame="hsides" rules="groups"><thead><tr><th/><th>PyDDM</th><th>HDDM</th><th>EZ-Diffusion</th><th>CHaRTr</th><th>DMAT</th><th>fast-dm</th></tr></thead><tbody><tr><td>Language</td><td>Python3</td><td>Python2/3</td><td>Matlab, R, Javascript, or Excel</td><td>Requires both R and C</td><td>Matlab</td><td>Command line</td></tr><tr><td>Solver</td><td style="author-callout-style-b2">Fokker-Planck, analytical</td><td style="author-callout-style-b4">Analytical numerical hybrid</td><td style="author-callout-style-b6">None</td><td style="author-callout-style-b6">None (Monte Carlo)</td><td style="author-callout-style-b4">Analytical numerical hybrid</td><td style="author-callout-style-b2">Fokker-Planck</td></tr><tr><td colspan="7"><bold>Task parameters</bold></td></tr><tr><td>Time dependence of drift/noise</td><td style="author-callout-style-b2">Any function</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b2">Any function</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b6">Constant</td></tr><tr><td>Position dependence of drift/noise</td><td style="author-callout-style-b2">Any function</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b2">Any function</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b6">Constant</td></tr><tr><td>Bounds</td><td style="author-callout-style-b2">Any function</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b2">Any function</td><td style="author-callout-style-b6">Constant</td><td style="author-callout-style-b6">Constant</td></tr><tr><td>Parameter dependence on task conditions</td><td style="author-callout-style-b2">Any relationship for any parameter</td><td style="author-callout-style-b4">Regression model</td><td style="author-callout-style-b6">Categorical</td><td style="author-callout-style-b6">Categorical</td><td style="author-callout-style-b4">Linear</td><td style="author-callout-style-b6">Categorical</td></tr><tr><td colspan="7"><bold>Across-trial variability</bold></td></tr><tr><td>Across-trial drift variability</td><td style="author-callout-style-b6">Slow discretization (via extension)</td><td style="author-callout-style-b4">Normal distribution</td><td style="author-callout-style-b6">None</td><td style="author-callout-style-b2">Any distribution</td><td style="author-callout-style-b4">Normal distribution</td><td style="author-callout-style-b4">Normal distribution</td></tr><tr><td>Across-trial starting point variability</td><td style="author-callout-style-b2">Any distribution</td><td style="author-callout-style-b4">Uniform distribution</td><td style="author-callout-style-b6">None</td><td style="author-callout-style-b2">Any distribution</td><td style="author-callout-style-b4">Uniform distribution</td><td style="author-callout-style-b4">Uniform distribution</td></tr><tr><td>Across-trial non-decision variability</td><td style="author-callout-style-b2">Any distribution</td><td style="author-callout-style-b4">Uniform distribution</td><td style="author-callout-style-b6">None</td><td style="author-callout-style-b2">Any distribution</td><td style="author-callout-style-b4">Uniform distribution</td><td style="author-callout-style-b4">Uniform distribution</td></tr><tr><td colspan="7"><bold>Model simulation and fitting</bold></td></tr><tr><td>Hierarchical fitting</td><td style="author-callout-style-b6">No</td><td style="author-callout-style-b2">Yes</td><td style="author-callout-style-b6">No</td><td style="author-callout-style-b6">No</td><td style="author-callout-style-b6">No</td><td style="author-callout-style-b6">No</td></tr><tr><td>Fitting methods</td><td style="author-callout-style-b2">Any numerical (default: differential evolution)</td><td style="author-callout-style-b4">MCMC</td><td style="author-callout-style-b2">Analytical</td><td style="author-callout-style-b2">Any numerical</td><td style="author-callout-style-b4">Nelder-Mead</td><td style="author-callout-style-b4">Nelder-Mead</td></tr><tr><td>Objective function</td><td style="author-callout-style-b2">Any function (default: likelihood)</td><td style="author-callout-style-b2">Likelihood</td><td style="author-callout-style-b6">Mean/stdev RT and P(correct)</td><td style="author-callout-style-b4">Any sampled (e.g. quantile maximum likelihood)</td><td style="author-callout-style-b4">Quantile maximum likelihood or chi-squared</td><td style="author-callout-style-b2">Likelihood, chi-squared, Kolmogorov-Smirnov</td></tr><tr><td>Mixture model</td><td style="author-callout-style-b2">Any distribution(s)</td><td style="author-callout-style-b4">Uniform</td><td style="author-callout-style-b6">None (extendable)</td><td style="author-callout-style-b6">None</td><td style="author-callout-style-b4">Uniform and undecided guesses</td><td style="author-callout-style-b4">Uniform</td></tr></tbody></table></table-wrap><p>The choice of software package depends on the model to be fit. As demonstrated in <xref ref-type="fig" rid="fig2">Figure 2</xref> and noted previously (<xref ref-type="bibr" rid="bib57">Ratcliff, 2008</xref>), fitting a diffusion model to a small number of summary statistics, as in EZ-Diffusion, can lead to poor qualitative and quantitative model fit to the RT distribution. Better options for fitting the DDM and ‘full DDM’ are available, such as fast-dm and HDDM. However, these packages are limited in model form, and make it difficult to extend beyond the ‘full DDM’ to test additional cognitive mechanisms or harness time-varying stimulus paradigms. As we have shown previously (<xref ref-type="fig" rid="fig2">Figure 2</xref>), the GDDM is desirable for obtaining good model fits with few parameters.</p><p>The major highlight of PyDDM compared to other packages is that it allows GDDMs to be simulated, and is thus compatible with time- and position-dependent drift rate and noise, time-varying bounds, starting point variability, and non-decision time variability. Besides PyDDM, the only other package to our knowledge capable of simulating GDDMs is the new R package CHaRTr (<xref ref-type="bibr" rid="bib11">Chandrasekaran and Hawkins, 2019</xref>). This package is a major innovation compared to previous approaches, as it allows flexible model forms to be tested. However, CHaRTr is based on trial-wise trajectory simulations. As shown previously (<xref ref-type="fig" rid="fig4">Figure 4</xref>), trial-wise trajectory simulation is especially inefficient, reducing the accuracy of the model simulations and increasing execution time by several orders of magnitude. In practice, this makes it infeasible to fit data to more flexible models. For the user to define new models, CHaRTr requires defining models in C and modifying the CHaRTr source code. By contrast, PyDDM facilitates the specification and implementation of models by defining them in user scripts written in pure Python.</p></sec><sec id="s2-5-3"><title>Parallel performance</title><p>In addition to PyDDM’s efficient algorithms for solving GDDM models, PyDDM also includes the ability to further speed up simulations by solving models in parallel. Any model can be converted from single-threaded to multi-threaded with a single function call. PyDDM solves models for different conditions on multiple CPUs via the ‘pathos’ library (<xref ref-type="bibr" rid="bib44">McKerns et al., 2012</xref>). It uses a so-called ‘embarrassingly parallel’ algorithm, which turns the most essential parts of the simulation into independent tasks and distributes them equally across CPUs. Such parallelization can be enabled in any PyDDM script with one line of code.</p><p>In a perfectly efficient parallelized environment, a simulation utilizing <italic>N</italic> CPUs should be <italic>N</italic> times faster than a simulation utilizing one CPU. In practice, this is rare due to communication latency and other overhead. We evaluate the speedup gained by solving an example DDM (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>)) on more than one CPU (<xref ref-type="fig" rid="fig5">Figure 5</xref>). As expected, the execution time decreases when more CPUs are used to perform the simulations. However, this speedup is strikingly linear (<xref ref-type="fig" rid="fig5">Figure 5a</xref>); for up to 20 CPUs, PyDDM achieves close to 100% parallel efficiency (<xref ref-type="fig" rid="fig5">Figure 5b</xref>). This means that PyDDM is able to effectively utilize multiple processors to decrease execution time. In practice, the 6-parameter GDDM shown above fits on one CPU in under 15 minutes, and the authors routinely fit 15-parameter GDDMs on 8 CPUs with differential evolution for global parameter optimization in under 5 hours.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Parallel performance.</title><p>Simulations of a standard DDM were performed, varying numbers of CPUs each with ten replicates. (<bold>a</bold>) The speedup, defined as (execution time on 1 CPU)/(execution time on <italic>N</italic> CPUs), and (<bold>b</bold>) parallel efficiency, defined as speedup/(<italic>N</italic> CPUs), are shown for different numbers of CPUs. Because measured execution time varied run to run, the mean parallel efficiency could sometimes exceed 1. Error bars are bootstrapped 95% confidence intervals. Solid black lines indicate data, and dashed green lines indicate the theoretical maximum under noiseless conditions. Confidence intervals in (<bold>a</bold>) are hidden beneath the markers.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Organization of PyDDM’s source code.</title><p>The PyDDM source code is organized in a way which mirrors the GDDM framework. Each model is built by defining five components, analogous to those described in ‘GDDM description’. Each of these components may have any number of fittable parameters, and these parameters may be shared across model components. They may also depend on task parameters, which vary trial to trial. There are three central functions which may be performed on a model: it may be fit to data, it can be simulated for a specific trial type (i.e. with a given set of task parameters), or it may visualized using the GUI.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Model GUI.</title><p>The PyDDM GUI allows the user to interactively change the parameters of their model (right pane) and see how these changes are reflected in the RT distribution. This makes it possible to gain an intuition for how various parameters and interactions among parameters impact the shape of the RT distribution. The GUI may be used with or without experimental data. When experimental data are provided the model’s simulated RT distribution is overlaid on the empirical RT histogram. The data may be shown all at once, or subset by experimental condition (left pane). Shown here are the data from <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref> and the 5-parameter GDDM from <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-56938-fig5-figsupp2-v2.tif"/></fig></fig-group></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The GDDM framework provides a consistent description of model extensions, and allows researchers to efficiently simulate and fit a wide variety of extensions to the DDM. Using two open datasets, we demonstrated that GDDMs are able to fit experimental data with high accuracy and few parameters. Model RT distributions are solved and fit numerically using multiple fast and accurate methods for solving the Fokker-Planck equation. We built the PyDDM software package to implement the GDDM framework. PyDDM makes it easy to utilize the efficiency of this framework, while providing additional technical advantages over other packages.</p><p>The major benefit of our GDDM framework is the flexibility and ease with which the model may be extended. The GDDM generalizes the DDM to support a wide variety of task paradigms and cognitive mechanisms. Any parameter in the standard DDM is allowed to be an arbitrary function of time and/or position when applicable. Mechanisms such as leaky or unstable integration, collapsing bounds, time-dependent drift rates, and complex non-decision time and starting point distributions all fall within the GDDM framework and can be readily simulated in PyDDM. GDDMs may be built modularly, eliminating the need to reimplement a separate model for each new mechanism. This modularity also helps promote code reuse through our online database of PyDDM model components.</p><p>GDDMs may be simulated by efficiently solving the Fokker-Planck equation. In our tests, trial-wise trajectory simulation performed poorly because large increases in the number of trials did not result in as large of increases in accuracy. While the smoothing procedure we applied improved accuracy, trial-wise trajectory simulation was unable to obtain low error for reasonable execution times. The Crank-Nicolson and backward Euler algorithms performed up to five orders of magnitude better than trial-wise trajectory simulation. This increase in performance is consistent with previous work which used Crank-Nicolson to solve the Fokker-Planck equation for the ‘full DDM’ (<xref ref-type="bibr" rid="bib82">Voss and Voss, 2008</xref>). However, increasing performance is critical for the GDDM not only because it reduces simulation time, but because it leads to a qualitative difference in the types of models which may be fit to data. While inefficient methods are sufficient for simulating a single instance of the model, fitting models with many parameters to data requires thousands of simulations or more. By providing efficient algorithms for solving this broad class of models, our framework makes it possible to fit parameters to data for models which were previously prohibitively time consuming. In this way, it expands the range of potential cognitive mechanisms which can be tested and experimental paradigms which can be modeled.</p><p>PyDDM enables researchers to build very complex models of decision-making processes, but complex models are not always desirable. Highly complex models may be able to provide an excellent fit to the data, but unless they represent specific, plausible cognitive mechanisms, they may not be appropriate. PyDDM’s complexity can be used to examine a range of potential cognitive mechanisms which were previously difficult to test, and to probe a range of task paradigms. Additionally, sometimes complex models which do represent appropriate cognitive mechanisms may exhibit inferior performance due to limitations of the available data. For instance, simulation studies suggest that when data are sparse, Bayesian fitting or EZ-Diffusion may be better able to recover parameters (<xref ref-type="bibr" rid="bib38">Lerche et al., 2017</xref>; <xref ref-type="bibr" rid="bib88">Wiecki et al., 2013</xref>; <xref ref-type="bibr" rid="bib75">van Ravenzwaaij et al., 2017</xref>).</p><p>When the hypothesis demands additional model complexity, though, one critical check is to make sure the parameters of the models are recoverable. That is, given a set of models, simulated data from each model should be best fit by the model and parameters that simulated it (<xref ref-type="bibr" rid="bib88">Wiecki et al., 2013</xref>). This explicitly tests against model and parameter degeneracy, a known weakness of the ‘full DDM’ (<xref ref-type="bibr" rid="bib76">van Ravenzwaaij and Oberauer, 2009</xref>; <xref ref-type="bibr" rid="bib7">Boehm et al., 2018</xref>). Individual GDDMs must be tested for model and parameter recovery before claims can be made about model mechanisms. PyDDM makes it straightforward to test whether a given GDDM has parameters which are recoverable, and whether any two given models can be distinguished from one another. This is facilitated because synthetic RT data can be sampled from a the calculated probability distributions of the GDDM without the need to run trial-wise trajectory simulations.</p><p>The GDDM framework supports across-trial variability in non-decision time and starting position according to any arbitrary distribution, but it does not support across-trial drift rate variability. Across-trial drift rate variability is an inherent weakness of the Fokker-Planck approach. It is technically possible to model across-trial drift rate variability using our framework, by discretizing the drift rate distribution and solving the Fokker-Planck equation for each case separately (<xref ref-type="bibr" rid="bib82">Voss and Voss, 2008</xref>). PyDDM is not set up to ergonomically reconcile across-trial drift rate variability with time-varying evidence, which is a core feature of PyDDM. As a result, the GDDM framework does not fully support the across-trial drift rate variability aspect of the ‘full DDM’. Across-trial drift rate variability is a potentially plausible cognitive mechanism used in prior literature to account for slow errors (<xref ref-type="bibr" rid="bib59">Ratcliff and Rouder, 1998</xref>). Of note, studies suggest this mechanism may not be necessary to include to fit data (<xref ref-type="bibr" rid="bib39">Lerche and Voss, 2016</xref>; <xref ref-type="bibr" rid="bib38">Lerche et al., 2017</xref>). GDDMs offer researchers the capability to explore alternative cognitive mechanisms which might also account for slow errors.</p><p>PyDDM uses numerical solutions of the Fokker-Planck equation to estimate the RT distribution of the model. Other methods may be used for this estimation, such as solving the Volterra integral equation (<xref ref-type="bibr" rid="bib67">Smith, 2000</xref>). Some prior work has used these alternative methods for solving GDDMs (<xref ref-type="bibr" rid="bib16">Drugowitsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib90">Zhang et al., 2014</xref>; <xref ref-type="bibr" rid="bib19">Drugowitsch et al., 2014c</xref>). Such methods already have fast solvers (<xref ref-type="bibr" rid="bib20">Drugowitsch, 2016</xref>), which could be implemented as an alternative method within PyDDM in the future.</p><p>PyDDM also has the potential to support other applications of interest in the field. First, it could be used as a simulation engine for fitting hierarchical models of GDDM parameters, in which each subject’s individual parameters are drawn from a group distribution. The most critical component to hierarchical model fitting is an efficient method for simulating model likelihood. Since PyDDM provides such a method, hierarchical fitting of GDDMs could be readily implemented as an extension to PyDDM. Second, while PyDDM currently only implements bounds which are symmetric around zero, support for asymmetric bounds is a feasible future addition. Third, PyDDM may be extended to support multi-dimensional diffusion, enabling it to represent interacting race models (<xref ref-type="bibr" rid="bib54">Purcell et al., 2010</xref>; <xref ref-type="bibr" rid="bib74">Usher and McClelland, 2001</xref>; <xref ref-type="bibr" rid="bib8">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib73">Tsetsos et al., 2012</xref>; <xref ref-type="bibr" rid="bib18">Drugowitsch et al., 2014b</xref>). The Fokker-Planck equation can be solved in higher dimensions, in principle enabling similar highly efficient methods to be used for these models as well. The GDDM framework would require only minor adaptations to be compatible with multi-dimensional diffusion. Fourth, it may be extended to allow fitting models to neural recordings or other data which represents the value of the decision variable, as in <xref ref-type="bibr" rid="bib92">Zoltowski et al., 2020</xref>. Currently, PyDDM only supports fitting to the RT distribution.</p><p>In summary, we show that the GDDM framework is a flexible and efficient way to explore new extensions on the DDM, allowing new possibilities in task design. Our package, PyDDM, offers a convenient and modular method for implementing this framework.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>GDDM description</title><p>The GDDM is described as the following differential equation for decision variable <italic>x</italic>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>W</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula>where ‘<inline-formula><mml:math id="inf13"><mml:mi mathvariant="normal">…</mml:mi></mml:math></inline-formula>’ represents task conditions and fittable parameters. Initial conditions are drawn from the distribution <inline-formula><mml:math id="inf14"><mml:mrow><mml:mi>x</mml:mi><mml:mo>∼</mml:mo><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. The process terminates when <inline-formula><mml:math id="inf15"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>≥</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. This can be parameterized by five functions:</p><list list-type="bullet"><list-item><p><inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> – The instantaneous drift rate, as a function of the decision variable position, time, and task parameters. In PyDDM, it is represented by the ‘Drift’ class and defaults to <inline-formula><mml:math id="inf17"><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><inline-formula><mml:math id="inf18"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> – The instantaneous noise, as a function of the decision variable position, time, and task parameters. In PyDDM, it is represented by the ‘Noise’ class and defaults to <inline-formula><mml:math id="inf19"><mml:mrow><mml:mrow><mml:mi>σ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><inline-formula><mml:math id="inf20"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> – The probability density function describing the initial position of the decision variable, as a function of task parameters. It must hold that <inline-formula><mml:math id="inf21"><mml:mrow><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>x</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. In PyDDM, it is represented by the ‘IC’ class and defaults to a Kronecker delta function centered at <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> – The boundaries of integration, as a function of time and task parameters. In PyDDM, it is represented by the ‘Bound’ class and defaults to <inline-formula><mml:math id="inf24"><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p><inline-formula><mml:math id="inf25"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>C</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf26"><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>E</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> – Post-processing performed on the simulated first passage time distributions of correct and error RT distributions <inline-formula><mml:math id="inf27"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf28"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, respectively. In PyDDM, these functions are represented by the ‘Overlay’ class and default to <inline-formula><mml:math id="inf29"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>C</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>C</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf30"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>p</mml:mi><mml:mi>E</mml:mi><mml:mo>*</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. In practice, ‘Overlay’ is used to implement non-decision times and mixture models for fitting contaminant responses.</p></list-item></list><p>The organization of the code in PyDDM mirrors this structure (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><p>The GDDM framework supports Markovian processes, in that the instantaneous drift rate and diffusion constant of a particle is determined by its position <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> and the current time <italic>t</italic>, without dependence on the prior trajectory leading to that state. Drift rate is discretized, such that the timestep of the discretization is equal to the timestep of the Fokker-Planck solution. In practice, for typical task paradigms and dataset properties, the impact of this discretization is expected to be minimal for a reasonable timestep size.</p></sec><sec id="s4-2"><title>Fokker-Planck formalism of the GDDM</title><p>PyDDM solves the GDDM using the Fokker-Planck equation, a partial differential equation describing the probability density <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the decision variable to be at position <italic>x</italic> at time <italic>t</italic>, with initial condition <inline-formula><mml:math id="inf33"><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mo>∂</mml:mo><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mo>∂</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo>∂</mml:mo><mml:mo>⁡</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>By definition, the GDDM has absorbing boundary conditions (<inline-formula><mml:math id="inf34"><mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for all <inline-formula><mml:math id="inf35"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:mrow><mml:mi>x</mml:mi><mml:mo>≥</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), and any mass of the probability density crossing the boundaries are treated as committed, irreversible decisions.</p><p>The Fokker-Planck equation permits numerical solutions of the GDDM. The numerical algorithm is described for the GDDM with fixed bounds in this section, and the algorithm for the GDDM with time-varying bounds is described in the next section. The system is first discretized with space and time steps Δ<italic>x</italic> and Δ<italic>t</italic>. Without loss of generality, we assume Δ<italic>x</italic> divides <inline-formula><mml:math id="inf37"><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and Δ<italic>t</italic> divides the simulated duration. The probability density solution of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, <inline-formula><mml:math id="inf38"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, can then be approximated as the probability distribution function at spatial grids <inline-formula><mml:math id="inf39"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and temporal grids <inline-formula><mml:math id="inf40"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> for any non-negative integer <italic>n</italic>. Due to absorbing bounds, the two spatial grids one step outward at <inline-formula><mml:math id="inf41"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> have 0 density, and are not considered explicitly. The choice of Δ<italic>t</italic> is constrained by the stability criteria and accuracy vs execution time tradeoff (see <xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Define <inline-formula><mml:math id="inf42"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:math></inline-formula> as the probability distribution at the <italic>i</italic>th space-grid and the <italic>m</italic>th time-grid. For clarity, here and below <inline-formula><mml:math id="inf43"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:math></inline-formula> denotes the total probability inside a grid of lengths Δ<italic>x</italic> and Δ<italic>t</italic>, and its center represented by (<italic>i</italic>,<italic>m</italic>). As such, <inline-formula><mml:math id="inf44"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi></mml:msubsup></mml:math></inline-formula> is unitless, in contrast to <inline-formula><mml:math id="inf45"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> which has a unit of the inverse of the spatial dimension. Probability distribution functions can also be viewed as a discrete proxy of the continuous probability density (i.e. approximating <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> at the corresponding space and time grid), which will then have a unit of the inverse of the spatial dimension. Both views are mathematically equivalent (and just off by a Δ<italic>x</italic> factor to the whole equation), but the former definition was used for simplicity and interpretability.</p><p>The Fokker-Planck equations can be discretized in several distinct ways, including the forward Euler method, the backward Euler method, and the Crank-Nicolson method. In the forward Euler method, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> is approximated as:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf47"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is the diffusion coefficient. Here and below, dependencies of space, time, and other variables are omitted for simplicity. Terms in parenthesis with subscripts and superscripts are evaluated at the subscripted space grid and superscripted time grid, in the same manner as <inline-formula><mml:math id="inf48"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula>. The spatial derivatives at the right side of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> are evaluated at the previous time step <inline-formula><mml:math id="inf49"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, such that only one term of the equation is at the current time step <italic>n</italic>, generated from the temporal derivative at the left side of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. As such, <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> for each <italic>j</italic> fully determines <inline-formula><mml:math id="inf50"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula> from the probability distributions at the previous time-step:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>It is important to note that the forward Euler method, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, is an approximation method. Since the probability distribution is iteratively propagated from early to later times, any errors of the solution accumulate over the iterative process. Moreover, the forward Euler method is prone to instability. Assuming small <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf52"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> (i.e. much less than 1), <inline-formula><mml:math id="inf53"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:math></inline-formula> is much larger than <inline-formula><mml:math id="inf54"><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:math></inline-formula> and thus the third term on the right side of <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> should in general be much larger than the second term. If <inline-formula><mml:math id="inf55"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf56"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> are such that the third term is large for any <italic>n</italic> and <italic>j</italic>, then <inline-formula><mml:math id="inf57"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula> will not be properly computed in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, and such errors will propagate to the whole space over time. The forward Euler method thus has a stability criteria:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow><mml:mo>&lt;</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ5">Equation 5</xref> poses a strict constraint on the step-sizes: while Δ<italic>x</italic> and Δ<italic>t</italic> should both be small to minimize the error due to discretization, decreasing Δ<italic>x</italic> demands a much stronger decrease in Δ<italic>t</italic>, resulting in a much longer execution time of the method. This can be seen in our simulations shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>.</p><p>The backward Euler and Crank-Nicolson methods largely circumvent the aforementioned problems. In the backward Euler method, the right side of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> is expressed at the current time step <italic>n</italic>:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>-</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi>P</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mi>j</mml:mi><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:mrow><mml:mo rspace="32.5pt">,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In the Crank-Nicolson method, half of the right side of <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>) is expressed at the current time step <italic>n</italic>, and half at the previous time step <inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>+</mml:mo><mml:mtext> </mml:mtext><mml:mn>0.5</mml:mn><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>μ</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:mi>D</mml:mi><mml:mi>P</mml:mi><mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>With multiple terms at the current time-step, <xref ref-type="disp-formula" rid="equ6">Equation 6</xref> and <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> cannot be directly solved. Instead, the equations across all spatial grids can be summarized in matrix form:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">𝟙</mml:mn></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn mathvariant="double-struck">𝟙</mml:mn></mml:mrow><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mrow><mml:mi mathvariant="double-struck">M</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="false" scriptlevel="0"><mml:mtext>for </mml:mtext></mml:mstyle><mml:msup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>⋮</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">M</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>2</mml:mn><mml:mi>ν</mml:mi></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>+</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:mi>ν</mml:mi></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>+</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>⋱</mml:mo></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>+</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mn>0</mml:mn></mml:mtd><mml:mtd><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mtd><mml:mtd><mml:mn>2</mml:mn><mml:mi>ν</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Here, <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>ν</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf60"><mml:mrow><mml:mi>χ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. In this formulation, <inline-formula><mml:math id="inf61"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for the forward Euler method, <inline-formula><mml:math id="inf62"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for the backward Euler method, and <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> for the Crank-Nicolson method. Since <inline-formula><mml:math id="inf64"><mml:mi>𝕄</mml:mi></mml:math></inline-formula> is a tridiagonal matrix, it can be inverted from the left side to the right side of the equation in an efficient manner. Practically, this allows the backward Euler and Crank-Nicolson method to rapidly generate the discretized probability distribution function inside the boundaries at each time-step.</p><p>Finally, the probability of decision formation at each time step is computed as the outward flux from the outermost grids <inline-formula><mml:math id="inf65"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mi>B</mml:mi></mml:mrow><mml:mo>∓</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> (to the hypothetical grids at <inline-formula><mml:math id="inf66"><mml:mrow><mml:mo>±</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:math></inline-formula>):<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mi>B</mml:mi><mml:mo>−</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>+</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mi>B</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mstyle></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>This provides the correct and incorrect reaction time distributions, as well as the total correct (<inline-formula><mml:math id="inf67"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) and incorrect (<inline-formula><mml:math id="inf68"><mml:mrow><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow></mml:mrow></mml:math></inline-formula>) probability.</p><p>Unlike the forward-Euler method which iteratively propagates the solution and the approximation error in a feedforward manner, the backward Euler and Crank-Nicolson methods solve for the matrix equation of the probability distribution evolution, and are less susceptible to error being propagated. In addition, the two methods do not have any stability criteria which constrain the step size. Contrasting the two methods, the backward Euler method has larger truncation error (<inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) than the Crank-Nicolson method (<inline-formula><mml:math id="inf70"><mml:mrow><mml:mi>O</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>x</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>) (<xref ref-type="table" rid="table1">Table 1</xref>), while the Crank-Nicolson method generally has twice the execution time of the backward Euler method, assuming the matrix construction is the slowest step of the algorithms. Crucially, the solution from the Crank-Nicolson method tends to be susceptible to oscillations over iterations, especially with varying bounds (see below). PyDDM automatically chooses the best solver for any given model (<xref ref-type="table" rid="table1">Table 1</xref>).</p></sec><sec id="s4-3"><title>Fokker-Planck formalism with time-varying bounds</title><p>The previous section considered the Fokker-Planck equation of the GDDM assuming fixed bounds. The formalism can be adapted to accommodate time-varying bounds. This is more generally studied as a free boundary problem, with a typical example including the Stefan problem (<xref ref-type="bibr" rid="bib45">Meyer, 1978</xref>; <xref ref-type="bibr" rid="bib41">Li, 1997</xref>).</p><p>In particular, consider upper and lower bounds <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>B</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> at an arbitrary time <inline-formula><mml:math id="inf73"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>. If <inline-formula><mml:math id="inf74"><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> is exactly on a grid point, the previous results apply. Alternatively, if <inline-formula><mml:math id="inf75"><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> is in between two grid points <inline-formula><mml:math id="inf76"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, the solution is approximated by a linearly weighted summation of the solutions in two grid systems: an inner grid system <inline-formula><mml:math id="inf77"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula> and an outer grid system <inline-formula><mml:math id="inf78"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>k</mml:mi></mml:msub><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>.</p><p>Define <inline-formula><mml:math id="inf79"><mml:msubsup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula> to be the probability distribution function at time <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> over the inner and outer grid systems, respectively. In the backward Euler method (<inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>), <inline-formula><mml:math id="inf82"><mml:msubsup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:math></inline-formula> can be computed by propagating from the actual probability distribution function at the previous step <inline-formula><mml:math id="inf83"><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>𝟙</mml:mn><mml:mo>+</mml:mo><mml:msub><mml:mi>𝕄</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <inline-formula><mml:math id="inf84"><mml:msub><mml:mi>𝕄</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>o</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are as in <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>, but defined over the domain of inner/outer grids respectively.</p><p>Furthermore, define<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>x</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>as the weights of the linear approximation of the solution to that of the outer and inner grid systems. The resulting probability distribution function and the probabilities of decision formation at step <italic>n</italic> are:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mi>n</mml:mi></mml:msup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mrow><mml:mover><mml:mi>P</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>+</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>+</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>ν</mml:mi><mml:mo>−</mml:mo><mml:mi>χ</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mi>n</mml:mi></mml:msubsup><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mi>n</mml:mi></mml:msubsup></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>In the literature of the DDM in cognitive psychology and neuroscience, collapsing bounds are the most common form of time-varying bounds considered. As bounds collapse, it is possible that the outermost grids at the previous step (with non-zero probability densities) become out-of-bound at the current step. Based on the side they become out of bound, such probabilities are added to the correct and incorrect probabilities respectively (and multiplied by <inline-formula><mml:math id="inf85"><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for each of the outer/inner grid system). Nevertheless, increasing bounds are also supported. The algorithm for the Crank-Nicolson scheme of time-varying bounds often generates oscillatory solutions, and is thus not described here or implemented in PyDDM.</p></sec><sec id="s4-4"><title>Empirical datasets</title><p>As testbeds for fitting different models, we used two empirical datasets of choices and RTs during perceptual decision-making tasks: one from <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref>, one from <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>. These references contain full details of the tasks and datasets. For the dataset of <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref>, we used trials from the ‘reaction time’ task variant, for both monkeys. For the dataset of <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>, we used trials from the task variant with no feedback delay.</p><p>In brief, these two publicly available datasets were selected for the following reasons. Both use random dot motion discrimination tasks, which facilitate comparison of behavior and fitting with the same GDDM. These two datasets span species, with behavior from monkeys (<xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref>) and human subjects (<xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref>). Furthermore, both datasets have been analyzed in prior work fitting DDMs with collapsing bounds. Specifically, <xref ref-type="bibr" rid="bib33">Hawkins et al., 2015a</xref> generally found that the dataset of <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref> was better fit with collapsing bounds than with constant bounds, in contrast with typical human datasets which are well fit with constant bounds. In line with those findings, <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref> showed that behavior in a task variant with no feedback delay yielded behavior which was more representative of human datasets and was well fit by a DDM with decision bounds with no or little collapse. These two datasets therefore provided useful testbeds for GDDM fitting with distinct a priori predictions about the utility of collapsing bounds in a GDDM.</p></sec><sec id="s4-5"><title>Compared models</title><p>The 6-parameter GDDM fit by PyDDM is parameterized by<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mi>α</mml:mi></mml:msup></mml:mrow><mml:mo>−</mml:mo><mml:mi>ℓ</mml:mi><mml:mi>x</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.95</mml:mn></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mn>0.025</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where fittable parameters are drift rate <inline-formula><mml:math id="inf86"><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, coherence nonlinearity α, leak constant ℓ, initial bound height <inline-formula><mml:math id="inf87"><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, bound collapse rate τ, and non-decision time <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Additionally, <inline-formula><mml:math id="inf89"><mml:msub><mml:mi>C</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:math></inline-formula> is the maximum coherence in the experiment, which is fixed at 0.512 for the <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref> dataset and 0.4 for the <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref> dataset.</p><p>The three-parameter DDM fit by PyDDM is a subset of the GDDM framework parameterized by<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mi>C</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mn>0.95</mml:mn></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mn>0.025</mml:mn></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <italic>C</italic> is coherence and fittable parameters are <inline-formula><mml:math id="inf90"><mml:msub><mml:mi>μ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf91"><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf92"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>.</p><p>The 11-parameter ‘full DDM’ fit by HDDM does not fall into the GDDM framework because the across-trial variability in drift rate <inline-formula><mml:math id="inf93"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a random variable. Thus, the ‘full DDM’ is parameterized by<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>s</mml:mi><mml:mi>v</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>s</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>z</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:mi>x</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msub><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">𝒩</mml:mi></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is a normal distribution, and fittable parameters are six independent drift rates <inline-formula><mml:math id="inf95"><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> for each coherence level <italic>j</italic>, non-decision time <inline-formula><mml:math id="inf96"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and bounds <inline-formula><mml:math id="inf97"><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, as well as variability in drift rate <inline-formula><mml:math id="inf98"><mml:msub><mml:mi>s</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula>, starting position <inline-formula><mml:math id="inf99"><mml:msub><mml:mi>s</mml:mi><mml:mi>z</mml:mi></mml:msub></mml:math></inline-formula>, and non-decision time <inline-formula><mml:math id="inf100"><mml:msub><mml:mi>s</mml:mi><mml:mi>T</mml:mi></mml:msub></mml:math></inline-formula>. It was fit an outlier probability of <inline-formula><mml:math id="inf101"><mml:mrow><mml:mtext>p_outlier</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The 8-parameter DDM fit by HDDM is a subset of the GDDM framework parameterized by<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where fittable parameters are <inline-formula><mml:math id="inf102"><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf103"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, and six independent drift rates <inline-formula><mml:math id="inf104"><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, one for each coherence level <italic>j</italic>. It was fit an outlier probability of <inline-formula><mml:math id="inf105"><mml:mrow><mml:mtext>p_outlier</mml:mtext><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>The 18-parameter DDM fit by EZ-Diffusion is a subset of the GDDM framework parameterized by<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where fittable parameters are independent drift rates <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>μ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, bounds <inline-formula><mml:math id="inf107"><mml:msub><mml:mi>B</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula>, and non-decision times <inline-formula><mml:math id="inf108"><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi></mml:mrow><mml:mi>j</mml:mi></mml:msubsup></mml:math></inline-formula>, for each of the six coherence level <italic>j</italic>.</p><p>All simulations were fit separately for each monkey. In the human fits, data were pooled across all subjects for the 0 s delay condition only.</p></sec><sec id="s4-6"><title>Execution time and accuracy analysis</title><p>To analyze the execution time vs error tradeoff, we benchmarked a DDM which fits into the GDDM framework, defined by:<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>The DDM was chosen because it permits closed-form solutions, providing a comparison for benchmarking simulation accuracy. Performance was measured on a Lenovo T480 computer with an Intel M540 i7-8650U 4.2 GHz CPU with hyperthreading disabled. Verification and parallelization were disabled for these evaluations. The simulated time was 2 s.</p><p>In order to examine combinations of Δ<italic>t</italic> and either Δ<italic>x</italic> or <italic>N</italic> which simultaneously minimize both execution time and error, we plot the product of the MSE and execution time in seconds. This product should be interpreted as a heuristic which encompasses both execution time and accuracy rather than a fundamentally important quantity in and of itself.</p></sec><sec id="s4-7"><title>Model for parallel performance</title><p>To analyze PyDDM’s parallel performance, we benchmarked a DDM which fits into the GDDM framework, defined by:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>C</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>σ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mn>1</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mstyle></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msubsup><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mo>∗</mml:mo></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mstyle></mml:math></disp-formula>where the model was simulated using backward Euler for 64 coherence values <italic>C</italic> uniformly spaced from 0 to 0.63. Because PyDDM parallelizes functions by utilizing an embarrassingly parallel approach over experimental conditions, we included many such conditions to enable benchmarks which reflect the simulations instead of the specifics of the model. Simulations were performed on a high performance computing cluster using a single node with 28 Xeon E5-2660v4 cores. Verification was disabled for these simulations.</p></sec><sec id="s4-8"><title>Validity of results</title><p>It is important that simulation results are valid and reliable. Results could be inaccurate either due to bugs in user-specified models or in PyDDM itself. Due to the ability to easily construct user-defined models, PyDDM takes this issue very seriously. In addition to utilizing standard software engineering practices such as unit and integration testing with continuous integration (<xref ref-type="bibr" rid="bib5">Beck Andres, 2004</xref>), PyDDM is the first neuroscience software to our knowledge which incorporates software verification methods for ensuring program correctness (<xref ref-type="bibr" rid="bib28">Ghezzi et al., 2002</xref>). It does so using a library which checks for invalid inputs and outputs within the code, and also checks for invalid inputs and outputs in user-defined models which do not explicitly incorporate the library (<xref ref-type="bibr" rid="bib65">Shinn, 2020</xref>). Invalid inputs and outputs include anything which invalidates the assumptions of the GDDM, such as initial conditions outside of the bounds, noise levels less than zero, or probability distributions which do not sum to 1. Such checking helps ensure that results generated by PyDDM are not due to a configuration or programming mistake, which is especially important because user-specified models logic can quickly become complicated for intricate experimental designs. This, combined with the ease of reproducibility and code reuse afforded by open source software in general, means that high validity and reliability are key advantages of PyDDM.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>This research was supported by NIH grant R01MH112746 (JDM), the Gruber Foundation (MS), and NSERC (NHL). We thank Robert Yang for his contribution to the analytic solver, and Daeyeol Lee and Hyojung Seo for helpful discussions. We also acknowledge the authors of <xref ref-type="bibr" rid="bib61">Roitman and Shadlen, 2002</xref> and <xref ref-type="bibr" rid="bib23">Evans and Hawkins, 2019</xref> for making their datasets publicly available.</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Software, Formal analysis, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Supervision, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-56938-transrepform-v2.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>The two analyzed datasets, which have been previously published, are both is publicly available for download: 1. Roitman &amp; Shadlen, 2002, J Neurosci: <ext-link ext-link-type="uri" xlink:href="https://shadlenlab.columbia.edu/resources/RoitmanDataCode.html">https://shadlenlab.columbia.edu/resources/RoitmanDataCode.html</ext-link> 2. Evans &amp; Hawkins, 2019, Cognition: <ext-link ext-link-type="uri" xlink:href="https://osf.io/2vnam/">https://osf.io/2vnam/</ext-link>.</p><p>The following previously published datasets were used:</p><p><element-citation id="dataset1" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Roitman</surname><given-names>JD</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2002">2002</year><data-title>Data from: Roitman and Shadlen (2002)</data-title><source>Shadlen Lab website</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://shadlenlab.columbia.edu/resources/RoitmanDataCode.html">RoitmanDataCode</pub-id></element-citation></p><p><element-citation id="dataset2" publication-type="data" specific-use="references"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>NJ</given-names></name><name><surname>Hawkins</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2019">2019</year><data-title>Data from: Evans and Hawkins (2019)</data-title><source>Open Science Framework</source><pub-id assigning-authority="Open Science Framework" pub-id-type="accession" xlink:href="https://osf.io/2vnam/">2vnam</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexandrowicz</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The diffusion model visualizer: an interactive tool to understand the diffusion model parameters</article-title><source>Psychological Research</source><volume>84</volume><fpage>1157</fpage><lpage>1165</lpage><pub-id pub-id-type="doi">10.1007/s00426-018-1112-6</pub-id><pub-id pub-id-type="pmid">30361811</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>TW</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>A modification of the sequential probability ratio test to reduce the sample size</article-title><source>The Annals of Mathematical Statistics</source><volume>31</volume><fpage>165</fpage><lpage>197</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177705996</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashby</surname> <given-names>FG</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>A biased random walk model for two choice reaction times</article-title><source>Journal of Mathematical Psychology</source><volume>27</volume><fpage>277</fpage><lpage>297</lpage><pub-id pub-id-type="doi">10.1016/0022-2496(83)90011-1</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Atiya</surname> <given-names>NAA</given-names></name><name><surname>Zgonnikov</surname> <given-names>A</given-names></name><name><surname>O'Hora</surname> <given-names>D</given-names></name><name><surname>Schoemann</surname> <given-names>M</given-names></name><name><surname>Scherbaum</surname> <given-names>S</given-names></name><name><surname>Wong-Lin</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Changes-of-mind in the absence of new post-decision evidence</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1007149</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1007149</pub-id><pub-id pub-id-type="pmid">32012147</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><collab>Beck Andres</collab></person-group><year iso-8601-date="2004">2004</year><source>Extreme Programming Explained</source><publisher-name>Addison Wesley</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blurton</surname> <given-names>SP</given-names></name><name><surname>Kesselmeier</surname> <given-names>M</given-names></name><name><surname>Gondan</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The first-passage time distribution for the diffusion model with variable drift</article-title><source>Journal of Mathematical Psychology</source><volume>76</volume><fpage>7</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.11.003</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehm</surname> <given-names>U</given-names></name><name><surname>Annis</surname> <given-names>J</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Heathcote</surname> <given-names>A</given-names></name><name><surname>Kellen</surname> <given-names>D</given-names></name><name><surname>Krypotos</surname> <given-names>A-M</given-names></name><name><surname>Lerche</surname> <given-names>V</given-names></name><name><surname>Logan</surname> <given-names>GD</given-names></name><name><surname>Palmeri</surname> <given-names>TJ</given-names></name><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Servant</surname> <given-names>M</given-names></name><name><surname>Singmann</surname> <given-names>H</given-names></name><name><surname>Starns</surname> <given-names>JJ</given-names></name><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Estimating across-trial variability parameters of the diffusion decision model: expert advice and recommendations</article-title><source>Journal of Mathematical Psychology</source><volume>87</volume><fpage>46</fpage><lpage>75</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2018.09.004</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>E</given-names></name><name><surname>Moehlis</surname> <given-names>J</given-names></name><name><surname>Holmes</surname> <given-names>P</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Smith</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Evaluating methods for approximating stochastic differential equations</article-title><source>Journal of Mathematical Psychology</source><volume>50</volume><fpage>402</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2006.03.004</pub-id><pub-id pub-id-type="pmid">18574521</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunton</surname> <given-names>BW</given-names></name><name><surname>Botvinick</surname> <given-names>MM</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Rats and humans can optimally accumulate evidence for decision-making</article-title><source>Science</source><volume>340</volume><fpage>95</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1126/science.1233912</pub-id><pub-id pub-id-type="pmid">23559254</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname> <given-names>C</given-names></name><name><surname>Hawkins</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>ChaRTr: an R toolbox for modeling choices and response times in decision-making tasks</article-title><source>Journal of Neuroscience Methods</source><volume>328</volume><elocation-id>108432</elocation-id><pub-id pub-id-type="doi">10.1016/j.jneumeth.2019.108432</pub-id><pub-id pub-id-type="pmid">31586868</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Decision-making with multiple alternatives</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>693</fpage><lpage>702</lpage><pub-id pub-id-type="doi">10.1038/nn.2123</pub-id><pub-id pub-id-type="pmid">18488024</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname> <given-names>P</given-names></name><name><surname>Puskas</surname> <given-names>GA</given-names></name><name><surname>El-Murr</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decisions in changing conditions: the urgency-gating model</article-title><source>Journal of Neuroscience</source><volume>29</volume><fpage>11560</fpage><lpage>11571</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1844-09.2009</pub-id><pub-id pub-id-type="pmid">19759303</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ditterich</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006a</year><article-title>Evidence for time-variant decision making</article-title><source>European Journal of Neuroscience</source><volume>24</volume><fpage>3628</fpage><lpage>3641</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.05221.x</pub-id><pub-id pub-id-type="pmid">17229111</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ditterich</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006b</year><article-title>Stochastic models of decisions about motion direction: behavior and physiology</article-title><source>Neural Networks</source><volume>19</volume><fpage>981</fpage><lpage>1012</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2006.05.042</pub-id><pub-id pub-id-type="pmid">16952441</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Churchland</surname> <given-names>AK</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The cost of accumulating evidence in perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>3612</fpage><lpage>3628</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4010-11.2012</pub-id><pub-id pub-id-type="pmid">22423085</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>DeAngelis</surname> <given-names>GC</given-names></name><name><surname>Klier</surname> <given-names>EM</given-names></name><name><surname>Angelaki</surname> <given-names>DE</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Optimal multisensory decision-making in a reaction-time task</article-title><source>eLife</source><volume>3</volume><elocation-id>e03005</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03005</pub-id><pub-id pub-id-type="pmid">24929965</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014b</year><chapter-title>Optimal decision-making with time-varying evidence reliability</chapter-title><person-group person-group-type="editor"><name><surname>Ghahramani</surname> <given-names>Z</given-names></name><name><surname>Welling</surname> <given-names>M</given-names></name><name><surname>Cortes</surname> <given-names>C</given-names></name><name><surname>Weinberger</surname> <given-names>K. Q</given-names></name><name><surname>Lawrence</surname> <given-names>N. D</given-names></name></person-group><source>Advances in Neural Information Processing Systems</source><volume>27</volume><publisher-name>Curran Associates, Inc</publisher-name><fpage>748</fpage><lpage>756</lpage></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name><name><surname>Moreno-Bote</surname> <given-names>R</given-names></name><name><surname>Pouget</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014c</year><article-title>Relation between belief and performance in perceptual decision making</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e96511</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0096511</pub-id><pub-id pub-id-type="pmid">24816801</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Fast and accurate monte carlo sampling of first-passage times from Wiener Diffusion models</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>20490</elocation-id><pub-id pub-id-type="doi">10.1038/srep20490</pub-id><pub-id pub-id-type="pmid">26864391</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erlich</surname> <given-names>JC</given-names></name><name><surname>Brunton</surname> <given-names>BW</given-names></name><name><surname>Duan</surname> <given-names>CA</given-names></name><name><surname>Hanks</surname> <given-names>TD</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct effects of prefrontal and parietal cortex inactivations on an accumulation of evidence task in the rat</article-title><source>eLife</source><volume>4</volume><elocation-id>e05457</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.05457</pub-id><pub-id pub-id-type="pmid">25869470</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>NJ</given-names></name><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Boehm</surname> <given-names>U</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The computations that support simple decision-making: a comparison between the diffusion and urgency-gating models</article-title><source>Scientific Reports</source><volume>7</volume><elocation-id>16433</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-017-16694-7</pub-id><pub-id pub-id-type="pmid">29180789</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname> <given-names>NJ</given-names></name><name><surname>Hawkins</surname> <given-names>GE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>When humans behave like monkeys: feedback delays and extensive practice increase the efficiency of speeded decisions</article-title><source>Cognition</source><volume>184</volume><fpage>11</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2018.11.014</pub-id><pub-id pub-id-type="pmid">30553935</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farashahi</surname> <given-names>S</given-names></name><name><surname>Ting</surname> <given-names>CC</given-names></name><name><surname>Kao</surname> <given-names>CH</given-names></name><name><surname>Wu</surname> <given-names>SW</given-names></name><name><surname>Soltani</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dynamic combination of sensory and reward information under time pressure</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1006070</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006070</pub-id><pub-id pub-id-type="pmid">29584717</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Sequential sampling models in cognitive neuroscience: advantages, applications, and extensions</article-title><source>Annual Review of Psychology</source><volume>67</volume><fpage>641</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1146/annurev-psych-122414-033645</pub-id><pub-id pub-id-type="pmid">26393872</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freyer</surname> <given-names>F</given-names></name><name><surname>Roberts</surname> <given-names>JA</given-names></name><name><surname>Ritter</surname> <given-names>P</given-names></name><name><surname>Breakspear</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>A canonical model of multistability and scale-invariance in biological systems</article-title><source>PLOS Computational Biology</source><volume>8</volume><elocation-id>e1002634</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1002634</pub-id><pub-id pub-id-type="pmid">22912567</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gard</surname> <given-names>TC</given-names></name></person-group><year iso-8601-date="1988">1988</year><source>Introduction to Stochastic Differential Equations. Monographs and Text-Books in Pure and Applied Mathematics</source><publisher-name>Dekker, Inc</publisher-name></element-citation></ref><ref id="bib28"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Ghezzi</surname> <given-names>C</given-names></name><name><surname>Jazayeri</surname> <given-names>M</given-names></name><name><surname>Mandrioli</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Fundamentals of Software Engineering</source><publisher-name>Pearson</publisher-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname> <given-names>JI</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grasman</surname> <given-names>RPPP</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name><name><surname>van der Maas</surname> <given-names>HLJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>On the mean and variance of response times under the diffusion model with an application to parameter estimation</article-title><source>Journal of Mathematical Psychology</source><volume>53</volume><fpage>55</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2009.01.006</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname> <given-names>TD</given-names></name><name><surname>Mazurek</surname> <given-names>ME</given-names></name><name><surname>Kiani</surname> <given-names>R</given-names></name><name><surname>Hopp</surname> <given-names>E</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Elapsed decision time affects the weighting of prior probability in a perceptual decision task</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>6339</fpage><lpage>6352</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5613-10.2011</pub-id><pub-id pub-id-type="pmid">21525274</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Harty</surname> <given-names>S</given-names></name><name><surname>Murphy</surname> <given-names>PR</given-names></name><name><surname>Robertson</surname> <given-names>IH</given-names></name><name><surname>O'Connell</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parsing the neural signatures of reduced error detection in older age</article-title><source>NeuroImage</source><volume>161</volume><fpage>43</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2017.08.032</pub-id><pub-id pub-id-type="pmid">28811254</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2015">2015a</year><article-title>Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making</article-title><source>Journal of Neuroscience</source><volume>35</volume><fpage>2476</fpage><lpage>2484</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2410-14.2015</pub-id><pub-id pub-id-type="pmid">25673842</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>GE</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name></person-group><year iso-8601-date="2015">2015b</year><article-title>Discriminating evidence accumulation from urgency signals in speeded decision making</article-title><source>Journal of Neurophysiology</source><volume>114</volume><fpage>40</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1152/jn.00088.2015</pub-id><pub-id pub-id-type="pmid">25904706</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname> <given-names>WR</given-names></name><name><surname>Trueblood</surname> <given-names>JS</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Bayesian analysis of the piecewise diffusion decision model</article-title><source>Behavior Research Methods</source><volume>50</volume><fpage>730</fpage><lpage>743</lpage><pub-id pub-id-type="doi">10.3758/s13428-017-0901-y</pub-id><pub-id pub-id-type="pmid">28597236</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huk</surname> <given-names>AC</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Neural activity in macaque parietal cortex reflects temporal integration of visual motion signals during perceptual decision making</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>10420</fpage><lpage>10436</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4684-04.2005</pub-id><pub-id pub-id-type="pmid">16280581</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Laming</surname> <given-names>DRJ</given-names></name></person-group><year iso-8601-date="1968">1968</year><source>Information Theory of Choice-Reaction Times</source><publisher-name>Academic Press</publisher-name><pub-id pub-id-type="doi">10.1111/j.2044-8317.1969.tb00423.x</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerche</surname> <given-names>V</given-names></name><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Nagler</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How many trials are required for parameter estimation in diffusion modeling? A comparison of different optimization criteria</article-title><source>Behavior Research Methods</source><volume>49</volume><fpage>513</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.3758/s13428-016-0740-2</pub-id><pub-id pub-id-type="pmid">27287445</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lerche</surname> <given-names>V</given-names></name><name><surname>Voss</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Model complexity in diffusion modeling: benefits of making the model more parsimonious</article-title><source>Frontiers in Psychology</source><volume>7</volume><elocation-id>1324</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2016.01324</pub-id><pub-id pub-id-type="pmid">27679585</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Lewis</surname> <given-names>M</given-names></name><name><surname>Fedor</surname> <given-names>A</given-names></name><name><surname>Öllinger</surname> <given-names>M</given-names></name><name><surname>Szathmáry</surname> <given-names>E</given-names></name><name><surname>Fernando</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2014">2014</year><chapter-title>Modelling reaction times in non-linear classification tasks</chapter-title><person-group person-group-type="editor"><name><surname>del Pobil</surname> <given-names>A. P</given-names></name><name><surname>Chinellato</surname> <given-names>E</given-names></name><name><surname>Martinez-Martin</surname> <given-names>E</given-names></name><name><surname>Hallam</surname> <given-names>J</given-names></name><name><surname>Cervera</surname> <given-names>E</given-names></name><name><surname>Morales</surname> <given-names>A</given-names></name></person-group><source>From Animals to Animats 13</source><publisher-name>Springer International Publishing</publisher-name><fpage>53</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1007/978-3-319-08864-8_6</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname> <given-names>Z</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Immersed interface methods for moving interface problems</article-title><source>Numerical Algorithms</source><volume>14</volume><fpage>269</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1023/A:1019173215885</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Luke</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><source>Essentials of Metaheuristics</source><publisher-name>Lulu</publisher-name></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname> <given-names>G</given-names></name><name><surname>Leslie</surname> <given-names>DS</given-names></name><name><surname>Ludwig</surname> <given-names>CJH</given-names></name><name><surname>Bogacz</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Time-varying decision boundaries: insights from optimality analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>971</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1340-6</pub-id><pub-id pub-id-type="pmid">28730465</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>McKerns</surname> <given-names>MM</given-names></name><name><surname>Strand</surname> <given-names>L</given-names></name><name><surname>Sullivan</surname> <given-names>T</given-names></name><name><surname>Fang</surname> <given-names>A</given-names></name><name><surname>Aivazis</surname> <given-names>MAG</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Building a framework for predictive science</article-title><conf-name>Proceedings of the 10th Python in Science Conference</conf-name></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>GH</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>The numerical solution of stefan problems with front-tracking and smoothing methods</article-title><source>Applied Mathematics and Computation</source><volume>4</volume><fpage>283</fpage><lpage>306</lpage><pub-id pub-id-type="doi">10.1016/0096-3003(78)90001-2</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Millner</surname> <given-names>AJ</given-names></name><name><surname>Gershman</surname> <given-names>SJ</given-names></name><name><surname>Nock</surname> <given-names>MK</given-names></name><name><surname>den Ouden</surname> <given-names>HEM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Pavlovian control of escape and avoidance</article-title><source>Journal of Cognitive Neuroscience</source><volume>30</volume><fpage>1379</fpage><lpage>1390</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01224</pub-id><pub-id pub-id-type="pmid">29244641</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mulder</surname> <given-names>MJ</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Boekel</surname> <given-names>W</given-names></name><name><surname>Forstmann</surname> <given-names>BU</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Bias in the brain: a diffusion model analysis of prior probability and potential payoff</article-title><source>Journal of Neuroscience</source><volume>32</volume><fpage>2335</fpage><lpage>2343</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4156-11.2012</pub-id><pub-id pub-id-type="pmid">22396408</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname> <given-names>PR</given-names></name><name><surname>Boonstra</surname> <given-names>E</given-names></name><name><surname>Nieuwenhuis</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Global gain modulation generates time-dependent urgency during perceptual choice in humans</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>13526</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms13526</pub-id><pub-id pub-id-type="pmid">27882927</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelder</surname> <given-names>JA</given-names></name><name><surname>Mead</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1965">1965</year><article-title>A simplex method for function minimization</article-title><source>The Computer Journal</source><volume>7</volume><fpage>308</fpage><lpage>313</lpage><pub-id pub-id-type="doi">10.1093/comjnl/7.4.308</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nguyen</surname> <given-names>KP</given-names></name><name><surname>Josić</surname> <given-names>K</given-names></name><name><surname>Kilpatrick</surname> <given-names>ZP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Optimizing sequential decisions in the drift-diffusion model</article-title><source>Journal of Mathematical Psychology</source><volume>88</volume><fpage>32</fpage><lpage>47</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2018.11.001</pub-id><pub-id pub-id-type="pmid">31564753</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Nocedal</surname> <given-names>J</given-names></name><name><surname>Wright</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><source>Numerical Optimization</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/b98874</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ossmy</surname> <given-names>O</given-names></name><name><surname>Moran</surname> <given-names>R</given-names></name><name><surname>Pfeffer</surname> <given-names>T</given-names></name><name><surname>Tsetsos</surname> <given-names>K</given-names></name><name><surname>Usher</surname> <given-names>M</given-names></name><name><surname>Donner</surname> <given-names>TH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The timescale of perceptual evidence integration can be adapted to the environment</article-title><source>Current Biology</source><volume>23</volume><fpage>981</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.039</pub-id><pub-id pub-id-type="pmid">23684972</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pinto</surname> <given-names>L</given-names></name><name><surname>Koay</surname> <given-names>SA</given-names></name><name><surname>Engelhard</surname> <given-names>B</given-names></name><name><surname>Yoon</surname> <given-names>AM</given-names></name><name><surname>Deverett</surname> <given-names>B</given-names></name><name><surname>Thiberge</surname> <given-names>SY</given-names></name><name><surname>Witten</surname> <given-names>IB</given-names></name><name><surname>Tank</surname> <given-names>DW</given-names></name><name><surname>Brody</surname> <given-names>CD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>An Accumulation-of-Evidence task using visual pulses for mice navigating in virtual reality</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>12</volume><elocation-id>36</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00036</pub-id><pub-id pub-id-type="pmid">29559900</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Purcell</surname> <given-names>BA</given-names></name><name><surname>Heitz</surname> <given-names>RP</given-names></name><name><surname>Cohen</surname> <given-names>JY</given-names></name><name><surname>Schall</surname> <given-names>JD</given-names></name><name><surname>Logan</surname> <given-names>GD</given-names></name><name><surname>Palmeri</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Neurally constrained modeling of perceptual decision making</article-title><source>Psychological Review</source><volume>117</volume><fpage>1113</fpage><lpage>1143</lpage><pub-id pub-id-type="doi">10.1037/a0020311</pub-id><pub-id pub-id-type="pmid">20822291</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Theoretical interpretations of the speed and accuracy of positive and negative responses</article-title><source>Psychological Review</source><volume>92</volume><fpage>212</fpage><lpage>225</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.92.2.212</pub-id><pub-id pub-id-type="pmid">3991839</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The EZ diffusion method: too EZ?</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>15</volume><fpage>1218</fpage><lpage>1228</lpage><pub-id pub-id-type="doi">10.3758/PBR.15.6.1218</pub-id><pub-id pub-id-type="pmid">19001593</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Smith</surname> <given-names>PL</given-names></name><name><surname>Brown</surname> <given-names>SD</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Diffusion decision model: Current issues and history</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>260</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2016.01.007</pub-id><pub-id pub-id-type="pmid">26952739</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Rouder</surname> <given-names>JN</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Modeling response times for two-choice decisions</article-title><source>Psychological Science</source><volume>9</volume><fpage>347</fpage><lpage>356</lpage><pub-id pub-id-type="doi">10.1111/1467-9280.00067</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Tuerlinckx</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Estimating parameters of the diffusion model: approaches to dealing with contaminant reaction times and parameter variability</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>9</volume><fpage>438</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.3758/BF03196302</pub-id><pub-id pub-id-type="pmid">12412886</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roitman</surname> <given-names>JD</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Response of neurons in the lateral intraparietal area during a combined visual discrimination reaction time task</article-title><source>The Journal of Neuroscience</source><volume>22</volume><fpage>9475</fpage><lpage>9489</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.22-21-09475.2002</pub-id><pub-id pub-id-type="pmid">12417672</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roxin</surname> <given-names>A</given-names></name><name><surname>Ledberg</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Neurobiological models of two-choice decision making can be reduced to a one-dimensional nonlinear diffusion equation</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000046</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000046</pub-id><pub-id pub-id-type="pmid">18369436</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Scott</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Multivariate Density Estimation</source><publisher-name>John Wiley &amp; Sons</publisher-name><pub-id pub-id-type="doi">10.1002/9781118575574</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinn</surname> <given-names>M</given-names></name><name><surname>Ehrlich</surname> <given-names>D</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Murray</surname> <given-names>JD</given-names></name><name><surname>Seo</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Confluence of timing and reward biases in perceptual decision-making dynamics</article-title><source>The Journal of Neuroscience</source><elocation-id>JN-RM-0544-20</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0544-20.2020</pub-id><pub-id pub-id-type="pmid">32839233</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shinn</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Refinement type contracts for verification of scientific investigative software</chapter-title><person-group person-group-type="editor"><name><surname>Chakraborty</surname> <given-names>S</given-names></name><name><surname>Navas</surname> <given-names>J. A</given-names></name></person-group><source>Verified Software. Theories, Tools, and Experiments</source><publisher-loc>Cham</publisher-loc><publisher-name>Springer International Publishing</publisher-name><fpage>143</fpage><lpage>160</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-03592-1</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simen</surname> <given-names>P</given-names></name><name><surname>Balci</surname> <given-names>F</given-names></name><name><surname>de Souza</surname> <given-names>L</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Holmes</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A model of interval timing by neural integration</article-title><source>Journal of Neuroscience</source><volume>31</volume><fpage>9238</fpage><lpage>9253</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3121-10.2011</pub-id><pub-id pub-id-type="pmid">21697374</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname> <given-names>PL</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Stochastic dynamic models of response time and accuracy: a foundational primer</article-title><source>Journal of Mathematical Psychology</source><volume>44</volume><fpage>408</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1006/jmps.1999.1260</pub-id><pub-id pub-id-type="pmid">10973778</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Srivastava</surname> <given-names>V</given-names></name><name><surname>Feng</surname> <given-names>SF</given-names></name><name><surname>Cohen</surname> <given-names>JD</given-names></name><name><surname>Leonard</surname> <given-names>NE</given-names></name><name><surname>Shenhav</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A martingale analysis of first passage times of time-dependent Wiener diffusion models</article-title><source>Journal of Mathematical Psychology</source><volume>77</volume><fpage>94</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2016.10.001</pub-id><pub-id pub-id-type="pmid">28630524</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Storn</surname> <given-names>R</given-names></name><name><surname>Price</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Differential evolution - a simple and efficient heuristic for global optimization over continuous spaces</article-title><source>Journal of Global Optimization</source><volume>11</volume><fpage>341</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1023/A:1008202821328</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tavares</surname> <given-names>G</given-names></name><name><surname>Perona</surname> <given-names>P</given-names></name><name><surname>Rangel</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The attentional drift diffusion model of simple perceptual Decision-Making</article-title><source>Frontiers in Neuroscience</source><volume>11</volume><elocation-id>468</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2017.00468</pub-id><pub-id pub-id-type="pmid">28894413</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname> <given-names>D</given-names></name><name><surname>Beauregard-Racine</surname> <given-names>J</given-names></name><name><surname>Fradet</surname> <given-names>CW</given-names></name><name><surname>Cisek</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decision making by urgency gating: theory and experimental support</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>2912</fpage><lpage>2930</lpage><pub-id pub-id-type="doi">10.1152/jn.01071.2011</pub-id><pub-id pub-id-type="pmid">22993260</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname> <given-names>D</given-names></name><name><surname>Cisek</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deliberation and commitment in the premotor and primary motor cortex during dynamic decision making</article-title><source>Neuron</source><volume>81</volume><fpage>1401</fpage><lpage>1416</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.01.031</pub-id><pub-id pub-id-type="pmid">24656257</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsetsos</surname> <given-names>K</given-names></name><name><surname>Chater</surname> <given-names>N</given-names></name><name><surname>Usher</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Salience driven value integration explains decision biases and preference reversal</article-title><source>PNAS</source><volume>109</volume><fpage>9659</fpage><lpage>9664</lpage><pub-id pub-id-type="doi">10.1073/pnas.1119569109</pub-id><pub-id pub-id-type="pmid">22635271</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Usher</surname> <given-names>M</given-names></name><name><surname>McClelland</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The time course of perceptual choice: the leaky, competing accumulator model</article-title><source>Psychological Review</source><volume>108</volume><fpage>550</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.108.3.550</pub-id><pub-id pub-id-type="pmid">11488378</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Donkin</surname> <given-names>C</given-names></name><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The EZ diffusion model provides a powerful test of simple empirical effects</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>24</volume><fpage>547</fpage><lpage>556</lpage><pub-id pub-id-type="doi">10.3758/s13423-016-1081-y</pub-id><pub-id pub-id-type="pmid">27352898</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Ravenzwaaij</surname> <given-names>D</given-names></name><name><surname>Oberauer</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>How to use the diffusion model: parameter recovery of three methods: ez, fast-dm, and DMAT</article-title><source>Journal of Mathematical Psychology</source><volume>53</volume><fpage>463</fpage><lpage>473</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2009.09.004</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name><name><surname>Matzke</surname> <given-names>D</given-names></name><name><surname>Wagenmakers</surname> <given-names>E-J</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Model Comparison and the Principle of Parsimony</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oxfordhb/9780199957996.013.14</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name><name><surname>Tuerlinckx</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Diffusion model analysis with MATLAB: a DMAT primer</article-title><source>Behavior Research Methods</source><volume>40</volume><fpage>61</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.3758/BRM.40.1.61</pub-id><pub-id pub-id-type="pmid">18411528</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Verdonck</surname> <given-names>S</given-names></name><name><surname>Meers</surname> <given-names>K</given-names></name><name><surname>Tuerlinckx</surname> <given-names>F</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Efficient simulation of diffusion-based choice RT models on CPU and GPU</article-title><source>Behavior Research Methods</source><volume>48</volume><fpage>13</fpage><lpage>27</lpage><pub-id pub-id-type="doi">10.3758/s13428-015-0569-0</pub-id><pub-id pub-id-type="pmid">25761391</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Voss</surname> <given-names>J</given-names></name><name><surname>Lerche</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Assessing cognitive processes with diffusion model analyses: a tutorial based on fast-dm-30</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>336</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00336</pub-id><pub-id pub-id-type="pmid">25870575</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Voss</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Fast-dm: a free program for efficient diffusion model analysis</article-title><source>Behavior Research Methods</source><volume>39</volume><fpage>767</fpage><lpage>775</lpage><pub-id pub-id-type="doi">10.3758/BF03192967</pub-id><pub-id pub-id-type="pmid">18183889</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voss</surname> <given-names>A</given-names></name><name><surname>Voss</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A fast numerical algorithm for the estimation of diffusion model parameters</article-title><source>Journal of Mathematical Psychology</source><volume>52</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2007.09.005</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>van der Maas</surname> <given-names>HL</given-names></name><name><surname>Grasman</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>An EZ-diffusion model for response time and accuracy</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>14</volume><fpage>3</fpage><lpage>22</lpage><pub-id pub-id-type="doi">10.3758/BF03194023</pub-id><pub-id pub-id-type="pmid">17546727</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Ratcliff</surname> <given-names>R</given-names></name><name><surname>Gomez</surname> <given-names>P</given-names></name><name><surname>McKoon</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008a</year><article-title>A diffusion model account of criterion shifts in the lexical decision task</article-title><source>Journal of Memory and Language</source><volume>58</volume><fpage>140</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2007.04.006</pub-id><pub-id pub-id-type="pmid">19122740</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>van der Maas</surname> <given-names>HL</given-names></name><name><surname>Dolan</surname> <given-names>CV</given-names></name><name><surname>Grasman</surname> <given-names>RP</given-names></name></person-group><year iso-8601-date="2008">2008b</year><article-title>EZ does it! extensions of the EZ-diffusion model</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>15</volume><fpage>1229</fpage><lpage>1235</lpage><pub-id pub-id-type="doi">10.3758/PBR.15.6.1229</pub-id><pub-id pub-id-type="pmid">19001594</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wald</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1945">1945</year><article-title>Sequential tests of statistical hypotheses</article-title><source>The Annals of Mathematical Statistics</source><volume>16</volume><fpage>117</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177731118</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wales</surname> <given-names>DJ</given-names></name><name><surname>Doye</surname> <given-names>JPK</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Global optimization by Basin-Hopping and the lowest energy structures of Lennard-Jones clusters containing up to 110 atoms</article-title><source>The Journal of Physical Chemistry A</source><volume>101</volume><fpage>5111</fpage><lpage>5116</lpage><pub-id pub-id-type="doi">10.1021/jp970984n</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiecki</surname> <given-names>TV</given-names></name><name><surname>Sofer</surname> <given-names>I</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>HDDM: Hierarchical bayesian estimation of the Drift-Diffusion model in Python</article-title><source>Frontiers in Neuroinformatics</source><volume>7</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2013.00014</pub-id><pub-id pub-id-type="pmid">23935581</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wong</surname> <given-names>KF</given-names></name><name><surname>Wang</surname> <given-names>XJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A recurrent network mechanism of time integration in perceptual decisions</article-title><source>Journal of Neuroscience</source><volume>26</volume><fpage>1314</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3733-05.2006</pub-id><pub-id pub-id-type="pmid">16436619</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname> <given-names>S</given-names></name><name><surname>Lee</surname> <given-names>MD</given-names></name><name><surname>Vandekerckhove</surname> <given-names>J</given-names></name><name><surname>Maris</surname> <given-names>G</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Time-varying boundaries for diffusion models of decision making and response time</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>1364</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01364</pub-id><pub-id pub-id-type="pmid">25538642</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhou</surname> <given-names>X</given-names></name><name><surname>Wong-Lin</surname> <given-names>K</given-names></name><name><surname>Philip</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Time-varying perturbations can distinguish among integrate-to-threshold models for perceptual decision making in reaction time tasks</article-title><source>Neural Computation</source><volume>21</volume><fpage>2336</fpage><lpage>2362</lpage><pub-id pub-id-type="doi">10.1162/neco.2009.07-08-817</pub-id><pub-id pub-id-type="pmid">19416080</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Zoltowski</surname> <given-names>DM</given-names></name><name><surname>Pillow</surname> <given-names>JW</given-names></name><name><surname>Linderman</surname> <given-names>SW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Unifying and generalizing models of neural dynamics during decision-making</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2001.04571">https://arxiv.org/abs/2001.04571</ext-link></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zylberberg</surname> <given-names>A</given-names></name><name><surname>Fetsch</surname> <given-names>CR</given-names></name><name><surname>Shadlen</surname> <given-names>MN</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The influence of evidence volatility on choice, reaction time and confidence in a perceptual decision</article-title><source>eLife</source><volume>5</volume><elocation-id>e17688</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.17688</pub-id><pub-id pub-id-type="pmid">27787198</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec id="s8" sec-type="appendix"><title>Implementing and fitting models in PyDDM</title><boxed-text><p>Briefly, we describe the process of building and fitting a model in PyDDM. For more detailed information, see the quickstart guide in the documentation at <ext-link ext-link-type="uri" xlink:href="https://pyddm.readthedocs.io/en/latest/quickstart.html">https://pyddm.readthedocs.io/en/latest/quickstart.html</ext-link>. A familiarity with Python and with Python classes is assumed.</p><p>PyDDM is based on a modular architecture, organized around the principles of the GDDM. There are five components, specified using Python classes, which correspond to the five functions listed in section ‘GDDM description’:</p><list list-type="bullet"><list-item><p>Drift: the drift rate. May depend on time, decision variable position, task conditions, and fittable parameters. Defaults to 0.</p></list-item><list-item><p>Noise: the standard deviation of diffusion process. May depend on time, decision variable position, fittable parameters, and task conditions. Defaults to 1.</p></list-item><list-item><p>Bound: The integration bounds (+B and -B). May depend on time, fittable parameters, and task conditions. Defaults to 1.</p></list-item><list-item><p>IC: The initial conditions at time t = 0, in terms of a probability density function over decision variable position. May depend on fittable parameters or task conditions. Defaults to a probability density function of a Kronecker delta function centered at <inline-formula><mml:math id="inf109"><mml:mrow><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>.</p></list-item><list-item><p>Overlay: Post-factum modifications to the histogram, for instance to create a mixture model or add a non-decision time. May depend on fittable parameters or task conditions. Defaults to an identity function.</p></list-item></list><p>These model components do not specify a single value for these parameters, but rather specify a function through which these values can be computed. Each may take any number of parameters and utilize any number of task variables. Most models will not implement all of these components from scratch, but will use the default component or a built-in component for most of them. For example, the GDDM in <xref ref-type="fig" rid="fig2">Figure 2</xref> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>) used the default ‘Noise’ and ‘IC’ components, built-in Bounds and Overlay components, and a customized ‘Drift’ component. A full list of built-in components, as well as a library of example components, can be found in the online documentation.</p><p>Each of these model components may depend on parameters; for example, the ‘Bound’ component requires the parameters <inline-formula><mml:math id="inf110"><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula> and τ. These parameters must be specified when the class for the model component is instantiated, for example, ‘BoundCollapsingExponential’ is the class for exponential collapsing bounds used in the GDDM in <xref ref-type="fig" rid="fig2">Figure 2</xref> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>). It can be instantiated (for <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>B</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) as</p><p><monospace>BoundCollapsingExponential(B0 = 2, tau = 3)</monospace></p><p>Of course, we would often like to fit these parameters to data, and thus, we cannot specify their exact values when instantiating a component. For these situations, we use an instance of a ‘Fittable’ class as a placeholder. Minimum and maximum values must be specified for bounded fitting algorithms, such as differential evolution. For example, we could fit both of these parameters using</p><p><monospace>BoundCollapsingExponential(B0 = Fittable(minval = 0.5, maxval = 3), tau = Fittable(minval = 0.1, maxval = 10))</monospace></p><p>Components which are not built-in to PyDDM may be specified by defining a class which gives four pieces of information: the component’s name, the parameters it depends on, the task conditions it depends on, and a function to compute its value. For example, here is ‘DriftCoherenceLeak’, the ‘Drift’ class used in our GDDM in <xref ref-type="fig" rid="fig2">Figure 2</xref> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>):<code xml:space="preserve">class DriftCoherenceLeak (ddm.models.Drift): 
    name = &quot;Leaky drift depends nonlinearly on coherence&quot; 
    # Parameters we want to include in the model 
    required_parameters = [&quot;driftcoh&quot;, &quot;leak&quot;, &quot;power&quot;, &quot;maxcoh&quot;] 
    # Task parameters/conditions. Should be the same name as in the sample. 
    required_conditions = [&quot;coh&quot;] 
    # The get_drift function is used to compute the instantaneous value of drift. 
    def get_drift(self, x, conditions, ∗∗kwargs): 
        return self.driftcoh ∗ (conditions[&quot;coh&quot;]/self.maxcoh)∗∗self.power + self.leak ∗ x</code></p><p>Once we have determined all of the components of our model, we may tie them together using an instance of the ‘Model’ class. When creating an instance of the ‘Model’ class, we must also define our timestep ‘dt’, grid spacing ‘dx’, and simulation duration ‘T_dur’. Shown below is the GDDM in <xref ref-type="fig" rid="fig2">Figure 2</xref> (<xref ref-type="disp-formula" rid="equ13">Equation 13</xref>):<code xml:space="preserve">m=Model(name=&quot;Roitman-Shadlen GDDM&quot;, 
         drift = DriftCoherenceLeak(driftcoh = Fittable(minval = 0, maxval = 20),
                                                    leak = Fittable(minval=-10, maxval = 10)), 
         # Do not specify noise=… since we want the default 
         # Do not specify IC=… since we want the default
          bound = BoundCollapsingExponential(B = Fittable(minval = 0.5, maxval = 3),
                                                                                        tau = Fittable(minval = 0.0001, maxval = 5)), 
         # OverlayChain strings together multiple overlays. 
         Overlay = OverlayChain(overlays=[
                                 OverlayNonDecision(nondectime = Fittable(minval = 0, maxval = 0.4)),
                                  OverlayUniformMixture(umixturecoef = 0.05)]), 
          dx = 0.001, dt = 0.001, T_dur = 2)</code></p><p>If the model ‘m’ includes at least one component with a ‘Fittable’ instance, we must first fit the model to data by calling the ‘fit_adjust_model’ function, passing our model and dataset as arguments. By default, it fits using differential evolution (<xref ref-type="bibr" rid="bib69">Storn and Price, 1997</xref>), however Nelder-Mead (<xref ref-type="bibr" rid="bib49">Nelder and Mead, 1965</xref>), BFGS (<xref ref-type="bibr" rid="bib51">Nocedal and Wright, 2006</xref>), basin hopping (<xref ref-type="bibr" rid="bib87">Wales and Doye, 1997</xref>), and hill climbing (<xref ref-type="bibr" rid="bib42">Luke, 2013</xref>) are also available. Fitting a model in parallel using <italic>n</italic> cores of a CPU is as simple as calling the function ‘set_N_cpus(<italic>n</italic>)’ before fitting the model. Once we load our sample ‘samp’ using one of a number of convenience functions for loading data, we may fit the model:<code xml:space="preserve">set_N_cpus(4) 
fit_adjust_model(m, sample = samp)</code></p><p>After fitting the model to data, the ‘Fittable’ instances are automatically replaced by their fitted values, so the model can be directly simulated using ‘m.solve()’. If any of the model components depend on task conditions, these must be passed as a dictionary to the solver; for example, since the ‘DriftCoherenceLeak’ component depends on ‘coh’ (the trial’s coherence), we must instead call</p><p><monospace>m.solve(conditions={‘coh’: 0.064})</monospace> to simulate with a coherence of 0.064. Alternatively, the function ‘solve_partial_conditions’ can simulate across many task conditions simultaneously. Both the functions ‘m.solve’ and ‘solve_partial_conditions’ will return an instance of the ‘Solution’ class, which has methods for obtaining the correct and error RT distributions as well as useful summary statistics.</p></boxed-text></sec></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56938.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Ding</surname><given-names>Long</given-names> </name><role>Reviewer</role><aff><institution>University of Pennsylvania</institution><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Wagenmakers</surname><given-names>Eric-Jan</given-names> </name><role>Reviewer</role><aff><institution>University of Amsterdam</institution><country>Netherlands</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>Drift diffusion models are widely used in psychology and neuroscience research. However, diffusion model applications are often constraint by the available software rather than the scientific question that ought to be asked. Thus, having a model and associated software package that provides more flexible model fits will be of great value for the scientific community.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;A flexible framework for simulating and fitting generalized drift-diffusion models&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by Joshua Gold as the Senior Editor, a Reviewing Editor, and three reviewers. The following individuals involved in review of your submission have agreed to reveal their identity: Long Ding (Reviewer #1); Eric-Jan Wagenmakers (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>We would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). Specifically, we are asking editors to accept without delay manuscripts, like yours, that they judge can stand as <italic>eLife</italic> papers without additional data, even if they feel that they would make the manuscript stronger. Thus, the revisions requested below only address clarity and presentation.</p><p>Summary:</p><p>Reviewers were generally impressed with this work as it presents a substantial step forward, providing a much-needed, flexible, and efficient way to fit choice and reaction time data common to many decision behaviors. Reviewers mostly made a number of suggestions for how to improve the manuscript. However, they also identified two essential issues that would need to be adequately addressed in a revised version. Specifically, it would be important to apply the models to a different data set and to consider additional criteria for model evaluation.</p><p>Essential revisions:</p><p>1) As explained in reviewer #2's major comment 2, the RT distribution in the Roitman and Shadlen data set is not very representative. Reviewers agree that the authors should include an addition, more representative data set for the current paper. In particular, reviewers suggested the following data set: Wagenmakers et al., (2008). These data are available at http://www.ejwagenmakers.com/Code/2008/LexDecData.zip</p><p>2) As detailed in reviewer #2's major comment 3, and reviewer #3 major comment 3, given the flexibility of the model, it is unclear whether goodness-of-fit is the only or even the most appropriate criteria to evaluate different models.</p><p><italic>Reviewer #1:</italic></p><p>This work provides a much-needed, flexible, and efficient way to fit choice and reaction time data common to many decision behaviors.</p><p>The detailed methods in the section &quot;Fokker-Plank formalism of the GDDM&quot; are outside my expertise. But the superior performance of the GDDM compared to other packages, both in accuracy and execution time, is very convincing and rigorously demonstrated. The writing is also very clear. I expect that the software package will be widely used in the field. This is a rare instance when I support publication as is.</p><p><italic>Reviewer #2:</italic></p><p>In general I am truly impressed with this work. It present a substantial step forward, and I strongly believe a revision should be published. At the same time, I also believe the manuscript can be greatly strengthened along the lines provided below.</p><p>Essential revisions:</p><p>1) &quot;Fifth, single trial parameter variability is supported within the GDDM framework for both starting point and non-decision time.&quot;</p><p>Initially, I was not sure what the authors mean with &quot;single trial parameter variability&quot;. Do they mean &quot;across-trial variability&quot;? And if so, does their methodology not allow for across-trial variability in drift rate, as the full DDM does? If the GDDM indeed does not allow such variability, this warrants explicit mention and an extensive discussion.</p><p>Then when I read the Discussion section the cat comes out of the bag: &quot;While the GDDM framework supports individual trial variability in non-decision time and starting position according to any arbitrary distribution, it does not support drift rate variability. As a result, the GDDM framework does not support the entire &quot;full DDM&quot;.&quot;</p><p>But this is a severe limitation, one that needs to be acknowledged and emphasized from the start. Variability in drift rate is a highly plausible mechanism that has been an integral part of the DDM since 1978. Without this mechanism to account for (fairly ubiquitous) slow errors, the model will have to turn to other mechanism to accomplish this, such as moving-in bounds. The authors mention &quot;However, the GDDM framework is broad enough to support an infinite number of extensions which create the same RT features which originally motivated inclusion of drift rate variability.&quot; But the goal of the modeling is not to fit the data well; the actual goal is to decompose the observed data into meaningful psychological processes, and a process of moving-in bounds is fundamentally different from drift-rate variability. I assume that the drift rate variability frustrates the mathematical work? The reader needs to know the reason why the model cannot handle drift rate variability.</p><p>Staying on the same point, the authors then point out that &quot;Indeed, in Figure 2, our 5-parameter GDDM provided a better fit to the asymmetry between correct and error RT distributions than the 11-parameter &quot;full DDM&quot;.&quot; True, but since these data are highly unrepresentative (see the next point below), this can just as well be used as an argument *against* the GDDM: apparently it is able to fit just about any pattern of data, even patterns that are highly unrepresentative.</p><p>2) The demonstration where the models are fit to the data is (deeply) problematic, because the monkey data are simply not representative. As can be seen from the grey histograms in Figure 2, the &quot;regular&quot; DDMs predict a long-tailed RT distribution, which is however not apparent from the data. This then creates ambiguity: yes, one can show that the existing models do not fit well, but they are fit to unrepresentative data. If these data patterns were regularly observed, the DDM would never have been proposed in the first place. It would be much more compelling to fit the models to any of the large benchmark data sets that are publicly available – these data sets all show a pronounced right skew.</p><p>3) As the progenitor of the much-maligned &quot;EZ&quot; model, I guess I am obliged to protest a little. Yes, of course EZ is not flexible, and does not have all the bells and whistles. But that is kind of the point of a simple model. There are some demonstrations that (contrary to my own expectations even) EZ actually does better than the full diffusion model when applied to sparse data. I am not talking about goodness-of-fit, but about recovering the ground truth in a simulation study (e.g., figuring out which parameters are the same and which differ). For sparse data, a simple model may work better. One reference:</p><p>van Ravenzwaaij, Donkin and Vandekerckhove, (2017).</p><p><italic>Reviewer #3:</italic></p><p>The manuscript introduces a generalized drift-diffusion model, together with the modular software package PyDDM to fit it. The generalized drift-diffusion model is a set of accumulation-to-bound models that feature highly flexible components, such as time- and location-varying drifts, variable starting points, time-varying decision boundaries, etc. The manuscript describes how to compute first-passage time distributions for this set of models by numerically and efficiently solving the Fokker-Plank Equation (FPE), of which PyDDM provides an implementation, together with a set of optimizers that can be used to adjust the model's parameters to behavioral data.</p><p>Having a model and associated software package that provides more flexible model fits is very welcome and timely, as diffusion model applications are frequently constraint by the available software rather than the scientific question that ought to be asked. PyDDM promises to provide such a package, and appears to be sufficiently well document to ease uptake by the community. Thus, it would be a welcome addition to the currently available diffusion model fitting packages.</p><p>Some concerns should be addressed before the manuscript can be published:</p><p>1) The manuscript frequently describes the numerical solution of the FPE a &quot;simulation&quot;, starting with the title, the introduction, the main text, etc. Commonly, solving the FPE is not considered a simulation, such that describing it as such is potentially confusing misnomer – in particular as there exist simulation-based &quot;likelihood-free&quot; methods to fit models solely based on simulating them. First, I urge the authors to not call solving FPEs a &quot;simulation&quot;, but instead describe their approach as numerically propagating the probability mass, which can intuitively be thought of as performing an infinite number of simulations. Second, the authors should acknowledge likelihood-free fitting methods, like ABC (see, for example, Holmes and Trueblood, 2018 and related) to fit diffusion models – subsection “Methods for simulating the DDM and GDDM” would be a good starting point.</p><p>2) The precision and the clarity of the manuscript can be improved in many instances:</p><p>- It should be made clear that GDDMs only support Markovian processes + some additional post-processing of the computed first-passage time distributions. This is, for example, the reason why they don't support continuously varying drift rates, but this isn't at all clear from the manuscript. An additional (current) limitation is that the implementation currently only supports boundaries that vary symmetrically around zero, even though this appears to be a limitation of the provided implementation rather than a fundamental limit to the chosen approach.</p><p>- The manuscript frequently claims that the solution is &quot;continuous&quot; (e.g., subsection “Methods for simulating the DDM and GDDM”, subsection “Software package for GDDM fitting”) in time, even though they only provide time-discretized numerical results.</p><p>- It isn't sufficiently clear from the manuscript that all provided FPE solutions are approximate, as they discretize time and space. Currently, the manuscript only highlights the approximate nature of the forward Euler scheme (subsection “Execution time vs error tradeoff”). Indeed, it has worse numerical properties, but that doesn't make the implicit schemes non-approximate.</p><p>3) The manuscript highlights a better fit (i.e., higher likelihood, better BIC, etc.) of a DDM with time-varying boundaries and a leak, and uses this fit (of a single dataset) to re-iterate throughout the manuscript that a better model fit with fewer parameters is always more desirable (e.g., subsection “Software package for GDDM fitting”). This mis-characterizes the modeling exercise, in which the structure and presence of different components of DDMs might be as important as the quality of the fit.</p><p>4) The authors highlight three approaches to &quot;simulate&quot; DDMs and GDDMs, but seem to be missing a fourth based on solving Volterra integral equation, as described by Smith, (2000) and implemented by the 'dm' package by Drugowitsch et al., – which is fairly flexible, but unfortunately only provides computing the first-passage times, but no model fitting code.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.56938.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) As explained in reviewer #2's major comment 2, the RT distribution in the Roitman and Shadlen data set is not very representative. Reviewers agree that the authors should include an addition, more representative data set for the current paper. In particular, reviewers suggested the following data set: Wagenmakers et al., (2008). These data are available at http://www.ejwagenmakers.com/Code/2008/LexDecData.zip</p></disp-quote><p>We thank the reviewers for this helpful suggestion, which we think will better contextualize the role of GDDMs in working with empirical datasets. We had utilized the Roitman-Shadlen dataset as an example testbed to demonstrate how GDDMs could be fit to empirical dataset to test hypothesized mechanisms (there, collapsing bounds and leaky integration, as motivated by prior studies). We agree that RT distributions in this dataset are not representative of many experiments from human subjects, and therefore that also fitting models to a representative human dataset is an important addition to the paper.</p><p>We thank reviewer 2 for the suggestion of the publicly available dataset from Wagenmakers et al., (2008a). However, after surveying other publicly available datasets, we decided that it is not most appropriate for this analysis, especially to serve as a comparison to Roitman and Shadlen, (2002). The mechanisms we focused on in this manuscript are those related to integration of evidence extended over time. In the dataset of Wagenmakers et al., (2008a), the task was to determine whether a string of characters formed a word or not, and collapsing bounds and leaky integration do not have clear interpretations as cognitive strategies for this task paradigm (see our response to Essential revision major point 2).</p><p>To address the reviewer’s concern, we sought to include a more representative dataset from human subjects which would also be more comparable in task design to the monkey experiment of Roitman and Shadlen, (2002). Here we were primarily inspired by the recent publication of Hawkins et al., (2015a) (“Revisiting the evidence for collapsing boundaries and urgency signals in perceptual decision-making”), which analyzed multiple studies from monkeys and human subjects performing perceptual decision-making tasks and specifically investigated evidence for collapsing bounds. Overall, they found that most monkey datasets better fit by collapsing bounds than fixed bounds, but that most human datasets were better fit by fixed bounds. Hawkins et al., (2015a) suggested these differences may arise from task design and training differences, as well as species differences.</p><p>In our revised manuscript, we now use publicly available data from the study of Evans and Hawkins, (2019) (“When humans behave like monkeys: Feedback delays and extensive practice increase the efficiency of speeded decisions”), which tested the Roitman-Shadlen task paradigm in human subjects, and manipulated feedback timing and amount of practice. The dominant effect they found was in feedback timing: having zero delay between response and feedback yielded RTs that were best fit by no or little collapse of bounds (similar to the human studies analyzed by Hawkins et al., (2015a)); whereas a 1-s delay in feedback (as in Roitman and Shadlen, (2002)) yielded RTs that were best fit by collapsing bounds (similar to the monkey studies analyzed by Hawkins et al. (2015a)). We therefore selected for our GDDM analysis the zero-delay trials from Evans and Hawkins, (2019), which the authors found are more representative of human datasets.</p><p>Using the two publicly available datasets of (i) Roitman and Shadlen, (2002) and (ii) Evans and Hawkins, (2019) as testbeds for our GDDM framework has several advantages. They span monkey and human studies. They use very similar random dot motion perceptual decision-making task paradigms. They allow us to test the same GDDM (i.e., with collapsing bounds, leaky integration, and input nonlinearity) across datasets. Finally, they provide benchmarks for comparison to PyDDM through different model-fitting methodologies: Hawkins et al., (2015a) found that the Roitman and Shadlen, (2002) dataset was better fit with collapsing bounds, whereas Evans and Hawkins, (2019) found that their dataset was best fit with no or little collapse.</p><p>We found that in the Evans-Hawkins dataset, the pre-specified GDDM did not perform better than the other models, but was more or less comparable. This contrast between the Roitman-Shadlen and EvansHawkins datasets nicely illustrates how our GDDM framework can be applied to test hypothesized mechanisms with empirical datasets, with example results that are consistent with prior literature. We now include a new Figure 2—figure supplement 1 on the Evans-Hawkins dataset, in parallel format to Figure 2.</p><p>We note that EZ-Diffusion performed especially poorly on the Evans-Hawkins dataset; in particular, the non-decision time was fit to be less than zero. We confirmed this was the case using two separate implementations of EZ-Diffusion: the javascript implementation, and the implementation built in to the HDDM Python package. The reason for this poor performance appears to be largely due to the inclusion of long RTs. We mirrored Evans and Hawkins, (2019) in using a cutoff of 7 seconds for the RT distribution. The high cutoff leads to a more skewed distribution with larger variance and a relatively smaller mean.</p><p>We introduced Figure 2—figure supplement 3 which describes the Evans-Hawkins dataset in a similar form as the Roitman-Shadlen dataset:</p><p>We added the following to describe this figure in the Results section:</p><p>&quot;The RT distributions produced by monkeys may not be representative of those produced by humans (e.g., Hawkins et al., 2015a), which show a characteristic skewed distribution. […] This lack of improved fit by the GDDM suggests that the human subjects employed a cognitive strategy closer to the standard DDM, and that the specific mechanisms of this GDDM (collapsing bounds, leaky integration, and input nonlinearity) are not import for explaining psychophysical behavior in this dataset.&quot;</p><p>Additionally, we updated the Materials and methods section to include information about how we fit these datasets, as well as a new subsection briefly describing the two datasets and our rationales for their selection:</p><p>&quot;As testbeds for fitting different models, we used two empirical datasets of choices and RTs during perceptual decision-making tasks: one from Roitman and Shadlen, (2002), one from Evans and Hawkins, (2019). These references contain full details of the tasks and datasets. For the dataset of Roitman and Shadlen, (2002), we used trials from the ‘‘reaction time’’ task variant, for both monkeys. For the dataset of Evans and Hawkins, (2019), we used trials from the task variant with no feedback delay.&quot;</p><p>&quot;In brief, these two publicly available datasets were selected for the following reasons. […] These two datasets therefore provided useful testbeds for GDDM fitting with distinct <italic>a priori</italic> predictions about the utility of collapsing bounds in a GDDM.&quot;</p><disp-quote content-type="editor-comment"><p>2) As detailed in reviewer #2's Essential revision 3, and reviewer #3 Essential revision 3, given the flexibility of the model, it is unclear whether goodness-of-fit is the only or even the most appropriate criteria to evaluate different models.</p></disp-quote><p>We thank the reviewers for pointing out to us that our intention of the manuscript and the role of the GDDM were not sufficiently clear. Our intention appears to be in line with that of the reviewers. Our manuscript previously gave the impression that the best feature of the GDDM was its ability to fit datasets more accurately and with fewer parameters than other models. However, we stress that our goal is not to suggest that a GDDM (let alone the specific one demonstrated here) is generically the most appropriate model. Furthermore, we agree that the goal of modeling is not simply to fit data with the least error and/or fewest parameters. Our goal with this study is to provide a flexible, powerful, and accessible framework which (i) makes it easy to test new hypothesized cognitive mechanisms by fitting them to data, and (ii) to model task paradigms with time-varying evidence, which are difficult or impossible within the classic DDM framework. We used the Roitman-Shadlen dataset as an example testbed to demonstrate how hypothesized mechanisms can be fit to empirical data and how evidence for a mechanism can be informed by improvement in model fit. This is now further complemented by inclusion of the Evans-Hawkins dataset.</p><p>We have done the following to highlight these changes and clarify how GDDMs can be used in the scientific process. First, we have slightly changed the specific GDDM which is used in the context of the empirical datasets in section “GDDM mechanisms characterize empirical data”. Since our goal is to highlight the possibilities of using GDDMs in general, rather than one specific GDDM with collapsing bounds and leaky integration, we added another feature/parameter to the model which does not substantially improve fit but highlights another attractive feature of the GDDM: a fittable coherence nonlinearity. While this adds an extra parameter (the exponent of the nonlinearity), it allows us to demonstrate another practical and interesting feature of GDDMs.</p><p>We focus on this new description of the model within the Results section. The first paragraph of the subsection “GDDM mechanisms characterize empirical data” was changed to the following:</p><p>&quot;The cognitive mechanisms by which evidence is used to reach a decision is still an open question. […] However, consistent with previous results (Hawkins et al., 2015a), the GDDM fit no better than the DDM or ‘‘full DDM’’ in the human dataset, suggesting that different cognitive strategies may be used in each case.&quot;</p><p>As a result of these changes, we change the subsection title to the following: &quot;GDDM mechanisms characterize empirical data&quot;</p><p>We have added the following paragraph to the Results section:</p><p>&quot;For the example datasets considered above, the GDDM presented here improved the fit to the monkey data and provided a good fit to the human data. We caution that this result does not necessarily mean that this particular GDDM is appropriate for any particular future study. Models should not only provide a good fit to data, but also be parsimonious in form and represent a clear mechanistic interpretation (Vandekerckhove et al., 2015). The examples here demonstrate the utility of GDDMs for quantitatively evaluating cognitive mechanisms of interest through fitting models to empirical data.&quot;</p><p>We added the following paragraph to the Discussion section:</p><p>&quot;PyDDM enables researchers to build very complex models of decision-making processes, but complex models are not always desirable. Highly complex models may be able to provide an excellent fit to the data, but unless they represent specific, plausible cognitive mechanisms, they may not be appropriate. PyDDM’s complexity can be used to examine a range of potential cognitive mechanisms which were previously difficult to test, and to probe a range of task paradigms.&quot;</p><p>Furthermore, we have removed the passages of the Roitman-Shadlen part of the Results section which imply that goodness of fit is more critical than the interpretability of the model over the goodness of fit.</p><disp-quote content-type="editor-comment"><p>Reviewer #2:</p><p>Essential revisions:</p><p>1) &quot;Fifth, single trial parameter variability is supported within the GDDM framework for both starting point and non-decision time.&quot;</p><p>Initially, I was not sure what the authors mean with &quot;single trial parameter variability&quot;. Do they mean &quot;across-trial variability&quot;? And if so, does their methodology not allow for across-trial variability in drift rate, as the full DDM does? If the GDDM indeed does not allow such variability, this warrants explicit mention and an extensive discussion.</p><p>Then when I read the Discussion section the cat comes out of the bag: &quot;While the GDDM framework supports individual trial variability in non-decision time and starting position according to any arbitrary distribution, it does not support drift rate variability. As a result, the GDDM framework does not support the entire &quot;full DDM&quot;.&quot;</p><p>But this is a severe limitation, one that needs to be acknowledged and emphasized from the start. Variability in drift rate is a highly plausible mechanism that has been an integral part of the DDM since 1978. Without this mechanism to account for (fairly ubiquitous) slow errors, the model will have to turn to other mechanism to accomplish this, such as moving-in bounds. The authors mention &quot;However, the GDDM framework is broad enough to support an infinite number of extensions which create the same RT features which originally motivated inclusion of drift rate variability.&quot; But the goal of the modeling is not to fit the data well; the actual goal is to decompose the observed data into meaningful psychological processes, and a process of moving-in bounds is fundamentally different from drift-rate variability. I assume that the drift rate variability frustrates the mathematical work? The reader needs to know the reason why the model cannot handle drift rate variability.</p><p>Staying on the same point, the authors then point out that &quot;Indeed, in Figure 2, our 5-parameter GDDM provided a better fit to the asymmetry between correct and error RT distributions than the 11-parameter &quot;full DDM&quot;.&quot; True, but since these data are highly unrepresentative (see the next point below), this can just as well be used as an argument *against* the GDDM: apparently it is able to fit just about any pattern of data, even patterns that are highly unrepresentative.</p></disp-quote><p>We thank the reviewer for drawing our attention to the importance of supporting the entire “full DDM”. Fokker-Planck methods, in general, do not readily support true across-trial variability in drift rate, and therefore this constitutes a limitation of the Fokker-Planck approach. Each numerical solution of the Fokker-Planck equation must be performed for a specified drift rate which can be a function of <italic>x</italic> and <italic>t</italic> but cannot account for across-trial variability of drift rate. As a result, PyDDM does not offer native support for across-trial drift rate variability.</p><p>However, it is possible to approximate across-trial drift rate variability within the Fokker-Planck approach by discretizing the drift rate’s probability distribution, running a Fokker-Planck numerical solution for each drift rate separately, and then combining the results to yield an estimation of RT distributions. As the reviewer pointed out, this feature would be of interest to users.</p><p>In response to the reviewer’s concern, and to address this use case, we have now added an example to the PyDDM online documentation demonstrating how to implement across-trial drift rate variability:</p><p><ext-link ext-link-type="uri" xlink:href="https://pyddm.readthedocs.io/en/latest/cookbook/driftnoise.html#across-trial-variability-in-drift-rate">https://p</ext-link>yddm.readthedocs.io/en/latest/cookbook/driftnoise.html#across-trial-variability-indrift-rate</p><p>We note that this support for across-trial drift rate variability is not without cost, due to limitations of the Fokker-Plank approach. In particular, this will cause simulations to be slow depending on the discretization of the drift rate distribution (due to repeated numerical solves of Fokker-Planck), and will interfere with the ability to automatically compute model likelihood. Thus, as the reviewer points out, it must be made clear to the reader that PyDDM does not have “first class” support for this feature, although it is implementable.</p><p>In summary, the original manuscript contained the following elements to highlight the fact that PyDDM does not support drift rate variability:</p><p>Mentioned it explicitly in the subsection “GDDM as a generalization of the DDM”.</p><p>– Highlighted this as a major negative for PyDDM (a “red cell”) in the table comparing software packages.</p><p>– Showed this in the diagram in Figure 1 by not drawing a distribution around the arrow (as we did for the “full DDM”).</p><p>– Included a paragraph in the Discussion section about it.</p><p>– Discussed technical aspects of not fitting the “full DDM” in the Materials and methods section.</p><p>Following the reviewer’s suggestion, we have changed our phrasing of “single-trial variability” to “across-trial variability” throughout the manuscript.</p><p>In addition to the elements listed above which were already in the manuscript, we made the following changes to further emphasize the fact that PyDDM does not natively support across-trial drift rate variability:</p><p>- Within the figure caption of Figure 1, we explicitly pointed out the meaning of the lack of a distribution around the arrow representing drift. The relevant portion of the caption now reads:</p><p>&quot;The GDDM is a generalization of the DDM framework which allows arbitrary distributions for starting position and non-decision time, as well as arbitrary functions (instead of distributions) for drift rate and collapsing bounds.&quot;</p><p>- We change the text within the table to read &quot;slow discretization (via extension)&quot; under the “Across-trial drift variability” section</p><p>- We rewrote the paragraph in the Discussion section to be the following, to better explain our GDDM framework vis-`a-vis across-trial drift rate variability:&quot;</p><p>The GDDM is a generalization of the DDM framework which allows arbitrary distributions for starting position and non-decision time, as well as arbitrary functions (instead of distributions) for drift rate and collapsing bounds.&quot;</p><p>- We change the text within the table to read &quot;slow discretization (via extension)&quot; under the “Across-trial drift variability” section</p><p>- We rewrote the paragraph in the Discussion section to be the following, to better explain our GDDM framework vis-`a-vis across-trial drift rate variability:</p><p>&quot;The GDDM framework supports across-trial variability in non-decision time and starting position according to any arbitrary distribution, but it does not support across-trial drift rate variability. […] GDDMs offer researchers the capability to explore alternative cognitive mechanisms which might also account for slow errors.&quot;</p><disp-quote content-type="editor-comment"><p>2) The demonstration where the models are fit to the data is (deeply) problematic, because the monkey data are simply not representative. As can be seen from the grey histograms in Figure 2, the &quot;regular&quot; DDMs predict a long-tailed RT distribution, which is however not apparent from the data. This then creates ambiguity: yes, one can show that the existing models do not fit well, but they are fit to unrepresentative data. If these data patterns were regularly observed, the DDM would never have been proposed in the first place. It would be much more compelling to fit the models to any of the large benchmark data sets that are publicly available – these data sets all show a pronounced right skew.</p></disp-quote><p>We thank the reviewer for this important comment. Please see our response above to Essential revision point 2, which covers this point in depth.</p><disp-quote content-type="editor-comment"><p>3) As the progenitor of the much-maligned &quot;EZ&quot; model, I guess I am obliged to protest a little. Yes, of course EZ is not flexible, and does not have all the bells and whistles. But that is kind of the point of a simple model. There are some demonstrations that (contrary to my own expectations even) EZ actually does better than the full diffusion model when applied to sparse data. I am not talking about goodness-of-fit, but about recovering the ground truth in a simulation study (e.g., figuring out which parameters are the same and which differ). For sparse data, a simple model may work better. One reference:</p><p>van Ravenzwaaij, Donkinand Vandekerckhove, (2017).</p></disp-quote><p>We thank the reviewer for pointing out the inadequacy of our discussion of the merits of EZ-Diffusion, especially its utility in the case of sparse data.</p><p>We have added the following passage to the Discussion section:</p><p>&quot;Additionally, sometimes complex models which do represent appropriate cognitive mechanisms may exhibit inferior performance due to limitations of the available data. For instance, simulation studies suggest that when data are sparse, Bayesian fitting or EZ-Diffusion may be better able to recover parameters (Lerche et al., 2017; Wiecki et al., 2013; van Ravenzwaaij et al., 2017).&quot;</p><disp-quote content-type="editor-comment"><p>Reviewer #3:</p><p>Some concerns should be addressed before the manuscript can be published:</p><p>1) The manuscript frequently describes the numerical solution of the FPE a &quot;simulation&quot;, starting with the title, the introduction, the main text, etc. Commonly, solving the FPE is not considered a simulation, such that describing it as such is potentially confusing misnomer – in particular as there exist simulation-based &quot;likelihood-free&quot; methods to fit models solely based on simulating them. First, I urge the authors to not call solving FPEs a &quot;simulation&quot;, but instead describe their approach as numerically propagating the probability mass, which can intuitively be thought of as performing an infinite number of simulations.</p></disp-quote><p>We agree with the reviewer that the term “simulation”, which usually refers to trial-wise trajectory simulations or sampling, is not the ideal terminology to use for Fokker-Planck. We agree that it is better to be technically precise with our language, to be clearer that we are not performing Monte Carlo simulations. As a result, throughout the main text we now use more precise language to describe the numerical Fokker-Planck solutions, describing propagation of the probability, rather than “simulation” which we use for Monte Carlo trajectories.</p><p>However, after careful consideration we have opted to keep the word “simulating” in the title, to accommodate the brevity and clarity of common usage required there. After considering different options, we believe using “simulating” in the title best communicates to a typical reader, in succinct and accessible language appropriate for a title, the generation of RT distributions for a specified GDDM. To the typical reader who would like to fit parameters of a GDDM to empirical data or estimate an RT distribution, it is not important whether it is through Monte Carlo trajectories or another method. We also note that usage of “simulation” of an RT distribution is arguably not technically incorrect, because the result of Fokker-Planck is the distribution of simulations as N → ∞. Likewise, by discretely sampling from the distribution produced by Fokker-Planck, it is indeed possible to use our framework for fast and efficient simulation of sets of RTs (albeit without originating trajectories of the decision variable).</p><p>Within the body text, we have replaced every instance of “simulate” with more descriptive and/or precise terminology when it refers to either Fokker-Planck, or collectively to both Fokker-Planck and trial-wise trajectory simulations. In most cases, this involved replacing “simulate” with “solve”. Overall, depending on the context, we made one of three substitutions: “solve”, when referring to Fokker-Planck or analytical solutions; “estimate the RT distribution”, when referring to the more general case which encompasses both Fokker-Planck and trial-wise trajectory simulation; and “post-factum modifications”, to replace “post-simulation modifications” (in reference to the “Overlay” object). As a result of these changes, the term “simulate” refers exclusively to trial-wise trajectory simulations in all but three places: the title, its first occurrence in the Abstract, and in one instance in the Introduction section where we specify the usage.</p><p>In order to ensure this does not cause confusion, we furthermore added the following to the Introduction section:</p><p>&quot;Solving the Fokker-Planck equation is analogous to simulating an infinite number of individual trials.&quot;</p><p>Second, the authors should acknowledge likelihood-free fitting methods, like ABC (see, for example, Holmes and Trueblood, 2018 and related) to fit diffusion models – subsection “Methods for simulating the DDM and GDDM” would be a good starting point.</p><p>We have changed the suggested line as follows:</p><p>&quot;Since this method does not produce a probability distribution, it is difficult to use efficient and robust methods such as full-distribution maximum likelihood for fitting data, though progress is ongoing in this area using likelihood-free methods such as through approximate Bayesian computation (Holmes and Trueblood, 2018).&quot;</p><disp-quote content-type="editor-comment"><p>2) The precision and the clarity of the manuscript can be improved in many:</p></disp-quote><p>We thank the reviewer for these comments. We agree that because language in the manuscript has heavily emphasized readability for typical target users, mathematical precision may be under-emphasized in the main text at times.</p><disp-quote content-type="editor-comment"><p>- It should be made clear that GDDMs only support Markovian processes + some additional post-processing of the computed first-passage time distributions. This is, for example, the reason why they don't support continuously varying drift rates, but this isn't at all clear from the manuscript.</p></disp-quote><p>The reviewer is correct in that we do not support non-Markovian processes, for instance mechanisms related to momentum and inertia.</p><p>We have added the following text to clarify this point:</p><p>&quot;The GDDM framework supports Markovian processes, in that the instantaneous drift rate and diffusion constant of a particle is determined by its position <italic>xt</italic> and the current time <italic>t</italic>, without dependence on the prior trajectory leading to that state. Drift rate is discretized, such that the timestep of the discretization is equal to the timestep of the Fokker-Planck solution. In practice, for typical task paradigms and dataset properties, the impact of this discretization is expected to be minimal for a reasonable timestep size.&quot;</p><p>An additional (current) limitation is that the implementation currently only supports boundaries that vary symmetrically around zero, even though this appears to be a limitation of the provided implementation rather than a fundamental limit to the chosen approach.</p><p>The reviewer is correct in that PyDDM currently only supports bounds which are symmetric around zero, that this is due to the implementation rather than the methodology, and that adding asymmetric bounds is a feasible future addition.</p><p>We have added the following text to clarify this point:</p><p>&quot;Second, while PyDDM currently only implements bounds which are symmetric around zero, support for asymmetric bounds is a feasible future addition.&quot;</p><p>- The manuscript frequently claims that the solution is &quot;continuous&quot; (e.g., subsection “Methods for simulating the DDM and GDDM”, subsection “Software package for GDDM fitting”) in time, even though they only provide time-discretized numerical results.</p><p>Our use of the word “continuous” in the manuscript overwhelmingly refers to “continuous maximum likelihood”, a term borrowed from Heathcote, Brown and Mewhort, (2002). It is not sufficient to describe our fitting procedure as “maximum likelihood”, because this can refer to a number of fitting procedures, such as quantile maximum likelihood (an approximation to likelihood based on data quantiles; Heathcote, Brown and Mewhort, 2002), or to the maximum likelihood derivations which utilize only summary statistics of the RT distribution rather than all of the data points (e.g. Palmer et al., 2005).</p><p>However, we also see the reviewer’s point that our use of the term “continuous” could be confusing in this context.</p><p>We have replaced all references to “continuous maximum likelihood” with “full-distribution maximum likelihood”. We have also confirmed that no other uses of the word “continuous” refer to a discrete approximation.</p><p>- It isn't sufficiently clear from the manuscript that all provided FPE solutions are approximate, as they discretize time and space. Currently, the manuscript only highlights the approximate nature of the forward Euler scheme (subsection “Execution time vs error tradeoff”). Indeed, it has worse numerical properties, but that doesn't make the implicit schemes non-approximate.</p><p>We thank the reviewer for pointing out our asymmetric use of the term “approximation”.</p><p>We updated the referenced lines to say the following:</p><p>&quot;…forward Euler iteratively approximates the probability distribution of trajectory position at each timestep using the distribution at the previous timestep, while backward Euler and Crank-Nicolson iteratively solve systems of linear equations to approximate the probability distribution (Voss and Voss, 2008).&quot;</p><disp-quote content-type="editor-comment"><p>3) The manuscript highlights a better fit (i.e., higher likelihood, better BIC, etc.) of a DDM with time-varying boundaries and a leak, and uses this fit (of a single dataset) to re-iterate throughout the manuscript that a better model fit with fewer parameters is always more desirable (e.g., subsection “Software package for GDDM fitting”). This mis-characterizes the modeling exercise, in which the structure and presence of different components of DDMs might be as important as the quality of the fit.</p></disp-quote><p>We thank the reviewer for pointing out that our description of model fitting could be misleading. For a full description of how this point was addressed, see our response above to Essential revisions point 2.</p><disp-quote content-type="editor-comment"><p>4) The authors highlight three approaches to &quot;simulate&quot; DDMs and GDDMs, but seem to be missing a fourth based on solving Volterra integral equation, as described by Smith, (2000) and implemented by the 'dm' package by Drugowitsch et al., – which is fairly flexible, but unfortunately only provides computing the first-passage times, but no model fitting code.</p></disp-quote><p>We thank the reviewer for bringing to our attention that we have not properly discussed the method of Smith, (2000).</p><p>We have modified the passage describing the “three methods” for solving the GDDM such that the method of Smith, (2000) is incorporated into the third method. The text is now as follows:</p><p>&quot;A third and better method is to iteratively propagate the distribution forward in time, which is achieved by solving the Fokker-Planck equation (Voss and Voss, 2008) or using the method of Smith, (2000). These methods allow the RT distribution of GDDMs to be estimated with better performance and lower error than trial-wise trajectory simulation. However, they are difficult to implement, which has thus far impeded their widespread use in cognitive psychology and neuroscience.&quot;</p><p>We also added the following passage to the Discussion section to bring attention to this fourth method and its relationship to PyDDM:</p><p>&quot;PyDDM uses numerical solutions of the Fokker-Planck equation to estimate the RT distribution of the model. Other methods may be used for this estimation, such as solving the Volterra integral equation (Smith, 2000). Some prior work has used these alternative methods for solving GDDMs (Drugowitsch et al., 2012; Zhang et al., 2014; Drugowitsch et al., 2014). Such methods already have fast solvers (Drugowitsch, 2016), which could be implemented as an alternative method within PyDDM in the future.&quot;</p></body></sub-article></article>