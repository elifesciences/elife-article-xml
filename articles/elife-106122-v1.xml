<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">106122</article-id><article-id pub-id-type="doi">10.7554/eLife.106122</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.106122.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Correlation detection as a stimulus computable account for audiovisual perception, causal inference, and saliency maps in mammals</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name><surname>Parise</surname><given-names>Cesare V</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0000-6092-561X</contrib-id><email>cesare.parise@liverpool.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xs57h96</institution-id><institution>Department of Psychology, Institute of Population Health, Faculty of Health and Life Sciences, University of Liverpool</institution></institution-wrap><addr-line><named-content content-type="city">Liverpool</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Noel</surname><given-names>Jean-Paul</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/017zqws13</institution-id><institution>University of Minnesota</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>University of Pennsylvania</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>04</day><month>11</month><year>2025</year></pub-date><volume>14</volume><elocation-id>RP106122</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-01-31"><day>31</day><month>01</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-02-04"><day>04</day><month>02</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.29.573621"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-05-13"><day>13</day><month>05</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106122.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-09-03"><day>03</day><month>09</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.106122.2"/></event></pub-history><permissions><copyright-statement>© 2025, Parise</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Parise</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-106122-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-106122-figures-v1.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.90841" id="ra1"/><abstract><p>Animals excel at seamlessly integrating information from different senses, a capability critical for navigating complex environments. Despite recent progress in multisensory research, the absence of stimulus-computable perceptual models fundamentally limits our understanding of how the brain extracts and combines task-relevant cues from the continuous flow of natural multisensory stimuli. Here, we introduce an image- and sound-computable population model for audiovisual perception, based on biologically plausible units that detect spatiotemporal correlations across auditory and visual streams. In a large-scale simulation spanning 69 psychophysical, eye-tracking, and pharmacological experiments, our model replicates human, monkey, and rat behaviour in response to diverse audiovisual stimuli with an average correlation exceeding 0.97. Despite relying on as few as 0–4 free parameters, our model provides an end-to-end account of audiovisual integration in mammals—from individual pixels and audio samples to behavioural responses. Remarkably, the population response to natural audiovisual scenes generates saliency maps that predict spontaneous gaze direction, Bayesian causal inference, and a variety of previously reported multisensory illusions. This study demonstrates that the integration of audiovisual stimuli, regardless of their spatiotemporal complexity, can be accounted for in terms of elementary joint analyses of luminance and sound level. Beyond advancing our understanding of the computational principles underlying multisensory integration in mammals, this model provides a bio-inspired, general-purpose solution for multimodal machine perception.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>multisensory integration</kwd><kwd>audiovisual perception</kwd><kwd>psychophysics</kwd><kwd>eye-tracking</kwd><kwd>Bayesian causal inference</kwd><kwd>computational neuroscience</kwd><kwd>saliency maps</kwd><kwd>stimulus computable model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Rat</kwd><kwd>Rhesus macaque</kwd></kwd-group><funding-group><funding-statement>No external funding was received for this work.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Optimal cue integration, Bayesian Causal Inference, spatial orienting, speech illusions and other key phenomena in audiovisual perception naturally emerge from the collective behavior of a population of Multisensory Correlation Detector.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Perception in natural environments is inherently multisensory. For example, during speech perception, the human brain integrates audiovisual information to enhance speech intelligibility, often beyond awareness. A compelling demonstration of this is the McGurk illusion (<xref ref-type="bibr" rid="bib32">McGurk and MacDonald, 1976</xref>), where the auditory perception of a syllable is altered by mismatched lip movements. Likewise, audiovisual integration plays a critical role in spatial localization, as illustrated by the ventriloquist illusion (<xref ref-type="bibr" rid="bib55">Stratton, 1897</xref>), where perceived sound location shifts toward a synchronous visual stimulus.</p><p>Extensive behavioural and neurophysiological findings demonstrate that audiovisual integration occurs when visual and auditory stimuli are presented in close spatiotemporal proximity (i.e. the spatial and temporal determinants of multisensory integration; <xref ref-type="bibr" rid="bib54">Stein, 2012</xref>, <xref ref-type="bibr" rid="bib53">Stein and Stanford, 2008</xref>). When redundant multisensory information is integrated, the resulting percept is more reliable (<xref ref-type="bibr" rid="bib19">Ernst and Banks, 2002</xref>) and salient (<xref ref-type="bibr" rid="bib57">Talsma et al., 2010</xref>). Various models have successfully described how audiovisual integration unfolds across time and space (<xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>, <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>, <xref ref-type="bibr" rid="bib29">Magnotti et al., 2013</xref>, <xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>)–often within a Bayesian Causal Inference framework, where the system determines the probability that visual and auditory stimuli have a common cause and weighs the senses accordingly. This is the case for the detection of spatiotemporal discrepancies across the senses, or susceptibility to phenomena such as the McGurk or Ventriloquist illusions (<xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>, <xref ref-type="bibr" rid="bib29">Magnotti et al., 2013</xref>, <xref ref-type="bibr" rid="bib30">Magnotti and Beauchamp, 2017</xref>).</p><p>Prevailing theoretical models of multisensory integration typically operate at what <xref ref-type="bibr" rid="bib31">Marr, 1982</xref> termed the computational level: they describe what the system is trying to achieve (e.g. obtain precise sensory estimates). However, these models are not stimulus-computable. That is, rather than analysing raw auditory and visual input directly, they rely on experimenter-defined, low-dimensional abstractions of the stimuli (<xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>, <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>, <xref ref-type="bibr" rid="bib29">Magnotti et al., 2013</xref>, <xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>, <xref ref-type="bibr" rid="bib30">Magnotti and Beauchamp, 2017</xref>)—such as the asynchrony between sound and image, expressed in seconds (<xref ref-type="bibr" rid="bib29">Magnotti et al., 2013</xref>, <xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>), or spatial location (<xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>, <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>). As a result, they solve a fundamentally different task than real perceptual systems, which must infer such properties from the stimuli themselves—from dynamic patterns of pixels and audio samples—without access to ground-truth parameters. From Marr’s perspective, what is missing is an account at the algorithmic level: a concrete description of the stimulus-driven representations and operations that could give rise to the observed computations.</p><p>Despite their clear success in accounting for behaviour in simple, controlled conditions, current models remain silent on how perceptual systems extract, process, and combine task-relevant information from the continuous and structured stream of audiovisual signals that real-world perception entails. This omission is critical: audiovisual perception involves the continuous analysis of images and sounds; hence, models that do not operate on the stimuli cannot provide a complete account of perception. Only a few models can process elementary audiovisual stimuli (<xref ref-type="bibr" rid="bib36">Parise and Ernst, 2016</xref>, <xref ref-type="bibr" rid="bib17">Cuppini et al., 2017</xref>), none can tackle the complexity of natural audiovisual input. Currently, there are no stimulus-computable models (<xref ref-type="bibr" rid="bib10">Burge, 2020</xref>) for multisensory perception that can take as input natural audiovisual data, like movies. This study explores how behaviour consistent with mammalian multisensory perception emerges from low-level analyses of natural auditory and visual signals.</p><p>In an image- and sound-computable model, visual and auditory stimuli can be represented as patterns in a three-dimensional space, where <italic>x</italic> and <italic>y</italic> are the two spatial dimensions, and <italic>t is</italic> the temporal dimension. An instance of such a three-dimensional diagram for the case of audiovisual speech is shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref> (top): moving lips generate patterns of light that vary in sync with the sound. In such a representation, audiovisual correspondence can be detected by a local correlator (i.e. multiplier), that operates across space, time, and the senses (<xref ref-type="bibr" rid="bib35">Parise et al., 2012</xref>). In previous studies, we proposed a biologically plausible solution to detect temporal correlation across the senses (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib36">Parise and Ernst, 2016</xref>; <xref ref-type="bibr" rid="bib42">Pesnot Lerousseau et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>; <xref ref-type="bibr" rid="bib21">Horsfall et al., 2021</xref>). Here, we will illustrate how a population of multisensory correlation detectors can take real-life footage as input and provide a comprehensive bottom-up account for multisensory integration in mammals, encompassing its temporal, spatial, and attentional aspects.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>The Multisensory Correlation Detector (MCD) population model.</title><p>(<bold>A</bold>) Schematic representation of a single MCD unit. The input visual signal represents the intensity of a pixel (mouth area) over time, while the audio input is the soundtrack (the syllable /ba/). The gray soundtrack represents the experimental manipulation of AV lag, obtained by delaying one sense with respect to the other. BPF and LPF indicate band-pass and low-pass temporal filters, respectively. (<bold>C</bold>) shows how single-unit responses vary as a function of cross-modal lag. (<bold>B</bold>) represents the architecture of the MCD population model. Each visual unit (in blue) receives input from a single pixel, while the auditory unit receives as input the intensity envelope of the soundtrack (mono audio; see <xref ref-type="fig" rid="fig4">Figure 4A</xref> for a version of the model capable of receiving spatialized auditory input). Sensory evidence is then integrated over time and space for perceptual decision-making, a process in which the two model responses are weighted, summed, corrupted with additive Gaussian noise, and compared to a criterion to generate a forced-choice response (<bold>D</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig1-v1.tif"/></fig><p>The present approach posits the existence of elementary processing units, the Multisensory Correlation Detectors (MCD; <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>), each integrating time-varying input from unimodal transient channels through a set of temporal filters and elementary operations (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, see Methods). Each unit returns two outputs, representing the temporal correlation and order of incoming visual and auditory signals (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). When arranged in a two-dimensional lattice (<xref ref-type="fig" rid="fig1">Figure 1B</xref>), a population of MCD units is naturally suited to take movies (e.g. dynamic images and sounds) as input, hence capable to process any stimuli used in previous studies in audiovisual integration. Given that the aim of this study is to provide an account for multisensory integration in biological system, the benchmark of our model is to reproduce observers’ behaviour in carefully controlled psychophysical and eye-tracking experiments. Emphasis will be given to studies using natural stimuli, which despite their manifest ecological value, simply cannot be handled by alternative models. Among them, particular attention will be dedicated to experiments involving speech, perhaps the most representative instance of audiovisual perception, and sometimes claimed to be processed via dedicated mechanisms in the human brain.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We tested the performance of our population model on three main aspects of audiovisual perception. The first concerns the temporal determinants of multisensory integration, primarily investigating how subjective audiovisual synchrony and integration depend on the physical lag across the senses. The second addresses the spatial determinants of audiovisual integration, focusing on the combination of visual and acoustic cues for spatial localization. The third one involves audiovisual attention and examines how gaze behaviour is spontaneously attracted to audiovisual stimuli even in the absence of explicit behavioural tasks. While most of the literature on audiovisual psychophysics involves human participants, in recent years monkeys and rats have also been trained to perform the same behavioural tasks. Therefore, to generalize our approach, whenever possible, we simulated experiments involving all available animal models.</p><sec id="s2-1"><title>Temporal determinants of audiovisual integration in humans and rats</title><p>Classic experiments on the temporal determinants of audiovisual integration usually manipulate the lag between the senses and assess the perception of synchrony, temporal order, and audiovisual speech integration (as measured in humans with the McGurk illusion, see Video 1) through psychophysical forced-choice tasks (<xref ref-type="bibr" rid="bib63">Venezia et al., 2016</xref>, <xref ref-type="bibr" rid="bib64">Vroomen and Keetels, 2010</xref>; <xref ref-type="bibr" rid="bib39">Parise et al., 2025</xref>). Among them, we obtained both the audiovisual footage and the psychophysical data from 43 experiments in humans that used ecological audiovisual stimuli (real-life recordings of, e.g. speech and performing musicians, <xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>, for the inclusion criteria, see Methods): 27 experiments were simultaneity judgments (<xref ref-type="bibr" rid="bib61">van Wassenhove et al., 2007</xref>; <xref ref-type="bibr" rid="bib25">Lee and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib65">Vroomen and Stekelenburg, 2011</xref>; <xref ref-type="bibr" rid="bib29">Magnotti et al., 2013</xref>; <xref ref-type="bibr" rid="bib46">Roseboom and Arnold, 2011</xref>; <xref ref-type="bibr" rid="bib67">Yuan et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Ikeda and Morishita, 2020</xref>; <xref ref-type="bibr" rid="bib60">van Laarhoven et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Lee and Noppeney, 2014</xref>) , 10 temporal order judgments (<xref ref-type="bibr" rid="bib65">Vroomen and Stekelenburg, 2011</xref>, <xref ref-type="bibr" rid="bib20">Freeman et al., 2013</xref>), six others assessed the McGurk effect (<xref ref-type="bibr" rid="bib61">van Wassenhove et al., 2007</xref>, <xref ref-type="bibr" rid="bib67">Yuan et al., 2014</xref>, <xref ref-type="bibr" rid="bib20">Freeman et al., 2013</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Natural audiovisual stimuli and psychophysical responses.</title><p>(<bold>A</bold>) Stimuli (still frame and soundtrack) and psychometric functions for McGurk illusion (<xref ref-type="bibr" rid="bib61">van Wassenhove et al., 2007</xref>), synchrony judgments (<xref ref-type="bibr" rid="bib25">Lee and Noppeney, 2011</xref>), and temporal order judgments (<xref ref-type="bibr" rid="bib65">Vroomen and Stekelenburg, 2011</xref>). In all panels, dots correspond to empirical data, lines to Multisensory Correlation Detectors (MCD) responses; negative lags represent vision first. (<bold>B</bold>) Stimuli and results of <xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>. The left panel displays the envelopes of auditory stimuli (clicks) recorded at different distances in a reverberant environment (the Sydney Opera House). While the reverberant portion of the sound is identical across distances, the intensity of the direct sound (the onset) decreases with depth. As a result, the centre of mass of the envelopes shifts rightward with increasing distance. The central panel shows empirical and predicted psychometric functions for the various distances. The four curves were fitted using the same decision-making parameters, so that the separation between the curves results purely from the operation of the MCD. The lag at which sound and light appear synchronous (point of subjective synchrony) scales with distance at a rate approximately matching the speed of sound (right panel). The dots in the right panel display the point of subjective synchrony (estimated separately for each curve), while the jagged line is the model prediction. (<bold>C</bold>) shows temporal order judgments for clicks and flashes from both rats and human observers (<xref ref-type="bibr" rid="bib28">Mafi et al., 2022</xref>). Rats outperform humans at short lag, and vice-versa. (<bold>D</bold>) Rats’ temporal order and synchrony judgments for flashes and clicks of varying intensity (<xref ref-type="bibr" rid="bib49">Schormans and Allman, 2018</xref>). Note that in the synchrony judgment task only the left flank of the psychometric curve (video-lead lags) was sampled. Importantly the tree curves in each task were fitted using the same decision-making parameters, so that the MCD alone accounts for the separation between the curves. (<bold>E</bold>) Pharmacologically-induced changes in rats’ audiovisual time perception. Left: Glutamatergic inhibition (MK-801 injection) leads to asymmetric broadening of the psychometric functions for simultaneity judgments. Right: GABA inhibition (Gabazine injection) abolishes rapid temporal adaptation, so that psychometric curves do not change based on the lag of the previous trials (as they do in controls) (<xref ref-type="bibr" rid="bib50">Schormans and Allman, 2023</xref>). All pharmacologically-induced changes in audiovisual time perception can be accounted for by changes in the decision-making process, with no need to postulate changes in low-level temporal processing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Temporal determinants of multisensory integration in humans.</title><p>Schematic representation of the stimuli with a manipulation of audiovisual lag (<bold>A</bold>). (<bold>B-J</bold>) shows the stimuli, psychophysical data (dots) and model response (lines) of simulated studies investigating the temporal determinants of multisensory integration in humans using ecological audiovisual stimuli. The colour of the psychometric curves represents the psychophysical task (red = McGurk, black = simultaneity judgments, blue = temporal order judgments). (<bold>K</bold>) shows the results of the permutation test: the histogram represents the permuted distribution of the correlation between data and model response. The arrow represents the observed correlation between model and data, and it is 4.7 standard deviation above the mean of the permuted distribution (p&lt;0.001). Panel I represents the funnel plot of the Pearson correlation between model and data across all studies in <bold>B-J</bold>, plotted against the number of trials used for each level of audiovisual lag. Red diamonds represent the McGurk task, blue diamonds and circles represent temporal order judgments, and black diamonds and circles represents simultaneity judgments. Diamonds represents human data, circles represent rat data. The dashed gray line represents the overall Pearson correlation between empirical and predicted psychometric curves, weighted by the number of trials of each curve. As expected, the correlation decreases with decreasing number of trials.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Temporal determinants of multisensory integration in rats.</title><p>Schematic representation of the stimuli (clicks and flashes) with a manipulation of audiovisual lag (<bold>A</bold>). (<bold>B-J</bold>) shows the psychophysical data (dots) and model response (lines) of simulated studies. The colour of the psychometric curves represents the psychophysical task (black = simultaneity judgments, blue = temporal order judgments).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Simultaneity judgment for impulse stimuli in humans, individual observer data (<xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>).</title><p>(<bold>A</bold>) represents the baseline condition; (<bold>B</bold>) represents the conservative condition (in which observers were instructed to respond ‘synchronous’ only when absolutely sure); (<bold>C</bold>) represents the post-test condition (in which observers were instructed to respond as they preferred, like in the baseline condition). Individual plots represent data from a single observer (dots) and model responses (lines), and consists of 135 trials each (total 7695 trials). The bottom-right plots represents the average empirical and predicted psychometric functions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Simultaneity and temporal order judgments for step stimuli (<xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>).</title><p>(<bold>A</bold>) represents the results of the simultaneity judgments; (<bold>B</bold>) represents the data from the temporal order judgment. Each column represents a different combination of audiovisual step polarity (i.e. onset steps in both modalities, offset steps in both modalities, video onset and audio offset, video offset and audio onset). Different rows represent curves from different observers (dots) and model responses (lines). Each curve consists of 150 trials (total of 9600 trials). The bottom row (light gray background) represents the average empirical and predicted psychometric functions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig2-figsupp4-v1.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>Simultaneity judgment for periodic stimuli (<xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>).</title><p>Individual plots represent data from a single observer (dots) and model responses (lines). Each curve consists of 600 trials (total of 3000 trials). The bottom-right curve represents the average empirical and predicted psychometric functions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig2-figsupp5-v1.tif"/></fig></fig-group><p>For each of the experiments, we can feed the stimuli to the model (<xref ref-type="fig" rid="fig1">Figure 1B and D</xref>), and compare the output to the empirical psychometric functions (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>, for details see Methods) (<xref ref-type="bibr" rid="bib36">Parise and Ernst, 2016</xref>, <xref ref-type="bibr" rid="bib42">Pesnot Lerousseau et al., 2022</xref>; <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>; <xref ref-type="bibr" rid="bib21">Horsfall et al., 2021</xref>). Results demonstrate that a population of MCDs can broadly account for audiovisual temporal perception of ecological stimuli, and near-perfectly (rho = 0.97) reproduces the empirical psychometric functions for simultaneity judgments, temporal order judgments, and the McGurk effect (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). To quantify the impact of the low-level properties of the stimuli on the performance of the model, we ran a permutation test, where psychometric functions were predicted from mismatching stimuli (see Methods). The psychometric curves predicted from the matching stimuli provided a significantly better fit than mismatching stimuli (p&lt;0.001, see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1K</xref>). This demonstrates that our model captures the subtle effects of how individual features affect observed responses, and it highlights the role of low-level stimulus properties on multisensory perception. All analyses performed so far relied on psychometric functions averaged across observers; individual observer analyses are included in the <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplements 3</xref>–<xref ref-type="fig" rid="fig2s5">5</xref>.</p><p>When estimating the perceived timing of audiovisual events, it is important to consider the different propagation speeds of light and sound, which introduce audio lags that are proportional to the observer’s distance from the source (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, right). Psychophysical temporal order judgments demonstrate that, to compensate for these lags, humans scale subjective audiovisual synchrony with distance (<xref ref-type="fig" rid="fig2">Figure 2B</xref>; <xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>). This result has been interpreted as evidence that humans exploit auditory spatial cues, such as the direct-to-reverberant energy ratio (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left), to estimate the distance of the sound source and adjust subjective synchrony by scaling distance estimates by the speed of sound (<xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>). When presented with the same stimuli, our model also predicts the observed shifts in subjective simultaneity (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, centre). However, rather than relying on explicit spatial representations and physics simulations, these shifts emerge from elementary analyses of natural audiovisual signals. Specifically, in reverberant environments, the intensity of the direct portion of a sound increases with source proximity, while the reverberant component remains constant. As a result, the envelopes of sounds originating close to the observers are more front-heavy than distant sounds (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left). These are low-level acoustic features that the lag detector of the MCD is especially sensitive to, thereby providing a computational shortcut to explicit physics simulations. A Matlab implementation of this simulation is included in <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p><p>In recent years, audiovisual timing has been systematically studied also in rats (<xref ref-type="bibr" rid="bib28">Mafi et al., 2022</xref>, <xref ref-type="bibr" rid="bib49">Schormans and Allman, 2018</xref>, <xref ref-type="bibr" rid="bib8">Al Youzbaki et al., 2023</xref>, <xref ref-type="bibr" rid="bib50">Schormans and Allman, 2023</xref>, <xref ref-type="bibr" rid="bib48">Schormans et al., 2016</xref>, <xref ref-type="bibr" rid="bib40">Paulcan et al., 2023</xref>), generally using minimalistic stimuli (such as clicks and flashes), and under a variety of manipulations of the stimuli (e.g. loudness) and pharmacological interventions (e.g. GABA and glutamatergic inhibition). Therefore, to further generalize our model to other species, we assessed whether it can also account for rats’ behaviour in synchrony and temporal order judgments. Overall, we could tightly replicate rats’ behaviour (rho = 0.981; see <xref ref-type="fig" rid="fig2">Figure 2C-E</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), including the effect of loudness on observed responses (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). Interestingly, the unimodal temporal constants for rats were 4 times faster than for humans: such a different temporal tuning is reflected in higher sensitivity in rats for short lags (&lt;0.1 s), and in humans for longer lags (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). This fourfold difference in temporal tuning between rats and humans closely mirrors analogous interspecies differences in physiological rhythms, such as heart rate (~4.7 times faster in rats) and breathing rate (~6.3 times faster in rats) (<xref ref-type="bibr" rid="bib4">Agoston, 2017</xref>).</p><p>While tuning the temporal constants of the model was necessary to account for the difference between humans and rats, this was not necessary to reproduce pharmacologically-induced changes in audiovisual time perception in rats (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2F-G</xref>), which could be accounted for solely by changes in the decision-making process(<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>). This suggests that the observed effects can be explained without altering low-level temporal processing. However, this does not imply that such changes did not occur—only that they were not required to reproduce the behavioural data in our simulations. Future studies using richer temporal stimuli—such as temporally modulated sequences that vary in frequency, rhythm, or phase—will be necessary to disentangle sensory and decisional contributions, as these stimuli can more selectively engage low-level temporal processing and better reveal whether perceptual changes arise from early encoding or later interpretive stages.</p><p>An asset of a low-level approach is that it allows one to inspect, at the level of individual pixels and frames, the features of the stimuli that determine the response of the model (i.e. the saliency maps). This is illustrated in <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="video" rid="video1">Videos 1</xref> and <xref ref-type="video" rid="video2">2</xref> for the case of audiovisual speech, where model responses cluster mostly around the mouth area and (to a lesser extent) the eyes. These are the regions where pixels’ luminance changes in synch with the audio track.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Ecological audiovisual stimuli and model responses.</title><p>(<bold>A</bold>) displays the frames and soundtrack of a dynamic audiovisual stimulus over time (in this example, video and audio tracks are synchronous, and the actress utters the syllable /ta/). (<bold>B</bold>) shows how the dynamic population responses <inline-formula><alternatives><mml:math id="inf1"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft1">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf2"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft2">\begin{document}$MCD_{lag}$\end{document}</tex-math></alternatives></inline-formula> vary across the frames of Panel A. Note how model responses highlight the pixels whose intensity changed with the soundtrack (i.e. the mouth area). The right side of Panel B represents the population read-out process, as implemented for the simulations in <xref ref-type="fig" rid="fig2">Figure 2</xref>: the population responses <inline-formula><alternatives><mml:math id="inf3"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft3">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf4"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft4">\begin{document}$MCD_{lag}$\end{document}</tex-math></alternatives></inline-formula> are integrated over space (i.e. pixels) and time (i.e. frames), scaled and weighted by the gain parameters <inline-formula><alternatives><mml:math id="inf5"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft5">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf6"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft6">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula> and summed to obtain a single decision variable that is fed to the decision-making stage (see <xref ref-type="fig" rid="fig1">Figure 1D</xref>). (<bold>C</bold>) represents the time-averaged population responses <inline-formula><alternatives><mml:math id="inf7"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft7">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf8"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft8">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> as a function of cross-modal lag (the central one corresponds to the time-averaged responses shown in <bold>B</bold>). Note how <inline-formula><alternatives><mml:math id="inf9"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft9">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> peaks at around zero lag and decreases with increasing lag (following the same trend shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>, left), while polarity of <inline-formula><alternatives><mml:math id="inf10"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft10">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> changes with the sign of the delay. The psychophysical data corresponding to the stimulus in this figure is shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>. See <xref ref-type="video" rid="video2">Video 2</xref> for a dynamic representation of the content of this figure.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig3-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106122-video1.mp4" id="video1"><label>Video 1.</label><caption><title>The McGurk Illusion – integration of mismatching audiovisual speech.</title><p>The soundtrack is from a recording where the actress utters the syllable /pa/, whereas in the video she utters /ka/. When the video and sound tracks are approximately synchronous, observers often experience the McGurk illusion, and perceive the syllable /ta/. To experience the illusion, try to recognize what the actress utters as we manipulate audiovisual lag. Note how the <inline-formula><alternatives><mml:math id="inf11"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft11">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> population response clusters around the mouth area, and how its magnitude scales with the probability of experiencing the illusion. See <xref ref-type="video" rid="video2">Video 2</xref> for details.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106122-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Population response to audiovisual speech stimuli.</title><p>The top left panel displays the stimulus from <xref ref-type="bibr" rid="bib61">van Wassenhove et al., 2007</xref>, where the actress utters the syllable /ta/. The central and right top panels represent the dynamic <inline-formula><alternatives><mml:math id="inf12"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft12">\begin{document}$MCD_{corr}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf13"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft13">\begin{document}$MCD_{lag}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> population responses, respectively (<xref ref-type="disp-formula" rid="equ6 equ7">Equations 6 and 7</xref>). The lower part of the video displays the temporal profile of the stimuli and model responses (averaged over space). The top two lines represent the stimuli: for the visual stimuli, the line represents the root-mean-squared difference of the pixel value from one frame to the next; the line for audio represents the envelope of the stimulus. <inline-formula><alternatives><mml:math id="inf14"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft14">\begin{document}$MCD_{Vid}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf15"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft15">\begin{document}$MCD_{Aud}$\end{document}</tex-math></alternatives></inline-formula> represents the output of the unimodal transient channels (averaged over space) that feed to the MCD (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). The two lower lines represent the <inline-formula><alternatives><mml:math id="inf16"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft16">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf17"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft17">\begin{document}$MCD_{lag}$\end{document}</tex-math></alternatives></inline-formula> responses and correspond to the average of the responses displayed in the central and top-right panels This movie corresponds to the data displayed in <xref ref-type="fig" rid="fig3">Figure 3A–B</xref>. Note how the magnitude of <inline-formula><alternatives><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft18">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> increases as the absolute lag decreases, while the polarity of <inline-formula><alternatives><mml:math id="inf19"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft19">\begin{document}$MCD_{lag}$\end{document}</tex-math></alternatives></inline-formula> changes depending on which modality came first.</p></caption></media></sec><sec id="s2-2"><title>Spatial determinants of audiovisual integration in humans and monkeys</title><p>Classic experiments on the spatial determinants of audiovisual integration usually require observers to localize the stimuli under systematic manipulations of the discrepancy and reliability (i.e. precision) of the spatial cues (<xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>; <xref ref-type="fig" rid="fig4">Figure 4</xref>). This allows one to assess how unimodal cues are weighted and combined to give rise to phenomena such as the ventriloquist illusion (<xref ref-type="bibr" rid="bib55">Stratton, 1897</xref>). When the spatial discrepancy across the senses is low, observers’ behaviour is well described by Maximum Likelihood Estimation (MLE; <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>), where unimodal information is combined in a statistically optimal fashion, so as to maximize the precision (reliability) of the multimodal percept (see <xref ref-type="disp-formula" rid="equ11 equ12 equ13 equ14">Equations 11–14</xref>, Methods).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Audiovisual integration in space.</title><p>(<bold>A</bold>) Top represents the Multisensory Correlation Detector (MCD) population model for spatialized audio. Visual and auditory input units receive input from corresponding spatial locations, and feed into spatially-tuned MCD units. The output of each MCD unit is eventually normalized by the total population output, so as to represent the probability distribution of stimulus location over space. The bottom part of Panel A represents the dynamic unimodal and bimodal population response over time and space (azimuth) and their marginals. When time is marginalized out, a population of MCDs implements integration as predicted by the maximum likelihood estimation (MLE) model. When space is marginalized, the output show the temporal response function of the model. In this example, visual and auditory stimuli were asynchronously presented from discrepant spatial locations (note how the blue and orange distributions are spatiotemporally offset). (<bold>B</bold>) shows a schematic representation of the stimuli used to test the MLE model by <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>. Stimuli were presented from different spatial positions, with a parametric manipulation of audiovisual spatial disparity and blob size (i.e. the standard deviation, <inline-formula><alternatives><mml:math id="inf20"><mml:mi>σ</mml:mi></mml:math><tex-math id="inft20">\begin{document}$\sigma $\end{document}</tex-math></alternatives></inline-formula> of the blob). (<bold>C</bold>) shows how the bimodal psychometric functions predicted by the MCD (lines, see <xref ref-type="disp-formula" rid="equ16">Equation 16</xref>) and the MLE (dots) models fully overlap. (<bold>D</bold>) shows how bimodal bias varies as a function of disparity and visual reliability (see legend on the left). The dots correspond to the empirical data from participant LM, while the lines are the predictions of the MCD model (compare to <xref ref-type="fig" rid="fig2">Figure 2A</xref>, of Alais and Burr, 2004). (<bold>E</bold>) shows how the just noticeable differences (just noticeable difference JND, the random localization error) vary as a function of blob size. The blue squares represent the visual JNDs, the purple dots the bimodal JNDs, while the dashed orange line represents the auditory JND. The continuous line shows the JNDs predicted by the MCD population model (compare to <xref ref-type="fig" rid="fig2">Figure 2B</xref>, of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>). (<bold>F</bold>) represents the breakdown of integration with spatial disparity. The magnitude of the MCD population output (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>, shown as the area under the curve of the bimodal response) decreases with increasing spatial disparity across the senses. This can be then transformed into a probability for a common cause (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>). (<bold>G</bold>) represents the stimuli and results of the experiment used by <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref> to test the Bayesian Causal Inference (BCI) model. Auditory and visual stimuli originate from one of five spatial locations, spanning a range of 20° . The plots show the perceived locations of visual (blue) and auditory (orange) stimuli for each combination of audiovisual spatial locations. The dots represent human data, while the lines represent the responses of the MCD population model. (<bold>H</bold>) shows the stimuli and results of the experiment of <xref ref-type="bibr" rid="bib33">Mohl et al., 2020</xref>. The plots on the right display the probability of a single (vs. double) fixation (top monkeys, bottom humans). The dots represent human data, while the lines represent the responses of the MCD population model. The remaining panels show the histogram of the fixated locations in bimodal trials: the jagged histograms are the empirical data, while the smooth ones are the model prediction (zero free parameters). The regions of overlap between empirical and predicted histograms are shown in black.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Multisensory Correlation Detector (MCD) model for trimodal integration.</title><p>Trimodal MCD units combine inputs from three unimodal transient channels via multiplicative interactions. When all three modalities are stimulated in close temporal and spatial proximity, the resulting trimodal population response closely replicates the predictions of maximum likelihood estimation (MLE).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig4-figsupp1-v1.tif"/></fig></fig-group><p>Given that both the MLE and the MCD operate by multiplying unimodal inputs (see Methods), the time-averaged MCD population response (<xref ref-type="disp-formula" rid="equ16">Equation 16</xref>) is equivalent to MLE (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). This can be illustrated by simulating the experiment of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref> using both models. In this experiment, observers had to report whether a probe audiovisual stimulus appeared left or right of a standard. To assess the weighing behaviour resulting from multisensory integration, they manipulated the spatial reliability of the visual stimuli and the disparity between the senses (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). <xref ref-type="fig" rid="fig4">Figure 4C</xref> shows that the integrated percept predicted by the two models is statistically indistinguishable. As such, a population of MCDs (<xref ref-type="disp-formula" rid="equ16">Equation 16</xref>) can jointly account for the observed bias and precision of the bimodal percept (<xref ref-type="fig" rid="fig4">Figure 4D–E</xref>), with zero parameters. A MATLAB implementation of this simulation is included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p><p>While fusing audiovisual cues is a sensible solution in the presence of minor spatial discrepancies across the senses, integration eventually breaks down with increasing disparity (<xref ref-type="bibr" rid="bib13">Chen and Vroomen, 2013</xref>)—when the spatial (or temporal) conflict is too large, visual and auditory signals may well be unrelated. To account for the breakdown of multisensory integration in the presence of intersensory conflicts, Körding and colleagues proposed the influential Bayesian Causal Inference (BCI) model (<xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>), where uni- and bimodal location estimates are weighted based on the probability that the two modalities share a common cause (<xref ref-type="disp-formula" rid="equ17">Equation 17</xref>). The BCI model was originally tested in an experiment in which sound and light were simultaneously presented from one of five random locations, and observers had to report the position of both modalities (<xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>; <xref ref-type="fig" rid="fig4">Figure 4G</xref>). Results demonstrate that visual and auditory stimuli preferentially bias each other when the discrepancy is low, with the bias progressively declining as the discrepancy increases.</p><p>Also a population of MCDs can compute the probability that auditory and visual stimuli share a common cause (<xref ref-type="fig" rid="fig1">Figure 1B and D</xref>; <xref ref-type="fig" rid="fig4">Figure 4F</xref>, <xref ref-type="disp-formula" rid="equ19">Equation 19</xref>), therefore, we can test whether it can also implement BCI. For that, we simulated the experiment of Körding and colleagues, and fed the stimuli to a population of MCDs (<xref ref-type="disp-formula" rid="equ18 equ19 equ20">Equations 18-20)</xref> which near-perfectly replicated the empirical data (rho = 0.99)–even slightly outperforming the BCI model. A MATLAB implementation of this simulation is included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p><p>To test the generalizability of these findings across species and behavioural paradigms, we simulated an experiment in which monkeys (<italic>Macaca mulatta</italic>) and humans directed their gaze toward audiovisual stimuli presented at varying spatial disparities (<xref ref-type="fig" rid="fig4">Figure 4H</xref>; <xref ref-type="bibr" rid="bib33">Mohl et al., 2020</xref>). If observers infer a common cause, they tend to make a single fixation; otherwise, two—one for each modality. As expected, the probability of a single fixation decreased with increasing disparity (<xref ref-type="fig" rid="fig4">Figure 4H</xref>, right). This pattern was captured by a population of MCDs: <inline-formula><alternatives><mml:math id="inf21"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft21">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> values were used to fit the probability of single vs. double saccades as a function of disparity (<xref ref-type="disp-formula" rid="equ21">Equation 21</xref>, <xref ref-type="fig" rid="fig4">Figure 4H</xref>, right). Critically, using this fit, the model was then able to predict the full distribution of gaze locations (<xref ref-type="disp-formula" rid="equ20">Equation 20</xref>, <xref ref-type="fig" rid="fig4">Figure 4H</xref>, left) in both species with zero additional free parameters. A Matlab implementation of this simulation is included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p><p>Taken together, these simulations show that behaviour consistent with BCI and MLE naturally emerges from a population of MCDs. Unlike BCI and MLE, however, the MCD population model is both image- and sound-computable, and it explicitly represents the spatiotemporal dynamics of the process (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, bottom; <xref ref-type="fig" rid="fig3">Figure 3B</xref>; <xref ref-type="fig" rid="fig5">Figure 5</xref>; <xref ref-type="fig" rid="fig6">Figure 6B–C</xref>). On one hand, this enables the model to be applied to complex, dynamic audiovisual stimuli—such as real-life videos—that were previously <italic>off limits</italic> to traditional BCI and MLE frameworks, whose probabilistic, non–stimulus-computable formulations prevent them from operating directly on such inputs. On the other, it permits direct, time-resolved comparisons between model responses and neurophysiological measures (<xref ref-type="bibr" rid="bib42">Pesnot Lerousseau et al., 2022</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Multisensory Correlation Detector (MCD) and the Ventriloquist Illusion.</title><p>The upper panel represents still frame of a performing ventriloquist. The central panel represents the MCD population response. The lower plot represents the horizontal profile of the MCD response for the same frame. Note how the population response clusters on the location of the dummy, where more pixels are temporally correlated with the soundtrack.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig5-v1.tif"/></fig><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Audiovisual saliency maps.</title><p>(<bold>A</bold>) represents a still frame of <xref ref-type="bibr" rid="bib15">Coutrot and Guyader, 2015</xref> stimuli. The white dots represent gaze direction of the various observers. (<bold>B</bold>) represents the Multisensory Correlation Detector (MCD) population response for the frame in Panel A. The dots represent observed gaze direction (and correspond to the white dots of Panel A). (<bold>C</bold>) represents how the MCD response varies over time and azimuth (with elevation marginalized-out). The black solid lines represent the active speaker, while the waveform on the right displays the soundtrack. Note how the MCD response was higher for the active speaker. (<bold>D</bold>) shows the distribution of model response at gaze direction (see Panel B) across all frames and observers in the database. Model response was normalized for each frame (Z-scores). The y axis represents the number of frames. The vertical gray line represents the mean. See <xref ref-type="video" rid="video4">Video 4</xref> for a dynamic representation of the content of this figure.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-106122-fig6-v1.tif"/></fig><p>As a practical demonstration, we applied the model (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) to a real-life video of a performing ventriloquist (<xref ref-type="fig" rid="fig5">Figure 5</xref>). The population response dynamically tracked the active talker, clustering around the dummy’s face whenever it produced speech <xref ref-type="video" rid="video3">Video 3</xref>.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106122-video3.mp4" id="video3"><label>Video 3.</label><caption><title>The ventriloquist illusion.</title><p>The top panel represents a video of a performing ventriloquist. The voice of the dummy was edited (pitch-shifted) and added in post-production. The second panel represents the dynamic <inline-formula><alternatives><mml:math id="inf22"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft22">\begin{document}$MCD_{corr}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> population response to a blurred version of the video (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>). The third panel shows the distribution of population responses along the horizontal axis (obtained by averaging the upper panel over the vertical dimension). This represents the dynamic, real-life version of the bimodal population response shown in <xref ref-type="fig" rid="fig4">Figure 4A</xref> for the case of minimalistic audiovisual stimuli. The lower panel represents the same information as the panel above displayed as a rolling timeline. For this video, the population response was temporally aligned to the stimuli to compensate for lags introduced by the temporal filters of the model. Note how the population response spatially follows the active speaker, hence capturing the sensed location of the audiovisual event towards correlated visuals.</p></caption></media></sec><sec id="s2-3"><title>Spatial orienting and audiovisual saliency maps</title><p>Multisensory stimuli are typically salient, and a vast body of literature demonstrates that spatial attention is commonly attracted to audiovisual stimuli (<xref ref-type="bibr" rid="bib57">Talsma et al., 2010</xref>). This aspect of multisensory perception is naturally captured by a population of MCDs, whose dynamic response explicitly represents the regions in space with the highest audiovisual correspondence for each point in time. Therefore, for a population of MCDs to provide a plausible account for audiovisual integration, such dynamic saliency maps should be able to predict human audiovisual gaze behaviour, in a purely bottom-up fashion and with no free parameters. <xref ref-type="fig" rid="fig6">Figure 6A</xref> shows the stimuli and eye-tracking data from the experiment of <xref ref-type="bibr" rid="bib15">Coutrot and Guyader, 2015</xref>, in which observers passively watched a video of four persons talking. Panel B shows the same eye-tracking data plotted over the corresponding MCD population response: across 20 observers, and 15 videos (for a total of over 16,000 frames), gaze was on average directed towards the locations (i.e. pixels) yielding the top 2% MCD response (<xref ref-type="fig" rid="fig6">Figure 6D</xref>, <xref ref-type="disp-formula" rid="equ22 equ23">Equations 22; 23</xref>). The tight correspondence of predicted and empirical salience is illustrated in <xref ref-type="fig" rid="fig6">Figure 6C</xref> and <xref ref-type="video" rid="video4">Video 4</xref>: note how population responses peak based on the active speaker.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-106122-video4.mp4" id="video4"><label>Video 4.</label><caption><title>Audiovisual saliency maps.</title><p>The top panel represents Movie 1 from <xref ref-type="bibr" rid="bib15">Coutrot and Guyader, 2015</xref>. The central panel represents <inline-formula><alternatives><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft23">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> in gray scales, while the colorful blobs represent observers’ gaze direction during passive viewing. The lower panel represents how the <inline-formula><alternatives><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft24">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> and gaze direction (co)vary over time and azimuth (with elevation marginalized-out). The black solid lines represent the active speaker. For the present simulations, movies were converted to grayscale and the upper and lower sections of the videos (which were mostly static) were cropped. Note how gaze is consistently directed towards the regions of the frames displaying the highest audiovisual correlation.</p></caption></media></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>This study demonstrates that elementary audiovisual analyses are sufficient to replicate behaviours consistent with multisensory perception in mammals. The proposed image- and sound-computable model, composed of a population of biologically plausible elementary processing units, provides a stimulus-driven framework for multisensory perception that transforms raw audiovisual input into behavioural predictions. Starting directly from pixels and audio samples, our model closely matched observed behaviour across a wide range of phenomena—including multisensory illusions, spatial orienting, and causal inference—with average correlations above 0.97. This was tested in a large-scale simulation spanning 69 audiovisual experiments, seven behavioural tasks, and data from 534 humans, 110 rats, and two monkeys.</p><p>We define a stimulus-computable model as one that receives input directly from the stimulus—such as raw images and sound waveforms—rather than from abstracted descriptors like lag, disparity, or reliability. Framed in Marr’s terms, stimulus-computable models operate at the algorithmic level, specifying how sensory information is represented and processed. This contrasts with computational-level models, such as Bayesian ideal observers, which define the goals of perception (e.g. maximizing reliability <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>; <xref ref-type="bibr" rid="bib19">Ernst and Banks, 2002</xref>) without specifying how those goals are achieved. Rather than competing with such normative accounts, the MCD provides a mechanistic substrate that could plausibly implement them. By operating directly on realistic audiovisual signals, our population model captures the richness of natural sensory input and directly addresses the problem of how biological systems represent and process multisensory information (<xref ref-type="bibr" rid="bib10">Burge, 2020</xref>). This allows the MCD to generate precise, stimulus-specific predictions across tasks, including subtle differences in behavioural outcomes that arise from the structure of individual stimuli (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1K</xref>).</p><p>The present approach naturally lends itself to be generalized and tested against a broad range of tasks, stimuli, and responses—as reflected by the breadth of the experiments simulated here. Among the perceptual effects emerging from elementary signal processing, one notable example is the scaling of subjective audiovisual synchrony with sound source distance (<xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>). As sound travels slower than light, humans compensate for audio delays by adjusting subjective synchrony based on the source’s distance scaled by the speed of sound. Although this phenomenon appears to rely on explicit physics modelling, our simulations demonstrate that auditory cues embedded in the envelope (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, left) are sufficient to scale subjective audiovisual synchrony. In a similar fashion, our simulations show that phenomena such as the McGurk illusion, the subjective timing of natural audiovisual stimuli, and saliency detection may emerge from elementary operations performed at pixel level, bypassing the need for more sophisticated analyses such as image segmentation, lip or face tracking, 3D reconstruction, etc (<xref ref-type="bibr" rid="bib12">Chandrasekaran et al., 2009</xref>). Elementary, general-purpose operations on natural stimuli can drive complex behaviour, sometimes even in the absence of advanced perceptual and cognitive contributions. Indeed, it is intriguing that a population of MCDs, a computational architecture originally proposed for motion vision in insects, can predict speech illusions in humans.</p><p>The fact that identical low-level analyses can account for all of the 69 experiments simulated here directly addresses several open questions in multisensory research. For instance, psychometric functions for speech and non-speech stimuli often differ significantly (<xref ref-type="bibr" rid="bib62">Vatakis et al., 2008</xref>). This has been interpreted as evidence that speech may be special and processed via dedicated mechanisms (<xref ref-type="bibr" rid="bib58">Tuomainen et al., 2005</xref>). However, identical low-level analyses are sufficient to account for all observed responses, regardless of the stimulus type (<xref ref-type="fig" rid="fig2">Figure 2</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref>). This suggests that most of the differences in psychometric curves across classes of stimuli (e.g. speech vs. non-speech vs. clicks-&amp;-flashes) are due to the low-level features of the stimuli themselves, not how the brain processes them. Similarly, experience and expertise also modulate multisensory perception. For example, audiovisual simultaneity judgments differ significantly between musicians and non-musicians (<xref ref-type="bibr" rid="bib25">Lee and Noppeney, 2011</xref>) (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C</xref>). Likewise, the McGurk illusion (<xref ref-type="bibr" rid="bib20">Freeman et al., 2013</xref>) and subjective audiovisual timing (<xref ref-type="bibr" rid="bib43">Petrini et al., 2009</xref>) vary over the lifespan in humans, and following pharmacological interventions in rats (<xref ref-type="bibr" rid="bib8">Al Youzbaki et al., 2023</xref>; <xref ref-type="bibr" rid="bib50">Schormans and Allman, 2023</xref>) (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1E and J</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2F-G</xref>). Our simulations show that adjustments at the decision-making level are sufficient to account for these effects, without requiring structural or parametric changes to low-level perceptual processing across observers or conditions.</p><p>Although the same model explains responses to multisensory stimuli in humans, rats, and monkeys, the temporal constants vary across species. For example, the model for rats is tuned to temporal frequencies over four times higher than those for humans. This not only explains the differential sensitivity of humans and rats to long and short audiovisual lags, but it also mirrors analogous interspecies differences in physiological rhythms, such as heart and breathing rates (<xref ref-type="bibr" rid="bib4">Agoston, 2017</xref>). Previous research has shown that physiological arousal modulates perceptual rhythms within individuals (<xref ref-type="bibr" rid="bib27">Legrand et al., 2018</xref>). It is an open question whether the same association between multisensory temporal tuning and physiological rhythms persists in other mammalian systems. Conversely, no major differences in the model’s spatial tuning were found between humans and macaques, possibly reflecting the close phylogenetic link between the two species.</p><p>How might these computations be implemented neurally? In a recent study (<xref ref-type="bibr" rid="bib42">Pesnot Lerousseau et al., 2022</xref>), we identified neural responses in the posterior superior temporal sulcus, superior temporal gyrus, and left superior parietal gyrus that tracked the output of an MCD model during audiovisual temporal tasks. Participants were presented with random sequences of clicks and flashes while performing either a causality judgment or a temporal order judgment task. By applying a time-resolved encoding model to MEG data, we demonstrated that MCD dynamics aligned closely with stimulus-evoked cortical activity. The present study considerably extends the scope of the MCD framework, allowing it to process more naturalistic stimuli and to account for a broader range of behaviours—including cue combination, attentional orienting, and gaze-based decisions. This expansion opens the door to new neurophysiological investigations into the implementation of multisensory integration. For instance, the dynamic, spatially distributed population responses generated by the MCD (see videos) can be directly compared with neural population activity recorded using techniques such as ECoG, Neuropixels, or high-density fMRI—similar to previous efforts that linked the Bayesian Causal Inference model to neural responses during audiovisual spatial integration (<xref ref-type="bibr" rid="bib45">Rohe et al., 2019</xref>; <xref ref-type="bibr" rid="bib7">Aller and Noppeney, 2019</xref>; <xref ref-type="bibr" rid="bib44">Rohe and Noppeney, 2015</xref>). Such comparisons may help bridge algorithmic and implementational levels of analysis, offering concrete hypotheses about how audiovisual correspondence detection and integration are instantiated in the brain.</p><p>An informative outcome of our simulations is the model’s ability to predict spontaneous gaze direction in response to naturalistic audiovisual stimuli. Saliency, the property by which some elements in a display stand out and attract observer’s attention and gaze direction, is a popular concept in both cognitive and computer sciences (<xref ref-type="bibr" rid="bib23">Itti et al., 1998</xref>). In computer vision, saliency models are usually complex and rely on advanced signal processing and semantic knowledge—typically with tens of millions of parameters (<xref ref-type="bibr" rid="bib14">Chen et al., 2023</xref>; <xref ref-type="bibr" rid="bib16">Coutrot, 2025</xref>). Despite successfully predicting gaze behaviour, current audiovisual saliency models are often computationally expensive, and the resulting maps are hard to interpret and inevitably affected by the datasets used for training (<xref ref-type="bibr" rid="bib2">Adebayo et al., 2023</xref>). In contrast, our model detects saliency ‘out of the box,’ without any free parameters, and operating purely at the individual pixel level. The elementary nature of the operations performed by a population of MCDs returns saliency maps that are easy to interpret: salient points are those with high audiovisual correlation. By grounding multisensory integration and saliency detection in biologically plausible computations, our study offers a new tool for machine perception and robotics to handle multimodal inputs in a more human-like way, while also improving system accountability.</p><p>This framework also provides a solution for self-supervised and unsupervised audiovisual learning in multimodal machine perception. A key challenge when handling raw audiovisual data is solving the causal inference problem—determining whether signals from different modalities are causally related or not (<xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>). Models in machine perception often depend on large, labelled datasets for training. In this context, a biomimetic module that handles saliency maps, audiovisual correspondence detection, and multimodal fusion can drive self-supervised learning through simulated observers, thereby reducing the dependency on labelled data (<xref ref-type="bibr" rid="bib52">Shahabaz and Sarkar, 2024</xref>; <xref ref-type="bibr" rid="bib9">Arandjelovic and Zisserman, 2017</xref>; <xref ref-type="bibr" rid="bib34">Ngiam et al., 2011</xref>). Furthermore, the simplicity of our population-based model provides a computationally efficient alternative for real-time multisensory integration in applications such as robotics, AR/VR, and other low-latency systems.</p><p>Although a population of MCDs can explain when phenomena such as the McGurk Illusion occur, it does not explain the process of phoneme categorization that ultimately determines what syllable is perceived (<xref ref-type="bibr" rid="bib30">Magnotti and Beauchamp, 2017</xref>). More generally, it is well known that cognitive and affective factors modulate our responses to multisensory stimuli (<xref ref-type="bibr" rid="bib54">Stein, 2012</xref>). In particular, the model does not currently incorporate linguistic mechanisms or top-down predictive processes, which play a central role in audiovisual speech perception—such as the integration of complementary articulatory features, lexical expectations, or syntactic constraints (<xref ref-type="bibr" rid="bib11">Campbell, 2008</xref>; <xref ref-type="bibr" rid="bib41">Peelle and Sommers, 2015</xref>; <xref ref-type="bibr" rid="bib56">Summerfield, 1987</xref>; <xref ref-type="bibr" rid="bib59">Tye-Murray et al., 2007</xref>). While a purely low-level model does not directly address these issues, the modularity of our approach makes it possible to extend the system to include high-level perceptual, cognitive, and affective factors. What is more, although this study focused on audiovisual integration in mammals, the same approach can be naturally extended to other instances of sensory integration (e.g. visuo- and audio-tactile) and animal classes. A possible extension of the model for trimodal integration is included in <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>.</p><p>Besides simulating behavioural responses, a stimulus-computable approach necessarily makes explicit all the intermediate steps of sensory information processing. This opens the system to inspection at all of its levels, thereby allowing for direct comparisons with neurophysiology (<xref ref-type="bibr" rid="bib42">Pesnot Lerousseau et al., 2022</xref>). In insect motion vision, this transparency made it possible for the Hassenstein-Reichardt detector to act as a searchlight to link computation, behavior, and physiology at the scale of individual cells (<xref ref-type="bibr" rid="bib51">Serbe et al., 2016</xref>). Being based on formally identical computational principles (<xref ref-type="bibr" rid="bib36">Parise and Ernst, 2016</xref>), the present approach holds the same potential for multisensory perception.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>The MCD population model</title><p>The architecture of each MCD unit used here is the same as described in <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>, however, units here receive time-varying visual and auditory input from spatiotopic receptive fields. The input stimuli (<inline-formula><alternatives><mml:math id="inf25"><mml:mi>s</mml:mi></mml:math><tex-math id="inft25">\begin{document}$s$\end{document}</tex-math></alternatives></inline-formula>) consist of luminance level and sound amplitude varying over space and time and are denoted as <inline-formula><alternatives><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft26">\begin{document}$s_{m}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> – with <inline-formula><alternatives><mml:math id="inf27"><mml:mi>x</mml:mi></mml:math><tex-math id="inft27">\begin{document}$x$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf28"><mml:mi>y</mml:mi></mml:math><tex-math id="inft28">\begin{document}$y$\end{document}</tex-math></alternatives></inline-formula>, representing the spatial coordinates along the horizontal and vertical axes, <inline-formula><alternatives><mml:math id="inf29"><mml:mi>t</mml:mi></mml:math><tex-math id="inft29">\begin{document}$t$\end{document}</tex-math></alternatives></inline-formula> is the temporal coordinate, and <inline-formula><alternatives><mml:math id="inf30"><mml:mi>m</mml:mi></mml:math><tex-math id="inft30">\begin{document}$m$\end{document}</tex-math></alternatives></inline-formula> is the modality (video and audio). When the input stimulus is a movie with mono audio, the visual input to each unit is a signal representing the luminance of a single pixel over time, while the auditory input is the amplitude envelope (later, we will consider more complex scenarios where auditory stimuli are also spatialized).</p><p>Each unit operates independently and detects changes in unimodal signals over time by temporal filtering based on two biphasic impulse response functions that are 90° out of phase (i.e. a quadrature pair). A physiologically plausible implementation of this process has been proposed by <xref ref-type="bibr" rid="bib3">Adelson and Bergen, 1985</xref> and consists of linear filters of the form:<disp-formula id="equ1"><label>(1)</label><alternatives><mml:math id="m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>⋅</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t1">\begin{document}$$\displaystyle f_{n}\left (t\right)=\left (\frac{t}{\tau _{bp}}\right)^{n}\cdot e^{- \frac{t}{\tau _{bp}}}\cdot \left [\frac{1}{n!}- \frac{1}{\left (n+2\right)!}\cdot \left (\frac{t}{\tau _{bp}}\right)^{2}\right ]$$\end{document}</tex-math></alternatives></disp-formula></p><p>The phase of the filter is determined by <inline-formula><alternatives><mml:math id="inf31"><mml:mi>n</mml:mi></mml:math><tex-math id="inft31">\begin{document}$n$\end{document}</tex-math></alternatives></inline-formula>, which based on <xref ref-type="bibr" rid="bib18">Emerson et al., 1992</xref> takes the values of 6 for the fast filter and 9 for the slow one. The temporal constant of the filters is determined by the parameter <inline-formula><alternatives><mml:math id="inf32"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft32">\begin{document}$\tau _{bp}$\end{document}</tex-math></alternatives></inline-formula>; in humans, its best fitting value is 0.045 s for vision and 0.0367 s for audition. In rats, the fitted temporal constant for vision and audition are nearly identical and their value is 0.010 s.</p><p>Fast and slow filters are applied to each unimodal input signal and the two resulting signals are squared and then summed. After that, a compressive non-linearity (square-root) is applied to the output, so as to constrain it within a reasonable range (<xref ref-type="bibr" rid="bib3">Adelson and Bergen, 1985</xref>). Therefore, the output of each unimodal unit feeding into the correlation detector takes the following form.<disp-formula id="equ2"><label>(2)</label><alternatives><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mn>9</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:msqrt><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t2">\begin{document}$$\displaystyle MCD_{mod}\left (x,y,t\right)=\sqrt{\left [s_{m}\left (x,y,t\right)\ast f_{6}\left (t\right)\right ]^{2}+\left [s_{m}\left (x,y,t\right)\ast f_{9}\left (t\right)\right ]^{2}},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf33"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mo>=</mml:mo><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft33">\begin{document}$mod=vid,aud$\end{document}</tex-math></alternatives></inline-formula> represents the sensory modality and * is the convolution operator.</p><p>As in the original version (<xref ref-type="bibr" rid="bib36">Parise and Ernst, 2016</xref>), each MCD consists of two sub-units, in which the unimodal input is low-pass filtered and multiplied as follows.<disp-formula id="equ3"><label> (3)</label><alternatives><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t3">\begin{document}$$\displaystyle u_{1}\left (x,y,t\right)=MCD_{vid}\left (x,y,t\right)\cdot \left [MCD_{aud}\left (x,y,t\right)\ast f_{lp}\left (t\right)\right ]$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ4"><label> (4)</label><alternatives><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t4">\begin{document}$$\displaystyle u_{2}\left (x,y,t\right)=MCD_{aud}\left (x,y,t\right)\cdot \left [MCD_{vid}\left (x,y,t\right)\ast f_{lp}\left (t\right)\right ]$$\end{document}</tex-math></alternatives></disp-formula></p><p>The impulse response of the low-pass filter of each sub-unit takes the form.<disp-formula id="equ5"><label>(5)</label><alternatives><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>⋅</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t5">\begin{document}$$\displaystyle f_{lp}\left (t\right)=\frac{t}{\tau _{lp}}\cdot e^{- \frac{t}{\tau _{lp}}},$$\end{document}</tex-math></alternatives></disp-formula></p><p>where <inline-formula><alternatives><mml:math id="inf34"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft34">\begin{document}$\tau _{lp}$\end{document}</tex-math></alternatives></inline-formula> represents the temporal constant, and its estimated value is 0.180 s for humans and 0.138 s for rats.</p><p>The response of the sub-units is eventually multiplied to obtain <inline-formula><alternatives><mml:math id="inf35"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft35">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula>, which represents the local spatiotemporal audiovisual correlation, and subtracted to obtain <inline-formula><alternatives><mml:math id="inf36"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft36">\begin{document}$MCD_{lag}$\end{document}</tex-math></alternatives></inline-formula> which describes the relative temporal order of vision and audition<disp-formula id="equ6"><label> (6)</label><alternatives><mml:math id="m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t6">\begin{document}$$\displaystyle  MCD_{corr}\left (x,y,t\right)=u_{1}\left (x,y,t\right)\cdot u_{2}\left (x,y,t\right)$$\end{document}</tex-math></alternatives></disp-formula><disp-formula id="equ7"><label> (7)</label><alternatives><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>u</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t7">\begin{document}$$\displaystyle  MCD_{lag}\left (x,y,t\right)=u_{1}\left (x,y,t\right)- u_{2}\left (x,y,t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>The outputs <inline-formula><alternatives><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft37">\begin{document}$MCD_{corr}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft38">\begin{document}$MCD_{lag}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> are the final products of the MCD.</p><p>The <bold>temporal constants</bold> of the filters were fitted using the Bayesian Adaptive Direct Search (BADS) algorithm (<xref ref-type="bibr" rid="bib1">Acerbi and Ma, 2017</xref>), set to maximize the correlation between the empirical and predicted psychometric functions of the temporal determinants of multisensory integration. For humans, that included all studies in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, besides Patient PH; For rats, it included all non-pharmacological studies in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>. To minimize the effect of starting parameters, the fitting was performed 200 times using random starting values (from 0.001 to 1.5 s). The parameters estimated using BADS were further refined using the <italic>fminsearch</italic> algorithm in MATLAB. Parameter estimation was considerably compute-intensive, hence, the amount of data had to be reduced by rescaling the videos to 15% of the original size (without affecting the frame rate). Besides reducing run-time, this simulates the (Gaussian) spatial pooling occurring in early visual pathways. Details on parameter fitting are provided below.</p><p>Now that the MCD population is defined and its parameters are fully constrained, what remains to be explained is how to read out, from the dynamic population responses, the relevant information that is needed to generate a behavioral response, such as eye movements, button presses, nose, or lick contacts, etc. While the MCD units are task-independent and operate in a purely bottom-up fashion, the exact nature of the read-out and decision-making processes depends on the behavioral task, which ultimately determines how to weigh and combine the dynamic population responses. Given that in the present study, we consider different types of behavioral experiments (investigating audiovisual integration through temporal tasks, spatial tasks, and passive observation), the read-out process for each task will be described in separate sections.</p></sec><sec id="s4-2"><title>Modeling the temporal determinants of multisensory integration</title><p>The experiments on the temporal constraints of audiovisual integration considered here rely on psychophysical forced-choice tasks to assess the effects of cross-modal lags on perceived synchrony, temporal order, and the McGurk illusion. In humans, such experiments entail pressing one of two buttons; in rats, nose-poking or licking one of two spouts. In the case of simultaneity judgments, on each trial, observers reported whether visual and auditory stimuli appeared synchronous or not <inline-formula><alternatives><mml:math id="inf39"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>n</mml:mi><mml:mi>o</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft39">\begin{document}$(resp=\left \{yes,no\right \})$\end{document}</tex-math></alternatives></inline-formula>. In temporal order judgments, observers reported which modality came first <inline-formula><alternatives><mml:math id="inf40"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft40">\begin{document}$(resp=\left \{vision\ first,\ audition\ first\right \})$\end{document}</tex-math></alternatives></inline-formula>. Finally, in the case of the McGurk, observers (humans) had to report what syllable they heard (e.g. ‘da,’ ‘ga,’ or ‘ba,’ usually recoded as ‘fused’ and ‘non-fused percept’ or ‘illusion present’ and ‘illusion absent’). When plotted against lag, the resulting empirical psychometric functions describe how audiovisual timing affects perceived synchrony, temporal order, and the McGurk illusion. Here, we model these tasks using the same read-out and decision-making process.</p><p>To account for observed responses, the dynamic, high-bandwidth population responses must be transformed (compressed) into a single number, representing response probability for each lag. In line with standard procedures (<xref ref-type="bibr" rid="bib37">Parise and Ernst, 2017</xref>), this was achieved by integrating <inline-formula><alternatives><mml:math id="inf41"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft41">\begin{document}$MCD_{corr}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf42"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft42">\begin{document}$MCD_{lag}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> over time and space, so as to obtain two summary decision variables<disp-formula id="equ8"><label>(8)</label><alternatives><mml:math id="m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t8">\begin{document}$$\displaystyle  \overline{MCD_{corr}} =\sum _{x}\sum _{y}\sum _{t}MCD_{corr}\left (x,y,t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>and<disp-formula id="equ9"><label>(9)</label><alternatives><mml:math id="m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t9">\begin{document}$$\displaystyle  \overline{MCD_{lag}}=\sum _{x}\sum _{y}\sum _{t}MCD_{lag}\left (x,y,t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>The temporal window for these analyses consisted of the duration of each stimulus, plus 2 s before and after (during which the audio was silent, and the video displayed a still frame); the exact extension of the temporal window used for the analyses had minimal effect on the results. <inline-formula><alternatives><mml:math id="inf43"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft43">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf44"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft44">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> are eventually linearly weighted and transformed into response probabilities through a cumulative normal function as follows<disp-formula id="equ10"><label> (10)</label><alternatives><mml:math id="m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t10">\begin{document}$$\displaystyle p_{MCD}\left (resp\right)=\mathrm{\Phi }\left (\beta _{crit}+\beta _{corr}\cdot \overline {MCD_{corr}}+\beta _{lag}\cdot \overline{MCD_{lag}}\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf45"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi mathvariant="normal">Φ</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft45">\begin{document}$\Phi$\end{document}</tex-math></alternatives></inline-formula> represents the cumulative normal, <inline-formula><alternatives><mml:math id="inf46"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft46">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf47"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft47">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula> are linear coefficients that weigh and scale <inline-formula><alternatives><mml:math id="inf48"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft48">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf49"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft49">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula>. <inline-formula><alternatives><mml:math id="inf50"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft50">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula> is a bias term, which corresponds to the response criterion. Finally, <inline-formula><alternatives><mml:math id="inf51"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>p</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft51">\begin{document}$p_{MCD}\left (resp\right)$\end{document}</tex-math></alternatives></inline-formula> is the probability of a response, which is <inline-formula><alternatives><mml:math id="inf52"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>S</mml:mi><mml:mi>y</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>h</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft52">\begin{document}$p_{MCD} (Synchronous)$\end{document}</tex-math></alternatives></inline-formula> for the simultaneity judgment task, <inline-formula><alternatives><mml:math id="inf53"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>A</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>i</mml:mi><mml:mi>r</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft53">\begin{document}$p_{MCD}\left (Audio\ first\right)$\end{document}</tex-math></alternatives></inline-formula> for the temporal order judgment task, and <inline-formula><alternatives><mml:math id="inf54"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>F</mml:mi><mml:mi>u</mml:mi><mml:mi>s</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft54">\begin{document}$p_{MCD}\left (Fusion\right)$\end{document}</tex-math></alternatives></inline-formula> for the McGurk task. Note how <xref ref-type="disp-formula" rid="equ9">Equation 9</xref> entails that simultaneity judgments, temporal order judgments, and the McGurk illusions are all simulated using the very same model architecture.</p><p>With the population model fully constrained (see above), the only free parameters in the present simulations are the ones controlling the decision-making process: <inline-formula><alternatives><mml:math id="inf55"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft55">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf56"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft56">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf57"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft57">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula>. These were separately fitted for each experiment using <italic>fitglm</italic> MATLAB (binomial distribution and probit link function). For the simulations of the effects of distance on audiovisual temporal order judgments in humans (<xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>; <xref ref-type="fig" rid="fig2">Figure 2B</xref>), and the manipulation of loudness in rats (<xref ref-type="bibr" rid="bib49">Schormans and Allman, 2018</xref>; <xref ref-type="fig" rid="fig2">Figure 2D</xref>) <inline-formula><alternatives><mml:math id="inf58"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft58">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf59"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft59">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf60"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft60">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula> were constant across conditions (i.e. distance <xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref> or loudness <xref ref-type="bibr" rid="bib49">Schormans and Allman, 2018</xref>). This way, differences in the psychometric functions as a function of distance (<xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>) or loudness (<xref ref-type="bibr" rid="bib49">Schormans and Allman, 2018</xref>) of the stimuli are fully explained by the MCD. Overall, the model provided a good fit to the empirical psychometric functions, and the average Pearson correlation between human and model response (weighted by sample size) is 0.981 for humans and 0.994 for rats. Naturally, model-data correlation varied across experiments, largely due to sample size. This can be appreciated when the MCD-data correlation for each experiment is plotted against the number of trials for each lag in a funnel plot (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1I</xref>). The number of trials for each lag determines the binomial error for data points in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplements 1</xref> and <xref ref-type="fig" rid="fig2s2">2</xref>; accordingly, the funnel plot shows that lower MCD-data correlation is more commonly observed for curves based on smaller sample size. This shows that the upper limit in the MCD-data correlation is mostly constrained by the reliability of the dataset, rather than systematic errors in the model.</p><p>To assess the contribution of the low-level properties of the stimuli on model’s performance, we ran a permutation test, where psychometric curves were generated (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref>) using stimuli from different experiments (but with the same manipulation of lag). If the low-level properties of the stimuli play a significant role, the correlation between data and model with permuted stimuli should be lower than with non-permuted stimuli. For this permutation, we used the data from simultaneity judgment tasks on humans (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>), as with 27 individual experiments, there are enough permutations to render the test meaningful. For that, we used the temporal constants of the MCD fitted before, so that each psychometric curve, each permutation had three free parameters (<inline-formula><alternatives><mml:math id="inf61"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft61">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf62"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft62">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf63"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft63">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula>). The results from 200 k permutations demonstrate that the goodness of fit obtained with the original stimuli is superior to that of permuted stimuli. Specifically, the permuted distribution of the mean Pearson correlation of predicted vs. empirical psychometric curves had a mean of 0.972 (σ=0.0036), while such a correlation rose to 0.989 when the MCD received the original stimuli.</p></sec><sec id="s4-3"><title>Modeling the spatial determinants of multisensory integration</title><p>Most studies on audiovisual space perception only investigated the horizontal spatial dimension; hence the stimuli can be reduced to <inline-formula><alternatives><mml:math id="inf64"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft64">\begin{document}$s_{m}\left (x,t\right)$\end{document}</tex-math></alternatives></inline-formula> instead of <inline-formula><alternatives><mml:math id="inf65"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft65">\begin{document}$s_{m}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula>, as in the simulations above (see <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). Additionally, based on preliminary observations, the output <inline-formula><alternatives><mml:math id="inf66"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft66">\begin{document}$MCD_{lag}$\end{document}</tex-math></alternatives></inline-formula> does not seem necessary to account for audiovisual integration in space, hence, only the population response <inline-formula><alternatives><mml:math id="inf67"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft67">\begin{document}$MCD_{corr}\left (x,t\right)$\end{document}</tex-math></alternatives></inline-formula> (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) will be considered here.</p><p><italic>MCD and MLE – simulation of</italic> <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>. In its general form, the MLE model can be expressed probabilistically as <inline-formula><alternatives><mml:math id="inf68"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced><mml:mo>∝</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo><mml:mo>∙</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft68">\begin{document}$p_{MLE}\left (x\right)\propto p_{vid}\left (x\right)\cdot p_{aud}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula>, where <inline-formula><alternatives><mml:math id="inf69"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft69">\begin{document}$p_{vid}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf70"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft70">\begin{document}$p_{aud}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> represent the probability distribution of the unimodal location estimate (i.e. likelihood functions), and <inline-formula><alternatives><mml:math id="inf71"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft71">\begin{document}$p_{MLE}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> is the bimodal distribution. When the <inline-formula><alternatives><mml:math id="inf72"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft72">\begin{document}$p_{vid}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf73"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft73">\begin{document}$p_{aud}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> follow a Gaussian distribution, also <inline-formula><alternatives><mml:math id="inf74"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft74">\begin{document}$p_{MLE}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> is Gaussian, with the variance (<inline-formula><alternatives><mml:math id="inf75"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft75">\begin{document}$\sigma _{MLE}^{2}$\end{document}</tex-math></alternatives></inline-formula>) equal to the product divided by the sum of the unimodal variances:<disp-formula id="equ11"><label>(11)</label><alternatives><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>⋅</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t11">\begin{document}$$\displaystyle \sigma _{MLE}^{2}=\frac{\sigma _{vid}^{2}\cdot \sigma _{aud}^{2}}{\sigma _{vid}^{2}+\sigma _{aud}^{2}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>and the mean consisting of a weighted average of the unimodal means<disp-formula id="equ12"><label>(12)</label><alternatives><mml:math id="m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>⋅</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t12">\begin{document}$$\displaystyle \mu _{MLE}=\omega _{vid}\cdot \mu _{vid}+\omega _{aud}\cdot \mu _{aud}$$\end{document}</tex-math></alternatives></disp-formula></p><p>where<disp-formula id="equ13"><label>(13)</label><alternatives><mml:math id="m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t13">\begin{document}$$\displaystyle \omega _{vid}=\frac{1/\sigma _{vid}^{2}}{1/\sigma _{vid}^{2}+1/\sigma _{aud}^{2}}$$\end{document}</tex-math></alternatives></disp-formula></p><p>and<disp-formula id="equ14"><label>(14)</label><alternatives><mml:math id="m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>ω</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t14">\begin{document}$$\displaystyle \omega _{aud}=1- \omega _{vid}$$\end{document}</tex-math></alternatives></disp-formula></p><p>To test whether a population of MCDs can replicate the study of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>, this simulation has two main goals. The first one is to compare the predictions of the MCD with that of the MLE model; the second one is to test whether the MCD model can predict observers’ responses.</p><p>To simulate the study of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>, we first need to generate the stimuli. The visual stimuli consisted of a 1-D Gaussian luminance profiles, presented for 10ms. Their standard deviations, which determined visual spatial reliability, were defined as the standard deviation of the visual psychometric functions. Likewise, the auditory stimuli also consisted of a 1-D Gaussian sound intensity profile (with a standard deviation determined by the unimodal auditory psychometric function). Note that the spatial reliability in the MCD model jointly depends on the stimulus and the receptive field of the input units; however, teasing apart the differential effects induced by these two sources of spatial uncertainty is beyond the scope of the present study. Hence, for simplicity, here we injected all spatial uncertainty into the stimulus (see also; <xref ref-type="bibr" rid="bib36">Parise and Ernst, 2016</xref>; <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>). For this simulation, we only used the data from observer LM of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>, as it is the only participant for which the full dataset is publicly available (other observers, however, had similar results).</p><p>The stimuli are fed to the model to obtain the population response <inline-formula><alternatives><mml:math id="inf76"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft76">\begin{document}$MCD_{corr}\left (x,t\right)$\end{document}</tex-math></alternatives></inline-formula> (see <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>), which is marginalized over time as follows.<disp-formula id="equ15"><label>(15)</label><alternatives><mml:math id="m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t15">\begin{document}$$\displaystyle  \overline{MCD_{corr}}\left (x\right)=\sum _{t}MCD_{corr}\left (x,t\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>This provides a distribution of the population response over the horizontal spatial dimension. Finally, a divisive normalization is performed to transform model response into a probability distribution.<disp-formula id="equ16"><label> (16)</label><alternatives><mml:math id="m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t16">\begin{document}$$\displaystyle  p_{MCD}\left (x\right)=\frac{\overline{MCD_{MCD}}\left (x\right)}{\sum _{x}\overline{MCD_{MCD}}\left (x\right)}$$\end{document}</tex-math></alternatives></disp-formula></p><p>It is important to note that <xref ref-type="disp-formula" rid="equ15 equ16">Equations 15; 16</xref> have no free parameters, and all simulations are now performed with a fully constrained model. To test whether the MCD model can perform audiovisual integration according to the MLE model, we replicated the various conditions run by observer LM and calculated the bimodal likelihoods distribution predicted by the MLE and MCD models (i.e. <inline-formula><alternatives><mml:math id="inf77"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft77">\begin{document}$p_{MLE}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf78"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft78">\begin{document}$p_{MCD}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula>). These were statistically identical, when considering rounding errors. The results of these simulations are plotted in <xref ref-type="fig" rid="fig4">Figure 4C</xref>, and displayed as cumulative distributions as in <xref ref-type="fig" rid="fig1">Figure 1</xref> of Alais and Burr, 2004.</p><p>Once demonstrated that <inline-formula><alternatives><mml:math id="inf79"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft79">\begin{document}$p_{MLE}\left (x\right)=p_{MCD}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula>, it is clear that the MCD is equally capable of predicting observed responses. However, for completeness, we compared the prediction of the MCD to the empirical data: the results demonstrate that, just like the MLE model, a population of MCDs can predict both audiovisual bias (<xref ref-type="fig" rid="fig4">Figure 4D</xref>) and just noticeable differences (JND, <xref ref-type="fig" rid="fig4">Figure 4E</xref>). A MATLAB implementation of this simulation is included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p></sec><sec id="s4-4"><title>MCD and BCI – simulation of <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref></title><p>To account for the spatial breakdown of multisensory integration, the BCI model operates in a hierarchical fashion: first, it estimates the probability that audiovisual stimuli share a common cause <inline-formula><alternatives><mml:math id="inf80"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft80">\begin{document}$(p\left (C=1\right))$\end{document}</tex-math></alternatives></inline-formula>. Next, the model weighs and integrates the unimodal and bimodal information (<inline-formula><alternatives><mml:math id="inf81"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft81">\begin{document}$p_{mod}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf82"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft82">\begin{document}$p_{mle}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula>) as follows:<disp-formula id="equ17"><label>(17)</label><alternatives><mml:math id="m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>L</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>B</mml:mi><mml:mi>C</mml:mi><mml:mi>I</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t17">\begin{document}$$\displaystyle p_{BCI,mod}\left (x\right)=p_{BCI}\left (C=1\right)\cdot p_{MLE}\left (x\right)+\left [1- p_{BCI}\left (C=1\right)\right ]\cdot p_{mod}\left (x\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>All the terms in <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> have a clear analogue in the computations of the MCD population response: <inline-formula><alternatives><mml:math id="inf83"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft83">\begin{document}$p_{mle}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> corresponds to <inline-formula><alternatives><mml:math id="inf84"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft84">\begin{document}$p_{MCD}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> (see previous section). Likewise, the homologous of <inline-formula><alternatives><mml:math id="inf85"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft85">\begin{document}$p_{mod}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> can be obtained by marginalizing over time and normalizing the output of the unimodal units as follows (see <xref ref-type="disp-formula" rid="equ15">Equation 15</xref>)<disp-formula id="equ18"><label>(18)</label><alternatives><mml:math id="m18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t18">\begin{document}$$\displaystyle  \overline{MCD_{mod}}\left (x\right)=\frac{\sum _{t}MCD_{mod}\left (x,t\right)}{\sum _{x}\sum _{t}MCD_{mod}\left (x,t\right)}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Finally, the MCD homologous of BCI’s <inline-formula><alternatives><mml:math id="inf86"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft86">\begin{document}$p(C=1)$\end{document}</tex-math></alternatives></inline-formula> can be read out from the population response following the same logic as <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>:<disp-formula id="equ19"><label>(19)</label><alternatives><mml:math id="m19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mtext> </mml:mtext></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t19">\begin{document}$$\displaystyle  p_{MCD}\left (C=1\right)=\mathrm{\ }\mathrm{\Phi }\left (\beta _{crit}+\beta _{corr}\log _{10} \sum _{x}\sum _{t}MCD_{corr}\left (x,t\right)\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Indeed, the output <inline-formula><alternatives><mml:math id="inf87"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft87">\begin{document}$\overline{MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ8">Equation 8</xref>) not only decreases with increasing temporal disparity (see <xref ref-type="fig" rid="fig1">Figure 1C</xref>, left), but it also decreases with increasing spatial disparity (<xref ref-type="fig" rid="fig4">Figure 4F</xref>), thereby providing a measure of spatiotemporal coincidence that can be readily transformed into a probability for common cause (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>). Here, we found that including a compressive non-linearity (logarithm of the total <inline-formula><alternatives><mml:math id="inf88"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft88">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> response) provided a tighter fit to the empirical data. With <inline-formula><alternatives><mml:math id="inf89"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="inft89">\begin{document}$p_{MCD}\left (C=1\right)$\end{document}</tex-math></alternatives></inline-formula> representing the probability that vision and audition share a common cause, and <inline-formula><alternatives><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft90">\begin{document}$p_{MCD}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf91"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft91">\begin{document}$\overline{MCD_{mod}}\left (x\right)$\end{document}</tex-math></alternatives></inline-formula> representing the bimodal and unimodal population responses, the MCD model can simulate observed responses as follows:<disp-formula id="equ20"><label>(20)</label><alternatives><mml:math id="m20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi><mml:mo>,</mml:mo><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t20">\begin{document}$$\displaystyle p_{MCD,mod}\left (x\right)=p_{MCD}\left (C=1\right)\cdot p_{MCD}\left (x\right)+\left [1- p_{MCD}\left (C=1\right)\right ]\cdot \overline{MCD_{mod}}\left (x\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>The similarity between <xref ref-type="disp-formula" rid="equ20">Equation 20</xref> and <xref ref-type="disp-formula" rid="equ17">Equation 17</xref> demonstrates the fundamental homology of the BCI and MCD models: what remains to be tested is whether the MCD can also account for the results of <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>. For that, just like the BCI model, the MCD model also relies on four free parameters: two are shared by both models, and represent the spatial uncertainty (i.e. the variance) of the unimodal input (i.e. <inline-formula><alternatives><mml:math id="inf92"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft92">\begin{document}$\sigma _{vid}^{2}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf93"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft93">\begin{document}$\sigma _{aud}^{2}$\end{document}</tex-math></alternatives></inline-formula>). Additionally, the MCD model then needs two linear coefficients (slope <inline-formula><alternatives><mml:math id="inf94"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft94">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula> and intercept <inline-formula><alternatives><mml:math id="inf95"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft95">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula>) to transform the dynamic population response into a probability of a common cause. Conversely, the remaining two parameters of the BCI model correspond to a prior for common cause and another for central location, neither of which are necessary to account for observed responses in the present framework. Parameters were fitted using BADS (<xref ref-type="bibr" rid="bib1">Acerbi and Ma, 2017</xref>) set to maximize the Pearson correlation between model and human responses (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). Overall, the MCD provided an excellent fit to the empirical data (<italic>r</italic>=0.99, <xref ref-type="fig" rid="fig4">Figure 4G</xref>), even slightly exceeding the performance of the BCI model while relying on the same degrees of freedom. Given that the fitted value of the slope parameter (<inline-formula><alternatives><mml:math id="inf96"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft96">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>) approached 1, we repeated the fitting while removing <inline-formula><alternatives><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft97">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula> from <xref ref-type="disp-formula" rid="equ19">Equation 19</xref>: even with just three free parameters (one fewer than the BCI model), the MCD is in line with the BCI model and the correlation with the empirical data was 0.98. A Matlab implementation of this simulation is included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p></sec><sec id="s4-5"><title>MCD and BCI – simulation of <xref ref-type="bibr" rid="bib33">Mohl et al., 2020</xref></title><p>Mohl and colleagues used eye movements to test whether humans and monkeys integrate audiovisual spatial cues according to BCI. The targets consisted of either unimodal or bimodal stimuli. Following the same logic as the simulation of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref> (see above), the unimodal input consisted of impulses with a Gaussian spatial profile (<xref ref-type="fig" rid="fig4">Figure 4A–B</xref>), whose the variance (i.e., <inline-formula><alternatives><mml:math id="inf98"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft98">\begin{document}$\sigma _{vid}^{2}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf99"><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:math><tex-math id="inft99">\begin{document}$\sigma _{aud}^{2}$\end{document}</tex-math></alternatives></inline-formula>) was set equal to the variance of the fixations measured in the unimodal trials (averaged across observers). Although the probability of a single fixation decreases with increasing disparity (<xref ref-type="fig" rid="fig4">Figure 4H</xref>, right), observers sometimes failed to make a second fixation even when the stimuli were noticeably far apart (lapses). This was especially true for monkeys, which had a lapse rate of 16%, indicative of low attention and compliance. To account for this, we can modify <xref ref-type="disp-formula" rid="equ19">Equation 19</xref> and obtain the probability of a single fixation as follows:<disp-formula id="equ21"><label> (21)</label><alternatives><mml:math id="m21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mrow><mml:mi mathvariant="normal">Φ</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>log</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:munder><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t21">\begin{document}$$\displaystyle  p_{MCD}\left (C=1\right)=p_{lapse}+\left (1- 2p_{lapse}\right)\cdot \mathrm{\Phi }\left (\beta _{crit}+\beta _{corr}\log _{10} \sum _{x}\sum _{t}MCD_{corr}\left (x,t\right)\right)$$\end{document}</tex-math></alternatives></disp-formula></p><p>Here, <inline-formula><alternatives><mml:math id="inf100"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft100">\begin{document}$p_{lapse}$\end{document}</tex-math></alternatives></inline-formula> is a free parameter that represents the probability of making the incorrect number of fixations, irrespective of the discrepancy. As in <xref ref-type="disp-formula" rid="equ19">Equation 19</xref>, <inline-formula><alternatives><mml:math id="inf101"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft101">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf102"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft102">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula> are free parameters that transform the dynamic population response into a probability of a common cause (i.e. single fixation). <xref ref-type="disp-formula" rid="equ21">Equation 21</xref> could tightly reproduce the observed probability of a single fixation (<xref ref-type="fig" rid="fig4">Figure 4H</xref>, right), and the Pearson correlation between the model and data was 0.995 for monkeys and 0.988 for humans.</p><p>With the parameters <inline-formula><alternatives><mml:math id="inf103"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft103">\begin{document}$p_{lapse}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf104"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft104">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf105"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft105">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula> fitted to the probability of a single fixation (i.e. the probability of a common cause), it is now possible to predict gaze direction (i.e. the perceived location of the stimuli) with zero free parameters. For that, we can use <xref ref-type="disp-formula" rid="equ21">Equation 21</xref> to get the probability of a common cause and predict gaze direction using <xref ref-type="disp-formula" rid="equ20">Equation 20</xref>. The distribution of fixations predicted by the MCD closely follows the empirical histogram in both species (<xref ref-type="fig" rid="fig4">Figure 4H</xref>) and the correlation between the model and data was 0.9 for monkeys and 0.93 for humans. Note that <xref ref-type="fig" rid="fig4">Figure 4H</xref> shows only a subset of the 20 conditions tested in the experiment (the same subset of conditions shown in the original paper). A MATLAB implementation of this simulation is included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p></sec><sec id="s4-6"><title>MCD and audiovisual gaze behavior</title><p>To test whether the dynamic MCD population response can account for gaze behavior during passive observation of audiovisual footage, a simple solution is to measure whether observers preferentially looked where the <inline-formula><alternatives><mml:math id="inf106"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft106">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> response is maximal. Such an analysis was separately performed for each frame. For that, gaze directions were first low pass filtered with a Gaussian kernel (<inline-formula><alternatives><mml:math id="inf107"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>14</mml:mn><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mi>i</mml:mi><mml:mi>x</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft107">\begin{document}$\sigma =14\,pixels$\end{document}</tex-math></alternatives></inline-formula>) and normalized to probabilities <inline-formula><alternatives><mml:math id="inf108"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>)</mml:mo></mml:math><tex-math id="inft108">\begin{document}$p_{gaze}\left (x,y\right)$\end{document}</tex-math></alternatives></inline-formula>. Next, we calculated the average MCD responses at gaze direction for each frame (t); this was done by weighing <inline-formula><alternatives><mml:math id="inf109"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft109">\begin{document}$MCD_{corr}$\end{document}</tex-math></alternatives></inline-formula> by <inline-formula><alternatives><mml:math id="inf110"><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft110">\begin{document}$p_{gaze}$\end{document}</tex-math></alternatives></inline-formula> as follows<disp-formula id="equ22"><label>(22)</label><alternatives><mml:math id="m22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:munder><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t22">\begin{document}$$\displaystyle  \overline{MCD}_{gaze}(t) = \sum_{x} \sum_{y} p_{gaze}(x,y,t) \cdot MCD_{corr}(x,y,t)$$\end{document}</tex-math></alternatives></disp-formula></p><p>To assess the MCD response at gaze <inline-formula><alternatives><mml:math id="inf111"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mo stretchy="false">(</mml:mo><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft111">\begin{document}$(\overline{MCD_{gaze}}\left (t\right))$\end{document}</tex-math></alternatives></inline-formula> is larger than the frame average, we calculated the standardized mean difference (SMD) for each frame as follows<disp-formula id="equ23"><label>(23)</label><alternatives><mml:math id="m23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>S</mml:mi><mml:mi>M</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>a</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mo>(</mml:mo><mml:mi>t</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t23">\begin{document}$$\displaystyle SMD\left (t\right)=\frac{\overline{MCD_{gaze}}\left (t\right)- \mu _{x,y}\left [MCD_{corr}\left (x,y,t\right)\right ]}{\sigma _{x,y}\left [MCD_{corr}\left (x,y,t\right)\right ]}$$\end{document}</tex-math></alternatives></disp-formula></p><p>Across the over 16,000 frames of the available dataset, the average SMD was 2.03 (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Given that the standardized mean difference serves as a metric for effect size, and that effect sizes surpassing 1.2 are deemed very large (<xref ref-type="bibr" rid="bib47">Sawilowsky, 2009</xref>), it is remarkable that the MCD population model can so tightly account for human gaze behavior in a purely bottom-up fashion and without free parameters.</p></sec><sec id="s4-7"><title>Trimodal integration</title><p>Much like Bayesian ideal observer models, the present framework can be naturally extended to trimodal integration. As described in <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, the, <inline-formula><alternatives><mml:math id="inf112"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfenced></mml:math><tex-math id="inft112">\begin{document}$MCD_{corr}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> response is based on the pointwise product of input transients across modalities. In the bimodal case, this corresponds to the product of auditory and visual transient channels. For three modalities (e.g. auditory, visual, tactile), this generalizes to a trimodal coincidence detector, in which MCD units compute:<disp-formula id="equ24"><alternatives><mml:math id="m24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mi>u</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>⋅</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∗</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="t24">\begin{document}$$\displaystyle MCD_{trimodal}(x,y,t) = MCD_{aud}(x,y,t) \cdot \left[ MCD_{aud}(x,y,t) \ast f_{lp}(t) \right] \cdot MCD_{vid}(x,y,t) \cdot \left[MCD_{vid}(x,y,t) \ast f_{lp}(t) \right] \cdot MCD_{tac}(x,y,t) \cdot \left[ MCD_{tac}(x,y,t) \ast f_{lp}(t) \right] $$\end{document}</tex-math></alternatives></disp-formula></p><p>This detector responds maximally when transients in all three modalities co-occur in time and space. As in the bimodal case (<xref ref-type="fig" rid="fig4">Figure 4</xref>), the trimodal MCD response closely approximates the predictions of the maximum likelihood estimation (MLE) model (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>).</p><p><inline-formula><alternatives><mml:math id="inf113"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft113">\begin{document}$MCD_{lag}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) is instead defined via opponency (subtraction) between the two subunits of the MCD, which introduces a directional asymmetry between modalities. This structure makes it fundamentally pairwise. As a result, extending <inline-formula><alternatives><mml:math id="inf114"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft114">\begin{document}$MCD_{lag}\left (x,y,t\right)$\end{document}</tex-math></alternatives></inline-formula> to three modalities would require computing pairwise lag estimates (AV, VT, AT) independently.</p></sec><sec id="s4-8"><title>Code availability statement</title><p>A variety MATLAB script running the MCD population model is included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>. This includes a code running the MCD on real-life footage (i.e., this is what was used to model MCD responses to the temporal determinants of multisensory integration, spatial orienting, and <xref ref-type="fig" rid="fig5">Figure 5</xref>). Additional codes simulate the experiment of <xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref> and all simulations in <xref ref-type="fig" rid="fig5">Figure 5</xref>.</p></sec><sec id="s4-9"><title>Pre-processing of ecological audiovisual footage</title><p>The diversity of the stimuli used in this dataset requires some preprocessing before the stimuli can be fed to the population model. First, all movies were converted to grayscale (scaled between 0 and 1) and the soundtrack was converted to rms envelope (scaled between 0 and 1), thereby removing chromatic and tonal information. Movies were then padded with 2 s of frozen frames at onset and offset to accommodate for the manipulation of the lag. Finally, the luminance of the first still frame was set as baseline and subtracted from all subsequent frames (see Matlab codes in the <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>). Along with the padding, this helps minimizing transient artefacts induced by the onset of the video.</p><p>Video frames were scaled to 15% of the original size, and the static background was cropped. On a practical side, this made the simulations much faster (which is crucial for parameter estimation); on a theoretical side, such a down sampling simulates the Gaussian spatial pooling of luminance across the visual field (unfortunately, the present datasets do not provide sufficient information to convert pixels into visual angles). In a similar fashion, we down sampled sound envelope to match the frame rate of the video.</p></sec><sec id="s4-10"><title>Simulation of <xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref></title><p>For the simulations of <xref ref-type="bibr" rid="bib21">Horsfall et al., 2021</xref>, the envelope of the auditory stimuli was extracted from the waveforms shown in the figures of the original publication. That was done using WebPlotDigitizer to trace the profile of the waveforms; the digitized points were then interpolated and resampled at 1000 Hz. To preserve the manipulation of the direct-to-reverberant waves, the section of the envelope with the reverberant signal was identical across the four conditions (i.e. distances), so that what varied across conditions was the initial portion of the signals (the direct waves). For the simulations of <xref ref-type="bibr" rid="bib21">Horsfall et al., 2021</xref>, all four psychometric functions were fitted simultaneously, so that the four psychometric functions all relied on just three free parameters: the ones related to the decision-making process. A MATLAB code running this simulation is now included as <xref ref-type="supplementary-material" rid="scode1">Source code 1</xref>.</p></sec><sec id="s4-11"><title>Individual observers’ analysis</title><p>Most of the simulations described so far rely on group-level data, where psychometric curves represent the average response across the pool of observers that took part in each experiment. Individual psychometric functions, however, sometimes vary dramatically across observers; hence, one might wonder whether the MCD, besides predicting stimulus-driven variations in the psychometric functions, can also capture individual differences. A recent study by Yarrow and colleagues (<xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>) directly addressed this question, and concluded that models of the Independent Channels family outperform the MCD at fitting responses individual differences.</p><p>Although it can be easily shown that such a conclusion was supported by an incomplete implementation of the MCD (which did not include the <inline-formula><alternatives><mml:math id="inf115"><mml:msub><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft115">\begin{document}$MCD_{lag}$\end{document}</tex-math></alternatives></inline-formula> output), a closer look at the two models against the same datasets help us illustrate their fundamental difference and highlight a key drawback of perceptual models that take parameters as input. Therefore, we first simulated the impulse stimuli used by Yarrow and colleagues (<xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>), fed them to the MCD, and used <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> to generate the individual psychometric curves. Given that their stimuli consisted of temporal impulses with no spatiotemporal manipulation, a single MCD unit is sufficient to run these simulations. Overall, the model provided an excellent fit to the original data and tightly captured individual differences: the average Pearson correlation between predicted and empirical psychometric functions across the 57 curves shown in <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref> is 0.98. Importantly, for such simulations, the MCD was fully constrained, and the only free parameters (3 in total) were the linear coefficients of <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>, which describe how the output of the MCD is used for perceptual decision-making. For comparison, also Independent Channels models achieved analogous goodness of fit, but they required at least five free parameters (depending on the exact implementation <xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>).</p><p>To assess the generalizability of this finding, we additionally simulated the individual psychometric functions from the experiments that informed the architecture of the MCD units used here. Specifically, <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref> run two psychophysical studies using minimalistic stimuli that only varied over time. In the first one, auditory and visual stimuli consisted of step increments and/or decrements in intensity. Audiovisual lag was parametrically manipulated using the method of constant stimuli, and observers were required to perform both simultaneity and temporal order judgments. Following the logic described above, we fed the stimuli to the model, and used <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> (with three free parameters) to simulate human responses. Results demonstrate that the MCD can account for individual differences regardless of the task (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>): the average Pearson correlation between empirical and predicted psychometric curves was 0.97 for the simultaneity judgments, and 0.96 for the temporal order judgments (64 individual psychometric curves from eight observers, for a total of 9600 trials). This generalizes the results of the previous simulation to a different type of stimuli (steps vs. impulses) and extends them to include a task, the temporal order judgment, which was not considered by Yarrow and colleagues (<xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>) (whose model can only perform simultaneity judgments).</p><p>The second study of <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref> consists of simultaneity judgments for periodic audiovisual stimuli defined by a square-wave intensity envelope (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>). Simultaneity judgments for this type of periodic stimuli are also periodic, with two complete oscillations in perceived simultaneity for each cycle of phase shifts between the senses (a phenomenon known as frequency doubling). Once again, using <xref ref-type="disp-formula" rid="equ1">Equation 10</xref>, the MCD could account for individual differences in observed behavior (five psychometric curves from five observers, for a total of 3000 trials) with an average Pearson correlation of 0.93, while relying on just three free parameters.</p><p>It is important to note that for these simulations, the same MCD model accurately predicted (in a purely bottom-up fashion) bell-shaped SJ curves for non-periodic stimuli, and sinusoidal curves for periodic stimuli. Alternative models of audiovisual simultaneity that directly take lag as input always enforce bell-shaped psychometric functions, where perceived synchrony monotonically decrease as we move away from the point of subjective simultaneity. As a result, in the absence of ad-hoc adjustments they all necessarily fail at replicating the results of Experiment 2 of <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>, due to their inability to generate periodic psychometric functions. Conversely, the MCD is agnostic regarding the shape of the psychometric functions, hence the very same model used to predict the standard bell-shaped simultaneity judgments of <xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref> can also predict the periodic psychometric functions of <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>, including individual differences across observers (all while relying on just three free parameters).</p></sec><sec id="s4-12"><title>Datasets</title><p>To thoroughly compare observed and model behavior, this study requires a large and diverse dataset consisting of both the raw stimuli and observers’ responses. For that, we adopted a convenience sampling and simulated the studies for which both stimuli and responses were available (either in public repositories, shared by the authors, or extracted from published figures). The inclusion criteria depend on what aspect of multisensory integration is being investigated, and they are described below.</p><p>For the <bold>temporal determinants of multisensory integration in humans</bold>, we only included studies that: (1) used real-life audiovisual footage, (2) performed a parametric manipulation of lag, and (3) engaged observers in a forced-choice behavioral task. Forty-three individual experiments met the inclusion criteria (<xref ref-type="fig" rid="fig2">Figure 2A-B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). These varied in terms of stimuli, observers, and tasks. In terms of stimuli, the dataset consists of responses from 105 unique real-life videos (see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> and <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>). The majority of the videos represented audiovisual speech (possibly the most common stimulus in audiovisual research), but they varied in terms of content (i.e. syllables, words, full sentences, etc.), intelligibility (i.e. sine-wave speech, amplitude-modulated noise, blurred visuals, etc.), composition (i.e. full face, mouth-only, oval frame, etc.), speaker identity, etc. The remaining non-speech stimuli consist of footage of actors playing a piano or a flute. The study from <xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref> was included in the dataset because, even if the visual stimuli were minimalistic (blobs), the auditory stimuli consisted of ecological auditory depth cues depth cues recorded in a real reverberant environment (the Sydney Opera House, see below for details on the dataset). The dataset contains forced-choice responses to three different tasks: speech categorization (i.e., for the McGurk illusion), simultaneity judgments, and temporal order judgment. In terms of observers, besides the general population, the dataset consists of experimental groups varying in terms of age, musical expertise, and even includes a patient, PH, who reports hearing speech before seeing mouth movements after a lesion in the pons and basal ganglia (<xref ref-type="bibr" rid="bib20">Freeman et al., 2013</xref>). Taken together, the dataset consists of ~1 k individual psychometric functions, from 454 unique observers, for a total of ~300 k trials; the psychometric curves for each experiment are shown in <xref ref-type="fig" rid="fig2">Figure 2A-B</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>. All these simulations are based on psychometric functions averaged across observers, for simulations of individual observers, see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplements 3</xref>–<xref ref-type="fig" rid="fig2s5">5</xref>.</p><p>For the <bold>temporal determinants of multisensory integration in rats</bold>, we included studies that performed a parametric manipulation of lag and engaged rats in simultaneity and temporal order judgment tasks. Sixteen individual experiments (<xref ref-type="bibr" rid="bib28">Mafi et al., 2022</xref>, <xref ref-type="bibr" rid="bib49">Schormans and Allman, 2018</xref>, <xref ref-type="bibr" rid="bib8">Al Youzbaki et al., 2023</xref>, <xref ref-type="bibr" rid="bib50">Schormans and Allman, 2023</xref>, <xref ref-type="bibr" rid="bib48">Schormans et al., 2016</xref>, <xref ref-type="bibr" rid="bib40">Paulcan et al., 2023</xref>) met the inclusion criteria (<xref ref-type="fig" rid="fig2">Figure 2C-D</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> and): all of them used minimalistic audiovisual stimuli (clicks and flashes) and with a parametric manipulation of audiovisual lag. Overall, the dataset consists of ~190 individual psychometric functions, from 110 rats (and 10 humans), for a total of ~300 k trials.</p><p>For the case of the <bold>spatial determinants of multisensory integration</bold><italic>,</italic> to the best of our knowledge, there are no available datasets with both stimuli and psychophysical responses. Fortunately, however, the spatial aspects of multisensory integration are often studied with minimalistic audiovisual stimuli (e.g. clicks and blobs), which can be simulated exactly. Audiovisual integration in space is commonly framed in terms of optimal statistical estimation, where the bimodal percept is modelled either through MLE or BCI. To provide a plausible account for audiovisual integration in space, a population of MCDs should also behave as a Bayesian-optimal estimator. This hypothesis was tested by comparing the population response to human data in the studies that originally tested the MLE and BCI models; hence, we simulated the study of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref> and <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>. Such simulations allow us to compare the data with our model, and our model with previous ones (MLE and BCI). Given that in these two simulations, a population of MCDs behaves just like the MLE and BCI models (and with the same number of free parameters or fewer), the current approach can be easily extended to other instances of sensory cue integration previously modelled in terms of optimal statistical estimation. This was tested by simulating the study of <xref ref-type="bibr" rid="bib33">Mohl et al., 2020</xref>, who used eye movements to assess whether BCI can account for audiovisual integration in monkeys and humans (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>Finally, we tested whether a population of MCDs can predict <bold>audiovisual orienting and gaze behaviour</bold> during passive observation of ecological audiovisual stimuli. <xref ref-type="bibr" rid="bib15">Coutrot and Guyader, 2015</xref> run the ideal testbed for this hypothesis: much like our previous simulations (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), they employed audiovisual speech stimuli, recorded indoor, with no camera shake. Specifically, they tracked eye movements from 20 observers who passively watched 15 videos of a lab meeting (see <xref ref-type="fig" rid="fig6">Figure 6A</xref>). Without fitting parameters, the population response tightly matched the empirical saliency maps (see <xref ref-type="fig" rid="fig6">Figure 6B–D</xref> and <xref ref-type="video" rid="video4">Video 4</xref>).</p></sec><sec id="s4-13"><title>MCD temporal filters: parameter estimation and generalizability</title><p>The temporal filters determine the temporal tuning of the model and consist of three parameters: two temporal constants of the unimodal band-pass filters (<inline-formula><alternatives><mml:math id="inf116"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft116">\begin{document}$\tau _{bpA},\tau _{bpV}$\end{document}</tex-math></alternatives></inline-formula>; <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>), and one bimodal constant (<inline-formula><alternatives><mml:math id="inf117"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math><tex-math id="inft117">\begin{document}$\tau _{lp},$\end{document}</tex-math></alternatives></inline-formula> <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). For humans, these parameter values were estimated by combining data from all 40 experiments shown in <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>, excluding the data from Patient PH. These experiments encompass SJ, TOJ, and McGurk tasks, each involving parametric manipulations of audiovisual lag, as well as tasks using ecological audiovisual stimuli (i.e. real-life footage). For rats, we followed the same approach, combining all psychometric curves shown in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref> (excluding pharmacological manipulation experiments), based on stimuli consisting of clicks and flashes in TOJ and SJ tasks.</p><p>To simulate each trial, we fed the stimuli into the population model to estimate internal response variables <inline-formula><alternatives><mml:math id="inf118"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft118">\begin{document}$\overline {MCD_{corr}}$\end{document}</tex-math></alternatives></inline-formula> and <inline-formula><alternatives><mml:math id="inf119"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft119">\begin{document}$\overline{MCD_{lag}}$\end{document}</tex-math></alternatives></inline-formula> (<xref ref-type="disp-formula" rid="equ8 equ9">Equations 8; 9</xref>). Response probabilities were then derived from these internal signals as described in <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> (see also <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>). This decision stage introduces three additional parameters: <inline-formula><alternatives><mml:math id="inf120"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft120">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula> (the criterion, a bias term), and <inline-formula><alternatives><mml:math id="inf121"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft121">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf122"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft122">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula> (gain parameters, acting as scaling factors). These decision-related parameters were estimated independently for each experiment. An exception was made for simulations based on <xref ref-type="bibr" rid="bib49">Schormans and Allman, 2018</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2E</xref>, which manipulated audio intensity across three levels—here, we used a single set of <inline-formula><alternatives><mml:math id="inf123"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft123">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf124"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft124">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf125"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mi>β</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft125">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula> values across all conditions.</p><p>Two search algorithms were used for parameter estimation. First, we applied a global optimization method—Bayesian Adaptive Direct Search (BADS; Acerbi). To minimize the influence of initial values, we ran the BADS 200 times with different starting points (from 0.001 to 1.5 s). The best-fitting parameter values were then further refined using a local optimizer (fminsearch in MATLAB). The cost function minimized by both algorithms was 1 minus the Pearson correlation between real and simulated data, averaged across all experiments (each experiment was weighted equally, regardless of trial count or sample size). To assess robustness, we repeated the procedure using a mean squared error (MSE) cost function, which produced a set of parameter values (<inline-formula><alternatives><mml:math id="inf126"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft126">\begin{document}$\tau _{bpA}$\end{document}</tex-math></alternatives></inline-formula>=0.042 s <inline-formula><alternatives><mml:math id="inf127"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft127">\begin{document}$\tau _{bpV}$\end{document}</tex-math></alternatives></inline-formula>=0.039 s; <inline-formula><alternatives><mml:math id="inf128"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft128">\begin{document}$\tau _{lp}$\end{document}</tex-math></alternatives></inline-formula>=0.156 s) that are closely aligned with the original estimate (<inline-formula><alternatives><mml:math id="inf129"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft129">\begin{document}$\tau _{bpA}$\end{document}</tex-math></alternatives></inline-formula>=0.045 s <inline-formula><alternatives><mml:math id="inf130"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mi>p</mml:mi><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft130">\begin{document}$\tau _{bpV}$\end{document}</tex-math></alternatives></inline-formula>=0.036 s; <inline-formula><alternatives><mml:math id="inf131"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft131">\begin{document}$\tau _{lp}$\end{document}</tex-math></alternatives></inline-formula>=0.180 s).</p><p>This approach to parameter estimation, which aggregates data from a broad range of paradigms (n=40 for humans, n=15 for rats), enables us to estimate the temporal constants of the MCD units using a variety of ecological audiovisual stimuli (n=105 unique stimuli). Given that the stimuli consist of raw audiovisual footage, parameter estimation is computationally demanding — each search iteration can take up to 10 min. This makes standard techniques such as leave-one-out cross-validation impractical. The generalizability of the estimated temporal constants can nevertheless be tested against new, unseen data.</p><p>First, we tested whether the MCD model, with fixed temporal constants, could predict a well-known finding: that perceived audiovisual synchrony varies with distance in reverberant environments (<xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>). To do this, we used the previously estimated temporal constants and fitted only a single set of three decision parameters to predict the four psychometric curves in <xref ref-type="fig" rid="fig2">Figure 2B</xref>—effectively using fewer than one free parameter per curve. Importantly, no changes were made to the model’s temporal tuning. The MCD was able to fully reproduce the separation of the four curves across conditions. This result is not simply an extension of the model to a new stimulus set. Rather, it provides a purely bottom-up account of how perceived synchrony scales with spatial depth—without invoking any explicit computation of distance or higher-level inference mechanisms.</p><p>Next, we examined whether the estimated temporal constants—which had so far only been tested on group-averaged data—could also predict individual-level responses. To do this, we simulated individual observer data using previously published datasets: SJ data from <xref ref-type="bibr" rid="bib66">Yarrow et al., 2023</xref>, and both TOJ and SJ data from <xref ref-type="bibr" rid="bib38">Parise and Ernst, 2025</xref>, Experiments 1 and 2. These simulations allowed us to assess whether the model could generalize from group-level patterns to individual-level behaviour. The results confirmed that the MCD, using a single set of fixed temporal constants across all individuals, could successfully predict both individual and group data. Only the decision parameters (<inline-formula><alternatives><mml:math id="inf132"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft132">\begin{document}$\beta _{crit}$\end{document}</tex-math></alternatives></inline-formula>, <inline-formula><alternatives><mml:math id="inf133"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft133">\begin{document}$\beta _{corr}$\end{document}</tex-math></alternatives></inline-formula>, and <inline-formula><alternatives><mml:math id="inf134"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="inft134">\begin{document}$\beta _{lag}$\end{document}</tex-math></alternatives></inline-formula>) were fitted per observer, showing that temporal tuning itself can be considered constant across different observers.</p><p>Finally, using the same set of temporal constants, we evaluated whether the <inline-formula><alternatives><mml:math id="inf135"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft135">\begin{document}$\overline{MCD_{corr}} $\end{document}</tex-math></alternatives></inline-formula> population output (<xref ref-type="disp-formula" rid="equ6">Equation 6</xref>) could predict gaze behaviour during passive observation of real-life audiovisual footage—this time with zero free parameters. For this, we presented the audiovisual frames to the model (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and computed the MCD response for each pixel and time frame (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). Across frames, the model’s output consistently predicted the location of gaze direction: the region of the screen with the highest <inline-formula><alternatives><mml:math id="inf136"><mml:mstyle><mml:mrow><mml:mstyle displaystyle="false"><mml:mover><mml:mrow><mml:mi>M</mml:mi><mml:mi>C</mml:mi><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>r</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mstyle></mml:mrow></mml:mstyle></mml:math><tex-math id="inft136">\begin{document}$\overline{MCD_{corr}} $\end{document}</tex-math></alternatives></inline-formula> response systematically attracted participants’ gaze (<xref ref-type="fig" rid="fig6">Figure 6D</xref>).</p><p>In summary, we have used data from a large pool of experiments consisting of 105 unique stimuli to estimate the temporal constants of the MCD model. These constants, once estimated, demonstrated predictive validity across a wide range of behavioural measures (SJ, TOJ, eye-tracking), stimulus types, species (humans and rats), and data granularities (group and individual). They generalized successfully to completely novel behavioural datasets and natural viewing conditions—including eye-tracking during passive video observation—all without any re-tuning of the model’s core temporal filters.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-106122-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="scode1"><label>Source code 1.</label><caption><title>This compressed folder includes Matlab codes and stimuli to simulate the experiments of <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref>; <xref ref-type="bibr" rid="bib6">Alais and Carlile, 2005</xref>; <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>; <xref ref-type="bibr" rid="bib33">Mohl et al., 2020</xref>.</title><p>Moreover, it also includes the code and video to replicate <xref ref-type="fig" rid="fig5">Figure 5</xref> and <xref ref-type="video" rid="video3">Video 3</xref>.</p></caption><media xlink:href="elife-106122-code1-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Summary of the experiments simulated in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</title><p>The first column contains the reference of the study, the second column the task (McGurk, Simultaneity Judgment, and Temporal Order Judgment). The third column describes the stimuli: n represents the number of individual instances of the stimuli, ‘HI’ and ‘LI’ in <xref ref-type="bibr" rid="bib30">Magnotti and Beauchamp, 2017</xref> indicate speech stimuli with High and Low Intelligibility, respectively. ‘Blur’ indicates that the videos were blurred. ‘Disamb’ indicates that ambiguous speech stimuli (i.e., sine-wave speech) were disambiguated by informing the observers of the original speech sound. The fourth column indicates whether visual and acoustic stimuli were congruent. Here, incongruent stimuli refer to the mismatching speech stimuli used in the McGurk task. ‘SWS’ indicates sine-wave speech; ‘noise’ in <xref ref-type="bibr" rid="bib22">Ikeda and Morishita, 2020</xref> indicates a stimulus similar to sine-wave speech but in which white noise was used instead of pure sinusoidal waves. The fifth column represents the country where the study was performed. The sixth column describes the observers included in the study: ‘c.s.’ indicates convenience sampling (usually undergraduate students) musicians in <xref ref-type="bibr" rid="bib25">Lee and Noppeney, 2011</xref>; <xref ref-type="bibr" rid="bib26">Lee and Noppeney, 2014</xref> were amateur piano players; <xref ref-type="bibr" rid="bib20">Freeman et al., 2013</xref> tested young observers (18–28 years old), a patient P.H. (67 years old) that after a lesion in the pons and basal ganglia reported hearing speech before seeing the lips move; and a group of age-matched controls (59–74 years old). The seventh column reports the number of observers included in the study. Overall, the full dataset consisted of 986 individual psychometric curves; however, several observers participated in more than one experiment, so that the total number of unique observers was 454. The eight column reports the number of lags used in the method of constant stimuli. The nineth column reports the number of trials included in the study. The tenth column reports the correlation between empirical and predicted psychometric functions. The bottom row contains some descriptive statistics of the dataset.</p></caption><media xlink:href="elife-106122-supp1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Summary of the experiments simulated in <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>.</title><p>The first column contains the reference of the study, the second column the task (Simultaneity Judgment and Temporal Order Judgment). The third column describes the stimuli. The fourth column indicates what rats were used as observers. The fifth column reports the number of rats in the study; ‘same’ means that the same rats took part in the experiment in the row above. The sixth column reports the number of lags used in the method of constant stimuli. The seventh column reports the number of trials included in the study (not available for all studies). The eighth column reports the correlation between empirical and predicted psychometric functions. The bottom row contains some descriptive statistics of the dataset.</p></caption><media xlink:href="elife-106122-supp2-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>No new data has been collected as part of this study. All datasets used here are publicly available and can be found here:- The tables in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref> contain all the details of the studies on the temporal constraints of audiovisual integration. That table includes the references to the original paper, where the data was either attached as a supplemetary file, or was directly grabbed from the figures in the original papers. For the spatial constraints of audiovisual integration, the data from <xref ref-type="bibr" rid="bib5">Alais and Burr, 2004</xref> was grabbed from the figures in the original paper. For the experiment of <xref ref-type="bibr" rid="bib24">Körding et al., 2007</xref>, the data has been shared by the authors and it is included with the Matlab files to show how the MCD model can be used to similate Bayesian Causal Inference. <xref ref-type="bibr" rid="bib33">Mohl et al., 2020</xref> dataset available online: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5281/zenodo.3632106">https://doi.org/10.5281/zenodo.3632106</ext-link>. <xref ref-type="bibr" rid="bib15">Coutrot and Guyader, 2015</xref> dataset is available online: <ext-link ext-link-type="uri" xlink:href="https://osf.io/kaqv2/overview">https://osf.io/kaqv2/overview</ext-link>.</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Mohl</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>jmohl/CI_behavioral: addressing reviews</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.3900181</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Coutrot</surname><given-names>A</given-names></name><name><surname>Guyader</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>Gaze - Conversation Scenes</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/kaqv2/overview">kaqv2</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Dr. Irene Senna for insightful comments and continuous support throughout all stages of this study. We are also grateful to Dr. Alessandro Moscatelli for valuable feedback on the manuscript, and to Prof. Marc Ernst for stimulating discussions during the early phases of this work.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Acerbi</surname><given-names>L</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Practical bayesian optimization for model fitting with bayesian adaptive direct search</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1705.04405">https://doi.org/10.48550/arXiv.1705.04405</ext-link></element-citation></ref><ref id="bib2"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Adebayo</surname><given-names>J</given-names></name><name><surname>Gilmer</surname><given-names>J</given-names></name><name><surname>Muelly</surname><given-names>M</given-names></name><name><surname>Goodfellow</surname><given-names>I</given-names></name><name><surname>Hardt</surname><given-names>M</given-names></name><name><surname>Kim</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Sanity checks for saliency maps</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1810.03292">https://doi.org/10.48550/arXiv.1810.03292</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adelson</surname><given-names>EH</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatiotemporal energy models for the perception of motion</article-title><source>Journal of the Optical Society of America A</source><volume>2</volume><elocation-id>284</elocation-id><pub-id pub-id-type="doi">10.1364/JOSAA.2.000284</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Agoston</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>How to translate time? the temporal aspect of human and rodent biology</article-title><source>Frontiers in Neurology</source><volume>8</volume><elocation-id>92</elocation-id><pub-id pub-id-type="doi">10.3389/fneur.2017.00092</pub-id><pub-id pub-id-type="pmid">28367138</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Burr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The ventriloquist effect results from near-optimal bimodal integration</article-title><source>Current Biology</source><volume>14</volume><fpage>257</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2004.01.029</pub-id><pub-id pub-id-type="pmid">14761661</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alais</surname><given-names>D</given-names></name><name><surname>Carlile</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Synchronizing to real events: Subjective audiovisual alignment scales with perceived auditory depth and speed of sound</article-title><source>PNAS</source><volume>102</volume><fpage>2244</fpage><lpage>2247</lpage><pub-id pub-id-type="doi">10.1073/pnas.0407034102</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aller</surname><given-names>M</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>To integrate or not to integrate: Temporal dynamics of hierarchical Bayesian causal inference</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000210</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000210</pub-id><pub-id pub-id-type="pmid">30939128</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Al Youzbaki</surname><given-names>MU</given-names></name><name><surname>Schormans</surname><given-names>AL</given-names></name><name><surname>Allman</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Past and present experience shifts audiovisual temporal perception in rats</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>17</volume><elocation-id>1287587</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2023.1287587</pub-id><pub-id pub-id-type="pmid">37908200</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Arandjelovic</surname><given-names>R</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Look, listen and learn</article-title><conf-name>2017 IEEE International Conference on Computer Vision (ICCV)</conf-name><fpage>609</fpage><lpage>617</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2017.73</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burge</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Image-computable ideal observers for tasks with natural stimuli</article-title><source>Annual Review of Vision Science</source><volume>6</volume><fpage>491</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-030320-041134</pub-id><pub-id pub-id-type="pmid">32580664</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Campbell</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The processing of audio-visual speech: empirical and neural bases</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>363</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1098/rstb.2007.2155</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Trubanova</surname><given-names>A</given-names></name><name><surname>Stillittano</surname><given-names>S</given-names></name><name><surname>Caplier</surname><given-names>A</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The natural statistics of audiovisual speech</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000436</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id><pub-id pub-id-type="pmid">19609344</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>L</given-names></name><name><surname>Vroomen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Intersensory binding across space and time: A tutorial review</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>75</volume><fpage>790</fpage><lpage>811</lpage><pub-id pub-id-type="doi">10.3758/s13414-013-0475-4</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Song</surname><given-names>W</given-names></name><name><surname>Guo</surname><given-names>L</given-names></name><name><surname>Jian</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A comprehensive survey on video saliency detection with auditory information: the audio-visual consistency perceptual is the key!</article-title><source>IEEE Transactions on Circuits and Systems for Video Technology</source><volume>33</volume><fpage>457</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1109/TCSVT.2022.3203421</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Coutrot</surname><given-names>A</given-names></name><name><surname>Guyader</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>2015 23rd European Signal Processing Conference (EUSIPCO)</article-title><conf-name>eusipco2015</conf-name><fpage>1531</fpage><lpage>1535</lpage></element-citation></ref><ref id="bib16"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Coutrot</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><chapter-title>Audiovisual saliency models: a short review</chapter-title><person-group person-group-type="editor"><name><surname>Mancas</surname><given-names>M</given-names></name><name><surname>Ferrera</surname><given-names>VP</given-names></name><name><surname>Coutrot</surname><given-names>A</given-names></name></person-group><source>Human Attention to Computational Attention</source><publisher-name>Springer Nature</publisher-name><fpage>259</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1007/978-3-031-84300-6_11</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cuppini</surname><given-names>C</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name><name><surname>Magosso</surname><given-names>E</given-names></name><name><surname>Ursino</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A biologically inspired neurocomputational model for audiovisual integration and causal inference</article-title><source>The European Journal of Neuroscience</source><volume>46</volume><fpage>2481</fpage><lpage>2498</lpage><pub-id pub-id-type="doi">10.1111/ejn.13725</pub-id><pub-id pub-id-type="pmid">28949035</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Emerson</surname><given-names>RC</given-names></name><name><surname>Bergen</surname><given-names>JR</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Directionally selective complex cells and the computation of motion energy in cat visual cortex</article-title><source>Vision Research</source><volume>32</volume><fpage>203</fpage><lpage>218</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(92)90130-b</pub-id><pub-id pub-id-type="pmid">1574836</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>Banks</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Humans integrate visual and haptic information in a statistically optimal fashion</article-title><source>Nature</source><volume>415</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/415429a</pub-id><pub-id pub-id-type="pmid">11807554</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>ED</given-names></name><name><surname>Ipser</surname><given-names>A</given-names></name><name><surname>Palmbaha</surname><given-names>A</given-names></name><name><surname>Paunoiu</surname><given-names>D</given-names></name><name><surname>Brown</surname><given-names>P</given-names></name><name><surname>Lambert</surname><given-names>C</given-names></name><name><surname>Leff</surname><given-names>A</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Sight and sound out of synch: fragmentation and renormalisation of audiovisual integration and subjective timing</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>49</volume><fpage>2875</fpage><lpage>2887</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2013.03.006</pub-id><pub-id pub-id-type="pmid">23664001</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Horsfall</surname><given-names>R</given-names></name><name><surname>Wuerger</surname><given-names>S</given-names></name><name><surname>Meyer</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Visual intensity-dependent response latencies predict perceived audio–visual simultaneity</article-title><source>Journal of Mathematical Psychology</source><volume>100</volume><elocation-id>102471</elocation-id><pub-id pub-id-type="doi">10.1016/j.jmp.2020.102471</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ikeda</surname><given-names>T</given-names></name><name><surname>Morishita</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>How are audiovisual simultaneity judgments affected by multisensory complexity and speech specificity?</article-title><source>Multisensory Research</source><volume>34</volume><fpage>49</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1163/22134808-bja10031</pub-id><pub-id pub-id-type="pmid">33706276</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Niebur</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A model of saliency-based visual attention for rapid scene analysis</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>20</volume><fpage>1254</fpage><lpage>1259</lpage><pub-id pub-id-type="doi">10.1109/34.730558</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Körding</surname><given-names>KP</given-names></name><name><surname>Beierholm</surname><given-names>U</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Quartz</surname><given-names>S</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Shams</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Causal inference in multisensory perception</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e943</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000943</pub-id><pub-id pub-id-type="pmid">17895984</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Long-term music training tunes how the brain temporally binds signals from multiple senses</article-title><source>PNAS</source><volume>108</volume><fpage>E1441</fpage><lpage>E1450</lpage><pub-id pub-id-type="doi">10.1073/pnas.1115267108</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal prediction errors in visual and auditory cortices</article-title><source>Current Biology</source><volume>24</volume><fpage>R309</fpage><lpage>R310</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2014.02.007</pub-id><pub-id pub-id-type="pmid">24735850</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Legrand</surname><given-names>FD</given-names></name><name><surname>Albinet</surname><given-names>C</given-names></name><name><surname>Canivet</surname><given-names>A</given-names></name><name><surname>Gierski</surname><given-names>F</given-names></name><name><surname>Morrone</surname><given-names>I</given-names></name><name><surname>Besche-Richard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Brief aerobic exercise immediately enhances visual attentional control and perceptual speed. Testing the mediating role of feelings of energy</article-title><source>Acta Psychologica</source><volume>191</volume><fpage>25</fpage><lpage>31</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2018.08.020</pub-id><pub-id pub-id-type="pmid">30195178</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mafi</surname><given-names>F</given-names></name><name><surname>Tang</surname><given-names>MF</given-names></name><name><surname>Afarinesh</surname><given-names>MR</given-names></name><name><surname>Ghasemian</surname><given-names>S</given-names></name><name><surname>Sheibani</surname><given-names>V</given-names></name><name><surname>Arabzadeh</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Temporal order judgment of multisensory stimuli in rat and human</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>16</volume><elocation-id>1070452</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2022.1070452</pub-id><pub-id pub-id-type="pmid">36710957</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Causal inference of asynchronous audiovisual speech</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>798</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00798</pub-id><pub-id pub-id-type="pmid">24294207</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magnotti</surname><given-names>JF</given-names></name><name><surname>Beauchamp</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A causal inference model explains perception of the McGurk effect and other incongruent audiovisual speech</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005229</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005229</pub-id><pub-id pub-id-type="pmid">28207734</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McGurk</surname><given-names>H</given-names></name><name><surname>MacDonald</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Hearing lips and seeing voices</article-title><source>Nature</source><volume>264</volume><fpage>746</fpage><lpage>748</lpage><pub-id pub-id-type="doi">10.1038/264746a0</pub-id><pub-id pub-id-type="pmid">1012311</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mohl</surname><given-names>JT</given-names></name><name><surname>Pearson</surname><given-names>JM</given-names></name><name><surname>Groh</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Monkeys and humans implement causal inference to simultaneously localize auditory and visual stimuli</article-title><source>Journal of Neurophysiology</source><volume>124</volume><fpage>715</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1152/jn.00046.2020</pub-id><pub-id pub-id-type="pmid">32727263</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Ngiam</surname><given-names>J</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Kim</surname><given-names>M</given-names></name><name><surname>Nam</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>H</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Multimodal deep learning</article-title><conf-name>Proceedings of the 28th international conference on machine learning (ICML-11)</conf-name><fpage>689</fpage><lpage>696</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>When correlation implies causation in multisensory integration</article-title><source>Current Biology</source><volume>22</volume><fpage>46</fpage><lpage>49</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2011.11.039</pub-id><pub-id pub-id-type="pmid">22177899</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Correlation detection as a general mechanism for multisensory integration</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11543</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11543</pub-id><pub-id pub-id-type="pmid">27265526</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Noise, multisensory integration, and previous response in perceptual disambiguation</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005546</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005546</pub-id><pub-id pub-id-type="pmid">28692700</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Multisensory integration operates on correlated input from unimodal transient channels</article-title><source>eLife</source><volume>12</volume><elocation-id>RP90841</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.90841</pub-id><pub-id pub-id-type="pmid">39841060</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Parise</surname><given-names>E</given-names></name><name><surname>Parise</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2025">2025</year><article-title>Perceiving audiovisual synchrony:a quantitative synthesis of simultaneity and temporal order judgments from 162 studies</article-title><source>Neuroscience and Biobehavioural Reviews</source><comment>In press</comment></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Paulcan</surname><given-names>S</given-names></name><name><surname>Giersch</surname><given-names>A</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Doyère</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Temporal order processing in rats depends on the training protocol</article-title><source>Journal of Experimental Psychology. Animal Learning and Cognition</source><volume>49</volume><fpage>31</fpage><lpage>45</lpage><pub-id pub-id-type="doi">10.1037/xan0000347</pub-id><pub-id pub-id-type="pmid">36795421</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Sommers</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prediction and constraint in audiovisual speech perception</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>68</volume><fpage>169</fpage><lpage>181</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.006</pub-id><pub-id pub-id-type="pmid">25890390</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pesnot Lerousseau</surname><given-names>J</given-names></name><name><surname>Parise</surname><given-names>CV</given-names></name><name><surname>Ernst</surname><given-names>MO</given-names></name><name><surname>van Wassenhove</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Multisensory correlation computations in the human brain identified by a time-resolved encoding model</article-title><source>Nature Communications</source><volume>13</volume><elocation-id>2489</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-022-29687-6</pub-id><pub-id pub-id-type="pmid">35513362</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petrini</surname><given-names>K</given-names></name><name><surname>Dahl</surname><given-names>S</given-names></name><name><surname>Rocchesso</surname><given-names>D</given-names></name><name><surname>Waadeland</surname><given-names>CH</given-names></name><name><surname>Avanzini</surname><given-names>F</given-names></name><name><surname>Puce</surname><given-names>A</given-names></name><name><surname>Pollick</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Multisensory integration of drumming actions: musical expertise affects perceived audiovisual asynchrony</article-title><source>Experimental Brain Research</source><volume>198</volume><fpage>339</fpage><lpage>352</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1817-2</pub-id><pub-id pub-id-type="pmid">19404620</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Cortical hierarchies perform Bayesian causal inference in multisensory perception</article-title><source>PLOS Biology</source><volume>13</volume><elocation-id>e1002073</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002073</pub-id><pub-id pub-id-type="pmid">25710328</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rohe</surname><given-names>T</given-names></name><name><surname>Ehlis</surname><given-names>AC</given-names></name><name><surname>Noppeney</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The neural dynamics of hierarchical Bayesian causal inference in multisensory perception</article-title><source>Nature Communications</source><volume>10</volume><elocation-id>096642</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-019-09664-2</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roseboom</surname><given-names>W</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Twice upon a time: multiple concurrent temporal recalibrations of audiovisual speech</article-title><source>Psychological Science</source><volume>22</volume><fpage>872</fpage><lpage>877</lpage><pub-id pub-id-type="doi">10.1177/0956797611413293</pub-id><pub-id pub-id-type="pmid">21690312</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sawilowsky</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>New effect size rules of thumb</article-title><source>Journal of Modern Applied Statistical Methods</source><volume>8</volume><fpage>597</fpage><lpage>599</lpage><pub-id pub-id-type="doi">10.22237/jmasm/1257035100</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schormans</surname><given-names>AL</given-names></name><name><surname>Scott</surname><given-names>KE</given-names></name><name><surname>Vo</surname><given-names>AMQ</given-names></name><name><surname>Tyker</surname><given-names>A</given-names></name><name><surname>Typlt</surname><given-names>M</given-names></name><name><surname>Stolzberg</surname><given-names>D</given-names></name><name><surname>Allman</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Audiovisual temporal processing and synchrony perception in the rat</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>10</volume><elocation-id>246</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2016.00246</pub-id><pub-id pub-id-type="pmid">28119580</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schormans</surname><given-names>AL</given-names></name><name><surname>Allman</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Behavioral plasticity of audiovisual perception: rapid recalibration of temporal sensitivity but not perceptual binding following adult-onset hearing loss</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>12</volume><elocation-id>256</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2018.00256</pub-id><pub-id pub-id-type="pmid">30429780</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schormans</surname><given-names>AL</given-names></name><name><surname>Allman</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>An imbalance of excitation and inhibition in the multisensory cortex impairs the temporal acuity of audiovisual processing and perception</article-title><source>Cerebral Cortex</source><volume>33</volume><fpage>9937</fpage><lpage>9953</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhad256</pub-id><pub-id pub-id-type="pmid">37464944</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Serbe</surname><given-names>E</given-names></name><name><surname>Meier</surname><given-names>M</given-names></name><name><surname>Leonhardt</surname><given-names>A</given-names></name><name><surname>Borst</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Comprehensive characterization of the major presynaptic elements to the <italic>Drosophila</italic> OFF motion detector</article-title><source>Neuron</source><volume>89</volume><fpage>829</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.01.006</pub-id><pub-id pub-id-type="pmid">26853306</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shahabaz</surname><given-names>A</given-names></name><name><surname>Sarkar</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2024">2024</year><source>Increasing Importance of Joint Analysis of Audio and Video in Computer Vision: A Survey</source><publisher-name>IEEE Access</publisher-name></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>BE</given-names></name><name><surname>Stanford</surname><given-names>TR</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multisensory integration: current issues from the perspective of the single neuron</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>255</fpage><lpage>266</lpage><pub-id pub-id-type="doi">10.1038/nrn2331</pub-id><pub-id pub-id-type="pmid">18354398</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Stein</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>The New Handbook of Multisensory Processing</source><publisher-name>MIT Press</publisher-name><pub-id pub-id-type="doi">10.7551/mitpress/8466.001.0001</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stratton</surname><given-names>GM</given-names></name></person-group><year iso-8601-date="1897">1897</year><article-title>Vision without inversion of the retinal image</article-title><source>Psychological Review</source><volume>4</volume><fpage>463</fpage><lpage>481</lpage><pub-id pub-id-type="doi">10.1037/h0071173</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Summerfield</surname><given-names>Q</given-names></name></person-group><year iso-8601-date="1987">1987</year><chapter-title>Some preliminaries to a compre hensive account of audio-visual speech perception</chapter-title><person-group person-group-type="editor"><name><surname>Dodd</surname><given-names>B</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name></person-group><source>Hearing by Eye: The Psychology of Lip-Reading</source><publisher-name>Lawrence Erlbaum Associates, Inc</publisher-name><fpage>3</fpage><lpage>226</lpage></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Talsma</surname><given-names>D</given-names></name><name><surname>Senkowski</surname><given-names>D</given-names></name><name><surname>Soto-Faraco</surname><given-names>S</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multifaceted interplay between attention and multisensory integration</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>400</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.06.008</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuomainen</surname><given-names>J</given-names></name><name><surname>Andersen</surname><given-names>TS</given-names></name><name><surname>Tiippana</surname><given-names>K</given-names></name><name><surname>Sams</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Audio-visual speech perception is special</article-title><source>Cognition</source><volume>96</volume><fpage>B13</fpage><lpage>B22</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2004.10.004</pub-id><pub-id pub-id-type="pmid">15833302</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tye-Murray</surname><given-names>N</given-names></name><name><surname>Sommers</surname><given-names>M</given-names></name><name><surname>Spehar</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Auditory and visual lexical neighborhoods in audiovisual speech perception</article-title><source>Trends in Amplification</source><volume>11</volume><fpage>233</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1177/1084713807307409</pub-id><pub-id pub-id-type="pmid">18003867</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Laarhoven</surname><given-names>T</given-names></name><name><surname>Stekelenburg</surname><given-names>JJ</given-names></name><name><surname>Vroomen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Increased sub-clinical levels of autistic traits are associated with reduced multisensory integration of audiovisual speech</article-title><source>Scientific Reports</source><volume>9</volume><elocation-id>9535</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-019-46084-0</pub-id><pub-id pub-id-type="pmid">31267024</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Wassenhove</surname><given-names>V</given-names></name><name><surname>Grant</surname><given-names>KW</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal window of integration in auditory-visual speech perception</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>598</fpage><lpage>607</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2006.01.001</pub-id><pub-id pub-id-type="pmid">16530232</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vatakis</surname><given-names>A</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Spence</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Facilitation of multisensory integration by the “unity effect” reveals that speech is special</article-title><source>Journal of Vision</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/8.9.14</pub-id><pub-id pub-id-type="pmid">18831650</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Venezia</surname><given-names>JH</given-names></name><name><surname>Thurman</surname><given-names>SM</given-names></name><name><surname>Matchin</surname><given-names>W</given-names></name><name><surname>George</surname><given-names>SE</given-names></name><name><surname>Hickok</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Timing in audiovisual speech perception: A mini review and new psychophysical data</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>78</volume><fpage>583</fpage><lpage>601</lpage><pub-id pub-id-type="doi">10.3758/s13414-015-1026-y</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Keetels</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perception of intersensory synchrony: A tutorial review</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>72</volume><fpage>871</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.3758/APP.72.4.871</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname><given-names>J</given-names></name><name><surname>Stekelenburg</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Perception of intersensory synchrony in audiovisual speech: not that special</article-title><source>Cognition</source><volume>118</volume><fpage>75</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2010.10.002</pub-id><pub-id pub-id-type="pmid">21035795</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yarrow</surname><given-names>K</given-names></name><name><surname>Solomon</surname><given-names>JA</given-names></name><name><surname>Arnold</surname><given-names>DH</given-names></name><name><surname>Roseboom</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>The best fitting of three contemporary observer models reveals how participants’ strategy influences the window of subjective synchrony</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>49</volume><fpage>1534</fpage><lpage>1563</lpage><pub-id pub-id-type="doi">10.1037/xhp0001154</pub-id><pub-id pub-id-type="pmid">37917421</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yuan</surname><given-names>X</given-names></name><name><surname>Bi</surname><given-names>C</given-names></name><name><surname>Yin</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>B</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The recalibration patterns of perceptual synchrony and multisensory integration after exposure to asynchronous speech</article-title><source>Neuroscience Letters</source><volume>569</volume><fpage>148</fpage><lpage>152</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2014.03.057</pub-id><pub-id pub-id-type="pmid">24704377</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106122.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Noel</surname><given-names>Jean-Paul</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Minnesota</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study evaluates a model for multisensory correlation detection, focusing on the detection of correlated transients in visual and auditory stimuli. Overall, the experimental design is sound and the evidence is <bold>compelling</bold>. The synergy between the experimental and theoretical aspects of the article is strong, and the work will be of interest to both neuroscientists and psychologists working in the domain of sensory processing and perception</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106122.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Parise presents another instantiation of the Multisensory Correlation Detector model that can now accept stimulus-level inputs. This is a valuable development as it removes researcher involvement in the characterization/labeling of features and allows analysis of complex stimuli with a high degree of nuance that was previously unconsidered (i.e. spatial/spectral distributions across time). The author demonstrates the power of the model by fitting data from dozens of previous experiments including multiple species, tasks, behavioral modality, and pharmacological interventions.</p><p>Strengths:</p><p>One of the model's biggest strengths, in my opinion, is its ability to extract complex spatiotemporal co-relationships from multisensory stimuli. These relationships have typically been manually computed or assigned based on stimulus condition and often distilled to a single dimension or even single number (e.g., &quot;-50 ms asynchrony&quot;). Thus, many models of multisensory integration depend heavily on human preprocessing of stimuli and these models miss out on complex dynamics of stimuli; the lead modality distribution apparent in figure 3b and c are provocative. I can imagine the model revealing interesting characteristics of the facial distribution of correlation during continuous audiovisual speech that have up to this point been largely described as &quot;present&quot; and almost solely focused on the lip area.</p><p>Another aspect that makes the MCD stand out among other models is the biological inspiration and generalizability across domains. The model was developed to describe a separate process - motion perception - and in a much simpler organism - drosophila. It could then describe a very basic neural computation that has been conserved across phylogeny (which is further demonstrated in the ability to predict rat, primate, and human data) and brain area. This aspect makes the model likely able to account for much more than what has already been demonstrated with only a few tweaks akin to the modifications described in this and previous articles from Parise.</p><p>What allows this potential is that, as Parise and colleagues have demonstrated in those papers since our (re)introduction of the model in 2016, the MCD model is modular - both in its ability to interface with different inputs/outputs and its ability to chain MCD units in a way that can analyze spatial, spectral, or any other arbitrary dimension of a stimulus. This fact leaves wide-open the possibilities for types of data, stimuli, and tasks a simplistic neutrally inspired model can account for.</p><p>And so it's unsurprising (but impressive!) that Parise has demonstrated the model's ability here to account for such a wide range of empirical data from numerous tasks (synchrony/temporal order judgement, localization, detection, etc.) and behavior types (manual/saccade responses, gaze, etc.) using only the stimulus and a few free parameters. This ability is another of the model's main strengths that I think deserves some emphasis: it represents a kind of validation of those experiments - especially in the context of cross-experiment predictions.</p><p>Finally, what is perhaps most impressive to me is that the MCD (and the accompanying decision model) does all this with very few (sometimes zero) free parameters. This highlights the utility of the model and the plausibility of its underlying architecture, but also helps to prevent extreme overfitting if fit correctly.</p><p>Weaknesses:</p><p>The model boasts an incredible versatility across tasks and stimulus configurations and its overall scope of the model is to understand how and what relevant sensory information is extracted from a stimulus. We still need to exercise care when interpreting its parameters, especially considering the broader context of top-down control of perception and that some multisensory mappings may not be derivable purely from stimulus statistics (e.g., the complementary nature of some phonemes/visemes).</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106122.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Building on previous models of multisensory integration (including their earlier correlation-detection framework used for non-spatial signals), the author introduces a population-level Multisensory Correlation Detector (MCD) that processes raw auditory and visual data. Crucially, it does not rely on abstracted parameters, as is common in normative Bayesian models,&quot; but rather works directly on the stimulus itself (i.e., individual pixels and audio samples). By systematically testing the model against a range of experiments spanning human, monkey, and rat data - the authors show that their MCD population approach robustly predicts perception and behavior across species with a relatively small (0-4) number of free parameters.</p><p>Strengths:</p><p>(1) Unlike prior Bayesian models that used simplified or parameterized inputs, the model here is explicitly computable from full natural stimuli. This resolves a key gap in understanding how the brain might extract &quot;time offsets&quot; or &quot;disparities&quot; from continuously changing audio-visual streams.</p><p>(2) The same population MCD architecture captures a remarkable range of multisensory phenomena, from classical illusions (McGurk, ventriloquism) and synchrony judgments, to attentional/gaze behavior driven by audio-visual salience. This generality strongly supports the idea that a single low-level computation (correlation detection) can underlie many distinct multisensory effects.</p><p>(3) By tuning model parameters to different temporal rhythms (e.g., faster in rodents, slower in humans), the MCD explains cross-species perceptual data without reconfiguring the underlying architecture.</p><p>(4) The authors frame their model as a plausible algorithmic account of the Bayesian multisensory-integration models in Marr's levels of hierarchy.</p><p>Weaknesses:</p><p>What remains unclear is how the parameters themselves relate to stimulus quantities (like stimulus uncertainty), as is often straightforward in Bayesian models. A theoretical missing link is the explicit relationship between the parameters of the MCD models and those of a cue combination model, thereby bridging Marr's levels of hierarchy.</p><p>Likely Impact and Usefulness</p><p>The work offers a compelling unification of multiple multisensory tasks-temporal order judgments, illusions, Bayesian causal inference, and overt visual attention-under a single, fully stimulus-driven framework. Its success with natural stimuli should interest computational neuroscientists, systems neuroscientists, and machine learning scientists. This paper thus makes an important contribution to the field by moving beyond minimalistic lab stimuli, illustrating how raw audio and video can be integrated using elementary correlation analyses.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.106122.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Parise</surname><given-names>Cesare V</given-names></name><role specific-use="author">Author</role><aff><institution>University of Liverpool</institution><addr-line><named-content content-type="city">Liverpool</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>Parise presents another instantiation of the Multisensory Correlation Detector model that can now accept stimulus-level inputs. This is a valuable development as it removes researcher involvement in the characterization/labeling of features and allows analysis of complex stimuli with a high degree of nuance that was previously unconsidered (i.e., spatial/spectral distributions across time). The author demonstrates the power of the model by fitting data from dozens of previous experiments, including multiple species, tasks, behavioral modalities, and pharmacological interventions.</p></disp-quote><p>Thanks for the kind words!</p><disp-quote content-type="editor-comment"><p>Strengths:</p><p>One of the model's biggest strengths, in my opinion, is its ability to extract complex spatiotemporal co-relationships from multisensory stimuli. These relationships have typically been manually computed or assigned based on stimulus condition and often distilled to a single dimension or even a single number (e.g., &quot;-50 ms asynchrony&quot;). Thus, many models of multisensory integration depend heavily on human preprocessing of stimuli, and these models miss out on complex dynamics of stimuli; the lead modality distribution apparent in Figures 3b and c is provocative. I can imagine the model revealing interesting characteristics of the facial distribution of correlation during continuous audiovisual speech that have up to this point been largely described as &quot;present&quot; and almost solely focused on the lip area.</p><p>Another aspect that makes the MCD stand out among other models is the biological inspiration and generalizability across domains. The model was developed to describe a separate process - motion perception - and in a much simpler organism - Drosophila. It could then describe a very basic neural computation that has been conserved across phylogeny (which is further demonstrated in the ability to predict rat, primate, and human data) and brain area. This aspect makes the model likely able to account for much more than what has already been demonstrated with only a few tweaks akin to the modifications described in this and previous articles from Parise.</p><p>What allows this potential is that, as Parise and colleagues have demonstrated in those papers since our (re)introduction of the model in 2016, the MCD model is modular - both in its ability to interface with different inputs/outputs and its ability to chain MCD units in a way that can analyze spatial, spectral, or any other arbitrary dimension of a stimulus. This fact leaves wide open the possibilities for types of data, stimuli, and tasks a simplistic, neutrally inspired model can account for.</p><p>And so it's unsurprising (but impressive!) that Parise has demonstrated the model's ability here to account for such a wide range of empirical data from numerous tasks (synchrony/temporal order judgement, localization, detection, etc.) and behavior types (manual/saccade responses, gaze, etc.) using only the stimulus and a few free parameters. This ability is another of the model's main strengths that I think deserves some emphasis: it represents a kind of validation of those experiments, especially in the context of cross-experiment predictions (but see some criticism of that below).</p><p>Finally, what is perhaps most impressive to me is that the MCD (and the accompanying decision model) does all this with very few (sometimes zero) free parameters. This highlights the utility of the model and the plausibility of its underlying architecture, but also helps to prevent extreme overfitting if fit correctly (but see a related concern below).</p></disp-quote><p>We sincerely thank the reviewer for their thoughtful and generous comments. We are especially pleased that the core strengths of the model—its stimulus-computable architecture, biological grounding, modularity, and cross-domain applicability—were clearly recognized. As the reviewer rightly notes, removing researcher-defined abstractions and working directly from naturalistic stimuli opens the door to uncovering previously overlooked dynamics in complex multisensory signals, such as the spatial and temporal richness of audiovisual speech.</p><p>We also appreciate the recognition of the model’s origins in a simple organism and its generalization across species and behaviors. This phylogenetic continuity reinforces our view that the MCD captures a fundamental computation with wide-ranging implications. Finally, we are grateful for the reviewer’s emphasis on the model’s predictive power across tasks and datasets with few or no free parameters—a property we see as key to both its parsimony and explanatory utility.</p><p>We have highlighted these points more explicitly in the revised manuscript, and we thank the reviewer for their generous and insightful endorsement of the work.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>There is an insufficient level of detail in the methods about model fitting. As a result, it's unclear what data the models were fitted and validated on. Were models fit individually or on average group data? Each condition separately? Is the model predictive of unseen data? Was the model cross-validated? Relatedly, the manuscript mentions a randomization test, but the shuffled data produces model responses that are still highly correlated to behavior despite shuffling. Could it be that any stimulus that varies in AV onset asynchrony can produce a psychometric curve that matches any other task with asynchrony judgements baked into the task? Does this mean all SJ or TOJ tasks produce correlated psychometric curves? Or more generally, is Pearson's correlation insensitive to subtle changes here, considering psychometric curves are typically sigmoidal? Curves can be non-overlapping and still highly correlated if one is, for example, scaled differently. Would an error term such as mean-squared or root mean-squared error be more sensitive to subtle changes in psychometric curves? Alternatively, perhaps if the models aren't cross-validated, the high correlation values are due to overfitting?</p></disp-quote><p>The reviewer is right: the current version of the manuscript only provides limited information about parameter fitting. In the revised version of the manuscript, we included a parameter estimation and generalizability section that includes all information requested by the reviewer.</p><p>To test whether using the MSE instead of Pearson correlation led to a similar estimated set of parameter values, we repeated the fitting using the MSE. The parameter estimated with this method (TauV, TauA, TauBim) closely followed those estimated using Pearson correlation (TauV, TauA, TauBim). Given the similarity of these results, we have chosen not to include further figures, however this analysis is now included in the new section (pages 23-24).</p><p>Regarding the permutation test, it is expected that different stimuli produce analogous psychometric functions: after all, all studies relied on stimuli containing identical manipulation of lags. As a result, MCD population responses tend to be similar across experiments. Therefore, it is not a surprise that the permuted distribution of MCD-data correlation in Supplementary Figure 1K has a mean as high as 0.97. However, what is important is to demonstrate that the non-permuted dataset has an even higher goodness of fit. Supplementary Figure 1K demonstrates that none of the permuted stimuli could outperform the non-permuted dataset; the mean of the non-permuted distribution is 4.7 (standard deviations) above the mean of the already high permuted distribution.</p><p>We believe the new section, along with the present response, fully addresses the legitimate concerns of the reviewer.</p><disp-quote content-type="editor-comment"><p>While the model boasts incredible versatility across tasks and stimulus configurations, fitting behavioral data well doesn't mean we've captured the underlying neural processes, and thus, we need to be careful when interpreting results. For example, the model produces temporal parameters fitting rat behavior that are 4x faster than when fitting human data. This difference in slope and a difference at the tails were interpreted as differences in perceptual sensitivity related to general processing speeds of the rat, presumably related to brain/body size differences. While rats no doubt have these differences in neural processing speed/integration windows, it seems reasonable that a lot of the differences in human and rat psychometric functions could be explained by the (over)training and motivation of rats to perform on every trial for a reward - increasing attention/sensitivity (slope) - and a tendency to make mistakes (compression evident at the tails). Was there an attempt to fit these data with a lapse parameter built into the decisional model as was done in Equation 21? Likewise, the fitted parameters for the pharmacological manipulations during the SJ task indicated differences in the decisional (but not the perceptual) process and the article makes the claim that &quot;all pharmacologically-induced changes in audiovisual time perception&quot; can be attributed to decisional processes &quot;with no need to postulate changes in low-level temporal processing.&quot; However, those papers discuss actual sensory effects of pharmacological manipulation, with one specifically reporting changes to response timing. Moreover, and again contrary to the conclusions drawn from model fits to those data, both papers also report a change in psychometric slope/JND in the TOJ task after pharmacological manipulation, which would presumably be reflected in changes to the perceptual (but not the decisional) parameters.</p></disp-quote><p>Fitting or predicting behaviour does not in itself demonstrate that a model captures the underlying neural computations—though it may offer valuable constraints and insights. In line with this, we were careful not to extrapolate the implications of our simulations to specific neural mechanisms.</p><p>Temporal sensitivity is, by definition, a behavioural metric, and—as the reviewer correctly notes—its estimation may reflect a range of contributing factors beyond low-level sensory processing, including attention, motivation, and lapse rates (i.e., stimulus-independent errors). In Equation 21, we introduced a lapse parameter specifically to account for such effects in the context of monkey eye-tracking data. For the rat datasets, however, the inclusion of a lapse term was not required to achieve a close fit to the psychometric data (ρ = 0.981). While it is likely that adding a lapse component would yield a marginally better fit, the absence of single-trial data prevents us from applying model comparison criteria such as AIC or BIC to justify the additional parameter. In light of this, and to avoid unnecessary model complexity, we opted not to include a lapse term in the rat simulations.</p><p>With respect to the pharmacological manipulation data, we acknowledge the reviewer’s point that observed changes in slope and bias could plausibly arise from alterations at either the sensory or decisional level—or both. In our model, low-level sensory processing is instantiated by the MCD architecture, which outputs the MCDcorr and MCDlag signals that are then scaled and integrated during decision-making. Importantly, this scaling operation influences the slope of the resulting psychometric functions, such that changes in slope can arise even in the absence of any change to the MCD’s temporal filters. In our simulations, the temporal constants of the MCD units were fixed to the values estimated from the non-pharmacological condition (see parameter estimation section above), and only the decision-related parameters were allowed to vary. From this modelling perspective, the behavioural effects observed in the pharmacological datasets can be explained entirely by changes at the decisional level. However, we do not claim that such an explanation excludes the possibility of genuine sensory-level changes. Rather, we assert that our model can account for the observed data without requiring modifications to early temporal tuning.</p><p>To rigorously distinguish sensory from decisional effects, future experiments will need to employ stimuli with richer temporal structure—e.g., temporally modulated sequences of clicks and flashes that vary in frequency, phase, rhythm, or regularity (see Fujisaki &amp; Nishida, 2007; Denison et al., 2012; Parise &amp; Ernst, 2016, 2025; Locke &amp; Landy, 2017; Nidiffer et al., 2018). Such stimuli engage the MCD in a more stimulus-dependent manner, enabling a clearer separation between early sensory encoding and later decision-making processes. Unfortunately, the current rat datasets—based exclusively on single click-flash pairings—lack the complexity needed for such disambiguation. As a result, while our simulations suggest that the observed pharmacologically induced effects can be attributed to changes in decision-level parameters, they do not rule out concurrent sensory-level changes.</p><p>In summary, our results indicate that changes in the temporal tuning of MCD units are not necessary to reproduce the observed pharmacological effects on audiovisual timing behaviour. However, we do not assert that such changes are absent or unnecessary in principle. Disentangling sensory and decisional contributions will ultimately require richer datasets and experimental paradigms designed specifically for this purpose. We have now modified the results section (page 6) and the discussion (page 11) to clarify these points.</p><disp-quote content-type="editor-comment"><p>The case for the utility of a stimulus-computable model is convincing (as I mentioned above), but its framing as mission-critical for understanding multisensory perception is overstated, I think. The line for what is &quot;stimulus computable&quot; is arbitrary and doesn't seem to be followed in the paper. A strict definition might realistically require inputs to be, e.g., the patterns of light and sound waves available to our eyes and ears, while an even more strict definition might (unrealistically) require those stimuli to be physically present and transduced by the model. A reasonable looser definition might allow an &quot;abstract and low-dimensional representation of the stimulus, such as the stimulus envelope (which was used in the paper), to be an input. Ultimately, some preprocessing of a stimulus does not necessarily confound interpretations about (multi)sensory perception. And on the flip side, the stimulus-computable aspect doesn't necessarily give the model supreme insight into perception. For example, the MCD model was &quot;confused&quot; by the stimuli used in our 2018 paper (Nidiffer et al., 2018; Parise &amp; Ernst, 2025). In each of our stimuli (including catch trials), the onset and offset drove strong AV temporal correlations across all stimulus conditions (including catch trials), but were irrelevant to participants performing an amplitude modulation detection task. The to-be-detected amplitude modulations, set at individual thresholds, were not a salient aspect of the physical stimulus, and thus only marginally affected stimulus correlations. The model was of course, able to fit our data by &quot;ignoring&quot; the on/offsets (i.e., requiring human intervention), again highlighting that the model is tapping into a very basic and ubiquitous computational principle of (multi)sensory perception. But it does reveal a limitation of such a stimulus-computable model: that it is (so far) strictly bottom-up.</p></disp-quote><p>We appreciate the reviewer’s thoughtful engagement with the concept of stimulus computability. We agree that the term requires careful definition and should not be taken as a guarantee of perceptual insight or neural plausibility. In our work, we define a model as “stimulus-computable” if all its inputs are derived directly from the stimulus, rather than from experimenter-defined summary descriptors such as temporal lag, spatial disparity, or cue reliability. In the context of multisensory integration, this implies that a model must account not only for how cues are combined, but also for how those cues are extracted from raw inputs—such as audio waveforms and visual contrast sequences.</p><p>This distinction is central to our modelling philosophy. While ideal observer models often specify how information should be combined once identified, they typically do not address the upstream question of how this information is extracted from sensory input. In that sense, models that are not stimulus-computable leave out a key part of the perceptual pipeline. We do not present stimulus computability as a marker of theoretical superiority, but rather as a modelling constraint that is necessary if one’s aim is to explain how structured sensory input gives rise to perception. This is a view that is also explicitly acknowledged and supported by Reviewer 2.</p><p>Framed in Marr’s (1982) terms, non–stimulus-computable models tend to operate at the computational level, defining what the system is doing (e.g., computing a maximum likelihood estimate), whereas stimulus-computable models aim to function at the algorithmic level, specifying how the relevant representations and operations might be implemented. When appropriately constrained by biological plausibility, such models may also inform hypotheses at the implementational level, pointing to potential neural substrates that could instantiate the computation.</p><p>Regarding the reviewer’s example illustrating a limitation of the MCD model, we respectfully note that the account appears to be based on a misreading of our prior work. In Parise &amp; Ernst (2025), where we simulated the stimuli from Nidiffer et al. (2018), the MCD model reproduced participants’ behavioural data without any human intervention or adjustment. The model was applied in a fully bottom-up, stimulus-driven manner, and its output aligned with observer responses as-is. We suspect the confusion may stem from analyses shown in Figure 6 - Supplement Figure 5 of Parise &amp; Ernst (2025), where we investigated the lack of a frequency-doubling effect in the Nidiffer et al. data. However, those analyses were based solely on the Pearson correlation between auditory and visual stimulus envelopes and did not involve the MCD model. No manual exclusion of onset/offset events was applied, nor was the MCD used in those particular figures. We also note that Parise &amp; Ernst (2025) is a separate, already published study and is not the manuscript currently under review.</p><p>In summary, while we fully agree that stimulus computability does not resolve all the complexities of multisensory perception (see comments below about speech), we maintain that it provides a valuable modelling constraint—one that enables robust, generalisable predictions when appropriately scoped.</p><disp-quote content-type="editor-comment"><p>The manuscript rightly chooses to focus a lot of the work on speech, fitting the MCD model to predict behavioral responses to speech. The range of findings from AV speech experiments that the MCD can account for is very convincing. Given the provided context that speech is &quot;often claimed to be processed via dedicated mechanisms in the brain,&quot; a statement claiming a &quot;first end-to-end account of multisensory perception,&quot; and findings that the MCD model can account for speech behaviors, it seems the reader is meant to infer that energetic correlation detection is a complete account of speech perception. I think this conclusion misses some facets of AV speech perception, such as integration of higher-order, non-redundant/correlated speech features (Campbell, 2008) and also the existence of top-down and predictive processing that aren't (yet!) explained by MCD. For example, one important benefit of AV speech is interactions on linguistic processes - how complementary sensitivity to articulatory features in the auditory and visual systems (Summerfield, 1987) allow constraint of linguistic processes (Peelle &amp; Sommers, 2015; Tye-Murray et al., 2007).</p></disp-quote><p>We thank the reviewer for their thoughtful comments, and especially for the kind words describing the range of findings from our AV speech simulations as “very convincing.”</p><p>We would like to clarify that it is not our view that speech perception can be reduced to energetic correlation detection. While the MCD model captures low- to mid-level temporal dependencies between auditory and visual signals, we fully agree that a complete account of audiovisual speech perception must also include higher-order processes—including linguistic mechanisms and top-down predictions. These are critical components of AV speech comprehension, and lie beyond the scope of the current model.</p><p>Our use of the term “end-to-end” is intended in a narrow operational sense: the model transforms raw audiovisual input (i.e., audio waveforms and video frames) directly into behavioural output (i.e., button press responses), without reliance on abstracted stimulus parameters such as lag, disparity or reliability. It is in this specific technical sense that the MCD offers an end-to-end model. We have revised the manuscript to clarify this usage to avoid any misunderstanding.</p><p>In light of the reviewer’s valuable point, we have now edited the Discussion to acknowledge the importance of linguistic processes (page 13) and to clarify what we mean by end-to-end account (page 11). We agree that future work will need to explore how stimulus-computable models such as the MCD can be integrated with broader frameworks of linguistic and predictive processing (e.g., Summerfield, 1987; Campbell, 2008; Peelle &amp; Sommers, 2015; Tye-Murray et al., 2007).</p><p>References</p><p>Campbell, R. (2008). The processing of audio-visual speech: empirical and neural bases. Philosophical Transactions of the Royal Society B: Biological Sciences, 363(1493), 1001-1010. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1098/rstb.2007.2155">https://doi.org/10.1098/rstb.2007.2155</ext-link></p><p>Nidiffer, A. R., Diederich, A., Ramachandran, R., &amp; Wallace, M. T. (2018). Multisensory perception reflects individual differences in processing temporal correlations. Scientific Reports 2018 8:1, 8(1), 1-15. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/s41598-018-32673-y">https://doi.org/10.1038/s41598-018-32673-y</ext-link></p><p>Parise, C. V, &amp; Ernst, M. O. (2025). Multisensory integration operates on correlated input from unimodal transient channels. ELife, 12. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.7554/ELIFE.90841">https://doi.org/10.7554/ELIFE.90841</ext-link></p><p>Peelle, J. E., &amp; Sommers, M. S. (2015). Prediction and constraint in audiovisual speech perception. Cortex, 68, 169-181. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.cortex.2015.03.006">https://doi.org/10.1016/j.cortex.2015.03.006</ext-link></p><p>Summerfield, Q. (1987). Some preliminaries to a comprehensive account of audio-visual speech perception. In B. Dodd &amp; R. Campbell (Eds.), Hearing by Eye: The Psychology of Lip-Reading (pp. 3-51). Lawrence Erlbaum Associates.</p><p>Tye-Murray, N., Sommers, M., &amp; Spehar, B. (2007). Auditory and Visual Lexical Neighborhoods in Audiovisual Speech Perception: Trends in Amplification, 11(4), 233-241. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/1084713807307409">https://doi.org/10.1177/1084713807307409</ext-link></p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>Building on previous models of multisensory integration (including their earlier correlation-detection framework used for non-spatial signals), the author introduces a population-level Multisensory Correlation Detector (MCD) that processes raw auditory and visual data. Crucially, it does not rely on abstracted parameters, as is common in normative Bayesian models,&quot; but rather works directly on the stimulus itself (i.e., individual pixels and audio samples). By systematically testing the model against a range of experiments spanning human, monkey, and rat data, the authors show that their MCD population approach robustly predicts perception and behavior across species with a relatively small (0-4) number of free parameters.</p><p>Strengths:</p><p>(1) Unlike prior Bayesian models that used simplified or parameterized inputs, the model here is explicitly computable from full natural stimuli. This resolves a key gap in understanding how the brain might extract &quot;time offsets&quot; or &quot;disparities&quot; from continuously changing audio-visual streams.</p><p>(2) The same population MCD architecture captures a remarkable range of multisensory phenomena, from classical illusions (McGurk, ventriloquism) and synchrony judgments, to attentional/gaze behavior driven by audio-visual salience. This generality strongly supports the idea that a single low-level computation (correlation detection) can underlie many distinct multisensory effects.</p><p>(3) By tuning model parameters to different temporal rhythms (e.g., faster in rodents, slower in humans), the MCD explains cross-species perceptual data without reconfiguring the underlying architecture.</p></disp-quote><p>We thank the reviewer for their positive evaluation of the manuscript, and particularly for highlighting the significance of the model's stimulus-computable architecture and its broad applicability across species and paradigms. Please find our responses to the individual points below.</p><disp-quote content-type="editor-comment"><p>Weaknesses:</p><p>(1) The authors show how a correlation-based model can account for the various multisensory integration effects observed in previous studies. However, a comparison of how the two accounts differ would shed light on the correlation model being an implementation of the Bayesian computations (different levels in Marr's hierarchy) or making testable predictions that can distinguish between the two frameworks. For example, how uncertainty in the cue combined estimate is also the harmonic mean of the unimodal uncertainties is a prediction from the Bayesian model. So, how the MCD framework predicts this reduced uncertainty could be one potential difference (or similarity) to the Bayesian model.</p></disp-quote><p>We fully agree with the reviewer that a comparison between the correlation-based MCD model and Bayesian accounts is valuable—particularly for clarifying how the two frameworks differ conceptually and where they may converge.</p><p>As noted in the revised manuscript, the key distinction lies in the level of analysis described by Marr (1982). Bayesian models operate at the computational level, describing what the system is aiming to compute (e.g., optimal cue integration). In contrast, the MCD functions at the algorithmic level, offering a biologically plausible mechanism for how such integration might emerge from stimulus-driven representations.</p><p>In this context, the MCD provides a concrete, stimulus-grounded account of how perceptual estimates might be constructed—potentially implementing computations with Bayesian-like characteristics (e.g., reduced uncertainty, cue weighting). Thus, the two models are not mutually exclusive but can be seen as complementary: the MCD may offer an algorithmic instantiation of computations that, at the abstract level, resemble Bayesian inference.</p><p>We have now updated the manuscript to explicitly highlight this relationship (pages 2 and 11). In the revised manuscript, we also included a new figure (Figure 5) and movie (Supplementary Movie 3), to show how the present approach extends previous Bayesian models for the case of cue integration (i.e., the ventriloquist effect).</p><disp-quote content-type="editor-comment"><p>(2) The authors show a good match for cue combination involving 2 cues. While Bayesian accounts provide a direction for extension to more cues (also seen empirically, for eg, in Hecht et al. 2008), discussion on how the MCD model extends to more cues would benefit the readers.</p></disp-quote><p>We thank the reviewer for this insightful comment: extending the MCD model to include more than two sensory modalities is a natural and valuable next step. Indeed, one of the strengths of the MCD framework lies in its modularity. Let us consider the MCDcorr output (Equation 6), which is computed as the pointwise product of transient inputs across modalities. Extending this to include a third modality, such as touch, is straightforward: MCD units would simply multiply the transient channels from all three modalities, effectively acting as trimodal coincidence detectors that respond when all inputs are aligned in time and space.</p><p>By contrast, extending MCDlag is less intuitive, due to its reliance on opponency between two subunits (via subtraction). A plausible solution is to compute MCDlag in a pairwise fashion (e.g., AV, VT, AT), capturing relative timing across modality pairs.</p><p>Importantly, the bulk of the spatial integration in our framework is carried by MCDcorr, which generalises naturally to more than two modalities. We have now formalised this extension and included a graphical representation in a supplementary section of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>Likely Impact and Usefulness:</p><p>The work offers a compelling unification of multiple multisensory tasks- temporal order judgments, illusions, Bayesian causal inference, and overt visual attention - under a single, fully stimulus-driven framework. Its success with natural stimuli should interest computational neuroscientists, systems neuroscientists, and machine learning scientists. This paper thus makes an important contribution to the field by moving beyond minimalistic lab stimuli, illustrating how raw audio and video can be integrated using elementary correlation analyses.</p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>Recommendations:</p><p>My biggest concern is a lack of specificity about model fitting, which is assuaged by the inclusion of sufficient detail to replicate the analysis completely or the inclusion of the analysis code. The code availability indicates a script for the population model will be included, but it is unclear if this code will provide the fitting details for the whole of the analysis.</p></disp-quote><p>We thank the reviewer for raising this important point. A new methodological section has been added to the manuscript, detailing the model fitting procedures used throughout the study. In addition, the accompanying code repository now includes MATLAB scripts that allow full replication of the spatiotemporal MCD simulations.</p><disp-quote content-type="editor-comment"><p>Perhaps it could be enlightening to re-evaluate the model with a measure of error rather than correlation? And I think many researchers would be interested in the model's performance on unseen data.</p></disp-quote><p>The model has now been re-evaluated using mean squared error (MSE), and the results remain consistent with those obtained using Pearson correlation. Additionally, we have clarified which parts of the study involve testing the model on unseen data (i.e., data not used to fit the temporal constants of the units). These analyses are now included and discussed in the revised fitting section of the manuscript (pages 23-24).</p><p>Otherwise, my concerns involve the interpretation of findings, and thus could be satisfied with minor rewording or tempering conclusions.</p><p>The manuscript has been revised to address these interpretative concerns, with several conclusions reworded or tempered accordingly. All changes are marked in blue in the revised version.</p><disp-quote content-type="editor-comment"><p>Miscellanea:</p><p>Should b0 in equation 10 be bcrit to match the below text?</p></disp-quote><p>Thank you for catching this inconsistency. We have corrected Equation 10 (and also Equation 21) to use the more transparent notation bcrit instead of b0, in line with the accompanying text.</p><p>Equation 23, should time be averaged separately? For example, if multiple people are speaking, the average correlation for those frames will be higher than the average correlation across all times.</p><p>We thank the reviewer for raising this thoughtful and important point. In response, we have clarified the notation of Equation 23 in the revised manuscript (page 20). Specifically, we now denote the averaging operations explicitly as spatial means and standard deviations across all pixel locations within each frame.</p><p>This equation computes the z-score of the MCD correlation value at the current gaze location, normalized relative to the spatial distribution of correlation values in the same frame. That is, all operations are performed at the frame level, not across time. This ensures that temporally distinct events are treated independently and that the final measure reflects relative salience within each moment, not a global average over the stimulus. In other words, the spatial distribution of MCD activity is re-centered and rescaled at each frame, exactly to avoid the type of inflation or confounding the reviewer rightly cautioned against.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>The authors have done a great job of providing a stimulus computable model of cue combination. I had just a few suggestions to strengthen the theoretical part of the paper:</p><p>(1) While the authors have shown a good match between MCD and cue combination, some theoretical justification or equivalence analysis would benefit readers on how the two relate to each other. Something like Zhang et al. 2019 (which is for motion cue combination) would add to the paper.</p></disp-quote><p>We agree that it is important to clarify the theoretical relationship between the Multisensory Correlation Detector (MCD) and normative models of cue integration, such as Bayesian combination. In the revised manuscript, we have now modified the introduction and added a paragraph in the Discussion addressing this link more explicitly. In brief, we see the MCD as an algorithmic-level implementation (in Marr’s terms) that may approximate or instantiate aspects of Bayesian inference.</p><disp-quote content-type="editor-comment"><p>(2) Simulating cue combination for tasks that require integration of more than two cues (visual, auditory, haptic cues) would more strongly relate the correlation model to Bayesian cue combination. If that is a lot of work, at least discussing this would benefit the paper</p></disp-quote><p>This point has now been addressed, and a new paragraph discussing the extension of the MCD model to tasks involving more than two sensory modalities has been added to the Discussion section.</p></body></sub-article></article>