<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">84357</article-id><article-id pub-id-type="doi">10.7554/eLife.84357</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Advance</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Inconsistencies between human and macaque lesion data can be resolved with a stimulus-computable model of the ventral visual stream</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-296448"><name><surname>Bonnen</surname><given-names>Tyler</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8709-1651</contrib-id><email>bonnen@stanford.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-107807"><name><surname>Eldridge</surname><given-names>Mark AG</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-4292-6832</contrib-id><email>mark.eldridge@nih.gov</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00f54p054</institution-id><institution>Stanford University</institution></institution-wrap><addr-line><named-content content-type="city">Stanford</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01cwqze88</institution-id><institution>Laboratory of Neuropsychology, National Institute of Mental Health,National Institutes of Health</institution></institution-wrap><addr-line><named-content content-type="city">Bethesda</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>de Lange</surname><given-names>Floris P</given-names></name><role>Senior Editor</role><aff><institution>Donders Institute for Brain, Cognition and Behaviour</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>06</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e84357</elocation-id><history><date date-type="received" iso-8601-date="2022-11-01"><day>01</day><month>11</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-06-05"><day>05</day><month>06</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-09-15"><day>15</day><month>09</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.12.507636"/></event></pub-history><permissions><ali:free_to_read/><license xlink:href="http://creativecommons.org/publicdomain/zero/1.0/"><ali:license_ref>http://creativecommons.org/publicdomain/zero/1.0/</ali:license_ref><license-p>This is an open-access article, free of all copyright, and may be freely reproduced, distributed, transmitted, modified, built upon, or otherwise used by anyone for any lawful purpose. The work is made available under the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/publicdomain/zero/1.0/">Creative Commons CC0 public domain dedication</ext-link>.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-84357-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-84357-figures-v2.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.36310" id="ra1"/><abstract><p>Decades of neuroscientific research has sought to understand medial temporal lobe (MTL) involvement in perception. Apparent inconsistencies in the literature have led to competing interpretations of the available evidence; critically, findings from human participants with naturally occurring MTL damage appear to be inconsistent with data from monkeys with surgical lesions. Here, we leverage a ‘stimulus-computable’ proxy for the primate ventral visual stream (VVS), which enables us to formally evaluate perceptual demands across stimulus sets, experiments, and species. With this modeling framework, we analyze a series of experiments administered to monkeys with surgical, bilateral damage to perirhinal cortex (PRC), an MTL structure implicated in visual object perception. Across experiments, PRC-lesioned subjects showed no impairment on perceptual tasks; this originally led us(Eldridge et al., 2018) to conclude that PRC is not involved in perception. Here, we find that a ‘VVS-like’ model predicts both PRC-intact and -lesioned choice behaviors, suggesting that a linear readout of the VVS should be sufficient for performance on these tasks. Evaluating these computational results alongside findings from human experiments, we suggest that results from (Eldridge et al., 2018) alone cannot be used as evidence against PRC involvement in perception. These data indicate that experimental findings from human and non-human primates are consistent. As such, what appeared to be discrepancies between species was in fact due to reliance on informal accounts of perceptual processing.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>visual object perception</kwd><kwd>deep learning</kwd><kwd>stimulus-computable methods</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Rhesus macaque</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>ZIAMH002032</award-id><principal-award-recipient><name><surname>Bonnen</surname><given-names>Tyler</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>F99NS125816</award-id><principal-award-recipient><name><surname>Bonnen</surname><given-names>Tyler</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection, and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A 'stimulus-computable' modeling approach resolves apparent inconsistencies between human and monkey lesion data, implicating perirhinal cortex in visual object perception.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Neuroanatomical structures within the medial temporal lobe (MTL) are known to support memory-related behaviors (<xref ref-type="bibr" rid="bib36">Scoville and Milner, 1957</xref>; <xref ref-type="bibr" rid="bib17">Eichenbaum and Cohen, 2004</xref>; <xref ref-type="bibr" rid="bib26">LaRocque and Wagner, 2015</xref>). For decades, experimentalists have also observed MTL-related impairments in tasks designed to test perceptual processing (<xref ref-type="bibr" rid="bib39">Suzuki, 2009</xref>; <xref ref-type="bibr" rid="bib3">Baxter, 2009</xref>). These findings centered on perirhinal cortex (PRC), an MTL structure situated at the apex of high-level sensory cortices (<xref ref-type="fig" rid="fig1">Figure 1a</xref>). Visual impairments were reported following lesions to PRC in humans and other animals, bolstering a perceptual-mnemonic account of perirhinal function (e.g. <xref ref-type="bibr" rid="bib30">Murray and Bussey, 1999</xref>; <xref ref-type="bibr" rid="bib9">Bussey et al., 2002</xref>; <xref ref-type="bibr" rid="bib27">Lee et al., 2005</xref>; <xref ref-type="bibr" rid="bib28">Lee et al., 2006</xref>; <xref ref-type="bibr" rid="bib1">Barense et al., 2007</xref>; <xref ref-type="bibr" rid="bib19">Inhoff et al., 2019</xref>). However, there were also visual experiments for which no impairments were observed following PRC lesions (e.g. <xref ref-type="bibr" rid="bib6">Buffalo et al., 1998a</xref>; <xref ref-type="bibr" rid="bib7">Buffalo et al., 1998b</xref>; <xref ref-type="bibr" rid="bib38">Stark and Squire, 2000</xref>; <xref ref-type="bibr" rid="bib24">Knutson et al., 2012</xref>). In this was, decades of evidence resulted in a pattern of seemingly inconsistent experimental outcomes, with no formal method for disambiguating between competing interpretations of the available data.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Formalizing medial temporal lobe (MTL) involvement in visual object perception.</title><p>(<bold>a</bold>) Perirhinal cortex (PRC) is an MTL structure situated at the apex of the primate ventral visual stream (VVS), located within rhinal cortex (RHC; see inset). (<bold>b</bold>) To formalize PRC involvement in visual object perception, here we leverage a computational model able to make predictions about VVS-supported performance directly from experimental stimuli. Early model layers best fit electrophysiological recordings from early stages of processing within the VVS (i.e. V4; left, gray); later layers best fit later stages of processing from the VVS (i.e. IT; left, green). We approximate VVS-supported performance by extracting responses from an ‘IT-like’ model layer (center). Our protocol approximates VVS-supported performance (right; green) while human participants nonetheless outperform model/VVS performance (<xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>; right, purple). (<bold>c</bold>) Given that humans can outperform a linear readout of the VVS, here we schematize the pattern of lesion results that would be consistent with the PRC involvement in perception (left), results that would indicate that non-PRC brain structures are required to outperform the VVS (center), as well as results which indicate that a visual discrimination task is supported by the VVS (i.e. ‘non-diagnostic’ because no extra-VVS perceptual processing is required).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig1-v2.tif"/></fig><p>One of the central challenges in this experimental literature has been isolating PRC-dependent behaviors from those supported by PRC-adjacent sensory cortex. In the primate, this requires disentangling PRC-dependent performance from visual behaviors supported by the ventral visual stream (VVS; <xref ref-type="bibr" rid="bib14">DiCarlo and Cox, 2007</xref>; <xref ref-type="bibr" rid="bib15">DiCarlo et al., 2012</xref>). Lacking more objective metrics, experimentalists had relied on informal, descriptive accounts of perceptual demands; terms such as ‘complexity’ and ‘feature ambiguity’ were intended to characterize those stimulus properties that are necessary to evaluate PRC involvement in visual object perception. However, this informal approach led to conflicting interpretations of the available evidence, without any means to arbitrate between them. For example, the absence of PRC-related deficits in a given study (e.g. <xref ref-type="bibr" rid="bib38">Stark and Squire, 2000</xref>) has led to the conclusion that PRC is not involved in perception (<xref ref-type="bibr" rid="bib39">Suzuki, 2009</xref>), while others argue that stimuli in stimuli from these studies are not ‘complex’ enough (i.e. can be represented by canonical visual cortices) and so no perceptual deficits are expected (<xref ref-type="bibr" rid="bib8">Bussey and Saksida, 2002</xref>).</p><p>In recent years, deep learning computational methods have become commonplace in the vision sciences. Remarkably, these models are able to predict neural responses throughout the primate VVS directly from experimental stimuli: given an experimental image as input, these models (e.g. convolutional neural networks, CNNs) are able to predict neural responses. These ‘stimulus-comptable’ methods currently provide the most quantitatively accurate predictions of neural responses throughout the primate VVS (<xref ref-type="bibr" rid="bib40">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Khaligh-Razavi and Kriegeskorte, 2014</xref>; <xref ref-type="bibr" rid="bib34">Rajalingham et al., 2018</xref>; <xref ref-type="bibr" rid="bib2">Bashivan et al., 2019</xref>). For example, early model layers within a CNN better predict earlier stages of processing within the VVS (e.g. V4; <xref ref-type="fig" rid="fig1">Figure 1b</xref>: left, gray) while later model layers better predict later stages of processing within the VVS (e.g. IT; <xref ref-type="fig" rid="fig1">Figure 1b</xref>: left, green). We note that there is not a 1–1 correspondence between these models and the primate VVS as they typically lack known biological properties (<xref ref-type="bibr" rid="bib41">Zhuang et al., 2021</xref>; <xref ref-type="bibr" rid="bib16">Doerig et al., 2022</xref>). Nonetheless, these models can be modified to evaluate domain-specific hypotheses (<xref ref-type="bibr" rid="bib16">Doerig et al., 2022</xref>)—for example by adding recurrence (<xref ref-type="bibr" rid="bib25">Kubilius et al., 2018</xref>; <xref ref-type="bibr" rid="bib23">Kietzmann et al., 2019</xref>) or eccentricity-dependent scaling (<xref ref-type="bibr" rid="bib13">Deza and Konkle, 2020</xref>; <xref ref-type="bibr" rid="bib20">Jonnalagadda et al., 2021</xref>).</p><p>Recently, <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref> leveraged these ‘VVS-like’ models to evaluate the performance of PRC-intact/-lesioned human participants in visual discrimination tasks. While VVS-like models are able to approximate performance supported by a linear readout of high-level visual cortex (<xref ref-type="fig" rid="fig1">Figure 1b</xref>: right, green), human participants are able to out outperform both VVS-like models and a linear readout of direct electrophysiological recordings from the VVS (<xref ref-type="fig" rid="fig1">Figure 1b</xref>: right, purple). Critically, VVS-like models approximate PRC-lesioned performance. While these data implicate PRC in visual object processing, there remains experimental data collected from non-human primates which have not been formally evaluated. Like the human literature, non-human primate data have been used to both support and refute PRC involvement in perception. Unlike the naturally occurring lesioned in humans, experiments with non-human primates have unparalleled control over the site and extent of PRC lesions—potentially, providing more incisive tests of competing claims over PRC function. As such, characterizing the discrepancies between human and non-human primate data is a critical step toward developing a more formal understanding of PRC involvement in perception.</p><p>In order to resolve this cross-species discrepancy, here we formalize perceptual demands in experiments administered to PRC-intact/-lesioned monkeys (<italic>Macaca mulatta</italic>). We draw from data collected by <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> which provides striking evidence against PRC involvement in perception: <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> created multiple stimulus sets, allowing for more a fine-grained evaluation of perceptual behaviors than previous, related work (e.g. <xref ref-type="bibr" rid="bib10">Bussey et al., 2003</xref>). Here, we estimate VVS-supported performance on stimuli from <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> and compare these predictions to PRC-intact and -lesioned choice behaviors. This modeling approach enables us to situate human and macaque lesion data within a shared metric space (i.e. VVS-model performance); as such, previous observations in the human (e.g. <xref ref-type="fig" rid="fig1">Figure 1b</xref>: right, green) constrain how data from <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> can be interpreted; critically, to evaluate PRC involvement in perception, the performance of non-lesioned participants must exceed VVS-modeled performance. Given this, supra-VVS performance may be due to PRC-dependent contributions (schematized in <xref ref-type="fig" rid="fig1">Figure 1c</xref>: left), or for reasons unrelated to PRC function (schematized in <xref ref-type="fig" rid="fig1">Figure 1c</xref>: middle). However, if VVS-supported performance approximates PRC-intact behavior, no perceptual processing beyond the VVS should be necessary (schematized in <xref ref-type="fig" rid="fig1">Figure 1c</xref>: right). We refer to stimuli in this category as ‘non-diagnostic’.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We begin with a task-optimized convolutional neural network, pretrained to perform object classification. We estimate the correspondence between this model and electrophysiological responses from high-level visual cortex using a protocol previously reported in <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>. We summarize this protocol here, but refer to the previous manuscript for a more detailed account. Using previously collected electrophysiological responses from macaque VVS (<xref ref-type="bibr" rid="bib29">Majaj et al., 2015</xref>), we identify a model layer that best fits high-level visual cortex: Given a set of images, we learn a linear mapping between model responses and a single electrode’s responses, then evaluate this mapping using independent data (i.e. left-out images). For each model layer, this analysis yields a median cross-validated fit to noise-corrected neural responses, for both V4 and IT. As is consistently reported (e.g. <xref ref-type="bibr" rid="bib35">Schrimpf et al., 2020</xref>), early model layers (i.e. first half of layers) better predict neural responses in V4 than do later layers (unpaired <italic>t</italic>-test: <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2.70</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.015</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>: left, gray), while later layers better predict neural responses in IT, a higher-level region (unpaired <italic>t</italic>-test: <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>3.70</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>: left, green). Peak V4 fits occur in model layer pool3 (noise-corrected <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn><mml:mo>±</mml:mo><mml:mn>0.30</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> STD) while peak IT fits occur in con5_1 (noise-corrected <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn><mml:mo>±</mml:mo><mml:mn>0.16</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> STD). For ease, in all subsequent analyses we use model responses from a con5_1-adjacent layer, fc6, which has comparable neural fits but a lower-dimensional representation.</p><p>Next we compare model, VVS-supported, and human performance within the same metric space: Instead of fitting model responses directly to electrophysiological recordings in high-level visual cortex, as above, here we evaluate the similarity between the performance supported by the model and high-level visual cortex, as well as human performance on these same stimuli. For this comparison, we leverage electrophysiological responses previously collected from macaque IT cortex (<xref ref-type="bibr" rid="bib29">Majaj et al., 2015</xref>), using a protocol originally detailed in <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>. We independently estimate model and VVS-supported performance on a stimulus set composed of concurrent visual discrimination trials, using a modified leave-one-out cross-validation strategy. We then determine the model-VVS fit over the performance estimates, as developed in <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref> and outlined in Methods. We can compare model performance with both VVS-supported performance and PRC-intact (human, <italic>n</italic> = 297) performance on these same stimuli, using data from <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>. On this dataset, a computational proxy for the VVS predicts IT-supported performance (<inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.81</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>13.33</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>, green), while each are outperformed by (<inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.24</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf9"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>31</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>9.50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>: right, purple). These data suggest that while these models are suitable proxies for VVS-supported performance, human performance is able to exceed a linear readout of the VVS.</p><p>With these ‘VVS-like’ models, we turn to analyses of macaque lesion data. First, we extract model responses to each stimulus in all four experiments administered by <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref>. In these experiments, subjects provided a binary classification for each stimulus: ‘cat’ or ‘dog.’ Critically, stimuli were composed not only of cats and dogs, but of ‘morphed’ images that parametrically vary the percent of category-relevant information present in each trial. For example, ‘10% morphs’ were 90% cat and 10% dog. These morphed stimuli were designed to evaluate PRC involvement in perception by creating maximal ‘feature ambiguity,’ a perceptual quality reported to elicit PRC dependence in previous work (<xref ref-type="bibr" rid="bib9">Bussey et al., 2002</xref>; <xref ref-type="bibr" rid="bib32">Norman and Eacott, 2004</xref>; <xref ref-type="bibr" rid="bib11">Bussey et al., 2006</xref>; <xref ref-type="bibr" rid="bib31">Murray and Richmond, 2001</xref>). On each trial, subjects were rewarded for responses that correctly identify which category best fits the image presented (e.g. 10% = ‘cat’, 90% = ‘dog’, correct response is ‘dog’). We evaluate data from two groups of monkeys in this study: an unoperated control group (<italic>n</italic> = 3) and a group with bilateral removal of rhinal cortex, which including peri- and entorhinal cortex. We formulate the modeling problem as a binary forced choice (i.e. ‘dog’ = 1, ‘cat’ = 0) and present the model with experimental stimuli. We then extract model responses from a layer that corresponds to ‘high-level’ visual cortex and learn a linear mapping from model responses to predict the category label. For all analyses, we report the results on held-out data (Methods: Determining model performance).</p><p>We first evaluate model performance with the aggregate metrics used by the original authors—not on the performance of individual images, but on the proportion of trials within the same ‘bin’ that are correct. With the original behavioral data, we average performance across images within each morph level (e.g. 10%, 20%, etc.) across subjects in each lesion group (PRC-intact <xref ref-type="fig" rid="fig2">Figure 2a</xref>, and -lesioned <xref ref-type="fig" rid="fig2">Figure 2b</xref>). As reported in <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref>, there is not a significant difference between the choice behaviors of PRC-lesioned and -intact subjects (no significant difference between PRC-intact/-lesion groups: <inline-formula><mml:math id="inf11"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.00</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>F</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>86</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> = <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0.07</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.941</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). For each of these experiments, we extract model responses to all stimuli from a model layer that best corresponds to a high-level visual region, inferior temporal (IT) cortex. Using the model responses from this ‘IT-like’ model layer to each image, we train a linear, binary classification model on the category label of each image (i.e. ‘dog’ or ‘cat’) on 4/5th of the available stimuli. We then evaluate model performance on the remaining 1/5th of those stimuli, repeating this procedure across 50 iterations of randomized train–test splits. A computational proxy for the VVS exhibits the same qualitative pattern of behavior as each subject group (<xref ref-type="fig" rid="fig2">Figure 2c</xref>, model performance across multiple train–test iterations in black). Moreover, we observe a striking correspondence between model and PRC-intact behavior (<xref ref-type="fig" rid="fig2">Figure 2b</xref>, purple: <inline-formula><mml:math id="inf15"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.98</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf16"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.97</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>33.12</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>6</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>19</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> ) as well as -lesioned subjects (green: <inline-formula><mml:math id="inf18"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.96</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>57.38</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>23</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). Employing the same metric used to claim no significant difference between PRC-lesion/-intact performance, we find no difference between subject and model behavior (<inline-formula><mml:math id="inf21"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.00</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>0.01</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>86</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.11</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.915</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>A computational proxy for the ventral visual stream (VVS) predicts perirhinal cortex (PRC)-intact and -lesioned behavior.</title><p>Averaging across subjects and morph levels (i.e. all 10% morphs, 20% morphs, etc.), (<bold>a</bold>) PRC-intact (<italic>n</italic> = 3) and (<bold>b</bold>) PRC-lesioned (<italic>n</italic> = 3) subjects exhibit a similar pattern of responses across experiments (rows 1–4). We present stimuli used in this experiment to a computational proxy for the VVS, extracting model responses from a layer that corresponds with ‘high-level’ perceptual cortex. From these model responses, we learn to predict the category membership of each stimulus, (<bold>c</bold>) testing this linear mapping on left-out images across multiple train–test iterations (black). (<bold>d</bold>) This computational proxy for the VVS accurately predicts the choice behavior of PRC-intact (purple) and -lesioned (green) grouped subjects (error bars indicate standard deviation from the mean, across model iterations and subject choice behaviors). As such, a linear readout of the VVS appears to be sufficient to perform these tasks, thus there need be no involvement of PRC to achieve neurotypical performance.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Experimental stimuli and protocol from <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref>.</title><p>(<bold>a</bold>) Example stimuli from experiment 1, illustrating multiple instances of stimuli across morph levels. (<bold>b</bold>) Example stimuli used used for masked morphs, in experiment 3. (<bold>c</bold>) Example stimuli used for ‘crossed morphs’ in experiment 2. (<bold>d</bold>) Protocol for all experiments. Subject’s initiate each trial with a lever press. A stimulus is presented, followed by a red dot at the central field of view. Subjects could avoid an extended inter-trial delay by releasing the bar in the first interval (signaled by a red target) for stimuli that were less than 50% dog, and were rewarded for releasing the bar in the second interval (signaled by a green target) for stimuli that were more than 50% dog. This amounts to an asymmetrical reward structure. They were rewarded randomly for releasing during the green interval for 50–50 morphs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Colinearity within the stimulus set revealed by a pixel-level analysis.</title><p>Classification behaviors on this stimulus set are learned and evaluated on images with a high degree of colinearity: stimuli with similar correct answers are highly overlapping, as can be seen in the example stimuli in <xref ref-type="fig" rid="fig1">Figure 1</xref>. We formalize this observation by adapting previous analyses (visualized in <xref ref-type="fig" rid="fig2">Figure 2d</xref>): In the place of a computational proxy for the primate ventral visual stream (VVS) we use raw pixel-level representations, training a linear readout of stimulus category directly from the vectorized (i.e. flattened) images. We find that these pixel-level representations are sufficient to achieve the performance observed across experimental subjects in both perirhinal cortex (PRC)-intact and -lesioned groups. The colinearity in these data suggests that ’high-level’ representations may not be necessary to classify stimuli in these experiments, as a linear operation over the stimuli themselves achieve group-level performance. Error bars in all experiments indicate standard deviation from the mean, across model iterations and subject choice behaviors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Pixel-level performance fails on a more conservative evaluation metric.</title><p>Previous evaluation of model performance employed a train–test split that was ‘naive’ to the colinearity in the available data. Here, we implement and validate a more conservative evaluation metric: To evaluate each image within a given morph sequence (i.e. a unique cat–dog combination that spans from 0% to 100% dog) we remove all instances in that morph sequence from the training data. This ensures that there are no image-adjacent stimuli that the model can exploit (e.g. training on 10% and testing on 0% within the same morph sequence). As anticipated, pixel-level classification only achieves chance performance under this more conservative evaluation metric. Error bars in all experiments indicate standard deviation from the mean, across model iterations and subject choice behaviors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Model approximates primate behavior even with a more conservative evaluation metric.</title><p>Here, we evaluate model performance using a more conservative train–test split. While pixel-level representations in <xref ref-type="fig" rid="fig3">Figure 3</xref> fail to approximate group-level performance, a computational proxy for the ventral visual stream (VVS) continues to predict group-level performance of both perirhinal cortex (PRC)-intact and -lesioned participants across experiments. Error bars in all experiments indicate standard deviation from the mean, across model iterations and subject choice behaviors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig2-figsupp4-v2.tif"/></fig></fig-group><p>We extend our analysis beyond the aggregate morph- and subject-level analyses used by the original authors, introducing a split-half reliability analysis (Methods: Split-half reliability estimates). This enables us to determine if there is reliable choice behavior, for each subject, at the level of individual images. We restrict our analyses to experiments with sufficient data, as this analysis requires multiple repetitions of each image; we exclude experiments 3 (’Masked Morphs’) and 4 (’Crossed Morphs’) due to insufficient repetitions (which can be seen in <xref ref-type="fig" rid="fig2">Figure 2</xref>, rows 3–4). Across both remaining experiments, we find consistent image-level choice behaviors for subjects with an intact (e.g. median <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mtext>exp1</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.94</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, median <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mtext>exp2</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.86</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) and lesioned (e.g. median <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mtext>exp1</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.91</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, median <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mtext>exp2</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) rhinal cortex (<xref ref-type="fig" rid="fig3">Figure 3a</xref>: within-subject reliability on the diagonal; PRC-intact subjects in purple, PRC-lesioned subjects in green). We also observe consistent image-level choice behaviors between subjects (e.g. median <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mtext>exp1</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.86</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, median <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mrow><mml:mtext>exp2</mml:mtext></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.79</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). These results indicate there is reliable within- and between-subject variance in the image-by-image choice behaviors of experimental subjects (<xref ref-type="fig" rid="fig3">Figure 3a</xref>: PRC-intact subjects in purple, PRC-lesioned subjects in green; between-group reliability in gray), suggesting that this behavior is a suitable target to evaluate how well we approximate more granular subject behaviors with a computational proxy for the VVS. We next examine whether the model can predict these more granular, subject- and image-level choice behaviors (see Methods: Consistency estimates).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Ventral visual stream (VVS) model fits subject behavior for aggregate but not image-level metrics.</title><p>Here, we perform more granular analyses than those conducted by the authors of the original study: evaluating the model’s correspondence with perirhinal cortex (PRC)-lesioned and -intact performance at the level of individual subjects and images. We restrict ourselves to experiments that had sufficient data to determine the split-half reliability of each subject’s choice behaviors. First, we determine whether there is reliable image-level choice behavior observed for each subject, that is no longer averaged across morph levels. (<bold>a</bold>) We estimate the correspondence between subject choice behaviors over 100 split-half iterations, for both experiments 1 (closed circles) and 2 (open circles), using <inline-formula><mml:math id="inf30"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> as a measure of fit. Each row contains a given subjects’ (e.g. subject 0, top row) correspondence with all other subjects’ choice behaviors, for PRC-intact (purple) and -lesioned (green) subjects. We find that the image-level choice behaviors are highly reliable both within (on diagonal) and between subjects (off diagonal), including between PRC-lesioned and -intact subjects (gray). We next compare model performance to the behavior of individual subjects, averaging over morph levels in accordance with previous analyses (i.e. averaging performance across all images within each morph level, e.g. 10%). (<bold>b</bold>) We observe a striking correspondence between the model and both PRC-lesioned (green) and PRC-intact (purple) performance for all subjects. (<bold>c</bold>) Finally, for each subject, we estimate the correspondence between model performance and the subject-level choice behaviors, at the resolution of individual images. Although model fits to subject behavior are statistically significant, it clearly does not exhibit ‘subject-like’ choice behavior at this resolution. Error bars in all experiments indicate standard deviation from the mean, across model iterations and subject choice behaviors.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Ventral visual stream (VVS) model is ‘subject-like’ for aggregate but not image-level metrics.</title><p>We estimate the correspondence between subject–subject choice behaviors. First, we generate a random split of each subject’s performance. We then compute the between-subject correlation, iterating across 100 random splits. Each row contains a given subjects’ (e.g. subject 1, top row) correspondence with all other experimental subjects, including perirhinal cortex (PRC)-intact (purple) and -lesioned (green) monkeys. Using this same subject–subject measure, we also estimate subject–model correspondence (gray). We visualize our results at two resolutions: (<bold>a</bold>) for the morph-level analysis, we average performance across all images within each morph level (e.g. 10%, 20%, etc.; as per the analysis in <xref ref-type="fig" rid="fig3">Figure 3b</xref>) and compare a single subject’s behaviors to all other experimental subjects, as well as model performance; (<bold>b</bold>) for the image-level analysis we average performance across a random split of trials containing each image, for each subject, then compare each single subject’s behaviors to all other experimental subjects, as well as model performance (as outlined in Methods: Consistency estimates). For the morph-level analysis, the model choice behavior is ‘subject-like’; the distribution of model–subject correspondence is within the distribution of between-subject correspondence (in <xref ref-type="fig" rid="fig3">Figure 3b</xref>, subject-level choices behaviors are on the diagonal). However, at the resolution of single images, model choice behavior is not subject-like; model correspondence to each subject is not likely observed under the between-subject distributions (i.e. subject-level choices behaviors do not fall along the diagonal in <xref ref-type="fig" rid="fig3">Figure 3c</xref>). We note the PRC-intact monkeys are subjects 1–3, PRC-lesioned monkeys are subjects 4–6.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-84357-fig3-figsupp1-v2.tif"/></fig></fig-group><p>Our computational approach is able to predict subject-level choice behavior when aggregated across morph levels, for both PRC-intact (e.g. subject 0; <inline-formula><mml:math id="inf31"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1.01</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>39.30</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and -lesioned (e.g. subject 4: <inline-formula><mml:math id="inf33"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.99</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1.01</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>21</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>45.01</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) subjects (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). Interestingly, the model’s fit to subject behavior is indistinguishable from the distribution of between-subject reliability estimates (<xref ref-type="fig" rid="fig1">Figure 1a</xref>; median of the empirical p(model <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mrow><mml:mtext>reliability</mml:mtext></mml:mrow><mml:mrow><mml:mrow><mml:mtext>between-subject</mml:mtext></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.592</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>) suggesting that the model exhibits ‘subject-like’ behaviors at this resolution. Our modeling approach is also able to significantly predict image-level choice behaviors for both PRC-lesioned (e.g. subject 3: <inline-formula><mml:math id="inf36"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.86</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.81</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>438</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>52.79</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>192</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>) and -intact subjects (e.g. subject 1: <inline-formula><mml:math id="inf38"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>438</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>53.24</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>193</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). However, the model behavior is unlikely to be observed under the distribution of between-subject reliability estimates (between-subject reliability distributions visualized in <xref ref-type="fig" rid="fig1">Figure 1b</xref>; median of the empirical p(model <inline-formula><mml:math id="inf40"><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msub><mml:mtext>reliability</mml:mtext><mml:mtext>between-subject</mml:mtext></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). That is, the model does not exhibit ‘subject-like’ choice behaviors at the resolution of individual images. This is an important caveat to note when evaluating the correspondence between model performance and animal behavior: as previously reported (<xref ref-type="bibr" rid="bib34">Rajalingham et al., 2018</xref>), even as these models approximate neural responses and choice behaviors in the aggregate (i.e. across images), they do not necessarily capture the trial-by-trial choice behaviors. We elaborate on this further in the discussion.</p><p>There are properties of the experimental design in <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> that encourage a more careful comparison between primate and model behavior. Experimental stimuli contain discrete interpolations between ‘cat’ and ‘dog’ images, such that adjacent stimuli within a morph sequence are highly similar (e.g. see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The colinearity in this stimulus set is revealed by running a classification analysis over pixels: a linear readout of stimulus category directly from the vectorized (i.e. flattened) images themselves is sufficient to approximate aggregate performance of all experimental groups (<inline-formula><mml:math id="inf41"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.94</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>26.74</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>28</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> ; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). To ensure that the VVS modeling approach is not simply a byproduct of the colinearity in the stimuli, we construct a conservative method for model evaluation by restricting training data to images from unrelated morphs sequences (i.e. train on morph sequences A–F, test on morph sequence G). Under this more conservative train–test split, pixels are no longer predictive of primate behavior (<inline-formula><mml:math id="inf44"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.45</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.42</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.164</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), but there remains a clear correspondence between the model and PRC-lesioned (<inline-formula><mml:math id="inf47"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1.17</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>16.94</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> ) and -intact performance (<inline-formula><mml:math id="inf50"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1.16</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>17.39</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). That is, although subjects were able to exploit the colinearity in the stimuli to improve their performance with experience, the correspondence between VVS models and primate choice behaviors is not an artifact of these low-level stimulus attributes.</p><p>There are properties of the experimental design in <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> that encourage a more careful comparison between primate and model behavior. Experimental stimuli contain discrete interpolations between ‘cat’ and ‘dog’ images, such that adjacent stimuli within a morph sequence are highly similar (e.g. see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The colinearity in this stimulus set is revealed by running a classification analysis over pixels: a linear readout of stimulus category directly from the vectorized (i.e. flattened) images themselves is sufficient to approximate aggregate performance of all experimental groups (<inline-formula><mml:math id="inf52"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.94</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.90</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>26.74</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>28</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> ; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). To ensure that the VVS modeling approach is not simply a byproduct of the colinearity in the stimuli, we construct a conservative method for model evaluation by restricting training data to images from unrelated morphs sequences (i.e. train on morph sequences A–F, test on morph sequence G). Under this more conservative train–test split, pixels are no longer predictive of primate behavior (<inline-formula><mml:math id="inf55"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.45</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1.42</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.164</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>), but there remains a clear correspondence between the model and PRC-lesioned (<inline-formula><mml:math id="inf58"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.87</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1.17</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>16.94</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> , <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> ) and -intact performance (<inline-formula><mml:math id="inf61"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn></mml:mrow><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1.16</mml:mn></mml:mrow></mml:math></inline-formula>,  <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>42</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>17.39</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>21</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). That is, although subjects were able to exploit the colinearity in the stimuli to improve their performance with experience, the correspondence between VVS models and primate choice behaviors is not an artifact of these low-level stimulus attributes.</p></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>To evaluate competing claims surrounding PRC involvement in perception, <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> administered a series of visual classification tasks to PRC-lesioned/-intact monkeys. These stimuli were carefully crafted to exhibit a qualitative, perceptual property that had previously been shown to elicit PRC dependence (i.e. ‘feature ambiguity’; <xref ref-type="bibr" rid="bib9">Bussey et al., 2002</xref>; <xref ref-type="bibr" rid="bib32">Norman and Eacott, 2004</xref>; <xref ref-type="bibr" rid="bib11">Bussey et al., 2006</xref>; <xref ref-type="bibr" rid="bib31">Murray and Richmond, 2001</xref>). The absence of PRC-related deficits across four experiments led the original authors to suggest that perceptual processing is not dependent on PRC. Here, we reevaluate this claim by situating these results within a more formal computational framework; leveraging task-optimized convolutional neural networks as a proxy for primate visual processing (<xref ref-type="bibr" rid="bib40">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Rajalingham et al., 2018</xref>; <xref ref-type="bibr" rid="bib35">Schrimpf et al., 2020</xref>). We first determined VVS-model performance on the experimental stimuli in <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref>. We then compared these computational results with monkey choice behaviors, including subjects with bilateral lesions to PRC (<italic>n</italic> = 3), as well as unoperated controls (<italic>n</italic> = 3). For both PRC-lesioned/-intact monkeys, we observe a striking correspondence between VVS model and experimental behavior at the group (<xref ref-type="fig" rid="fig2">Figure 2d</xref>) and subject level (<xref ref-type="fig" rid="fig3">Figure 3b</xref>). These results suggest that a linear readout of the VVS should be sufficient to enable the visual classification behaviors in <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref>; no PRC-related impairments are expected.</p><p>In isolation, it is ambiguous how these data should be interpreted. For example, if VVS-modeled accuracy was sufficient to explain PRC-intact performance across all known stimulus sets, this would suggest that PRC is not involved in visual object perception. However, previous computational results from humans demonstrate that PRC-intact participants are able to outperform a linear readout of the VVS (schematized in <xref ref-type="fig" rid="fig1">Figure 1b</xref>: right, purple). Because results from these human experiments are in the same metric space as our current results (i.e. VVS-modeled performance), these data unambiguously constrain our interpretation: for a stimulus set to evaluate PRC involvement in visual processing, participants must be able to outperform a linear readout of the VVS. That is, supra-VVS performance must be observed in order to isolate PRC contributions from those of other possible contributors to these behaviors (e.g. prefrontal cortex, schematized in <xref ref-type="fig" rid="fig1">Figure 1c</xref>: center). Given that supra-VVS performance is not observed in the current stimulus set (<xref ref-type="fig" rid="fig2">Figure 2d</xref>; schematized in <xref ref-type="fig" rid="fig1">Figure 1c</xref>: right), we conclude that experiments in <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> are not diagnostic of PRC involvement in perception. Consequently, we suggest that these data do not offer absolute evidence against PRC involvement in perception—revising the original conclusions made from this study.</p><p>We note that there is meaningful variance in the trial-level behaviors not captured by the current modeling framework. By conducting a more granular analyses than the original study (i.e. an image-level analysis, instead of averaging across multiple images within the same morph level), we found that image-level choice behaviors are reliable both within and between subjects (<xref ref-type="fig" rid="fig3">Figure 3a</xref>). At this image-level resolution, however, the VVS model does not match the pattern of choice behaviors evident in experimental subjects (<xref ref-type="fig" rid="fig3">Figure 3c</xref> <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). This observation is consistent with previous reports (<xref ref-type="bibr" rid="bib34">Rajalingham et al., 2018</xref>), suggesting that these VVS-like models are best suited to approximate aggregate choice behaviors, not responses to individual images. Many sources of variance have been identified as possible contributors to these subject–model divergences, such as biologically implausible training data (<xref ref-type="bibr" rid="bib41">Zhuang et al., 2021</xref>), or lack of known properties of the primate visual system—for example recurrence (<xref ref-type="bibr" rid="bib21">Kar and DiCarlo, 2020</xref>) or eccentricity-dependent scaling (<xref ref-type="bibr" rid="bib20">Jonnalagadda et al., 2021</xref>).</p><p>While admittedly coarse, these computational proxies for the VVS provide an unprecedented opportunity to understand perirhinal function. Their contribution is, principally, to isolate PRC-dependent behaviors from those supported by the VVS. More generally, however, this is possible because these methods directly interface with experimental data—making predictions of VVS-supported performance directly from experimental stimuli, instead of relying on the discretion of experimentalists. This stimulus-computable property of these models provides a formal ‘linking function’ between theoretical claims with experimental evidence. In turn, this modeling approach creates a unified metric space (in this case, ‘model performance’) that enables us to evaluate experimental outcomes across labs, across studies, and even across species. We believe that a judicious application of these computational tools, alongside a careful consideration of animal behavior, will enrich the next generation of empirical studies surrounding MTL-dependent perceptual processing.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Evaluating model and VVS-supported performance</title><p>We begin with a task-optimized convolutional neural network, pretrained to perform object classification. We estimate the correspondence between this model and electrophysiological responses from high-level visual cortex using a protocol previously reported in <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>. We summarize this protocol here, but refer to the previous manuscript for a more detailed account. Using previously collected electrophysiological responses from macaque VVS (<xref ref-type="bibr" rid="bib29">Majaj et al., 2015</xref>), we identify a model layer that best fits high-level visual cortex: Given a set of images, we learn a linear mapping between model responses and a single electrode’s responses, then evaluate this mapping using independent data. For each model layer, this analysis yields a median cross-validated fit to noise-corrected neural responses, for both V4 and IT. As is consistently reported (<xref ref-type="bibr" rid="bib34">Rajalingham et al., 2018</xref>; <xref ref-type="bibr" rid="bib40">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Schrimpf et al., 2020</xref>), early model layers (i.e. first half of layers) better predict neural responses in V4 than do later layers (unpaired <italic>t</italic>-test: <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>2.70</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.015</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>: left, gray), while later layers better predict neural responses in IT, a higher-level region (unpaired <italic>t</italic>-test: <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>3.70</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>0.002</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; <xref ref-type="fig" rid="fig1">Figure 1b</xref>: left, green). Peak V4 fits occur in model layer pool3 (noise-corrected <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.95</mml:mn><mml:mo>±</mml:mo><mml:mn>0.30</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> STD) while peak IT fits occur in con5_1 (noise-corrected <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mo>=</mml:mo><mml:mn>0.88</mml:mn><mml:mo>±</mml:mo><mml:mn>0.16</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> STD).</p><p>Next we compare model performance with VVS-supported performance: Instead of fitting model responses directly to electrophysiological recordings in high-level visual cortex, as above, we evaluate the similarity between the performance supported by the model and high-level visual cortex. For this comparison, we again use electrophysiological responses previously collected from macaque IT cortex (<xref ref-type="bibr" rid="bib29">Majaj et al., 2015</xref>), using a protocol detailed in <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>. We independently estimate model and VVS-supported performance on stimulus set composed of concurrent visual discrimination trials, using a modified leave-one-out cross-validation strategy. We then determine the model-VVS fit over the performance estimates, as developed in <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>. Each concurrent visual discrimination trial is composed of three images: two images contain the same object<sub><italic>i</italic></sub>, randomly rotated and projected onto an artificial background; the other image (the ‘oddity’) contains a second object<sub><italic>j</italic></sub>, again presented at a random orientation on an artificial background. For each trial, the task is to identify the oddity—that is, the object which does not have a pair—ignoring the viewpoint variation across images.</p><p>We use a modified leave-one-out cross-validation strategy to estimate model performance across stimuli in this experiment. For a given sample<sub><italic>ij</italic></sub> trial, we construct a random combination of three-way oddity tasks to be used as training data; we sample without replacement from the pool of all images of object<sub><italic>i</italic></sub> and object<sub><italic>j</italic></sub>, excluding only those three stimuli that were present in sample<sub><italic>ij</italic></sub>. This yields ‘pseudo oddity experiments’ where each trial contains two typical objects and one oddity that have the same identity as the objects in sample<sub><italic>ij</italic></sub> and are randomly configured (different viewpoints, different backgrounds, different orders). These ‘pseudo oddity experiments’ are used as training data. We reshape all images, present them to the model independently, and extract model responses from an ‘IT-like’ model layer (in this case, we use fc6 which has a similar fit to IT as conv5_1 but fewer parameters to fit in subsequent steps). From these model responses, we train an L2 regularized linear classifier to identify the oddity across all (<inline-formula><mml:math id="inf67"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>52</mml:mn></mml:mrow></mml:math></inline-formula>) trials in this permutation of pseudo oddity experiments generated for sample<sub><italic>ij</italic></sub>. After learning this weighted, linear readout, we evaluate the classifier on the model responses to sample<sub><italic>ij</italic></sub>. This results in a prediction which is binarized into a single outcome <inline-formula><mml:math id="inf68"><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn> 0</mml:mn><mml:mo>∣</mml:mo><mml:mn>1</mml:mn><mml:mo rspace="4.2pt" stretchy="false">}</mml:mo></mml:mrow></mml:math></inline-formula>, either correct or incorrect. We repeat this protocol across 100 random sample<sub><italic>ij</italic></sub>s, and average across them, resulting in a single estimate of model performance for each pair<sub><italic>ij</italic></sub>.To relate model performance with the electrophysiological data, we repeat the leave-one-out cross-validation strategy described above, but in place of the fc6 model representations we run the same protocol on the population-level neural responses from IT and V4 cortex. We perform all analyses comparing model and VVS-supported performance at the object level: for each object<sub><italic>i</italic></sub> we average the performance on this object across all oddities (i.e. object<sub><italic>j</italic></sub>, object<sub><italic>k</italic></sub>, …) resulting in a single estimate of performance on this item across all oddity tasks (<inline-formula><mml:math id="inf69"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>32</mml:mn></mml:mrow></mml:math></inline-formula>). We can compare model performance with both VVS-supported performance and PRC-intact (human) performance on these same stimuli, using data from <xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>. On this dataset, PRC-intact human behavior outperforms a linear readout of macaque IT (<xref ref-type="fig" rid="fig1">Figure 1c</xref>: <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.24</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf71"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>31</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>9.50</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), while IT significantly outperforms V4 (<inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.18</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf74"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>31</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>6.56</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>7</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>). A computational proxy for IT demonstrates the same pattern, predicting IT-supported performance (<inline-formula><mml:math id="inf76"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>.81</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">F</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>30</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>13.33</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>14</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), outperforming V4 (<inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.26</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf80"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>31</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>8.02</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>9</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>), and being outperformed by PRC-intact participants (<inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>0.16</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf83"><mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>31</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>5.38</mml:mn></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mo>=</mml:mo><mml:mn>7</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p></sec><sec id="s4-2"><title>Determining model performance</title><p>For all estimates of model performance we use a task-optimized convolutional neural network pretrained on Imagenet (<xref ref-type="bibr" rid="bib12">Deng et al., 2009</xref>). For transparency, we report the results from the first instance of this model class used to evaluate these data (<xref ref-type="bibr" rid="bib37">Simonyan and Zisserman, 2014</xref>), but note that these results hold across all model instances evaluated. We preprocess each image from <xref ref-type="bibr" rid="bib18">Eldridge et al., 2018</xref> using a standard computer vision preprocessing pipeline; resizing images to a width and height of 224 × 224, then normalizing each image by the mean ([0.485, 0.456, 0.406]) and standard deviation ([0.229, 0.224, 0.225]) of the distribution of images used to train this model. We present each preprocessed image to the model and extract responses to each image from a layer (fc6) that exhibits a high correspondence with electrophysiological responses to high-level visual cortex (<xref ref-type="bibr" rid="bib4">Bonnen et al., 2021</xref>; and see <xref ref-type="fig" rid="fig1">Figure 1b</xref>: left). For each experiment, we generate a random train–test split, using 4/5th of the data to train a linear readout (in this case, a logistic regression model). To train this linear readout from model responses, we use an L2-normed logistic regression model implemented in sklearn (<xref ref-type="bibr" rid="bib33">Pedregosa et al., 2011</xref>) to predict the binary category classification (i.e. ‘dog’ = 1, ‘cat’ = 0) for each image in the training set. Within the training set, we estimate the optimal regularization strength (‘C’ from <inline-formula><mml:math id="inf85"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> to <inline-formula><mml:math id="inf86"><mml:msup><mml:mn>10</mml:mn><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula>) for the logistic regression model through fivefold cross-validation. We then evaluate model performance on each experiment on independent data (i.e. the remaining 1/5th of stimuli). We repeat this process for 100 permutations (i.e. random 4/5th splits) of stimuli in each condition. Each iteration’s model predictions (on independent data) are plotted in <xref ref-type="fig" rid="fig2">Figure 2c</xref>.</p></sec><sec id="s4-3"><title>Consistency estimates</title><p>We estimate within- and between-subject consistency using a common protocol. For the given resolution of analysis (either morph- or image level), we require multiple presentations of the same items. For the morph-level analysis, which aggregates stimuli within ‘morph levels’ (e.g. aggregating across all stimuli that are 0% dog morphs, 10% dog morphs, etc.), all stimulus sets meet this criterion. There are, however, multiple experiments that do not contain sufficient data to perform the image-level analysis, which requires multiple presentations of each stimulus; experiment 4 contains only one presentation of each stimulus, precluding it from our consistency analyses, and experiment 3 contains only four repetitions, which is insufficient for reliable within- and between-subject consistency estimates. Thus, we restrict our consistency estimates to experiments 1 (10 repetitions per image) and 2 (8 repetitions per image).</p><p>We estimate all consistency metrics over 100 iterations of random split halves. For each iteration, across all items within a given resolution (where items can refer to either a given morph percent, for the morph-level analysis, or a given image, for the image-level analysis), we randomly split choice behavior into two random splits. In the image-level analysis, for example, for each image <italic>x</italic><sub><italic>i</italic></sub> within the set of <inline-formula><mml:math id="inf87"><mml:mi>n</mml:mi></mml:math></inline-formula> images, we randomly select half of all trials of <italic>x</italic><sub><italic>i</italic></sub> (i.e. <inline-formula><mml:math id="inf88"><mml:msub><mml:mi>x</mml:mi><mml:msub><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>), and compute the mean of this random sample (<inline-formula><mml:math id="inf89"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:msub><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>). We repeat this for all of the <inline-formula><mml:math id="inf90"><mml:mi>n</mml:mi></mml:math></inline-formula> images in this condition (i.e. generating <inline-formula><mml:math id="inf91"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:msub><mml:mn>1</mml:mn><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf92"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:msub><mml:mn>2</mml:mn><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>, …, <inline-formula><mml:math id="inf93"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:msub><mml:mi>n</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>). We repeat this procedure for the remaining half of trial on each <inline-formula><mml:math id="inf94"><mml:mi>n</mml:mi></mml:math></inline-formula> images (i.e. generating <inline-formula><mml:math id="inf95"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:msub><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf96"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:msub><mml:mn>2</mml:mn><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>, …, <inline-formula><mml:math id="inf97"><mml:msub><mml:mover accent="true"><mml:mi>x</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:msub><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:msub></mml:math></inline-formula>). Thus, we have two <inline-formula><mml:math id="inf98"><mml:mi>n</mml:mi></mml:math></inline-formula> dimensional vectors, <inline-formula><mml:math id="inf99"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf100"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>, where the element in each vector corresponds to a random half of trials drawn from all trials containing that image. We use <inline-formula><mml:math id="inf101"><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> between these vectors as a measure of fit and repeat this measure over 100 iterations, resulting in a distribution of fits.</p><p>For the between-subject consistency metrics, split halves are computed using the same protocol used for the within-subject consistency. For the between-subject analysis, however, <inline-formula><mml:math id="inf102"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> from subject<sub><italic>i</italic></sub>s choice behavior is compared to <inline-formula><mml:math id="inf103"><mml:msub><mml:mover accent="true"><mml:mi>v</mml:mi><mml:mo stretchy="false">→</mml:mo></mml:mover><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula> from subject<sub><italic>j</italic></sub>s choice behavior (i.e. we generate a random split from each subject to compare, identical to the within-subject protocol). This approach is an alternative to simply computing the fit between two subjects by aggregating over all collected data. We take this random split approach because when all data are used to compare two subjects, this analysis results in a single-point estimate of the between-subject consistency—not a distribution of values, as is the case in our protocol. This single-point estimate could overestimate the between-subject correspondence, in relation to the within-subject measure. Instead, estimating a random split for each subject and then comparing each subject’s data results in a distribution of scores, which provides a measure not only of the average subject–subject correspondence, but also a measure of the variance of the correspondence between subjects (i.e. variation over random splits). Moreover, this approach ensures that both the within- and between-subject correspondence measures are equally powered (i.e. there are not more samples used to compare between subjects, resulting in a biased estimation of between-subject correspondence).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Data curation, Supervision, Methodology, Writing – original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures conformed to the Institute of Medicine Guide for the Care and Use of Laboratory Animals and were performed under an Animal Study Protocol approved by the Animal Care and Use Committee of the National Institute of Mental Health, covered by project number: MH002032.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-84357-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All scripts used for analysis and visualization can be accessed via github at <ext-link ext-link-type="uri" xlink:href="https://github.com/tzler/eldridge_reanalysis">https://github.com/tzler/eldridge_reanalysis</ext-link> (copy archived at <xref ref-type="bibr" rid="bib5">Bonnen, 2023</xref>). All stimuli and behavioral data used in these analyses can be downloaded via Dryad at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.5061/dryad.r4xgxd2h7">https://doi.org/10.5061/dryad.r4xgxd2h7</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>T</given-names></name><name><surname>Eldridge</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Data from: Inconsistencies between human and macaque lesion data can be resolved with a stimulus-computable model of the ventral visual stream</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.r4xgxd2h7</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work is supported by the Intramural Research Program, National Institute of Mental Health, National Institutes of Health, Department of Health and Human Services (annual report number ZIAMH002032), as well as the National Institute of Neurological Disorders and Stroke of the National Institutes of Health (Award Number F99NS125816), and Stanford’s Center for Mind Brain Behavior and Technology. We thank Elizabeth Murray and Anthony Wagner for insightful conversations and suggestions on this manuscript and throughout the course of this work.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barense</surname><given-names>MD</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Graham</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The human medial temporal lobe processes Online representations of complex objects</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>2963</fpage><lpage>2974</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.05.023</pub-id><pub-id pub-id-type="pmid">17658561</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural population control via deep image synthesis</article-title><source>Science</source><volume>364</volume><elocation-id>eaav9436</elocation-id><pub-id pub-id-type="doi">10.1126/science.aav9436</pub-id><pub-id pub-id-type="pmid">31048462</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baxter</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Involvement of medial temporal lobe structures in memory and perception</article-title><source>Neuron</source><volume>61</volume><fpage>667</fpage><lpage>677</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.007</pub-id><pub-id pub-id-type="pmid">19285463</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>T</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>When the ventral visual stream is not enough: A deep learning account of medial temporal lobe involvement in perception</article-title><source>Neuron</source><volume>109</volume><fpage>2755</fpage><lpage>2766</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.06.018</pub-id><pub-id pub-id-type="pmid">34265252</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Analysis and visualization scripts</data-title><version designator="swh:1:rev:f04b8ab7a44e297bc94e9ef77c3eb20f80d16b91">swh:1:rev:f04b8ab7a44e297bc94e9ef77c3eb20f80d16b91</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:af4e8e55fbe804078f631cca8abb816b0d563819;origin=https://github.com/tzler/eldridge_reanalysis;visit=swh:1:snp:be1cd4e1f66f55f6b82e97a28e5112cb987fa079;anchor=swh:1:rev:f04b8ab7a44e297bc94e9ef77c3eb20f80d16b91">https://archive.softwareheritage.org/swh:1:dir:af4e8e55fbe804078f631cca8abb816b0d563819;origin=https://github.com/tzler/eldridge_reanalysis;visit=swh:1:snp:be1cd4e1f66f55f6b82e97a28e5112cb987fa079;anchor=swh:1:rev:f04b8ab7a44e297bc94e9ef77c3eb20f80d16b91</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buffalo</surname><given-names>EA</given-names></name><name><surname>Reber</surname><given-names>PJ</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="1998">1998a</year><article-title>The human Perirhinal cortex and recognition memory</article-title><source>Hippocampus</source><volume>8</volume><fpage>330</fpage><lpage>339</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1098-1063(1998)8:4&lt;330::AID-HIPO3&gt;3.0.CO;2-L</pub-id><pub-id pub-id-type="pmid">9744420</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buffalo</surname><given-names>EA</given-names></name><name><surname>Stefanacci</surname><given-names>L</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name><name><surname>Zola</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="1998">1998b</year><article-title>A reexamination of the concurrent discrimination learning task: the importance of anterior Inferotemporal cortex, area Te</article-title><source>Behavioral Neuroscience</source><volume>112</volume><fpage>3</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1037//0735-7044.112.1.3</pub-id><pub-id pub-id-type="pmid">9517811</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>The organization of visual object representations: a Connectionist model of effects of lesions in Perirhinal cortex</article-title><source>European Journal of Neuroscience</source><volume>15</volume><fpage>355</fpage><lpage>364</lpage><pub-id pub-id-type="doi">10.1046/j.0953-816x.2001.01850.x</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Perirhinal cortex resolves feature ambiguity in complex visual discriminations</article-title><source>European Journal of Neuroscience</source><volume>15</volume><fpage>365</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1046/j.0953-816x.2001.01851.x</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Impairments in visual discrimination after Perirhinal cortex lesions: testing ‘declarative’ vs. ‘perceptual-Mnemonic’ views of Perirhinal cortex function</article-title><source>The European Journal of Neuroscience</source><volume>17</volume><fpage>649</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1046/j.1460-9568.2003.02475.x</pub-id><pub-id pub-id-type="pmid">12581183</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perirhinal cortex and feature-ambiguous discriminations</article-title><source>Learning &amp; Memory</source><volume>13</volume><fpage>103</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1101/lm.163606</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Dong</surname><given-names>W</given-names></name><name><surname>Socher</surname><given-names>R</given-names></name><name><surname>Li</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>ImageNet: A large-scale hierarchical image database</article-title><conf-name>2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)</conf-name><conf-loc>Miami, FL</conf-loc><fpage>248</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2009.5206848</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Deza</surname><given-names>A</given-names></name><name><surname>Konkle</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Emergent Properties of Foveated Perceptual Systems</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2006.07991">https://arxiv.org/abs/2006.07991</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Cox</surname><given-names>DD</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Untangling invariant object recognition</article-title><source>Trends in Cognitive Sciences</source><volume>11</volume><fpage>333</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2007.06.010</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Zoccolan</surname><given-names>D</given-names></name><name><surname>Rust</surname><given-names>NC</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>How does the brain solve visual object recognition?</article-title><source>Neuron</source><volume>73</volume><fpage>415</fpage><lpage>434</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.01.010</pub-id><pub-id pub-id-type="pmid">22325196</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Doerig</surname><given-names>A</given-names></name><name><surname>Sommers</surname><given-names>R</given-names></name><name><surname>Seeliger</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The Neuroconnectionist Research Programme</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2209.03718">https://arxiv.org/abs/2209.03718</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name><name><surname>Cohen</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><source>From Conditioning to Conscious Recollection: Memory Systems of the Brain</source><publisher-name>Oxford University Press on Demand</publisher-name><pub-id pub-id-type="doi">10.1093/acprof:oso/9780195178043.001.0001</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldridge</surname><given-names>MA</given-names></name><name><surname>Matsumoto</surname><given-names>N</given-names></name><name><surname>Wittig</surname><given-names>JH</given-names></name><name><surname>Masseau</surname><given-names>EC</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Richmond</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptual processing in the ventral visual stream requires area Te but not Rhinal cortex</article-title><source>eLife</source><volume>7</volume><elocation-id>e36310</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.36310</pub-id><pub-id pub-id-type="pmid">30311907</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inhoff</surname><given-names>MC</given-names></name><name><surname>Heusser</surname><given-names>AC</given-names></name><name><surname>Tambini</surname><given-names>A</given-names></name><name><surname>Martin</surname><given-names>CB</given-names></name><name><surname>O’Neil</surname><given-names>EB</given-names></name><name><surname>Köhler</surname><given-names>S</given-names></name><name><surname>Meager</surname><given-names>MR</given-names></name><name><surname>Blackmon</surname><given-names>K</given-names></name><name><surname>Vazquez</surname><given-names>B</given-names></name><name><surname>Devinsky</surname><given-names>O</given-names></name><name><surname>Davachi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Understanding Perirhinal contributions to perception and memory: evidence through the lens of selective Perirhinal damage</article-title><source>Neuropsychologia</source><volume>124</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2018.12.020</pub-id><pub-id pub-id-type="pmid">30594569</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Jonnalagadda</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>WY</given-names></name><name><surname>Manjunath</surname><given-names>B</given-names></name><name><surname>Eckstein</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Foveater: Foveated Transformer for Image Classification</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2105.14173">https://arxiv.org/abs/2105.14173</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Fast Recurrent Processing via Ventral Prefrontal Cortex Is Needed by the Primate Ventral Stream for Robust Core Visual Object Recognition</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.05.10.086959</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not Unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Spoerer</surname><given-names>CJ</given-names></name><name><surname>Sörensen</surname><given-names>LKA</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Hauk</surname><given-names>O</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recurrence is required to capture the representational Dynamics of the human visual system</article-title><source>PNAS</source><volume>116</volume><fpage>21854</fpage><lpage>21863</lpage><pub-id pub-id-type="doi">10.1073/pnas.1905544116</pub-id><pub-id pub-id-type="pmid">31591217</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Knutson</surname><given-names>AR</given-names></name><name><surname>Hopkins</surname><given-names>RO</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Visual discrimination performance, memory, and medial temporal lobe function</article-title><source>PNAS</source><volume>109</volume><fpage>13106</fpage><lpage>13111</lpage><pub-id pub-id-type="doi">10.1073/pnas.1208876109</pub-id><pub-id pub-id-type="pmid">22826243</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Bear</surname><given-names>D</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>CORnet: Modeling the Neural Mechanisms of Core Object Recognition</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/408385</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LaRocque</surname><given-names>K</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>The Medial Temporal Lobe and Episodic Memory</source><publisher-name>Elsevier</publisher-name></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>ACH</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Saksida</surname><given-names>LM</given-names></name><name><surname>Epstein</surname><given-names>RA</given-names></name><name><surname>Kapur</surname><given-names>N</given-names></name><name><surname>Hodges</surname><given-names>JR</given-names></name><name><surname>Graham</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Perceptual deficits in amnesia: challenging the medial temporal lobe ‘Mnemonic’View</article-title><source>Neuropsychologia</source><volume>43</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2004.07.017</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lee</surname><given-names>ACH</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Gaffan</surname><given-names>D</given-names></name><name><surname>Emery</surname><given-names>T</given-names></name><name><surname>Hodges</surname><given-names>JR</given-names></name><name><surname>Graham</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Differentiating the roles of the hippocampus and Perirhinal cortex in processes beyond long-term declarative memory: a double dissociation in dementia</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>5198</fpage><lpage>5203</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3157-05.2006</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Majaj</surname><given-names>NJ</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Simple learned weighted sums of inferior temporal neuronal firing rates accurately predict human core object recognition performance</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>13402</fpage><lpage>13418</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5181-14.2015</pub-id><pub-id pub-id-type="pmid">26424887</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Bussey</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Perceptual–Mnemonic functions of the Perirhinal cortex</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>142</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01303-0</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>EA</given-names></name><name><surname>Richmond</surname><given-names>BJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Role of Perirhinal cortex in object perception, memory, and associations</article-title><source>Current Opinion in Neurobiology</source><volume>11</volume><fpage>188</fpage><lpage>193</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(00)00195-1</pub-id><pub-id pub-id-type="pmid">11301238</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Norman</surname><given-names>G</given-names></name><name><surname>Eacott</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Impaired object recognition with increasing levels of feature ambiguity in rats with Perirhinal cortex lesions</article-title><source>Behavioural Brain Research</source><volume>148</volume><fpage>79</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1016/s0166-4328(03)00176-1</pub-id><pub-id pub-id-type="pmid">14684250</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Scikit-learn: machine learning in python</article-title><source>Journal of Machine Learning Research</source><volume>12</volume><fpage>2825</fpage><lpage>2830</lpage></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rajalingham</surname><given-names>R</given-names></name><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>Bashivan</surname><given-names>P</given-names></name><name><surname>Kar</surname><given-names>K</given-names></name><name><surname>Schmidt</surname><given-names>K</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Large-scale, high-resolution comparison of the core visual object recognition behavior of humans, monkeys, and state-of-the-art deep artificial neural networks</article-title><source>The Journal of Neuroscience</source><volume>38</volume><fpage>7255</fpage><lpage>7269</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0388-18.2018</pub-id><pub-id pub-id-type="pmid">30006365</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>MJ</given-names></name><name><surname>Ratan Murty</surname><given-names>NA</given-names></name><name><surname>Ajemian</surname><given-names>R</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Integrative Benchmarking to advance Neurally mechanistic models of human intelligence</article-title><source>Neuron</source><volume>108</volume><fpage>413</fpage><lpage>423</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.07.040</pub-id><pub-id pub-id-type="pmid">32918861</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scoville</surname><given-names>WB</given-names></name><name><surname>Milner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1957">1957</year><article-title>Loss of recent memory after bilateral hippocampal lesions</article-title><source>Journal of Neurology, Neurosurgery, and Psychiatry</source><volume>20</volume><fpage>11</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.1136/jnnp.20.1.11</pub-id><pub-id pub-id-type="pmid">13406589</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Very Deep Convolutional Networks for Large-Scale Image Recognition</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</ext-link></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stark</surname><given-names>CEL</given-names></name><name><surname>Squire</surname><given-names>LR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Intact visual perceptual discrimination in humans in the absence of Perirhinal cortex</article-title><source>Learning &amp; Memory</source><volume>7</volume><fpage>273</fpage><lpage>278</lpage><pub-id pub-id-type="doi">10.1101/lm.35000</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suzuki</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Perception and the medial temporal lobe: evaluating the current evidence</article-title><source>Neuron</source><volume>61</volume><fpage>657</fpage><lpage>666</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.008</pub-id><pub-id pub-id-type="pmid">19285462</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-Optimized Hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhuang</surname><given-names>C</given-names></name><name><surname>Yan</surname><given-names>S</given-names></name><name><surname>Nayebi</surname><given-names>A</given-names></name><name><surname>Schrimpf</surname><given-names>M</given-names></name><name><surname>Frank</surname><given-names>MC</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name><name><surname>Yamins</surname><given-names>DLK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised neural network models of the ventral visual stream</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2014196118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2014196118</pub-id><pub-id pub-id-type="pmid">33431673</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84357.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.09.12.507636" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.12.507636"/></front-stub><body><p>This article contributes to our section on research advances which offers important follow-up information about previously published articles in <italic>eLife</italic>. This advance offers a valuable integration of work across species that contribute to an ongoing debate about the precise role of medial temporal lobe structures in processes supporting perception as well as memory. The work presented herein uses a model of the ventral visual stream to harmonize predictions across species and leads to compelling evidence for more principled predictions about when and how one might expect contributions to performance. Using this approach has allowed the authors to revise the conclusions of previous work and will likely contribute significantly to future work in this area.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84357.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Davachi</surname><given-names>Lila</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Winawer</surname><given-names>Jonathan</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0190ak572</institution-id><institution>New York University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.12.507636">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.09.12.507636v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>[Editors’ note: the authors submitted for reconsideration following the decision after peer review. What follows is the decision letter after the first round of review.]</p><p>Thank you for submitting your Research Advance &quot;Inconsistencies between human and macaque lesion data can be resolved with a stimulus-computable model of the ventral visual stream&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and a Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Jonathan Winawer (Reviewer #2).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>This article describes the application of a computational model, previously published in 2021 in Neuron, to an empirical dataset from monkeys, previously published in 2018 in eLife. The 2021 modeling paper argued that the model can be used to determine whether a particular task depends on the perirhinal cortex as opposed to being soluble using ventral visual stream structures alone. The 2018 empirical paper used a series of visual discrimination tasks in monkeys that were designed to contain high levels of 'feature ambiguity' (in which the stimuli that must be discriminated share a large proportion of overlapping features), and yet animals with rhinal cortex lesions were unimpaired, leading the authors to conclude that perirhinal cortex is not involved in the visual perception of objects. The present article revisits and revises that conclusion: when the 2018 tasks are run through the 2021 computational model, the model suggests that they should not depend on perirhinal cortex function after all, because the model of VVS function achieves the same levels of performance as both controls and PRC-lesioned animals from the 2018 paper. This leads the authors of the present study to conclude that the 2018 data are simply &quot;non-diagnostic&quot; in terms of the involvement of the perirhinal cortex in object perception.</p><p>The authors have successfully applied the computational tool from 2021 to empirical data, in exactly the way the tool was designed to be used. To the extent that the model can be accepted as a veridical proxy for primate VVS function, its conclusions can be trusted and this study provides a useful piece of information in the interpretation of often contradictory literature. However, I found the contribution to be rather modest. The results of this computational study pertain to only a single empirical study from the literature on perirhinal function (Eldridge et al, 2018). Thus, it cannot be argued that by reinterpreting this study, the current contribution resolves all controversy or even most of the controversy in the foregoing literature. The Bonnen et al. 2021 paper provided a potentially useful computational tool for evaluating the empirical literature, but using that tool to evaluate (and ultimately rule out as non-diagnostic) a single study does not seem to warrant an entire manuscript: I would expect to see a reevaluation of a much larger sample of data in order to make a significant contribution to the literature, above and beyond the paper already published in 2021. In addition, the manuscript in its current form leaves the motivations for some analyses under-specified and the methods occasionally obscure.</p><p>– The manuscript does not make a compelling argument as to why Eldridge et al. (2018) is a particularly important example of the prior literature whose reevaluation will change the interpretation of the literature as a whole.</p><p>– Considerable effort is expended on evaluating how well the model can &quot;approximate more granular subject behaviors&quot; but it is not explained why this is important, or whether it matters that the model cannot, in fact, approximate image-level subject behavior.</p><p>– The section &quot;determining model performance&quot; does not provide sufficient detail for a reader to reproduce the modeling work. The statement that &quot;we estimate the optimal regularization strength for the logistic regression model&quot; appears to be the only statement detailing how the model is trained. This is too sparse and opaque and needs expanding considerably.</p><p>– The section &quot;8.2 Consistency estimates&quot; and the caption to Figure S4 both refer to the procedure for estimating the correspondence between subject-subject or subject-model choice behaviors. But these two sections appear to contradict each other. The figure caption says that the authors generate a random split of each subject's data. But in Section 8.2, the last sentence implies (although it's not completely clear) that for the between-subjects metric, all the data from each subject is used. (And it is true that, for a between-subjects analysis, you could use all the data to compute a correlation). Please clarify exactly how the 'split' was generated and whether a split was used for all analyses including between subjects.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The goal of this paper is to use a model-based approach, developed by one of the authors and colleagues in 2021, to critically re-evaluate the claims made in a prior paper from 2018, written by the other author of this paper (and colleagues), concerning the role of perirhinal cortex in visual perception. The prior paper compared monkeys with and without lesions to the perirhinal cortex and found that their performance was indistinguishable on a difficult perceptual task (categorizing dog-cat morphs as dogs or cats). Because the performance was the same, the conclusion was that the perirhinal cortex is not needed for this task, and probably not needed for perception in general, since this task was chosen specifically to be a task that the perirhinal cortex *might* be important for. Well, the current work argues that in fact the task and stimuli were poorly chosen since the task can be accomplished by a model of the ventral visual cortex. More generally, the authors start with the logic that the perirhinal cortex gets input from the ventral visual processing stream and that if a task can be performed by the ventral visual processing stream alone, then the perirhinal cortex will add no benefit to that task. Hence to determine whether the perirhinal cortex plays a role in perception, one needs a task (and stimulus set) that cannot be done by the ventral visual cortex alone (or cannot be done at the level of monkeys or humans).</p><p>There are two important questions the authors then address. First, can their model of the ventral visual cortex perform as well as macaques (with no lesion) on this task? The answer is yes, based on the analysis of this paper. The second question is, are there any tasks that humans or monkeys can perform better than their ventral visual model? If not, then maybe the ventral visual model (and biological ventral visual processing stream) is sufficient for all recognition. The answer here too is yes, there are some tasks humans can perform better than the model. These then would be good tasks to test with a lesion approach to the perirhinal cortex. It is worth noting, though, that none of the analyses showing that humans can outperform the ventral visual model are included in this paper - the papers which showed this are cited but not discussed in detail.</p><p>Major strength:</p><p>The computational and conceptual frameworks are very valuable. The authors make a compelling case that when patients (or animals) with perirhinal lesions perform equally to those without lesions, the interpretation is ambiguous: it could be that the perirhinal cortex doesn't matter for perception in general, or it could be that it doesn't matter for this stimulus set. They now have a way to distinguish these two possibilities, at least insofar as one trusts their ventral visual model (a standard convolutional neural network). While of course, the model cannot be perfectly accurate, it is nonetheless helpful to have a concrete tool to make a first-pass reasonable guess at how to disambiguate results. Here, the authors offer a potential way forward by trying to identify the kinds of stimuli that will vs won't rely on processing beyond the ventral visual stream. The re-interpretation of the 2018 paper is pretty compelling.</p><p>Major weakness:</p><p>It is not clear that an off-the-shelf convolution neural network really is a great model of the ventral visual stream. Among other things, it lacks eccentricity-dependent scaling. It also lacks recurrence (as far as I could tell). To the authors' credit, they show detailed analysis on an image-by-image basis showing that in fine detail the model is not a good approximation of monkey choice behavior. This imposes limits on how much trust one should put in model performance as a predictor of whether the ventral visual cortex is sufficient to do a task or not. For example, suppose the authors had found that their model did more poorly than the monkeys (lesioned or not lesioned). According to their own logic, they would have, it seems, been led to the interpretation that some area outside of the ventral visual cortex (but not the perirhinal cortex) contributes to perception, when in fact it could have simply been that their model missed important aspects of ventral visual processing. That didn't happen in this paper, but it is a possible limitation of the method if one wanted to generalize it. There is work suggesting that recurrence in neural networks is essential for capturing the pattern of human behavior on some difficult perceptual judgments (e.g., Kietzmann et al 2019, PNAS). In other words, if the ventral model does not match human (or macaque) performance on some recognition task, it does not imply that an area outside the ventral stream is needed - it could just be that a better ventral model (eg with recurrence, or some other property not included in the model) is needed. This weakness pertains to the generalizability of the approach, not to the specific claims made in this paper, which appear sound.</p><p>A second issue is that the title of the paper, &quot;Inconsistencies between human and macaque lesion data can be resolved with a stimulus-computable model of the ventral visual stream&quot; does not seem to be supported by the paper. The paper challenges a conclusion about macaque lesion data. What inconsistency is reconciled, and how?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.84357.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><p>[Editors’ note: The authors appealed the original decision. What follows is the authors’ response to the first round of review.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>This article describes the application of a computational model, previously published in 2021 in Neuron, to an empirical dataset from monkeys, previously published in 2018 in eLife. The 2021 modeling paper argued that the model can be used to determine whether a particular task depends on the perirhinal cortex as opposed to being soluble using ventral visual stream structures alone. The 2018 empirical paper used a series of visual discrimination tasks in monkeys that were designed to contain high levels of 'feature ambiguity' (in which the stimuli that must be discriminated share a large proportion of overlapping features), and yet animals with rhinal cortex lesions were unimpaired, leading the authors to conclude that perirhinal cortex is not involved in the visual perception of objects. The present article revisits and revises that conclusion: when the 2018 tasks are run through the 2021 computational model, the model suggests that they should not depend on perirhinal cortex function after all, because the model of VVS function achieves the same levels of performance as both controls and PRC-lesioned animals from the 2018 paper. This leads the authors of the present study to conclude that the 2018 data are simply &quot;non-diagnostic&quot; in terms of the involvement of the perirhinal cortex in object perception.</p></disp-quote><p>We appreciate the Reviewer’s careful reading and synthesis of the background and general findings of this manuscript.</p><disp-quote content-type="editor-comment"><p>The authors have successfully applied the computational tool from 2021 to empirical data, in exactly the way the tool was designed to be used. To the extent that the model can be accepted as a veridical proxy for primate VVS function, its conclusions can be trusted and this study provides a useful piece of information in the interpretation of often contradictory literature. However, I found the contribution to be rather modest. The results of this computational study pertain to only a single empirical study from the literature on perirhinal function (Eldridge et al, 2018). Thus, it cannot be argued that by reinterpreting this study, the current contribution resolves all controversy or even most of the controversy in the foregoing literature. The Bonnen et al. 2021 paper provided a potentially useful computational tool for evaluating the empirical literature, but using that tool to evaluate (and ultimately rule out as non-diagnostic) a single study does not seem to warrant an entire manuscript: I would expect to see a reevaluation of a much larger sample of data in order to make a significant contribution to the literature, above and beyond the paper already published in 2021. In addition, the manuscript in its current form leaves the motivations for some analyses under-specified and the methods occasionally obscure.</p></disp-quote><p>We believe that our comments below outline our rationale for focusing our current analysis on data from Eldridge et al. In brief, these data provide compelling evidence against PRC involvement in perception, and are the only such data with PRC-lesioned/-intact macaques that we were able to secure the stimuli for. As such, data from Eldridge et al. provide a singular opportunity to address discrepancies between human and macaque lesion data. For this reason, we propose the current work as a Research Advance Article type, building off of a manuscript that was previously published in eLife.</p><disp-quote content-type="editor-comment"><p>– The manuscript does not make a compelling argument as to why Eldridge et al. (2018) is a particularly important example of the prior literature whose reevaluation will change the interpretation of the literature as a whole.</p></disp-quote><p>As the Reviewer implies, there are multiple visual discrimination experiments administered to PRC-lesioned and -intact monkeys. We offer two reasons here why we have chosen to focus our analyses on Eldridge et al. 2018.</p><p>First, Eldridge et al. 2018 is currently the only relevant visual discrimination experiment administered to PRC-lesioned/-intact macaques for which we have been able to secure experimental stimuli. Prior to the submitting the current manuscript, we solicited authors of the following studies used to support and <italic>refute</italic> PRC involvement in visual perception:</p><list list-type="bullet"><list-item><p>Buffalo, E. A., Ramus, S. J., Clark, R. E., Teng, E., Squire, L. R., &amp; Zola, S. M. (1999). Dissociation between the effects of damage to perirhinal cortex and area TE. Learning &amp; Memory, 6(6), 572-599.</p></list-item><list-item><p>Buckley, M. J., Booth, M. C., Rolls, E. T., &amp; Gaffan, D. (2001). Selective perceptual impairments after perirhinal cortex ablation. Journal of Neuroscience, 21(24), 9824-9836.</p></list-item><list-item><p>Bussey, T. J., Saksida, L. M., &amp; Murray, E. A. (2002). Perirhinal cortex resolves feature ambiguity in complex visual discriminations. European Journal of Neuroscience, 15(2), 365-374.</p></list-item><list-item><p>Bussey, T. J., Saksida, L. M., &amp; Murray, E. A. (2006). Perirhinal cortex and feature-ambiguous discriminations. Learning &amp; Memory, 13(2), 103-105</p></list-item><list-item><p>Bussey, T. J., Saksida, L. M., &amp; Murray, E. A. (2003). Impairments in visual discrimination after perirhinal cortex lesions: testing ‘declarative’ vs. ‘perceptual‐mnemonic’ views of perirhinal cortex function. European Journal of Neuroscience, 17(3), 649-660.</p></list-item><list-item><p>Eldridge, M. A., Matsumoto, N., Wittig, J. H., Masseau, E. C., Saunders, R. C., &amp; Richmond, B. J. (2018). Perceptual processing in the ventral visual stream requires area TE but not rhinal cortex. elife, 7, e36310.</p></list-item></list><p>After reaching out to the original authors, we were only able to secure stimuli from Buckley et al. 2001 and Eldridge et al. 2018. However, the Buckley et al. stimuli have previously been modeled/published by Bonnen, Yamins, and Wagner, 2021 (identical stimuli were later used in human PRC lesioned experiments, with consistent results). As such, we believe that modeling stimuli from Eldridge et al. provide the only novel contribution to the field.</p><p>Second, we believe that findings from Eldridge et al. provide the most incisive test of the cross-species discrepancies between human and macaque lesion studies. Currently, the human lesion data have been shown to be entirely consistent PRC involvement in visual object perception (Bonnen, Yamins, and Wagner, 2021). Unlike other macaque stimulus sets we have access to (i.e. Buckley et al. 2001) data from Eldridge et al. were used to refute accounts of PRC involvement in perception. As such, the relative impact of accounting for data in Eldridge et al. is far greater than the majority of studies present in the literature. Moreover, because the design used in Eldridge et al. is different from those used in the human lesion experiments previously modeled (including Buckley et al. 2001) this experiment provides a powerful proof of principle that this modeling framework is useful to understand PRC involvement in perception not only across species, but also across experimental designs.</p><disp-quote content-type="editor-comment"><p>– Considerable effort is expended on evaluating how well the model can &quot;approximate more granular subject behaviors&quot; but it is not explained why this is important, or whether it matters that the model cannot, in fact, approximate image-level subject behavior.</p></disp-quote><p>In order to highlight the logic to these analyses, we have provided a brief explanation in the Results section and clarified our ideas in the discussion. In sum, we hope that this modeling approach will be useful to future experimentalists, and so would like to make clear what the limitations are on predicting animal behaviors. That is, this analysis is not directly relevant to claims about perirhinal function, but more of a methodological claim about model abilities.</p><p>From the Results section:</p><p>“Our computational approach is able to predict subject-level choice behavior when aggregated across morph levels, for both PRC-intact (e.g. subject 0; R2 = 0.99 β = 1.01, t(21) = 39.30, P = 2 x 10−20) and -lesioned (e.g. subject 4: R2 = 0.99 β = 1.01, t(21) = 45.01, P = 1 x 10−21) subjects (Figure 3b). Interestingly, the model’s fit to subject behavior is indistinguishable from the distribution of between-subject reliability estimates (Figure 1a; median of the empirical P(model|reliability<sub>between-subject</sub>) = 0.592) suggesting that the model exhibits ‘subject-like’ behaviors at this resolution. Our modeling approach is also able to significantly predict image-level choice behaviors for both PRC-lesioned (e.g. subject 3: R2 = 0.86 β = 0.81, F(1, 438) = 52.79, P = 5 x 10−192) and -intact subjects (e.g. subject 1: R2 = 0.87 β = 0.88, F(1, 438) = 53.24, P = 2 x 10−193). However, the model behavior is unlikely to be observed under the distribution of between-subject reliability estimates (between-subject reliability distributions visualized in Figure 1b; median of the empirical P(model|reliability<sub>between-subject</sub>) = 0). That is, the model does not exhibit ‘subject-like’ choice behaviors at the resolution of individual images. This is an important caveat to note when evaluating the correspondence between model performance and animal behavior: as previously reported (Rajalingham et al., 2018), even as these models approximate neural responses and choice behaviors in the aggregate (i.e. across images), they do not necessarily capture the trial-by-trial choice behaviors. We elaborate on this further in the discussion.”</p><p>From the Discussion section:</p><p>“We note that there is meaningful variance in the trial-level behaviors not captured by the current modeling framework. By conducting a more granular analyses than the original study (i.e. an image-level analysis, instead of averaging across multiple images within the same morph level), we found that image-level choice behaviors are reliable both within- and between-subjects (Figure 3a). At this image-level resolution, however, the VVS model does not match the pattern of choice behaviors evident in experimental subjects (Figure 3c; Supplemental Figure 1b). This observation is consistent with previous reports (Rajalingham et al., 2018), suggesting that these VVS-like models are best suited to approximate aggregate choice behaviors, not responses to individual images. Many sources of variance have been identified as possible contributors to these subject-model divergences, such as biologically implausible training data (Zhuang et al., 2021), or lack of known properties of the primate visual system—e.g. recurrence (Kar and DiCarlo, 2020) or eccentricity-dependent scaling (Jonnalagadda et al., 2021).”</p><disp-quote content-type="editor-comment"><p>– The section &quot;determining model performance&quot; does not provide sufficient detail for a reader to reproduce the modeling work. The statement that &quot;we estimate the optimal regularization strength for the logistic regression model&quot; appears to be the only statement detailing how the model is trained. This is too sparse and opaque and needs expanding considerably.</p></disp-quote><p>We appreciate this request for clarification and agree that this section was not sufficiently clear. We have expanded our description in this section as outlined below:</p><p>“For all estimates of model performance we use a task-optimized convolutional neural network pretrained on Imagenet (Deng et al., 2009). For transparency, we report the results from the first instance of this model class used to evaluate these data (Simonyan and Zisserman, 2014), but note that these results hold across all model instances evaluated. We preprocess each image from Eldridge et al., 2018 using a standard computer vision preprocessing pipeline; resizing images to a width and height of 224x224, then normalizing each image by the mean ([0.485, 0.456, 0.406]) and standard deviation ([0.229, 0.224, 0.225]) of the distribution of images used to train this model. We present each preprocessed image to the model and extract responses to each image from a layer (fc6) that exhibits a high correspondence with electrophysiological responses to high-level visual cortex (Bonnen et al., 2021; and see Figure 1b: left). For each experiment, we generate a random train-test split, using 4/5th of the data to train a linear readout (in this case, a logistic regression model). To train this linear readout from model responses, we use a l2-normed logistic regression model implemented in sklearn (Pedregosa et al., 2011) to predict the binary category classification (i.e. ‘dog’=1, ‘cat’=0) for each image in the training set. Within the training set, we estimate the optimal regularization strength (‘C’ from 10−5 to 10−5) for the logistic regression model through 5-fold cross validation. We then evaluate model performance on each experiment on independent data (i.e. the remaining 1/5th of stimuli). We repeat this process for 100 permutations (i.e. random 4/5th splits) of stimuli in each condition. Each iteration’s model predictions (on independent data) are plotted in Figure 2c.”</p><disp-quote content-type="editor-comment"><p>– The section &quot;8.2 Consistency estimates&quot; and the caption to Figure S4 both refer to the procedure for estimating the correspondence between subject-subject or subject-model choice behaviors. But these two sections appear to contradict each other. The figure caption says that the authors generate a random split of each subject's data. But in Section 8.2, the last sentence implies (although it's not completely clear) that for the between-subjects metric, all the data from each subject is used. (And it is true that, for a between-subjects analysis, you could use all the data to compute a correlation). Please clarify exactly how the 'split' was generated and whether a split was used for all analyses including between subjects.</p></disp-quote><p>We appreciate this request for clarification and agree that this section was not sufficiently clear. We have added a few sentences to the “consistency estimates” section in order to clarify that the splits for the within- and between-subject analyses are generated in an identical manner, as well as including some of the rationale behind this decision:</p><p><italic>“</italic>We estimate within- and between-subject consistency using a common protocol. For the given resolution of analysis (either morph- or image-level), we require multiple presentations of the same items. For the morph-level analysis, which aggregates stimuli within ‘morph levels’ (e.g. all stimuli that are designed to be 0% dog, 10%, etc.), all stimulus sets meet this criterion. There are, however, multiple experiments that do not contain sufficient data to perform the image-level analysis, which requires multiple presentations of each stimulus; experiment four contains only one presentation of each stimulus, precluding it from our consistency analyses, and experiment three contains only 4 repetitions, which is insufficient for reliable within- and between-subject consistency estimates. Thus, we restrict our consistency estimates to experiments one (10 repetitions per image) and two (8 repetitions per image).</p><p>We estimate all consistency metrics over 100 iterations of random split-halves. For each iteration, across all items within a given resolution (where items can refer to either a given morph percent, for the morph-level analysis, or a given image, for the image-level analysis), we randomly split choice behavior into two random splits. In the image-level analysis, for example, for each image x<sub>i</sub> within the set of n images, we randomly select half of all trials of x<sub>i</sub> (i.e.<inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), and compute the mean of this random sample (<inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:msub><mml:mi>i</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We repeate this for all of the n images in this condition (i.e. generating <inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). We repeat this procedure for the remaining half of trial on each n images (i.e. generating <inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mn>2</mml:mn><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover><mml:mi>x</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). Thus, we have two n dimensional vectors, <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="sa2m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where the element in each vector corresponds to a random half of trials drawn from all trial containing that image. We use R<sup>2</sup> between these vectors as a measure of fit and repeat this measure over 100 iterations, resulting in a distribution of fits. For the between-subject consistency metrics, split halves are computed using the same protocol used for the within-subject consistency. For the between-subject analysis, however, <inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from subject<italic><sub>i</sub></italic>s choice behavior is compared to <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mover><mml:mi>v</mml:mi><mml:mo>→</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from subject<italic><sub>j</sub></italic>s choice behavior (i.e. we generate a random split from each subject to compare, identical to the within-subject protocol). This approach is an alternative to simply computing the fit between two subjects by aggregating over all collected data. We take this random split approach because when all data are used to compare two subjects, this analysis results in a single point estimate of the between-subject consistency—not a distribution of values, as is the case in our protocol. This single-point estimate could overestimate the between-subject correspondence, in relation to the within-subject measure. Instead, estimating a random split for each subject and then comparing each subjects’ data results in a distribution of scores, which provides a measure not only of the average subject-subject correspondence, but also a measure of the variance of the correspondence between subjects (i.e. variation over random splits). Moreover, this approach ensures that both the within- and between-subject correspondence measures are equally powered (i.e. there are not more samples used to compare between subjects, resulting in a biased estimation of between-subject correspondence).”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The goal of this paper is to use a model-based approach, developed by one of the authors and colleagues in 2021, to critically re-evaluate the claims made in a prior paper from 2018, written by the other author of this paper (and colleagues), concerning the role of perirhinal cortex in visual perception. The prior paper compared monkeys with and without lesions to the perirhinal cortex and found that their performance was indistinguishable on a difficult perceptual task (categorizing dog-cat morphs as dogs or cats). Because the performance was the same, the conclusion was that the perirhinal cortex is not needed for this task, and probably not needed for perception in general, since this task was chosen specifically to be a task that the perirhinal cortex *might* be important for. Well, the current work argues that in fact the task and stimuli were poorly chosen since the task can be accomplished by a model of the ventral visual cortex. More generally, the authors start with the logic that the perirhinal cortex gets input from the ventral visual processing stream and that if a task can be performed by the ventral visual processing stream alone, then the perirhinal cortex will add no benefit to that task. Hence to determine whether the perirhinal cortex plays a role in perception, one needs a task (and stimulus set) that cannot be done by the ventral visual cortex alone (or cannot be done at the level of monkeys or humans).</p><p>There are two important questions the authors then address. First, can their model of the ventral visual cortex perform as well as macaques (with no lesion) on this task? The answer is yes, based on the analysis of this paper. The second question is, are there any tasks that humans or monkeys can perform better than their ventral visual model? If not, then maybe the ventral visual model (and biological ventral visual processing stream) is sufficient for all recognition. The answer here too is yes, there are some tasks humans can perform better than the model. These then would be good tasks to test with a lesion approach to the perirhinal cortex. It is worth noting, though, that none of the analyses showing that humans can outperform the ventral visual model are included in this paper - the papers which showed this are cited but not discussed in detail.</p><p>Major strength:</p><p>The computational and conceptual frameworks are very valuable. The authors make a compelling case that when patients (or animals) with perirhinal lesions perform equally to those without lesions, the interpretation is ambiguous: it could be that the perirhinal cortex doesn't matter for perception in general, or it could be that it doesn't matter for this stimulus set. They now have a way to distinguish these two possibilities, at least insofar as one trusts their ventral visual model (a standard convolutional neural network). While of course, the model cannot be perfectly accurate, it is nonetheless helpful to have a concrete tool to make a first-pass reasonable guess at how to disambiguate results. Here, the authors offer a potential way forward by trying to identify the kinds of stimuli that will vs won't rely on processing beyond the ventral visual stream. The re-interpretation of the 2018 paper is pretty compelling.</p></disp-quote><p>We thank the Reviewer for the careful reading of our manuscript and for providing a fantistics synthesis of the current work.</p><disp-quote content-type="editor-comment"><p>Major weakness:</p><p>It is not clear that an off-the-shelf convolution neural network really is a great model of the ventral visual stream. Among other things, it lacks eccentricity-dependent scaling. It also lacks recurrence (as far as I could tell).</p></disp-quote><p>We agree with the Reviewer completely on this point: there is little reason to expect that off-the-shelf convolutional neural networks should predict neural responses from the ventral visual stream, for the reasons outlined above (no eccentricity-dependent scaling, no recurrence) as well as others (weight sharing is biologically implausible, as well as the data distributions and objective functions use to optimize these models). Perhaps surprisingly, these models <italic>do</italic> provide quantitatively accurate accounts of information processing throughout the VVS; while this is well established within the literature, we were careless to simply assert this as a given without providing an account of these data. We appreciate the Reviewer for making this clear and we have changed the manuscript in several critical ways in order to avoid making unsubstantiated claims in the current version. We hope that these changes also make it easier for the casual reader to appreciate the logic in our analyses.</p><p>First, in the introduction, we outline some of the prior experimental work that demonstrates how deep learning models are effective proxies for neural responses throughout the VVS. We also demonstrate this model-neural fit in the current paper using electrophysiological recordings (more on that below), but also including comments about the limitation of these models raised by the Reviewer:</p><p><italic>“</italic>In recent years, deep learning computational methods have become commonplace in the vision sciences. Remarkably, these models are able to predict neural responses throughout the primate VVS directly from experimental stimuli: given an experimental image as input, these models (e.g. convolutional neural networks, CNNs) are able to predict neural responses. These ‘stimulus-comptable’ methods currently provide the most quantitatively accurate predictions of neural responses throughout the primate VVS (Bashivan et al., 2019; Khaligh-Razavi and Kriegeskorte, 2014; Rajalingham et al., 2018; Yamins et al., 2014). For example, early model layers within a CNN better predict earlier stages of processing within the VVS (e.g. V4; Fig. 1b: left, grey) while later model layers better predict later stages of processing 50 within the VVS (e.g. IT; Fig. 1b: left, green). We note that there is not a 1-1 correspondence between these models and the primate VVS as they typically lack known biological properties (Doerig et al., 2022; Zhuang et al., 2021). Nonetheless, these models can be modified to evaluate domain-specific hypotheses (Doerig et al., 2022)—e.g. by adding recurrence (Kietzmann et al., 2019; Kubilius et al., 2018) or eccentricity-dependent scaling (Deza and Konkle, 2020; Jonnalagadda et al., 2021).”</p><p>In the introduction we also more clearly demarcate prior contributions from our recent computational work, and highlight how models approximate the performance supported by a linear readout of the VVS, but fail to reach human-level performance:</p><p><italic>“</italic>Recently, Bonnen et al., 2021 leveraged these ‘VVS-like’ models to evaluate the performance of PRC- intact/-lesioned human participants in visual discrimination tasks. While VVS-like models are able to approximate performance supported by a linear readout of high-level visual cortex (Fig. 1b: right, green), human participants are able to out outperform both VVS-like models and a linear readout of direct electrophysiological recordings from the VVS (Fig. 1b: right, purple). Critically, VVS-like models approximate PRC-lesioned performance. While these data implicate PRC in visual object processing,…”</p><p>Results from these analyses were essential to understanding the logic of the paper but previously (as noted by the Reviewer) this critical evidence was cited but not directly presented. We include a description to these we describe these data in the introduction more thoroughly, and substantially change figure one, in order to visualize these data (b)</p><p>Moreover, we include a over of the methods and data used to generate these plots in the results and methods sections (only showing the results (lines 85-112), for brevity):</p><p>While there is little reason to expect that off-the-shelf convolutional neural networks should predict neural responses from the ventral visual stream, we believe that these modifications to the manuscript (to the introduction and figure one, as well as the results and methods sections) make clear that these models are nonetheless useful methods for predicting VVS responses and the behaviors that depend on the VVS.</p><disp-quote content-type="editor-comment"><p>To the authors' credit, they show detailed analysis on an image-by-image basis showing that in fine detail the model is not a good approximation of monkey choice behavior. This imposes limits on how much trust one should put in model performance as a predictor of whether the ventral visual cortex is sufficient to do a task or not. For example, suppose the authors had found that their model did more poorly than the monkeys (lesioned or not lesioned). According to their own logic, they would have, it seems, been led to the interpretation that some area outside of the ventral visual cortex (but not the perirhinal cortex) contributes to perception, when in fact it could have simply been that their model missed important aspects of ventral visual processing. That didn't happen in this paper, but it is a possible limitation of the method if one wanted to generalize it. There is work suggesting that recurrence in neural networks is essential for capturing the pattern of human behavior on some difficult perceptual judgments (e.g., Kietzmann et al 2019, PNAS). In other words, if the ventral model does not match human (or macaque) performance on some recognition task, it does not imply that an area outside the ventral stream is needed - it could just be that a better ventral model (eg with recurrence, or some other property not included in the model) is needed. This weakness pertains to the generalizability of the approach, not to the specific claims made in this paper, which appear sound.</p></disp-quote><p>We could not agree more with the Reviewer on these points. It <italic>could have</italic> been the case that these models' lack of correspondence with known biological properties (e.g. recurrence) led them to lack something important about VVS-supported performance, and that this would derail the entire modeling effort here. Surprisingly, this has not been the case, as is evident in the clear correspondence between model performance and monkey data in Eldridge et al. 2018. Nonetheless, we would expect that other experimental paradigms should be able to reveal these model failings. And future work evaluating PRC involvement in perception must contend with this very problem in order to move forward with this modeling framework. That is, it is of critical importance that these VVS models and the VVS itself exhibit similar failure modes, otherwise it is not possible to use these models to isolate behaviors that may depend on PRC.</p><disp-quote content-type="editor-comment"><p>A second issue is that the title of the paper, &quot;Inconsistencies between human and macaque lesion data can be resolved with a stimulus-computable model of the ventral visual stream&quot; does not seem to be supported by the paper. The paper challenges a conclusion about macaque lesion data. What inconsistency is reconciled, and how?</p></disp-quote><p>It appears that this point was lost in the original manuscript; we have tried to clarify this idea in both the abstract and the introduction. In summary, the cumulative evidence from the human lesion data suggest that PRC is involved in visual object perception, while there are still studies in the monkey literature that suggest otherwise (e.g. Eldridge et al. 2018). In this manuscript, we suggest that this apparent inconsistency is, in fact, simply a consequence of reliance on information interpretations of the monkey lesion data.</p><p>We have made substantive changes to the abstract so this is an obvious, central claim.</p><p>We have also made substantive changes to the introduction to make resolving this cross-species discrepancy a more central aim of the current manuscript, (lines 56-83)</p></body></sub-article></article>