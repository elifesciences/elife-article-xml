<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">86464</article-id><article-id pub-id-type="doi">10.7554/eLife.86464</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.86464.2</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Courtship behaviour reveals temporal regularity is a critical social cue in mouse communication</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-305110"><name><surname>Perrodin</surname><given-names>Catherine</given-names></name><email>catherine.perrodin@alumni.epfl.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-305111"><name><surname>Verzat</surname><given-names>Colombine</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-61675"><name><surname>Bendor</surname><given-names>Daniel</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6621-793X</contrib-id><email>d.bendor@ucl.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>Institute of Behavioural Neuroscience, Department of Experimental Psychology, University College London</institution></institution-wrap><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05932h694</institution-id><institution>Idiap Research Institute</institution></institution-wrap><addr-line><named-content content-type="city">Martigny</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02feahw73</institution-id><institution>CNRS</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Shinn-Cunningham</surname><given-names>Barbara G</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05x2bcf33</institution-id><institution>Carnegie Mellon University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>27</day><month>12</month><year>2023</year></pub-date><volume>12</volume><elocation-id>RP86464</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-02-12"><day>12</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2020-09-02"><day>02</day><month>09</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.01.28.922773"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-04-18"><day>18</day><month>04</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.86464.1"/></event></pub-history><permissions><copyright-statement>© 2023, Perrodin et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Perrodin et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-86464-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-86464-figures-v1.pdf"/><abstract><p>While animals navigating the real world face a barrage of sensory input, their brains evolved to perceptually compress multidimensional information by selectively extracting the features relevant for survival. Notably, communication signals supporting social interactions in several mammalian species consist of acoustically complex sequences of vocalisations. However, little is known about what information listeners extract from such time-varying sensory streams. Here, we utilise female mice’s natural behavioural response to male courtship songs to identify the relevant acoustic dimensions used in their social decisions. We found that females were highly sensitive to disruptions of song temporal regularity and preferentially approached playbacks of intact over rhythmically irregular versions of male songs. In contrast, female behaviour was invariant to manipulations affecting the songs’ sequential organisation or the spectro-temporal structure of individual syllables. The results reveal temporal regularity as a key acoustic cue extracted by mammalian listeners from complex vocal sequences during goal-directed social behaviour.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>courtship</kwd><kwd>vocalisation</kwd><kwd>acoustic</kwd><kwd>approach behaviour</kwd><kwd>auditory</kwd><kwd>temporal regularity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Mouse</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>P2SKP3_158691</award-id><principal-award-recipient><name><surname>Perrodin</surname><given-names>Catherine</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100010269</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id award-id-type="doi">10.35802/110238</award-id><principal-award-recipient><name><surname>Perrodin</surname><given-names>Catherine</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000265</institution-id><institution>Medical Research Council</institution></institution-wrap></funding-source><award-id>MR/M002889/1</award-id><principal-award-recipient><name><surname>Bendor</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>RG130622</award-id><principal-award-recipient><name><surname>Bendor</surname><given-names>Daniel</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication. For the purpose of Open Access, the authors have applied a CC BY public copyright license to any Author Accepted Manuscript version arising from this submission.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A behavioural paradigm demonstrates that during courtship song preference by female mice relies on the temporal regularity of the male's production of song syllables.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Animals behaving in the real world need to efficiently discriminate and monitor relevant sensory input, such as information communicated by conspecifics, from a constantly evolving, complex, and multidimensional barrage of stimulation. In many social species, including humans, vocal communication takes the form of time-varying sequences of acoustic elements and demands a rapid, socially appropriate behavioural response. Yet how the listener’s brain effectively extracts salient information from acoustically complex vocal patterns during social goal-directed behaviour remains unclear. Here, we asked which acoustic cues from communication sequences were informative to female mouse listeners when responding to male courtship songs.</p><p>Mice, like most other mammals, use complex acoustic patterns to communicate with each other in a number of different social contexts (<xref ref-type="bibr" rid="bib49">Portfors, 2007</xref>; <xref ref-type="bibr" rid="bib38">Liu et al., 2003</xref>). In particular, adult males produce rhythmic sequences of discrete, frequency-modulated pure tone elements in response to sensing the recent presence of a fertile female (<xref ref-type="bibr" rid="bib31">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib9">Chabout et al., 2012</xref>; <xref ref-type="bibr" rid="bib11">Chabout et al., 2017</xref>). These courtship songs are attractive to sexually receptive females (<xref ref-type="bibr" rid="bib48">Pomerantz et al., 1983</xref>), who respond to song playback with approach behaviour (<xref ref-type="bibr" rid="bib2">Asaba et al., 2014b</xref>; <xref ref-type="bibr" rid="bib54">Shepard and Liu, 2011</xref>; <xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Musolf et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Hammerschmidt et al., 2009</xref>; <xref ref-type="bibr" rid="bib3">Asaba et al., 2017</xref>). Male songs facilitate copulatory success (<xref ref-type="bibr" rid="bib45">Nomoto et al., 2018</xref>) and are thought to serve as fitness displays (<xref ref-type="bibr" rid="bib14">Egnor and Seagraves, 2016</xref>; <xref ref-type="bibr" rid="bib29">Hoffmann et al., 2012</xref>).</p><p>Female mice are able to detect and use acoustic information in male vocalisations. On the one hand, reinforcement learning experiments have shown that mouse listeners are able to detect and report spectro-temporal differences between individual ultrasonic call elements (<xref ref-type="bibr" rid="bib44">Neilans et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Holfoth et al., 2014</xref>), indicating that mice are <italic>perceptually</italic> sensitive to a wide range of acoustic features in the vocalisations. On the other hand, experiments using ethologically relevant place preference paradigms without external reward have shown that females use, or are <italic>behaviourally</italic> sensitive to, certain acoustic characteristics in male songs to guide social decisions, such as discriminating between social contexts (<xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>), singer species (<xref ref-type="bibr" rid="bib43">Musolf et al., 2015</xref>), strain (<xref ref-type="bibr" rid="bib1">Asaba et al., 2014a</xref>; <xref ref-type="bibr" rid="bib57">Sugimoto et al., 2011</xref>), and kin (<xref ref-type="bibr" rid="bib42">Musolf et al., 2010</xref>). However, which of the many acoustic cues available in male courtship songs are used by motivated female listeners during natural behaviour remains unclear.</p><p>In this study, we asked how listeners process and use complex vocal sequences during natural behaviour. We exploited female mice’s natural behavioural response to playbacks of male courtship songs in order to independently manipulate and identify which of the several candidate acoustic features are monitored by the listeners. We found that female approach behaviour was highly sensitive to disruptions of song temporal regularity, but that it was not affected by global manipulations of the songs’ sequential structure, or the local removal of syllable spectro-temporal dynamics. The results highlight temporal regularity as a key social cue monitored by female listeners during goal-directed social behaviour.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Acoustic characteristics of vocal sequences emitted by male C57Bl/6 mice</title><p>In order to study female listeners’ behavioural response to vocal sequences, we first collected a database of male C57Bl/6 mice vocalisations for acoustic stimulation in playback experiments. Following a previously published protocol (<xref ref-type="bibr" rid="bib11">Chabout et al., 2017</xref>), we obtained audio recordings of ultrasonic vocalisations emitted by males in response to the presentation of mixtures of urine samples collected from conspecific females in oestrus. This paradigm was deemed optimal for generating stimuli to use in ethologically inspired playback experiments as vocalisations emitted in this context are emitted by solitary males and reflect long-range acoustic ‘courtship’ displays with the purpose of attracting a female (<xref ref-type="bibr" rid="bib45">Nomoto et al., 2018</xref>; <xref ref-type="bibr" rid="bib50">Portfors and Perkel, 2014</xref>; <xref ref-type="bibr" rid="bib41">Matsumoto and Okanoya, 2016</xref>). The vocal patterns emitted in this context consisted of sequences of individual frequency-modulated calls (e.g. see <xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>; <xref ref-type="bibr" rid="bib31">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib60">Tschida et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Hage et al., 2013</xref>; <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref>). Across the set of recorded male vocalisations, the duration of continuous call elements, or syllables, followed a bimodal distribution, with a majority of short syllables (local maximum = 23 ms, 86% [2039/2373] of recorded syllables with durations shorter than 63 ms [local minimum]) and a minority of long syllables (local maximum = 88 ms, 14% [334/2373] of recorded syllables with durations longer than 63 ms, <xref ref-type="fig" rid="fig1">Figure 1B</xref>). These male calls were restricted to the ultrasonic frequency range, with the maximal energy in individual syllables occurring around 76 kHz (median peak frequency and 95% confidence interval [CI], 76 ± 0.3 kHz, n = 2373 syllables) and spanning a bandwidth of 16 kHz ([68–84 kHz]; <xref ref-type="fig" rid="fig1">Figure 1C</xref>). Individual syllables were organised in temporally regular sequences and often started at an approximately constant time delay from each other (<xref ref-type="bibr" rid="bib31">Holy and Guo, 2005</xref>; <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref>), as evidenced by the narrow concentration of inter-syllable interval durations around 104 ± 2 ms (median inter-syllable interval and 95% CI; interquartile range of syllable interval = 61 ms, n = 2322 syllables followed by another syllable within 2 s; <xref ref-type="fig" rid="fig1">Figure 1D</xref>). From this set of ultrasonic vocalisations, we selected a smaller, representative sample for use in subsequent playback experiments (n = 957 syllables, dark grey histograms in <xref ref-type="fig" rid="fig1">Figure 1B–D</xref>). This stimulus set was composed of seven distinct songs produced by C57Bl/6 male mice aged 22 wk on average (median age, range [10–27 wk]) in response to the presentation of female urine. The songs varied in duration, lasting on average 33.4 s (median duration, range [13–41 s]), and were composed of an average of 145 syllables (median, range [95–186]).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Acoustic features of C57Bl/6 mouse courtship songs.</title><p>(<bold>A</bold>) Spectrogram of a segment of ultrasonic male vocalisations used for stimulation. (<bold>B</bold>) Distribution of individual ultrasonic syllable durations across a large set of male mouse vocalisations (n = 2373 syllables, light grey histogram; red line is smoothed distribution), including the stimulus set used in subsequent playback experiments (n = 957 syllables, dark grey), emitted in response to the presentation of urine samples from females in oestrus. (<bold>C</bold>) Distribution of syllable peak frequency (point of maximum amplitude across the call element in kHz) across all recorded syllables (n = 2373,, light grey), and the subset of syllables used for playback (n = 957, dark grey). (<bold>D</bold>) Distribution of inter-syllable interval durations. The analysis was restricted to syllables with a subsequent syllable starting within 2 s (all recorded syllables: n = 2322, light grey; playback stimulus set: n = 942, dark grey).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86464-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>Female mice preferentially approach playbacks of male songs over silence</title><p>Using our pre-recorded subset of male vocal sequences for stimulation, we then aimed to confirm that female mice displayed a preferential approach response to male songs in our ethologically inspired behavioural paradigm. We used a place preference assay to evaluate female listeners’ behaviour in response to the playback of our pre-recorded song set in a two-compartment behavioural chamber (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). Importantly, olfactory stimulation from mixed male bedding samples placed under each of the loudspeakers was present throughout the testing session in order to increase sexual arousal and motivation in the female listeners (<xref ref-type="bibr" rid="bib27">Hammerschmidt et al., 2009</xref>; <xref ref-type="bibr" rid="bib1">Asaba et al., 2014a</xref>) and create a multisensory coherent approximation of a natural social situation. Following a silent habituation period during which the animal was free to explore the behavioural box (at least 10 min; grey traces in <xref ref-type="fig" rid="fig2">Figure 2B and C</xref>), one-sided playback of male vocalisations was initiated while the second speaker remained inactive (red traces in <xref ref-type="fig" rid="fig2">Figure 2B and C</xref>). The pre-recorded set of seven songs was repeated four times during one behavioural session, with the order of individual songs randomised in each trial, and the playback side alternating between trials (<xref ref-type="fig" rid="fig2">Figure 2B and C</xref>). A female mouse’s approach response to song playback was measured by quantifying the time the animal spent within a ‘speaker zone’ over the course of a behavioural session (white [grey] trapezoidal outlines in <xref ref-type="fig" rid="fig2">Figure 2A and C</xref>, respectively). Young female C57Bl/6 mice (aged 5–11 wk; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>) participated once in the playback experiment, while in the fertile (proestrus or oestrus) stage of their oestrous cycle (as assessed by vaginal cytology, see ‘Methods’).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Female mice preferentially approach playbacks of male songs over silence.</title><p>(<bold>A</bold>) Video frame showing the testing box with a soundproof partition (middle) positioned between two ultrasonic loudspeakers. White outlines show the two ‘speaker zones’ used as regions of interest for quantifying the animal’s position. (<bold>B</bold>) Experimental timeline. (<bold>C</bold>) Tracking of the animal’s position during an example behavioural session, in which a silent baseline period (leftmost panel) is followed by the playback of intact songs from one side contrasted with silence from the other side (coloured panels). Colour saturation indicates time since start of the experimental period of interest. Dark grey outlines indicate the speaker zones. (<bold>D</bold>) Time the animal spent in the speaker zones during the silent baseline period in (<bold>C</bold>). (<bold>E</bold>) Index quantifying the animal’s relative place preference to the left vs right speaker zones during the silent baseline. This index was computed as the difference between the time spent in either speaker zones, normalised by the sum of the time in both speaker zones (see ‘Methods’). A preference index value close to zero indicates no preference to either side. (<bold>F</bold>) Time spent in the speaker zones corresponding to song playback and silence during each song presentation trial (coloured circles). The black circle shows the median of the four sound presentation trials. Error bars: standard error of the mean. (<bold>G</bold>) Preference index in response to the playback of intact songs compared to silence over the course of the example session. The black circle shows the median of the four sound presentation trials. The preference index is the difference between the time in the song playback and the silent speaker zones, divided by the sum of the time in both speaker zones. A positive index value reflects the animal’s preferential approach to the sources of song playback over silence. (<bold>H</bold>) Population summary of female approach response to playback of intact male songs (positive values) over silence (negative). Each circle is the preference index displayed by individual animals in one behavioural session (median of four sound presentation trials). The red circle corresponds to the example session in (<bold>C</bold>) and (<bold>G</bold>). Open circles: sound playback at 58 dB SPL, filled circles: 68 dB SPL. Bar plot shows mean preference index across sessions and 95% confidence interval (CI). One-sample two-tailed <italic>t</italic>-test, **p&lt;0.01. (<bold>I</bold>) Temporal profile of approach behaviour over the four sound presentation trials in the example session in (<bold>C</bold>), calculated as the cumulative sum of time in the intact song playback (positively weighted) vs silent (negatively weighted) speaker zone. (<bold>J</bold>) Trial-averaged profile of approach behaviour to song playback in the example session, calculated as in (<bold>I</bold>). (<bold>K</bold>) Population-averaged approach behaviour time course in response to intact mouse songs vs silence, calculated as in (<bold>J</bold>). The dark grey trace indicates the mean of trial-based temporal profiles across all sessions (n = 29). For each session, the median of four sound presentation trials (e.g. black trace in <bold>J</bold>) was normalised to its maximal amplitude. The horizontal black bar indicates time bins during the course of a sound playback trial in which the cumulative approach behaviour significantly deviates from zero (one-sample two-tailed <italic>t</italic>-test). (<bold>L</bold>) Normalised temporal profiles of approach behaviour to mouse songs vs silence over the course of four sound presentation trials (x-axis, coloured bars) for each of the behavioural sessions (y-axis, each animal is one line, n = 29), calculated as in (<bold>I</bold>). Sessions (lines) are ordered by the amplitude of their last element. (<bold>F, G, I, J</bold>) Black traces indicate the session average (median) across the four sound presentation trials (coloured traces).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86464-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>No difference in listener age across playback experiments.</title><p>The mice participating in each type of playback experiment were of similar age (one-way ANOVA, F(6, 153) = 0.013, p=1.0). Shown is mean and 95% confidence interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86464-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Approach behaviour is consistent across varying speaker zone lengths.</title><p>(<bold>A</bold>) Tracking of the mouse’s position over the course of a behavioural session in the two-compartment behavioural box with a soundproof partition (middle) positioned between two ultrasonic loudspeakers (top). Green outlines show the two ‘speaker zones’ used as regions of interest for quantifying the animal’s position. The grey-shaded image illustrates the speaker zone used throughout the study. Additional images illustrate speaker zones with varying lengths as the distal edge is moved closer and/or further away from the speakers. (<bold>B</bold>) Population preference index used to quantify approach behaviour in response to playback of intact songs (positive values) relative to silence (negative value) for varying speaker zone lengths in panel (<bold>A</bold>). The grey shading indicates the speaker zone used throughout the study. Shown is mean and 95% confidence interval. One-sample two-tailed <italic>t</italic>-test, **p&lt;0.01, ns: non-significant, p&gt;0.05. (<bold>C</bold>) Population preference index used to quantify approach behaviour in response to intact songs (positive values) relative to sequences of randomly ordered syllables (negative value), displayed as in panel (<bold>B</bold>). (<bold>D</bold>) Population preference index used to quantify approach behaviour in response to intact songs (positive values) relative to sequences of phase-scrambled syllables (negative value). (<bold>E</bold>) Population preference index used to quantify approach behaviour in response to intact (positive values) relative to temporally irregular (negative value) songs. (<bold>F</bold>) Population preference index used to quantify approach behaviour in response to intact (positive values) relative to reversed (negative value) songs. (<bold>G</bold>) Population preference index used to quantify approach behaviour in response to intact songs (positive values) relative to sequences of pure-tone approximation of syllables (negative value). (<bold>H</bold>) Population preference index used to quantify approach behaviour in response to intact (positive values) relative to temporally super-regular (negative value) songs.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86464-fig2-figsupp2-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86464-fig2-video1.mp4" id="fig2video1"><label>Figure 2—video 1.</label><caption><title>Example video tracking of mouse approach behaviour.</title><p>Third sound presentation trial from example behavioural session (green trace in <xref ref-type="fig" rid="fig2">Figure 2C</xref>), with intact song playback from the left speaker.</p></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-86464-fig2-video2.mp4" id="fig2video2"><label>Figure 2—video 2.</label><caption><title>Example video tracking of mouse approach behaviour.</title><p>Fourth sound presentation trial from example behavioural session (green trace in <xref ref-type="fig" rid="fig2">Figure 2C</xref>), with intact song playback from the left speaker.</p></caption></media></fig-group><p>In the example session illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>, the mouse occupied both speaker zones equally during the silent baseline (<xref ref-type="fig" rid="fig1">Figure 1D</xref>) and the first sound playback trial (red trace in <xref ref-type="fig" rid="fig2">Figure 2F and I</xref>). Over the course of the three subsequent trials, the mouse displayed a consistent preferential approach towards the source of male song playback over the silent speaker, irrespective of the playback side (blue, purple, and green traces, <xref ref-type="fig" rid="fig2">Figure 2F and I</xref>, <xref ref-type="video" rid="fig2video1 fig2video2">Figure 2—videos 1 and 2</xref>). An animal’s approach behaviour over the course of an entire session can be quantified using a preference index (see ‘Methods’ and <xref ref-type="fig" rid="fig2">Figure 2E and G</xref>), which accurately captures the cumulative preferential dwell time at the song playback vs silent speaker (see correspondence between preference index values in <xref ref-type="fig" rid="fig2">Figure 2G</xref> and endpoints of trial temporal profiles in <xref ref-type="fig" rid="fig2">Figure 2J</xref>). This preference index metric was also shown by a number of previous studies to be the most sensitive readout for similar assays of female approach to male song playback (<xref ref-type="bibr" rid="bib43">Musolf et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Hammerschmidt et al., 2009</xref>; <xref ref-type="bibr" rid="bib1">Asaba et al., 2014a</xref>). In this example, the mouse preferentially approached the song playback over silence in ¾ trials, resulting in a preference index value of 25.3% (median of four trials; black dot in <xref ref-type="fig" rid="fig2">Figure 2G</xref>). The behaviour of female listeners tested in this paradigm significantly discriminated between song playback and silence (n = 29 sessions, mean preference index significantly different from 0, one-sample <italic>t</italic>-test, <italic>t</italic>(28) = 2.81, p=0.0088; <xref ref-type="fig" rid="fig2">Figure 2H</xref>), and mouse listeners were overwhelmingly attracted to the playback of our subset of male songs. The preference for song over silence was robust to different lengths of the speaker zone used to calculate the preference index (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A and B</xref>). Mouse listeners demonstrated significant approach response after as little as 16.9 s of sound playback across the four sound presentation trials (normalised cumulative time in speaker zone significantly different from 0, one-sample <italic>t</italic>-test, p&lt;0.05; <xref ref-type="fig" rid="fig2">Figure 2K</xref>), with preference further increasing over the course of song playback. The build-up of preferential approach to song was evident in the vast majority of animals, mostly sustained over a behavioural session (<xref ref-type="fig" rid="fig2">Figure 2L</xref>), and similar across the four sound presentation trials (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>). Together, these results replicate previous work by other laboratories (<xref ref-type="bibr" rid="bib2">Asaba et al., 2014b</xref>; <xref ref-type="bibr" rid="bib54">Shepard and Liu, 2011</xref>; <xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Musolf et al., 2015</xref>; <xref ref-type="bibr" rid="bib27">Hammerschmidt et al., 2009</xref>) and confirm the feasibility of recreating an ethologically driven behavioural assay of female approach response to male song playback in the controlled laboratory environment.</p></sec><sec id="s2-3"><title>Female approach behaviour is not affected by changes to global song structure</title><p>Given this proof of principle that socially motivated female listeners approach the source of male song playback, we then extended this natural behavioural paradigm to directly evaluate how listeners perceive and use acoustic features from vocal sequences. Specifically, we tested the hypothesis that, during naturalistic goal-directed behaviour in a social context, female listeners perceptually compress the high sensory dimensionality of male songs by selectively monitoring a reduced subset of meaningful acoustic features in isolation. We quantified females’ place preference in the same behavioural assay in response to the near-simultaneous playback of the pre-recorded set of male songs from one speaker and an acoustically modified version of the songs from the second speaker. Given that females preferentially approach the speaker playing the male song, differences in the relative time spent at the source of intact vs manipulated sound playback are interpreted to demonstrate the behavioural relevance of the manipulated acoustic dimension. In contrast, equal approach behaviour to both intact and manipulated song playback is interpreted as females not perceiving, or not relying on, the tested acoustic dimension in their natural behavioural response to courtship song.</p><p>In the first instance, we evaluated whether listeners would be sensitive to two types of global manipulations affecting the song structure at longer timescales. Building on previous work suggesting that female mice use syntactic information to discriminate songs produced in different social contexts (<xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>), we tested whether randomising the order of the syllables in the song would affect their approach behaviour (<xref ref-type="fig" rid="fig3">Figure 3A</xref>, left, and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>). We found that the sequential organisation of syllables in the male songs was not necessary for normal approach behaviour (mean preference index in response to intact vs randomised songs did not differ from zero, one-sample <italic>t-</italic>test, <italic>t</italic>(20) = 0.126, p=0.90; <xref ref-type="fig" rid="fig3">Figure 3A</xref>, right). This behavioural invariance to changes in the syllable order was apparent at all timepoints of song set playback (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), as well as at different speaker zone lengths (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2C</xref>), and was comparable across the four sound presentation trials (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1D</xref>). This confirms that the lack of sensitivity to syllable sequence structure we observe here was not caused by the selection of specific temporal or spatial parameters for analysis.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Female approach behaviour is not affected by changes to global song structure or the removal of syllable spectro-temporal dynamics.</title><p>(<bold>A</bold>) Female approach behaviour during simultaneous playback of intact male songs (top) and corresponding randomised syllable sequences (bottom), displayed as in <xref ref-type="fig" rid="fig2">Figure 2H</xref>. Each circle is the preference index displayed by individual animals in one behavioural session (median of four sound presentation trials). Open circles indicate sound playback at 58 dB SPL, filled circles at 68 dB SPL. Bar plot shows mean preference index across sessions, and error bar show 95% confidence interval (CI). One-sample two-tailed <italic>t</italic>-test, ns: non-significant, p&gt;0.05. (<bold>B</bold>) Population time course of approach behaviour during the playback of intact (positively weighted) vs randomised songs (negatively weighted), displayed as in <xref ref-type="fig" rid="fig2">Figure 2K</xref>. The dark grey trace indicates the population mean of trial-based temporal profiles across all sessions (n = 21 sessions). For each session, the median of four sound presentation trials (e.g. black trace in <bold>J</bold>) was normalised to the absolute value of its maximal amplitude. At no time bins during the course of a sound playback trial did the cumulative approach behaviour significantly deviate from zero (one-sample two-tailed <italic>t</italic>-test, all p&gt;0.05). (<bold>C</bold>) Playback of intact songs (top) contrasted with temporally reversed songs (bottom). (<bold>D</bold>) Typical time course of approach behaviour during the playback of intact (positively weighted) vs reversed songs (negatively weighted). (<bold>E</bold>) Playback of intact songs (top) and sequences of phase-scrambled syllables (bottom). (<bold>F</bold>) Typical time course of approach behaviour during the playback of intact (positively weighted) vs phase-scrambled syllable sequences (negatively weighted). (<bold>G</bold>) Playback of intact songs (top) contrasted with sequences of pure tones (bottom). (<bold>H</bold>) Typical time course of approach behaviour during the playback of intact (positively weighted) vs pure tone sequences (negatively weighted).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86464-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Trial-based temporal profiles of approach behaviour.</title><p>(<bold>A</bold>) Time course of approach behaviour during one presentation of intact (positively weighted) vs temporally irregular (negatively weighted) mouse song (see <xref ref-type="fig" rid="fig2">Figure 2E</xref>), grouped by sound presentation trial (coloured bars), for each behavioural session (n = 24, y-axis). Each trial’s trace was normalised to the absolute value of its maximal amplitude. Within a trial, sessions are ordered by the amplitude of their last element. (<bold>B</bold>) Time course of approach behaviour during one presentation of intact mouse songs (positively weighted) vs silence (negatively weighted). (<bold>C</bold>) Time course of approach behaviour during one presentation of intact (positively weighted) vs temporally super-regular (negatively weighted) mouse song. (<bold>D</bold>) Time course of approach behaviour during one presentation of intact mouse song (positively weighted) vs sequences of randomly ordered syllables (negatively weighted). (<bold>E</bold>) Time course of approach behaviour during one presentation of intact (positively weighted) vs reversed (negatively weighted) mouse song. (<bold>F</bold>) Time course of approach behaviour during one presentation of intact mouse songs (positively weighted) vs sequences of phase-scrambled syllables (negatively weighted). (<bold>G</bold>) Time course of approach behaviour during one presentation of intact mouse songs (positively weighted) vs sequences of pure-tone approximation of syllables (negatively weighted).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86464-fig3-figsupp1-v1.tif"/></fig></fig-group><p>Next, we contrasted intact songs with a time-reversed version of the songs. While preserving the long-term spectra and range of spectral changes, this manipulation generated sounds with novel temporal features that different subsets of auditory neurons are tuned to, such as reversing the direction of frequency trajectories (<xref ref-type="bibr" rid="bib32">Issa et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Lui and Mendelson, 2003</xref>; <xref ref-type="bibr" rid="bib56">Sollini et al., 2018</xref>; <xref ref-type="bibr" rid="bib58">Tian and Rauschecker, 1998</xref>; <xref ref-type="bibr" rid="bib18">Fuzessery, 1994</xref>; <xref ref-type="bibr" rid="bib51">Razak and Fuzessery, 2006</xref>; <xref ref-type="bibr" rid="bib20">Geis and Borst, 2013</xref>; <xref ref-type="fig" rid="fig3">Figure 3C</xref>, left, and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Temporal reversal of the vocal sequence did not significantly affect females’ approach behaviour (mean preference index in response to intact vs reversed songs did not differ from zero, one-sample <italic>t-</italic>test, <italic>t</italic>(20) = –1.46, p=0.16; <xref ref-type="fig" rid="fig3">Figure 3C</xref>, right). This lack of behavioural sensitivity to song temporal reversal was present throughout sound playback in the vast majority of animals tested (<xref ref-type="fig" rid="fig3">Figure 3D</xref>), across the sound presentation trials (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1E</xref>), and for different spatial regions of interest (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Together, these observations suggest that female listeners are not relying on global cues reflecting the overall structure of male song sequences when selecting which sound source to approach.</p></sec><sec id="s2-4"><title>Female approach behaviour is robust to the removal of syllable spectro-temporal dynamics</title><p>Since auditory neurons are typically tuned to specific spectro-temporal features in the song syllables, and mice are able to use those features to discriminate between individual syllables (<xref ref-type="bibr" rid="bib44">Neilans et al., 2014</xref>; <xref ref-type="bibr" rid="bib30">Holfoth et al., 2014</xref><italic>),</italic> we next wondered whether manipulations disrupting the fine acoustic structure of individual calls in the male vocal sequences would be more easily discriminated by female listeners. We first created a phase-scrambled version of the songs that preserved the average frequency spectrum of the syllables, but removed the spectro-temporal dynamics of individual syllables and shaped each noise burst with a flat envelope (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, left, and <xref ref-type="supplementary-material" rid="supp4">Supplementary file 4</xref>). Females approached the intact and phase-scrambled song playback similarly (mean preference index in response to intact vs phase-scrambled songs did not differ from zero, one-sample <italic>t-</italic>test, <italic>t</italic>(22) = –0.27, p=0.79; <xref ref-type="fig" rid="fig3">Figure 3E</xref>, right). Second, comparing intact songs with more drastically simplified sound sequences that stripped down each syllable into a pure tone at the peak syllable frequency (76 kHz) also failed to elicit differential approach behaviour (mean preference index in response to intact vs pure tone sequences did not differ from zero, one-sample <italic>t-</italic>test, <italic>t</italic>(20) = 0.034, p=0.97; <xref ref-type="fig" rid="fig3">Figure 3G</xref> and <xref ref-type="supplementary-material" rid="supp5">Supplementary file 5</xref>). These two instances of behavioural insensitivity to changes in the syllable acoustic trajectories were robust to variations in temporal (<xref ref-type="fig" rid="fig3">Figure 3F and H</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1F and G</xref>) and spatial (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2D and G</xref>) parameters used for analysis. Together, these findings show that female behaviour was robust to the removal of fast spectro-temporal structure in the syllables, at least when sound energy was still present at the natural syllable peak frequency.</p></sec><sec id="s2-5"><title>Female approach behaviour is reduced by the disruption of song temporal regularity</title><p>Lastly, following the earlier observation that syllable onsets occur at consistent time intervals from each other across the song sequences (see <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref> and <xref ref-type="fig" rid="fig1">Figure 1D</xref>), we directly probed whether female listeners would be sensitive to manipulations of the temporal regularity in the male song. To do this, we generated a set of temporally irregular songs that consisted of the same syllable sequence with a broadened distribution of silent pauses compared to intact songs (interquartile range of syllable interval in irregular songs = 134 ms compared to 61 ms for intact songs; <xref ref-type="fig" rid="fig4">Figure 4A, B and E</xref>, left, and <xref ref-type="supplementary-material" rid="supp6">Supplementary file 6</xref>). Interestingly, females were highly sensitive to the disruption of song temporal regularity and preferentially approached intact over irregular song playback (mean preference index in response to intact vs irregular songs significantly differed from zero, one-sample <italic>t-</italic>test, <italic>t</italic>(23) = 3.43, p=0.00023, <xref ref-type="fig" rid="fig4">Figure 4E</xref>, right). The sensitivity to temporal regularity was robust and similarly strong across all sizes of speaker zones (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2E</xref>). The temporal profile of approach behaviour of a majority of animals favoured intact, regular songs over the course of a session (<xref ref-type="fig" rid="fig4">Figure 4G</xref>), and across sound presentation trials (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>). On average, mouse listeners started to show this sustained preference for regular over irregular songs after 85.4 s of sound playback (normalised cumulative time in speaker zone significantly different from 0, one-sample <italic>t</italic>-test, p&lt;0.05; <xref ref-type="fig" rid="fig4">Figure 4F</xref>). This longer latency to behavioural discrimination after song onset compared to when approaching intact song playback over silence (16.9 s, <xref ref-type="fig" rid="fig2">Figure 2K</xref>) might reflect the need for additional accumulation of evidence and temporal integration when discriminating between the two concurrent sound streams. These striking results in response to disruptions of the song temporal regularity contrast with the lack of noticeable effect of other manipulations of the song and syllable structure on female approach behaviour.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Female approach behaviour is sensitive to disruption of courtship temporal regularity.</title><p>(<bold>A</bold>) Distribution of inter-syllable interval (ISI) durations across the set of temporally irregular songs, calculated as in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. (<bold>B</bold>) Sequential relationships of ISI durations in the intact (shaded grey dots) and temporally irregular (shaded blue dots). (<bold>C</bold>) Distribution of ISI durations across the super-regular song set. (<bold>D</bold>) Sequential relationship of ISI durations in the intact (shaded grey dots) and super-regular (shaded red dots). (<bold>A–D</bold>) n = 957 syllables. (<bold>E</bold>) Female approach behaviour during simultaneous playback of intact male songs (top) and temporally irregular songs (bottom), displayed as in <xref ref-type="fig" rid="fig2">Figure 2H</xref>. Each circle indicates the preference index displayed by individual animals in one behavioural session (median of four sound presentation trials). Open circles: sound playback at 58 dB SPL, filled circles: 68 dB SPL. One-sample two-tailed <italic>t</italic>-test. **p&lt;0.01, ns: non-significant, p&gt;0.05. Bar plots show means, and error bars show 95% CI. (<bold>F</bold>) Population-averaged time course of approach behaviour in response to intact (positively weighted) vs temporally irregular (negatively weighted) mouse songs, displayed as in <xref ref-type="fig" rid="fig2">Figure 2K</xref>. The dark grey trace indicates the mean of normalised, trial-based temporal profiles across all sessions (n = 24). The black bar indicates time bins in which the cumulative approach behaviour significantly deviates from zero (one-sample two-tailed <italic>t</italic>-test). (<bold>G</bold>) Temporal profiles of approach behaviour to intact vs temporally irregular songs over the course of four sound presentation trials (x-axis, coloured bars) for each of the behavioural sessions (y-axis. n=24), displayed as in <xref ref-type="fig" rid="fig2">Figure 2L</xref>. (<bold>H</bold>) Female approach behaviour during simultaneous playback of intact songs (top) and temporally super-regular songs (bottom), displayed as in panel (<bold>E</bold>). (<bold>I</bold>) Population time course of approach behaviour to intact (positively weighted) vs temporally super-regular (negatively weighted) mouse songs, displayed as in panel (<bold>F</bold>). (<bold>J</bold>) Temporal profiles of approach behaviour to intact vs temporally super-regular songs over the course of four sound presentation trials, displayed as in panel (<bold>G</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-86464-fig4-v1.tif"/></fig><p>Finally, we asked whether female mice, being obviously attracted by stronger rhythmic regularity in the songs, would preferentially approach an artificial set of ‘super-regular’ songs over intact ones. To this end, we created a set of highly rhythmic songs in which the silent breaks were modified such that the onset of the syllables in each song occurred exactly at the dominant inter-syllable interval, effectively narrowing the distribution of inter-syllable times (interquartile range of syllable interval in super-regular songs = 24 ms compared to 61 ms for intact songs; <xref ref-type="fig" rid="fig4">Figure 4C, D and H</xref>, left, and <xref ref-type="supplementary-material" rid="supp7">Supplementary file 7</xref>). Female listeners did not show any consistent behavioural preference when the playback of intact songs was contrasted with that of super-regular songs (one-sample <italic>t</italic>-test, <italic>t</italic>(21) = 1.48, p=0.15; <xref ref-type="fig" rid="fig4">Figure 4H</xref>, right). This was reflected in the time course of approach behaviour, which showed large variability across animals over a session with no clear emerging pattern (<xref ref-type="fig" rid="fig4">Figure 4J</xref>). All four sound presentation trials elicited comparable response profiles (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). This behavioural invariance to improvements of the songs’ temporal regularity remained when tested at different timepoints during song playback (<xref ref-type="fig" rid="fig4">Figure 4I</xref>) or using different spatial parameters for analysis (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2H</xref>). Together, these results suggest that the rhythmicity of natural male songs is already sufficiently salient to reach the criterion for approach behaviour by female listeners and cannot be improved by additional regularisation of syllable onsets.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this study, we developed an ethological assay to probe the behavioural relevance of several candidate acoustic features in mouse communication. By exploiting female mice’s natural approach response to male-produced courtship songs and using competing playbacks of intact and manipulated songs, we directly tested how listeners use acoustic information from vocal sequences. The results show that female mice monitor temporal regularity in male songs, while their behaviour was unaffected by changes in the song sequential organisation or the fine acoustic structure of individual syllables. The findings highlight the selective extraction of song temporal regularity, which may be an acoustic signature of singer fitness, as an effective, potentially evolutionarily conserved behavioural strategy for the sensory compression of complex vocal sequences during goal-directed social behaviour.</p><p>On the one hand, neurons across the mouse auditory system are tuned to different spectro-temporal features in song syllables, and mouse listeners have been shown to be perceptually sensitive to, or able to detect, the acoustic manipulations of individual syllables used in the current study. The upper limit of this perceptual ability has been determined using go/no-go operant conditioning to extensively train animals to discriminate between syllables from spectro-temporally distinct categories, between intact and temporally reversed or pure tone versions (<xref ref-type="bibr" rid="bib44">Neilans et al., 2014</xref>), or between partial from whole syllables (<xref ref-type="bibr" rid="bib30">Holfoth et al., 2014</xref>). However, such externally rewarded reinforcement learning and the associated sharpening of sensory acuity and neuronal tuning likely bias and amplify the perceptual ability of a naive animal (<xref ref-type="bibr" rid="bib17">Fritz et al., 2005</xref>; <xref ref-type="bibr" rid="bib16">Fritz et al., 2003</xref>). On the other hand, social behaviour in naturalistic settings requires the selective monitoring of specific informative sensory features, while cognitively suppressing or ignoring other dimensions that may however be perceptible, as classically demonstrated in the toad’s visual system during prey capture (<xref ref-type="bibr" rid="bib62">Wachowitz and Ewert, 1996</xref>). Our work complements previous studies that quantified the upper bounds of mouse hearing sensitivity. We show that, in the context of an ethologically valid behaviour, female listeners show behavioural invariance to several acoustic features that they are, or can be trained to become, perceptually sensitive to.</p><p>We found that females’ approach to male song playback was not consistently affected by changes to the global structure of the vocal sequence, such as the temporal reversal of the song and the randomisation of the syllable order in the song. Both of these manipulations preserve the relative spectro-temporal relationships within each syllable, as well as the physical complexity and acoustic characteristics of the sequence, and tested whether the order of the syllables was informative to the listener. Indeed, songs from male mice have been shown to demonstrate some characteristic sequential features, with short syllables typically dominating the beginning of a vocal phrase (<xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Matsumoto and Okanoya, 2016</xref>; <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Castellucci et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Hertz et al., 2020</xref>). This structure would be violated both by the syllable order randomisation and the temporal reversal of the song. While mouse listeners are known to perceptually discriminate between syllable types differing in their spectro-temporal features (<xref ref-type="bibr" rid="bib44">Neilans et al., 2014</xref>), our finding that female approach behaviour was not affected by a shuffling of the syllable sequence indicates that, during courtship behaviour, females monitor the presence/absence or relative occurrence of specific syllable types, or other acoustic features of the songs, independent of the syllable order. Our result also disambiguates previous findings on mouse sensitivity to syntactic information in male songs: <xref ref-type="bibr" rid="bib10">Chabout et al., 2015</xref> previously showed, in a similar behavioural paradigm, that female listeners discriminated between syntactically simple and complex songs produced by males in different social contexts. Songs produced in the two tested social contexts differed both in the first-order syllable sequencing and in the composition of the syllable repertoire. Our finding on females’ behavioural insensitivity to syllable order thus suggests that listeners may have been primarily exploiting differences in repertoire composition to discriminate and ultimately select songs produced from a specific social context. Thus, our results show that female listeners do not strongly rely on the global structure of the syllable sequence during courtship behaviour.</p><p>When manipulating the songs at the more local level of individual syllables, we found that female listeners approached sequences of noise bursts or pure tones at the syllable peak frequency similarly to intact male songs. Our finding on female mice’s behavioural invariance to replacing syllables by pure tone sequences replicates previous work that showed female mice approaching playbacks of synthetic 70 kHz ultrasounds over silence, when these were presented from behind one of two devocalised males (<xref ref-type="bibr" rid="bib48">Pomerantz et al., 1983</xref>). In contrast, <xref ref-type="bibr" rid="bib27">Hammerschmidt et al., 2009</xref> used a place preference paradigm comparable to the one used in this study, and an artificial sequence of irregularly timed short ultrasounds that matched neither the duration or syllable rate of mouse songs. In this case, the authors showed that female mice preferentially approached male songs over pure tones. Our results resolve the apparent discrepancy between these previous studies: by showing both females’ behavioural invariance to pure tone approximations of syllables and their high sensitivity to temporal regularity, our work suggests that the preference for intact over ultrasound sequence observed in the Hammerschmidt study is driven by the co-occurring disruption in sequence temporal regularity. Thus, the evidence suggests that sound energy in and around a critical band corresponding to the syllable frequency range, displayed at the natural temporal organisation of male songs, is a sufficient approximation of male courtship songs to drive female approach behaviour. Our and others’ results on female courtship behaviour complement the body of work on maternal behaviour by lactating mothers: as long as the temporal structure of pup calls is maintained, pup retrieval behaviour is robust to the removal of most of the spectral structure of ultrasonic pup isolation calls (<xref ref-type="bibr" rid="bib15">Ehret and Haack, 1982</xref>), and three-harmonic stack approximations of sonic wriggling calls elicit normal maternal responses (<xref ref-type="bibr" rid="bib19">Gaub and Ehret, 2005</xref>). Similar importance of temporal patterns for the perception of social communication, over the fine acoustic structure of individual elements, has been demonstrated behaviourally in other animal species, including humans: as illustrated in vocoded speech, the intelligibility of speech degraded in the spectral domain remains partially preserved if it is shaped with the correct amplitude envelopes (<xref ref-type="bibr" rid="bib53">Shannon et al., 1995</xref>; <xref ref-type="bibr" rid="bib61">Van Tasell et al., 1987</xref>).</p><p>Of the subset of acoustic features tested in this study, temporal regularity in the song was the only feature we observed to be extracted by female listeners from male acoustic courtship displays. Our other tested contrasts show that several other perceptually dramatic disruptions of song features were nevertheless not impacting on the female approach behaviour as long as the temporal patterning of the songs was preserved, suggesting a high degree of saliency for temporal regularity as a socially informative cue. We found that mouse listeners preferentially approached intact over irregular courtship songs, suggesting that regularity is attractive to females in the context of vocal-based mate selection. The temporal regularity observed in mouse songs is a consequence of breathing patterns regulating the production of syllables by the emitter as individual calls are generated following the onset of exhalation (<xref ref-type="bibr" rid="bib60">Tschida et al., 2019</xref>; <xref ref-type="bibr" rid="bib8">Castellucci et al., 2018</xref>; <xref ref-type="bibr" rid="bib55">Sirotin et al., 2014</xref>). Disruption to the temporal regularity of male song, as artificially introduced in this study, may, for instance, result from irregular breathing cycles and thus ‘stuttering’ singing by the male, or by the inability to sustain bouts of vocal production of sufficient duration to elicit a salient percept of regularity in the listener. A recent study of courtship songs by Foxp2 knockout mice provided direct evidence for a link between male emitter fitness and song temporal regularity by showing songs by mutant mice contain rhythmic irregularities (<xref ref-type="bibr" rid="bib7">Castellucci et al., 2016</xref>). From the perspective of the listener, temporal predictability has been shown to facilitate auditory detection of near-threshold stimuli compared to an identical aperiodic sequence (<xref ref-type="bibr" rid="bib37">Lawrance et al., 2014</xref>). Additionally, in human listeners, speech intelligibility is disrupted by temporal jittering the sentences (<xref ref-type="bibr" rid="bib22">Ghitza and Greenberg, 2009</xref>; <xref ref-type="bibr" rid="bib47">Pichora-Fuller et al., 2007</xref>). Thus, perhaps by improving the perceptual signal-to-noise ratio of male courtship songs, temporal regularity may represent an acoustic cue signalling the fitness or suitability of the potential mating partner to the listener (<xref ref-type="bibr" rid="bib14">Egnor and Seagraves, 2016</xref>), to which we show that socially motivated listeners are highly attuned to.</p><p>Female behaviour did not appear to discriminate between intact songs and an artificial ‘super-regular’ version of the songs, suggesting that natural mouse songs might already be at ceiling in terms of the perceptual saliency of their temporal regularity, and therefore, their attractiveness to female listeners. In the somatosensory system, neuronal responses in barrel cortex responded indistinguishably to lightly temporally jittered and perfectly regular sequences of whisker deflections, when the stimulus presentation rate was slower than 20 Hz (<xref ref-type="bibr" rid="bib36">Lak et al., 2008</xref>). In a hypothesis that remains to be directly tested in the auditory system, it is thus possible that, at the natural syllable rates of mouse songs, removing the low temporal jitter present in intact songs to create super-regular songs does not significantly impact on perceptually relevant neuronal substrates.</p><p>The syllable rate in mouse songs (~7 Hz; <xref ref-type="bibr" rid="bib31">Holy and Guo, 2005</xref>) broadly matches with that of other mammalian vocal patterns such as monkey vocalisations and human speech (<xref ref-type="bibr" rid="bib21">Ghazanfar and Takahashi, 2014</xref>; <xref ref-type="bibr" rid="bib12">Chandrasekaran et al., 2009</xref>). On the one hand, this stimulus presentation rate in mouse songs is directly determined by the breathing rhythm of the singer. Given that active sensation during whisking and sniffing operates within the frequency range of both respiratory-coupled oscillations (<xref ref-type="bibr" rid="bib59">Tort et al., 2018</xref>) and the hippocampal theta rhythm in rodents (<xref ref-type="bibr" rid="bib34">Kleinfeld et al., 2006</xref>; <xref ref-type="bibr" rid="bib33">Kepecs et al., 2007</xref>; <xref ref-type="bibr" rid="bib25">Grion et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Kleinfeld et al., 2016</xref>), similar constraints may be placed on the cortical mechanisms of hearing (<xref ref-type="bibr" rid="bib52">Schroeder et al., 2010</xref>), especially during natural behaviour. This intriguing hypothesis could be addressed in future work by simultaneously tracking the listener’s hippocampal theta oscillations and sniffing behaviour during song playback. On the other hand, the syllable rate in mouse songs generally corresponds to the timescale of slow amplitude envelope in human speech, which is known to contribute to the identification of both prosodic and syllabic content (<xref ref-type="bibr" rid="bib46">Peelle and Davis, 2012</xref>). While vocal sequences such as human speech and mouse songs are quasi-periodic rather than metronomically regular signals, they both contain sufficiently salient temporal regularity to allow listeners to make predictions about the incoming signal (<xref ref-type="bibr" rid="bib46">Peelle and Davis, 2012</xref>). In particular, the comparable range of temporal regularities found in mouse and human vocal sequences corresponds to the theta frequency of cortical oscillations that have been shown to be susceptible to entrainment by streams of regularly presented stimuli (<xref ref-type="bibr" rid="bib46">Peelle and Davis, 2012</xref>; <xref ref-type="bibr" rid="bib13">Doelling et al., 2014</xref>). An influential model of speech processing proposes that cortical theta oscillations in the 4–8 Hz range entrain to the slow temporal envelope modulations in speech and may play a role in parsing continuous sensory input by coordinating neuronal excitability to optimally support the decoding of phonemes (<xref ref-type="bibr" rid="bib23">Giraud and Poeppel, 2012</xref>). Whether similar neuronal dynamics can be observed in response to courtship song sequences in the mouse auditory system, and what neuronal substrates causally support the behaviourally relevant extraction and representation of acoustic features identified in this study, remains to be determined.</p><sec id="s3-1"><title>Conclusion</title><p>In summary, we used an ethologically driven approach to evaluate the behavioural relevance of several acoustic features in mouse communication. Our results identify a key role of temporal regularity, but invariance to global and local song structure, in the goal-directed use of vocal patterns by female listeners. The findings highlight the selective monitoring of temporal regularity in communication sounds as an effective, potentially evolutionarily conserved behavioural strategy for the sensory compression of complex vocal sequences during mammalian vocal perception.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Animals</title><p>A total of 83 female C57Bl/6J inbred mice (<italic>Mus musculus</italic>, Charles River Laboratories, UK) participated in the experiments. All experimental procedures were carried out in accordance with a UK Home Office Project License approved under the United Kingdom Animals (Scientific Procedures) Act of 1986, and in compliance with international legislation governing the maintenance and use of laboratory animals in scientific experiments (European Communities Council Directive of 24 November 1986, 86/609/EEC). Animals were housed in same-sex groups of 3–5 per cage and under a reversed 12 hr light/dark cycle. Food pellets and water were provided ad libitum. Animals were tested in eight batches of 8–12 animals between September 2017 and September 2019. Mice participated in the playback experiments between the ages of 5–11 wk (mean = 9 wk; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). In order to prevent any impact of handling stress from influencing behavioural testing, the animals were regularly handled and habituated to all aspects of the behavioural protocol for at least a week before starting experiments. Tube handling (<xref ref-type="bibr" rid="bib24">Gouveia and Hurst, 2013</xref>) was used exclusively. The mice had no sexual experience, but were exposed to two different groups of males through a mesh division for 5 min each following each behavioural testing session. This allowed olfactory, auditory, and visual contact with males while preventing physical contact and intercourse, and has been suggested to help maintain female’s motivation for approach behaviour (<xref ref-type="bibr" rid="bib54">Shepard and Liu, 2011</xref>).</p></sec><sec id="s4-2"><title>Acoustic stimuli</title><p>Ultrasonic vocalisations were obtained from five male C57BL/6 mice in response to urine samples from conspecific females in oestrus, across 11 recording sessions. The urine stimulus consisted of the tip of one sterile cotton bud soaked in a mixture of freshly collected urine from at least two animals. Sounds were recorded at a sampling frequency of 250 kHz (Avisoft Bioacoustics, Germany) using a condenser ultrasonic microphone (CM16/CMPA), recording interface (UltrasoundGate 416H), and software (Avisoft Recorder version 5.2.09, all from Avisoft Bioacoustics).</p><p>The stimulus set used in the playback experiments (‘intact/original songs’) consisted of a subset of seven songs produced by three C57BL/6 male mice (median age 22 wk) over six recording sessions. The sound files were high-pass filtered above 40 kHz and were denoised using the frequency-domain noise reduction algorithm in Avisoft SASlab Pro (version 5.2.09, Avisoft Bioacoustics). Syllable detection and segmentation was verified manually. Sound files were up-sampled to 260,420 Hz with anti-aliasing for playback.</p></sec><sec id="s4-3"><title>Acoustic manipulations</title><p>An artificial set of randomised syllable sequences was created by shuffling the sequential order of the syllables within each song. Each syllable remained paired with its subsequent silent interval in order to preserve the distribution of inter-syllable intervals across the songs.</p><p>The set of temporally reversed songs was creating by playing each original song backwards, for example, flipped along the time axis such that the last sample of the last syllable was played first, and so on, until the first sample of the first syllable which was played last.</p><p>Phase-scrambled songs were created by replacing the phase of the original signal with a random phase value for each original song separately. The resulting high-frequency noise syllables were then ramped with a 0.5 ms linear rise/fall time.</p><p>Pure tone sequences were created by replacing each syllable with a pure tone of the matching duration, including a 0.5 ms linear rise/fall time. Tone frequency was selected to match the median peak frequency of all recorded syllables, for example, 76 kHz.</p><p>For the phase-scrambled and pure tone sequence sets, sound amplitude was constant across all syllables in one song, and was adjusted for each song separately in order to match the mean root mean square amplitude of syllables in the corresponding original songs.</p><p>A set of temporally irregular songs was created by replacing the silent pauses associated with each syllable (time intervals between the offset of one syllable and the onset of following syllable) with randomly generated break durations under the condition that the duration of the new song be equal to that of the original song.</p><p>A super-rhythmic version of each of the original songs was created by scaling the silent pauses between each pair of successive syllables such that every inter-syllable interval (time between the onset of one syllable and the offset of the following syllable) matched the median inter-syllable interval of a given song. For syllables that were longer than the median inter-syllable interval, the silent pause was adjusted to ‘skip a cycle’ such that the next syllable occurred at twice the median syllable interval. The total duration of each super-rhythmic song was matched to that of the corresponding original song by modifying the longer inter-bout interval (silent pause between regular sequences of syllables).</p><p>Sounds were played using MATLAB (MATLAB version R20015a; MathWorks, Natwick, MA) and a digital signal processor (RX6, Tucker-Davis Technologies, FL). Acoustic stimuli were delivered via two free-field electrostatic speakers driven (ES1, Tucker-Davis Technologies) placed at ear level, 3 cm from the mesh window at the edge of the enclosure. The frequency response of the loudspeaker was ±6 dB across the frequency range used for stimulation [68–84 kHz]. Sounds were played either at 53–59 dbSPL, or 63–70 dbSPL, as measured just inside the testing box (7 cm from the speakers). Before the start of the first behavioural session of the day, correct playback of ultrasonic stimuli was confirmed visually using the Avisoft-RECORDER software and condenser ultrasonic microphones (Avisoft CM16/CMPA, both by Avisoft Bioacoustics) placed at the edge of the behavioural box.</p></sec><sec id="s4-4"><title>Oestrous staging</title><p>Oestrous stage was assessed daily based on vaginal cytology (<xref ref-type="bibr" rid="bib5">Byers et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Caligioni, 2009</xref>). Samples were collected by flushing warmed sterile saline over the vaginal opening. The unstained cell samples collected via vaginal lavage were then visualised using bright-field microscopy (Leica DFC365FX). Oestrous stage was estimated visually based on the relative proportions of leukocyte, nucleated and cornified epithelial cells, following the oestrous cycle stage identification tool outlined in <xref ref-type="bibr" rid="bib5">Byers et al., 2012</xref>. In a subset of animals tested between October 2017 and March 2018, oestrous stage was identified based on visual examination of the vagina (<xref ref-type="bibr" rid="bib5">Byers et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Caligioni, 2009</xref>). Both oestrous staging methods required brief (10 s) tail restraint and took place daily at similar times in the morning at least 2–3 hr before a behavioural experiment in order to prevent any handling stress from impacting the behaviour.</p></sec><sec id="s4-5"><title>Behavioural testing</title><p>All behavioural tests were conducted during the dark (active) phase of the light cycle, between 12:00 and 20:00. Animals participated in the experiments once per oestrous cycle, when identified as being in oestrus or proestus. A minimum of 5 d separated successive test sessions for each mouse. Animals were tested at most once in each given sound contrast. The order of experimental contrasts each animal participated in was assigned pseudo-randomly in order to balance the ages of animals taking part in each type of behavioural tests (no difference in mean listener age across all tested contrasts, one-way ANOVA, F(6, 153) = 0.013, p=1.0; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p><p>Behavioural assays took place in a two-compartment Plexiglas behavioural box (22.7 × 25.2 × 22.5 cm) under infrared LED illumination (no visible light, <xref ref-type="fig" rid="fig2">Figure 2B</xref>). A soundproof partition partly divided the behavioural box down the middle into two ‘speaker zones’ joined by a larger ‘neutral zone’. Wall panels with a mesh insert were positioned at the end of each speaker zone, such that the mesh-covered openings were level with two speakers placed outside of the box, on either side of the partition. This allowed undistorted delivery of acoustic stimuli separately into each of the two speaker zones, while playbacks from both speakers were equally audible in the centre of the neutral zone. The behavioural box was placed inside a double-walled soundproof booth (IAC Acoustics), whose interior was covered by 4-cm-thick acoustic absorption foam (E-foam, UK). Two trays containing 2 g of a mixture of soiled bedding freshly collected from two cages of males were placed below each speaker, outside of the behavioural box. After each behavioural testing session, the wall panels were washed with soapy water and dried, and the behavioural boxed wiped down first with 70% ethanol, then with distilled water.</p><p>One behavioural session lasted for 40 min and contained a 10 min silent habituation period and four sound presentation trials (160 s of song playback) each followed by a 3 min break (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). The session started with the mouse being placed in the middle of the neutral zone using the handling cylinder. After the 10 min silent habituation period, the first playback trial was initiated manually as soon as the animal returned to the middle of the neutral zone. Each playback trial consisted of one concatenated presentation of the songs in the stimulus set (seven songs, 2 min 40 s total sound duration), played in random order. Each trial (playback of one full stimulus set) was followed by a 3 min silent break, before the next trial could be initiated.</p><p>In a ‘song vs silence’ contrast experiment, the song set was played back through one of the two speakers, while the other remained silent. In a ‘original vs manipulated song’ contrast experiment, the original song set and corresponding manipulated version of the song set were played back simultaneously, each through a different speaker. The playback onsets of individual pairs of original and manipulated songs were jittered with respect to each other by a time delay randomly sampled between 20 and 90 ms, with the leading stimulus set randomly assigned for each trial. The speaker/playback side for each sound presentation programmes was randomly assigned at the beginning of one session and alternated across the four consecutive trials within each session. This strategy allowed to distinguish, for each session, between a behavioural preference for one of the sound types despite changes in the location of the sound source, and a preference for one of the physical locations (side bias).</p></sec><sec id="s4-6"><title>Video tracking</title><p>The position of the animal was recorded under infrared illumination using an overhead camera (640 × 480 pixel resolution, PlayStation 3 Eye, Sony) that had been customised by manual removal of its infrared blocking filter. A custom image processing pipeline in Bonsai (<xref ref-type="bibr" rid="bib39">Lopes et al., 2015</xref>) detected the position of the animal against the white background and saved the x and y coordinates of the centroid of the body, as well as the corresponding timestamps (ca. 30 frames/s). On- and offset timestamps for song playback were recorded by manual key presses.</p></sec><sec id="s4-7"><title>Data analysis</title><p>Tracking data were analysed in MATLAB using custom scripts (version R2018a; MathWorks). For timestamps during which the detection of the mouse’s body failed, the x–y position was interpolated by repeating the previously available position. For each playback side, a ‘speaker zone’ was defined as the area of the behavioural box adjacent to each mesh opening in the wall panel, ranging 86 mm into each arm of the divided section of the box. The relative strength of the animal’s place preference for one stimulus over the other was captured using a preference index defined as follows: <inline-formula><mml:math id="inf1"><mml:mi>P</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mi>e</mml:mi><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>t</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>p</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>s</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi>g</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac></mml:math></inline-formula> , where <italic>t(song</italic>) is the time in seconds the animal spent in the speaker zone adjacent to the side of original song playback for this trial and <italic>t(manipulated song</italic>) is the time spent in the other speaker zone, for example, the side of manipulated song playback or silence. The preference index was computed for each sound presentation trial. The preference index for one behavioural session was calculated as the median of the four trial-based sound preference indices for that session.</p><p>Side bias across a testing session was assessed by quantifying the time in seconds an animal spent in the left and right speaker zone during each of the four sound presentation trials. Sessions in which the animal displayed a clear side bias (consistent preference for the left or right side of the behavioural box despite changes in the sound playback side, evaluated as all four trials showing a positive, or all negative, difference between the times in the left and right speaker zones) were excluded from the analysis (29% ± 7% of behavioural sessions excluded due to side bias, mean and 95% CI).</p><p>To evaluate the temporal evolution of each session’s approach behaviour, one positive (resp. negative) increment was assigned to each video frame during which the animal was inside the area (speaker zone) in front of the speaker playing back intact (resp. manipulated) songs. Frames during which the animal was outside either speaker zones were assigned a value of zero. Temporal profiles quantifying the accumulation of dwell time in either speaker zones were computed by taking the cumulative sum of these traces along the relevant trial(s) and dividing by the video frame rate to obtain times in seconds. Before comparing or pooling data across animals, each trace from an individual session or trial was normalised to the absolute value of its maximum amplitude. When displaying all traces individually, sessions were sorted by the amplitude of their last element.</p><p>All statistical analyses were performed using MATLAB (version R2018A). Data were tested for deviation from normality using Lilliefors’ composite goodness-of-fit test (MATLAB function lillietest) and Shapiro–Wilk’s parametric hypothesis test of composite normality (MATLAB function swtest) at p&lt;0.05 before using parametric tests. Significant population place preference for each contrast was tested using one-sample two-tailed <italic>t</italic>-test against zero. Unless otherwise stated, all statistical tests are two-tailed. In all figures, significance levels are indicated as follows: *significant at the 0.05 level, **significant at the 0.01 level.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Resources, Software, Investigation</p></fn><fn fn-type="con" id="con3"><p>Resources, Funding acquisition, Methodology, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>All experimental procedures and post-operative care were approved and carried out in accordance with the UK Home Office, subject to the restrictions and provisions contained within the Animal Scientific Procedures Act of 1986. Experiments were conducted under PPL P61EA6A72 (Bendor).</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-86464-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Example of intact song – from top panel in <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</title></caption><media xlink:href="elife-86464-supp1-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Example of randomised song – from bottom panel in <xref ref-type="fig" rid="fig3">Figure 3A</xref>.</title></caption><media xlink:href="elife-86464-supp2-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Example of reversed song – from bottom panel in <xref ref-type="fig" rid="fig3">Figure 3C</xref>.</title></caption><media xlink:href="elife-86464-supp3-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material><supplementary-material id="supp4"><label>Supplementary file 4.</label><caption><title>Example of phase-scrambled song – from bottom panel in <xref ref-type="fig" rid="fig3">Figure 3E</xref>.</title></caption><media xlink:href="elife-86464-supp4-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material><supplementary-material id="supp5"><label>Supplementary file 5.</label><caption><title>Example of pure tone syllables – from bottom panel in <xref ref-type="fig" rid="fig3">Figure 3G</xref>.</title></caption><media xlink:href="elife-86464-supp5-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material><supplementary-material id="supp6"><label>Supplementary file 6.</label><caption><title>Example of irregular song – from bottom panel in <xref ref-type="fig" rid="fig4">Figure 4E</xref>.</title></caption><media xlink:href="elife-86464-supp6-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material><supplementary-material id="supp7"><label>Supplementary file 7.</label><caption><title>Example of super-regular song – from bottom panel in <xref ref-type="fig" rid="fig4">Figure 4H</xref>.</title></caption><media xlink:href="elife-86464-supp7-v1.zip" mimetype="application" mime-subtype="zip"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data used to produce the figures in this study are available in figshare with the identifier (<ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.24474760">https://doi.org/10.6084/m9.figshare.24474760</ext-link>). The computer code used in this study is available at (<ext-link ext-link-type="uri" xlink:href="https://github.com/bendor-lab/Perrodin-et-al-eLife/">https://github.com/bendor-lab/Perrodin-et-al-eLife/</ext-link>, copy archived at <xref ref-type="bibr" rid="bib4">Bendor, 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Perrodin</surname><given-names>C</given-names></name><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><source>figshare</source><year iso-8601-date="2023">2023</year><data-title>Approach behaviour for female mice listening to male vocalisations (natural or experimentally manipulated)</data-title><pub-id pub-id-type="doi">10.6084/m9.figshare.24474760</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Margaux Silvestre for help with testing initial experimental designs, Camille Suess for contributing to data collection, and Jennifer Linden for feedback and support. We thank Joshua Jones-Macopson and Erich Jarvis for advice on optimising murine singing behaviour, and developing an approach behaviour paradigm. Tania Barkat, Florian Studer, and Sebastian Reinartz provided helpful comments on the manuscript. This work was supported by the Swiss National Science Foundation (P2SKP3_158691, CP) the Wellcome Trust (110238/Z/15/Z, CP), the Medical Research Council (MR/M002889/1, DB), and a Royal Society Research Grant (RG130622, DB).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaba</surname><given-names>A</given-names></name><name><surname>Hattori</surname><given-names>T</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name><name><surname>Kikusui</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Sexual attractiveness of male chemicals and vocalizations in mice</article-title><source>Frontiers in Neuroscience</source><volume>8</volume><elocation-id>231</elocation-id><pub-id pub-id-type="doi">10.3389/fnins.2014.00231</pub-id><pub-id pub-id-type="pmid">25140125</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaba</surname><given-names>A</given-names></name><name><surname>Okabe</surname><given-names>S</given-names></name><name><surname>Nagasawa</surname><given-names>M</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name><name><surname>Koshida</surname><given-names>N</given-names></name><name><surname>Osakada</surname><given-names>T</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name><name><surname>Kikusui</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Developmental social environment imprints female preference for male song in mice</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e87186</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0087186</pub-id><pub-id pub-id-type="pmid">24505280</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Asaba</surname><given-names>A</given-names></name><name><surname>Osakada</surname><given-names>T</given-names></name><name><surname>Touhara</surname><given-names>K</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name><name><surname>Kikusui</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Male mice ultrasonic vocalizations enhance female sexual approach and hypothalamic kisspeptin neuron activity</article-title><source>Hormones and Behavior</source><volume>94</volume><fpage>53</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.yhbeh.2017.06.006</pub-id><pub-id pub-id-type="pmid">28645693</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Bendor</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Perrodin-et-al-eLife</data-title><version designator="swh:1:rev:f05c64019f79f9a22a14f9c2bf59d3219fea62a9">swh:1:rev:f05c64019f79f9a22a14f9c2bf59d3219fea62a9</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:b88919cc997e1394f6a11ed3600444068587d4b0;origin=https://github.com/bendor-lab/Perrodin-et-al-eLife;visit=swh:1:snp:373a306189bd9fa5346625f1d542bda9b9556a4d;anchor=swh:1:rev:f05c64019f79f9a22a14f9c2bf59d3219fea62a9">https://archive.softwareheritage.org/swh:1:dir:b88919cc997e1394f6a11ed3600444068587d4b0;origin=https://github.com/bendor-lab/Perrodin-et-al-eLife;visit=swh:1:snp:373a306189bd9fa5346625f1d542bda9b9556a4d;anchor=swh:1:rev:f05c64019f79f9a22a14f9c2bf59d3219fea62a9</ext-link></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Byers</surname><given-names>SL</given-names></name><name><surname>Wiles</surname><given-names>MV</given-names></name><name><surname>Dunn</surname><given-names>SL</given-names></name><name><surname>Taft</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Mouse estrous cycle identification tool and images</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e35538</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0035538</pub-id><pub-id pub-id-type="pmid">22514749</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caligioni</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Assessing reproductive status/stages in mice</article-title><source>Current Protocols in Neuroscience</source><volume>Appendix 4</volume><elocation-id>Appendix</elocation-id><pub-id pub-id-type="doi">10.1002/0471142301.nsa04is48</pub-id><pub-id pub-id-type="pmid">19575469</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellucci</surname><given-names>GA</given-names></name><name><surname>McGinley</surname><given-names>MJ</given-names></name><name><surname>McCormick</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Knockout of Foxp2 disrupts vocal development in mice</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>23305</elocation-id><pub-id pub-id-type="doi">10.1038/srep23305</pub-id><pub-id pub-id-type="pmid">26980647</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Castellucci</surname><given-names>GA</given-names></name><name><surname>Calbick</surname><given-names>D</given-names></name><name><surname>McCormick</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The temporal organization of mouse ultrasonic vocalizations</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0199929</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0199929</pub-id><pub-id pub-id-type="pmid">30376572</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Serreau</surname><given-names>P</given-names></name><name><surname>Ey</surname><given-names>E</given-names></name><name><surname>Bellier</surname><given-names>L</given-names></name><name><surname>Aubin</surname><given-names>T</given-names></name><name><surname>Bourgeron</surname><given-names>T</given-names></name><name><surname>Granon</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Adult male mice emit context-specific ultrasonic vocalizations that are modulated by prior isolation or group rearing environment</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e29401</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0029401</pub-id><pub-id pub-id-type="pmid">22238608</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Sarkar</surname><given-names>A</given-names></name><name><surname>Dunson</surname><given-names>DB</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Male mice song syntax depends on social contexts and influences female preferences</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>9</volume><elocation-id>76</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2015.00076</pub-id><pub-id pub-id-type="pmid">25883559</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chabout</surname><given-names>J</given-names></name><name><surname>Jones-Macopson</surname><given-names>J</given-names></name><name><surname>Jarvis</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Eliciting and analyzing male mouse ultrasonic vocalization (USV) songs</article-title><source>Journal of Visualized Experiments</source><elocation-id>54137</elocation-id><pub-id pub-id-type="doi">10.3791/54137</pub-id><pub-id pub-id-type="pmid">28518074</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chandrasekaran</surname><given-names>C</given-names></name><name><surname>Trubanova</surname><given-names>A</given-names></name><name><surname>Stillittano</surname><given-names>S</given-names></name><name><surname>Caplier</surname><given-names>A</given-names></name><name><surname>Ghazanfar</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The natural statistics of audiovisual speech</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000436</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000436</pub-id><pub-id pub-id-type="pmid">19609344</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname><given-names>KB</given-names></name><name><surname>Arnal</surname><given-names>LH</given-names></name><name><surname>Ghitza</surname><given-names>O</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Acoustic landmarks drive delta-theta oscillations to enable speech comprehension by facilitating perceptual parsing</article-title><source>NeuroImage</source><volume>85 Pt 2</volume><fpage>761</fpage><lpage>768</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.06.035</pub-id><pub-id pub-id-type="pmid">23791839</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egnor</surname><given-names>SR</given-names></name><name><surname>Seagraves</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>The contribution of ultrasonic vocalizations to mouse courtship</article-title><source>Current Opinion in Neurobiology</source><volume>38</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2015.12.009</pub-id><pub-id pub-id-type="pmid">26789140</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ehret</surname><given-names>G</given-names></name><name><surname>Haack</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Ultrasound recognition in house mice: Key-Stimulus configuration and recognition mechanism</article-title><source>Journal of Comparative Physiology? A</source><volume>148</volume><fpage>245</fpage><lpage>251</lpage><pub-id pub-id-type="doi">10.1007/BF00619131</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>J</given-names></name><name><surname>Shamma</surname><given-names>S</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Klein</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Rapid task-related plasticity of spectrotemporal receptive fields in primary auditory cortex</article-title><source>Nature Neuroscience</source><volume>6</volume><fpage>1216</fpage><lpage>1223</lpage><pub-id pub-id-type="doi">10.1038/nn1141</pub-id><pub-id pub-id-type="pmid">14583754</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fritz</surname><given-names>JB</given-names></name><name><surname>Elhilali</surname><given-names>M</given-names></name><name><surname>Shamma</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Differential dynamic plasticity of A1 receptive fields during multiple spectral tasks</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>7623</fpage><lpage>7635</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1318-05.2005</pub-id><pub-id pub-id-type="pmid">16107649</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fuzessery</surname><given-names>ZM</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Response selectivity for multiple dimensions of frequency sweeps in the pallid bat inferior colliculus</article-title><source>Journal of Neurophysiology</source><volume>72</volume><fpage>1061</fpage><lpage>1079</lpage><pub-id pub-id-type="doi">10.1152/jn.1994.72.3.1061</pub-id><pub-id pub-id-type="pmid">7807196</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gaub</surname><given-names>S</given-names></name><name><surname>Ehret</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Grouping in auditory temporal perception and vocal production is mutually adapted: the case of wriggling calls of mice</article-title><source>Journal of Comparative Physiology. A, Neuroethology, Sensory, Neural, and Behavioral Physiology</source><volume>191</volume><fpage>1131</fpage><lpage>1135</lpage><pub-id pub-id-type="doi">10.1007/s00359-005-0036-y</pub-id><pub-id pub-id-type="pmid">16075266</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geis</surname><given-names>H</given-names></name><name><surname>Borst</surname><given-names>JGG</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Intracellular responses to frequency modulated tones in the dorsal cortex of the mouse inferior colliculus</article-title><source>Frontiers in Neural Circuits</source><volume>7</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fncir.2013.00007</pub-id><pub-id pub-id-type="pmid">23386812</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghazanfar</surname><given-names>AA</given-names></name><name><surname>Takahashi</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The evolution of speech: vision, rhythm, cooperation</article-title><source>Trends in Cognitive Sciences</source><volume>18</volume><fpage>543</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2014.06.004</pub-id><pub-id pub-id-type="pmid">25048821</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname><given-names>O</given-names></name><name><surname>Greenberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>On the possible role of brain rhythms in speech perception: intelligibility of time-compressed speech with periodic and aperiodic insertions of silence</article-title><source>Phonetica</source><volume>66</volume><fpage>113</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1159/000208934</pub-id><pub-id pub-id-type="pmid">19390234</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname><given-names>AL</given-names></name><name><surname>Poeppel</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouveia</surname><given-names>K</given-names></name><name><surname>Hurst</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Reducing mouse anxiety during handling: effect of experience with handling tunnels</article-title><source>PLOS ONE</source><volume>8</volume><elocation-id>e66401</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0066401</pub-id><pub-id pub-id-type="pmid">23840458</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grion</surname><given-names>N</given-names></name><name><surname>Akrami</surname><given-names>A</given-names></name><name><surname>Zuo</surname><given-names>Y</given-names></name><name><surname>Stella</surname><given-names>F</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Coherence between rat sensorimotor system and hippocampus is enhanced during tactile discrimination</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002384</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002384</pub-id><pub-id pub-id-type="pmid">26890254</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hage</surname><given-names>SR</given-names></name><name><surname>Gavrilov</surname><given-names>N</given-names></name><name><surname>Salomon</surname><given-names>F</given-names></name><name><surname>Stein</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Temporal vocal features suggest different call-pattern generating mechanisms in mice and bats</article-title><source>BMC Neuroscience</source><volume>14</volume><elocation-id>99</elocation-id><pub-id pub-id-type="doi">10.1186/1471-2202-14-99</pub-id><pub-id pub-id-type="pmid">24020588</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hammerschmidt</surname><given-names>K</given-names></name><name><surname>Radyushkin</surname><given-names>K</given-names></name><name><surname>Ehrenreich</surname><given-names>H</given-names></name><name><surname>Fischer</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Female mice respond to male ultrasonic “songs” with approach behaviour</article-title><source>Biology Letters</source><volume>5</volume><fpage>589</fpage><lpage>592</lpage><pub-id pub-id-type="doi">10.1098/rsbl.2009.0317</pub-id><pub-id pub-id-type="pmid">19515648</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hertz</surname><given-names>S</given-names></name><name><surname>Weiner</surname><given-names>B</given-names></name><name><surname>Perets</surname><given-names>N</given-names></name><name><surname>London</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Temporal structure of mouse courtship vocalizations facilitates syllable labeling</article-title><source>Communications Biology</source><volume>3</volume><elocation-id>333</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-020-1053-7</pub-id><pub-id pub-id-type="pmid">32591576</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hoffmann</surname><given-names>F</given-names></name><name><surname>Musolf</surname><given-names>K</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Spectrographic analyses reveal signals of individuality and kinship in the ultrasonic courtship vocalizations of wild house mice</article-title><source>Physiology &amp; Behavior</source><volume>105</volume><fpage>766</fpage><lpage>771</lpage><pub-id pub-id-type="doi">10.1016/j.physbeh.2011.10.011</pub-id><pub-id pub-id-type="pmid">22037196</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holfoth</surname><given-names>DP</given-names></name><name><surname>Neilans</surname><given-names>EG</given-names></name><name><surname>Dent</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Discrimination of partial from whole ultrasonic vocalizations using a go/no-go task in mice</article-title><source>The Journal of the Acoustical Society of America</source><volume>136</volume><elocation-id>3401</elocation-id><pub-id pub-id-type="doi">10.1121/1.4900564</pub-id><pub-id pub-id-type="pmid">25480084</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holy</surname><given-names>TE</given-names></name><name><surname>Guo</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Ultrasonic songs of male mice</article-title><source>PLOS Biology</source><volume>3</volume><elocation-id>e386</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0030386</pub-id><pub-id pub-id-type="pmid">16248680</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname><given-names>JB</given-names></name><name><surname>Haeffele</surname><given-names>BD</given-names></name><name><surname>Young</surname><given-names>ED</given-names></name><name><surname>Yue</surname><given-names>DT</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multiscale mapping of frequency sweep rate in mouse auditory cortex</article-title><source>Hearing Research</source><volume>344</volume><fpage>207</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2016.11.018</pub-id><pub-id pub-id-type="pmid">28011084</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kepecs</surname><given-names>A</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Rapid and precise control of sniffing during olfactory discrimination in rats</article-title><source>Journal of Neurophysiology</source><volume>98</volume><fpage>205</fpage><lpage>213</lpage><pub-id pub-id-type="doi">10.1152/jn.00071.2007</pub-id><pub-id pub-id-type="pmid">17460109</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinfeld</surname><given-names>D</given-names></name><name><surname>Ahissar</surname><given-names>E</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Active sensation: insights from the rodent vibrissa sensorimotor system</article-title><source>Current Opinion in Neurobiology</source><volume>16</volume><fpage>435</fpage><lpage>444</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2006.06.009</pub-id><pub-id pub-id-type="pmid">16837190</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinfeld</surname><given-names>D</given-names></name><name><surname>Deschênes</surname><given-names>M</given-names></name><name><surname>Ulanovsky</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whisking, sniffing, and the hippocampal θ-rhythm: a tale of two oscillators</article-title><source>PLOS Biology</source><volume>14</volume><elocation-id>e1002385</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1002385</pub-id><pub-id pub-id-type="pmid">26890361</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lak</surname><given-names>A</given-names></name><name><surname>Arabzadeh</surname><given-names>E</given-names></name><name><surname>Diamond</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Enhanced response of neurons in rat somatosensory cortex to stimuli containing temporal noise</article-title><source>Cerebral Cortex</source><volume>18</volume><fpage>1085</fpage><lpage>1093</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhm144</pub-id><pub-id pub-id-type="pmid">17712164</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrance</surname><given-names>ELA</given-names></name><name><surname>Harper</surname><given-names>NS</given-names></name><name><surname>Cooke</surname><given-names>JE</given-names></name><name><surname>Schnupp</surname><given-names>JWH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal predictability enhances auditory detection</article-title><source>The Journal of the Acoustical Society of America</source><volume>135</volume><fpage>EL357</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1121/1.4879667</pub-id><pub-id pub-id-type="pmid">24907846</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>RC</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Acoustic variability and distinguishability among mouse ultrasound vocalizations</article-title><source>The Journal of the Acoustical Society of America</source><volume>114</volume><fpage>3412</fpage><lpage>3422</lpage><pub-id pub-id-type="doi">10.1121/1.1623787</pub-id><pub-id pub-id-type="pmid">14714820</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lopes</surname><given-names>G</given-names></name><name><surname>Bonacchi</surname><given-names>N</given-names></name><name><surname>Frazão</surname><given-names>J</given-names></name><name><surname>Neto</surname><given-names>JP</given-names></name><name><surname>Atallah</surname><given-names>BV</given-names></name><name><surname>Soares</surname><given-names>S</given-names></name><name><surname>Moreira</surname><given-names>L</given-names></name><name><surname>Matias</surname><given-names>S</given-names></name><name><surname>Itskov</surname><given-names>PM</given-names></name><name><surname>Correia</surname><given-names>PA</given-names></name><name><surname>Medina</surname><given-names>RE</given-names></name><name><surname>Calcaterra</surname><given-names>L</given-names></name><name><surname>Dreosti</surname><given-names>E</given-names></name><name><surname>Paton</surname><given-names>JJ</given-names></name><name><surname>Kampff</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Bonsai: an event-based framework for processing and controlling data streams</article-title><source>Frontiers in Neuroinformatics</source><volume>9</volume><elocation-id>7</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2015.00007</pub-id><pub-id pub-id-type="pmid">25904861</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lui</surname><given-names>B</given-names></name><name><surname>Mendelson</surname><given-names>JR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Frequency modulated sweep responses in the medial geniculate nucleus</article-title><source>Experimental Brain Research</source><volume>153</volume><fpage>550</fpage><lpage>553</lpage><pub-id pub-id-type="doi">10.1007/s00221-003-1618-y</pub-id><pub-id pub-id-type="pmid">12961056</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matsumoto</surname><given-names>YK</given-names></name><name><surname>Okanoya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Phase-specific vocalizations of male mice at the initial encounter during the courtship sequence</article-title><source>PLOS ONE</source><volume>11</volume><elocation-id>e0147102</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0147102</pub-id><pub-id pub-id-type="pmid">26841117</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musolf</surname><given-names>K</given-names></name><name><surname>Hoffmann</surname><given-names>F</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Ultrasonic courtship vocalizations in wild house mice, <italic>Mus musculus musculus</italic></article-title><italic>.</italic><source>Animal Behaviour</source><volume>79</volume><fpage>757</fpage><lpage>764</lpage><pub-id pub-id-type="doi">10.1016/j.anbehav.2009.12.034</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musolf</surname><given-names>K</given-names></name><name><surname>Meindl</surname><given-names>S</given-names></name><name><surname>Larsen</surname><given-names>AL</given-names></name><name><surname>Kalcounis-Rueppell</surname><given-names>MC</given-names></name><name><surname>Penn</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Ultrasonic vocalizations of male mice differ among species and females show assortative preferences for male calls</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0134123</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0134123</pub-id><pub-id pub-id-type="pmid">26309246</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neilans</surname><given-names>EG</given-names></name><name><surname>Holfoth</surname><given-names>DP</given-names></name><name><surname>Radziwon</surname><given-names>KE</given-names></name><name><surname>Portfors</surname><given-names>CV</given-names></name><name><surname>Dent</surname><given-names>ML</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Discrimination of ultrasonic vocalizations by CBA/CaJ mice (<italic>Mus musculus</italic>) is related to spectrotemporal dissimilarity of vocalizations</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e85405</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0085405</pub-id><pub-id pub-id-type="pmid">24416405</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nomoto</surname><given-names>K</given-names></name><name><surname>Ikumi</surname><given-names>M</given-names></name><name><surname>Otsuka</surname><given-names>M</given-names></name><name><surname>Asaba</surname><given-names>A</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name><name><surname>Koshida</surname><given-names>N</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name><name><surname>Kikusui</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Female mice exhibit both sexual and social partner preferences for vocalizing males</article-title><source>Integrative Zoology</source><volume>13</volume><fpage>735</fpage><lpage>744</lpage><pub-id pub-id-type="doi">10.1111/1749-4877.12357</pub-id><pub-id pub-id-type="pmid">30019858</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname><given-names>JE</given-names></name><name><surname>Davis</surname><given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural oscillations carry speech rhythm through to comprehension</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>320</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id><pub-id pub-id-type="pmid">22973251</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pichora-Fuller</surname><given-names>MK</given-names></name><name><surname>Schneider</surname><given-names>BA</given-names></name><name><surname>Macdonald</surname><given-names>E</given-names></name><name><surname>Pass</surname><given-names>HE</given-names></name><name><surname>Brown</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Temporal jitter disrupts speech intelligibility: a simulation of auditory aging</article-title><source>Hearing Research</source><volume>223</volume><fpage>114</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.heares.2006.10.009</pub-id><pub-id pub-id-type="pmid">17157462</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pomerantz</surname><given-names>SM</given-names></name><name><surname>Nunez</surname><given-names>AA</given-names></name><name><surname>Bean</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>Female behavior is affected by male ultrasonic vocalizations in house mice</article-title><source>Physiology &amp; Behavior</source><volume>31</volume><fpage>91</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(83)90101-4</pub-id><pub-id pub-id-type="pmid">6685321</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portfors</surname><given-names>CV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Types and functions of ultrasonic vocalizations in laboratory rats and mice</article-title><source>Journal of the American Association for Laboratory Animal Science</source><volume>46</volume><fpage>28</fpage><lpage>34</lpage><pub-id pub-id-type="pmid">17203913</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Portfors</surname><given-names>CV</given-names></name><name><surname>Perkel</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The role of ultrasonic vocalizations in mouse communication</article-title><source>Current Opinion in Neurobiology</source><volume>28</volume><fpage>115</fpage><lpage>120</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2014.07.002</pub-id><pub-id pub-id-type="pmid">25062471</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Razak</surname><given-names>KA</given-names></name><name><surname>Fuzessery</surname><given-names>ZM</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural mechanisms underlying selectivity for the rate and direction of frequency-modulated sweeps in the auditory cortex of the pallid bat</article-title><source>Journal of Neurophysiology</source><volume>96</volume><fpage>1303</fpage><lpage>1319</lpage><pub-id pub-id-type="doi">10.1152/jn.00020.2006</pub-id><pub-id pub-id-type="pmid">16775213</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname><given-names>CE</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name><name><surname>Radman</surname><given-names>T</given-names></name><name><surname>Scharfman</surname><given-names>H</given-names></name><name><surname>Lakatos</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dynamics of active sensing and perceptual selection</article-title><source>Current Opinion in Neurobiology</source><volume>20</volume><fpage>172</fpage><lpage>176</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2010.02.010</pub-id><pub-id pub-id-type="pmid">20307966</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shannon</surname><given-names>RV</given-names></name><name><surname>Zeng</surname><given-names>FG</given-names></name><name><surname>Kamath</surname><given-names>V</given-names></name><name><surname>Wygonski</surname><given-names>J</given-names></name><name><surname>Ekelid</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Speech recognition with primarily temporal cues</article-title><source>Science</source><volume>270</volume><fpage>303</fpage><lpage>304</lpage><pub-id pub-id-type="doi">10.1126/science.270.5234.303</pub-id><pub-id pub-id-type="pmid">7569981</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shepard</surname><given-names>KN</given-names></name><name><surname>Liu</surname><given-names>RC</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Experience restores innate female preference for male ultrasonic vocalizations</article-title><source>Genes, Brain, and Behavior</source><volume>10</volume><fpage>28</fpage><lpage>34</lpage><pub-id pub-id-type="doi">10.1111/j.1601-183X.2010.00580.x</pub-id><pub-id pub-id-type="pmid">20345895</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sirotin</surname><given-names>YB</given-names></name><name><surname>Costa</surname><given-names>ME</given-names></name><name><surname>Laplagne</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Rodent ultrasonic vocalizations are bound to active sniffing behavior</article-title><source>Frontiers in Behavioral Neuroscience</source><volume>8</volume><elocation-id>399</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00399</pub-id><pub-id pub-id-type="pmid">25477796</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sollini</surname><given-names>J</given-names></name><name><surname>Chapuis</surname><given-names>GA</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Chadderton</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>ON-OFF receptive fields in auditory cortex diverge during development and contribute to directional sweep selectivity</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>2084</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-04548-3</pub-id><pub-id pub-id-type="pmid">29802383</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sugimoto</surname><given-names>H</given-names></name><name><surname>Okabe</surname><given-names>S</given-names></name><name><surname>Kato</surname><given-names>M</given-names></name><name><surname>Koshida</surname><given-names>N</given-names></name><name><surname>Shiroishi</surname><given-names>T</given-names></name><name><surname>Mogi</surname><given-names>K</given-names></name><name><surname>Kikusui</surname><given-names>T</given-names></name><name><surname>Koide</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A role for strain differences in waveforms of ultrasonic vocalizations during male-female interaction</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e22093</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0022093</pub-id><pub-id pub-id-type="pmid">21818297</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>B</given-names></name><name><surname>Rauschecker</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Processing of frequency-modulated sounds in the cat’s posterior auditory field</article-title><source>Journal of Neurophysiology</source><volume>79</volume><fpage>2629</fpage><lpage>2642</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.79.5.2629</pub-id><pub-id pub-id-type="pmid">9582234</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tort</surname><given-names>ABL</given-names></name><name><surname>Ponsel</surname><given-names>S</given-names></name><name><surname>Jessberger</surname><given-names>J</given-names></name><name><surname>Yanovsky</surname><given-names>Y</given-names></name><name><surname>Brankačk</surname><given-names>J</given-names></name><name><surname>Draguhn</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Parallel detection of theta and respiration-coupled oscillations throughout the mouse brain</article-title><source>Scientific Reports</source><volume>8</volume><elocation-id>6432</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-018-24629-z</pub-id><pub-id pub-id-type="pmid">29691421</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tschida</surname><given-names>K</given-names></name><name><surname>Michael</surname><given-names>V</given-names></name><name><surname>Takatoh</surname><given-names>J</given-names></name><name><surname>Han</surname><given-names>BX</given-names></name><name><surname>Zhao</surname><given-names>S</given-names></name><name><surname>Sakurai</surname><given-names>K</given-names></name><name><surname>Mooney</surname><given-names>R</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A specialized neural circuit gates social vocalizations in the mouse</article-title><source>Neuron</source><volume>103</volume><fpage>459</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2019.05.025</pub-id><pub-id pub-id-type="pmid">31204083</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Van Tasell</surname><given-names>DJ</given-names></name><name><surname>Soli</surname><given-names>SD</given-names></name><name><surname>Kirby</surname><given-names>VM</given-names></name><name><surname>Widin</surname><given-names>GP</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Speech waveform envelope cues for consonant recognition</article-title><source>The Journal of the Acoustical Society of America</source><volume>82</volume><fpage>1152</fpage><lpage>1161</lpage><pub-id pub-id-type="doi">10.1121/1.395251</pub-id><pub-id pub-id-type="pmid">3680774</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wachowitz</surname><given-names>S</given-names></name><name><surname>Ewert</surname><given-names>JP</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A key by which the toad’s visual system gets access to the domain of prey</article-title><source>Physiology &amp; Behavior</source><volume>60</volume><fpage>877</fpage><lpage>887</lpage><pub-id pub-id-type="doi">10.1016/0031-9384(96)00070-4</pub-id><pub-id pub-id-type="pmid">8873264</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86464.2.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bathellier</surname><given-names>Brice</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>CNRS</institution><country>France</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>This <bold>valuable</bold> work advances our understanding of the acoustic features driving the attraction of female mice to male vocalisations. The evidence supporting the conclusions is <bold>solid</bold>, with well-designed place preference assays and manipulations of male song structure. The work will be of broad interest to neurobiologists and ethologists working on mouse social interactions, auditory processing and communication.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86464.2.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>This work deals with courtship behaviour in mice. Authors try to identify the acoustic features that influence the attractivity level of male courtship songs to females. Courtship songs are made of sequences of short ultrasound syllables emitted at a rate of 7-10Hz. Authors manipulated these syllables by changing either the spectrotemporal content of each syllable or the intersyllable intervals. The authors found that it was only when sequences of syllables were irregular (with highly variable intersyllable intervals) that the female was less attracted to the song. The data, therefore, brings evidence that the acoustic features of syllables account less than the song's temporal regularity for the attractivity of courtship songs. The authors suggest that temporal regularity of syllable emission, building on breathing patterns, could reflect male fitness. They also suggest that temporal regularity could be an acoustic cue compressing the complex acoustic information carried by songs.</p><p>Strengths:</p><p>The study is well-written, very straightforward, and easy to follow. Behavioral tasks are well-designed and many tests, on a large enough set of animals have been done to support the conclusions. Results are clearly presented and provide enough details to see individual points. The discussion makes interesting connections between syllable rhythms and animals' fitness or brain rhythms.</p><p>Weaknesses:</p><p>Although the study is easy to understand and provides interesting results, the data analysis remains incomplete, and the interpretation of results is not cautious enough.</p><p>For instance, Fig. 2 shows a preference for song playback but we cannot determine if it is a general preference for a sound or a specific preference for male songs because only the difference between the presence of song or silence is tested. I acknowledge that the authors did not overstate their results, but the experimental design is incomplete and hard to interpret in that respect. For instance, the expression &quot;preferential approach to song&quot; is ambiguous.</p><p>There is no analysis of individual preference across tests and we might have the feeling that the effect shown mostly depends on the preference of only a few animals. Indeed, it seems that roughly one-third of animals showed a strong preference for the intact song while another third showed a strong preference for the modified song, whatever the modification. A few animals are therefore &quot;swing voters&quot;. It would have been interesting, if not pertinent, to have a deeper analysis of the behavior of these later animals. Do they choose less (i.e. spend less time close to speakers) or do they swing from one corner to another? What about the animals which always chose the modified song? Are these animals that already showed a weak or strong preference for silence, therefore showing they were not comfortable with the songs played? There is no discussion of these aspects either.</p><p>Also, on page 11, it is written &quot;female listeners perceptually compress the high sensory dimensionality of male songs by selectively monitoring a reduced subset of meaningful acoustic features in isolation.&quot; This statement or hypothesis is questionable. After all, if someone would change the inter-syllable intervals in human speech, that would become cryptic or at least annoying for the listener. Humans would definitely prefer normal speech. Is this because we compress acoustic features? Not really. It is likely that this modified speech just differs too much from the set of parameters typically encountered and therefore understood/interpreted while learning a language in childhood. Thus, the hypothesis here is rather to determine, for a given acoustic feature, if there is a range within which the perception of the message carried by the song (courtship) is maintained. Interpretation of &quot;compressed acoustic features&quot; with regards to animals' preference seems an overinterpretation. Same remark at the end of the conclusion.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86464.2.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>In the present manuscript, Perrodin et al. investigated which properties of ultrasonic vocalizations determine their attractiveness for female mice. They collected a set of male courtship vocalizations and compared their attractiveness for female mice against a number of conditions, including silence, and a number of modified sequences.</p><p>The study has a clear design and used insightful modifications on the vocalization sequences, which allow the present results to be linked to previous results. The most interesting outcome of the study is that female mice prefer regularly timed sequences of vocalizations over less regularly timed sequences. This result is novel and adds to our understanding of the determinants of social communication between mice. Overall the study is likely underpowered, which was, however, hard to assess as animal numbers were largely not reported for the individual tests, and statistical analysis was carried out on the level of sessions only.</p><p>The study has a very good discussion embedding the current results with the previous literature, although the discussion steps beyond the results in a few respects, in particular when trying to determine the underlying reasons for the preference for regularly spaced sequences.</p><p>Methodologically the study is carried out at the appropriate level, although some improvements could be made to the experimental apparatus to avoid reflections.</p><p>The study will likely have a substantial impact on the field of mouse communication because the regularity of spacing has not been a focus of previous research. In addition, the confirmation that a lot of other modifications are less determining for the attractiveness of the vocalizations provides solid data on which to base future work.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86464.2.sa3</article-id><title-group><article-title>Reviewer #3 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Perrodin, Verzat and Bendor describe the response of female mice to the playback of male mouse ultrasonic songs. The experiments were performed in a Y-maze-like apparatus with two acoustically separate response chambers. Sounds were presented in 4 trials, alternating strictly between the left and right branches of the Y. Cumulative dwell time in the two chambers was measured, and used as an index of female preference. They first show, consistent with previous observations, that female mice will spend more time near a speaker playing a male mouse song than near a speaker playing nothing. They then performed several manipulations-time reversals, syllable order randomization, phase scrambled replacement, pure tone replacement, and 'hyper-regular' inter-syllable-intervals-which female mice did not discriminate from the normal song in this assay. Finally, they show that females spent more time near normal songs than near songs with more variable inter-syllable-intervals</p><p>The authors' approach to the problem was ethologically sensible -- females were tested in proestrus and estrus, the male odor was used to increase motivation, mouse handling was with tube transfers to reduce stress, mice were age-matched across conditions, and experiments were conducted in the dark (active) phase. In addition, animals were habituated to handling and to the apparatus.</p><p>The acoustics were very good. The acoustic structure of the vocal signals was well described. Specific ranges of dB SPL were reported, speaker flatness was evaluated, the sound amplitude was matched in manipulated and unmanipulated songs, and playback onset timing jittered randomly between manipulated and unmanipulated signals.</p><p>I think it is a reasonable result. My concerns are the following:</p><p>1. The authors use &quot;approach&quot; as it has been used in other publications, but what is actually measured is dwell time. Pomerantz et al, 1983 observed that female mice approached mute and singing males the same number of times (e.g. approached both at the same rate), but spent more time with the singing than the mute male. Their use of &quot;approach&quot; to describe dwell time was a bit confusing to me, but sticking with the way the literature is defensible. However, they also refer to the assay as a &quot;place preference assay&quot;, which I found confusing.</p><p>2. I am a bit worried about their method of removing side bias (29% of trials). It certainly seems like a reasonable thing to exclude mice that simply picked one side or the other, but, because the stimulus always alternated between the sides, this exclusion of mice exhibiting a side bias is also excluding, specifically, behavior that would be incorrect.</p><p>3. Given the observation by Hammerschmidt et al, 2009, that female mice would only discriminate male songs in a playback assay on the first presentation, it is important to know whether females were used across the different manipulations. How many conditions did each female experience? How often did a female display positive discrimination in a condition after having displayed no discrimination?</p><p>Specific comments:</p><p>1. For Figure 2L</p><p>The heat map legend is labeled &quot;Towards&quot; indicating a motion towards either the speaker playing the song or the silent speaker. However, there is nothing in the methods that indicates that the direction of movement was ever measured. I may have missed it, but I can't figure out how this heat map was generated and what it represents. The figure legend states: &quot;Normalized temporal profiles of approach behaviour to mouse songs vs silence over the course of 4 sound presentation trials (x-axis, coloured bars) for each of the behavioural sessions (y-axis, each animal is one line, n = 29), calculated as in I. Sessions (lines) are ordered by the amplitude of their last element.&quot; 2I states &quot; I. Temporal profile of approach behaviour over the four sound presentation trials in the example session in C, calculated as the cumulative sum of time in the intact song playback (positively weighted) vs silent (negatively weighted) speaker zone.&quot; I interpret this to mean that &quot;Towards&quot; is an inaccurate description of what is being plotted, as there is no motion, only dwell time.</p><p><bold>References</bold></p><p>K. Hammerschmidt, K. Radyushkin, H. Ehrenreich &amp; J. Fischer (2009) Female mice respond to male ultrasonic 'songs' with approach behavior. Biol. Lett. 5:589-592.</p><p>Pomerantz, S.M., Nunez, A.A. &amp; Bean, J (1983) Female behavior is affected by male ultrasonic vocalizations in house mouse. Physiol. Behav. 31:91-96.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.86464.2.sa4</article-id><title-group><article-title>Author Response:</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Perrodin</surname><given-names>Catherine</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Verzat</surname><given-names>Colombine</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib><contrib contrib-type="author"><name><surname>Bendor</surname><given-names>Daniel</given-names></name><role specific-use="author">Author</role><aff><institution>University College London</institution><addr-line><named-content content-type="city">London</named-content></addr-line><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><p>We would like to thank the editor and the three reviewers for their time and effort taken in reviewing our manuscript and providing constructive feedback. Unfortunately, the first author of this manuscript is no longer involved in academia, and does not wish to further revise this manuscript. However, we agree with the entirety of the feedback and critiques provided by the referees, and feel these points should be taken into account when interpreting our results and conclusions.</p></body></sub-article></article>