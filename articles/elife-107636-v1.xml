<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">107636</article-id><article-id pub-id-type="doi">10.7554/eLife.107636</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.107636.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group></article-categories><title-group><article-title>The representation of facial emotion expands from sensory to prefrontal cortex with development</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fan</surname><given-names>Xiaoxu</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8115-8621</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Tripathi</surname><given-names>Abhishek</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Bijanki</surname><given-names>Kelly</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1624-8767</contrib-id><email>bijanki@bcm.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02pttbw34</institution-id><institution>Baylor College of Medicine</institution></institution-wrap><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/008zs3103</institution-id><institution>Rice University</institution></institution-wrap><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Hu</surname><given-names>Xiaoqing</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02zhqgq86</institution-id><institution>University of Hong Kong</institution></institution-wrap><country>China</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Luo</surname><given-names>Huan</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02v51f717</institution-id><institution>Peking University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>30</day><month>01</month><year>2026</year></pub-date><volume>14</volume><elocation-id>RP107636</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2025-05-23"><day>23</day><month>05</month><year>2025</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2025-05-23"><day>23</day><month>05</month><year>2025</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2025.05.23.655726"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-08-07"><day>07</day><month>08</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107636.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2025-12-22"><day>22</day><month>12</month><year>2025</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.107636.2"/></event></pub-history><permissions><copyright-statement>© 2025, Fan et al</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Fan et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-107636-v1.pdf"/><abstract><p>Facial expression recognition develops rapidly during infancy and improves from childhood to adulthood. As a critical component of social communication, this skill enables individuals to interpret others’ emotions and intentions. However, the brain mechanisms driving the development of this skill remain largely unclear due to the difficulty of obtaining data with both high spatial and temporal resolution from young children. By analyzing intracranial EEG data collected from childhood (5–10 years old) and post-childhood groups (13–55 years old), we find differential involvement of high-level brain area in processing facial expression information. For the post-childhood group, both the posterior superior temporal cortex (pSTC) and the dorsolateral prefrontal cortex (DLPFC) encode facial emotion features from a high-dimensional space. However, in children, the facial expression information is only significantly represented in the pSTC, not in the DLPFC. Furthermore, the encoding of complex emotions in pSTC is shown to increase with age. Taken together, young children rely more on low-level sensory areas than on the prefrontal cortex for facial emotion processing, suggesting that the prefrontal cortex matures with development to enable a full understanding of facial emotions, especially complex emotions that require social and life experience to comprehend.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>Facial expressions are a primary source of social information, allowing humans to infer others’ emotions and intentions. The ability to discriminate basic facial expressions emerges early in life, and infants can distinguish sad from happy faces. The ability to read facial expressions and to recognize emotions continues to improve from childhood through adolescence.</p><p>Most developmental evidence comes from behavioral measures or non-invasive imaging methods, with limited insight into the timing and distribution of neurons involved in these processes. This makes it difficult to characterize how neural representations of facial emotions change with age.</p><p>A method known as intracranial EEG (iEEG) provides direct, high-spatial and temporal resolution measurements of neural activity. It enables precise comparisons of how different brain regions encode facial emotion information across developmental stages, and how perceptual and cognitive systems mature to support social understanding. However, iEEG is rarely available in both children and adults due to its invasive nature.</p><p>Fan, Tripathi and Bijanki wanted to better understand how the brain learns to recognize emotions from facial emotions as children grow older by analyzing an existing, open-access iEEG data set. Specifically, they asked whether young children rely mainly on brain areas that process what a face looks like, while older individuals also use brain areas involved in interpretation and understanding.</p><p>Fan et al. found clear age-related differences in how the brain processes facial emotions. In young children, information about facial expressions is mainly processed in brain areas that analyze facial features. In older individuals, additional brain regions involved in interpretation and higher-level understanding also become engaged. The researchers also found that the brain’s ability to distinguish complex emotions increases gradually with age. Together, these findings suggest that as children grow and gain social experience, the brain increasingly relies on higher-level systems to interpret the emotional meaning of faces, not just their visual features.</p><p>Understanding how recognizing emotions becomes more accurate and complex with age is important for clinicians, educators, and families concerned with children’s social and emotional development – especially for conditions in which emotion understanding develops atypically. Before these findings can be applied in practice, future studies will need to confirm them in larger and more diverse groups and use non-invasive brain measures suitable for everyday settings. Translating these insights into tools for assessment or intervention will also require close collaboration between researchers, clinicians and educators.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>emotion</kwd><kwd>face</kwd><kwd>development</kwd><kwd>top-down modulation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04t0s7x83</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01-MH127006</award-id><principal-award-recipient><name><surname>Bijanki</surname><given-names>Kelly</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Facial emotion representations expand from sensory cortex to prefrontal regions across development, suggesting that the prefrontal cortex matures with development to enable a full understanding of facial emotion.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Understanding others' emotional states through their facial expressions is an important aspect of effective social interactions throughout the lifespan. Behavioral data suggest that facial emotion processing emerges very early in life (<xref ref-type="bibr" rid="bib3">Barrera and Maurer, 1981</xref>; <xref ref-type="bibr" rid="bib44">Walker-Andrews, 1997</xref>), as infants just months old can distinguish happy and sad faces from surprised faces (<xref ref-type="bibr" rid="bib10">Caron et al., 1982</xref>; <xref ref-type="bibr" rid="bib34">Nelson and Dolgin, 1985</xref>; <xref ref-type="bibr" rid="bib33">Nelson et al., 1979</xref>). However, children’s emotion recognition is substantially less accurate than adults, and this ability prominently improves across childhood and adolescence (<xref ref-type="bibr" rid="bib24">Johnston et al., 2011</xref>; <xref ref-type="bibr" rid="bib27">Kolb et al., 1992</xref>; <xref ref-type="bibr" rid="bib29">Lawrence et al., 2015</xref>; <xref ref-type="bibr" rid="bib38">Rodger et al., 2015</xref>; <xref ref-type="bibr" rid="bib39">Romani-Sponchiado et al., 2022</xref>; <xref ref-type="bibr" rid="bib43">Thomas et al., 2007</xref>). Although extensive research in cognitive and affective neuroscience has assessed developmental changes using behavioral and non-invasive neuroimaging approaches, our understanding of brain development related to facial expression perception remains limited.</p><p>One influential perspective on the development of face recognition is that it depends on the maturation of face-selective brain regions, including the fusiform face area (FFA), occipital face area (OFA), and posterior superior temporal sulcus (pSTS) (<xref ref-type="bibr" rid="bib13">Duchaine and Yovel, 2015</xref>). Supporting this view, <xref ref-type="bibr" rid="bib19">Gomez et al., 2017</xref> found evidence for microstructural proliferation in the fusiform gyrus during childhood, suggesting that improvements in face recognition are a product of an interplay between structural and functional changes in the cortex. Additionally, monkeys raised without exposure to faces fail to develop normal face-selective patches, suggesting that face experience is necessary for the development of the face-processing network (<xref ref-type="bibr" rid="bib2">Arcaro et al., 2017</xref>). It is likely that the gradual maturation of pSTS and FFA, two early sensory areas involved in the processing of facial expressions (<xref ref-type="bibr" rid="bib7">Bernstein and Yovel, 2015</xref>; <xref ref-type="bibr" rid="bib14">Engell and Haxby, 2007</xref>), contributes to the improved facial expression recognition over development. Yet, few studies have investigated the development of neural representation of emotional facial expressions in FFA and pSTS from early childhood to adulthood in humans.</p><p>Besides the visual processing of facial configurations, understanding the emotional meaning of faces requires the awareness and interpretation of the emotional state of the other person, which is significantly shaped by life experience (<xref ref-type="bibr" rid="bib36">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib48">Yurgelun-Todd and Killgore, 2006</xref>). Thus, some researchers have proposed that the maturation of emotional information processing is related to the progressive increase in functional activity in the prefrontal cortex (<xref ref-type="bibr" rid="bib27">Kolb et al., 1992</xref>; <xref ref-type="bibr" rid="bib42">Tessitore et al., 2005</xref>; <xref ref-type="bibr" rid="bib47">Williams et al., 2006</xref>). With development, greater engagement of the prefrontal cortex may facilitate top-down modulation of activity in more primitive subcortical and limbic regions, such as the amygdala (<xref ref-type="bibr" rid="bib22">Hariri et al., 2003</xref>; <xref ref-type="bibr" rid="bib21">Hariri et al., 2000</xref>; <xref ref-type="bibr" rid="bib49">Yurgelun-Todd, 2007</xref>). However, despite these theoretical advances, the functional changes in the prefrontal cortex during the perceptual processing of emotional facial expressions over development remain largely unknown.</p><p>Here, we analyze intracranial EEG (iEEG) data collected from childhood (5–10 years old) and post-childhood groups (13–55 years old) while participants were watching a short audiovisual film. In our results, children’s dorsolateral prefrontal cortex (DLPFC) shows minimal involvement in processing facial expression, unlike the post-childhood group. In contrast, for both children and post-childhood individuals, facial expression information is encoded in the pSTC, a brain region that contributes to the perceptual processing of facial expressions (<xref ref-type="bibr" rid="bib7">Bernstein and Yovel, 2015</xref>; <xref ref-type="bibr" rid="bib13">Duchaine and Yovel, 2015</xref>; <xref ref-type="bibr" rid="bib15">Fan et al., 2020</xref>; <xref ref-type="bibr" rid="bib16">Flack et al., 2015</xref>). Furthermore, the encoding of complex emotions in the pSTC increases with age. These iEEG results imply that social and emotional experiences shape the prefrontal cortex’s involvement in processing the emotional meaning of faces throughout development, probably through top-down modulation of early sensory areas.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Using AI and encoding models to study the neural representation of facial expression</title><p>In this study, we analyzed iEEG data collected from a large group of epilepsy patients while they watched a short audiovisual film at the University Medical Center Utrecht (<xref ref-type="bibr" rid="bib6">Berezutskaya et al., 2022</xref>). The movie consisted of 13 interleaved blocks of videos accompanied by speech or music, 30 s each (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). To characterize the neural representation of facial expression in the prefrontal cortex and low-level sensory areas across development, we analyzed iEEG data from 11 children (5–10 years old) and 31 post-childhood individuals (13–55 years old) who have electrode coverage in DLPFC, pSTC, or both. First, Hume AI facial expression models were used to continuously extract facial emotion features from the movie (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Then, we tested how well encoding models constructed from the 48 facial emotion features (e.g. fear, joy) predict cortical high-frequency band (HFB) activity (110–140 Hz) induced by the presented movie (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). We focused on HFB activity because it is widely considered to reflect the responses of local neuronal populations near the recording electrode (<xref ref-type="bibr" rid="bib35">Parvizi and Kastner, 2018</xref>). The model performance was quantified as the correlation between the predicted and actual HFB activities, which is also called prediction accuracy.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Task design and analysis methods.</title><p>(<bold>A</bold>) Movie structure. A 6.5 min short film was created by editing fragments from <italic>Pippi on the Run</italic> into a coherent narrative. The movie consisted of 13 interleaved blocks of videos accompanied by speech or music. (<bold>B</bold>) Data analysis schematic. Standard analysis pipeline for extracting emotion features from the movie and constructing an encoding model to predict intracranial EEG (iEEG) responses while participants watch the short film.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107636-fig1-v1.tif"/><permissions><copyright-statement>© 1970, Beta Film GmbH. All rights reserved</copyright-statement><copyright-year>1970</copyright-year><copyright-holder>Beta Film GmbH</copyright-holder><license><license-p>Screenshots in panel A are taken from 'Pippi on the Run' (1970). These are not covered by the CC-BY 4.0 license.</license-p></license></permissions></fig></sec><sec id="s2-2"><title>Differential representation of facial expression in children’s DLPFC</title><p>Using the analysis approach described above, we examined how facial emotion information is represented by DLPFC (<xref ref-type="fig" rid="fig2">Figure 2A</xref>) while watching videos accompanied by speech (i.e. speech condition) in childhood and post-childhood groups. The prediction accuracy of the encoding model was significantly greater than zero in the post-childhood group (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, gray bar, <italic>p</italic>=0.012, t<sub>12</sub>=2.954, one-sample t-test), suggesting that the neural responses in DLPFC were dynamically modulated by the facial emotion features from the movie. However, facial emotion features were not encoded in children’s DLPFC (<xref ref-type="fig" rid="fig2">Figure 2B</xref>, blue bar, <italic>p</italic>=0.73, t<sub>7</sub>=0.355, one-sample t-test). The prediction accuracy in children’s DLPFC was significantly lower than in the post-childhood group (<italic>p</italic>=0.036, unpaired t-test). Similar results were observed in the music condition. The prediction accuracy of the encoding model was significantly greater than zero in the post-childhood group (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <italic>p</italic>=0.012, t<sub>12</sub>=2.975, one-sample t-test), but not in the childhood group (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, <italic>p</italic>=0.19, t<sub>7</sub>=1.469, one-sample t-test). Together, these findings show that the DLPFC dynamically encodes facial expression information in post-childhood individuals but not in young children.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Prediction performance of encoding models in dorsolateral prefrontal cortex (DLPFC).</title><p>(<bold>A</bold>) Spatial distribution of electrodes in DLPFC. Electrodes in all participants from each group are projected onto the Montreal Neurological Institute (MNI) space and shown on the average brain. Red shaded areas indicate middle frontal cortex provided by the FreeSurfer Desikan-Killiany atlas (<xref ref-type="bibr" rid="bib12">Desikan et al., 2006</xref>). Electrodes outside the DLPFC are not shown. (<bold>B</bold>) Averaged prediction accuracy across participants for speech condition. The performance of the encoding model is measured as Pearson correlation coefficient (<bold>r</bold>) between measured and predicted brain activities. (<bold>C</bold>) Averaged prediction accuracy across participants for music condition. (<bold>D</bold>) Prediction accuracy difference between speech condition and music condition for each group. Error bars are standard error of the mean. *<italic>p</italic>&lt;0.05. (N<sub>childhood</sub>=8, N<sub>post-childhood</sub>=13).</p><p><supplementary-material id="fig2sdata1"><label>Figure 2—source data 1.</label><caption><title>Prediction performance of encoding models in dorsolateral prefrontal cortex (DLPFC).</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-107636-fig2-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107636-fig2-v1.tif"/></fig><p>To further understand the functional development of children’s DLPFC, we compared the effect of human voice on the representation of facial expression in DLPFC between the two groups. The effect of human voice was quantified as a difference in prediction accuracy between the speech and music conditions. Our results showed that human voice influences facial expression representation in the DLPFC differently across development (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, <italic>p</italic>=0.018, t<sub>19</sub>=2.588, unpaired t-test). The presence of human voice enhances facial expression representation in the DLPFC of post-childhood individuals but impairs it in children.</p><p>Taken together, there are significant developmental changes in DLPFC involvement in facial expression perception.</p></sec><sec id="s2-3"><title>The neural representation of facial expression in children’s pSTC</title><p>After identifying developmental differences in the involvement of high-level brain areas in processing facial expression, we next examined the neural representation of facial expression in children’s early sensory areas. As an area in the core face network, pSTS has been associated with early stages of facial expression processing stream (<xref ref-type="bibr" rid="bib7">Bernstein and Yovel, 2015</xref>; <xref ref-type="bibr" rid="bib13">Duchaine and Yovel, 2015</xref>; <xref ref-type="bibr" rid="bib15">Fan et al., 2020</xref>; <xref ref-type="bibr" rid="bib16">Flack et al., 2015</xref>). Although previous studies suggested that the development of facial recognition depends on the maturation of face-selective brain regions (<xref ref-type="bibr" rid="bib2">Arcaro et al., 2017</xref>; <xref ref-type="bibr" rid="bib19">Gomez et al., 2017</xref>), it is still unclear how facial expression information is encoded in children’s pSTS. Here, we examined the performance of the facial expression encoding model in a rare sample of children with electrode coverage in pSTC (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Our results showed that the encoding model significantly predicts the HFB neural signals in children’s pSTC under the speech condition (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <italic>p</italic>&lt;0.002, two-tailed permutation test, see <bold>Method</bold> for details). Moreover, the prediction accuracy is significantly reduced when human voice is absent from the video (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <italic>p</italic>=0.0306<italic>,</italic> two-tailed permutation test). In an example subject (S19: 8 years old) who has electrodes in both DLPFC and pSTC, the model fitting results clearly showed that facial emotion is encoded in pSTC (S19<sub>speech</sub>: <italic>p</italic>=0.0014, <italic>r</italic>=0.1951), but not DLPFC (S19<sub>speech</sub>: <italic>p</italic>=0.295, <italic>r</italic>=−0.0628). Similarly, group-level results showed that the model performance is significantly greater than zero in the pSTC of post-childhood individuals (<xref ref-type="fig" rid="fig3">Figure 3D and E</xref>, speech condition: <italic>p</italic>=0.0001, t<sub>24</sub>=4.601, one-sample t-test), and this neural representation of facial expression information is significantly reduced when human voice is absent (<xref ref-type="fig" rid="fig3">Figure 3E</xref>, paired-t-test, t<sub>24</sub>=2.897, <italic>p</italic>=0.0079). These results provide evidence that children’s sensory areas encode facial emotion features in a manner similar to that of post-childhood individuals.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Prediction performance of encoding models in posterior superior temporal cortex (pSTC).</title><p>(<bold>A</bold>) The electrode distribution in the native brain space for the four children. Electrodes in pSTC are green, and electrodes in dorsolateral prefrontal cortex (DLPFC) are yellow. (<bold>B</bold>) Prediction accuracy of encoding models in children. Bars indicate mean prediction accuracy across participants and lines indicate individual data. (<bold>C</bold>) Prediction accuracy of encoding models for S19 in both DLPFC and pSTC. (<bold>D</bold>) Spatial distribution of recording contacts in post-childhood participants’ pSTC. The pSTC electrodes identified in individual space are projected onto Montreal Neurological Institute (MNI) space and shown on the average brain. Contacts other than pSTC are not shown. Blue shaded areas indicate superior temporal cortex provided by the FreeSurfer Desikan-Killiany atlas (<xref ref-type="bibr" rid="bib12">Desikan et al., 2006</xref>). (<bold>E</bold>) Averaged prediction accuracy across post-childhood participants (N=25). Error bars are standard error of the mean. **<italic>p</italic>&lt;0.01. *<italic>p</italic>&lt;0.05.</p><p><supplementary-material id="fig3sdata1"><label>Figure 3—source data 1.</label><caption><title>Prediction performance of encoding models in posterior superior temporal cortex (pSTC).</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-107636-fig3-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107636-fig3-v1.tif"/></fig></sec><sec id="s2-4"><title>The complexity of facial expression encoding in the pSTC increases across development</title><p>To understand how facial expression representation in pSTC changes across development, we examined the feature weights of the facial expression encoding models in all participants with significant prediction accuracy (10 post-childhood individuals and 2 children). The weight for each feature represents its relative contribution to predicting the neural response. First, we calculated the encoding weights for complex emotions (averaging guilt, embarrassment, pride, and envy, which were selected as the most representative complex emotions based on previous studies <xref ref-type="bibr" rid="bib1">Alba-Ferrara et al., 2011</xref>; <xref ref-type="bibr" rid="bib9">Burnett et al., 2011</xref>; <xref ref-type="bibr" rid="bib41">Russell and Paris, 1994</xref>) and basic emotions (averaging joy, sadness, fear, anger, disgust, and surprise). Then, we calculated their correlations with age separately. Our results showed that the encoding weight of complex emotion was significantly positively correlated with age (r<sub>12</sub>=0.8512, <italic>p</italic>=0.004, <xref ref-type="fig" rid="fig4">Figure 4A</xref> left). No significant correlation between encoding weight of basic emotion and age was observed (r<sub>12</sub>=0.3913, <italic>p</italic>=0.2085, <xref ref-type="fig" rid="fig4">Figure 4A</xref> right). In addition, we computed Pearson correlations between each individual feature weight and age, ranking the r values from largest to smallest (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The highest correlations were found for embarrassment, guilt, pride, interest, and envy—emotions that are all considered complex emotions. Among them, the weights for embarrassment, guilt, pride, and interest showed significant positive correlations with age (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, embarrassment: <italic>r</italic>=0.7666, <italic>p</italic>=0.0036; pride: <italic>r</italic>=0.6773, <italic>p</italic>=0.0155; guilt: <italic>r</italic>=0.6421, <italic>p</italic>=0.0244, interest: <italic>r</italic>=0.6377, <italic>p</italic>=0.0257, uncorrected for multiple comparisons), suggesting that the encoding of these complex emotions in pSTC increases with age. Thus, our results suggest that as development progresses, the pSTC becomes increasingly engaged in encoding complex emotions which requires representing others' mental states and emerges later in development (<xref ref-type="bibr" rid="bib8">Burnett et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Garcia and Scherf, 2015</xref>; <xref ref-type="bibr" rid="bib32">Motta-Mena and Scherf, 2017</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Correlation between encoding weights and age.</title><p>(<bold>A</bold>) Left: Correlation between averaged encoding weights of five complex emotions and age. Right: Correlation between averaged encoding weights of six basic emotions and age. (<bold>B</bold>) Pearson correlation coefficient between encoding weights of 48 facial expression features and age. The results are ranked from largest to smallest. Significant correlations noted with * (<italic>p</italic>&lt;0.05, uncorrected) or ** (<italic>p</italic>&lt;0.01, uncorrected). (<bold>C</bold>) Correlation between encoding weights of embarrassment, pride, guilt, interest and age (N=12).</p><p><supplementary-material id="fig4sdata1"><label>Figure 4—source data 1.</label><caption><title>Encoding weight and age.</title></caption><media mimetype="application" mime-subtype="xlsx" xlink:href="elife-107636-fig4-data1-v1.xlsx"/></supplementary-material></p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107636-fig4-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The current study examines functional changes in both low-level and high-level brain areas across development to provide valuable insights into the neural mechanisms underlying the maturation of facial expression perception. Based on our findings, we propose that young children rely primarily on early sensory areas rather than the prefrontal cortex for facial emotion processing. As development progresses, the prefrontal cortex becomes increasingly involved, perhaps serving to modulate responses in early sensory areas based on emotional context and enabling them to process complex emotions. This developmental progression ultimately enables the full comprehension of facial emotions in adulthood.</p><p>Behavioral results suggest that infants as young as only 7–8 months can categorize some emotions (<xref ref-type="bibr" rid="bib10">Caron et al., 1982</xref>; <xref ref-type="bibr" rid="bib34">Nelson and Dolgin, 1985</xref>; <xref ref-type="bibr" rid="bib33">Nelson et al., 1979</xref>). However, sensitivity to facial expressions in young children does not mean that they can understand the meaning of that affective state. For example, <xref ref-type="bibr" rid="bib25">Kaneshige and Haryu, 2015</xref> found that although 4-month-old infants could discriminate facial configurations of anger and happiness, they responded positively to both, suggesting that at this stage, they may lack knowledge of the affective meaning behind these expressions. This underscores the idea that additional processes need to be developed for children to fully grasp the emotional content conveyed by facial expressions. Although the neural mechanism behind this development is still unclear, a reasonable perspective is that it requires both visual processing of facial features and emotion-related processing for the awareness of the emotional state of the other person (<xref ref-type="bibr" rid="bib36">Pereira et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Pollak et al., 2009</xref>; <xref ref-type="bibr" rid="bib40">Ruba and Pollak, 2020</xref>). Indeed, growing evidence suggests that the prefrontal cortex plays an important role in integrating prior knowledge with incoming sensory information, allowing interpretation of the current situation in light of past emotional experience (<xref ref-type="bibr" rid="bib5">Batty and Taylor, 2006</xref>; <xref ref-type="bibr" rid="bib47">Williams et al., 2006</xref>).</p><p>In the current study, we observed differential representation of facial expressions in the DLPFC between children and post-childhood individuals. First, in post-childhood individuals, neural activity in the DLPFC encodes high-dimensional facial expression information, whereas this encoding is absent in children. Second, while human voice enhances the representation of facial expressions in the DLPFC of post-childhood individuals, it instead reduces this representation in children. These results suggest that the DLPFC undergoes developmental changes in how it processes facial expressions. The absence of high-dimensional facial expression encoding in children implies that the DLPFC may not yet be fully engaged in emotional interpretation at an early age. Additionally, the opposite effects of human voice on facial expression representation indicate that multimodal integration of social cues develops over time. In post-childhood individuals, voices may enhance emotional processing by providing congruent information (<xref ref-type="bibr" rid="bib11">Collignon et al., 2008</xref>; <xref ref-type="bibr" rid="bib26">Klasen et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Kreifelts et al., 2007</xref>), whereas in children, the presence of voice might interfere with or redirect attentional resources away from facial expression processing (<xref ref-type="bibr" rid="bib23">Jacob et al., 2014</xref>; <xref ref-type="bibr" rid="bib26">Klasen et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Weissman et al., 2004</xref>).</p><p>There have been few neuroimaging studies directly examining the functional role of young children’s DLPFC in facial emotion perception. Some evidence suggests that the prefrontal cortex continues to develop until adulthood to achieve its mature function in emotion perception (<xref ref-type="bibr" rid="bib20">Gunning-Dixon et al., 2003</xref>; <xref ref-type="bibr" rid="bib42">Tessitore et al., 2005</xref>; <xref ref-type="bibr" rid="bib43">Thomas et al., 2007</xref>), and for some emotion categories, this development may extend across the lifespan. For example, prefrontal cortex activation during viewing fearful faces increases with age (<xref ref-type="bibr" rid="bib31">Monk et al., 2003</xref>; <xref ref-type="bibr" rid="bib47">Williams et al., 2006</xref>; <xref ref-type="bibr" rid="bib48">Yurgelun-Todd and Killgore, 2006</xref>). As there were not enough participants for us to calculate correlation between encoding model performance in DLPFC and age, it is still unclear whether the representation of facial expression in DLPFC increases linearly with age. One possibility is that the representation of facial expressions in the DLPFC gradually increases with age until it reaches an adult-like level. This would suggest a continuous developmental trajectory, where incremental improvements in neural processing accumulate over time. Another possibility is that development follows a more nonlinear pattern, showing improvement with prominent changes at specific ages. Interestingly, research has shown that performance on matching emotional expressions improves steadily over development, with notable gains in accuracy occurring between 9 and 10 years and again between 13 and 14 years, after which performance reaches adult-like levels (<xref ref-type="bibr" rid="bib27">Kolb et al., 1992</xref>).</p><p>Our results clearly showed that facial expression is encoded in children’s pSTC. Childhood and the post-childhood group showed comparable levels of prediction accuracy. In the 8-year-old child (S19) who had electrode coverage in both DLPFC and pSTC, facial expressions were represented in the pSTC but not in the DLPFC. This rare sampling allows us to rule out the possibility that the low prediction accuracy of the facial expression encoding model in the DLPFC is due to the reduced engagement in the movie-watching task for children. Consistent with our findings, previous studies have shown that the fusiform and superior temporal gyri are involved in emotion-specific processing in 10-year-old children (<xref ref-type="bibr" rid="bib30">Lobaugh et al., 2006</xref>). Meanwhile, some other researchers found that responses to facial expression in the amygdala and posterior fusiform gyri decreased as people got older (<xref ref-type="bibr" rid="bib42">Tessitore et al., 2005</xref>), but the use of frontal regions increased with age (<xref ref-type="bibr" rid="bib20">Gunning-Dixon et al., 2003</xref>). Therefore, we propose that early sensory areas like the fusiform and superior temporal gyri play a key role in facial expression processing in children, but their contribution may shift with age as frontal regions become more involved. Consistent with this perspective, our results revealed that the encoding weights for complex emotions in pSTC increased with age, implying a developmental trajectory in the neural representation of complex emotions in pSTC. This finding aligns with previous behavioral studies showing that social complex emotion recognition does not fully mature until young adulthood (<xref ref-type="bibr" rid="bib8">Burnett et al., 2009</xref>; <xref ref-type="bibr" rid="bib17">Garcia and Scherf, 2015</xref>; <xref ref-type="bibr" rid="bib32">Motta-Mena and Scherf, 2017</xref>). In fact, our results imply that the representation of complex facial expressions in pSTC continues to develop over the lifespan. As for the correlation between basic emotion encoding and age, the lack of a significant effect in our study does not necessarily indicate an absence of developmental change but may instead be due to the limited sample size.</p><p>Our study examines how facial emotions presented in naturalistic video stimuli are represented in the human brain across developmental stages. The facial emotional features used in our encoding models describe the emotions portrayed by characters in the stimulus, rather than participants’ subjective perception or experience of those emotions. While our results suggest the potential neural basis for participants’ perception of these emotions, we cannot conclude that such information was consciously perceived without direct behavioral measures. In fact, automatic and efficient emotion perception is not driven by the structural features of a face alone but is also shaped by other factors, such as contextual information, prior experiences, and current mood states (<xref ref-type="bibr" rid="bib4">Barrett et al., 2011</xref>). How the conscious percept of facial emotion is constructed remains far from fully understood. Recent evidence suggests that dynamic interactions between visual cortices and frontal regions integrate stimulus-defined features with contextual and internal factors to give rise to subjective emotional perception (<xref ref-type="bibr" rid="bib45">Wang et al., 2014</xref>).</p><sec id="s3-1"><title>Limitations and future directions</title><p>Our study provides novel insights into the neural mechanisms underlying the development of facial expression processing. As with any study, several limitations should be acknowledged. First, most electrode coverage in our study was in the left hemisphere, potentially limiting our understanding of lateralization effects. Second, while our results provide insights into the role of DLPFC during development, we were unable to examine other prefrontal regions, such as the orbitofrontal cortex (OFC) and anterior cingulate cortex (ACC), to examine their unique contributions to emotion processing. Third, due to sample size constraints, we were unable to divide participants into more granular developmental stages, such as early childhood, adolescence, and adulthood, which could provide a more detailed characterization of the neural mechanisms underlying the development of facial expression processing. Future studies using non-invasive methods with more age-diverse samples, or with longitudinal designs to directly track developmental trajectories, will be essential for refining our understanding of how facial emotion processing develops across the lifespan. In addition, future studies are needed to systematically examine how multimodal cues and prior experiences contribute to the understanding of emotion from faces.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Reagent type (species) or resource</th><th align="left" valign="top">Designation</th><th align="left" valign="top">Source or reference</th><th align="left" valign="top">Identifiers</th><th align="left" valign="top">Additional information</th></tr></thead><tbody><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">MNE-Python</td><td align="left" valign="top">MNE-Python</td><td align="left" valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_005972">SCR_005972</ext-link></td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Python</td><td align="left" valign="top">Python</td><td align="left" valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_008394">SCR_008394</ext-link></td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">FreeSurfer</td><td align="left" valign="top">FreeSurfer</td><td align="left" valign="top">RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001847">SCR_001847</ext-link></td><td align="left" valign="top"/></tr><tr><td align="left" valign="top">Software, algorithm</td><td align="left" valign="top">Hume AI</td><td align="left" valign="top">Hume AI</td><td align="left" valign="top"/><td align="left" valign="top"/></tr></tbody></table></table-wrap><p>In this study, iEEG data from an open multimodal iEEG-fMRI dataset were analyzed (<xref ref-type="bibr" rid="bib6">Berezutskaya et al., 2022</xref>).</p><sec id="s4-1"><title>Participants and electrode distribution</title><p>Due to the research purposes of the current study, only participants who had at least four electrode contacts in either DLPFC or pSTC were included in the data analysis (<xref ref-type="table" rid="table1 table2">Tables 1 and 2</xref>). Eleven children (5–10 years old, 7 females) and thirty-one post-childhood individuals (13–55 years old, 18 females) are included in the present study. In the childhood group, eight participants had enough electrodes implanted in the DLPFC, and four had enough electrodes implanted in the pSTC. In the post-childhood group, thirteen participants had enough electrodes implanted in the DLPFC, and twenty-five had enough electrodes implanted in the pSTC.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Demographic information of childhood group.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">ID</th><th align="left" valign="bottom">Sex</th><th align="left" valign="bottom">Age</th><th align="left" valign="bottom">Number of recording contacts in left pSTC</th><th align="left" valign="bottom">Number of recording contacts in right pSTC</th><th align="left" valign="bottom">Number of recording contacts in left DLPFC</th><th align="left" valign="bottom">Number of recording contacts in right DLPFC</th></tr></thead><tbody><tr><td align="left" valign="top">s02</td><td align="left" valign="top">F</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s10</td><td align="left" valign="top">F</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s19</td><td align="left" valign="top">F</td><td align="left" valign="top">8</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s32</td><td align="left" valign="top">F</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">5</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s33</td><td align="left" valign="top">F</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s37</td><td align="left" valign="top">M</td><td align="left" valign="top">5</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s39</td><td align="left" valign="top">F</td><td align="left" valign="top">5</td><td align="left" valign="top">7</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s41</td><td align="left" valign="top">M</td><td align="left" valign="top">7</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s49</td><td align="left" valign="top">F</td><td align="left" valign="top">10</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s50</td><td align="left" valign="top">F</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s63</td><td align="left" valign="top">M</td><td align="left" valign="top">5</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td></tr></tbody></table></table-wrap><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Demographic information of post-childhood group.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">ID</th><th align="left" valign="bottom">Sex</th><th align="left" valign="bottom">Age</th><th align="left" valign="bottom">Number of recording contacts in left pSTC</th><th align="left" valign="bottom">Number of recording contacts in right pSTC</th><th align="left" valign="bottom">Number of recording contacts in left DLPFC</th><th align="left" valign="bottom">Number of recording contacts in right DLPFC</th></tr></thead><tbody><tr><td align="left" valign="top">s1</td><td align="left" valign="top">M</td><td align="left" valign="top">55</td><td align="left" valign="top">0</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s3</td><td align="left" valign="top">F</td><td align="left" valign="top">33</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s5</td><td align="left" valign="top">F</td><td align="left" valign="top">33</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s6</td><td align="left" valign="top">F</td><td align="left" valign="top">43</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s12</td><td align="left" valign="top">M</td><td align="left" valign="top">37</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">12</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s14</td><td align="left" valign="top">F</td><td align="left" valign="top">18</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s16</td><td align="left" valign="top">M</td><td align="left" valign="top">17</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s17</td><td align="left" valign="top">M</td><td align="left" valign="top">28</td><td align="left" valign="top">0</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s18</td><td align="left" valign="top">F</td><td align="left" valign="top">15</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">11</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s20</td><td align="left" valign="top">F</td><td align="left" valign="top">25</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s22</td><td align="left" valign="top">M</td><td align="left" valign="top">21</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s24</td><td align="left" valign="top">F</td><td align="left" valign="top">47</td><td align="left" valign="top">11</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s25</td><td align="left" valign="top">M</td><td align="left" valign="top">14</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s26</td><td align="left" valign="top">F</td><td align="left" valign="top">48</td><td align="left" valign="top">11</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s27</td><td align="left" valign="top">M</td><td align="left" valign="top">15</td><td align="left" valign="top">11</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s28</td><td align="left" valign="top">M</td><td align="left" valign="top">21</td><td align="left" valign="top">0</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s31</td><td align="left" valign="top">F</td><td align="left" valign="top">13</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">7</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s34</td><td align="left" valign="top">F</td><td align="left" valign="top">51</td><td align="left" valign="top">5</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s38</td><td align="left" valign="top">F</td><td align="left" valign="top">14</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s40</td><td align="left" valign="top">M</td><td align="left" valign="top">49</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s43</td><td align="left" valign="top">M</td><td align="left" valign="top">19</td><td align="left" valign="top">0</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td><td align="left" valign="top">8</td></tr><tr><td align="left" valign="top">s45</td><td align="left" valign="top">M</td><td align="left" valign="top">19</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s48</td><td align="left" valign="top">F</td><td align="left" valign="top">18</td><td align="left" valign="top">5</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s51</td><td align="left" valign="top">M</td><td align="left" valign="top">46</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">9</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s54</td><td align="left" valign="top">F</td><td align="left" valign="top">31</td><td align="left" valign="top">4</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s55</td><td align="left" valign="top">F</td><td align="left" valign="top">23</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s57</td><td align="left" valign="top">F</td><td align="left" valign="top">36</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">7</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s58</td><td align="left" valign="top">F</td><td align="left" valign="top">16</td><td align="left" valign="top">6</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s59</td><td align="left" valign="top">F</td><td align="left" valign="top">30</td><td align="left" valign="top">7</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s60</td><td align="left" valign="top">M</td><td align="left" valign="top">42</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr><tr><td align="left" valign="top">s61</td><td align="left" valign="top">F</td><td align="left" valign="top">16</td><td align="left" valign="top">8</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td><td align="left" valign="top">0</td></tr></tbody></table></table-wrap></sec><sec id="s4-2"><title>Experimental procedures</title><p>A 6.5 min short film was crafted by editing fragments from <italic>Pippi on the Run</italic> into a coherent narrative. The film is structured into 13 interleaved 30 s blocks of either speech or music, with seven blocks featuring background music only and six blocks retaining the original dialogue and voice from the video. Patients were asked to watch the movie while the intracranial EEG signals were recorded. No fixation cross was displayed in the middle of the screen or elsewhere. The movie was presented using the Presentation software (Neurobehavioral Systems, Berkeley, CA) and the sound was synchronized with the neural recordings. All patients were asked to participate in the experiment outside the window of seizures. More data acquisition details can be found in <xref ref-type="bibr" rid="bib6">Berezutskaya et al., 2022</xref>.</p></sec><sec id="s4-3"><title>iEEG data processing</title><p>Electrode contacts and epochs contaminated with excessive artifacts and epileptiform activity were removed from data analysis by visual inspection. Raw data were filtered with a 50 Hz notch filter and re-referenced to the common average reference. For each electrode contact in each patient, the preprocessed data were band-pass filtered (110–140 Hz, fourth-order Butterworth). The Hilbert transform was then applied to extract the analytic amplitude. Each event (block) was extracted in the 0–30 s time window around its onset. The fifth music block was excluded, as there were no faces presented on screen. Subsequently, the data were down-sampled to 400 Hz and square-root transformed. Finally, the data were normalized by z-scoring with respect to baseline periods (−0.2–0 s before stimulus onset).</p></sec><sec id="s4-4"><title>Contact location and regions of interest</title><p>We identified electrode contacts in STC in individual brains using individual anatomical landmarks (i.e. gyri and sulci). Superior temporal sulci and lateral sulci were used as boundaries. A coronal plane, including the posterior tip of the hippocampus served as an anterior/posterior boundary. To identify electrode contacts in DLPFC, we projected the electrode contact positions provided by the open dataset onto the Montreal Neurological Institute-152 template brain (MNI) space, using FreeSurfer. DLPFC was defined based on the following sets of HCP-MMP1 (<xref ref-type="bibr" rid="bib18">Glasser et al., 2016</xref>) labels on both left and right hemispheres: 9-46d, 46, a9-46v, and p9-46v.</p></sec><sec id="s4-5"><title>Emotion feature extraction</title><p>Hume AI (<ext-link ext-link-type="uri" xlink:href="https://www.hume.ai">https://www.hume.ai</ext-link>) was used to extract the facial emotion features from the video. When multiple faces appeared in the movie, the maximum score of the facial expression features across all faces was used for each emotion category. All the time courses of facial emotion features were resampled to 2 Hz. No facial emotion features were extracted for the fifth music block due to the absence of faces. The full list of the 48 facial emotion features is shown in <xref ref-type="fig" rid="fig4">Figure 4B</xref>.</p></sec><sec id="s4-6"><title>Encoding model fitting</title><p>To model iEEG responses to emotion, we used a linear regression approach with 48 facial emotion features extracted by Hume AI. Time-lagged versions of each feature (with 0, 0.5, and 1 s delays) were used in the model fitting. For each participant, high-frequency broadband (HFB) responses from all electrode contacts within each area were concatenated. To match the temporal resolution of the emotion feature time course, the HFB responses were binned into 500 ms windows. We then modeled the processed HFB response for each participant, each brain area, and each condition (speech vs music) using ridge regression. The optimal regularization parameter was assessed using fivefold cross-validation, with the 20 different regularization parameters (log spaced between 10 and 10000). To keep the scale of the weights consistent, a single best overall value of the regularization coefficient was used for all areas in both the speech and music conditions in all patients. We used a cross-validation iterator to fit the model and test it on held-out data. The model performance was evaluated by calculating Pearson correlation coefficients between measured and predicted HFB response of individual brain areas. The mean prediction accuracy (r value) of the encoding model with fivefold cross-validation was then calculated. A one-sample t-test was used to test whether the encoding model performance was significantly &gt;0. For children’s pSTC, non-parametric permutation tests were used to test whether the encoding model performance was significantly &gt;0 and whether there was a significant difference between groups. Specifically, we shuffled facial emotion feature data in time, and then we conducted the standard data analysis steps (described above) using the shuffled facial emotion features. This shuffle procedure was repeated 5000 times to generate a null distribution, and p-values were calculated as the proportion of results from shuffled data more extreme than the observed real value. A two-sided paired t-test was used to examine differences in encoding accuracy between speech and music conditions in the post-childhood group.</p></sec><sec id="s4-7"><title>Weight analysis</title><p>To examine the correlation between encoding model weights and age, we obtained 48 encoding model weights from all folds of cross-validation for all participants whose pSTC significantly encoded facial expression (i.e. the p-value of prediction accuracy is less than 0.05). Thus, 10 post-childhood individuals and 2 children were involved in the weight analysis. The weight for each feature represents its relative contribution to predicting the neural response. A higher weight indicates that the corresponding feature has a stronger influence on neural activity, meaning that variations in this feature more significantly impact the predicted response. We used the absolute value of weights and, therefore, did not discriminate whether facial emotion features were mapped to an increase or decrease in the HFB response.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Methodology, Writing - original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Formal analysis</p></fn><fn fn-type="con" id="con3"><p>Funding acquisition, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-107636-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>iEEG data is available in openneuro. Source code is available at OSF (<ext-link ext-link-type="uri" xlink:href="https://osf.io/d2th6">https://osf.io/d2th6</ext-link>). Figure 2-source data 1, Figure 3-source data 1 and Figure 4-source data 1 contain the numerical data used to generate the figures.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2025">2025</year><data-title>The representation of facial emotion expands from sensory to prefrontal cortex with development</data-title><source>Open Science Framework</source><pub-id pub-id-type="accession" xlink:href="https://osf.io/d2th6">d2th6</pub-id></element-citation></p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><name><surname>Berezutskaya</surname><given-names>J</given-names></name><name><surname>Vansteensel</surname><given-names>MJ</given-names></name><name><surname>Aarnoutse</surname><given-names>EJ</given-names></name><name><surname>Freudenburg</surname><given-names>ZV</given-names></name><name><surname>Piantoni</surname><given-names>G</given-names></name><name><surname>Branco</surname><given-names>MP</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film</data-title><source>OpenNeuro</source><pub-id pub-id-type="doi">10.18112/openneuro.ds003688.v1.0.7</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We would like to thank Dr. Julia Berezutskaya for providing the audiovisual film that was used for iEEG data collection. This work was supported by funding from the United States National Institutes of Health (R01-MH127006).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alba-Ferrara</surname><given-names>L</given-names></name><name><surname>Hausmann</surname><given-names>M</given-names></name><name><surname>Mitchell</surname><given-names>RL</given-names></name><name><surname>Weis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The neural correlates of emotional prosody comprehension: disentangling simple from complex emotion</article-title><source>PLOS ONE</source><volume>6</volume><elocation-id>e28701</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0028701</pub-id><pub-id pub-id-type="pmid">22174872</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arcaro</surname><given-names>MJ</given-names></name><name><surname>Schade</surname><given-names>PF</given-names></name><name><surname>Vincent</surname><given-names>JL</given-names></name><name><surname>Ponce</surname><given-names>CR</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Seeing faces is necessary for face-domain formation</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>1404</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1038/nn.4635</pub-id><pub-id pub-id-type="pmid">28869581</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrera</surname><given-names>ME</given-names></name><name><surname>Maurer</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The perception of facial expressions by the three-month-old</article-title><source>Child Development</source><volume>52</volume><fpage>203</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.2307/1129231</pub-id><pub-id pub-id-type="pmid">7238144</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barrett</surname><given-names>LF</given-names></name><name><surname>Mesquita</surname><given-names>B</given-names></name><name><surname>Gendron</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Context in emotion perception</article-title><source>Current Directions in Psychological Science</source><volume>20</volume><fpage>286</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1177/0963721411422522</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Batty</surname><given-names>M</given-names></name><name><surname>Taylor</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The development of emotional face processing during childhood</article-title><source>Developmental Science</source><volume>9</volume><fpage>207</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2006.00480.x</pub-id><pub-id pub-id-type="pmid">16472321</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berezutskaya</surname><given-names>J</given-names></name><name><surname>Vansteensel</surname><given-names>MJ</given-names></name><name><surname>Aarnoutse</surname><given-names>EJ</given-names></name><name><surname>Freudenburg</surname><given-names>ZV</given-names></name><name><surname>Piantoni</surname><given-names>G</given-names></name><name><surname>Branco</surname><given-names>MP</given-names></name><name><surname>Ramsey</surname><given-names>NF</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Open multimodal iEEG-fMRI dataset from naturalistic stimulation with a short audiovisual film</article-title><source>Scientific Data</source><volume>9</volume><elocation-id>91</elocation-id><pub-id pub-id-type="doi">10.1038/s41597-022-01173-0</pub-id><pub-id pub-id-type="pmid">35314718</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Two neural pathways of face processing: A critical evaluation of current models</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>55</volume><fpage>536</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2015.06.010</pub-id><pub-id pub-id-type="pmid">26067903</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burnett</surname><given-names>S</given-names></name><name><surname>Bird</surname><given-names>G</given-names></name><name><surname>Moll</surname><given-names>J</given-names></name><name><surname>Frith</surname><given-names>C</given-names></name><name><surname>Blakemore</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Development during adolescence of the neural processing of social emotion</article-title><source>Journal of Cognitive Neuroscience</source><volume>21</volume><fpage>1736</fpage><lpage>1750</lpage><pub-id pub-id-type="doi">10.1162/jocn.2009.21121</pub-id><pub-id pub-id-type="pmid">18823226</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burnett</surname><given-names>S</given-names></name><name><surname>Thompson</surname><given-names>S</given-names></name><name><surname>Bird</surname><given-names>G</given-names></name><name><surname>Blakemore</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pubertal development of the understanding of social emotions: Implications for education</article-title><source>Learning and Individual Differences</source><volume>21</volume><fpage>681</fpage><lpage>689</lpage><pub-id pub-id-type="doi">10.1016/j.lindif.2010.05.007</pub-id><pub-id pub-id-type="pmid">22211052</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Caron</surname><given-names>RF</given-names></name><name><surname>Caron</surname><given-names>AJ</given-names></name><name><surname>Myers</surname><given-names>RS</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Abstraction of invariant face expressions in infancy</article-title><source>Child Development</source><volume>53</volume><fpage>1008</fpage><lpage>1015</lpage><pub-id pub-id-type="pmid">7128251</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Collignon</surname><given-names>O</given-names></name><name><surname>Girard</surname><given-names>S</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Roy</surname><given-names>S</given-names></name><name><surname>Saint-Amour</surname><given-names>D</given-names></name><name><surname>Lassonde</surname><given-names>M</given-names></name><name><surname>Lepore</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Audio-visual integration of emotion expression</article-title><source>Brain Research</source><volume>1242</volume><fpage>126</fpage><lpage>135</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2008.04.023</pub-id><pub-id pub-id-type="pmid">18495094</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Desikan</surname><given-names>RS</given-names></name><name><surname>Ségonne</surname><given-names>F</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name><name><surname>Quinn</surname><given-names>BT</given-names></name><name><surname>Dickerson</surname><given-names>BC</given-names></name><name><surname>Blacker</surname><given-names>D</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name><name><surname>Dale</surname><given-names>AM</given-names></name><name><surname>Maguire</surname><given-names>RP</given-names></name><name><surname>Hyman</surname><given-names>BT</given-names></name><name><surname>Albert</surname><given-names>MS</given-names></name><name><surname>Killiany</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest</article-title><source>NeuroImage</source><volume>31</volume><fpage>968</fpage><lpage>980</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.01.021</pub-id><pub-id pub-id-type="pmid">16530430</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duchaine</surname><given-names>B</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>A revised neural framework for face processing</article-title><source>Annual Review of Vision Science</source><volume>1</volume><fpage>393</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1146/annurev-vision-082114-035518</pub-id><pub-id pub-id-type="pmid">28532371</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Engell</surname><given-names>AD</given-names></name><name><surname>Haxby</surname><given-names>JV</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Facial expression and gaze-direction in human superior temporal sulcus</article-title><source>Neuropsychologia</source><volume>45</volume><fpage>3234</fpage><lpage>3241</lpage><pub-id pub-id-type="doi">10.1016/j.neuropsychologia.2007.06.022</pub-id><pub-id pub-id-type="pmid">17707444</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fan</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>F</given-names></name><name><surname>Shao</surname><given-names>H</given-names></name><name><surname>Zhang</surname><given-names>P</given-names></name><name><surname>He</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The bottom-up and top-down processing of faces in the human occipitotemporal cortex</article-title><source>eLife</source><volume>9</volume><elocation-id>e48764</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.48764</pub-id><pub-id pub-id-type="pmid">31934855</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flack</surname><given-names>TR</given-names></name><name><surname>Andrews</surname><given-names>TJ</given-names></name><name><surname>Hymers</surname><given-names>M</given-names></name><name><surname>Al-Mosaiwi</surname><given-names>M</given-names></name><name><surname>Marsden</surname><given-names>SP</given-names></name><name><surname>Strachan</surname><given-names>JWA</given-names></name><name><surname>Trakulpipat</surname><given-names>C</given-names></name><name><surname>Wang</surname><given-names>L</given-names></name><name><surname>Wu</surname><given-names>T</given-names></name><name><surname>Young</surname><given-names>AW</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Responses in the right posterior superior temporal sulcus show a feature-based response to facial expression</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>69</volume><fpage>14</fpage><lpage>23</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2015.03.002</pub-id><pub-id pub-id-type="pmid">25967084</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Garcia</surname><given-names>NV</given-names></name><name><surname>Scherf</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Emerging sensitivity to socially complex expressions: a unique role for adolescence?</article-title><source>Child Development Perspectives</source><volume>9</volume><fpage>84</fpage><lpage>90</lpage><pub-id pub-id-type="doi">10.1111/cdep.12114</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Hacker</surname><given-names>CD</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gomez</surname><given-names>J</given-names></name><name><surname>Barnett</surname><given-names>MA</given-names></name><name><surname>Natu</surname><given-names>V</given-names></name><name><surname>Mezer</surname><given-names>A</given-names></name><name><surname>Palomero-Gallagher</surname><given-names>N</given-names></name><name><surname>Weiner</surname><given-names>KS</given-names></name><name><surname>Amunts</surname><given-names>K</given-names></name><name><surname>Zilles</surname><given-names>K</given-names></name><name><surname>Grill-Spector</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Microstructural proliferation in human cortex is coupled with the development of face processing</article-title><source>Science</source><volume>355</volume><fpage>68</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1126/science.aag0311</pub-id><pub-id pub-id-type="pmid">28059764</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gunning-Dixon</surname><given-names>FM</given-names></name><name><surname>Gur</surname><given-names>RC</given-names></name><name><surname>Perkins</surname><given-names>AC</given-names></name><name><surname>Schroeder</surname><given-names>L</given-names></name><name><surname>Turner</surname><given-names>T</given-names></name><name><surname>Turetsky</surname><given-names>BI</given-names></name><name><surname>Chan</surname><given-names>RM</given-names></name><name><surname>Loughead</surname><given-names>JW</given-names></name><name><surname>Alsop</surname><given-names>DC</given-names></name><name><surname>Maldjian</surname><given-names>J</given-names></name><name><surname>Gur</surname><given-names>RE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Age-related differences in brain activation during emotional face processing</article-title><source>Neurobiology of Aging</source><volume>24</volume><fpage>285</fpage><lpage>295</lpage><pub-id pub-id-type="doi">10.1016/s0197-4580(02)00099-4</pub-id><pub-id pub-id-type="pmid">12498962</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hariri</surname><given-names>AR</given-names></name><name><surname>Bookheimer</surname><given-names>SY</given-names></name><name><surname>Mazziotta</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Modulating emotional responses: effects of a neocortical network on the limbic system</article-title><source>Neuroreport</source><volume>11</volume><fpage>43</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1097/00001756-200001170-00009</pub-id><pub-id pub-id-type="pmid">10683827</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hariri</surname><given-names>AR</given-names></name><name><surname>Mattay</surname><given-names>VS</given-names></name><name><surname>Tessitore</surname><given-names>A</given-names></name><name><surname>Fera</surname><given-names>F</given-names></name><name><surname>Weinberger</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Neocortical modulation of the amygdala response to fearful stimuli</article-title><source>Biological Psychiatry</source><volume>53</volume><fpage>494</fpage><lpage>501</lpage><pub-id pub-id-type="doi">10.1016/s0006-3223(02)01786-9</pub-id><pub-id pub-id-type="pmid">12644354</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>H</given-names></name><name><surname>Brück</surname><given-names>C</given-names></name><name><surname>Domin</surname><given-names>M</given-names></name><name><surname>Lotze</surname><given-names>M</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>I can’t keep your face and voice out of my head: neural correlates of an attentional bias toward nonverbal emotional cues</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1460</fpage><lpage>1473</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhs417</pub-id><pub-id pub-id-type="pmid">23382516</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Johnston</surname><given-names>PJ</given-names></name><name><surname>Kaufman</surname><given-names>J</given-names></name><name><surname>Bajic</surname><given-names>J</given-names></name><name><surname>Sercombe</surname><given-names>A</given-names></name><name><surname>Michie</surname><given-names>PT</given-names></name><name><surname>Karayanidis</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Facial emotion and identity processing development in 5- to 15-year-old children</article-title><source>Frontiers in Psychology</source><volume>2</volume><elocation-id>26</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2011.00026</pub-id><pub-id pub-id-type="pmid">21713170</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaneshige</surname><given-names>T</given-names></name><name><surname>Haryu</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Categorization and understanding of facial expressions in 4‐month‐old infants</article-title><source>Japanese Psychological Research</source><volume>57</volume><fpage>135</fpage><lpage>142</lpage><pub-id pub-id-type="doi">10.1111/jpr.12075</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Klasen</surname><given-names>M</given-names></name><name><surname>Chen</surname><given-names>YH</given-names></name><name><surname>Mathiak</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Multisensory emotions: perception, combination and underlying neural processes</article-title><source>Reviews in the Neurosciences</source><volume>23</volume><fpage>381</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1515/revneuro-2012-0040</pub-id><pub-id pub-id-type="pmid">23089604</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kolb</surname><given-names>B</given-names></name><name><surname>Wilson</surname><given-names>B</given-names></name><name><surname>Taylor</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Developmental changes in the recognition and comprehension of facial expression: implications for frontal lobe function</article-title><source>Brain and Cognition</source><volume>20</volume><fpage>74</fpage><lpage>84</lpage><pub-id pub-id-type="doi">10.1016/0278-2626(92)90062-q</pub-id><pub-id pub-id-type="pmid">1389123</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kreifelts</surname><given-names>B</given-names></name><name><surname>Ethofer</surname><given-names>T</given-names></name><name><surname>Grodd</surname><given-names>W</given-names></name><name><surname>Erb</surname><given-names>M</given-names></name><name><surname>Wildgruber</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Audiovisual integration of emotional signals in voice and face: an event-related fMRI study</article-title><source>NeuroImage</source><volume>37</volume><fpage>1445</fpage><lpage>1456</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.06.020</pub-id><pub-id pub-id-type="pmid">17659885</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lawrence</surname><given-names>K</given-names></name><name><surname>Campbell</surname><given-names>R</given-names></name><name><surname>Skuse</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Age, gender, and puberty influence the development of facial emotion recognition</article-title><source>Frontiers in Psychology</source><volume>6</volume><elocation-id>761</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2015.00761</pub-id><pub-id pub-id-type="pmid">26136697</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lobaugh</surname><given-names>NJ</given-names></name><name><surname>Gibson</surname><given-names>E</given-names></name><name><surname>Taylor</surname><given-names>MJ</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Children recruit distinct neural systems for implicit emotional face processing</article-title><source>Neuroreport</source><volume>17</volume><fpage>215</fpage><lpage>219</lpage><pub-id pub-id-type="doi">10.1097/01.wnr.0000198946.00445.2f</pub-id><pub-id pub-id-type="pmid">16407774</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monk</surname><given-names>CS</given-names></name><name><surname>McClure</surname><given-names>EB</given-names></name><name><surname>Nelson</surname><given-names>EE</given-names></name><name><surname>Zarahn</surname><given-names>E</given-names></name><name><surname>Bilder</surname><given-names>RM</given-names></name><name><surname>Leibenluft</surname><given-names>E</given-names></name><name><surname>Charney</surname><given-names>DS</given-names></name><name><surname>Ernst</surname><given-names>M</given-names></name><name><surname>Pine</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Adolescent immaturity in attention-related brain engagement to emotional facial expressions</article-title><source>NeuroImage</source><volume>20</volume><fpage>420</fpage><lpage>428</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(03)00355-0</pub-id><pub-id pub-id-type="pmid">14527602</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motta-Mena</surname><given-names>NV</given-names></name><name><surname>Scherf</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Pubertal development shapes perception of complex facial expressions</article-title><source>Developmental Science</source><volume>20</volume><elocation-id>12451</elocation-id><pub-id pub-id-type="doi">10.1111/desc.12451</pub-id><pub-id pub-id-type="pmid">27321445</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>CA</given-names></name><name><surname>Morse</surname><given-names>PA</given-names></name><name><surname>Leavitt</surname><given-names>LA</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Recognition of facial expressions by seven-month-old infants</article-title><source>Child Development</source><volume>50</volume><fpage>1239</fpage><lpage>1242</lpage><pub-id pub-id-type="doi">10.2307/1129358</pub-id><pub-id pub-id-type="pmid">535438</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>CA</given-names></name><name><surname>Dolgin</surname><given-names>KG</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>The generalized discrimination of facial expressions by seven-month-old infants</article-title><source>Child Development</source><volume>56</volume><fpage>58</fpage><lpage>61</lpage><pub-id pub-id-type="pmid">3987408</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Parvizi</surname><given-names>J</given-names></name><name><surname>Kastner</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Promises and limitations of human intracranial electroencephalography</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>474</fpage><lpage>483</lpage><pub-id pub-id-type="doi">10.1038/s41593-018-0108-2</pub-id><pub-id pub-id-type="pmid">29507407</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>MR</given-names></name><name><surname>Barbosa</surname><given-names>F</given-names></name><name><surname>de Haan</surname><given-names>M</given-names></name><name><surname>Ferreira-Santos</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Understanding the development of face and emotion processing under a predictive processing framework</article-title><source>Developmental Psychology</source><volume>55</volume><fpage>1868</fpage><lpage>1881</lpage><pub-id pub-id-type="doi">10.1037/dev0000706</pub-id><pub-id pub-id-type="pmid">31464491</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollak</surname><given-names>SD</given-names></name><name><surname>Messner</surname><given-names>M</given-names></name><name><surname>Kistler</surname><given-names>DJ</given-names></name><name><surname>Cohn</surname><given-names>JF</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Development of perceptual expertise in emotion recognition</article-title><source>Cognition</source><volume>110</volume><fpage>242</fpage><lpage>247</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2008.10.010</pub-id><pub-id pub-id-type="pmid">19059585</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rodger</surname><given-names>H</given-names></name><name><surname>Vizioli</surname><given-names>L</given-names></name><name><surname>Ouyang</surname><given-names>X</given-names></name><name><surname>Caldara</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Mapping the development of facial expression recognition</article-title><source>Developmental Science</source><volume>18</volume><fpage>926</fpage><lpage>939</lpage><pub-id pub-id-type="doi">10.1111/desc.12281</pub-id><pub-id pub-id-type="pmid">25704672</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Romani-Sponchiado</surname><given-names>A</given-names></name><name><surname>Maia</surname><given-names>CP</given-names></name><name><surname>Torres</surname><given-names>CN</given-names></name><name><surname>Tavares</surname><given-names>I</given-names></name><name><surname>Arteche</surname><given-names>AX</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Emotional face expressions recognition in childhood: developmental markers, age and sex effect</article-title><source>Cognitive Processing</source><volume>23</volume><fpage>467</fpage><lpage>477</lpage><pub-id pub-id-type="doi">10.1007/s10339-022-01086-1</pub-id><pub-id pub-id-type="pmid">35362838</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ruba</surname><given-names>AL</given-names></name><name><surname>Pollak</surname><given-names>SD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The development of emotion reasoning in infancy and early childhood</article-title><source>Annual Review of Developmental Psychology</source><volume>2</volume><fpage>503</fpage><lpage>531</lpage><pub-id pub-id-type="doi">10.1146/annurev-devpsych-060320-102556</pub-id><pub-id pub-id-type="pmid">40277873</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russell</surname><given-names>JA</given-names></name><name><surname>Paris</surname><given-names>FA</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Do children acquire concepts for complex emotions abruptly?</article-title><source>International Journal of Behavioral Development</source><volume>17</volume><fpage>349</fpage><lpage>365</lpage><pub-id pub-id-type="doi">10.1177/016502549401700207</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tessitore</surname><given-names>A</given-names></name><name><surname>Hariri</surname><given-names>AR</given-names></name><name><surname>Fera</surname><given-names>F</given-names></name><name><surname>Smith</surname><given-names>WG</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Weinberger</surname><given-names>DR</given-names></name><name><surname>Mattay</surname><given-names>VS</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Functional changes in the activity of brain regions underlying emotion processing in the elderly</article-title><source>Psychiatry Research</source><volume>139</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.pscychresns.2005.02.009</pub-id><pub-id pub-id-type="pmid">15936178</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thomas</surname><given-names>LA</given-names></name><name><surname>De Bellis</surname><given-names>MD</given-names></name><name><surname>Graham</surname><given-names>R</given-names></name><name><surname>LaBar</surname><given-names>KS</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Development of emotional facial recognition in late childhood and adolescence</article-title><source>Developmental Science</source><volume>10</volume><fpage>547</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1111/j.1467-7687.2007.00614.x</pub-id><pub-id pub-id-type="pmid">17683341</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walker-Andrews</surname><given-names>AS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Infants’ perception of expressive behaviors: differentiation of multimodal information</article-title><source>Psychological Bulletin</source><volume>121</volume><fpage>437</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.121.3.437</pub-id><pub-id pub-id-type="pmid">9136644</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>S</given-names></name><name><surname>Tudusciuc</surname><given-names>O</given-names></name><name><surname>Mamelak</surname><given-names>AN</given-names></name><name><surname>Ross</surname><given-names>IB</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name><name><surname>Rutishauser</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neurons in the human amygdala selective for perceived emotion</article-title><source>PNAS</source><volume>111</volume><fpage>E3110</fpage><lpage>E3119</lpage><pub-id pub-id-type="doi">10.1073/pnas.1323342111</pub-id><pub-id pub-id-type="pmid">24982200</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weissman</surname><given-names>DH</given-names></name><name><surname>Warner</surname><given-names>LM</given-names></name><name><surname>Woldorff</surname><given-names>MG</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The neural mechanisms for minimizing cross-modal distraction</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>10941</fpage><lpage>10949</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3669-04.2004</pub-id><pub-id pub-id-type="pmid">15574744</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Williams</surname><given-names>LM</given-names></name><name><surname>Brown</surname><given-names>KJ</given-names></name><name><surname>Palmer</surname><given-names>D</given-names></name><name><surname>Liddell</surname><given-names>BJ</given-names></name><name><surname>Kemp</surname><given-names>AH</given-names></name><name><surname>Olivieri</surname><given-names>G</given-names></name><name><surname>Peduto</surname><given-names>A</given-names></name><name><surname>Gordon</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The mellow years?: neural basis of improving emotional stability over age</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>6422</fpage><lpage>6430</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0022-06.2006</pub-id><pub-id pub-id-type="pmid">16775129</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yurgelun-Todd</surname><given-names>DA</given-names></name><name><surname>Killgore</surname><given-names>WDS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Fear-related activity in the prefrontal cortex increases with age during adolescence: a preliminary fMRI study</article-title><source>Neuroscience Letters</source><volume>406</volume><fpage>194</fpage><lpage>199</lpage><pub-id pub-id-type="doi">10.1016/j.neulet.2006.07.046</pub-id><pub-id pub-id-type="pmid">16942837</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yurgelun-Todd</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Emotional and cognitive changes during adolescence</article-title><source>Current Opinion in Neurobiology</source><volume>17</volume><fpage>251</fpage><lpage>257</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2007.03.009</pub-id><pub-id pub-id-type="pmid">17383865</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107636.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Hu</surname><given-names>Xiaoqing</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>University of Hong Kong</institution><country>China</country></aff></contrib></contrib-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group></front-stub><body><p>This study examines an <bold>important</bold> question regarding the developmental trajectory of neural mechanisms supporting facial expression processing. Leveraging a rare intracranial EEG (iEEG) dataset including both children and adults, the authors reported that facial expression recognition mainly engaged the posterior superior temporal cortex (pSTC) among children, while both pSTC and the prefrontal cortex were engaged among adults. In terms of strength of evidence, the <bold>solid</bold> methods, data and analyses broadly support the claims with minor weaknesses.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107636.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This study investigates how the brain processes facial expressions across development by analyzing intracranial EEG (iEEG) data from children (ages 5-10) and post-childhood individuals (ages 13-55). The researchers used a short film containing emotional facial expressions and applied AI-based models to decode brain responses to facial emotions. They found that in children, facial emotion information is represented primarily in the posterior superior temporal cortex (pSTC)-a sensory processing area-but not in the dorsolateral prefrontal cortex (DLPFC), which is involved in higher-level social cognition. In contrast, post-childhood individuals showed emotion encoding in both regions. Importantly, the complexity of emotions encoded in the pSTC increased with age, particularly for socially nuanced emotions like embarrassment, guilt, and pride.The authors claim that these findings suggest that emotion recognition matures through increasing involvement of the prefrontal cortex, supporting a developmental trajectory where top-down modulation enhances understanding of complex emotions as children grow older.</p><p>Strengths:</p><p>(1) The inclusion of pediatric iEEG makes this study uniquely positioned to offer high-resolution temporal and spatial insights into neural development compared to non-invasive approaches, e.g., fMRI, scalp EEG, etc.</p><p>(2) Using a naturalistic film paradigm enhances ecological validity compared to static image tasks often used in emotion studies.</p><p>(3) The idea of using state-of-the-art AI models to extract facial emotion features allows for high-dimensional and dynamic emotion labeling in real time.</p><p>Weaknesses:</p><p>(1) The study has notable limitations that constrain the generalizability and depth of its conclusions. The sample size was very small, with only nine children included and just two having sufficient electrode coverage in the posterior superior temporal cortex (pSTC), which weakens the reliability and statistical power of the findings, especially for analyses involving age. Authors pointed out that a similar sample size has been used in previous iEEG studies, but the cited works focus on adults and do not look at the developmental perspectives. Similar work looking at developmental changes in iEEG signals usually includes many more subjects (e.g., n = 101 children from Cross ZR et al., Nature Human Behavior, 2025) to account for inter-subject variabilities.</p><p>(2) Electrode coverage was also uneven across brain regions, with not all participants having electrodes in both the dorsolateral prefrontal cortex (DLPFC) and pSTC, making the conclusion regarding the different developmental changes between DLPFC and pSTC hard to interpret (related to point 3 below). It is understood that it is rare to have such iEEG data collected in this age group, and the electrode location is only determined by clinical needs. However, the scientific rigor should not be compromised by the limited data access. It's the authors' decision whether such an approach is valid and appropriate to address the scientific questions, here the developmental changes in the brain, given all the advantages and constraints of the data modality.</p><p>(3) The developmental differences observed were based on cross-sectional comparisons rather than longitudinal data, reducing the ability to draw causal conclusions about developmental trajectories. Also, see comments in point 2.</p><p>(4) Moreover, the analysis focused narrowly on DLPFC, neglecting other relevant prefrontal areas such as the orbitofrontal cortex (OFC) and anterior cingulate cortex (ACC), which play key roles in emotion and social processing. Agree that this might be beyond the scope of this paper, but a discussion section might be insightful.</p><p>(5) Although the use of a naturalistic film stimulus enhances ecological validity, it comes at the cost of experimental control, with no behavioral confirmation of the emotions perceived by participants and uncertain model validity for complex emotional expressions in children. A non-facial music block that could have served as a control was available but not analyzed. The validation of AI model's emotional output needs to be tested. It is understood that we cannot collect these behavioral data retrospectively within the recorded subjects. Maybe potential post-hoc experiments and analyses could be done, e.g., collect behavioral, emotional perception data from age-matched healthy subjects.</p><p>(6) Generalizability is further limited by the fact that all participants were neurosurgical patients, potentially with neurological conditions such as epilepsy that may influence brain responses. At least some behavioral measures between the patient population and the healthy groups should be done to ensure the perception of emotions is similar.</p><p>(7) Additionally, the high temporal resolution of intracranial EEG was not fully utilized, as data were downsampled and averaged in 500-ms windows. It seems like the authors are trying to compromise the iEEG data analyses to match up with the AI's output resolution, which is 2Hz. It is not clear then why not directly use fMRI, which is non-invasive and seems to meet the needs here already. The advantages of using iEEG in this study are missing here.</p><p>(8) Finally, the absence of behavioral measures or eye-tracking data makes it difficult to directly link neural activity to emotional understanding or determine which facial features participants attended to. Related to point 5 as well.</p><p>Comments on revisions:</p><p>A behavioral measurement will help address a lot of these questions. If the data continues collecting, additional subjects with iEEG recording and also behavioral measurements would be valuable.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107636.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>In this paper, Fan et al. aim to characterize how neural representations of facial emotions evolve from childhood to adulthood. Using intracranial EEG recordings from participants aged 5 to 55, the authors assess the encoding of emotional content in high-level cortical regions. They report that while both the posterior superior temporal cortex (pSTC) and dorsolateral prefrontal cortex (DLPFC) are involved in representing facial emotions in older individuals, only the pSTC shows significant encoding in children. Moreover, the encoding of complex emotions in the pSTC appears to strengthen with age. These findings lead the authors to suggest that young children rely more on low-level sensory areas and propose a developmental shift from reliance on lower-level sensory areas in early childhood to increased top-down modulation by the prefrontal cortex as individuals mature.</p><p>Strengths:</p><p>(1) Rare and valuable dataset: The use of intracranial EEG recordings in a developmental sample is highly unusual and provides a unique opportunity to investigate neural dynamics with both high spatial and temporal resolution.</p><p>(2) Developmentally relevant design: The broad age range and cross-sectional design are well-suited to explore age-related changes in neural representations.</p><p>(3) Ecological validity: The use of naturalistic stimuli (movie clips) increases the ecological relevance of the findings.</p><p>(4) Feature-based analysis: The authors employ AI-based tools to extract emotion-related features from naturalistic stimuli, which enables a data-driven approach to decoding neural representations of emotional content. This method allows for a more fine-grained analysis of emotion processing beyond traditional categorical labels.</p><p>Weaknesses:</p><p>(1) While the authors leverage Hume AI, a tool pre-trained on a large dataset, its specific performance on the stimuli used in this study remains unverified. To strengthen the foundation of the analysis, it would be important to confirm that Hume AI's emotional classifications align with human perception for these particular videos. A straightforward way to address this would be to recruit human raters to evaluate the emotional content of the stimuli and compare their ratings to the model's outputs.</p><p>(2) Although the study includes data from four children with pSTC coverage-an increase from the initial submission-the sample size remains modest compared to recent iEEG studies in the field.</p><p>(3) The &quot;post-childhood&quot; group (ages 13-55) conflates several distinct neurodevelopmental periods, including adolescence, young adulthood, and middle adulthood. As a finer age stratification is likely not feasible with the current sample size, I would suggest authors temper their developmental conclusions.</p><p>(4) The analysis of DLPFC-pSTC directional connectivity would be significantly strengthened by modeling it as a continuous function of age across all participants, rather than relying on an unbalanced comparison between a single child and a (N=7) post-childhood group. This continuous approach would provide a more powerful and nuanced view of the developmental trajectory. I would also suggest including the result in the main text.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.107636.3.sa3</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Fan</surname><given-names>Xiaoxu</given-names></name><role specific-use="author">Author</role><aff><institution>Baylor College of Medicine</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Tripathi</surname><given-names>Abhishek</given-names></name><role specific-use="author">Author</role><aff><institution>Rice University</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Bijanki</surname><given-names>Kelly</given-names></name><role specific-use="author">Author</role><aff><institution>Baylor College of Medicine</institution><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>eLife Assessment</bold></p><p>This study examines a valuable question regarding the developmental trajectory of neural mechanisms supporting facial expression processing. Leveraging a rare intracranial EEG (iEEG) dataset including both children and adults, the authors reported that facial expression recognition mainly engaged the posterior superior temporal cortex (pSTC) among children, while both pSTC and the prefrontal cortex were engaged among adults. However, the sample size is relatively small, with analyses appearing incomplete to fully support the primary claims.</p><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public review):</bold></p><p>Summary:</p><p>This study investigates how the brain processes facial expressions across development by analyzing intracranial EEG (iEEG) data from children (ages 5-10) and post-childhood individuals (ages 13-55). The researchers used a short film containing emotional facial expressions and applied AI-based models to decode brain responses to facial emotions. They found that in children, facial emotion information is represented primarily in the posterior superior temporal cortex (pSTC) - a sensory processing area - but not in the dorsolateral prefrontal cortex (DLPFC), which is involved in higher-level social cognition. In contrast, post-childhood individuals showed emotion encoding in both regions. Importantly, the complexity of emotions encoded in the pSTC increased with age, particularly for socially nuanced emotions like embarrassment, guilt, and pride. The authors claim that these findings suggest that emotion recognition matures through increasing involvement of the prefrontal cortex, supporting a developmental trajectory where top-down modulation enhances understanding of complex emotions as children grow older.</p><p>Strengths:</p><p>(1) The inclusion of pediatric iEEG makes this study uniquely positioned to offer high-resolution temporal and spatial insights into neural development compared to non-invasive approaches, e.g., fMRI, scalp EEG, etc.</p><p>(2) Using a naturalistic film paradigm enhances ecological validity compared to static image tasks often used in emotion studies.</p><p>(3) The idea of using state-of-the-art AI models to extract facial emotion features allows for high-dimensional and dynamic emotion labeling in real time</p><p>Weaknesses:</p><p>(1) The study has notable limitations that constrain the generalizability and depth of its conclusions. The sample size was very small, with only nine children included and just two having sufficient electrode coverage in the posterior superior temporal cortex (pSTC), which weakens the reliability and statistical power of the findings, especially for analyses involving age</p></disp-quote><p>We appreciated the reviewer’s point regarding the constrained sample size.</p><p>As an invasive method, iEEG recordings can only be obtained from patients undergoing electrode implantation for clinical purposes. Thus, iEEG data from young children are extremely rare, and rapidly increasing the sample size within a few years is not feasible. However, we are confident in the reliability of our main conclusions. Specifically, 8 children (53 recording contacts in total) and 13 control participants (99 recording contacts in total) with electrode coverage in the DLPFC are included in our DLPFC analysis. This sample size is comparable to other iEEG studies with similar experiment designs [1-3].</p><p>For pSTC, we returned to the data set and found another two children who had pSTC coverage. After involving these children’s data, the group-level analysis using permutation test showed that children’s pSTC significantly encode facial emotion in naturalistic contexts (Figure 3B). Notably, the two new children’s (S33 and S49) responses were highly consistent with our previous observations. Moreover, the averaged prediction accuracy in children’s pSTC (r<sub>speech</sub>=0.1565) was highly comparable to that in post-childhood group (r<sub>speech</sub>=0.1515).</p><p>(1) Zheng, J. et al. Multiplexing of Theta and Alpha Rhythms in the Amygdala-Hippocampal Circuit Supports Pafern Separation of Emotional Information. Neuron 102, 887-898.e5 (2019).</p><p>(2) Diamond, J. M. et al. Focal seizures induce spatiotemporally organized spiking activity in the human cortex. Nat. Commun. 15, 7075 (2024).</p><p>(3) Schrouff, J. et al. Fast temporal dynamics and causal relevance of face processing in the human temporal cortex. Nat. Commun. 11, 656 (2020).</p><disp-quote content-type="editor-comment"><p>(2) Electrode coverage was also uneven across brain regions, with not all participants having electrodes in both the dorsolateral prefrontal cortex (DLPFC) and pSTC, and most coverage limited to the left hemisphere-hindering within-subject comparisons and limiting insights into lateralization.</p></disp-quote><p>The electrode coverage in each patient is determined entirely by the clinical needs. Only a few patients have electrodes in both DLPFC and pSTC because these two regions are far apart, so it’s rare for a single patient’s suspected seizure network to span such a large territory. However, it does not affect our results, as most iEEG studies combine data from multiple patients to achieve sufficient electrode coverage in each target brain area. As our data are mainly from left hemisphere (due to the clinical needs), this study was not designed to examine whether there is a difference between hemispheres in emotion encoding. Nevertheless, lateralization remains an interesting question that should be addressed in future research, and we have noted this limitation in the Discussion (Page 8, in the last paragraph of the Discussion).</p><disp-quote content-type="editor-comment"><p>(3) The developmental differences observed were based on cross-sectional comparisons rather than longitudinal data, reducing the ability to draw causal conclusions about developmental trajectories.</p></disp-quote><p>In the context of pediatric intracranial EEG, longitudinal data collection is not feasible due to the invasive nature of electrode implantation. We have added this point to the Discussion to acknowledge that while our results reveal robust age-related differences in the cortical encoding of facial emotions, longitudinal studies using non-invasive methods will be essential to directly track developmental trajectories (Page 8, in the last paragraph of Discussion). In addition, we revised our manuscript to avoid emphasis causal conclusions about developmental trajectories in the current study (For example, we use “imply” instead of “suggest” in the fifth paragraph of Discussion).</p><disp-quote content-type="editor-comment"><p>(4) Moreover, the analysis focused narrowly on DLPFC, neglecting other relevant prefrontal areas such as the orbitofrontal cortex (OFC) and anterior cingulate cortex (ACC), which play key roles in emotion and social processing.</p></disp-quote><p>We agree that both OFC and ACC are critically involved in emotion and social processing. However, we have no recordings from these areas because ECoG rarely covers the ACC or OFC due to technical constraints. We have noted this limitation in the Discussion(Page 8, in the last paragraph of Discussion). Future follow-up studies using sEEG or non-invasive imaging methods could be used to examine developmental patterns in these regions.</p><disp-quote content-type="editor-comment"><p>(5) Although the use of a naturalistic film stimulus enhances ecological validity, it comes at the cost of experimental control, with no behavioral confirmation of the emotions perceived by participants and uncertain model validity for complex emotional expressions in children. A nonfacial music block that could have served as a control was available but not analyzed.</p></disp-quote><p>The facial emotion features used in our encoding models were extracted by Hume AI models, which were trained on human intensity ratings of large-scale, experimentally controlled emotional expression data [1-2]. Thus, the outputs of Hume AI model reflect what typical facial expressions convey, that is, the presented facial emotion. Our goal of the present study was to examine how facial emotions presented in the videos are encoded in the human brain at different developmental stages. We agree that children’s interpretation of complex emotions may differ from that of adults, resulting in different perceived emotion (i.e., the emotion that the observer subjectively interprets). Behavioral ratings are necessary to study the encoding of subjectively perceived emotion, which is a very interesting direction but beyond the scope of the present work. We have added a paragraph in the Discussion (see Page 8) to explicitly note that our study focused on the encoding of presented emotion.</p><p>We appreciated the reviewer’s point regarding the value of non-facial music blocks. However, although there are segments in music condition that have no faces presented, these cannot be used as a control condition to test whether the encoding model’s prediction accuracy in pSTC or DLPFC drops to chance when no facial emotion is present. This is because, in the absence of faces, no extracted emotion features are available to be used for the construction of encoding model (see Author response image 1 below). Thus, we chose to use a different control analysis for the present work. For children’s pSTC, we shuffled facial emotion feature in time to generate a null distribution, which was then used to test the statistical significance of the encoding models (see Methods/Encoding model fitting for details).</p><p>(1) Brooks, J. A. et al. Deep learning reveals what facial expressions mean to people in different cultures. iScience 27, 109175 (2024).</p><p>(2) Brooks, J. A. et al. Deep learning reveals what vocal bursts express in different cultures. Nat. Hum. Behav. 7, 240–250 (2023).</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><caption><title>Time courses of Hume AI extracted facial expression features for the first block of music condition.</title><p>Only top 5 facial expressions were shown here to due to space limitation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107636-sa3-fig1-v1.tif"/><permissions><copyright-statement>© 1970, Beta Film GmbH. All rights reserved</copyright-statement><copyright-year>1970</copyright-year><copyright-holder>Beta Film GmbH</copyright-holder><license><license-p>Screenshots in panel A are taken from 'Pippi on the Run' (1970). These are not covered by the CC-BY 4.0 license.</license-p></license></permissions></fig><disp-quote content-type="editor-comment"><p>(6) Generalizability is further limited by the fact that all participants were neurosurgical patients, potentially with neurological conditions such as epilepsy that may influence brain responses.</p></disp-quote><p>We appreciated the reviewer’s point. However, iEEG data can only be obtained from clinical populations (usually epilepsy patients) who have electrodes implantation. Given current knowledge about focal epilepsy and its potential effects on brain activity, researchers believe that epilepsy-affected brains can serve as a reasonable proxy for normal human brains when confounding influences are minimized through rigorous procedures [1]. In our study, we took several steps to ensure data quality: (1) all data segments containing epileptiform discharges were identified and removed at the very beginning of preprocessing, (2) patients were asked to participate the experiment several hours outside the window of seizures. Please see Method for data quality check description (Page 9/ Experimental procedures and iEEG data processing).</p><p>(1) Parvizi J, Kastner S. 2018. Promises and limitations of human intracranial electroencephalography. Nat Neurosci 21:474–483. doi:10.1038/s41593-018-0108-2</p><disp-quote content-type="editor-comment"><p>(7) Additionally, the high temporal resolution of intracranial EEG was not fully utilized, as data were down-sampled and averaged in 500-ms windows.</p></disp-quote><p>We agree that one of the major advantages of iEEG is its millisecond-level temporal resolution. In our case, the main reason for down-sampling was that the time series of facial emotion features extracted from the videos had a temporal resolution of 2 Hz, which were used for the modelling neural responses. In naturalistic contexts, facial emotion features do not change on a millisecond timescale, so a 500 ms window is sufficient to capture the relevant dynamics. Another advantage of iEEG is its tolerance to motion, which is excessive in young children (e.g., 5-year-olds). This makes our dataset uniquely valuable, suggesting robust representation in the pSTC but not in the DLPFC in young children. Moreover, since our method framework (Figure 1) does not rely on high temporal resolution method, so it can be transferred to non-invasive modalities such as fMRI, enabling future studies to test these developmental patterns in larger populations.</p><disp-quote content-type="editor-comment"><p>(8) Finally, the absence of behavioral measures or eye-tracking data makes it difficult to directly link neural activity to emotional understanding or determine which facial features participants afended to.</p></disp-quote><p>We appreciated this point. Part of our rationale is presented in our response to (5) for the absence of behavioral measures. Following the same rationale, identifying which facial features participants attended to is not necessary for testing our main hypotheses because our analyses examined responses to the overall emotional content of the faces. However, we agree and recommend future studies use eye-tracking and corresponding behavioral measures in studies of subjective emotional understanding.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public review):</bold></p><p>Summary:</p><p>In this paper, Fan et al. aim to characterize how neural representations of facial emotions evolve from childhood to adulthood. Using intracranial EEG recordings from participants aged 5 to 55, the authors assess the encoding of emotional content in high-level cortical regions. They report that while both the posterior superior temporal cortex (pSTC) and dorsolateral prefrontal cortex (DLPFC) are involved in representing facial emotions in older individuals, only the pSTC shows significant encoding in children. Moreover, the encoding of complex emotions in the pSTC appears to strengthen with age. These findings lead the authors to suggest that young children rely more on low-level sensory areas and propose a developmental shiZ from reliance on lower-level sensory areas in early childhood to increased top-down modulation by the prefrontal cortex as individuals mature.</p><p>Strengths:</p><p>(1) Rare and valuable dataset: The use of intracranial EEG recordings in a developmental sample is highly unusual and provides a unique opportunity to investigate neural dynamics with both high spatial and temporal resolution.</p><p>(2) Developmentally relevant design: The broad age range and cross-sectional design are well-suited to explore age-related changes in neural representations.</p><p>(3) Ecological validity: The use of naturalistic stimuli (movie clips) increases the ecological relevance of the findings.</p><p>(4) Feature-based analysis: The authors employ AIbased tools to extract emotion-related features from naturalistic stimuli, which enables a data-driven approach to decoding neural representations of emotional content. This method allows for a more fine-grained analysis of emotion processing beyond traditional categorical labels.</p><p>Weaknesses:</p><p>(1) The emotional stimuli included facial expressions embedded in speech or music, making it difficult to isolate neural responses to facial emotion per se from those related to speech content or music-induced emotion.</p></disp-quote><p>We thank the reviewer for their raising this important point. We agree that in naturalistic settings, face often co-occur with speech, and that these sources of emotion can overlap. However, background music induced emotions have distinct temporal dynamics which are separable from facial emotion (See the Author response image 2 (A) and (B) below). In addition, face can convey a wide range of emotions (48 categories in Hume AI model), whereas music conveys far fewer (13 categories reported by a recent study [1]). Thus, when using facial emotion feature time series as regressors (with 48 emotion categories and rapid temporal dynamics), the model performance will reflect neural encoding of facial emotion in the music condition, rather than the slower and lower-dimensional emotion from music.</p><p>For the speech condition, we acknowledge that it is difficult to fully isolate neural responses to facial emotion from those to speech when the emotional content from faces and speech highly overlaps. However, in our study, (1) the time courses of emotion features from face and voice are still different (Author response image 2 (C) and (D)), (2) our main finding that DLPFC encodes facial expression information in postchildhood individuals but not in young children was found in both speech and music condition (Figure 2B and 2C). In music condition, neural responses to facial emotion are not affected by speech. Thus, we have included the DLPFC results from the music condition in the revised manuscript (Figure 2C), and we acknowledge that this issue should be carefully considered in future studies using videos with speech, as we have indicated in the future directions in the last paragraph of Discussion.</p><p>(1) Cowen, A. S., Fang, X., Sauter, D. &amp; Keltner, D. What music makes us feel: At least 13 dimensions organize subjective experiences associated with music across different cultures. Proc Natl Acad Sci USA 117, 1924–1934 (2020).</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><caption><title>Time courses of the amusement.</title><p>(A) and (B) Amusement conveyed by face or music in a 30-s music block. Facial emotion features are extracted by Hume AI. For emotion from music, we approximated the amusement time course using a weighted combination of low-level acoustic features (RMS energy, spectral centroid, MFCCs), which capture intensity, brightness, and timbre cues linked to amusement. Notice that music continues when there are no faces presented. (C) and (D) Amusement conveyed by face or voice in a 30-s speech block. From 0 to 5 s, a girl is introducing her friend to a stranger. The camera focuses on the friend, who appears nervous, while the girl’s voice sounds cheerful. This mismatch explains why the shapes of the two time series differ at the beginning. Such situations occur frequently in naturalistic movies</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107636-sa3-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(2) While the authors leveraged Hume AI to extract facial expression features from the video stimuli, they did not provide any validation of the tool's accuracy or reliability in the context of their dataset. It remains unclear how well the AI-derived emotion ratings align with human perception, particularly given the complexity and variability of naturalistic stimuli. Without such validation, it is difficult to assess the interpretability and robustness of the decoding results based on these features.</p></disp-quote><p>Hume AI models were trained and validated by human intensity ratings of large-scale, experimentally controlled emotional expression data [1-2]. The training process used both manual annotations from human raters and deep neural networks. Over 3000 human raters categorized facial expressions into emotion categories and rated on a 1-100 intensity scale. Thus, the outputs of Hume AI model reflect what typical facial expressions convey (based on how people actually interpret them), that is, the presented facial emotion. Our goal of the present study was to examine how facial emotions presented in the videos are encoded in the human brain at different developmental stages. We agree that the interpretation of facial emotions may be different in individual participants, resulting in different perceived emotion (i.e., the emotion that the observer subjectively interprets). Behavioral ratings are necessary to study the encoding of subjectively perceived emotion, which is a very interesting direction but beyond the scope of the present work. We have added text in the Discussion to explicitly note that our study focused on the encoding of presented emotion (second paragraph in Page 8).</p><p>(1) Brooks, J. A. et al. Deep learning reveals what facial expressions mean to people in different cultures. iScience 27, 109175 (2024).</p><p>(2) Brooks, J. A. et al. Deep learning reveals what vocal bursts express in different cultures. Nat. Hum. Behav. 7, 240–250 (2023).</p><disp-quote content-type="editor-comment"><p>(3) Only two children had relevant pSTC coverage, severely limiting the reliability and generalizability of results.</p></disp-quote><p>We appreciated this point and agreed with both reviewers who raised it as a significant concern. As described in response to reviewer 1 (comment 1), we have added data from another two children who have pSTC coverage. Group-level analysis using permutation test showed that children’s pSTC significantly encode facial emotion in naturalistic contexts (Figure 3B). Because iEEG data from young children are extremely rare, rapidly increasing the sample size within a few years is not feasible. However, we are confident in the reliability of our conclusion that children’s pSTC can encode facial emotion. First, the two new children’s responses (S33 and S49) from pSTC were highly consistent with our previous observations (see individual data in Figure 3B). Second, the averaged prediction accuracy in children’s pSTC (r<sub>speech</sub>=0.1565) was highly comparable to that in post-childhood group (r<sub>speech</sub>=0.1515).</p><disp-quote content-type="editor-comment"><p>(4) The rationale for focusing exclusively on high-frequency activity for decoding emotion representations is not provided, nor are results from other frequency bands explored.</p></disp-quote><p>We focused on high-frequency broadband (HFB) activity because it is widely considered to reflect the responses of local neuronal populations near the recording electrode, whereas low-frequency oscillations in the theta, alpha, and beta ranges are thought to serve as carrier frequencies for long-range communication across distributed networks[1-2]. Since our study aimed to examine the representation of facial emotion in localized cortical regions (DLPFC and pSTC), HFB activity provides the most direct measure of the relevant neural responses. We have added this rationale to the manuscript (Page 3).</p><p>(1) Parvizi, J. &amp; Kastner, S. Promises and limitations of human intracranial electroencephalography. Nat. Neurosci. 21, 474–483 (2018).</p><p>(2) Buzsaki, G. Rhythms of the Brain. (Oxford University Press, Oxford, 200ti).</p><disp-quote content-type="editor-comment"><p>(5) The hypothesis of developmental emergence of top-down prefrontal modulation is not directly tested. No connectivity or co-activation analyses are reported, and the number of participants with simultaneous coverage of pSTC and DLPFC is not specified.</p></disp-quote><p>Directional connectivity analysis results were not shown because only one child has simultaneous coverage of pSTC and DLPFC. However, the Granger Causality results from post-childhood group (N=7) clearly showed that the influence in the alpha/beta band from DLPFC to pSTC (top-down) is gradually increased above the onset of face presentation (Author response image 3, below left, plotted in red). By comparison, the influence in the alpha/beta band from pSTC to DLPFC (bottom-up) is gradually decreased after the onset of face presentation (Author response image 3, below left, blue curve). The influence in alpha/beta band from DLPFC to pSTC was significantly increased at 750 and 1250 ms after the face presentation (face vs nonface, paired t-test, Bonferroni corrected P=0.005, 0.006), suggesting an enhanced top-down modulation in the post-childhood group during watching emotional faces. Interestingly, this top-down influence appears very different in the 8-year-old child at 1250 ms after the face presentation (Author response image 3, below left, black curve).</p><p>As we cannot draw direct conclusions from the single-subject sample presented here, the top-down hypothesis is introduced only as a possible explanation for our current results. We have removed potentially misleading statements, and we plan to test this hypothesis directly using MEG in the future.</p><fig id="sa3fig3" position="float"><label>Author response image 3.</label><caption><title>Difference of Granger causality indices (face – nonface) in alpha/beta and gamma band for both directions.</title><p>We identified a series of face onset in the movie that paticipant watched. Each trial was defined as -0.1 to 1.5 s relative to the onset. For the non-face control trials, we used houses, animals and scenes. Granger causality was calculated for 0-0.5 s, 0.5-1 s and 1-1.5 s time window. For the post-childhood group, GC indices were averaged across participants. Error bar is sem.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-107636-sa3-fig3-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>(6) The &quot;post-childhood&quot; group spans ages 13-55, conflating adolescence, young adulthood, and middle age. Developmental conclusions would benefit from finer age stratification.</p></disp-quote><p>We appreciate this insightful comment. Our current sample size does not allow such stratification. But we plan to address this important issue in future MEG studies with larger cohorts.</p><disp-quote content-type="editor-comment"><p>(7) The so-called &quot;complex emotions&quot; (e.g., embarrassment, pride, guilt, interest) used in the study often require contextual information, such as speech or narrative cues, for accurate interpretation, and are not typically discernible from facial expressions alone. As such, the observed age-related increase in neural encoding of these emotions may reflect not solely the maturation of facial emotion perception, but rather the development of integrative processing that combines facial, linguistic, and contextual cues. This raises the possibility that the reported effects are driven in part by language comprehension or broader social-cognitive integration, rather than by changes in facial expression processing per se.</p></disp-quote><p>We agree with this interpretation. Indeed, our results already show that speech influences the encoding of facial emotion in the DLPFC differently in the childhood and post-childhood groups (Figure 2D), suggesting that children’s ability to integrate multiple cues is still developing. Future studies are needed to systematically examine how linguistic cues and prior experiences contribute to the understanding of complex emotions from faces, which we have added to our future directions section (last paragraph in Discussion, Page 8-9).</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations for the authors):</bold></p><p>In the introduction: &quot;These neuroimaging data imply that social and emotional experiences shape the prefrontal cortex's involvement in processing the emotional meaning of faces throughout development, probably through top-down modulation of early sensory areas.&quot; Aren't these supposed to be iEEG data instead of neuroimaging?</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations for the authors):</bold></p><p>This manuscript would benefit from several improvements to strengthen the validity and interpretability of the findings:</p><p>(1) Increase the sample size, especially for children with pSTC coverage.</p></disp-quote><p>We added data from another two children who have pSTC coverage. Please see our response to reviewer 2’s comment 3 and reviewer 1’s comment 1.</p><disp-quote content-type="editor-comment"><p>(2) Include directional connectivity analyses to test the proposed top-down modulation from DLPFC to pSTC.</p></disp-quote><p>Thanks for the suggestion. Please see our response to reviewer 2’s comment 5.</p><disp-quote content-type="editor-comment"><p>(3) Use controlled stimuli in an additional experiment to separate the effects of facial expression, speech, and music.</p></disp-quote><p>This is an excellent point. However, iEEG data collection from children is an exceptionally rare opportunity and typically requires many years, so we are unable to add a controlled-stimulus experiment to the current study. We plan to consider using controlled stimuli to study the processing of complex emotion using non-invasive method in the future. In addition, please see our response to reviewer 2’s comment 1 for a description of how neural responses to facial expression and music are separated in our study.</p></body></sub-article></article>