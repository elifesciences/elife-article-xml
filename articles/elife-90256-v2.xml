<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">90256</article-id><article-id pub-id-type="doi">10.7554/eLife.90256</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Emergence of brain-like mirror-symmetric viewpoint tuning in convolutional neural networks</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-323671"><name><surname>Farzmahdi</surname><given-names>Amirhossein</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-6926-546X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-60560"><name><surname>Zarco</surname><given-names>Wilbert</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3599-0476</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-118400"><name><surname>Freiwald</surname><given-names>Winrich A</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8456-5030</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-154647"><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7433-9005</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-323776"><name><surname>Golan</surname><given-names>Tal</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7940-7473</contrib-id><email>golan.neuro@bgu.ac.il</email><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0420db125</institution-id><institution>Laboratory of Neural Systems, The Rockefeller University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04xreqs31</institution-id><institution>School of Cognitive Sciences, Institute for Research in Fundamental Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Tehran</named-content></addr-line><country>Islamic Republic of Iran</country></aff><aff id="aff3"><label>3</label><institution>The Center for Brains, Minds &amp; Machines</institution><addr-line><named-content content-type="city">Cambridge</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Zuckerman Mind Brain Behavior Institute, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Psychology, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Neuroscience, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Department of Electrical Engineering, Columbia University</institution></institution-wrap><addr-line><named-content content-type="city">New York</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale de Lausanne</institution></institution-wrap><country>Switzerland</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Bi</surname><given-names>Yanchao</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022k4wk35</institution-id><institution>Beijing Normal University</institution></institution-wrap><country>China</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>25</day><month>04</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e90256</elocation-id><history><date date-type="received" iso-8601-date="2023-06-19"><day>19</day><month>06</month><year>2023</year></date><date date-type="accepted" iso-8601-date="2024-04-25"><day>25</day><month>04</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2023-01-05"><day>05</day><month>01</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.01.05.522909"/></event></pub-history><permissions><copyright-statement>© 2024, Farzmahdi et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Farzmahdi et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-90256-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-90256-figures-v2.pdf"/><abstract><p>Primates can recognize objects despite 3D geometric variations such as in-depth rotations. The computational mechanisms that give rise to such invariances are yet to be fully understood. A curious case of partial invariance occurs in the macaque face-patch AL and in fully connected layers of deep convolutional networks in which neurons respond similarly to mirror-symmetric views (e.g. left and right profiles). Why does this tuning develop? Here, we propose a simple learning-driven explanation for mirror-symmetric viewpoint tuning. We show that mirror-symmetric viewpoint tuning for faces emerges in the fully connected layers of convolutional deep neural networks trained on object recognition tasks, even when the training dataset does not include faces. First, using 3D objects rendered from multiple views as test stimuli, we demonstrate that mirror-symmetric viewpoint tuning in convolutional neural network models is not unique to faces: it emerges for multiple object categories with bilateral symmetry. Second, we show why this invariance emerges in the models. Learning to discriminate among bilaterally symmetric object categories induces reflection-equivariant intermediate representations. AL-like mirror-symmetric tuning is achieved when such equivariant responses are spatially pooled by downstream units with sufficiently large receptive fields. These results explain how mirror-symmetric viewpoint tuning can emerge in neural networks, providing a theory of how they might emerge in the primate brain. Our theory predicts that mirror-symmetric viewpoint tuning can emerge as a consequence of exposure to bilaterally symmetric objects beyond the category of faces, and that it can generalize beyond previously experienced object categories.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>primate vision</kwd><kwd>face processing</kwd><kwd>symmetry</kwd><kwd>neural networks</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01EY021594</award-id><principal-award-recipient><name><surname>Freiwald</surname><given-names>Winrich A</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01EY029998</award-id><principal-award-recipient><name><surname>Freiwald</surname><given-names>Winrich A</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000065</institution-id><institution>National Institute of Neurological Disorders and Stroke</institution></institution-wrap></funding-source><award-id>RF1NS128897</award-id><principal-award-recipient><name><surname>Kriegeskorte</surname><given-names>Nikolaus</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100009917</institution-id><institution>Naval Research Laboratory</institution></institution-wrap></funding-source><award-id>N00014-20-1-2292</award-id><principal-award-recipient><name><surname>Freiwald</surname><given-names>Winrich A</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100001312</institution-id><institution>Charles H. Revson Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Golan</surname><given-names>Tal</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Mirror-symmetric view tuning in the macaque AL face patch can be explained by the spatial pooling of learned reflection-equivariant representations.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Primates can recognize objects robustly despite considerable image variation. Although we experience object recognition as immediate and effortless, the process involves a large portion of cortex and considerable metabolic cost (<xref ref-type="bibr" rid="bib40">Laughlin et al., 1998</xref>), and determining the neural mechanisms and computational principles that enable this ability remains a major neuroscientific challenge. One particular object category, faces, offers an especially useful window into how the visual cortex transforms retinal signals to object representations. The macaque brain contains a network of interconnected areas devoted to the processing of faces. This network, the face-patch system, forms a subsystem of the inferotemporal (IT) cortex (<xref ref-type="bibr" rid="bib67">Tsao et al., 2006</xref>; <xref ref-type="bibr" rid="bib46">Moeller et al., 2008</xref>; <xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib29">Hesse and Tsao, 2020</xref>). Neurons across the network show response selectivity for faces, but are organized in face patches–spatially and functionally distinct modules (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>; <xref ref-type="bibr" rid="bib22">Freiwald, 2020</xref>). These patches exhibit an information processing hierarchy from posterior to anterior areas. In the most posterior face-patch, PL (posterior lateral), neurons respond to face components (<xref ref-type="bibr" rid="bib32">Issa and DiCarlo, 2012</xref>). In ML/MF (middle lateral/middle fundus), neurons respond to whole faces in a view-specific manner. In AL (anterior lateral), responses are still view-specific, but mostly reflection-invariant. Finally in AM (anterior medial), neurons respond with sensitivity to the identity of the face, but in a view-invariant fashion (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). The average neuronal response latencies increase across this particular sequence of stages (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). Thus, it appears as if visual information is transformed across this hierarchy of representational stages in a way that facilitates the recognition of individual faces despite view variations.</p><p>What are the computational principles that give rise to the representational hierarchy evident in the face-patch system? Seeking potential answers to this and similar questions, neuroscientists have been increasingly turning to convolutional neural networks (CNNs) as baseline computational models of the primate ventral visual stream. Although CNNs lack essential features of the primate ventral stream, such as recurrent connectivity, they offer a simple hierarchical model of its feedforward cascade of linear-non-linear transformations. Feedforward CNNs remain among the best models for predicting mid- and high-level cortical representations of novel natural images within the first 100–200ms after stimulus onset (<xref ref-type="bibr" rid="bib71">Yamins et al., 2014</xref>; <xref ref-type="bibr" rid="bib33">Khaligh-Razavi and Kriegeskorte, 2014</xref>). Diverse CNN models, trained on tasks such as face identification (<xref ref-type="bibr" rid="bib19">Farzmahdi et al., 2016</xref>; <xref ref-type="bibr" rid="bib1">Abudarham et al., 2021</xref>; <xref ref-type="bibr" rid="bib55">Raman and Hosoya, 2020</xref>), object recognition (<xref ref-type="bibr" rid="bib10">Chang et al., 2021</xref>), inverse graphics (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>), sparse coding (<xref ref-type="bibr" rid="bib31">Hosoya and Hyvärinen, 2017</xref>), and unsupervised generative modeling <xref ref-type="bibr" rid="bib30">Higgins et al., 2021</xref> have all been shown to replicate at least some aspects of face-patch system representations. Face-selective artificial neurons occur even in untrained CNNs (<xref ref-type="bibr" rid="bib4">Baek et al., 2021a</xref>), and functional specialization between object and face representation emerges in CNNs trained on the dual task of recognizing objects and identifying faces (<xref ref-type="bibr" rid="bib17">Dobs et al., 2022</xref>).</p><p>To better characterize and understand the computational mechanisms employed by the primate face-patch system and test whether the assumptions implemented by current CNN models are sufficient for explaining its function, we should carefully inspect the particular representational motifs the face-patch system exhibits. One of the more salient and intriguing of these representational motifs is the <italic>mirror-symmetric viewpoint tuning</italic> in the AL face-patch (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). Neurons in this region typically respond with different firing rates to varying views of a face (e.g. a lateral profile vs. a frontal view), but they respond with similar firing rates to views that are horizontal reflections of each other (e.g. left and right lateral profiles) (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>).</p><p>To date, two distinct computational models have been put forward as potential explanations for AL’s mirror-symmetric viewpoint tuning. Leibo and colleagues (<xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>) considered unsupervised learning in an HMAX-like (<xref ref-type="bibr" rid="bib58">Riesenhuber and Poggio, 1999</xref>) four-layer neural network exposed to a sequence of face images rotating in depth about a vertical axis. When the learning of the mapping from the complex-cell-like representation of the second layer to the penultimate layer was governed by Hebbian-like synaptic updates (<xref ref-type="bibr" rid="bib49">Oja, 1982</xref>), approximating a principal components analysis (PCA) of the input images, the penultimate layer developed mirror-symmetric viewpoint tuning. In another modeling study, Yildirim and colleagues (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>) trained a CNN to invert the rendering process of 3D faces, yielding a hierarchy of intermediate and high-level face representations. Mirror-symmetric viewpoint tuning emerged in an intermediate representation between two densely-connected transformations mapping 2.5D surface representations to high-level shape and texture face-space representations. Each of these two models (<xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>; <xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>) provides a plausible explanation of AL’s mirror-symmetric viewpoint tuning, but each requires particular assumptions about the architecture and learning conditions, raising the question whether a more general computational principle can provide a unifying account of the emergence of mirror-symmetric viewpoint tuning.</p><p>Here, we propose a parsimonious, bottom-up explanation for the emergence of mirror-symmetric viewpoint tuning for faces (<xref ref-type="fig" rid="fig1">Figure 1</xref>). We find that learning to discriminate among bilaterally symmetric object categories promotes the learning of representations that are <italic>reflection-equivariant</italic> (i.e. they code a mirror image by a mirrored representation). Spatial pooling of the features, as occurs in the transition between the convolutional and fully connected layers in CNNs, then yields <italic>reflection-invariant</italic> representations (i.e. these representations code a mirror image as they would code the original image). These reflection-invariant representations are not fully view-invariant: They are still tuned to particular views of faces (e.g. respond more to a half-profile than to a frontal view, or vice versa), but they do not discriminate between mirrored views. In other words, these representations exhibit mirror-symmetric viewpoint tuning (in the twin sense of the neuron responding equally to left-right-reflected images and the tuning function, hence, being mirror-symmetric). We propose that the same computational principles may explain the emergence of mirror-symmetric viewpoint tuning in the primate face-patch system.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>An overview of our claim: convolutional deep neural networks trained on discriminating among bilaterally symmetric object categories provide a parsimonious explanation for the mirror-symmetric viewpoint tuning of the macaque AL face-patch.</title><p>(<bold>A</bold>) The macaque face-patch system. Face-selective cortical areas are highlighted in yellow. The areas ML, AL, and AM exhibit substantially different tuning proprieties when presented with faces of different head orientations (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). These distinct tuning profiles are evident in population-level representational dissimilarity matrices (RDMs). From posterior to anterior face areas, invariance to viewpoints gradually increases: from view-tuned in ML, through mirror-symmetric in AL, to view-invariant identity selectivity in AM (neural data from <xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). (<bold>B</bold>) Training convolutional deep neural networks on recognizing specific symmetric object categories (e.g. faces, cars, the digit 8) gives rise to AL-like mirror-symmetric tuning. It is due to a cascade of two effects: First, learning to discriminate among symmetric object categories promotes tuning for reflection-equivariant representations throughout the entire processing layers. This reflection equivariance increases with depth. Then, long-range spatial pooling (as in the transformation of the last convolution layer to the first fully connected layer in CNNs) transforms the equivariant representations into reflection-invariant representations. (<bold>C</bold>) Schematic representations of three viewpoints of a face (left profile, frontal view, right profile) are shown in three distinct stages of processing. Each tensor depicts the width (<bold>w</bold>), height (<bold>h</bold>), and depth (<bold>c</bold>) of an activation pattern. Colors indicate channel activity. From left to right: In a mid-level convolutional layer, representations are view-specific. A deeper convolutional layer produces reflection-equivariant representations that are view-specific. Feature vectors of a fully connected layer become invariant to reflection by pooling reflection-equivariant representations from the last convolutional layer. (<bold>D</bold>) A graphical comparison of reflection-equivariance and reflection-invariance. Circles denote input images, and squares denote representations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig1-v2.tif"/></fig><p>Our results further suggest that emergent reflection-invariant representations may also exist for non-face objects: the same training conditions give rise to CNN units that show mirror-symmetric tuning profiles for non-face objects that have a bilaterally symmetric structure. Extrapolating from CNNs back to primate brains, we predict AL-like mirror-symmetric viewpoint tuning in non-face-specific visual regions that are parallel to AL in terms of the ventral stream representational hierarchy. Such tuning could be revealed by probing these regions with non-face objects that are bilaterally symmetric.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Deep layers in CNNs exhibit mirror-symmetric viewpoint tuning to multiple object categories</title><p>We investigated whether reflection-invariant yet view-specific tuning emerges naturally in deep convolutional neural networks. To achieve this, we generated a diverse set of 3D objects rendered in multiple views. We evaluated the hidden-layer activations of an ImageNet-trained AlexNet CNN model (<xref ref-type="bibr" rid="bib38">Krizhevsky et al., 2012</xref>) presented with nine views of each object exemplar. We constructed a 9 × 9 representational dissimilarity matrix (RDM; <xref ref-type="bibr" rid="bib36">Kriegeskorte et al., 2008</xref>) for each exemplar object and each CNN layer, summarizing the view tuning of the layer’s artificial neurons (‘units’) by means of between-view representational distances. The resulting RDMs revealed a progression throughout the CNN layers for objects with one or more symmetry planes: These objects induce mirror-symmetric RDMs in the deeper CNN layers (<xref ref-type="fig" rid="fig2">Figure 2A</xref>), reminiscent of the symmetric RDMs measured for face-related responses in the macaque AL face-patch (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). We defined a ‘mirror-symmetric viewpoint tuning index’ to quantify the degree to which representations are view-selective yet reflection-invariant (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). Consider a dissimilarity matrix <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> where <inline-formula><mml:math id="inf2"><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the distance between view <inline-formula><mml:math id="inf3"><mml:mi>j</mml:mi></mml:math></inline-formula> and view <inline-formula><mml:math id="inf4"><mml:mi>k</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf5"><mml:mi>n</mml:mi></mml:math></inline-formula> denotes the number of views. The RDM is symmetric about the main diagonal by definition: <inline-formula><mml:math id="inf6"><mml:mrow><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>k</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, independent of the tuning of the units. The views are ordered from left to right, such that <inline-formula><mml:math id="inf7"><mml:mi>j</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf8"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> refer to horizontally reflected views. The mirror-symmetric viewpoint tuning index is defined as the Pearson linear correlation coefficient between <inline-formula><mml:math id="inf9"><mml:mi>D</mml:mi></mml:math></inline-formula> and its horizontally flipped counterpart, <inline-formula><mml:math id="inf10"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). Note that this is equivalent to the correlation between vertically flipped RDMs, because of the symmetry of the RDMs about the diagonal: <inline-formula><mml:math id="inf11"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>V</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>. This mirror-symmetric viewpoint tuning index is positive and large to the extent that the units are view-selective but reflection-invariant (like the neurons in macaque AL face-patch). The index is near zero for units with view-invariant tuning (such as the AM face-patch), where the dissimilarities are all small and any variations are caused by noise.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Mirror-symmetric viewpoint tuning of higher level deep neural network representations emerges for multiple object categories.</title><p>(<bold>A</bold>) Different viewpoint tuning across the layers of AlexNet for four example objects. For each object, the responses to nine views (–90° to +90° in the steps of 22.5°) were measured in six key AlexNet layers, shallow (input, <italic>left</italic>) to deep (fc6, <italic>right</italic>). For each layer, a Representational Dissimilarity Matrix (RDM) depicts how the population activity vector varies across different object views. Each element of the RDM represents the dissimilarity (1 - Pearson correlation coefficient) between a pair of activity vectors evoked in response to two particular views. The symmetry of the RDMs about the major diagonal is inherent to their construction. However, the symmetry about the minor diagonal (for the face and chair, in fc6, and for the car, already in conv2) indicates mirror-symmetric viewpoint tuning. (<bold>B</bold>) The schematic shows how the mirror-symmetric viewpoint tuning index was quantified. We first fed the network with images of each object from nine viewpoints and recorded the activity patterns of its layers. Then, we computed the dissimilarity between activity patterns of different viewpoints to create an RDM. Next, we measured the correlation between the obtained RDM and its horizontally flipped counterpart, excluding the frontal view (which is unaffected by the reflection). (<bold>C</bold>) The Mirror-symmetric viewpoint tuning index across all AlexNet layers for nine object categories (car, boat, face, chair, airplane, animal, tool, fruit, and flower). Each solid circle denotes the average of the index over 25 exemplars within each object category. Error bars indicate the standard error of the mean. The mirror-symmetric viewpoint tuning index values of the four example objects in panel B are shown at the bottom right of each RDM in panel B. <xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref> shows the same analysis applied to representations of the face stimulus set used in <xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>, across various neural network models. (<bold>D</bold>) 3D Objects have different numbers of symmetry axes. A face (left column), a non-face object with bilateral symmetry (a chair, second column), an object with quadrilateral symmetry (a car, third column), and an object with no obvious reflective symmetry planes (a flower, right column).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Assessment of symmetry planes in 3D renders across viewpoints.</title><p>For each 3D object (25 exemplars for each of the nine categories) and each rendering viewpoint (nine viewpoints from –90° to 90° at 22.5° intervals) used in the stimulus set, we measured the horizontal symmetry of the resulting 2D render by correlating the left half of the 2D image with a flipped version of its right half. In each such measurement, we systematically shifted the plane of reflection and used the highest correlation across all shifts. The resulting correlation coefficients, representing horizontal symmetry as a function of viewpoint, are displayed on polar plots. In these plots, each depicting a single object category, thin lines indicate individual object exemplars (e.g. a particular face), and bold lines indicate the average correlation coefficients across the 25 exemplars of each category. By setting a threshold at half a standard deviation above the mean correlation, we heuristically counted the number of symmetry axes for each object category. Notably, images of cars and boats have strong image-space symmetry in both frontal and side views, explaining the pronounced mirror-symmetric viewpoint tuning index observed already in early convolutional layers. These two categories exhibit dual symmetry axes—left–right and front–back. In comparison, objects like faces, chairs, airplanes, tools, and animals have a single left-right symmetry plane, expressed in the 2D renders as high horizontal symmetry of the frontal view. Fruits and flowers have relatively uniform correlation values across views, which is indicative of radial symmetry. This radial symmetry translates to a lower mirror-symmetric viewpoint tuning index of the neural network representations of these categories.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>The mirror-symmetric viewpoint tuning index remains unchanged as the signal moves into the fully connected layers of the untrained network.</title><p>(<bold>A</bold>) Each solid circle represents the average index for 25 exemplars within each object category (car, boat, face, chair, airplane, animal, tool, fruit, flower) for the untrained AlexNet network. (<bold>B</bold>) Each solid circle refers to the difference between the mirror-symmetric viewpoint tuning index of the trained versus the untrained AlexNet network. We evaluated the difference using the rank-sum test. We used the <xref ref-type="bibr" rid="bib8">Benjamini and Hochberg, 1995</xref> procedure for controlling the False discovery rate (FDR) across 90 comparisons at q&lt;0.05 (9 categories and 10 layers, excluding the input layer, as it is the same in both networks). The solid circles with gray outlines indicate where the difference after FDR adjustment is significant. Error bars indicate the standard error of the mean.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Convolutional networks, regardless of their architecture and training objectives, exhibit peak mirror-symmetric viewpoint tuning at the fully-connected and average pooling layers.</title><p>(<bold>A–H</bold>) The colored curves represent the mirror-symmetric viewpoint tuning indices across nine object categories (car, boat, face, chair, airplane, animal, tool, fruit, and flower) across the neural network layers. Each solid circle indicates the average index value across 25 exemplars within each object category. Error bars denote the standard error of the mean. In all of the convolutional networks, the mirror-symmetric viewpoint tuning index peaks at the fully-connected or average pooling layers. ViT, with its non-convolutional architecture, does not exhibit this tuning profile. For face stimuli, there is a unique progression in mirror-symmetric viewpoint tuning: the index is negative for the convolutional layers, and it abruptly becomes highly positive when transitioning to the first fully connected layer. The negative indices in the convolutional layers can be attributed to the image-space asymmetry of non-frontal faces; compared to other categories, faces demonstrate pronounced front-back asymmetry, which translates to asymmetric images for all but frontal views (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The features that drive the highly positive mirror-symmetric viewpoint tuning for faces in the fully connected layers are training-dependent (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), and hence, may reflect asymmetric image features that do not elicit equivariant maps in low-level representations; for example, consider a profile view of a nose. Note that cars and boats elicit high mirror-symmetric viewpoint tuning indices already in early processing layers. This early mirror-symmetric tuning is independent of training (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), and hence, may be driven by low-level features. Both these object categories show pronounced quadrilateral symmetry, which translates to symmetric images for both frontal and side views (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Mirror-symmetric viewpoint tuning of various neural network architectures measured with respect to the FIV face stimulus set (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>) and compared to the mirror-symmetric viewpoint tuning of three face-patches (MLMF, AL, and AM).</title><p>This figure contrasts the mirror-symmetric viewpoint tuning index of macaque face patches with equivalent measurements in different neural network layers. Solid circles indicate indices for network layers, averaged across 25 face exemplars of the FIV stimulus set. The error bars show the standard error. The colored horizontal lines represent estimated mirror-symmetric viewpoint indices for three face patches (MLMF, AL, AM). To ensure that neural noise does not attenuate the measured mirror-symmetric viewpoint tuning, we divided the raw index estimated for each patch with a reliability estimate. This estimate was obtained by correlating neural RDMs pertaining to two equally sized disjoint sets of neurons recorded in that patch, averaging the result over 100 random splits, and applying a Spearman-Brown correction. Notably, the AL face patch demonstrates the most pronounced mirror-symmetric viewpoint tuning among the face patches, closely aligning with the measurements in deeper network layers. Conversely, the MLMF patch, characterized by its asymmetric representation, shows a negative index value, similar to the early and mid-level network layers. The positive index of the AM face patch, though lower than that of the AL, is consistent with a view-invariant representation (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). Diverse convolutional architectures mimic the emergence of mirror-symmetric viewpoint tuning between the MLMF and AL face patches.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp4-v2.tif"/></fig><fig id="fig2s5" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 5.</label><caption><title>The highest mirror-symmetric viewpoint tuning index across all layers of each evaluated neural network model.</title><p>We evaluated the following networks: HMAX, VGGFace, VGG16, AlexNet, EIG, ResNet50, ConvNeXt, and ViT. Each panel indicates the layer displaying the peak mirror-symmetric viewpoint tuning index for one object category, measured separately for each network. The deepest layers of the ConvNeXt network, especially the average pooling (avgpool) and classifier layers, exhibit the highest indices for nearly all categories. <xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref> reported that CNNs trained on faces, notably VGGFace, exhibited lower mirror-symmetric viewpoint tuning compared to neural representations in area AL. Consistent with their findings, our results demonstrate that VGGFace, trained on face identification, has a low mirror-symmetric viewpoint tuning index. This is especially notable in comparison to ImageNet-trained models such as VGG16. This difference between VGG16 and VGGFace can be attributed to the distinct characteristics of their training datasets and objective functions. The VGGFace training task consists of mapping frontal face images to identities; this task may exclusively emphasize higher-level physiognomic information. In contrast, training on recognizing objects in natural images may result in a more detailed, view-dependent representation. To test this potential explanation, we measured the average correlation-distance between the fc6 representations of different views of the same face exemplar in VGGFace and VGG16 trained on ImageNet. The average correlation-distance between views is 0.70 ± 0.04 in VGGFace and 0.93 ± 0.04 in VGG16 trained on ImageNet. The converse correlation distance between different exemplars depicted from the same view is 0.84 ± 0.14 in VGGFace and 0.58 ± 0.06 in VGG16 trained on ImageNet. Therefore, as suggested by Yildirim and colleagues, training on face identification alone may result in representations that cannot explain intermediate levels of face processing.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp5-v2.tif"/></fig><fig id="fig2s6" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 6.</label><caption><title>One of the key operations in fully-connected layers is spatial pooling.</title><p>We analyzed the impact of this operation by artificially introducing global average pooling (GAP) instead of the first fully-connected layer (fc6) of ImageNet-trained AlexNet. Each element of the GAP representation refers to a spatial average of unit activations of one pool5 feature map. The scatterplot shows the mirror-symmetric viewpoint tuning index of GAP applied to pool5 (x-axis) relative to an fc6 representation (y-axis). Each circle represents one exemplar object. These results indicate that global spatial pooling introduced instead of fc6 is sufficient for rendering the pool5 representation mirror-symmetric viewpoint selective, reproducing the symmetry levels of the different fc6 view tuning curves across objects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp6-v2.tif"/></fig><fig id="fig2s7" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 7.</label><caption><title>Layer-wise mirror-symmetric viewpoint tuning profiles measured by linear correlation without employing unit-specific z-score normalization.</title><p>As in <xref ref-type="fig" rid="fig2">Figure 2</xref>, colored curves show the mirror-symmetric viewpoint tuning indices for nine object categories across AlexNet layers. Each solid circle indicates the average index value derived from 25 exemplars in each object category. Error bars indicate the standard error of the mean. In <xref ref-type="fig" rid="fig2">Figure 2</xref>, representational dissimilarities were measured using unit activations first centered and normalized across images (a procedure denoted as RSA<sub>CorrDem</sub> in <xref ref-type="bibr" rid="bib57">Revsine et al., 2024</xref>). Here, first-level correlations were calculated using raw activations (a procedure denoted as RSA<sub>Corr</sub> in <xref ref-type="bibr" rid="bib57">Revsine et al., 2024</xref>). Revsine and colleagues noted that under linear-system assumptions, RSA<sub>Corr</sub> yields a representational dissimilarity measure invariant to response gain; response gain might be strongly influenced by low-level factors such as luminance and contrast. The similarity of the tuning profiles observed here and in <xref ref-type="fig" rid="fig2">Figure 2</xref> is consistent with the interpretation of the emergent mirror-symmetric viewpoint tuning in our models as driven by learned equivariant mid-level features rather than low-level stimulus features. This result, however, does not preclude the possibility that other, uncontrolled stimulus sets could elicit viewpoint-tuning profiles that are driven by low-level confounds, as demonstrated by Revsine and colleagues.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp7-v2.tif"/></fig><fig id="fig2s8" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 8.</label><caption><title>Comparison of mirror-symmetric viewpoint tuning in a supervised, PCA-based model (<xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>) and a supervised CNN (AlexNet) trained on object recognition.</title><p>Panels A and B depict how mirror-symmetric viewpoint tuning in a re-implementation of the Leibo and colleagues model (<xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>) sharply declines for off-center test stimuli. In contrast, the same shift in center of the test stimuli has only a negligible effect on mirror-symmetric viewpoint tuning in AlexNet (Panel C). Implementation details: To reproduce the model described in <xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>, we generated a training stimulus set using the Basel Face Model. The stimulus set consisted of untextured synthetic faces of 40 identities, each depicted from 39 viewpoints. For panel A, we estimated a PCA of the pixel-space representation of this stimulus set. For panel B, we estimated a PCA of the stimulus set’s HMAX C1 layer representation. In both cases, the resulting latent representation had 1560 features (40×39). To test the model, we used the face stimulus set containing 25 exemplars in 9 viewpoints employed in <xref ref-type="fig" rid="fig2">Figure 2</xref>. The viewpoints ranged from –90° to 90°, with a step of 22.5°. Mirror-symmetric viewpoint tuning was extracted from a representational dissimilarity matrix (RDM) created per exemplar. Green and purple circles represent mirror-symmetric viewpoint tuning in centered and shifted images (with 15-pixel shifts in the x and y axes), respectively. White circles indicate the mean across all exemplars.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig2-figsupp8-v2.tif"/></fig></fig-group><p><xref ref-type="fig" rid="fig2">Figure 2C</xref> displays the average mirror-symmetric viewpoint tuning index for each object category across AlexNet layers. Several categories—faces, chairs, airplanes, tools, and animals—elicited low (below 0.1) or even negative mirror-symmetric viewpoint tuning values throughout the convolutional layers, transitioning to considerably higher (above 0.6) values starting from the first fully connected layer (fc6). In contrast, for fruits and flowers, mirror-symmetric viewpoint tuning was low in both the convolutional and the fully connected layers. For cars and boats, mirror-symmetric viewpoint tuning was notably high already in the shallowest convolutional layer and remained so across the network’s layers. To explain these differences, we quantified the symmetry of the various 3D objects in each category by analyzing their 2D projections (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). We found that all of the categories that show high mirror-symmetric viewpoint tuning index in fully connected but not convolutional layers have a single plane of symmetry. For example, the left and right halves of a human face are reflected versions of each other (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). This 3D structure yields symmetric 2D projections only when the object is viewed frontally, thus hindering lower level mirror-symmetric viewpoint tuning. Cars and boats have two planes of symmetry: in addition to the symmetry between their left and right halves, there is an approximate symmetry between their back and front halves. The quintessential example of such quadrilateral symmetry would be a Volkswagen Beetle viewed from the outside. Such 3D structure enables mirror-symmetric viewpoint tuning even for lower-level representations, such as those in the convolutional layers. Fruits and flowers exhibit radial symmetry but lack discernible symmetry planes, a characteristic that impedes viewpoint tuning altogether.</p><p>However, for an untrained AlexNet, the mirror-symmetric viewpoint tuning index remains relatively constant across the layers (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2A</xref>). Statistically contrasting mirror-symmetric viewpoint tuning between a trained and untrained AlexNet demonstrates that the leap in mirror-symmetric viewpoint tuning in fc6 is training-dependent (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>).</p><p>Shallow and deep convolutional neural network models with varied architectures and objective functions replicate the emergence of mirror-symmetric viewpoint tuning (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). These models include VGG16 (<xref ref-type="bibr" rid="bib62">Simonyan and Zisserman, 2015</xref>), ‘VGGFace’ network (trained on face identification) (<xref ref-type="bibr" rid="bib51">Parkhi et al., 2015</xref>), EIG (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>), HMAX (<xref ref-type="bibr" rid="bib58">Riesenhuber and Poggio, 1999</xref>), ResNet50 (<xref ref-type="bibr" rid="bib27">He et al., 2016</xref>), ConvNeXt (<xref ref-type="bibr" rid="bib44">Liu et al., 2022</xref>). In all these convolutional networks, the mirror-symmetric viewpoint tuning index peaks at the fully-connected or average pooling layers. ViT (<xref ref-type="bibr" rid="bib18">Dosovitskiy et al., 2021</xref>), featuring a non-convolutional architecture, does not exhibit this feature (<xref ref-type="fig" rid="fig2s5">Figure 2—figure supplement 5</xref>).</p><p>Why does the transition to the fully connected layers induce mirror-symmetric viewpoint tuning for bilaterally symmetric objects? One potential explanation is that the learned weights that map the last convolutional representation (pool5) to the first fully connected layer (fc6) combine the pool5 activations in a specific pattern that induces mirror-symmetric viewpoint tuning. However, replacing fc6 with spatial global average pooling (collapsing each pool5 feature map into a scalar activation) yields a representation with very similar mirror-symmetric viewpoint tuning levels (<xref ref-type="fig" rid="fig2s6">Figure 2—figure supplement 6</xref>). This result is suggestive of an alternative explanation: that training the network on ImageNet gives rise to a reflection-equivariant representation in pool5. We therefore investigated the reflection equivariance of the convolutional representations.</p></sec><sec id="s2-2"><title>Reflection equivariance versus reflection invariance of convolutional layers</title><p>Consider a representation <inline-formula><mml:math id="inf12"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, defined as a function that maps input images to sets of feature maps, and a geometric image transformation <inline-formula><mml:math id="inf13"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, applicable to either feature maps or raw images. <inline-formula><mml:math id="inf14"><mml:mi>f</mml:mi></mml:math></inline-formula> is equivariant under <inline-formula><mml:math id="inf15"><mml:mi>g</mml:mi></mml:math></inline-formula> if <inline-formula><mml:math id="inf16"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for any input image <inline-formula><mml:math id="inf17"><mml:mi>x</mml:mi></mml:math></inline-formula> (see also <xref ref-type="bibr" rid="bib39">Kvinge et al., 2022</xref>). While convolutional feature maps are approximately equivariant under translation (but see <xref ref-type="bibr" rid="bib3">Azulay and Weiss, 2019</xref>), they are not in general equivariant under reflection or rotation. For example, an asymmetrical filter along reflection axes in the first convolutional layer would yield an activation map that is not equivariant under reflection. And yet, the demands of the task on which a CNN is trained may lead to the emergence of representations that are approximately equivariant under reflection or rotation (see <xref ref-type="bibr" rid="bib11">Cohen and Welling, 2016</xref>; <xref ref-type="bibr" rid="bib69">Weiler et al., 2018</xref> for neural network architectures that are equivariant to reflection or rotation by construction). If a representation <inline-formula><mml:math id="inf18"><mml:mi>f</mml:mi></mml:math></inline-formula> is equivariant under a transformation <inline-formula><mml:math id="inf19"><mml:mi>g</mml:mi></mml:math></inline-formula> that is a spatial permutation of its input (e.g. <inline-formula><mml:math id="inf20"><mml:mi>g</mml:mi></mml:math></inline-formula> is a horizontal or vertical reflection or a 90° rotation) then <inline-formula><mml:math id="inf21"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> are spatially permuted versions of each other. If a spatially invariant function <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (i.e. a function that treats the pixels as a set, such as the average or the maximum) is then applied to the feature maps, the composed function <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>h</mml:mi><mml:mo>∘</mml:mo><mml:mi>f</mml:mi></mml:mrow></mml:math></inline-formula> is <italic>invariant</italic> to <inline-formula><mml:math id="inf25"><mml:mi>g</mml:mi></mml:math></inline-formula> since <inline-formula><mml:math id="inf26"><mml:mrow><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. Transforming a stack of feature maps into a channel vector by means of global average pooling is a simple case of such a spatially invariant function <inline-formula><mml:math id="inf27"><mml:mi>h</mml:mi></mml:math></inline-formula>. Therefore, if task-training induces approximately reflection-equivariant representations in the deepest convolutional layer of a CNN and approximately uniform pooling in the following fully connected layer, the resulting pooled representation would be approximately reflection-invariant.</p><p>We examined the emergence of approximate equivariance and invariance in CNN layers (<xref ref-type="fig" rid="fig3">Figure 3</xref>). We considered three geometric transformations: horizontal reflection, vertical reflection, and 90° rotation. Note that given their architecture alone, CNNs are not expected to show greater equivariance and invariance for horizontal reflection compared to vertical reflection or 90° rotation. However, greater invariance and equivariance for horizontal reflection may be expected on the basis of natural image statistics and the demands of invariant recognition. Many object categories in the natural world are bilaterally symmetric with respect to a plane parallel to the axis of gravity and are typically viewed (or photographed) in an upright orientation. Horizontal image reflection, thus, tends to yield equally natural images of similar semantic content, whereas vertical reflection and 90° rotation yield unnatural images.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Equivariance and invariance in trained and untrained deep convolutional neural networks.</title><p>Each solid circle represents an equivariance or invariance measure, averaged across images. Hues denote different transformations (horizontal flipping, vertical flipping, or 90° rotation). Error bars depict the standard deviation across images (each test condition consists of 2025 images). Invariance is a measure of similarity between the activity pattern an image elicits and the activity pattern its transformed (e.g. flipped) counterpart (solid lines) elicits. Equivariance is a measure of the similarity between the activity pattern of a transformed image elicits and the <italic>transformed</italic> version of the activity pattern the untransformed image elicits (dashed lines). In the convolutional layers, both invariance and equivariance can be measured. In the fully connected layers, whose representations have no explicit spatial structure, only invariance is measurable. (<bold>A</bold>) ImageNet-trained AlexNet tested on the rendered 3D objects. (<bold>B</bold>) Untrained AlexNet tested on rendered 3D objects. (<bold>C</bold>) ImageNet-trained AlexNet tested on the natural images (images randomly selected from the test set of ImageNet). (<bold>D</bold>) Untrained AlexNet tested on the natural images. (<bold>E</bold>) ImageNet-trained AlexNet tested on the random noise images. (<bold>F</bold>) Untrained AlexNet tested on the random noise images.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Image-specific representational invariance and equivariance across 3D object renders, natural images, and random noise images, measured in a deep convolutional neural network (AlexNet) trained on ImageNet or alternatively, left untrained.</title><p>Invariance is measured by the linear correlation between the activity pattern elicited by an image and the activity pattern elicited by a transformed version of the image. Equivariance is measured by the linear correlation between the activity pattern elicited by a transformed image and a transformed version of the activity pattern of the untransformed image. Each violin plot depicts the distribution of invariance (panels <bold>A-C</bold>) or equivariance (<bold>D–F</bold>) image-specific measures across 2025 images. The different hues denote the transformations against which the equivariance and invariance were measured: horizontal flipping (red), vertical flipping (green), or 90° rotation (blue). The solid circles denote the median, and the thick bars, the first and third quantiles. Panels A, B, and C show the invariance over horizontally flipped, vertically flipped, and 90° rotated images, respectively. Panels D, E, and F depict the equivariance over the same transformations. ImageNet training induces equivariance (in convolutional layers) and invariance (in fully connected layers) to the horizontal reflection of most natural images and 3D renders. This effect is less pronounced for vertical reflection and 90° rotation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Training-induced enhancement of horizontal reflection invariance in the first fully connected layer (fc6), across different object categories.</title><p>Elaborating on <xref ref-type="fig" rid="fig3">Figure 3</xref> and <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>, we examined horizontal reflection invariance in each object category in a trained (left panel) and an untrained (right panel) AlexNet network. Reflection invariance was quantified as the correlation between representations of horizontally flipped images. The violin plots show the distribution of these correlation coefficients across views and exemplars for each object category, with vertical bars marking the median and the first and third quartiles. In an untrained network, the differences between object categories primarily reflect pixel-level symmetry. Note that frontal faces, due to their inherent left-right symmetry, elicit a higher correlation compared to other viewpoints (appearing as a positive outlier).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig3-figsupp2-v2.tif"/></fig></fig-group><p>To measure equivariance and invariance, we presented the CNNs with pairs of original and transformed images. To measure the invariance of a fully-connected CNN layer, we calculated an across-unit Pearson correlation coefficient for each pair of activation vectors that were induced by a given image and its transformed version. We averaged the resulting correlation coefficients across all image pairs (Materials and methods, <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>). For convolutional layers, this measure was applied after flattening stacks of convolutional maps into vectors. In the case of horizontal reflection, this invariance measure would equal 1.0 if the activation vectors induced by each image and its mirrored version are identical (or perfectly correlated).</p><p>Equivariance could be quantified only in convolutional layers because units in fully connected layers do not form visuotopic maps that can undergo the same transformations as images. It was quantified similarly to invariance, except that we applied the transformation of interest (i.e. reflection or rotation) not only to the image but also to the convolutional map of activity elicited by the untransformed image (<xref ref-type="disp-formula" rid="equ3">Equation 3</xref>). We correlated the representation of the transformed image with the transformed representation of the image. In the case of horizontal reflection, this equivariance measure would equal 1.0 if each activation map induced by an image and its reflected version are reflected versions of each other (or are perfectly correlated after horizontally flipping one of them).</p><p>We first evaluated equivariance and invariance with respect to the set of 3D object images described in the previous section. In an ImageNet-trained AlexNet, horizontal-reflection equivariance increased across convolutional layers (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). Equivariance under vertical reflection was less pronounced and equivariance under 90° rotation was even weaker (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). In this trained AlexNet, invariance jumped from a low level in convolutional layers to a high level in the fully connected layers and was highest for horizontal reflection, lower for vertical reflection, and lowest for 90° rotation.</p><p>In an untrained AlexNet, the reflection equivariance of the first convolutional layer was higher than in the trained network. However, this measure subsequently decreased in the deeper convolutional layers to a level lower than that observed for the corresponding layers in the trained network. The higher level of reflection-equivariance of the first layer of the untrained network can be explained by the lack of strongly oriented filters in the randomly initialized layer weights. While the training leads to oriented filters in the first layer, it also promotes downstream convolutional representations that have greater reflection-equivariance than those in a randomly-initialized, untrained network.</p><p>The gap between horizontal reflection and vertical reflection in terms of both equivariance and invariance was less pronounced in the untrained network (<xref ref-type="fig" rid="fig3">Figure 3B</xref>), indicating a contribution of task training to the special status of horizontal reflection. In contrast, the gap between vertical reflection and 90° rotation in terms of both equivariance and invariance was preserved in the untrained network. This indicates that the greater degree of invariance and equivariance for vertical reflection compared to 90° rotation is largely caused by the test images’ structure rather than task training. One interpretation is that, unlike 90° rotation, vertical and horizontal reflection both preserve the relative prevalence of vertical and horizontal edge energy, which may not be equal in natural images (<xref ref-type="bibr" rid="bib12">Coppola et al., 1998</xref>; <xref ref-type="bibr" rid="bib66">Torralba and Oliva, 2003</xref>; <xref ref-type="bibr" rid="bib28">Henderson and Serences, 2021</xref>; <xref ref-type="bibr" rid="bib24">Girshick et al., 2011</xref>). To test if the emergence of equivariance and invariance under horizontal reflection is unique to our controlled stimulus set (which contained many horizontally symmetrical images), we repeated these analyses using natural images sampled from the ImageNet validation set (<xref ref-type="fig" rid="fig3">Figure 3C–D</xref>). The training-dependent layer-by-layer increase in equivariance and invariance to horizontal reflection was as pronounced for natural images as it was for the rendered 3D object images. Therefore, the emergent invariance and equivariance under horizontal reflection are not an artifact of the synthetic object stimulus set.</p><p>Repeating these analyses on random noise images, the ImageNet-trained AlexNet still showed a slightly higher level of horizontal reflection-equivariance (<xref ref-type="fig" rid="fig3">Figure 3E</xref>), demonstrating the properties of the features learned in the task independently of symmetry structure in the test images. When we evaluated an untrained AlexNet on random noise images (<xref ref-type="fig" rid="fig3">Figure 3F</xref>), that is, when there was no structure in either the test stimuli or the network weights, the differences between horizontal reflection, vertical reflection, and rotation measures disappeared, and the invariance and equivariance measures were zero, as expected (see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> for the distribution of equivariance and invariance across test images and <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for analysis of horizontal reflection invariance across different object categories).</p><p>To summarize this set of analyses, a high level of reflection-invariance is associated with the layer’s pooling size and the reflection-equivariance of its feeding representation. The pooling size depends only on the architecture, but the reflection-equivariance of the feeding representation depends on both architecture and training. Training on recognizing objects in natural images induces a greater degree of invariance and equivariance to horizontal reflection compared to vertical reflection or 90° rotation. This is consistent with the statistics of natural images as experienced by an upright observer looking, along a horizontal axis, at upright bilaterally symmetric objects. Image reflection, in such a world ordered by gravity, does not change the category of an object (although rare examples of dependence of meaning on handedness exist, such as the letters p and q, and molecules whose properties depend on their chirality). However, the analyses reported thus far leave unclear whether natural image statistics alone or the need to disregard the handedness for categorization drive mirror-symmetric viewpoint tuning. In the following section, we examine what it is about the training that drives viewpoint tuning to be mirror-symmetric.</p></sec><sec id="s2-3"><title>Learning to discriminate among categories of bilaterally symmetric objects induces mirror-symmetric viewpoint tuning</title><p>To examine how task demand and visual diet influence mirror-symmetric viewpoint tuning, we trained four deep convolutional neural networks of the same architecture on different datasets and tasks (<xref ref-type="fig" rid="fig4">Figure 4</xref>). The network architecture and training hyper-parameters are described in the Materials and Methods section (for training-related metrics, see <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). Once trained, each network was evaluated on the 3D object images used in <xref ref-type="fig" rid="fig2"><xref ref-type="fig" rid="fig2">Figure 2</xref></xref>, measuring mirror-symmetric viewpoint tuning qualitatively (<xref ref-type="fig" rid="fig4">Figure 4B</xref>) and quantitatively (<xref ref-type="fig" rid="fig4">Figure 4C</xref>).</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The effect of training task and training dataset on mirror-symmetric viewpoint tuning.</title><p>(<bold>A</bold>) Four datasets are used to train deep neural networks of the same architecture: CIFAR-10, a natural image dataset with ten bilaterally symmetric object categories; SVHN, a dataset with mostly asymmetric categories (the ten numerical digits); symSVHN, a version of the SVHN dataset in which the categories were made bilaterally symmetric by horizontally reflecting half of the training images (so the digit 7 and its mirrored version count as members of the same category); asymSVHN, the same image set as in symSVHN but with the mirrored images assigned to ten new distinct categories (so the digit 7 and its mirrored version count as members of distinct categories). (<bold>B</bold>) Each row represents the RDMs of the face exemplar images from nine viewpoints for each trained network corresponding to its left side panel. Each entry of the RDM represents the dissimilarity (1 - Pearson’s r) between two pairs of image-induced activity vectors in the corresponding layer. The RDMs’ order from left to right refers to the depth of layers within the network. As the dissimilarity color bar indicates, the dissimilarity values increase from black to white color. (<bold>C</bold>) Mirror-symmetric viewpoint tuning index values across layers for nine object categories in each of the four networks. The solid circles refer to the average of the index across 25 exemplars within each object category for three networks trained on 10 labels. The red dashed line with open circles belongs to the asymSVHN network trained on 20 labels. The gray dashed lines indicate the index of zero. Error bars represent the standard error of the mean calculated across exemplars.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Network learning curves.</title><p>(<bold>A–D</bold>) Loss and accuracy curves for the networks trained by CIFAR-10 (<bold>A</bold>), SVHN (<bold>B</bold>), symSVHN (<bold>C</bold>), asymSVHN (<bold>D</bold>) datasets. The x-axis denotes training epochs. Note that the accuracy of asymSVHN might be negatively affected by the inclusion of relatively symmetric categories such as 0 and 8. We used drop-out during training, which resulted in higher training loss compared to the validation loss.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig4-figsupp1-v2.tif"/></fig></fig-group><p>First, we considered a network trained on CIFAR-10 (<xref ref-type="bibr" rid="bib37">Krizhevsky and Hinton, 2009</xref>), a dataset of small images of 10 bilaterally symmetric categories (airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks). Although this dataset contains no human face images (such images appear coincidentally in the ImageNet dataset, <xref ref-type="bibr" rid="bib72">Yang et al., 2022</xref>), the CIFAR-10-trained network reproduced the result of a considerable level of mirror-symmetric viewpoint tuning for faces in layers fc1 and fc2 (<xref ref-type="fig" rid="fig4">Figure 4B</xref>, top row). This network also showed mirror-symmetric viewpoint tuning for other bilaterally symmetric objects such as cars, airplanes, and boats (<xref ref-type="fig" rid="fig4">Figure 4C</xref>, blue lines).</p><p>We then considered a network trained on SVHN (Street View House Numbers; <xref ref-type="bibr" rid="bib48">Netzer et al., 2011</xref>), a dataset of photographs of numerical digits. Its categories are mostly asymmetric (since all 10 digits except for ‘0’ and ‘8’ are asymmetric). Unlike the network trained on CIFAR-10, the SVHN-trained network showed a very low level of mirror-symmetric viewpoint tuning for faces. Furthermore, its levels of mirror-symmetric viewpoint tuning for cars, airplanes, and boats were reduced relative to the CIFAR-10-trained network.</p><p>SVHN differs from CIFAR-10 both in its artificial content and the asymmetry of its categories. To disentangle these two factors, we designed a modified dataset, ‘symSVHN’. Half of the images in symSVHN were horizontally reflected SVHN images. All the images maintained their original category labels (e.g. images of the digit 7 and images of a mirrored 7 belonged to the same category). We found that the symSVHN-trained network reproduced the mirror-symmetric viewpoint tuning observed in the CIFAR-10-trained network.</p><p>Last, we modified the labels of symSVHN such that the flipped digits would count as 10 separate categories, in addition to the 10 unflipped digit categories. This dataset (‘asymSVHN’) has the same images as symSVHN, but it is designed to require reflection-sensitive recognition. The asymSVHN-trained network reproduced the low levels of mirror-symmetric viewpoint tuning observed for the original SVHN dataset. Together, these results suggest that given the spatial pooling carried out by fc1, the task demand of <italic>reflection-invariant recognition</italic> is a sufficient condition for the emergence of mirror-symmetric viewpoint tuning for faces.</p></sec><sec id="s2-4"><title>Equivariant local features drive mirror-symmetric viewpoint tuning</title><p>What are the image-level visual features that drive the observed mirror-symmetric viewpoint tuning? Do mirror-reflected views of an object induce similar representations because of global 2D configurations shared between such views? Or alternatively, are reflection-equivariant local features sufficient to explain the finding of similar responses to reflected views in fc1? We used a masking-based importance mapping technique (<xref ref-type="bibr" rid="bib54">Petsiuk et al., 2018</xref>) to characterize which features drive the responses of units with mirror-symmetric viewpoint tuning. First, we created importance maps whose elements represent how local features influence each unit’s response to different object views. The top rows of panels A and B in <xref ref-type="fig" rid="fig5">Figure 5</xref> show examples of such maps for two units, one that shows considerable mirror-symmetric viewpoint tuning for cars and another that shows considerable mirror-symmetric viewpoint tuning for faces.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Reflection-invariant viewpoint-specific responses are driven mostly by local features.</title><p>This figure traces image-level causes for the mirror-symmetric viewpoint tuning using Randomized Input Sampling for Explanation (RISE, <xref ref-type="bibr" rid="bib54">Petsiuk et al., 2018</xref>). (<bold>A</bold>) Analysis of the features of different views of a car exemplar that drive one particular unit in fully connected layer fc6 of AlexNet. The topmost row in each panel depicts an image-specific <italic>importance map</italic> overlaid to each view of the car, charting the contribution of each pixel to the unit’s response. The second row (‘deletion’) depicts a version of each input image in which the 25% most contributing pixels are masked with the background gray color. The third row (‘insertion‘) depicts a version of the input images in which only the most contributing 25% of pixels appear. The last row represents the shuffled spatial configuration of extracted local features, which maintains their structure and changes their locations. The charts on the right depict the units’ responses to the original, deletion, insertion, and shuffled images. The dashed line indicates the units’ response to a blank image. The y-axis denotes the unit’s responses compared to its response to a blank image. (<bold>B</bold>) Analogous analysis of the features of different views of a face that drive a different unit in fully connected layer fc6 of AlexNet. (<bold>C</bold>) Testing local contributions to mirror-symmetric viewpoint tuning across all object exemplars and insertion/deletion thresholds. For each object exemplar, we selected a unit with a highly view-dependent but symmetric viewpoint tuning (the unit whose tuning function was maximally correlated with its reflection). We then measured the correlation between this tuning function and the tuning function induced by insertion or deletion images that were generated by a range of thresholding levels (from 10 to 90%). Note that each threshold level consists of images with the same number of non-masked pixels appearing in the insertion and deletion conditions. In the insertion condition, only the most salient pixels are retained, and in the deletion condition, only the least salient pixels are retained. The solid circles and error bars indicate the median and standard deviation over 225 objects, respectively. The right y-axis depicts the difference between insertion and deletion conditions. Error bars represent the SEM. (<bold>D</bold>) For each of 225 objects, we selected units with mirror-symmetric viewpoint tuning above the 95 percentile (≈200 units) and averaged their corresponding importance maps. Next, we extracted the top 25% most contributing pixels from the averaged maps (insertion) and shuffled their spatial configuration (shuffled). We then measured the viewpoint-RDMs for either the inserted or shuffled object image set. The scatterplot compares the mirror-symmetric viewpoint tuning index between insertion and shuffled conditions, calculated across the selected units. Each solid circle represents an exemplar object. The high explained variance indicates that the global configuration does not play a significant role in the emergence of mirror-symmetric viewpoint tuning.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>The emergence of mirror symmetric weight tensors in AlexNet.</title><p>In order to examine the symmetry of neural network weights, we measured the linear correlation between each convolutional weight kernel and its horizontally (panel A) or vertically (panel B) flipped counterpart. To avoid replicated observations in the correlation analysis, we considered only the left (or top) half of the matrix, and excluded the central column (or row). Each dot represents one channel. This measurement was done for each convolutional layer in an AlexNet trained on ImageNet, as well as in an untrained AlexNet. The symmetry of the incoming weights to fc6 was evaluated in a similar fashion (note that the weights leading into this layer still have an explicit spatial layout, unlike fc7 and fc8). This analysis demonstrates that in the ImageNet-trained AlexNet network, weight symmetry increases with depth. Note that ImageNet training induces some highly asymmetrical kernels in conv1 and conv2. Together, these results suggest that while asymmetrical filters are useful low-level representations, the trained network incorporates symmetric weight kernels to generate downstream reflection-invariant representations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Individual neural network units exhibiting mirror-symmetric view tuning according to the criterion employed by <xref ref-type="bibr" rid="bib4">Baek et al., 2021a</xref>.</title><p>We screened the units of the deepest convolutional layer of an untrained AlexNet according to the selection criterion proposed by Baek and colleagues (Figure S10 in <xref ref-type="bibr" rid="bib4">Baek et al., 2021a</xref>), using the official code sharedon <ext-link ext-link-type="uri" xlink:href="https://github.com/vsnnlab/Face">https://github.com/vsnnlab/Face</ext-link> (<xref ref-type="bibr" rid="bib5">Baek et al., 2021b</xref>). Each trace represents an individual unit response profile. The x-axis shows the views: left profile (LP), left half-profile (LHP), frontal (<bold>F</bold>), right half-profile (RHP), and right profile (RP). The y-axis depicts the response of an individual unit, z-scored standardized across images. The left panel displays units with full-profile symmetry response tuning, and the right panel displays units with half-profile response tuning. Reproducing Baek and colleagues’ findings, we identified many randomly initialized units that met the selection criterion Baek and colleagues proposed. However, as this figure illustrates, a large proportion of these units exhibit markedly asymmetric tuning profiles. Specifically, while the selection criterion requires unit activation to peak at either full-profile or half-profile views, many such units exhibit less pronounced or even minimal responses to opposite views. In our subsequent analyses (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplements 3</xref> and <xref ref-type="fig" rid="fig5s4">4</xref>), we applied a stricter selection criterion.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 3.</label><caption><title>Selecting individual units with genuine mirror-symmetric viewpoint tuning.</title><p>(Left column) Aggregated full-profile (panel A) and half-profile (panel D) mirror-symmetric units (detailed individually in <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>), accompanied by their average tuning curves (represented as thick lines). Note that the average viewpoint tuning profile demonstrates strong mirror symmetry, yet this profile is unrepresentative of the individual units. (Middle column) The tuning profiles of units selected using a revised selection criterion. Specifically, we required the second peak to occur in response to the view opposite the first peak and ensured that the frontal view elicited the lowest response. This criterion led to fewer units being selected yet ensured each unit individually exhibited mirror-symmetric viewpoint tuning. (Right column) Units meeting the revised criterion in a trained network. Training increased the number of units individually exhibiting mirror-symmetry tuning profiles, as quantified further in <xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig5-figsupp3-v2.tif"/></fig><fig id="fig5s4" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 4.</label><caption><title>Training-dependent emergence of units with mirror-symmetric viewpoint tuning across neural network layers.</title><p>Using our revised criterion for identifying units with mirror-symmetric tuning, we estimated the percentage of such units in each layer of an AlexNet network (Torchvision implementation), before and after training on ImageNet. (Left panel) The percentage of units with mirror-symmetric tuning out of units defined as ‘face-selective’ according to the face-selectivity criterion proposed by <xref ref-type="bibr" rid="bib4">Baek et al., 2021a</xref>. (Right panel) The percentage of units with mirror-symmetric viewpoint tuning, out of all of the units in each layer. Note that the latter measurement aligns more closely with the population RSA analyses in the main text, which likewise consider all units rather than just a face-selective sub-population. For each layer, the orange bars indicate the average percentage of mirror-symmetric units observed across 10 random network initializations, with the orange error bars denoting a 95% confidence interval for this proportion. The blue bars indicate the percentage of such units post-training. Since we used a single trained network for this analysis, the blue error bars denote 95% binomial confidence intervals calculated within each layer rather than across realizations. The first fully connected layer shows the most pronounced training-dependent emergence of mirror-symmetric viewpoint tuning units, consistent with the findings obtained with the population-level RSA findings described in the main text.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig5-figsupp4-v2.tif"/></fig></fig-group><p>Next, we empirically tested whether the local features highlighted by the importance maps are sufficient and necessary for generating mirror-symmetric viewpoint tuning. We used two image manipulations: insertion and deletion (<xref ref-type="bibr" rid="bib54">Petsiuk et al., 2018</xref>; <xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, middle rows). When we retained only the most salient pixels (i.e. insertion), we observed that the units’ mirror-symmetric viewpoint tuning levels were similar to those induced by unmodified images (<xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, dark blue lines). This result demonstrates that the local features suffice for driving mirror-symmetrically tuned responses. Conversely, greying out the most salient pixels (deletion) led to a complete loss of mirror-symmetric viewpoint tuning (<xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, red lines). This result demonstrates that the local features are necessary to drive mirror-symmetrically tuned responses. To examine this effect systematically, we selected one unit for each of the 225 3D objects that showed high mirror-symmetric viewpoint tuning. We then tested these 225 units with insertion and deletion images produced with different thresholds (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Across all threshold levels, the response to insertion images was more similar to the response to unmodified images, whereas deletion images failed to induce mirror-symmetric viewpoint tuning.</p><p>These results indicate a role for local features in mirror-symmetric tuning. However, the features may form larger-scale configurations synergistically. To test the potential role of such configurations, we shuffled contiguous pixel patches that were retained in the insertion condition. This manipulation destroyed global structure while preserving local features (<xref ref-type="fig" rid="fig5">Figure 5A–B</xref>, bottom row). We found that the shuffled images largely preserved the units’ mirror-symmetric viewpoint tuning (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). Thus, it is the mere presence of a similar set of reflected local features (rather than a reflected global configuration) that explains most of the acquired mirror-symmetric viewpoint tuning. Note that such local features must be either symmetric at the image level (e.g. the wheel of a car in a side view), or induce a reflection-equivariant representation (e.g. an activation map that highlights profile views of a nose, regardless of their orientation). The fc6 layer learns highly symmetrical weight maps, reducing the sensitivity to local feature configurations and enabling the generation of downstream reflection-invariant representations compared to convolutional layers (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p></sec><sec id="s2-5"><title>Representational alignment between artificial networks and macaque face patches</title><p>How does the emergence of mirror-invariance in CNNs manifest in the alignment of these networks with neural representations of faces in the macaque face-patch system? In line with (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>), we reanalyzed the neural recordings from <xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref> by correlating neural population RDMs, each describing the dissimilarities among neural responses to face images of varying identities and viewpoints, with corresponding model RDMs, derived from neural network layer representations of the stimulus set (<xref ref-type="fig" rid="fig6">Figure 6</xref>, top row). In addition to the AL face-patch, we considered MLMF, which is sensitive to reflection (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>), and AM, which is mostly viewpoint invariant (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). Following the approach of Yildirim and colleagues, the neural networks were presented with segmented reconstructions, where non-facial pixels were replaced by a uniform background.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Reflection-invariant and reflection-sensitive contributions to the representational similarity between monkey face patch neurons and AlexNet layers.</title><p>The neural responses were obtained from <xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>, where electrophysiological recordings were conducted in three faces patches while the monkeys were presented with human faces of various identities and views. (Top row) linear correlations between RDMs from each network layer and each monkey face patch (MLMF, AL, AM). Error bars represent standard deviations estimated by bootstrapping individual stimuli (see Materials and methods). The gray area represents the neural data’s noise ceiling, whose lower bound was determined by Spearman-Brown-corrected split-half reliability, with the splits applied across neurons. (Bottom row) Each model–brain RDM correlation is decomposed into the additive contribution of two feature components: reflection-sensitive (purple) and reflection-invariant (yellow). <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplements 1</xref>–<xref ref-type="fig" rid="fig6s3">3</xref> present the same analyses applied to a diverse set of neural network models, across the three regions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Alignment of MLMF and neural network representations across diverse architecures.</title><p>As in <xref ref-type="fig" rid="fig6">Figure 6</xref>, representational alignment was measured with respect to the FIV dataset. Top row depicts the correlation between model RDMs, measured in each individual neural network layer, and a neural population RDM estimated using neural recordings from the MLMF face patch. Black circles represent correlation coefficients averaged across bootstrap simulations (resampling individual stimuli), with error bars denoting standard deviations across bootstrap simulations. The gray area represents the neural RDM’s noise ceiling; its lower bound was determined through a Spearman-Brown corrected split-half reliability estimate, splitting the neurons into equally sized random subsets. The bottom row displays Shapley values reflecting the contributions of the reflection-invariant and reflection-sensitive components in the model RDMs. Deeper convolutional layers in various convolutional architectures demonstrated strong alignment with MLMF data; this alignment is primarily explained by reflection-sensitive features.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Alignment of AL and neural network representations across diverse architecures.</title><p>The analysis is analogous to what is described in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, but for the AL face patch. In various convolutional architectures, the fully connected and average pooling layers showed notable representational alignment with the AL patch. This alignment is predominantly explained by features that are invariant to reflection, rather than those sensitive to reflection.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig6-figsupp2-v2.tif"/></fig><fig id="fig6s3" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 3.</label><caption><title>Alignment of AM and neural network representations across diverse architecures.</title><p>The analysis is analogous to what is described in <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>, but for the AM face patch. The deepest layers in different network architectures, with the exception of ViT, show strong representational alignment with the AM face patch. This alignment is predominantly explained by features that are invariant to reflection, rather than those sensitive to it.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-90256-fig6-figsupp3-v2.tif"/></fig></fig-group><p>Consistent with previous findings (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>), MLMF was more aligned with the CNNs’ mid-level representation, notably the last convolutional layers (<xref ref-type="fig" rid="fig6">Figure 6A</xref>). The AL face patch showed its highest representational alignment with the first fully connected layer (<xref ref-type="fig" rid="fig6">Figure 6B</xref>), coinciding with the surge of the mirror-symmetric viewpoint tuning index at this processing level (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). The AM face patch aligned most with the fully connected layers (<xref ref-type="fig" rid="fig6">Figure 6C</xref>).</p><p>These correlations between model and neural RDMs reflect the contribution of multiple underlying image features. To disentangle the contribution of reflection-invariant and reflection-sensitive representations to the resulting RDM correlation, we computed two additional model representations for each neural network layer: (1) a reflection-invariant representation, obtained by element-wise addition of two activation tensors, one elicited in response to the original stimuli and the other in response to mirror-reflected versions of the stimuli; and, (2) a reflection-sensitive representation, obtained by element-wise subtraction of these two tensors. The two resulting feature components sum to the original activation tensor; a fully reflection-invariant representation would be entirely accounted for by the first component. For each CNN layer, we obtained the two components and correlated each of them with the unaltered neural RDMs. Through the Shapley value feature attribution method (<xref ref-type="bibr" rid="bib61">Shapley, 1953</xref>), we transformed the resulting correlation coefficients into additive contributions of the reflection-invariant and reflection-sensitive components to the original model-brain RDM correlations (<xref ref-type="fig" rid="fig6">Figure 6D–F</xref>).</p><p>In the MLMF face patch, reflection-sensitive features contributed more than reflection-invariant ones, consistent with the dominance of reflection-sensitive information in aligning network layers with MLMF data (<xref ref-type="fig" rid="fig6">Figure 6D</xref>). Conversely, in the AL and AM face patches, reflection-invariant features accounted for nearly all the observed model–brain RDM correlations (<xref ref-type="fig" rid="fig6">Figure 6E, F</xref>). For most of the convolutional layers, the contribution of the reflection-sensitive component to AL or AM alignment was negative—meaning that if the layers’ representations were more reflection-invariant, they could have explained the neural data better.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this paper, we propose a simple learning-driven explanation for the mirror-symmetric viewpoint tuning for faces in the macaque AL face-patch. We found that CNNs trained on object recognition reproduce this tuning in their fully connected layers. Based on in silico experiments, we suggest two jointly sufficient conditions for the emergence of mirror-symmetric viewpoint tuning. First, training the network to discriminate among bilaterally symmetric 3D objects yields reflection-equivariant representations in the deeper convolutional layers. Then, subsequent pooling of these reflection-equivariant responses by units with large receptive fields leads to reflection-invariant representations with mirror-symmetric view tuning similar to that observed in the AL face patch. Like our models, monkeys need to recognize bilaterally symmetric objects that are oriented by gravity. To achieve robustness to view, the primate visual system can pool responses from earlier stages of representation. We further show that in CNNs, such tuning is not limited to faces and occurs for multiple object categories with bilateral symmetry. This result yields a testable prediction for primate electrophysiology and fMRI.</p><sec id="s3-1"><title>Mirror-symmetric viewpoint tuning in brains and machines</title><p>Several species, including humans, confuse lateral mirror images (e.g. the letters b and d) more often than vertical mirror images (e.g. the letters b and p; <xref ref-type="bibr" rid="bib63">Sutherland, 1960</xref>; <xref ref-type="bibr" rid="bib65">Todrin and Blough, 1983</xref>). Children often experience this confusion when learning to read and write (<xref ref-type="bibr" rid="bib47">Nelson and Peoples, 1975</xref>; <xref ref-type="bibr" rid="bib9">Bornstein et al., 1978</xref>; <xref ref-type="bibr" rid="bib14">Cornell, 1985</xref>; <xref ref-type="bibr" rid="bib15">Dehaene et al., 2010</xref>). Single-cell recordings in macaque monkeys presented with simple stimuli indicate a certain degree of reflection-invariance in IT neurons (<xref ref-type="bibr" rid="bib59">Rollenhagen and Olson, 2000</xref>; <xref ref-type="bibr" rid="bib7">Baylis and Driver, 2001</xref>). Human neuroimaging experiments also revealed reflection-invariance across higher-level visual regions for human heads (<xref ref-type="bibr" rid="bib2">Axelrod and Yovel, 2012</xref>; <xref ref-type="bibr" rid="bib34">Kietzmann et al., 2012</xref>; <xref ref-type="bibr" rid="bib56">Ramírez et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Kietzmann et al., 2017</xref>) and other bilaterally symmetric objects (<xref ref-type="bibr" rid="bib16">Dilks et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Ramírez et al., 2014</xref>).</p><p>When a neuron’s response is reflection-invariant and yet the neuron responds differently to different object views, it is exhibiting mirror-symmetric viewpoint tuning. Such tuning has been reported in a small subset of monkeys’ STS and IT cells in early recordings (<xref ref-type="bibr" rid="bib53">Perrett et al., 1991</xref>; <xref ref-type="bibr" rid="bib45">Logothetis et al., 1995</xref>). fMRI-guided single-cell recordings revealed the prevalence of this tuning profile among the cells of face patch AL (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). The question of why mirror-symmetric viewpoint tuning emerges in the cortex has drawn both mechanistic and functional explanations. Mechanistic explanations suggest that mirror-symmetric viewpoint tuning is a by-product of increasing interhemispheric connectivity and receptive field sizes. Due to the anatomical symmetry of the nervous system and its cross-hemispheric interconnectivity, mirror-image pairs activate linked neurons in both hemispheres (<xref ref-type="bibr" rid="bib13">Corballis and Beale, 1976</xref>; <xref ref-type="bibr" rid="bib25">Gross et al., 1977</xref>). A functional perspective explains partial invariance as a stepping stone toward achieving fully view-invariant object recognition (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). Our results support a role for both of these explanations. We showed that global spatial pooling is a sufficient condition for the emergence of reflection-invariant responses, <italic>if</italic> the pooled representation is reflection-equivariant. Global average pooling extends the spatially integrated stimulus region. Likewise, interhemispheric connectivity may result in cells with larger receptive fields that cover both hemifields.</p><p>A recent work by <xref ref-type="bibr" rid="bib57">Revsine et al., 2024</xref> incorporated biological constraints, including interhemispheric connectivity, into a model processing solely low-level stimulus features, namely intensity and contrast. Their results suggest that such features might be sufficient for explaining apparent mirror-symmetric viewpoint tuning in fMRI studies. In our study, we standardized stimulus intensity and contrast across objects and viewpoints (see Methods), eliminating these features as potential confounds. Additionally, applying a dissimilarity measure that is invariant to the overall magnitude of the representations did not alter the observed trends in mirror-symmetric viewpoint tuning results (<xref ref-type="fig" rid="fig2s7">Figure 2—figure supplement 7</xref>). Therefore, we suggest that spatial pooling can yield genuine mirror-symmetric viewpoint tuning in CNNs and brains by summating equivariant mid-level visual features (see <xref ref-type="fig" rid="fig5">Figure 5</xref>) that are learning-dependent (<xref ref-type="fig" rid="fig4">Figure 4</xref>).</p><p>We also showed that equivariance can be driven by the task demand of discriminating among objects that have bilateral symmetry (see <xref ref-type="bibr" rid="bib50">Olah et al., 2020</xref> for an exploration of emergent equivariance using activation maximization). The combined effect of equivariance and pooling leads to a leap in reflection-invariance between the last convolutional layer and the fully connected layers in CNNs. This transition may be similar to the transition from view-selective cells in face patches ML/MF to mirror-symmetric viewpoint-selective cells in AL. In both CNNs and primate cortex, the mirror-symmetrically viewpoint-tuned neurons are a penultimate stage on the path to full view invariance (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>).</p></sec><sec id="s3-2"><title>Unifying the computational explanations of mirror-symmetric viewpoint tuning</title><p>Two computational models have been suggested to explain AL’s mirror-symmetric viewpoint tuning, the first attributing it to Hebbian learning with Oja’s rule (<xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>), the second to training a CNN to invert a face-generative model (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>). A certain extent of mirror-symmetric viewpoint tuning was also observed in CNNs trained on face identification (Fig. 3Eii in <xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>, Figure 2 in <xref ref-type="bibr" rid="bib55">Raman and Hosoya, 2020</xref>). In light of our findings here, these models can be viewed as special cases of a considerably more general class of models. Our results generalize the computational account in terms of both stimulus domain and model architecture. Both (<xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>) and (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>) trained neural networks with face images. Here, we show that it is not necessary to train on a specific object category (including faces) in order to acquire reflection equivariance and invariance for exemplars of that category. Instead, learning mirror-invariant stimulus-to-response mappings gives rise to equivariant and invariant representations also for novel stimulus classes.</p><p>Our claim that mirror-symmetric viewpoint tuning is learning-dependent may seem to be in conflict with findings by Baek and colleagues (<xref ref-type="bibr" rid="bib4">Baek et al., 2021a</xref>). Their work demonstrated that units with mirror-symmetric viewpoint tuning profile can emerge in randomly initialized networks. Reproducing Baek and colleagues’ analysis, we confirmed that such units occur in untrained networks (<xref ref-type="fig" rid="fig5s3">Figure 5—figure supplement 3</xref>). However, we also identified that the original criterion for mirror-symmetric viewpoint tuning employed in <xref ref-type="bibr" rid="bib4">Baek et al., 2021a</xref> was satisfied by many units with asymmetric tuning profiles (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplements 2</xref> and <xref ref-type="fig" rid="fig5s3">3</xref>). Once we applied a stricter criterion, we observed a more than twofold increase in mirror-symmetric units in the first fully connected layer of a trained network compared to untrained networks of the same architecture (<xref ref-type="fig" rid="fig5s4">Figure 5—figure supplement 4</xref>). This finding highlights the critical role of training in the emergence of mirror-symmetric viewpoint tuning in neural networks also at the level of individual units.</p><p>Our results also generalize the computational account of mirror-symmetric viewpoint tuning in terms of the model architectures. The two previous models incorporated the architectural property of spatial pooling: the inner product of inputs and synaptic weights in the penultimate layer of the HMAX-like model in <xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref> and the global spatial pooling in the f4 layer of the EIG model (<xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>). We showed that in addition to the task, such spatial pooling is an essential step toward the emergence of mirror-symmetric tuning in our findings.</p></sec><sec id="s3-3"><title>Limitations</title><p>The main limitation of the current study is that our findings are simulation-based and empirical in nature. Therefore, they might be limited to the particular design choices shared across the range of CNNs we evaluated. This limitation stands in contrast with the theoretical model proposed by Leibo and colleagues (<xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref>), which is reflection-invariant by construction. However, it is worth noting that the model proposed by Leibo and colleagues is reflection-invariant only with respect to the horizontal center of the input image (<xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref>). CNNs trained to discriminate among bilaterally symmetric categories develop mirror-symmetric viewpoint tuning across the visual field (<xref ref-type="fig" rid="fig2s8">Figure 2—figure supplement 8</xref>). The latter result pattern is more consistent with the relatively position-invariant response properties of AL neurons (Fig. S10 in <xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>).</p><p>A second consequence of the simulation-based nature of this study is that our findings only establish that mirror-symmetric viewpoint tuning is a viable computational means for achieving view invariance; they do not prove it to be a necessary condition. In fact, previous modeling studies <xref ref-type="bibr" rid="bib19">Farzmahdi et al., 2016</xref>; <xref ref-type="bibr" rid="bib42">Leibo et al., 2015</xref>; <xref ref-type="bibr" rid="bib43">Leibo et al., 2017</xref> have demonstrated that a direct transition from view-specific processing to view invariance is possible. However, in practice, we observe that both CNNs and the face-patch network adopt solutions that include intermediate representations with mirror-symmetric viewpoint tuning.</p></sec><sec id="s3-4"><title>A novel prediction: mirror-symmetric viewpoint tuning for non-face objects</title><p>Mirror-symmetric viewpoint tuning has been mostly investigated using face images. Extrapolating from the results in CNNs, we hypothesize that mirror-symmetric viewpoint tuning for non-face objects should exist in cortical regions homologous to AL. The mirror-symmetric tuning of these objects does not necessarily have to be previously experienced by the animal.</p><p>This hypothesis is consistent with the recent findings of <xref ref-type="bibr" rid="bib6">Bao et al., 2020</xref>. They report a functional clustering of IT into four separate networks. Each of these networks is elongated across the IT cortex and consists of three stages of processing. We hypothesize that the intermediate nodes of the three non-face selective networks have reflection-invariant yet view-selective tuning, analogous to AL’s representation of faces.</p><p>Our controlled stimulus set, which includes systematic 2D snapshots of 3D real-world naturalistic objects, is available online. Future electrophysiological and fMRI experiments utilizing this stimulus set can verify whether the mirror-symmetric viewpoint tuning for non-face categories we observe in task-trained CNNs also occurs in the primate IT.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>3D object stimulus set</title><p>We generated a diverse image set of 3D objects rendered from multiple views in the depth rotation. Human faces were generated using the Basel Face Model (<xref ref-type="bibr" rid="bib23">Gerig et al., 2018</xref>). For the non-face objects, we purchased access to 3D models on TurboSquid (<ext-link ext-link-type="uri" xlink:href="http://www.turbosquid.com">http://www.turbosquid.com</ext-link>). The combined object set consisted of nine categories (cars, boats, faces, chairs, airplanes, animals, tools, fruits, and flowers). Each category included 25 exemplars. We rendered each exemplar from nine views, giving rise a total of 2025 images. The views span from –90° (left profile) to +90°, with steps of 22.5°. The rendered images were converted to grayscale, placed on a uniform gray background, and scaled to 227 × 227 pixels to match the input image size of AlexNet, or to 224 × 224 to match the input image size of the VGG-like network architectures. Mean luminance and contrast of non-background pixels were equalized across images using the SHINE toolbox (<xref ref-type="bibr" rid="bib70">Willenbockel et al., 2010</xref>).</p></sec><sec id="s4-2"><title>Pre-trained neural networks</title><p>We selected both shallow and deep networks with varied architectures and objective functions. We evaluated convolutional networks trained on ImageNet, including AlexNet (<xref ref-type="bibr" rid="bib38">Krizhevsky et al., 2012</xref>), VGG16 (<xref ref-type="bibr" rid="bib62">Simonyan and Zisserman, 2015</xref>), ResNet50, ConvNeXt. Additionally, we evaluated VGGFace–a similar architecture to VGG16, trained on the VGG Face dataset (<xref ref-type="bibr" rid="bib51">Parkhi et al., 2015</xref>), ViT with its non-convolutional architecture, EIG as a face generative model, and the shallow, biologically inspired HMAX model. All these networks, except for VGGFace, EIG, and HMAX, were trained on the ImageNet dataset (<xref ref-type="bibr" rid="bib60">Russakovsky et al., 2015</xref>), which consists of ∼1.2 million natural images from 1000 object categories (available on Matlab Deep Learning Toolbox and Pytorch frameworks, <xref ref-type="bibr" rid="bib64">The MathWorks Inc, 2019</xref>; <xref ref-type="bibr" rid="bib52">Paszke et al., 2019</xref>). The VGGFace model was trained on ∼2.6 million face images from 2622 identities (available on the MatConvNet library, <xref ref-type="bibr" rid="bib68">Vedaldi and Lenc, 2015</xref>). Each convolutional network features a distinct number of convolutional (conv), max-pooling (pool), rectified linear unit (relu), normalization (norm), average pooling (avgpool), and fully connected (fc) layers, among others, dictated by its architecture. For untrained AlexNet and VGG16 networks, we initialized the weights and biases using a random Gaussian distribution with a zero mean and a variance inversely proportional to the number of inputs per unit (<xref ref-type="bibr" rid="bib41">LeCun et al., 2012</xref>).</p></sec><sec id="s4-3"><title>Trained-from-scratch neural networks</title><p>To control for the effects of the training task and ‘visual diet’, we trained four networks employing the same convolutional architecture on four different datasets: CIFAR-10, SVHN, symSVHN, and asymSVHN.</p><sec id="s4-3-1"><title>CIFAR-10</title><p>CIFAR-10 consists of 60,000 RGB images of 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck) downscaled to 32 × 32 pixels (<xref ref-type="bibr" rid="bib37">Krizhevsky and Hinton, 2009</xref>). We randomly split CIFAR-10’s designated training set into 45,000 images used for training and 5,000 images used for validation. No data augmentation was employed. The reported classification accuracy (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>) was evaluated on the remaining 10,000 CIFAR-10 test images.</p></sec><sec id="s4-3-2"><title>SVHN</title><p>SVHN (<xref ref-type="bibr" rid="bib48">Netzer et al., 2011</xref>) contains 99,289 RGB images of 10 digits (0–9) taken from real-world house number photographs (<xref ref-type="bibr" rid="bib48">Netzer et al., 2011</xref>), cropped to character bounding boxes and downsized to 32 × 32 pixels. We split the dataset into 73,257 images for the training set and 26,032 images for the test set. As with the CIFAR-10 dataset, we randomly selected 10% of training images as the validation set.</p></sec><sec id="s4-3-3"><title>symSVHN and asymSVHN</title><p>As a control experiment, we horizontally flipped half of the SVHN training images while keeping their labels unchanged. This manipulation encouraged the model trained on these images to become reflection-invariant in its decisions. This dataset was labeled as ‘symSVHN’. In a converse manipulation, we applied the same horizontal flipping but set the flipped images’ labels to 10 new classes. Therefore, each image in this dataset pertained to one of 20 classes. This manipulation removed the shared response mapping of mirror-reflected images and encouraged the model trained on these images to become sensitive to the reflection operation. This dataset was labeled as ‘asymSVHN’.</p></sec><sec id="s4-3-4"><title>Common architecture and training procedure</title><p>The networks’ architecture resembled the VGG architecture. It contained two convolutional layers followed by a max-pooling layer, two additional convolutional layers, and three fully connected layers. The size of convolutional filters was set to 3 × 3 with a stride of 1. The four convolutional layers consisted of 32, 32, 64, and 128 filters, respectively. The size of the max-pooling window was set to 2 × 2 with a stride of 2. The fully-connected layers had 128, 256, and 10 channels and were followed by a softmax operation (the asymSVHN network had 20 channels in its last fully connected layer instead of 10). We added a batch normalization layer after the first and the third convolutional layers and a dropout layer (probability = 0.5) after each fully-connected layer to promote quick convergence and avoid overfitting.</p><p>The networks’ weights and biases were initialized randomly using the uniform He initialization (<xref ref-type="bibr" rid="bib26">He et al., 2015</xref>). We trained the models using 250 epochs and a batch size of 256 images. The CIFAR-10 network was trained using stochastic gradient descent (SGD) optimizer starting with a learning rate of 10<sup>-3</sup> and momentum of 0.9. The learning rate was halved every 20 epochs. The SVHN/symSVHN/asymSVHN networks were trained using the Adam optimizer. The initial learning rate was set to 10<sup>-5</sup> and reduced by half every 50 epochs. The hyper-parameters were determined using the validation data. The models reached around 83% test accuracy (CIFAR-10: 81%, SVHN: 89%, symSVHN: 83%, asymSVHN: 80%). <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref> shows the models’ learning curves.</p></sec></sec><sec id="s4-4"><title>Measuring representational dissimilarities</title><p>For the analyses described in <xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig4">4</xref>, Deep layers in CNNs exhibit mirror-symmetric viewpoint tuning to multiple object categories, and Reflection equivariance versus reflection invariance of convolutional layers, we first normalized the activation level of each individual neural network unit by subtracting its mean response level across all images of the evaluated dataset and dividing it by its standard deviation. The dissimilarity between the representations of two stimuli in a particular neural network layer (<xref ref-type="fig" rid="fig2">Figures 2</xref> and <xref ref-type="fig" rid="fig4">4</xref>) was quantified as one minus the Pearson linear correlation coefficient calculated across all of the layer’s units (i.e. across the flattened normalized activation vectors). The <italic>similarity</italic> between representations (<xref ref-type="fig" rid="fig3">Figure 3</xref>) was quantified by the linear correlation coefficient itself.</p></sec><sec id="s4-5"><title>Measuring mirror-symmetric viewpoint tuning</title><p>Using the representational dissimilarity measure described above, we generated an <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:math></inline-formula> dissimilarity matrix for each exemplar object <inline-formula><mml:math id="inf29"><mml:mi>i</mml:mi></mml:math></inline-formula> and layer <inline-formula><mml:math id="inf30"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula>, where <inline-formula><mml:math id="inf31"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of views (9 in our dataset). Each element of the matrix, <inline-formula><mml:math id="inf32"><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>i</mml:mi></mml:msubsup></mml:math></inline-formula>, denotes the representational distance between views <inline-formula><mml:math id="inf33"><mml:mi>j</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf34"><mml:mi>k</mml:mi></mml:math></inline-formula> of object exemplar <italic>i</italic>. The views are ordered such that <inline-formula><mml:math id="inf35"><mml:mi>j</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf36"><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:math></inline-formula> refer to horizontally reflected views. We measured the mirror-symmetric viewpoint tuning index of the resulting RDMs by<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>s</mml:mi><mml:mi>v</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>r</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:msup><mml:mi>D</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf37"><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo>,</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the Pearson linear correlation coefficient across view pairs, <inline-formula><mml:math id="inf38"><mml:msup><mml:mi>D</mml:mi><mml:mi>H</mml:mi></mml:msup></mml:math></inline-formula> refers to horizontally flipped matrix such that <inline-formula><mml:math id="inf39"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mi>k</mml:mi></mml:mrow><mml:mi>H</mml:mi></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>D</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>-</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf40"><mml:mi>N</mml:mi></mml:math></inline-formula> refers to number of object exemplars. The frontal view (which is unaltered by reflection) was excluded from this measure to avoid spurious inflation of the correlation coefficient. Previous work quantified mirror-symmetric viewpoint tuning by comparing neural RDMs to idealized mirror-symmetric RDM (see Fig. 3Ciii in <xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>). Although highly interpretable, such an idealized RDM inevitably encompasses implicit assumptions about representational geometry that are unrelated to mirror-symmetry. For example, consider a representation featuring perfect mirror-symmetric viewpoint tuning and wherein for each view, the representational distances among all of the exemplars are equal. Its neural RDM would fit an idealized mirror-symmetric RDM better than the neural RDM of a representation featuring perfect mirror-symmetric viewpoint tuning yet non-equidistant exemplars. In contrast, the measure proposed in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> equals 1.0 in both cases.</p></sec><sec id="s4-6"><title>Measuring equivariance and invariance</title><p>Representational equivariance and invariance were measured for an ImageNet-trained AlexNet and an untrained AlexNet with respect to three datasets: the 3D object image dataset described above, a random sample of 2025 ImageNet test images, and a sample of 2025 random noise images (<xref ref-type="fig" rid="fig3">Figure 3</xref>). Separately for each layer <inline-formula><mml:math id="inf41"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula> and image set <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>2025</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we measured invariance by<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>r</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf43"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi mathvariant="normal">ℓ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the mapping from an input image <inline-formula><mml:math id="inf44"><mml:mi>x</mml:mi></mml:math></inline-formula> to unit activations in layer <inline-formula><mml:math id="inf45"><mml:mi mathvariant="normal">ℓ</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the image transformation of interest–vertical reflection, horizontal reflection, or rotation and <inline-formula><mml:math id="inf47"><mml:mi>r</mml:mi></mml:math></inline-formula> is the Pearson linear correlation coefficient calculated across units, flattening the units’ normalized activations into a vector in the case of convolutional layers. In order to estimate equivariance, we used the following definition:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mi>r</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>ℓ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that in this case, <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> was applied both to the input images and the feature maps. This measure can be viewed as the inverse of an additive realization of latent space G-empirical equivariance deviation (G-EED; <xref ref-type="bibr" rid="bib39">Kvinge et al., 2022</xref>). To prevent spurious correlations that may result from flipping and rotating operations, we have removed the central column when flipping horizontally, the central row when flipping vertically, and the central pixel when rotating 90 degrees. As a result, any correlations we observe are unbiased.</p></sec><sec id="s4-7"><title>Importance mapping</title><p>We used an established masking-based importance mapping procedure (<xref ref-type="bibr" rid="bib54">Petsiuk et al., 2018</xref>) to identify visual features that drive units that exhibit mirror-symmetric viewpoint tuning profiles. Given an object for which the target unit showed mirror-symmetric viewpoint tuning, we dimmed the intensities of the images’ pixels in random combinations to estimate the importance of image features. Specifically, for each image, we generated 5000 random binary masks. Multiplying the image with these masks yielded 5000 images in which different subsets of pixels were grayed out. These images were then fed to the network as inputs. The resulting importance maps are averages of these masks, weighted by target unit activity. To evaluate the explanatory power of the importance map of each stimulus, we sorted the pixels according to their absolute values in the importance map and identified the top quartile of salient pixels. We then either retained (‘insertion’) or grayed out (‘deletion’) these pixels, and the resulting stimulus was fed into the network (<xref ref-type="fig" rid="fig5">Figure 5A–B</xref>). Due to the uniform gray background, we only considered foreground pixels. A second analysis compared viewpoint tuning between original images, deletion images, and insertion images across 10 thresholds, from 10% to 90%, with steps of 10% (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). We conducted an additional analysis to examine the influence of global structure on the mirror-symmetric viewpoint tuning of the first fully connected layer (<xref ref-type="fig" rid="fig5">Figure 5D</xref>). To conduct this analysis at the unit population level, we generated one insertion image-set per object. First, we correlated each unit’s view tuning curve against a V-shaped tuning template (i.e. a response proportional to the absolute angle of deviation from a frontal view) and retained only the units with positive correlations. We then correlated each unit’s view-tuning curve with its reflected counterpart. We selected the top 5% most mirror-symmetric units (i.e. those showing the highest correlation coefficients).</p><p>For each object view, we generated an importance map for each of the selected units and averaged these maps across units. Using this average importance map, we generated an insertion image by retaining the top 25% most salient pixels. To test the role of global configuration, we generated a shuffled version of each insertion image by randomly relocating connected components. To assess model response to these images for each object exemplar, we computed the corresponding (9 × 9 views) RDM of fc1 responses given either the insertion images or their shuffled versions and quantified the mirror-symmetric viewpoint tuning of each RDM.</p></sec><sec id="s4-8"><title>Measuring brain alignment</title><p>To measure the alignment between artificial networks and macaque face patches, we used the face-identities-view (FIV) stimulus set (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>), as well as single-unit responses to these stimuli previously recorded from macaque face patches (<xref ref-type="bibr" rid="bib21">Freiwald and Tsao, 2010</xref>). The FIV stimulus set includes images of 25 identities, each depicted in five views: left-profile, left-half profile, straight (frontal), right-half profile, and right-profile. The original recordings also included views of the head from upward, downward, and rear angles; these views were not analyzed in the current study to maintain comparability with its other analyses, which focused on yaw rotations. We measured the dissimilarity between the representations of each image pair using 1 minus the Pearson correlation and constructed an RDM. To assess the variability of this measurement, we adopted a stimulus-level bootstrap analysis, as outlined in <xref ref-type="bibr" rid="bib73">Yildirim et al., 2020</xref>. A bootstrap sample was generated by selecting images with replacement from the FIV image set. From this sample, we calculated both the neural and model RDMs. To prevent spurious positive correlations, any nondiagonal identity pairs resulting from the resampling were removed. Subsequently, we determined the Pearson correlation coefficient between each pair of RDMs. This entire process was repeated across 1000 bootstrap samples. The authors declare no competing interest. The stimulus set and the source code required for reproducing our results are available at the following link: <ext-link ext-link-type="uri" xlink:href="https://github.com/amirfarzmahdi/AL-Symmetry">https://github.com/amirfarzmahdi/AL-Symmetry</ext-link>, (copy archived at <xref ref-type="bibr" rid="bib20">Farzmahdi, 2024</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Visualization, Methodology, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Writing – original draft, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-90256-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The stimulus set and the source code required for reproducing our results are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/amirfarzmahdi/AL-Symmetry">https://github.com/amirfarzmahdi/AL-Symmetry</ext-link> (copy archived at <xref ref-type="bibr" rid="bib20">Farzmahdi, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>Research reported in this publication was supported by the National Eye Institute of the National Institutes of Health under Award Numbers R01EY021594 and R01EY029998; by the National Institute Of Neurological Disorders And Stroke of the National Institutes of Health under Award Number RF1NS128897; and by the Department of the Navy, Office of Naval Research under ONR award number N00014-20-1-2292. This publication was made possible in part with the support of the Charles H Revson Foundation to TG. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health or the Charles H Revson Foundation. We thank Fernando Ramírez for an insightful discussion of an earlier version of this manuscript. We acknowledge Dr. T Vetter, Department of Computer Science, and the University of Basel, for the Basel Face Model.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abudarham</surname><given-names>N</given-names></name><name><surname>Grosbard</surname><given-names>I</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Face recognition depends on specialized mechanisms tuned to view-invariant facial features: Insights from deep neural networks optimized for face or object recognition</article-title><source>Cognitive Science</source><volume>45</volume><elocation-id>e13031</elocation-id><pub-id pub-id-type="doi">10.1111/cogs.13031</pub-id><pub-id pub-id-type="pmid">34490907</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axelrod</surname><given-names>V</given-names></name><name><surname>Yovel</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Hierarchical processing of face viewpoint in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>2442</fpage><lpage>2452</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4770-11.2012</pub-id><pub-id pub-id-type="pmid">22396418</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Azulay</surname><given-names>A</given-names></name><name><surname>Weiss</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Why do deep convolutional networks generalize so poorly to small image transformations</article-title><source>Journal of Machine Learning Research</source><volume>20</volume><fpage>1</fpage><lpage>25</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baek</surname><given-names>S</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Jang</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>G</given-names></name><name><surname>Paik</surname><given-names>S-B</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>Face detection in untrained deep neural networks</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>7328</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-27606-9</pub-id><pub-id pub-id-type="pmid">34916514</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Baek</surname><given-names>S</given-names></name><name><surname>Song</surname><given-names>M</given-names></name><name><surname>Jang</surname><given-names>J</given-names></name><name><surname>Kim</surname><given-names>G</given-names></name><name><surname>Paik</surname><given-names>S-B</given-names></name></person-group><year iso-8601-date="2021">2021b</year><data-title>Face</data-title><version designator="c6abac5">c6abac5</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/vsnnlab/Face">https://github.com/vsnnlab/Face</ext-link></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bao</surname><given-names>P</given-names></name><name><surname>She</surname><given-names>L</given-names></name><name><surname>McGill</surname><given-names>M</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A map of object space in primate inferotemporal cortex</article-title><source>Nature</source><volume>583</volume><fpage>103</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2350-5</pub-id><pub-id pub-id-type="pmid">32494012</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baylis</surname><given-names>GC</given-names></name><name><surname>Driver</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Shape-coding in IT cells generalizes over contrast and mirror reversal, but not figure-ground reversal</article-title><source>Nature Neuroscience</source><volume>4</volume><fpage>937</fpage><lpage>942</lpage><pub-id pub-id-type="doi">10.1038/nn0901-937</pub-id><pub-id pub-id-type="pmid">11528426</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing</article-title><source>Journal of the Royal Statistical Society Series B</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bornstein</surname><given-names>MH</given-names></name><name><surname>Gross</surname><given-names>CG</given-names></name><name><surname>Wolf</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Perceptual similarity of mirror images in infancy</article-title><source>Cognition</source><volume>6</volume><fpage>89</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(78)90017-3</pub-id><pub-id pub-id-type="pmid">679649</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Egger</surname><given-names>B</given-names></name><name><surname>Vetter</surname><given-names>T</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Explaining face representation in the primate brain using different computational models</article-title><source>Current Biology</source><volume>31</volume><fpage>2785</fpage><lpage>2795</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2021.04.014</pub-id><pub-id pub-id-type="pmid">33951457</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Cohen</surname><given-names>T</given-names></name><name><surname>Welling</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Group equivariant convolutional networks</article-title><conf-name>International conference on machine learning</conf-name><fpage>2990</fpage><lpage>2999</lpage></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Coppola</surname><given-names>DM</given-names></name><name><surname>Purves</surname><given-names>HR</given-names></name><name><surname>McCoy</surname><given-names>AN</given-names></name><name><surname>Purves</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The distribution of oriented contours in the real world</article-title><source>PNAS</source><volume>95</volume><fpage>4002</fpage><lpage>4006</lpage><pub-id pub-id-type="doi">10.1073/pnas.95.7.4002</pub-id><pub-id pub-id-type="pmid">9520482</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Corballis</surname><given-names>MC</given-names></name><name><surname>Beale</surname><given-names>IL</given-names></name></person-group><year iso-8601-date="1976">1976</year><source>The Psychology of Left and Right</source><publisher-name>Routledge</publisher-name><pub-id pub-id-type="doi">10.4324/9781003049029</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cornell</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spontaneous mirror-writing in children</article-title><source>Canadian Journal of Psychology / Revue Canadienne de Psychologie</source><volume>39</volume><fpage>174</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1037/h0080122</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dehaene</surname><given-names>S</given-names></name><name><surname>Nakamura</surname><given-names>K</given-names></name><name><surname>Jobert</surname><given-names>A</given-names></name><name><surname>Kuroki</surname><given-names>C</given-names></name><name><surname>Ogawa</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Why do children make mirror errors in reading? Neural correlates of mirror invariance in the visual word form area</article-title><source>NeuroImage</source><volume>49</volume><fpage>1837</fpage><lpage>1848</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.09.024</pub-id><pub-id pub-id-type="pmid">19770045</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dilks</surname><given-names>DD</given-names></name><name><surname>Julian</surname><given-names>JB</given-names></name><name><surname>Kubilius</surname><given-names>J</given-names></name><name><surname>Spelke</surname><given-names>ES</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Mirror-image sensitivity and invariance in object and scene processing pathways</article-title><source>The Journal of Neuroscience</source><volume>31</volume><fpage>11305</fpage><lpage>11312</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1935-11.2011</pub-id><pub-id pub-id-type="pmid">21813690</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dobs</surname><given-names>K</given-names></name><name><surname>Martinez</surname><given-names>J</given-names></name><name><surname>Kell</surname><given-names>AJE</given-names></name><name><surname>Kanwisher</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Brain-like functional specialization emerges spontaneously in deep neural networks</article-title><source>Science Advances</source><volume>8</volume><elocation-id>eabl8913</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abl8913</pub-id><pub-id pub-id-type="pmid">35294241</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dosovitskiy</surname><given-names>A</given-names></name><name><surname>Beyer</surname><given-names>L</given-names></name><name><surname>Kolesnikov</surname><given-names>A</given-names></name><name><surname>Weissenborn</surname><given-names>D</given-names></name><name><surname>Zhai</surname><given-names>X</given-names></name><name><surname>Unterthiner</surname><given-names>T</given-names></name><name><surname>Dehghani</surname><given-names>M</given-names></name><name><surname>Minderer</surname><given-names>M</given-names></name><name><surname>Heigold</surname><given-names>G</given-names></name><name><surname>Gelly</surname><given-names>S</given-names></name><name><surname>Uszkoreit</surname><given-names>J</given-names></name><name><surname>Houlsby</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Farzmahdi</surname><given-names>A</given-names></name><name><surname>Rajaei</surname><given-names>K</given-names></name><name><surname>Ghodrati</surname><given-names>M</given-names></name><name><surname>Ebrahimpour</surname><given-names>R</given-names></name><name><surname>Khaligh-Razavi</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A specialized face-processing model inspired by the organization of monkey face patches explains several face-specific phenomena observed in humans</article-title><source>Scientific Reports</source><volume>6</volume><elocation-id>25025</elocation-id><pub-id pub-id-type="doi">10.1038/srep25025</pub-id><pub-id pub-id-type="pmid">27113635</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Farzmahdi</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>AL-symmetry</data-title><version designator="swh:1:rev:656e7c177762e770b1264fd6864d1d94fac90bc0">swh:1:rev:656e7c177762e770b1264fd6864d1d94fac90bc0</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0afd248fc7f4ce22a262d9104740e3960add6978;origin=https://github.com/amirfarzmahdi/AL-Symmetry;visit=swh:1:snp:a2035d6abe95a7fdcad3c88959353c6932cb8f8e;anchor=swh:1:rev:656e7c177762e770b1264fd6864d1d94fac90bc0">https://archive.softwareheritage.org/swh:1:dir:0afd248fc7f4ce22a262d9104740e3960add6978;origin=https://github.com/amirfarzmahdi/AL-Symmetry;visit=swh:1:snp:a2035d6abe95a7fdcad3c88959353c6932cb8f8e;anchor=swh:1:rev:656e7c177762e770b1264fd6864d1d94fac90bc0</ext-link></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Functional compartmentalization and viewpoint generalization within the macaque face-processing system</article-title><source>Science</source><volume>330</volume><fpage>845</fpage><lpage>851</lpage><pub-id pub-id-type="doi">10.1126/science.1194908</pub-id><pub-id pub-id-type="pmid">21051642</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freiwald</surname><given-names>WA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The neural mechanisms of face processing: cells, areas, networks, and models</article-title><source>Current Opinion in Neurobiology</source><volume>60</volume><fpage>184</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.12.007</pub-id><pub-id pub-id-type="pmid">31958622</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Gerig</surname><given-names>T</given-names></name><name><surname>Morel-Forster</surname><given-names>A</given-names></name><name><surname>Blumer</surname><given-names>C</given-names></name><name><surname>Egger</surname><given-names>B</given-names></name><name><surname>Luthi</surname><given-names>M</given-names></name><name><surname>Schoenborn</surname><given-names>S</given-names></name><name><surname>Vetter</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Morphable Face Models - An Open Framework</article-title><conf-name>2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition</conf-name><fpage>75</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1109/FG.2018.00021</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Girshick</surname><given-names>AR</given-names></name><name><surname>Landy</surname><given-names>MS</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cardinal rules: visual orientation perception reflects knowledge of environmental statistics</article-title><source>Nature Neuroscience</source><volume>14</volume><fpage>926</fpage><lpage>932</lpage><pub-id pub-id-type="doi">10.1038/nn.2831</pub-id><pub-id pub-id-type="pmid">21642976</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gross</surname><given-names>CG</given-names></name><name><surname>Bender</surname><given-names>DB</given-names></name><name><surname>Mishkin</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Contributions of the corpus callosum and the anterior commissure to visual activation of inferior temporal neurons</article-title><source>Brain Research</source><volume>131</volume><fpage>227</fpage><lpage>239</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(77)90517-0</pub-id><pub-id pub-id-type="pmid">407973</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</article-title><conf-name>2015 IEEE International Conference on Computer Vision</conf-name><pub-id pub-id-type="doi">10.1109/ICCV.2015.123</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>He</surname><given-names>K</given-names></name><name><surname>Zhang</surname><given-names>X</given-names></name><name><surname>Ren</surname><given-names>S</given-names></name><name><surname>Sun</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Residual Learning for Image Recognition</article-title><conf-name>2016 IEEE Conference on Computer Vision and Pattern Recognition</conf-name><fpage>770</fpage><lpage>778</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2016.90</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henderson</surname><given-names>M</given-names></name><name><surname>Serences</surname><given-names>JT</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Biased orientation representations can be explained by experience with nonuniform training set statistics</article-title><source>Journal of Vision</source><volume>21</volume><elocation-id>10</elocation-id><pub-id pub-id-type="doi">10.1167/jov.21.8.10</pub-id><pub-id pub-id-type="pmid">34351397</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hesse</surname><given-names>JK</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The macaque face patch system: a turtle’s underbelly for the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>21</volume><fpage>695</fpage><lpage>716</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-00393-w</pub-id><pub-id pub-id-type="pmid">33144718</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Higgins</surname><given-names>I</given-names></name><name><surname>Chang</surname><given-names>L</given-names></name><name><surname>Langston</surname><given-names>V</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name><name><surname>Botvinick</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Unsupervised deep learning identifies semantic disentanglement in single inferotemporal face patch neurons</article-title><source>Nature Communications</source><volume>12</volume><elocation-id>6456</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-021-26751-5</pub-id><pub-id pub-id-type="pmid">34753913</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hosoya</surname><given-names>H</given-names></name><name><surname>Hyvärinen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A mixture of sparse coding models explaining properties of face neurons related to holistic and parts-based processing</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005667</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005667</pub-id><pub-id pub-id-type="pmid">28742816</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Issa</surname><given-names>EB</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Precedence of the eye region in neural processing of faces</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>16666</fpage><lpage>16682</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2391-12.2012</pub-id><pub-id pub-id-type="pmid">23175821</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khaligh-Razavi</surname><given-names>S-M</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Deep supervised, but not unsupervised, models may explain IT cortical representation</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003915</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003915</pub-id><pub-id pub-id-type="pmid">25375136</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Swisher</surname><given-names>JD</given-names></name><name><surname>König</surname><given-names>P</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Prevalence of selectivity for mirror-symmetric views of faces in the ventral and dorsal visual pathways</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>11763</fpage><lpage>11772</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0126-12.2012</pub-id><pub-id pub-id-type="pmid">22915118</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kietzmann</surname><given-names>TC</given-names></name><name><surname>Gert</surname><given-names>AL</given-names></name><name><surname>Tong</surname><given-names>F</given-names></name><name><surname>König</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Representational dynamics of facial viewpoint encoding</article-title><source>Journal of Cognitive Neuroscience</source><volume>29</volume><fpage>637</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01070</pub-id><pub-id pub-id-type="pmid">27791433</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Learning Multiple Layers of Features from Tiny Images</source><publisher-name>University of Toronto</publisher-name></element-citation></ref><ref id="bib38"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Krizhevsky</surname><given-names>A</given-names></name><name><surname>Sutskever</surname><given-names>I</given-names></name><name><surname>Hinton</surname><given-names>GE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>ImageNet classification with deep convolutional neural networks</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1097</fpage><lpage>1105</lpage></element-citation></ref><ref id="bib39"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kvinge</surname><given-names>H</given-names></name><name><surname>Emerson</surname><given-names>T</given-names></name><name><surname>Jorgenson</surname><given-names>G</given-names></name><name><surname>Vasquez</surname><given-names>S</given-names></name><name><surname>Doster</surname><given-names>T</given-names></name><name><surname>Lew</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>In what ways are deep neural networks invariant and how should we measure this</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laughlin</surname><given-names>SB</given-names></name><name><surname>de Ruyter van Steveninck</surname><given-names>RR</given-names></name><name><surname>Anderson</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>The metabolic cost of neural information</article-title><source>Nature Neuroscience</source><volume>1</volume><fpage>36</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1038/236</pub-id><pub-id pub-id-type="pmid">10195106</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>LeCun</surname><given-names>YA</given-names></name><name><surname>Bottou</surname><given-names>L</given-names></name><name><surname>Orr</surname><given-names>GB</given-names></name><name><surname>Müller</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2012">2012</year><chapter-title>Efficient Backprop</chapter-title><source>Neural Networks: Tricks of the Trade</source><publisher-name>Springer</publisher-name><fpage>9</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.1007/978-3-642-35289-8_3</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Liao</surname><given-names>Q</given-names></name><name><surname>Anselmi</surname><given-names>F</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The invariance hypothesis implies domain-specific regions in visual cortex</article-title><source>PLOS Computational Biology</source><volume>11</volume><elocation-id>e1004390</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1004390</pub-id><pub-id pub-id-type="pmid">26496457</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Liao</surname><given-names>Q</given-names></name><name><surname>Anselmi</surname><given-names>F</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>View-tolerant face recognition and hebbian learning imply mirror-symmetric neural tuning to head orientation</article-title><source>Current Biology</source><volume>27</volume><fpage>62</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2016.10.015</pub-id><pub-id pub-id-type="pmid">27916522</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Z</given-names></name><name><surname>Mao</surname><given-names>H</given-names></name><name><surname>Wu</surname><given-names>C</given-names></name><name><surname>Feichtenhofer</surname><given-names>C</given-names></name><name><surname>Darrell</surname><given-names>T</given-names></name><name><surname>Xie</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Convnet for the 2020s</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/2201.03545">https://arxiv.org/abs/2201.03545</ext-link></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Logothetis</surname><given-names>NK</given-names></name><name><surname>Pauls</surname><given-names>J</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Shape representation in the inferior temporal cortex of monkeys</article-title><source>Current Biology</source><volume>5</volume><fpage>552</fpage><lpage>563</lpage><pub-id pub-id-type="doi">10.1016/s0960-9822(95)00108-4</pub-id><pub-id pub-id-type="pmid">7583105</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moeller</surname><given-names>S</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tsao</surname><given-names>DY</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Patches with links: a unified system for processing faces in the macaque temporal lobe</article-title><source>Science</source><volume>320</volume><fpage>1355</fpage><lpage>1359</lpage><pub-id pub-id-type="doi">10.1126/science.1157436</pub-id><pub-id pub-id-type="pmid">18535247</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nelson</surname><given-names>RO</given-names></name><name><surname>Peoples</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>A stimulus-response analysis of letter reversals</article-title><source>Journal of Reading Behavior</source><volume>7</volume><fpage>329</fpage><lpage>340</lpage><pub-id pub-id-type="doi">10.1080/10862967509547152</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Netzer</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Coates</surname><given-names>A</given-names></name><name><surname>Bissacco</surname><given-names>A</given-names></name><name><surname>Wu</surname><given-names>B</given-names></name><name><surname>Ng</surname><given-names>AY</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Reading digits in natural images with unsupervised feature learning</article-title><conf-name>NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011</conf-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Oja</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>A simplified neuron model as a principal component analyzer</article-title><source>Journal of Mathematical Biology</source><volume>15</volume><fpage>267</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1007/BF00275687</pub-id><pub-id pub-id-type="pmid">7153672</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Olah</surname><given-names>C</given-names></name><name><surname>Cammarata</surname><given-names>N</given-names></name><name><surname>Voss</surname><given-names>C</given-names></name><name><surname>Schubert</surname><given-names>L</given-names></name><name><surname>Goh</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Naturally occurring equivariance in neural networks</article-title><source>Distill</source><volume>5</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.23915/distill.00024.004</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Parkhi</surname><given-names>OM</given-names></name><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Deep face recognition</article-title><conf-name>Proceedings of the British Machine Vision Conference</conf-name><fpage>41.1</fpage><lpage>41.2</lpage><pub-id pub-id-type="doi">10.5244/C.29.41</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Paszke</surname><given-names>A</given-names></name><name><surname>Gross</surname><given-names>S</given-names></name><name><surname>Massa</surname><given-names>F</given-names></name><name><surname>Lerer</surname><given-names>A</given-names></name><name><surname>Bradbury</surname><given-names>J</given-names></name><name><surname>Chanan</surname><given-names>G</given-names></name><name><surname>Killeen</surname><given-names>T</given-names></name><name><surname>Lin</surname><given-names>Z</given-names></name><name><surname>Gimelshein</surname><given-names>N</given-names></name><name><surname>Antiga</surname><given-names>L</given-names></name><name><surname>Desmaison</surname><given-names>A</given-names></name><name><surname>Kopf</surname><given-names>A</given-names></name><name><surname>Yang</surname><given-names>E</given-names></name><name><surname>DeVito</surname><given-names>Z</given-names></name><name><surname>Raison</surname><given-names>M</given-names></name><name><surname>Tejani</surname><given-names>A</given-names></name><name><surname>Chilamkurthy</surname><given-names>S</given-names></name><name><surname>Steiner</surname><given-names>B</given-names></name><name><surname>Fang</surname><given-names>L</given-names></name><name><surname>Bai</surname><given-names>J</given-names></name><name><surname>Chintala</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Pytorch: an imperative style, high-performance deep learning library</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perrett</surname><given-names>DI</given-names></name><name><surname>Oram</surname><given-names>MW</given-names></name><name><surname>Harries</surname><given-names>MH</given-names></name><name><surname>Bevan</surname><given-names>R</given-names></name><name><surname>Hietanen</surname><given-names>JK</given-names></name><name><surname>Benson</surname><given-names>PJ</given-names></name><name><surname>Thomas</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Viewer-centred and object-centred coding of heads in the macaque temporal cortex</article-title><source>Experimental Brain Research</source><volume>86</volume><fpage>159</fpage><lpage>173</lpage><pub-id pub-id-type="doi">10.1007/BF00231050</pub-id><pub-id pub-id-type="pmid">1756786</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Petsiuk</surname><given-names>V</given-names></name><name><surname>Das</surname><given-names>A</given-names></name><name><surname>Saenko</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>RISE: randomized input sampling for explanation of black-box models</article-title><conf-name>British Machine Vision Conference</conf-name></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raman</surname><given-names>R</given-names></name><name><surname>Hosoya</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Convolutional neural networks explain tuning properties of anterior, but not middle, face-processing areas in macaque inferotemporal cortex</article-title><source>Communications Biology</source><volume>3</volume><elocation-id>221</elocation-id><pub-id pub-id-type="doi">10.1038/s42003-020-0945-x</pub-id><pub-id pub-id-type="pmid">32385392</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ramírez</surname><given-names>FM</given-names></name><name><surname>Cichy</surname><given-names>RM</given-names></name><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>The neural code for face orientation in the human fusiform face area</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>12155</fpage><lpage>12167</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3156-13.2014</pub-id><pub-id pub-id-type="pmid">25186759</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Revsine</surname><given-names>C</given-names></name><name><surname>Gonzalez-Castillo</surname><given-names>J</given-names></name><name><surname>Merriam</surname><given-names>EP</given-names></name><name><surname>Bandettini</surname><given-names>PA</given-names></name><name><surname>Ramírez</surname><given-names>FM</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>A Unifying Model for Discordant and Concordant Results in Human Neuroimaging Studies of Facial Viewpoint Selectivity</article-title><source>The Journal of Neuroscience</source><volume>44</volume><elocation-id>e0296232024</elocation-id><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0296-23.2024</pub-id><pub-id pub-id-type="pmid">38438256</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riesenhuber</surname><given-names>M</given-names></name><name><surname>Poggio</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Hierarchical models of object recognition in cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1019</fpage><lpage>1025</lpage><pub-id pub-id-type="doi">10.1038/14819</pub-id><pub-id pub-id-type="pmid">10526343</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rollenhagen</surname><given-names>JE</given-names></name><name><surname>Olson</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Mirror-image confusion in single neurons of the macaque inferotemporal cortex</article-title><source>Science</source><volume>287</volume><fpage>1506</fpage><lpage>1508</lpage><pub-id pub-id-type="doi">10.1126/science.287.5457.1506</pub-id><pub-id pub-id-type="pmid">10688803</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Russakovsky</surname><given-names>O</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Su</surname><given-names>H</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>Satheesh</surname><given-names>S</given-names></name><name><surname>Ma</surname><given-names>S</given-names></name><name><surname>Huang</surname><given-names>Z</given-names></name><name><surname>Karpathy</surname><given-names>A</given-names></name><name><surname>Khosla</surname><given-names>A</given-names></name><name><surname>Bernstein</surname><given-names>M</given-names></name><name><surname>Berg</surname><given-names>AC</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>ImageNet Large Scale Visual Recognition Challenge</article-title><source>International Journal of Computer Vision</source><volume>115</volume><fpage>211</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1007/s11263-015-0816-y</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Shapley</surname><given-names>LS</given-names></name></person-group><year iso-8601-date="1953">1953</year><chapter-title>17. A value for N-person games</chapter-title><person-group person-group-type="editor"><name><surname>Shapley</surname><given-names>LS</given-names></name></person-group><source>Contributions to the Theory of Games (AM-28)</source><publisher-name>Princeton University Press</publisher-name><fpage>307</fpage><lpage>318</lpage><pub-id pub-id-type="doi">10.1515/9781400881970-018</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Zisserman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Very deep convolutional networks for large-scale image recognition</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutherland</surname><given-names>NS</given-names></name></person-group><year iso-8601-date="1960">1960</year><article-title>Visual discrimination of orientation by octopus: mirror images</article-title><source>British Journal of Psychology</source><volume>51</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1111/j.2044-8295.1960.tb00719.x</pub-id><pub-id pub-id-type="pmid">13835827</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="software"><person-group person-group-type="author"><collab>The MathWorks Inc</collab></person-group><year iso-8601-date="2019">2019</year><data-title>Deep learning Toolbox</data-title><source>MATLAB</source><ext-link ext-link-type="uri" xlink:href="https://www.mathworks.com/products/deep-learning.html">https://www.mathworks.com/products/deep-learning.html</ext-link></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Todrin</surname><given-names>DC</given-names></name><name><surname>Blough</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The discrimination of mirror-image forms by pigeons</article-title><source>Perception &amp; Psychophysics</source><volume>34</volume><fpage>397</fpage><lpage>402</lpage><pub-id pub-id-type="doi">10.3758/BF03203053</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torralba</surname><given-names>A</given-names></name><name><surname>Oliva</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Statistics of natural image categories</article-title><source>Network</source><volume>14</volume><fpage>391</fpage><lpage>412</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_14_3_302</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsao</surname><given-names>DY</given-names></name><name><surname>Freiwald</surname><given-names>WA</given-names></name><name><surname>Tootell</surname><given-names>RBH</given-names></name><name><surname>Livingstone</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>A cortical region consisting entirely of face-selective cells</article-title><source>Science</source><volume>311</volume><fpage>670</fpage><lpage>674</lpage><pub-id pub-id-type="doi">10.1126/science.1119983</pub-id><pub-id pub-id-type="pmid">16456083</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Vedaldi</surname><given-names>A</given-names></name><name><surname>Lenc</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Matconvnet – Convolutional neural networks for MATLAB</article-title><conf-name>Proceeding of the ACM Int. Conf. on Multimedia</conf-name><pub-id pub-id-type="doi">10.1145/2733373.2807412</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Weiler</surname><given-names>M</given-names></name><name><surname>Hamprecht</surname><given-names>FA</given-names></name><name><surname>Storath</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning Steerable Filters for Rotation Equivariant CNNs</article-title><conf-name>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</conf-name><fpage>849</fpage><lpage>858</lpage><pub-id pub-id-type="doi">10.1109/CVPR.2018.00095</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Willenbockel</surname><given-names>V</given-names></name><name><surname>Sadr</surname><given-names>J</given-names></name><name><surname>Fiset</surname><given-names>D</given-names></name><name><surname>Horne</surname><given-names>GO</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Tanaka</surname><given-names>JW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Controlling low-level image properties: the SHINE toolbox</article-title><source>Behavior Research Methods</source><volume>42</volume><fpage>671</fpage><lpage>684</lpage><pub-id pub-id-type="doi">10.3758/BRM.42.3.671</pub-id><pub-id pub-id-type="pmid">20805589</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yamins</surname><given-names>DLK</given-names></name><name><surname>Hong</surname><given-names>H</given-names></name><name><surname>Cadieu</surname><given-names>CF</given-names></name><name><surname>Solomon</surname><given-names>EA</given-names></name><name><surname>Seibert</surname><given-names>D</given-names></name><name><surname>DiCarlo</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performance-optimized hierarchical models predict neural responses in higher visual cortex</article-title><source>PNAS</source><volume>111</volume><fpage>8619</fpage><lpage>8624</lpage><pub-id pub-id-type="doi">10.1073/pnas.1403112111</pub-id><pub-id pub-id-type="pmid">24812127</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>K</given-names></name><name><surname>Yau</surname><given-names>JH</given-names></name><name><surname>Fei-Fei</surname><given-names>L</given-names></name><name><surname>Deng</surname><given-names>J</given-names></name><name><surname>Russakovsky</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A study of face obfuscation in ImageNet</article-title><conf-name>Proceedings of the 39th International Conference on Machine Learning, Proceedings of Machine Learning Research</conf-name><fpage>25313</fpage><lpage>25330</lpage></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yildirim</surname><given-names>I</given-names></name><name><surname>Belledonne</surname><given-names>M</given-names></name><name><surname>Freiwald</surname><given-names>W</given-names></name><name><surname>Tenenbaum</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Efficient inverse graphics in biological face processing</article-title><source>Science Advances</source><volume>6</volume><elocation-id>eaax5979</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.aax5979</pub-id><pub-id pub-id-type="pmid">32181338</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90256.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale de Lausanne</institution></institution-wrap><country>Switzerland</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2023.01.05.522909" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.05.522909"/></front-stub><body><p>This computational study is a valuable empirical investigation into the common trait of neurons in brains and artificial neural networks: responding effectively to both objects and their mirror images and it focuses on uncovering conditions that lead to mirror symmetry in visual networks and the evidence convincingly demonstrates that learning contributes to expanding mirror symmetry tuning, given its presence in the data. Additionally, the paper delves into the transformation of face patches in primate visual hierarchy, shifting from view specificity to mirror symmetry to view invariance. It empirically analyzes factors behind similar effects in many network architectures, and key claims highlight the emergence of invariances in architectures with spatial pooling, driven by learning bilateral symmetry discrimination and importantly, these effects extend beyond faces, suggesting broader relevance.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90256.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Mathis</surname><given-names>Mackenzie W</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>École Polytechnique Fédérale de Lausanne</institution></institution-wrap><country>Switzerland</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Schrimpf</surname><given-names>Martin</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s376052</institution-id><institution>EPFL</institution></institution-wrap><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2023.01.05.522909">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2023.01.05.522909v2">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Emergence of brain-like mirror-symmetric viewpoint tuning in convolutional neural networks&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by Mackenzie Mathis as Reviewing Editor and Yanchao Bi as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions (for the authors):</p><p>Overall we collectively found the study valuable, but requires two major revisions to significantly strengthen the work. The concerns, and thus recommendations for major revision, largely revolve around the combination of choosing two weaker architectures and not establishing any correspondence to brain data. Thus, we agree to the two following essential revisions:</p><p>1. Please broaden the architectures that are being analyzed, including at least one state-of-the-art model for the effect you are studying. We think EIG is the most natural choice. To really strengthen the paper, recommend to include ResNets (strong computer vision models and good alignment to brain data), Transformers (strong computer vision models with different building blocks which would &quot;stress-test&quot; their learning and max-pooling claims), and HMAX (Leibo et al. claim this explains mirror-symmetric tuning, but not sure how well it aligns to data relative to EIG and other models).</p><p>2. Please defend your current choice of architectures by testing the brain alignment of VGG/AlexNet versus the current state-of-the-art EIG. we suggest performing comparisons of model activations to brain data for recordings where the subjects viewed mirror-symmetric stimuli (e.g., neural predictivity and/or CKA), or metrics that focus more explicitly on the mirror-symmetry effects, similar to Figure 3 in the Yildirim et al. paper. The most relevant result for connecting the models under study in this paper to neural recordings is this the paper by Yildirim et al. where VGG is found to be a poorer model than their proposed EIG. Depending on the choice of metric I think it is well possible that VGG is comparable to EIG, but it's up to the authors of this manuscript to defend their choice, and to make the comparison.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. The potential mechanisms underlying mirror-symmetric tuning have been investigated in the past as has been acknowledged and intensively discussed in the paper. However, some of the conclusions made in this paper were already made in previous work, although not explicitly tested. As stated by Leibo et al. &quot;the Hebb-type learning rule generates mirror-symmetric tuning for bilaterally symmetric objects, like faces&quot;, suggesting that it will also allow for mirror-symmetric tuning for other object categories. It would be important to acknowledge this. Moreover, while Yildirim et al. reports mirror-symmetric viewpoint tuning in VGGFace, they find that this tuning is less present than the view-invariant identity tuning, thereby contradicting neural findings in area AL. It would be beneficial to compare mirror-symmetry with view-invariant coding (as introduced in Figure 1A) in the fully-connected layers of the CNNs in this work, or at least to discuss and acknowledge this potential mismatch of CNNs with neural data.</p><p>2. The authors propose a mirror-symmetric viewpoint tuning index, which, although innovative, complicates comparison with previous work. This index is based on correlating representational dissimilarity matrices (RDMs) with their flipped versions, a method differing from previous approaches that correlated RDMs with idealized model RDMs corresponding to mirror symmetry. The mirror-symmetric tuning reported by Yildirim et al. for VGG-raw (corresponding to VGGFace in this study; Figure S2) appears significantly lower than the current study (Figure S1). To clarify these differences, an analysis using an idealized model, as in prior studies, or to better motivate the index used in this study would be valuable.</p><p>3. The analysis of reflection equivariance vs. invariance in both trained and untrained CNNs' convolutional layers is informative. However, an illustrative figure would enhance clarity and underscore the distinction between equivariance and invariance.</p><p>4. Faces seem to behave quite differently from all other object categories tested in this study. The mirror-symmetric viewpoint tuning for faces starts very low but then achieves similar values as other categories in the fully-connected layers. Moreover, the difference between task training and between trained and untrained AlexNet is maximal for faces (Figure 4 and S2). None of these differences are discussed in the paper. Given the critical role of faces for this study, the authors should provide speculations or potential explanations for these observations.</p><p>5. The study's analysis of training task and dataset raises some questions. Some categories, such as faces, seem to depend strongly on the task/dataset compared to others (e.g. boats). Acknowledging this finding and discussing potential reasons could enhance the manuscript.</p><p>6. It is surprising that the face-trained VGGFace shows lower mirror-symmetric viewpoint tuning as VGG16 (Figure S1). What could be possible explanations for this result? Could it be due to an overfitting to natural images of faces (compared to the Basel Face model faces used in this study) of the VGGFace network? A discussion of this finding would be beneficial to the paper.</p><p>7. The authors suggest that mirror-symmetric viewpoint tuning is a stepping stone towards view invariance. However, the study lacks explicit testing that this tuning is necessary for viewpoint invariance to emerge. For instance, if one prohibits a network to learn mirror-symmetric viewpoint tuning, would it still achieve viewpoint invariance for bilaterally symmetric objects? While such an analysis might be beyond the scope of this study, it could be beneficial to discuss this as a future direction.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I believe you have all the tools available to address the major weakness with respect to model architectures: https://github.com/CNCLgithub/EIG-faces provides code for the EIG model from Yildirim et al. 2020, and this model also includes pooling and versions of it have been trained for other datasets. As far as I am aware, the EIG model is state-of-the-art for the neural data of mirror-symmetric viewpoint tuning, so applying your analyses to this model would make them a lot more relevant and general.</p><p>You could also see if more recent &quot;standard&quot; convents such as ResNet-50 explain the neural data at least as well as EIG and then generalize your analyses to this (and other) model(s). To make this case you would have to quantitatively compare the two models on their neural alignment (I believe the data from Yildirim's paper should be public). You might also say that AlexNet and VGG are in the same ballpark for explaining the neural data, but then you should include that analysis in the paper and in the Yildirim paper, VGG seems to fall behind EIG's neural similarity by 10-20 percent points (Figure 3 D iv and E iv).</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>– It would be reassuring to know that the object classes have independent measures of symmetry *on which the networks operate*. If the statements about object-class-specific symmetry came after performing the experiments, then I recommend re-writing so that those statements are interpretations.</p><p>– It would be a great contribution to the field if the authors could clarify the relationship between this work and Baek et al.'s. Can they confirm that untrained networks have mirror-tuned units? If that is not replicable, then the &quot;emergence&quot; framing is accurate, and one can exercise appropriate weighing of that other study. This would help the field. If learning just amplifies this symmetry (by strengthening connections of view-dependent units, for example), that is also helpful to learn.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.90256.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions (for the authors):</p><p>Overall we collectively found the study valuable, but requires two major revisions to significantly strengthen the work. The concerns, and thus recommendations for major revision, largely revolve around the combination of choosing two weaker architectures and not establishing any correspondence to brain data. Thus, we agree to the two following essential revisions:</p><p>1. Please broaden the architectures that are being analyzed, including at least one state-of-the-art model for the effect you are studying. We think EIG is the most natural choice. To really strengthen the paper, recommend to include ResNets (strong computer vision models and good alignment to brain data), Transformers (strong computer vision models with different building blocks which would &quot;stress-test&quot; their learning and max-pooling claims), and HMAX (Leibo et al. claim this explains mirror-symmetric tuning, but not sure how well it aligns to data relative to EIG and other models).</p></disp-quote><p>We have extended our analyses to include EIG, ResNet50, ConvNeXt, ViT, and HMAX. The results reveal a common characteristic among convolutional networks that incorporate a fully-connected or average pooling layer, irrespective of their architecture (i.e., HMAX, EIG, ResNet50, and ConvNeXt). In all these networks, the mirror-symmetric viewpoint tuning index peaks around the fully-connected or average pooling layers. This result extends the generality of our previous observations in shallower networks such as VGG and AlexNet (see Figures 2—figure supplement 3 and 5).</p><disp-quote content-type="editor-comment"><p>2. Please defend your current choice of architectures by testing the brain alignment of VGG/AlexNet versus the current state-of-the-art EIG. we suggest performing comparisons of model activations to brain data for recordings where the subjects viewed mirror-symmetric stimuli (e.g., neural predictivity and/or CKA), or metrics that focus more explicitly on the mirror-symmetry effects, similar to Figure 3 in the Yildirim et al. paper. The most relevant result for connecting the models under study in this paper to neural recordings is this the paper by Yildirim et al. where VGG is found to be a poorer model than their proposed EIG. Depending on the choice of metric I think it is well possible that VGG is comparable to EIG, but it's up to the authors of this manuscript to defend their choice, and to make the comparison.</p></disp-quote><p>We evaluated the representational alignment of various architectures—AlexNet, VGG, EIG, ResNet50, ConvNeXt, and ViT against the neural dataset from Freiwald &amp; Tsao 2010 (including three face patches), following the analytic conventions introduced by Yildirim and colleagues. Going beyond standard RSA, we disentangled the contribution of reflection invariant and reflection-sensitive features to the observed alignment (see Figure 6 and Figure 6—figure supplement 1-3). Last, we estimated the mirror-symmetric viewpoint tuning in the three face patches recorded by Freiwald &amp; Tsao, allowing a direct comparison of this index in artificial neural networks and neural recordings (Figure 2—figure supplement 4).</p><p>In the revised manuscript, we no longer emphasize shallower convolutional architectures such as AlexNet or VGG16 as better models; the explanation convolutional neural networks offer to mirror-symmetric viewpoint tuning as observed in the face patch system applies also when using deeper models.</p><p>Manuscript changes: (main text, Results section)</p><p>“Representational alignment between artificial networks and macaque face patches</p><p>How does the emergence of mirror-invariance in CNNs manifest in the alignment of these networkswith neural representations of faces in the macaque face-patch system? In line with Yildirim andcolleagues (2020) [14], we reanalyzed the neural recordings from Freiwald and Tsao (2010) [4] by correlating neural population RDMs, each describing the dissimilarities among neural responses to face images of varying identities and viewpoints, with corresponding model RDMs, derived from neural network layer representations of the stimulus set (Figure 6, top row). In addition to the AL face-patch, we considered MLMF, which is sensitive to reflection [4], and AM, which is mostly viewpoint invariant [4]. Following the approach of Yildirim and colleagues, the neural networks were presented with segmented reconstructions, where non-facial pixels were replaced by a uniform background.</p><p>Consistent with previous findings [14], MLMF was more aligned with the CNNs’ mid-level representation, notably the last convolutional layers (Figure 6, A). The AL face patch showed its highest representational alignment with the first fully connected layer (Figure 6, B), coinciding with the surge of the mirror-symmetric viewpoint tuning index at this processing level (see Figure 2). The AM face patch aligned most with the fully connected layers (Figure 6, C).</p><p>These correlations between model and neural RDMs reflect the contribution of multiple underlying image features. To disentangle the contribution of reflection-invariant and reflection-sensitive representations to the resulting RDM correlation, we computed two additional model representations for each neural network layer: (1) a reflection-invariant representation, obtained by element-wise addition of two activation tensors, one elicited in response to the original stimuli and the other in response to mirror-reflected versions of the stimuli; and, (2) a reflection-sensitive representation, obtained by element-wise subtraction of these two tensors. The two resulting feature components sum to the original activation tensor; a fully reflection-invariant representation would be entirely accounted for by the first component. For each CNN layer, we obtained the two components and correlated each of them with the unaltered neural RDMs. Through the Shapley value feature attribution method [41], we transformed the resulting correlation coefficients into additive contributions of the reflection-invariant and reflection-sensitive components to the original model-brain RDM correlations (Figure 6, D-F).</p><p>In the MLMF face patch, reflection-sensitive features contributed more than reflection-invariant ones, consistent with the dominance of reflection-sensitive information in aligning network layers with MLMF data (Figure 6, D). Conversely, in the AL and AM face patches, reflection-invariant features accounted for nearly all the observed model–brain RDM correlations (Figure 6, E and F). For most of the convolutional layers, the contribution of the reflection-sensitive component to AL or AM alignment was negative—meaning that if the layers’ representations were more reflection-invariant, they could have explained the neural data better.”</p><p>(main text, Methods section)</p><p>“Measuring brain alignment</p><p>To measure the alignment between artificial networks and macaque face patches, we used the face-identities-view (FIV) stimulus set [4], as well as single-unit responses to these stimuli previously recorded from macaque face patches [4]. The FIV stimulus set includes images of 25 identities, each depicted in five views: left-profile, left-half profile, straight (frontal), right-half profile, and right-profile. The original recordings also included views of the head from upward, downward, and rear angles; these views were not analyzed in the current study to maintain comparability with its other analyses, which focused on yaw rotations. We measured the dissimilarity between the representations of each image pair using 1 minus the Pearson correlation and constructed an RDM. To assess the variability of this measurement, we adopted a stimulus-level bootstrap analysis, as outlined in [14]. A bootstrap sample was generated by selecting images with replacement from the FIV image set. From this sample, we calculated both the neural and model RDMs. To prevent spurious positive correlations, any nondiagonal identity pairs resulting from the resampling were removed. Subsequently, we determined the Pearson correlation coefficient between each pair of RDMs. This entire process was repeated across 1,000 bootstrap samples.”</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>1. The potential mechanisms underlying mirror-symmetric tuning have been investigated in the past as has been acknowledged and intensively discussed in the paper. However, some of the conclusions made in this paper were already made in previous work, although not explicitly tested. As stated by Leibo et al. &quot;the Hebb-type learning rule generates mirror-symmetric tuning for bilaterally symmetric objects, like faces&quot;, suggesting that it will also allow for mirror-symmetric tuning for other object categories. It would be important to acknowledge this.</p></disp-quote><p>We respectfully hold a different perspective: the Hebb rule requires training on each object category in order to achieve a view-invariant representation of its exemplars. However, our findings indicate a generalization of mirror-symmetric viewpoint tuning across object categories; the model does not have to be trained on a certain category to acquire a view-invariant representation of its exemplars, provided they exhibit bilateral symmetry. Additionally, as highlighted in the discussion’s limitations section, the model by Leibo et al. exhibits reflection invariance only with respect to the horizontal center of the input image, unlike AL neurons (see Figure 2—figure supplement 8).</p><disp-quote content-type="editor-comment"><p>Moreover, while Yildirim et al. reports mirror-symmetric viewpoint tuning in VGGFace, they find that this tuning is less present than the view-invariant identity tuning, thereby contradicting neural findings in area AL. It would be beneficial to compare mirror-symmetry with view-invariant coding (as introduced in Figure 1A) in the fully-connected layers of the CNNs in this work, or at least to discuss and acknowledge this potential mismatch of CNNs with neural data.</p></disp-quote><p>Please refer to the relevant revision quoted in Point 1.3 above. Moreover, as illustrated in Figures 6 and Figure 6—figure supplement 1-3, CNNs trained on object categorization exhibit similarities to face patches comparable to those in the EIG model. We concur that training on face identification with VGGFace results in a model that does not exhibit AL-like mirror-symmetric viewpoint tuning.</p><disp-quote content-type="editor-comment"><p>2. The authors propose a mirror-symmetric viewpoint tuning index, which, although innovative, complicates comparison with previous work. This index is based on correlating representational dissimilarity matrices (RDMs) with their flipped versions, a method differing from previous approaches that correlated RDMs with idealized model RDMs corresponding to mirror symmetry. The mirror-symmetric tuning reported by Yildirim et al. for VGG-raw (corresponding to VGGFace in this study; Figure S2) appears significantly lower than the current study (Figure S1). To clarify these differences, an analysis using an idealized model, as in prior studies, or to better motivate the index used in this study would be valuable.</p></disp-quote><p>We have added a new paragraph in the Methods section detailing the motivation for introducing a new index. Please refer to the relevant revision in Point 1.1 above.</p><disp-quote content-type="editor-comment"><p>3. The analysis of reflection equivariance vs. invariance in both trained and untrained CNNs' convolutional layers is informative. However, an illustrative figure would enhance clarity and underscore the distinction between equivariance and invariance.</p></disp-quote><p>Following this suggestion, we included a new panel in Figure 1D to augment clarity and emphasize the distinction between equivariance and invariance.</p><disp-quote content-type="editor-comment"><p>4. Faces seem to behave quite differently from all other object categories tested in this study. The mirror-symmetric viewpoint tuning for faces starts very low but then achieves similar values as other categories in the fully-connected layers. Moreover, the difference between task training and between trained and untrained AlexNet is maximal for faces (Figure 4 and S2). None of these differences are discussed in the paper. Given the critical role of faces for this study, the authors should provide speculations or potential explanations for these observations.</p><p>5. The study's analysis of training task and dataset raises some questions. Some categories, such as faces, seem to depend strongly on the task/dataset compared to others (e.g. boats). Acknowledging this finding and discussing potential reasons could enhance the manuscript.</p></disp-quote><p>We revised the caption of Figure 2—figure supplement 3 to explicitly address this point.</p><p>Manuscript changes:</p><p>“For face stimuli, there is a unique progression in mirror-symmetric viewpoint tuning: the index is negative for the convolutional layers and it abruptly becomes highly positive when transitioning to the first fully connected layer. The negative indices in the convolutional layers can be attributed to the image-space asymmetry of non-frontal faces; compared to other categories, faces demonstrate pronounced front-back asymmetry, which translates to asymmetric images for all but frontal views (Figure 2—figure supplement 1). The features that drive the highly positive mirror-symmetric viewpoint tuning for faces in the fully connected layers are trainingdependent (Figure 2—figure supplement 2), and hence, may reflect asymmetric image features that do not elicit equivariant maps in low-level representations; for example, consider a profile view of a nose. Note that cars and boats elicit high mirror-symmetric viewpoint tuning indices already in early processing layers. This early mirror-symmetric tuning is independent of training (Figure 2—figure supplement 2), and hence, may be driven by low-level features. Both of these object categories show pronounced quadrilateral symmetry, which translates to symmetric images for both frontal and side views (Figure 2—figure supplement 1).”</p><disp-quote content-type="editor-comment"><p>6. It is surprising that the face-trained VGGFace shows lower mirror-symmetric viewpoint tuning as VGG16 (Figure S1). What could be possible explanations for this result? Could it be due to an overfitting to natural images of faces (compared to the Basel Face model faces used in this study) of the VGGFace network? A discussion of this finding would be beneficial to the paper.</p></disp-quote><p>This point is now addressed explicitly in the caption of Figure 2—figure supplement 5.</p><p>Manuscript changes:</p><p>“Yildirim and colleagues [14] reported that CNNs trained on faces, notably VGGFace, exhibited lower mirror symmetric viewpoint tuning compared to neural representations in area AL. Consistent with their findings, our results demonstrate that VGGFace, trained on face identification, has a low mirror-symmetric viewpoint tuning index. This is especially notable in comparison to ImageNet-trained models such as VGG16. This difference between VGG16 and VGGFace can be attributed to the distinct characteristics of their training datasets and objective functions. The VGGFace training task consists of mapping frontal face images to identities; this task may exclusively emphasize higher-level physiognomic information. In contrast, training on recognizing objects in natural images may result in a more detailed, view-dependent representation. To test this potential explanation, we measured the average correlation-distance between the fc6 representations of different views of the same face exemplar in VGGFace and VGG16 trained on ImageNet. The average correlation-distance between views is 0.70±0.04 in VGGFace and 0.93±0.04 in VGG16 trained on ImageNet. The converse correlation distance between different exemplars depicted from the same view is 0.84±0.14 in VGGFace and 0.58±0.06 in VGG16 trained on ImageNet. Therefore, as suggested by Yildirim and colleagues, training on face identification alone may result in representations that cannot explain intermediate levels of face processing.”</p><disp-quote content-type="editor-comment"><p>7. The authors suggest that mirror-symmetric viewpoint tuning is a stepping stone towards view invariance. However, the study lacks explicit testing that this tuning is necessary for viewpoint invariance to emerge. For instance, if one prohibits a network to learn mirror-symmetric viewpoint tuning, would it still achieve viewpoint invariance for bilaterally symmetric objects? While such an analysis might be beyond the scope of this study, it could be beneficial to discuss this as a future direction.</p></disp-quote><p>We believe that mirror-symmetric viewpoint tuning is not strictly necessary for achieving view-invariance. However, it is a plausible path from view-dependence to view invariance. We addressed this point in the updated limitations subsection of the discussion.</p><p>Manuscript changes:</p><p>“A second consequence of the simulation-based nature of this study is that our findings only establish that mirror-symmetric viewpoint tuning is a viable computational means for achieving view invariance; they do not prove it to be a necessary condition. In fact, previous modeling studies [10, 19, 61] have demonstrated that a direct transition from view-specific processing to view invariance is possible. However, in practice, we observe that both CNNs and the face-patch network adopt solutions that include intermediate representations with mirror-symmetric viewpoint tuning.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I believe you have all the tools available to address the major weakness with respect to model architectures: https://github.com/CNCLgithub/EIG-faces provides code for the EIG model from Yildirim et al. 2020, and this model also includes pooling and versions of it have been trained for other datasets. As far as I am aware, the EIG model is state-of-the-art for the neural data of mirror-symmetric viewpoint tuning, so applying your analyses to this model would make them a lot more relevant and general.</p></disp-quote><p>Thank you for your suggestion. We have integrated the EIG model from Yildirim et al. (2020) into our analysis, using the resources provided in the GitHub link. This has allowed us to extend our comparisons to include state-of-the-art models, enhancing the relevance and scope of our findings. Results are now included in the revised Figure 2—figure supplement 3, and new Figure 2—figure supplement 5 and Figure 6—figure supplement 1-3.</p><disp-quote content-type="editor-comment"><p>You could also see if more recent &quot;standard&quot; convents such as ResNet-50 explain the neural data at least as well as EIG and then generalize your analyses to this (and other) model(s). To make this case you would have to quantitatively compare the two models on their neural alignment (I believe the data from Yildirim's paper should be public). You might also say that AlexNet and VGG are in the same ballpark for explaining the neural data, but then you should include that analysis in the paper and in the Yildirim paper, VGG seems to fall behind EIG's neural similarity by 10-20 percent points (Figure 3 D iv and E iv).</p></disp-quote><p>The findings of Yildirim et al. pertain to VGGFace, a VGG16-like architecture trained for face identification. Unlike VGG16, which is trained on the ImageNet dataset that encompasses a wide array of objects with significant intra- and inter-class variability, the VGGFace network’s training dataset lacks such diversity, consisting primarily of frontal views of faces. Therefore, VGGFace’s weaker brain alignment compared to EIG may stem from its training dataset and task, rather than from its architecture.</p><p>We have broadened our analysis to encompass EIG, ResNet50, ConvNeXt, ViT, and HMAX. Our findings indicate that VGGFace, in line with Yildirim’s research, exhibits one of the lowest values in mirror-symmetric viewpoint tuning compared to other models.</p><p>Conversely, networks such as ResNet50, VGG, AlexNet, and ConvNeXt demonstrate results comparable to EIG. These findings are detailed in Figures 2—figure supplement 3 and 5.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>– It would be reassuring to know that the object classes have independent measures of symmetry *on which the networks operate*. If the statements about object-class-specific symmetry came after performing the experiments, then I recommend re-writing so that those statements are interpretations.</p></disp-quote><p>Implemented. We have changed the order so that the explanations follow the experimental results. This includes the relevant main text paragraph, as well as the relevant figure—both the order of panels and the phrasing of the figure caption.</p><p>Manuscript changes:</p><p>“Figure 2C displays the average mirror-symmetric viewpoint tuning index for each object category across AlexNet layers. Several categories—faces, chairs, airplanes, tools, and animals—elicited low (below 0.1) or even negative mirror-symmetric viewpoint tuning values throughout the convolutional layers, transitioning to considerably higher (above 0.6) values starting from the first fully connected layer (fc6). In contrast, for fruits and flowers, mirror-symmetric viewpoint tuning was low in both the convolutional and the fully connected layers. For cars and boats, mirror-symmetric viewpoint tuning was notably high already in the shallowest convolutional layer and remained so across the network’s layers. To explain these differences, we quantified the symmetry of the various 3D objects in each category by analyzing their 2D projections (Figure 2—figure supplement 1). We found that all of the categories that show high mirror-symmetric viewpoint tuning index in fully connected but not convolutional layers have a single plane of symmetry. For example, the left and right halves of a human face are reflected versions of each other (Figure 2D). This 3D structure yields symmetric 2D projections only when the object is viewed frontally, thus hindering lower-level mirror-symmetric viewpoint tuning. Cars and boats have two planes of symmetry: in addition to the symmetry between their left and right halves, there is an approximate symmetry between their back and front halves. The quintessential example of such quadrilateral symmetry would be a Volkswagen Beetle viewed from the outside. Such 3D structure enables mirror-symmetric viewpoint tuning even for lower-level representations, such as those in the convolutional layers. Fruits and flowers exhibit radial symmetry but lack discernible symmetry planes, a characteristic that impedes viewpoint tuning altogether.”</p><disp-quote content-type="editor-comment"><p>– It would be a great contribution to the field if the authors could clarify the relationship between this work and Baek et al.'s. Can they confirm that untrained networks have mirror-tuned units? If that is not replicable, then the &quot;emergence&quot; framing is accurate, and one can exercise appropriate weighing of that other study. This would help the field. If learning just amplifies this symmetry (by strengthening connections of view-dependent units, for example), that is also helpful to learn.</p></disp-quote><p>Implemented. We agree with the reviewer that random initialization may result in units that show mirror-symmetric viewpoint tuning for faces in the absence of training. In the revised manuscript, we quantify the occurrence of such units, first reported by Baek et al., in detail, and discuss the relation between Baek et al., 2021 and our work. In brief, our analysis affirms that units with mirror-symmetric viewpoint tuning for faces appear even in untrained CNNs, although we believe their rate is lower than previously reported. Regardless of the question of the exact proportion of such units, we believe it is unequivocal that at the population level, mirror-symmetric viewpoint tuning to faces (and other objects with a single plane of symmetry) is strongly training-dependent.</p><p>First, we refer the reviewer to Figure 2—figure supplement 2, which directly demonstrates the effect of training on the population-level mirror symmetric viewpoint tuning.</p><p>Note the non-mirror-symmetric reflection invariant tuning profile for faces in the untrained network.</p><p>Second, the above-zero horizontal reflection Second, the above-zero horizontal reflection-invariance referred by the reviewer (Figure 3) is distinct from mirror-symmetric viewpoint tuning; the latter requires both reflection-invariance and viewpoint tuning. More importantly, it was measured with respect to all of the object categories grouped together; this includes objects with quadrilateral symmetry, which elicit mirror-symmetric viewpoint tuning even in shallow layers and without training. To clarify the confusion that this grouping might have caused, we repeated the measurement of invariance in fc6, separately for each 3D object category.</p><p>Disentangling the contributions of different categories to the reflection-invariance measurements, this analysis underscores the necessity of training for the emergence of mirror-symmetric viewpoint symmetry.</p><p>Last, we refer the reviewer to Figure 5—figure supplement 1, which shows that the symmetry of untrained convolutional filters has a narrow, zero-centered distribution. Indeed, the upper limit of this distribution includes filters with a certain degree of symmetry. This level of symmetry, however, becomes the lower limit of the filters’ symmetry distribution following training.</p><p>Therefore, we believe that training induces a shift in the tuning of the unit population that is qualitatively distinct from, and not explained by, random-lottery-related mirror-symmetric viewpoint tuned units. In the revised manuscript, we clarify the distinction between mirror-symmetric viewpoint tuning at the population level and the existence of individual units showing pre-training mirror symmetric viewpoint tuning, as shown by Baek et al.</p><p>Manuscript changes: (Discussion section)</p><p>“Our claim that mirror-symmetric viewpoint tuning is learning-dependent may seem to be in conflict with findings by Baek and colleagues [17]. Their work demonstrated that units with mirror-symmetric viewpoint tuning profile can emerge in randomly initialized networks. Reproducing Baek and colleagues’ analysis, we confirmed that such units occur in untrained networks (Figure 5—figure supplement 3). However, we also identified that the original criterion for mirror-symmetric viewpoint tuning employed in [17] was satisfied by many units with asymmetric tuning profiles (Figures 5—figure supplement 2 and 5—figure supplement 3). Once we applied a stricter criterion, we observed a more than twofold increase in mirror-symmetric units in the first fully connected layer of a trained network compared to untrained networks of the same architecture (Figure 5—figure supplement 4). This finding highlights the critical role of training in the emergence of mirror-symmetric viewpoint tuning in neural networks also at the level of individual units.”</p><p>References:</p><p>4 Winrich A Freiwald and Doris Y Tsao. Functional compartmentalization and viewpoint generalization within the macaque face-processing system. Science, 330(6005):845–851, 2010. doi:10.1126/science.1194908.</p><p>10 Amirhossein Farzmahdi, Karim Rajaei, Masoud Ghodrati, Reza Ebrahimpour, and Seyed-Mahdi Khaligh-Razavi. A specialized face-processing model inspired by the organization of monkey face patches explains several face-specific phenomena observed in humans. Scientific reports, 6 (1):1–17, 2016. doi:10.1038/srep25025.</p><p>14 Ilker Yildirim, Mario Belledonne, Winrich Freiwald, and Josh Tenenbaum. Efficient inverse graphics in biological face processing. Science Advances, 6(10):eaax5979, 2020. doi:10.1126/sciadv.aax5979.</p><p>17 Seungdae Baek, Min Song, Jaeson Jang, Gwangsu Kim, and Se-Bum Paik. Face detection in untrained deep neural networks. Nature Communications, 12(1):7328, December 2021. doi:10.1038/s41467-021-27606-9.</p><p>19 Joel Z Leibo, Qianli Liao, Fabio Anselmi, Winrich A Freiwald, and Tomaso Poggio. View-tolerant face recognition and hebbian learning imply mirror-symmetric neural tuning to head orientation. Current Biology, 27(1):62–67, 2017. doi:10.1016</p><p>41 L. S. Shapley. 17. A Value for n-Person Games, pages 307–318. Princeton University Press, Princeton, 1953. ISBN 9781400881970. doi:10.1515/9781400881970-018.</p><p>60. Chris Olah, Nick Cammarata, Chelsea Voss, Ludwig Schubert, and Gabriel Goh. Naturally occurring equivariance in neural networks. Distill, 2020. doi:10.23915/distill 00024.004.</p><p>61. Joel Z Leibo, Qianli Liao, Fabio Anselmi, and Tomaso Poggio. The invariance hypothesis implies domain-specific regions in visual cortex. PLoS computational biology, 11(10):e1004390, 2015. doi:10.1371/journal.pcbi.1004390.</p><p>70 Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2015.</p></body></sub-article></article>