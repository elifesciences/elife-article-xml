<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">61077</article-id><article-id pub-id-type="doi">10.7554/eLife.61077</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Signed and unsigned reward prediction errors dynamically enhance learning and memory</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-197816"><name><surname>Rouhani</surname><given-names>Nina</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-2814-0462</contrib-id><email>nrouhani@caltech.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-198577"><name><surname>Niv</surname><given-names>Yael</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-0259-8371</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Chen Neuroscience Institute, California Institute of Technology</institution><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Psychology, Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution>Princeton Neuroscience Institute, Princeton University</institution><addr-line><named-content content-type="city">Princeton</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Büchel</surname><given-names>Christian</given-names></name><role>Senior Editor</role><aff><institution>University Medical Center Hamburg-Eppendorf</institution><country>Germany</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>04</day><month>03</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e61077</elocation-id><history><date date-type="received" iso-8601-date="2020-07-14"><day>14</day><month>07</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-02-26"><day>26</day><month>02</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Rouhani and Niv</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Rouhani and Niv</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-61077-v2.pdf"/><abstract><p>Memory helps guide behavior, but which experiences from the past are prioritized? Classic models of learning posit that events associated with unpredictable outcomes as well as, paradoxically, predictable outcomes, deploy more attention and learning for those events. Here, we test reinforcement learning and subsequent memory for those events, and treat signed and unsigned reward prediction errors (RPEs), experienced at the reward-predictive cue or reward outcome, as drivers of these two seemingly contradictory signals. By fitting reinforcement learning models to behavior, we find that both RPEs contribute to learning by modulating a dynamically changing learning rate. We further characterize the effects of these RPE signals on memory and show that both signed and unsigned RPEs enhance memory, in line with midbrain dopamine and locus-coeruleus modulation of hippocampal plasticity, thereby reconciling separate findings in the literature.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>reinforcement learning</kwd><kwd>memory</kwd><kwd>reward prediction error</kwd><kwd>computational model</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000183</institution-id><institution>Army Research Office</institution></institution-wrap></funding-source><award-id>W911NF-14-1-0101</award-id><principal-award-recipient><name><surname>Niv</surname><given-names>Yael</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000025</institution-id><institution>National Institute of Mental Health</institution></institution-wrap></funding-source><award-id>R01MH098861</award-id><principal-award-recipient><name><surname>Niv</surname><given-names>Yael</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>Graduate Student Fellowship</award-id><principal-award-recipient><name><surname>Rouhani</surname><given-names>Nina</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R21MH120798</award-id><principal-award-recipient><name><surname>Niv</surname><given-names>Yael</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Throughout learning, both unexpected outcomes and the predictive value of cues modulate learning rate and episodic memory, thereby supporting two seemingly opposing theories of associative learning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The reward prediction error (RPE) is a canonical learning signal in reinforcement learning, updating stored information about the values of different experiences. This signal modulates dopaminergic firing from the midbrain, increasing dopamine release when rewards are better than expected, and decreasing its release when rewards are worse than expected (‘signed RPE’; <xref ref-type="bibr" rid="bib3">Barto, 1995</xref>; <xref ref-type="bibr" rid="bib35">Montague et al., 1996</xref>). Over the course of learning, this dopaminergic RPE transfers from unpredictable reward outcome to the cue predicting the reward (<xref ref-type="bibr" rid="bib53">Schultz et al., 1997</xref>). The resulting signed RPE at cue putatively supports an associative model (‘Mackintosh model’; <xref ref-type="bibr" rid="bib32">Mackintosh, 1975</xref>) where attention increases for cues that reliably predict reward. This signed RPE could also give rise to stronger memory traces, given that neural plasticity in the hippocampus – the key structure for episodic memory – is modulated by dopamine (<xref ref-type="bibr" rid="bib31">Lisman and Grace, 2005</xref>; <xref ref-type="bibr" rid="bib55">Shohamy and Adcock, 2010</xref>).</p><p>An alternative possibility is that RPE magnitude, regardless of its sign (‘unsigned RPE’), enhances learning and memory for surprisingly good or bad outcomes. In fact, the Pearce-Hall model of learning (<xref ref-type="bibr" rid="bib43">Pearce and Hall, 1980</xref>), which contradicts the Mackintosh model, posits that attention is enhanced for cues that are accompanied by surprise, that is, those that co-occur with large unsigned RPEs. The effects of unsigned RPEs are thought to be mediated by the locus coeruleus-norepinephrine system, which responds to unexpected changes in stimulus-reinforcement contingencies, regardless of the sign of the outcome (for a review, see <xref ref-type="bibr" rid="bib52">Sara, 2009</xref>). Moreover, recent evidence points to the locus coeruleus (LC), which co-releases dopamine with norpeinephrine, as providing an alternative source of dopamine to the hippocampus, giving rise to hippocampal memories (<xref ref-type="bibr" rid="bib28">Kempadoo et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Takeuchi et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Wagatsuma et al., 2018</xref>).</p><p>Albeit paradoxical, it is theoretically possible that both surprise (Pearce-Hall model) and predictability (Mackintosh model) modulate memory throughout learning (<xref ref-type="bibr" rid="bib30">Le Pelley, 2004</xref>; <xref ref-type="bibr" rid="bib5">Beesley et al., 2015</xref>), but in different ways, and through distinct neural mechanisms. Previously, we found that unsigned, but not signed, RPEs experienced at reward outcome boost learning and memory (<xref ref-type="bibr" rid="bib48">Rouhani et al., 2018</xref>), consistent with work showing better memory for surprising events (<xref ref-type="bibr" rid="bib20">Greve et al., 2017</xref>; <xref ref-type="bibr" rid="bib2">Antony et al., 2021</xref>). There is also recent support for signed RPEs experienced at reward-predictive cue to enhance memory (<xref ref-type="bibr" rid="bib26">Jang et al., 2019</xref>), reminiscent of work showing memory benefits during periods of high-reward anticipation (<xref ref-type="bibr" rid="bib1">Adcock et al., 2006</xref>; <xref ref-type="bibr" rid="bib38">Murty and Adcock, 2014</xref>; <xref ref-type="bibr" rid="bib56">Stanek et al., 2019</xref>; <xref ref-type="bibr" rid="bib66">Wittmann et al., 2005</xref>).</p><p>Accordingly, we hypothesized a signed-RPE effect on memory during the reward-predicting cue once participants had learned cue values, as well as an unsigned-RPE effect on memory during reward outcome throughout learning. We included two trial-unique images on every learning trial, one at cue and one at outcome, to dissociate the effects of the two RPEs on memory (<xref ref-type="fig" rid="fig1">Figure 1</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Reward prediction error (RPE) signals in a learning trial in Experiments 1 and 2.</title><p>(<bold>A,D</bold>) Each trial was initiated by a reward-predicting cue represented by a trial-unique image. Participants were then asked to indicate how much that reward category was worth ‘on average.’ They then saw the reward outcome (a proportion of which they received) along with a second trial-unique image. In Experiment 1 (<bold>A</bold>), all images were of objects (single reward category), whereas in Experiment 2 (<bold>D</bold>), each trial included either two indoor or two outdoor scenes (two cue categories). (<bold>B-F</bold>) Theoretical RPE signals (<bold>B,E</bold>) and their calculation (<bold>C,F</bold>). Unsigned RPEs at outcome (in blue) were calculated by taking the absolute difference between the participant’s value for that reward cue and the subsequent outcome. We expected this (putatively noradrenergic) unsigned signal to enhance memory for more surprising outcomes, which we tested in both Experiments 1 and 2. Signed RPEs at cue (<bold>E</bold>, in red) were calculated by taking the difference between the participant’s predicted value for the current reward category (here, outdoor scenes) and their most recently predicted value of the other category (indoor scenes). We expected this (putatively dopaminergic) signed signal to boost memory for more valued events, that is, better memory the more positive the RPE. Prediction errors at outcome gradually transfer to cue through the learning process (<bold>E</bold>, dotted lines represent signed RPE in previous two trials, darker red indicates more recent trial).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig1-v2.tif"/></fig><p>We characterized these effects in two experiments that each prioritized the influence of one of these RPE signals. In Experiment 1, participants experienced large unsigned RPEs brought on by periods of high outcome variance (‘high’ versus ‘low variance’ contexts) and reward-value change points (changes to the mean of the underlying reward distribution). We expected these large unsigned RPEs, experienced at reward outcome, to modulate learning rate and boost memory for events throughout learning (<xref ref-type="fig" rid="fig1">Figure 1A–C</xref>). In Experiment 2, in contrast, participants learned the values of two categories of cues, eliciting RPEs at cue as well as outcome (<xref ref-type="fig" rid="fig1">Figure 1D–F</xref>). Here, the underlying reward distribution associated with each category did not change, allowing for RPEs at cue (i.e. a relative value signal) to increase in magnitude with more experience with each category. We expected both signed RPEs at cue and unsigned RPEs at outcome to influence learning rate and memory for those events.</p><p>To assess differential effects of RPEs on instructed versus incidental memory, we ran two versions of Experiment 2. In the instructed version (and in Experiment 1), participants were explicitly prompted to memorize as they were told they would later choose between the items presented during learning and win their associated reward values again. We thus incentivized participants to associate the trial-unique items with their reward values during learning. In the incidental version, we removed this instruction, making memory for the trial-unique items completely unintentional.</p><p>We analyzed learning, memory, and choice data using complementary approaches. To understand the learning process, we compared computational models of learning that formalized different putative effects of signed cue RPEs and unsigned outcome RPEs on subjects’ predictions of trial-by-trial cue values. To test the effects of signed and unsigned cue and outcome RPEs on memory performance, we used mixed-effects modeling (also used to analyze learning and later choice) and Bayesian hierarchical modeling.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Learning results</title><sec id="s2-1-1"><title>Unsigned reward prediction errors at outcome and signed reward prediction errors at cue influenced learning rate</title><p>We first tested whether RPEs experienced during reward learning predicted empirical, trial-by-trial, learning rates. Learning rates were measured by comparing consecutive predictions for the same cue category, and dividing the difference in predictions by the empirical outcome prediction error experienced on the earlier of the two trials (see <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> in ‘Materials and methods’). We treated the unsigned RPE at reward outcome (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, in blue) as a ‘Pearce-Hall’ signal, as it reflects how unpredictable the reward was. We found the unsigned RPE at reward outcome boosted learning rate in both experiments, thereby providing direct behavioral evidence for this ‘Pearce-Hall’ component on learning rate (mixed-effects linear regression, Experiment 1: <italic>β</italic> = 0.10, <italic>t</italic> = 6.39, p&lt;0.001; Experiment 2: <italic>β</italic> = 0.07 <italic>t</italic>=8.79, p&lt;0.001; see <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref> for empirical learning rates).</p><p>We treated the learned value difference between two reward-predictive cues in Experiment 2 as a ‘Mackintosh’ signal, as higher learned values for one cue versus the other implied better reward predictiveness. We refer to this value signal as a signed cue RPE (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, red), as when there are several possible cues, the onset of a cue resolves the prediction for the current trial, and is accompanied by an RPE that reflects the signed difference between the current predicted reward and the average reward predicted before cue onset (<xref ref-type="bibr" rid="bib41">Niv and Schoenbaum, 2008</xref>). We found that a signed cue RPE was anti-correlated with learning rate, potentially demonstrating stronger associative links and more stable values for more valuable cues (<italic>β</italic> = −0.02 <italic>t</italic> = −2.31, p=0.02). Critically, we found this effect even when controlling for any effect of the unsigned cue RPE on learning rate (which was not itself significant, <italic>β</italic> = −0.01 <italic>t</italic> = −1.35, p=0.18). This suggests that the cue RPE modulation of learning rate was not merely due to the greater learned separation between the two cue values, but specific to more stable updating for the high-valued cues.</p></sec><sec id="s2-1-2"><title>Learning behavior in the experimental conditions of Experiment 2</title><p>Experiment 2 involved four conditions in a between-participants 2 × 2 design. First, two learning conditions varied in difficulty due to different degrees of overlap between the reward distributions of the two categories. In the 40¢−60¢ condition, the means of the two reward categories were 40¢ and 60¢, with considerable overlap in the two reward distributions. In the 20¢−80¢ condition, on the other hand, the two means were 20¢ and 80¢, and there was no overlap between the two reward distributions. As expected, participants separated the values of two scene categories more in the 20¢−80¢ condition than in the 40¢−60¢ condition (<xref ref-type="fig" rid="fig2">Figure 2B</xref>), both in general and as a function of trial number during learning (mixed-effects linear regression, value separation as a function of learning condition: <italic>β</italic> = 18.62, <italic>t</italic> = 25.87, p&lt;0.001; interaction between learning condition and trial number: <italic>β</italic> = 5.40, <italic>t</italic> = 13.64, p&lt;0.001).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Learning behavior and modeling results.</title><p>(<bold>A</bold>) Experiment 1 average participant value estimates as a function of trial number (blue and red lines represent two different outcome-variance contexts: blue = ‘low variance’ learning context, red = ‘high variance’ learning context; shading indicates 95% confidence intervals), and average predictions of the RW-PH-D model with SEM bars in black. Actual reward outcomes on each trial are indicated by x’s, stars indicate a change-point trial. (<bold>B-C</bold>) Experiment 2 average participant value estimates for the two scene categories (green and yellow) as a function of trial number in the 40¢−60¢ condition (<bold>B</bold>; means of the two scene categories 40¢ and 60¢) and the 20¢−80¢ condition (<bold>C</bold>; average means 20¢ and 80¢), and average predictions of the RW-PH-M-D model in black. Actual rewards varied across subjects. Although each subject saw only 15 trials of each scene type (one of two scene-value categories on each trial), we pseudo-randomized the sequence of scene-value categories so that across participants, we had data for both categories on each trial. (<bold>D-E</bold>) Total negative log-likelihood scores across subjects for each of the models tested. Lower scores indicate better fit between model predictions and empirical data; bars on the winning model indicate the minimum difference needed for a significant difference between models in the likelihood-ratio test, given the number of extra parameters in the more complex model; ‘RW’: Rescorla-Wagner, ‘PH’: Pearce-Hall, ‘M’: Mackintosh, ‘D’: Decay. In Experiment 1 (<bold>D</bold>), the RW-PH-D model, which included a Pearce-Hall and a decay component, was the winning model. In Experiment 2 (<bold>E</bold>), the RW-PH-M-D, which additionally included a Mackintosh component, outperformed the other models.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Empirical learning rates in Experiments 1 and 2.</title><p>Learning rates are plotted on the trial that generated the learning (so the learning rate on trial 1 is the proportion of the prediction error applied to the value that was then predicted on trial 2, and so forth.) (<bold>A</bold>) Experiment 1 average learning rate as a function of trial number; starred points indicate change-point trials within a context (note there were also changes between contexts). (<bold>B-C</bold>) Experiment 2 average learning rate for the two scene categories (green and yellow) as a function of trial number in the 40¢−60¢ condition (<bold>B</bold>) and the 20¢−80¢ condition (<bold>C</bold>).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Model validation simulations from Experiment 2.</title><p>‘RW’: Rescorla-Wagner, ‘PH’: Pearce-Hall, ‘M’: Mackintosh, ‘D’: Decay. (<bold>A</bold>) Distribution of learning rates across trials (light to dark colors) from the winning model, ‘RW-PH-M-D’. Learning rates were simulated using the best fit parameters for each of the participants. Distributions were approximated from the 685 learning rates simulated per trial (one for each participant). Model learning rates decrease over time. (<bold>B</bold>) Confusion matrix showing <italic>p</italic>(fit model|true model), the probability that data generated by a model (‘true model’) were best fit by that model (‘fit model’), using Bayesian information criterion (BIC) score. Higher values on the diagonal indicate successful model recovery, and show that data simulated by a model were adequately best fit by that same model. Models: 1 = ‘RW’, 2 = ‘RW-D’, 3 = ‘RW-PH’, 4 = ‘RW-M’, 5 = ‘RW-PH-M’, 6 = ‘RW-PH-D’, 7 = ‘RW-M-D’, 8 = ‘RW-PH-M-D’.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig2-figsupp2-v2.tif"/></fig></fig-group><p>We also manipulated whether participants were intentionally or incidentally encoding the trial-unique scenes. We either instructed them to attend to the scenes in order to choose between them and win their associated reward later in the task (‘instructed memory’), or we did not provide any instruction motivating remembering of the scenes (‘incidental memory’). Although the instructions regarding value learning and prediction were identical for all participants, we did find overall differences in learning across the two memory-instruction conditions. Participants learned better and were more accurate in the instructed memory version: their estimates were closer to the actual underlying means of the scene categories and those estimates became more accurate over time (learning accuracy as a function of instructed-incidental memory: <italic>β</italic> = 4.35, <italic>t</italic> = 8.16, p&lt;0.001; interaction between instructed-incidental memory and trial number: <italic>β</italic> = 0.48, <italic>t</italic> = 2.08, p=0.04).</p><p>Although we did not expect this difference in learning, it is possible that motivating the remembering of more valuable scenes led participants to attend more to learning those values as well. Additionally, we note that the instructed and incidental memory versions of the task were tested during different social climates, with the data for the incidental memory version collected during 2020’s global pandemic, potentially accounting for the difference in learning performance. Interestingly, participants’ estimates, and thus expectations for reward, were, on average, lower during the pandemic than before, demonstrating more pessimistic expectations overall (<italic>β</italic> = −1.17, <italic>t</italic> = −2.65, p=0.008).</p></sec></sec><sec id="s2-2"><title>Reinforcement-learning models</title><p>To further determine how unsigned RPEs at reward outcome (‘outcome RPEs’) and signed RPEs at reward-predicting cue (‘cue RPEs’) influence learning, we modeled participants’ trial-by-trial value estimates testing a series of reinforcement-learning models. We modeled Experiments 1 and 2 separately due to their different learning structures, and modeled all learning and memory conditions of Experiment 2 together since the learning instructions and structure were the same across all variants. We also performed and confirmed model recovery on simulated data to verify that our data can arbitrate between these models (see ‘Model fitting and comparison’ in ‘Materials and methods’).</p><sec id="s2-2-1"><title>Experiment 1</title><p>We fit learning behavior in Experiment 1 using four models: a Rescorla-Wagner model with a fixed learning rate (‘RW’), an RW-model with a Pearce-Hall (<xref ref-type="bibr" rid="bib43">Pearce and Hall, 1980</xref>) component modulating learning rate (‘RW-PH’), an RW-model with a decaying learning rate (‘RW-D’), and a full model with both Pearce-Hall modulation and decay (‘RW-PH-D’); see ‘Materials and methods’. Note, we did not test models that included cue RPEs since there was a single reward category in this experiment. We found that the full model that included a Pearce-Hall component, which modulated learning rate by the unsigned outcome RPE, along with a decay, fit better than models without those components (likelihood-ratio tests, RW-PH-D vs. RW: <inline-formula><mml:math id="inf1"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (243) = 568.45, p&lt;0.001; RW-PH-D vs. RW-PH: <inline-formula><mml:math id="inf2"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (162) = 212.87, p=0.005; RW-PH-D vs. RW-D: <inline-formula><mml:math id="inf3"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (81) = 255.71, p&lt;0.001; <xref ref-type="fig" rid="fig2">Figure 2A,D</xref>, <xref ref-type="table" rid="table1">Table 1</xref>).</p></sec><sec id="s2-2-2"><title>Experiment 2</title><p>Here, participants experienced RPEs at both cue and outcome, allowing us to test the models above, as well as four models that included a Mackintosh-type component (denoted by ‘M’: ‘RW-M’, ‘RW-PH-M’, ‘RW-M-D’, ‘RW-PH-M-D’; see ‘Materials and methods’). The model that included all three tested modulators of learning rate—an unsigned RPE at outcome (Pearce-Hall component), a signed RPE at cue (Mackintosh component), and an exponential decay—predicted participant value estimates best (<xref ref-type="fig" rid="fig2">Figure 2B,C,E</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). This model had a significantly better (i.e. lower) likelihood compared to every other model as assessed by the likelihood-ratio test (RW-PH-M-D vs. RW: <inline-formula><mml:math id="inf4"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (2740)=6358.34, p&lt;0.001; RW-PH-M-D vs. RW-M: <inline-formula><mml:math id="inf5"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (2055)=5017.65, p&lt;0.001; RW-PH-M-D vs. RW-PH: <inline-formula><mml:math id="inf6"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (2055)=4803.86, p&lt;0.001; RW-PH-M-D vs. RW-PH-M: <inline-formula><mml:math id="inf7"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (1370)=3130.53, p&lt;0.001; RW-PH-M-D vs. RW-D: <inline-formula><mml:math id="inf8"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (1370)=2989.18, p&lt;0.001; RW-PH-M-D vs. RW-M-D: <inline-formula><mml:math id="inf9"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (685)=1203.79, p&lt;0.001; RW-PH-M-D vs. RW-PH-D: <inline-formula><mml:math id="inf10"><mml:msup><mml:mi>χ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula> (685)=1603.55, p&lt;0.001).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Model parameters and fit results.</title><p>‘RW’: Rescorla-Wagner, ‘PH’: Pearce-Hall, ‘M’: Mackintosh, ‘D’: Decay. Negative log-likelihood across participants for Experiment 1 (first row within each model) and Experiment 2 (second row within each model); ‘d’ refers to the difference in score between the tested model and the baseline fixed learning rate model (‘RW’). Lower scores indicate better fit. In Experiments 1 and 2, models that included all tested components of learning rate performed best according to the likelihood-ratio test (which penalizes nested models for added parameters).</p></caption><table frame="hsides" rules="groups"><thead><tr><th>Model</th><th>Parameters</th><th>-LL</th></tr></thead><tbody><tr><td rowspan="2">RW</td><td rowspan="2">α</td><td>20911.33</td></tr><tr><td>82049.33</td></tr><tr><td rowspan="2">RW-PH</td><td rowspan="2">η, κ</td><td>20733.54 (d = -177.79)</td></tr><tr><td>81272.09 (d = -777.24)</td></tr><tr><td>RW-M</td><td>η, γ</td><td>81378.98 (d = -670.35)</td></tr><tr><td rowspan="2">RW-D</td><td rowspan="2">η, N, λ</td><td>20754.96 (d = -156.37)</td></tr><tr><td>80364.74 (d = -1684.58)</td></tr><tr><td>RW-PH-M</td><td>η, κ, γ</td><td>80435.42 (d = -1613.90)</td></tr><tr><td rowspan="2">RW-PH-D</td><td rowspan="2">η, κ, N, λ</td><td>20627.11 (d = -284.22)</td></tr><tr><td>79671.93 (d = -2377.39)</td></tr><tr><td>RW-M-D</td><td>η, γ, N, λ</td><td>79472.05 (d = -2577.28)</td></tr><tr><td>RW-PH-M-D</td><td>η, κ, γ, N, λ</td><td>78870.16 (d = -3179.17)</td></tr></tbody></table></table-wrap></sec></sec><sec id="s2-3"><title>Memory results by learning condition</title><p>To understand how cue and outcome RPEs affected memory for the trial-unique images, we analyzed memory results separately for Experiment 1 and the four conditions of Experiment 2 (instructed or incidental memory x learning difficulty). For each participant and each item tested, we calculated a memory score that combines memory accuracy (hit or miss) with confidence (from 1 = ‘guessing’ to 4 = ‘completely certain’), ranging from a ‘completely certain’ miss (1) to a ‘completely certain’ hit (8).</p><sec id="s2-3-1"><title>High reward variance boosted memory for outcome events</title><p>Experiment 1 allowed us to test how reward variance modulates memory for cue and outcome events. In line with our previous work (<xref ref-type="bibr" rid="bib48">Rouhani et al., 2018</xref>), we expected that the larger unsigned RPEs in a high-variance context would improve memory for related events, and therefore memory for high-variance items would be better overall. We found an interaction of cue versus outcome memory by variance condition, such that in the high-variance condition, there was a lower average memory score for cue events, and a higher average memory score for outcome events, compared to the low-variance condition (μ high-variance cue memory = 6.44, μ low-variance cue memory = 6.57, μ high-variance outcome memory = 5.79, μ low-variance outcome memory = 5.54; mixed-effects linear regression: <italic>β</italic> = −0.37, <italic>t</italic> = −2.78, p=0.005). Within the interaction, there was a significant difference in memory for outcome events (<italic>β</italic> = −0.25, <italic>t</italic> = −2.09, p=0.04) but not for cue events (<italic>β</italic> = 0.12, <italic>t</italic> = 1.41, p=0.16). This suggests a role for the high-variance context, characterized by larger unsigned RPEs, in boosting memory for outcome events.</p></sec><sec id="s2-3-2"><title>Memory for cue events increased with reward learning</title><p>We tested the effects of cue RPEs on memory by first comparing differences in cue memory in Experiment 1 and the instructed memory version of Experiment 2. In both experiments, participants were told they would have a chance to select among previously-seen items in a later phase, which encouraged explicit encoding of the items. Experiment 1 involved a single reward category and therefore did not elicit RPEs at cue. Frequent change points in the underlying reward distribution encouraged ongoing new learning. In Experiment 2, on the other hand, participants learned about two reward categories, evoking RPEs at cue, and the underlying reward distributions did not change, encouraging convergence of learning. We therefore predicted memory for cue items to be modulated by learning in Experiment 2 but not in Experiment 1. Since the additional monetary outcome accompanying the outcome image may, in general, interfere with encoding of the image, we also expected overall better memory for cue as compared to outcome images. We controlled for this nuisance effect in all our analyses.</p><p>In line with our predictions, in Experiment 1, we did not find cue memory to change relative to outcome memory throughout learning (mixed-effects linear regression, memory as a function of the interaction between event type (cue or outcome) and learning trial, <italic>β</italic> = 0.009, <italic>t</italic> = 0.13, p=0.90; <xref ref-type="fig" rid="fig3">Figure 3A</xref>), whereas in Experiment 2, memory for cue events improved throughout the experiment, relative to memory for outcome events, in both difficulty conditions (interaction between event type and learning trial, 40¢−60¢ condition: <italic>β</italic> = −0.24, <italic>t</italic> = −3.76, p&lt;0.001; 20¢−80¢ condition: <italic>β</italic> = −0.20, <italic>t</italic> = −3.29, p&lt;0.001; <xref ref-type="fig" rid="fig3">Figure 3B–C</xref>).</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Memory accuracy across learning.</title><p>(<bold>A</bold>) Experiment 1 memory score as a function of trial number; starred points indicate change-point events. Background shading indicates condition (low- or high-reward variance). Cue memory (in orange) was in general better than outcome memory (in purple; this effect was controlled for in all of our analyses). Cue memory did not change relative to outcome memory throughout learning. Reward change-points (starred) increased memory for the outcome event. (<bold>B-E</bold>) Experiment 2 memory scores in the 40¢−60¢ condition (<bold>B,D</bold>) and the 20¢−80¢ condition (<bold>C,E</bold>), as a function of ‘learning phase’ (first, second, and third bins of learning trials). In the instructed memory version of Experiment 2 (<bold>B,C</bold>), learning enhanced cue memory in both conditions, whereas in the incidental memory version, this enhancement only occurred in the 20¢−80¢ condition, the easier learning condition (<bold>E</bold>). Differences between the instructed and incidental memory versions of the 40¢−60¢ condition were related to differences in learning performance (see main text).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Individual differences in memory for cue and outcome events as a function of learning performance in the instructed (<bold>A</bold>) and incidental (<bold>B</bold>) memory versions of Experiment 2.</title><p>Better learners, as determined by the average difference between the high and low-value scene categories in the last five trials of learning of each category (‘learning separation’), were more likely to show better memory for cue events, but not for outcome events, as learning proceeded ('memory slope’). This relationship was strongest for the 40¢−60¢ condition, the more difficult learning condition, in the incidental memory task (<bold>B</bold>). Learning performance was worse and more variable in the incidental memory version (note location of points leftward on X axis as compared to the instructed version), which can potentially explain why there was no overall increase in cue memory over learning in the 40¢−60¢ condition of the incidental task. These results further link stronger reward expectations to increasing memory for cue events.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig3-figsupp1-v2.tif"/></fig></fig-group><p>We next compared memory results between the instructed and incidental memory versions of Experiment 2. We did not find the increase in cue memory as a function of learning in the 40¢−60¢ learning condition (interaction between event type and learning trial: <italic>β</italic> = 0.07, <italic>t</italic> = 1.11, p=0.27; three-way interaction between event type, instructed-incidental memory and learning trial <italic>β</italic> = 0.31, <italic>t</italic> = 3.39, p&lt;0.001; <xref ref-type="fig" rid="fig3">Figure 3D</xref>). However, we did replicate the increase in cue memory in the 20¢−80¢ condition (interaction between event type and learning trial: <italic>β</italic> = −0.17, <italic>t</italic> = −2.79, p=0.005; three-way interaction between event type, instructed-incidental memory and learning trial: <italic>β</italic> = 0.04, <italic>t</italic> = 0.41, p=0.68; <xref ref-type="fig" rid="fig3">Figure 3E</xref>).</p><p>To explain the difference in cue memory between the instructed and incidental memory versions of the 40¢−60¢ condition, we first note that learning to predict the outcome value was in general worse in the incidental memory version (see ‘Learning behavior in the experimental conditions of Experiment 2’, above). As the 40¢−60¢ condition was the more difficult learning environment, it is possible that worse learning prevented an average increase in cue memory as a function of learning. To test this hypothesis, we investigated whether individual differences in outcome-value learning predict differences in cue memory over learning. We computed learning performance for each participant by averaging the estimates of the last five trials of each scene category, and subtracting the average estimate of the low-value scene category from that of the high-value scene category. A larger, positive difference between the two scene categories indicates greater learned separation of the values of the two scene categories, whereas a smaller or negative difference indicates worse learning. To measure the individual increase in memory for events as a function of learning, we ran two mixed-effects models predicting memory as a function of trial number for (1) cue events and (2) outcome events, and extracted participant slopes from each model. We then tested whether individual learning performance predicted this change in cue or outcome memory over learning.</p><p>Overall, better individual learning predicted a greater increase in memory for cue events (linear regression: <italic>β</italic> = 0.02, <italic>t</italic> = 3.07, p=0.002), but not for outcome events (<italic>β</italic> = 0.001, <italic>t</italic> = 0.24, p=0.81; interaction between event type and learning performance: <italic>β</italic> = −0.01, <italic>t</italic> = −2.29, p=0.02). This relationship was stronger in the incidental memory task (<italic>β</italic> = 0.02, <italic>t</italic> = 3.61, p&lt;0.001; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>) than in the instructed one (<italic>β</italic> = −0.004, <italic>t</italic> = −0.61, p=0.55, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>; interaction between instructed-incidental memory and learning performance: <italic>β</italic> = 0.03, <italic>t</italic> = 2.61, p=0.009), and in the 40¢−60¢ condition, the more difficult learning task, relative to the 20¢−80¢ condition (40¢−60¢ condition: <italic>β</italic> = 0.03, <italic>t</italic> = 4.57, p&lt;0.001; 20¢−80¢ condition: <italic>β</italic> = 0.01, <italic>t</italic> = 1.50, p=0.13; interaction between condition and learning performance: <italic>β</italic> = −0.04, <italic>t</italic> = −3.17, p=0.002). These results confirm that more learning led to a greater increase in memory for cue events. Furthermore, it suggests that the difference in results between the instructed and incidental memory versions of the 40¢−60¢ condition could be accounted for by worse learning performance. In this condition, there is a strong relationship between learning performance and learning-modulated cue memory: here, only a minority of participants who had learned to separate the values of the scene categories showed an increase in cue memory over learning, leading to an overall lack of effect at the group level.</p></sec></sec><sec id="s2-4"><title>Memory results by reward prediction error</title><p>We investigated the effects of trial-by-trial reward prediction errors at cue and outcome in two ways, and modeled Experiment 1 and the instructed and incidental memory versions of Experiment 2 separately (see ‘Materials and methods’ for details). First, we used mixed-effects linear regression modeling to test the overall effects of RPEs on memory, including interactions between cue and outcome events, and then examined the effects of RPEs on cue and outcome memory separately. This resulted in three mixed-effects regression models for each experiment. We report model estimates and significance testing for these tests. Second, we ran three Bayesian hierarchical models (again, one model per experiment) including all RPEs as regressors and using a confound regressor to control for the, on average, better memory for cue events. We report the median (measure of centrality, ‘M’) and the high-density interval (measure of uncertainty, ‘HDI’) from the posterior parameter distributions generated by the Bayesian hierarchical model.</p><sec id="s2-4-1"><title>Unsigned but not signed reward prediction errors at outcome enhanced memory</title><p>In all experiments, we found that unsigned outcome RPEs enhanced memory for outcome images (outcome memory as a function of |outcome RPE|, Experiment 1: M = 0.14, HDI [0.06, 0.23], <italic>β</italic> = 0.19, <italic>t</italic> = 3.51, p&lt;0.001, <xref ref-type="fig" rid="fig4">Figure 4A</xref>; Experiment 2 - instructed memory: M = 0.14, HDI [0.05, 0.23], <italic>β</italic> = 0.09, <italic>t</italic> = 2.59, p=0.009, <xref ref-type="fig" rid="fig4">Figure 4D</xref>; Experiment 2 - incidental memory: M = 0.13, HDI [0.06, 0.19], <italic>β</italic> = 0.11, <italic>t</italic> = 3.01, p=0.003, <xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Parameter distributions from hierarchical Bayesian models of memory in Experiment 1 (<bold>A–B</bold>) and Experiment 2 (<bold>C-F,</bold> yellow background indicates incidental memory version).</title><p>Distributions significantly above or below zero indicate an effect, black stars indicate significance: p&lt;0.1∼, p&lt;0.05*, p&lt;0.01**. Unsigned outcome RPEs (<bold>A,D</bold>) increased memory for outcome events, whereas signed outcome RPEs (<bold>B,F</bold>) did not. Signed cue RPEs (<bold>E</bold>) boosted memory for the cue item, and also enhanced memory for the outcome item in the instructed memory version. Unsigned cue RPEs (<bold>C</bold>) additionally, and separately, enhanced memory for cue events in the incidental memory version of the task; this effect was trending in increasing cue and outcome events in the instructed memory task.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig4-v2.tif"/></fig><p>Unsigned outcome RPEs in Experiment 1 resulted from either reward variance or changes to the underlying distribution of the mean. We found separate effects of RPEs due only to reward variance and those due to change points on increasing memory for outcome images (outcome memory as a function of variance RPEs: <italic>β</italic> = 0.14, <italic>t</italic> = 2.38, p=0.02; outcome memory as a function of change-point RPEs: <italic>β</italic> = 0.24, <italic>t</italic> = 2.08, p=0.04).</p><p>In Experiment 1, we also found unsigned outcome RPEs to boost memory for cue images from that same trial, but with a median parameter estimate half the size of that for outcome images (cue memory as a function of |outcome RPE|: M = 0.08, HDI [−0.01, 0.16], <italic>β</italic> = 0.10, <italic>t</italic> = 2.29, p=0.02, <xref ref-type="fig" rid="fig4">Figure 4A</xref>). We did not find this effect in any of the conditions of Experiment 2, where unsigned outcome RPE enhanced memory only for outcome images (Experiment 2 - instructed memory, cue memory as a function of |outcome RPE|: M = −0.02, HDI [−0.11, 0.06], <italic>β</italic> = −0.01, <italic>t</italic> = −0.46, p=0.64, interaction between event type (cue or outcome event) and |outcome RPE|: <italic>β</italic> = 0.09, <italic>t</italic> = 2.06, p=0.04; Experiment 2 - incidental memory, cue memory as a function of |outcome RPE|: M = −0.004, HDI [−0.07, 0.07], <italic>β</italic> = 0.006, <italic>t</italic> = 0.20, p=0.84, interaction between event type and |outcome RPE|: <italic>β</italic> = 0.09, <italic>t</italic> = 1.98, p=0.05, <xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><p>As expected based on previous work (<xref ref-type="bibr" rid="bib48">Rouhani et al., 2018</xref>), we did not find any influence of signed outcome RPEs on memory for cue or outcome images in either experiment (<xref ref-type="fig" rid="fig4">Figure 4B,F</xref>; cue memory as a function of signed outcome RPE, Experiment 1: M = −0.02, HDI [−0.11, 0.06], <italic>β</italic> = −0.05, <italic>t</italic> = −1.04, p=0.30; Experiment 2 - instructed memory, M = 0.002, HDI [−0.07, 0.07], <italic>β</italic> = −0.02, <italic>t</italic> = −0.66, p=0.51; Experiment 2 - incidental memory, M = −0.01, HDI [−0.08, 0.05], <italic>β</italic> = −0.003, <italic>t</italic> = −0.08, p=0.93; outcome memory as a function of signed outcome RPE, Experiment 1: M = −0.0007, HDI [−0.09, 0.08], <italic>β</italic> = −0.004, <italic>t</italic> = −0.07, p=0.95; Experiment 2 - instructed memory, M = −0.03, HDI [−0.10, 0.03], <italic>β</italic> = −0.05, <italic>t</italic> = −1.46, p=0.15; Experiment 2 - incidental memory, M = −0.05, HDI [−0.11, 0.02], <italic>β</italic> = −0.06, <italic>t</italic> = −1.61, p=0.11).</p></sec><sec id="s2-4-2"><title>Reward prediction errors at cue enhanced memory</title><p>Prediction errors at cue were elicited only in Experiment 2. In the instructed memory version of Experiment 2, we found that signed cue RPEs (i.e. the signed difference between the participant-reported value of the current cue and the most-recently reported value of the alternative cue) boosted memory for both cue and outcome events, such that memory of higher (relative) value scenes was better than that for lower (relative) value scenes (cue memory as a function of signed cue RPE: M = 0.08, HDI [0.01, 0.15], <italic>β</italic> = 0.08, <italic>t</italic> = 2.64, p=0.008; outcome memory as a function of signed cue RPE: M = 0.06, HDI [−0.01, 0.12], <italic>β</italic> = 0.07, <italic>t</italic> = 2.23, p=0.03; <xref ref-type="fig" rid="fig4">Figure 4E</xref>). In the incidental version, we replicated the effect of cue RPE on memory for cue images, but not on memory for outcome images (cue memory as a function of signed cue RPE: M = 0.10, HDI [0.04, 0.17], <italic>β</italic> = 0.09, <italic>t</italic> = 3.04, p=0.002; outcome memory as a function of signed cue RPE: M = 0.03, HDI [−0.04, 0.09], <italic>β</italic> = 0.03, <italic>t</italic> = 0.80, p=0.42; <xref ref-type="fig" rid="fig4">Figure 4E</xref>).</p><p>We found a separate effect of unsigned cue RPE on memory, such that the more participants had separated the values of the two scene categories (i.e. the more they had learned), the better their memory for scene images. This effect was evident in overall memory in the instructed memory version of Experiment 2 (memory as a function of |cue RPE|: <italic>β</italic> = 0.07, <italic>t</italic> = 2.49, p=0.01), however, when quantifying the effect for cue and outcome memory separately, each of them was only trending to significance (cue memory as a function of |cue RPE|: M = 0.09, HDI [0.01, 0.16], <italic>β</italic> = 0.07, <italic>t</italic> = 1.88, p=0.06; outcome memory as a function of |cue RPE|: M = 0.06, HDI [−0.01, 0.13], <italic>β</italic> = 0.08, <italic>t</italic> = 1.82, p=0.07; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). In the incidental version of Experiment 2, unsigned cue RPEs significantly increased memory for cue images (cue memory as a function of |cue RPE|: M = 0.12, HDI [0.05, 0.18], <italic>β</italic> = 0.10, <italic>t</italic> = 2.93, p=0.003), but not outcome images (outcome memory as a function of |cue RPE|: M = 0.02, HDI [−0.05, 0.08], <italic>β</italic> = 0.01, <italic>t</italic> = 0.30, p=0.76; interaction between event type and |cue RPE|: <italic>β</italic> = −0.10, <italic>t</italic> = −2.22, p=0.03; <xref ref-type="fig" rid="fig4">Figure 4C</xref>). The unsigned cue RPE’s increase of, in particular, memory for cue images suggests an additional mechanism for the learning effects described above.</p></sec><sec id="s2-4-3"><title>Outcomes and values did not predict memory without reward prediction error</title><p>To rule out alternative modulators of memory, we also tested the effect of the trial-by-trial reward outcomes and value estimates. We did not find reward outcome, in of itself or in an interaction with cue or outcome event, to predict memory in any of the experiments (memory as a function of reward outcome, Experiment 1: <italic>β</italic> = −0.03, <italic>t</italic> = −0.68, p=0.50, interaction between event type and reward outcome: <italic>β</italic> = 0.07, <italic>t</italic> = 1.07, p=0.29; Experiment 2 - instructed memory: <italic>β</italic> = 0.03, <italic>t</italic> = 0.91, p=0.36, interaction between event type and reward outcome: <italic>β</italic> = 0.02, <italic>t</italic> = 0.55, p=0.58; Experiment 2 - incidental memory: <italic>β</italic> = 0.05, <italic>t</italic> = 1.51, p=0.13, interaction between event type and reward outcome: <italic>β</italic> = −0.04, <italic>t</italic> = −0.98, p=0.33).</p><p>When testing the effect of participant value estimates on memory, we similarly did not find participant value estimates, in of themselves or in an interaction with cue or outcome event, to predict memory in Experiment 1 (memory as a function of value estimate, <italic>β</italic> = −0.007, <italic>t</italic> = −0.15, p=0.88, interaction between event type and value estimate: <italic>β</italic> = −0.004, <italic>t</italic> = −0.06, p=0.95). In Experiment 2, the value estimates were strongly correlated with the signed RPE at cue (<italic>r</italic> &gt; 0.80), therefore we could not enter them into a single model. When tested alone, value estimates were trending in enhancing memory in the instructed memory version of Experiment 2 (memory as a function of value estimate, <italic>β</italic> = 0.06, <italic>t</italic> = 1.89, p=0.06, interaction between event type and value estimate: <italic>β</italic> = 0.05, <italic>t</italic> = 1.19, p=0.24), and enhanced memory in the incidental memory version (memory as a function of value estimate, <italic>β</italic> = 0.08, <italic>t</italic> = 2.58, p=0.01, interaction between event type and value estimate: <italic>β</italic> = −0.03, <italic>t</italic> = −0.62, p=0.53). However, given that value estimates did not influence memory when there was no prediction error at cue (Experiment 1), we believe the parsimonious interpretation is that this mnemonic modulation is attributable to prediction errors rather than value estimates alone.</p></sec></sec><sec id="s2-5"><title>Choice results</title><p>At the end of both experiments, we asked participants to make choices between previously seen items. Choices were between items that were either (1) two cue or two outcome items from different trials (Experiment 2 only), or (2) a cue and an outcome item from a single trial, associated with a single value estimate and reward outcome (both experiments). No outcomes were presented after choices were made.</p><p>Recall that in Experiment 1 and the instructed memory version of Experiment 2, participants were told in advance to pay attention to the images and their outcomes as they would later have a chance to make choices between them and win their associated reward again. These instructions encouraged participants to encode the images along with their reward value. In the incidental version of Experiment 2, however, no such preview was provided, and participants were not given any incentive to encode the images nor their associated values or rewards.</p><sec id="s2-5-1"><title>Outcomes and values increased choice</title><p>Choices between pairs of images associated with different reward outcomes and values were tested in Experiment 2. As expected, in the instructed memory version, participants preferred both cues and outcomes associated with higher rewards (mixed-effects logistic regression, choice as a function of reward difference: <italic>β</italic> = 1.59, <italic>z</italic> = 20.13, p&lt;0.001; <xref ref-type="fig" rid="fig5">Figure 5A</xref>), and preferred images for which they had reported higher subjective value when controlling for the effect of reward outcome on choice (choice as a function of value difference: <italic>β</italic> = 0.92, <italic>z</italic> = 7.58, p&lt;0.001; <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Interestingly, both effects were replicated in the incidental memory version of the task, despite not motivating participants to encode nor associate images with their values and reward outcomes (choice as a function of reward difference: <italic>β</italic> = 1.21, <italic>z</italic> = 12.28, p&lt;0.001; <xref ref-type="fig" rid="fig5">Figure 5C</xref>; choice as a function of value difference, controlling for reward difference: <italic>β</italic> = 1.03, <italic>z</italic> = 7.84, p&lt;0.001; <xref ref-type="fig" rid="fig5">Figure 5D</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Choice probability as a function of rewards and values in Experiment 2.</title><p>(<bold>A,C</bold>) Choice probability as a function of the difference in reward outcomes between two cue or two outcome items in Experiment 2. Participants were more likely to choose cue and outcome items that had been associated with higher reward outcomes in both the instructed (<bold>A</bold>) and incidental memory (<bold>C</bold>) versions of the task. (<bold>B,D</bold>) Choice probability as a function of the difference in value between two cue or two outcome items. Participants were more likely to choose cue and outcome items that they had associated with a more valuable scene category (relative to the other scene category) at the time of encoding in both the instructed (<bold>B</bold>) and incidental memory (<bold>D</bold>) versions of the task. Size of circles reflects the size of that sample. Choice was fit using a logistic function, and shaded regions reflect 95% confidence intervals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig5-v2.tif"/></fig></sec><sec id="s2-5-2"><title>Signed RPEs at outcome biased choice</title><p>The above results confirmed that participants associated both the cue and outcome event with their value of that category, as well as with the specific reward outcome on that trial, even when they were not instructed to do so. We also asked participants to choose between cue and outcome items from the same trial – two items that had the same associated value and reward outcome. In all experiments, we found that participants were more likely to prefer the outcome event the more positive the outcome RPE, and to prefer the cue event the more negative the outcome RPE (mixed-effects logistic regression, choice for outcome event as a function of signed outcome RPE, Experiment 1: <italic>β</italic> = 0.27, <italic>z</italic> = 3.27, p=0.001, <xref ref-type="fig" rid="fig6">Figure 6A</xref>; Experiment 2 - instructed memory: <italic>β</italic> = 0.23, <italic>z</italic> = 5.70, p&lt;0.001, <xref ref-type="fig" rid="fig6">Figure 6B</xref>; Experiment 2 - incidental memory: <italic>β</italic> = 0.16, <italic>z</italic> = 4.30, p&lt;0.001, <xref ref-type="fig" rid="fig6">Figure 6C</xref>). Moreover, each one of these effects held when controlling for the magnitude of the outcome RPE, that is the unsigned outcome RPE (which boosted memory for outcome images, see ‘Unsigned but not signed reward prediction errors at outcome enhanced memory’, above), thus suggesting this preference may not be driven by better memory for large RPE events. Therefore, although signed outcome RPEs did not modulate memory, they did predict subsequent choice, pointing to a hedonic component of the signed RPE in shaping preference.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Choice between cue and outcome items from a single trial in Experiment 1 (<bold>A</bold>), the instructed memory version of Experiment 2 (<bold>B</bold>), and the incidental memory version of Experiment 2 (<bold>C</bold>).</title><p>As these items were associated with the same reward outcome and value, we would not expect preference for either item. In all cases, participants preferred the outcome item on trials with a more positive outcome RPE and the cue item on trials with a more negative outcome RPE. Size of circles reflects the size of that sample. Choice was fit using a logistic function, and shaded regions reflect 95% confidence intervals.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-fig6-v2.tif"/></fig></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We found that distinct reward prediction error (RPE) signals, one occurring at cue and one at outcome, dynamically influenced learning and memory for those events. Drawing on classic associative models of attention (<xref ref-type="bibr" rid="bib44">Pearce and Mackintosh, 2010</xref>), we found that an unsigned RPE at reward outcome modulated trial-specific learning rate, consistent with a Pearce-Hall model of learning that assumes more attention – and therefore more learning – for cues associated with unpredictable outcomes (<xref ref-type="bibr" rid="bib43">Pearce and Hall, 1980</xref>). Similarly, a signed RPE at reward cue also modulated learning rate, consistent with a <xref ref-type="bibr" rid="bib32">Mackintosh, 1975</xref> model of learning that assumes enhanced attention to reward-predicting cues. Reinforcement learning models that included both modulatory components predicted behavior better than models without those attentional components.</p><p>RPE signals at cue and outcome also enhanced memory for associated events. In Experiment 1, participants learned the value of a single reward category while experiencing large unsigned RPEs at outcome due to high (versus low) levels of outcome variance and unexpected changes in the mean of the underlying reward distribution (‘change points’). We found that unsigned RPEs at reward outcome improved memory for trial-unique scenes accompanying both the cue and outcome, most prominently the latter.</p><p>In Experiment 2, participants learned the value of two reward categories (designated by indoor and outdoor trial-unique scenes), which meant that they experienced RPEs both at the time of the cue (as they could not predict which category would be offered on any given trial) and at the time of the reward outcome. Unlike Experiment 1, where memory for the cue event remained relatively stable throughout the task, in Experiment 2, memory for the cue event (but not for the outcome event) increased with learning. This mnemonic increase for cue events was supported by the gradual buildup of a signed RPE at cue, which enhanced memory for more valued reward cues, as well as an unsigned RPE at cue that benefited memory the more participants had separated the values of the two reward categories (i.e. the more they had learned). Furthermore, we found that individual learning performance predicted the increase in cue memory over learning, providing another link between stronger reward expectations and enhanced memory for cue events.</p><p>Similarly to Experiment 1, we again found that unsigned RPEs at reward outcome boosted memory for outcome events in Experiment 2, although we did not find them to influence memory for cue events. Importantly, our results were similar in both versions of Experiment 2, one that instructed memory for the images and their values and one where memory was completely incidental, confirming that the mnemonic benefits of RPEs are not a result of strategic encoding. Thus, our findings can be directly related to previous work characterizing implicit, incidental encoding during reward learning, as well as studies of animal conditioning in which encoding is not explicitly instructed (<xref ref-type="bibr" rid="bib13">Duszkiewicz et al., 2019</xref>).</p><p>Last, in a choice test administered at the end of the experiment, participants preferred both cue and outcome scenes that had been associated with higher reward outcomes and more valued scene categories. This result was obtained when participants had been explicitly instructed to associate the events with their reward outcomes and values (instructed memory task) and when these associations were not instructed (incidental memory task). Additionally, when choosing between the cue and outcome scenes of a single trial (i.e. two scenes associated with the same reward and value), higher signed RPEs at outcome, which we did not find to modulate memory, nevertheless led to ‘irrational’ preference for the outcome event.</p><sec id="s3-1"><title>Reward prediction errors dynamically modulated learning rate</title><p>We compared different reinforcement-learning models that included the contribution of attentional components in modulating learning rate on a trial-by-trial basis. We focused on attention’s effect in enhancing overall learning rate, departing from classic paradigms that investigate the allocation of attention (or learning resources) between competing stimuli presented simultaneously. That is, we presented one stimulus at a time. Nevertheless, our experimental design allowed us to model and test the amount of learning for each stimulus on each trial, and investigate its relationship to RPEs. Empirically, we found that large unsigned RPEs boosted learning rate, in line with a Pearce-Hall model of attention for learning (<xref ref-type="bibr" rid="bib43">Pearce and Hall, 1980</xref>), and previous work (<xref ref-type="bibr" rid="bib48">Rouhani et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Nassar et al., 2010</xref>). Model comparison suggested that modulating learning rate according to unsigned outcome RPEs fit learning behavior better than models without this modulation.</p><p>We also tested the influence of a Mackintosh-like attention component, which contrary to the Pearce-Hall model, predicts a change in attention for more valuable and predictive cues. We modeled the Mackintosh signal as the (signed) difference in learned value between the reward-predicting cues. We found that this signed cue RPE decreased empirical learning rates, meaning that values estimated for higher valued cues were more stable in light of unpredicted outcomes than for lower valued cues. This effect was not due to an overall decrease in learning rate after having learned the values of the two categories, as we did not find a significant effect of unsigned cue RPE on learning rate; instead, participants demonstrated more stable learning for higher-valued cues. A computational model that updated learning rate according to this signed RPE at cue, in addition to the unsigned RPE at outcome, provided the best fit to behavior, of all the models we tested.</p><p>In order to observe the effects of a building-up of RPE at cue onset, Experiment 2, unlike Experiment 1, did not include shifts in the underlying reward distribution of the categories. Future studies could examine how this Mackintosh signal changes with shifting predictions, where a change in the underlying mean of the rewards increases learning rate (<xref ref-type="bibr" rid="bib59">Vaghi et al., 2017</xref>).</p></sec><sec id="s3-2"><title>Reward prediction error at cue benefited memory for cue events</title><p>Experiment 2, which included RPEs at cue, allowed us to test whether a putative (signed) dopaminergic RPE, which moves from reward outcome to the cue predicting reward over learning (<xref ref-type="bibr" rid="bib3">Barto, 1995</xref>; <xref ref-type="bibr" rid="bib35">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib53">Schultz et al., 1997</xref>), enhances memory for cue events. As predicted, we found an incremental increase in memory for cue events (and not outcome events) throughout learning. Moreover, we found this increase to be supported by a signed RPE at cue that boosted memory for cue events. That is, as learning progressed, cues that were more valuable (and therefore elicited a larger signed RPE at cue) were better remembered. This finding is consistent with previous work showing better memory for cues associated with future higher rewards (<xref ref-type="bibr" rid="bib56">Stanek et al., 2019</xref>; <xref ref-type="bibr" rid="bib26">Jang et al., 2019</xref>). As in our prior work (<xref ref-type="bibr" rid="bib48">Rouhani et al., 2018</xref>), we did not find signed RPEs at reward outcome to modulate memory for any event, but we note that such an effect has been demonstrated in paradigms outside of reinforcement learning (<xref ref-type="bibr" rid="bib34">Marvin and Shohamy, 2016</xref>; <xref ref-type="bibr" rid="bib16">Ergo et al., 2019</xref>; <xref ref-type="bibr" rid="bib11">De Loof et al., 2018</xref>; <xref ref-type="bibr" rid="bib17">Ergo et al., 2020</xref>) and for adolescents, but not adults, in reinforcement learning (<xref ref-type="bibr" rid="bib10">Davidow et al., 2016</xref>).</p><p>Along with the effect of signed cue RPE, the increase in cue memory over learning was additionally and separately supported by an unsigned RPE at cue, which improved memory for both high-value cues and low-value cues as participants learned to separate the values of the two reward categories. Although we consider the signed RPE at cue to reflect a Mackintosh-type attention signal (<xref ref-type="bibr" rid="bib32">Mackintosh, 1975</xref>), in our paradigm, larger unsigned RPEs at cue also demonstrated that greater reward predictiveness strengthens encoding of those cue events.</p><p>Our findings further shed light on whether this mnemonic enhancement for cue events was due to cue values or cue RPE. We could not distinguish the effects of participant value estimates versus cue RPEs in Experiment 2, as the measures were highly correlated. However, in Experiment 1, where there were no RPEs experienced at cue, we did not find value estimates to modulate memory. We therefore experimentally dissociated the effects of cue values and RPEs on memory by showing that an increase in cue memory over learning requires the build-up of an RPE signal, which in Experiment 2 was elicited by learning about more than one reward category.</p><p>We also investigated whether individual differences in learning performance were related to this increase in cue memory over learning. More specifically, we tested whether value separation of the two reward categories at the end of learning predicted individual changes in cue memory as a function of trial number during learning (the estimated slope of the effect). We indeed found learning performance to predict greater increases in cue memory, but not outcome memory, over learning. This relationship was strongest in the experimental condition associated with the worst learning performance (40¢−60¢, incidental memory condition). This was the only condition in which we did not find an overall increase in cue memory throughout learning, suggesting that this null result may be in part driven by worse learning in this condition, while better learners were still showing the effect. This analysis complemented our within-subjects approach by providing a between-subjects link between learning performance and increasing memory for cue events.</p></sec><sec id="s3-3"><title>Unsigned reward prediction error at outcome boosted memory throughout learning</title><p>We replicated previous results showing better memory for images associated with high unsigned RPEs at reward outcome, due to either outcome variance (<xref ref-type="bibr" rid="bib48">Rouhani et al., 2018</xref>) or reward change-points (<xref ref-type="bibr" rid="bib49">Rouhani et al., 2020</xref>). We found this mnemonic benefit of outcome RPEs to be particularly strong for events experienced at the time of the (surprising) outcome. Although unsigned outcome RPEs enhanced memory for cue events as well (see Experiment 1), this effect was weaker than that for outcome events, and did not replicate in Experiment 2. Critically, unsigned outcome RPEs benefited memory in both instructed and incidental memory conditions. We therefore hypothesize that, regardless of any explicit encoding strategy, increased attention due to large unsigned RPEs during reward outcome engages the locus coeruleus (LC), which co-releases norepinephrine and dopamine to modulate hippocampal plasticity (<xref ref-type="bibr" rid="bib28">Kempadoo et al., 2016</xref>; <xref ref-type="bibr" rid="bib58">Takeuchi et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Wagatsuma et al., 2018</xref>). Although we only tested the effects of reward prediction errors, we speculate that any large prediction-error event would engage this putative neural mechanism to modulate memory, as supported by work showing mnemonic enhancement for large prediction-error events during fear conditioning (<xref ref-type="bibr" rid="bib27">Kalbe and Schwabe, 2019</xref>).</p><p>Of note, we did not find effects of reward outcome on memory in any of our experiments, showing that the reward outcome by itself did not lead to the strategic prioritization of more rewarding events in the instructed memory version of our experiments, nor did it lead to the incidental encoding of those events. These results are consistent with reported null effects of reward outcome on immediate memory (<xref ref-type="bibr" rid="bib36">Murty et al., 2016a</xref>). Indeed, reward effects on incidental memory typically emerge when tested after consolidation, a process in which rewarding events, and their associates, are prioritized in memory (<xref ref-type="bibr" rid="bib7">Braun et al., 2018</xref>; <xref ref-type="bibr" rid="bib56">Stanek et al., 2019</xref>; <xref ref-type="bibr" rid="bib42">Patil et al., 2017</xref>). In our paradigm, encoding and recognition memory were separated by only a short delay, leaving open the possibility that reward effects on cue and outcome events would emerge after consolidation.</p></sec><sec id="s3-4"><title>Instructed versus incidental memory effects</title><p>In both Experiment 1 and the instructed memory version of Experiment 2, we incentivized participants to associate trial-unique images with their reward outcome: in the initial instructions, participants were told that they would later have an opportunity to choose between the images and win their reward outcomes again. These instructions may have encouraged participants to strategically encode the images, similar to work investigating explicit memory strategies for remembering rewarding events (e.g. <xref ref-type="bibr" rid="bib24">Hennessee et al., 2019</xref>). Such explicit strategies may use different neural mechanisms than those motivating our questions, namely, the dopaminergic and noradrenergic modulation of hippocampal memories characterized in animal conditioning paradigms.</p><p>We addressed this potentially critical difference by running a version of Experiment 2 where memory for the images, as well as their association with the reward values and outcomes, were completely incidental. Here, we replicated our main results: signed and unsigned RPEs at cue enhanced memory for cue events, and unsigned RPEs at outcome enhanced memory for outcome events. Unlike the instructed memory version of Experiment 2, we did not find the signed RPE at cue to additionally increase memory for outcome events. This difference could potentially reflect a more explicit encoding of the outcome event for the later choice task in the instructed memory version, as participants were told that cue and outcome events from a single trial were associated with the same value and outcome. However, it is worth noting that a good explicit strategy would have been to remember the most rewarding events, yet, reward outcome did not modulate memory in any of our experiments. We therefore conclude that our RPE effects do not rely on explicit memory strategies and are likely to be unintentional, connecting our findings to literature investigating incidental memory during reward learning.</p><p>We also found that learning was, on average, worse in the incidental memory compared to the instructed memory version of Experiment 2. We did not anticipate this difference since the only instructions we changed were related to the final choice task. However, it is possible that motivating participants to associate the images with their reward values and outcomes additionally encouraged them to learn and better separate the average values of the two reward categories. Alternatively, participant pool and motivation may be related to this difference, as we ran the incidental memory task during 2020’s global pandemic. As a potential signature for decreased motivation in the 2020 participant pool, we found participants to provide lower value estimates, indicating greater pessimism when anticipating rewards. Nevertheless, this greater variability in learning performance in the incidental memory task did allow us to demonstrate that individual differences in learning predicted the degree to which cue memory increased over learning (see ‘Reward prediction error at cue benefits memory for cue events’, above).</p></sec><sec id="s3-5"><title>Interactions between reinforcement learning and memory systems</title><p>Although we did not measure neural activity in this study, distinguishing the mnemonic effects of signed and unsigned RPEs in the brain may be fruitful in characterizing two distinct memory mechanisms. As mentioned, one dominant hypothesis is that dopaminergic midbrain signals convey signed RPEs to target areas (<xref ref-type="bibr" rid="bib3">Barto, 1995</xref>; <xref ref-type="bibr" rid="bib35">Montague et al., 1996</xref>; <xref ref-type="bibr" rid="bib53">Schultz et al., 1997</xref>). Less well accepted, but also quite dominant is the idea that unsigned RPEs increase noradrenergic (as well as dopaminergic) firing from the LC (<xref ref-type="bibr" rid="bib58">Takeuchi et al., 2016</xref>; <xref ref-type="bibr" rid="bib28">Kempadoo et al., 2016</xref>; <xref ref-type="bibr" rid="bib61">Wagatsuma et al., 2018</xref>). Recent work makes predictions about how these distinct mechanisms may differentially influence memory (<xref ref-type="bibr" rid="bib23">Hauser et al., 2019</xref>). Midbrain dopamine initiates ‘behavioral activation’ (<xref ref-type="bibr" rid="bib9">Clewett and Murty, 2019</xref>), such as increased vigor during periods of reward anticipation (<xref ref-type="bibr" rid="bib40">Niv et al., 2007</xref>), which is thought to promote the integration of higher-order representations, like value formation, giving rise to semantic memories (<xref ref-type="bibr" rid="bib13">Duszkiewicz et al., 2019</xref>). The LC-norepinephrine system, on the other hand, is thought to promote selectivity for salient events such as (positively or negatively) surprising outcomes, giving rise to distinctive, episodic memories (<xref ref-type="bibr" rid="bib13">Duszkiewicz et al., 2019</xref>). Although our findings suggest that RPEs act on both episodic (high-confidence recognition) and semantic (value formation) memory, distinguishing the effects these RPE signals may have on other features of episodic and semantic memory is an important avenue for future research (<xref ref-type="bibr" rid="bib21">Greve et al., 2019</xref>).</p><p>In this paradigm, we did not dissociate the effects of cue RPE versus reward anticipation on memory (for an experiment that does this, see <xref ref-type="bibr" rid="bib56">Stanek et al., 2019</xref>). However, based on our findings and the above mapping to neural substrates, we predict that phasic signed RPEs at cue would initiate and enhance a sustained (potentially ramping) period of reward anticipation, leading to memory benefits for ensuing events, regardless of their exact timepoint. Iigaya and colleagues recently offered such a computational model whereby RPEs amplify anticipatory value (i.e. the ‘pleasure of savoring’; <xref ref-type="bibr" rid="bib25">Iigaya et al., 2019</xref>). They further suggested a neural circuit whereby the hippocampus – tracking unsigned RPEs at outcome – enhances the functional coupling between the dopaminergic midbrain (encoding the signed RPE at outcome) and the ventromedial prefrontal cortex (encoding anticipatory value) to boost reward anticipation. The authors speculate that the cognitive imagining of future rewards may drive hippocampal orchestration of reward anticipation. It is, however, unclear whether hippocampal activation here reflects greater engagement in retrieval processes (supporting the mental simulation of future rewards) or encoding processes, consistent with previous work showing better memory for events experienced during reward anticipation (<xref ref-type="bibr" rid="bib56">Stanek et al., 2019</xref>; <xref ref-type="bibr" rid="bib38">Murty and Adcock, 2014</xref>; <xref ref-type="bibr" rid="bib66">Wittmann et al., 2005</xref>). Future work should identify the dynamics of hippocampal encoding and retrieval states (<xref ref-type="bibr" rid="bib22">Hasselmo et al., 2002</xref>; <xref ref-type="bibr" rid="bib12">Duncan et al., 2012</xref>; <xref ref-type="bibr" rid="bib6">Bein et al., 2020</xref>) over the period of reward anticipation.</p><p>In our experiments, we found a collaborative interaction between reinforcement learning and episodic memory systems: more rewarding cues and more surprising outcomes were prioritized in memory, thereby promoting adaptive behavior. Nonetheless, in other paradigms, these two systems have been shown to compete for processing resources: compromised feedback-based learning has been associated with enhanced episodic memory, both behaviorally and neurally (<xref ref-type="bibr" rid="bib18">Foerde et al., 2013</xref>; <xref ref-type="bibr" rid="bib64">Wimmer et al., 2014</xref>). In fact, <xref ref-type="bibr" rid="bib64">Wimmer et al., 2014</xref> showed that better memory for reward-predicting cues was associated with weaker striatal RPEs at reward outcome. In our experiments, we did not find such effects of signed RPEs at reward outcome on memory. However, there are several notable differences between our task and that of Wimmer and colleagues: we tested Pavlovian (passive) learning, not instrumental learning (choice); we presented one cue at a time, rather than two competing cues on every trial; and we tested for memory immediately after learning, rather than 24 hr later (that is, our test did not reflect consolidation effects).</p><p>In our Pavlovian paradigm, participants’ actions (i.e. their estimates) were not rewarded, instead, participants were told they would receive a portion of the reward outcome on every trial, regardless of their estimate. This was done on purpose to prevent positive RPEs due to unexpected reward when participants’ predictions were correct (in our task, correct prediction implies an RPE of zero). Indeed, participants were asked to make predictions only to ensure they paid attention to outcomes in this passive-viewing, online task. It would be interesting in future work to investigate which learning conditions (e.g. Pavlovian versus instrumental) engage more collaborative versus competitive interactions between reinforcement learning and episodic memory systems.</p></sec><sec id="s3-6"><title>Positive reward prediction errors biased preference</title><p>At the end of our experiments, we investigated how RPE signals influence subsequent choice. When prompted to make choices between previously experienced scene images, participants chose both cue and outcome events linked to a higher reward outcome as well as higher (relative) value of a scene category, regardless of whether they were explicitly instructed to create these associations prior to learning (instructed memory task) or not (incidental memory task). These findings replicate previous work showing that people choose episodic events associated with higher rewards, both when these associations are formed explicitly (<xref ref-type="bibr" rid="bib37">Murty et al., 2016b</xref>; <xref ref-type="bibr" rid="bib19">Gluth et al., 2015</xref>) and incidentally (<xref ref-type="bibr" rid="bib65">Wimmer and Büchel, 2016</xref>).</p><p>Participants thus associated both the cue and outcome scenes with the value of that scene category as well as with the specific reward outcome on that trial. Interestingly, when asked to choose between cue and outcome scenes from the same trial (where there should be no preference for either item), we found and replicated an effect (in both Experiments 1 and 2) whereby the higher the signed RPE at outcome, the more participants preferred the outcome event. This result further held when controlling for the magnitude of the outcome RPE (i.e. the unsigned outcome RPE that increased memory for outcome images), suggesting this preference was not driven by memory for large RPE events. Therefore, although signed RPEs at outcome did not modulate memory, they did predict subsequent choice, pointing to a hedonic component of the signed RPE in shaping preferences. This finding is consistent with work maintaining that RPEs drive changes in emotional or affective states (<xref ref-type="bibr" rid="bib60">Villano et al., 2020</xref>; <xref ref-type="bibr" rid="bib15">Eldar and Niv, 2015</xref>; <xref ref-type="bibr" rid="bib14">Eldar et al., 2016</xref>; <xref ref-type="bibr" rid="bib51">Rutledge et al., 2014</xref>), and we propose that this putative change in affect biased preference for the associated outcome event.</p></sec><sec id="s3-7"><title>Conclusion</title><p>Taken together, our results suggest that reward prediction errors generated both by reward-predicting cues and by reward outcomes modulate learning rate during reinforcement learning, in line with classic attentional models of learning. These signals further enhanced memory for events associated with larger unsigned prediction errors experienced at outcome (corresponding to general surprise), and larger signed prediction errors experienced at cue (corresponding to higher expected value). These findings highlight the interaction of prediction errors, potentially signaled by midbrain dopamine and locus-coeruleus norepinephrine, with mnemonic processes.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Experimental conditions</title><sec id="s4-1-1"><title>Participants</title><p>We recruited participants from Amazon Mechanical Turk (MTurk): Experiment 1: 100 participants; Experiment 2, instructed memory task: 400 participants (200 for each condition); Experiment 2, incidental memory task: 500 participants (250 for each condition). The sample size was chosen (1) based on a simulation-based power analysis revealing that at least 55 participants would give sufficient power (80% probability) to detect the effect of unsigned RPEs on memory (<xref ref-type="bibr" rid="bib48">Rouhani et al., 2018</xref>), and (2) taking into account that 20% of participants typically meet one of the following exclusion criteria. Participants were excluded if they (1) had a memory score of less than 0.5 (A’: Sensitivity index in signal detection; <xref ref-type="bibr" rid="bib46">Pollack and Norman, 1964</xref>), or (2) missed more than three trials. More participants were recruited in Experiment 2 to test the additional effect of cue RPEs on memory. Furthermore, in the incidental version of Experiment 2, where there was no instruction to motivate remembering of the scenes, memory was worse (as could be expected), and we recruited more participants (50 per condition) to obtain similar power between the instructed and incidental memory versions of the task.</p><p>This led to a final sample of 81 participants in Experiment 1, 331 participants in Experiment 2, instructed memory task (40¢−60¢ condition: 163, 20¢−80¢ condition: 168), and 354 participants in Experiment 2, incidental memory task (40¢−60¢ condition: 168, 20¢−80¢ condition: 186). We obtained informed consent online, and participants had to correctly answer questions checking for their understanding of the instructions before proceeding; procedures were approved by Princeton University’s Institutional Review Board.</p></sec><sec id="s4-1-2"><title>Task design</title><p>Participants each completed (1) a reward-learning task, (2) a recognition-memory task, and (3) a choice task. Before reward learning, participants completed a practice task (12 trials) to ensure they had learned the structure of the reward-learning task using different reward contingencies than what would be learned in the experimental task. In the practice trials of Experiment 1, participants experienced one reward change-point, from a mean of 30¢ to 50¢. In the practice trials of Experiment 2, in all conditions, the low-value scene category was worth 30¢ and the high-value scene category was worth 70¢, on average. Participants were additionally asked to complete a risk questionnaire (DOSPERT; <xref ref-type="bibr" rid="bib62">Weber et al., 2002</xref>) between reward learning and memory to create a 5–10 min delay between item encoding and recognition.</p></sec><sec id="s4-1-3"><title>Memory instructions</title><p>In the initial instructions for both Experiment 1 and the instructed memory version of Experiment 2, participants were told they would be choosing between the trial-unique images later in the experiment for a chance to win the reward associated with those events again. The aim of this choice phase was to assess learning, and informing participants about future choices was aimed at increasing attention of online participants. This instruction explicitly incentivized participants to associate images with their reward outcomes.</p><p>In the incidental memory version of Experiment 2, we tested whether our results would replicate without any incentive to remember the items. Accordingly, no instructions were given to motivate the encoding of the trial-unique images nor their association with the reward outcome on that trial. Therefore, all memory and choice results from this experiment reflect incidental encoding (see Appendix 1 for Experiment 2 instructions).</p><sec id="s4-1-3-1"><title>Experiment 1 learning task</title><p>Participants learned the average value of objects in two different reward contexts, defined by background images of different cities (‘Paris’ and ‘London’). They experienced each reward context in interleaved blocks (8 blocks total). Each block was comprised of 6 or 9 trials (60 trials total), each trial involved two trial-unique objects (120 objects in total) that were randomly assigned to each trial. On each trial, participants were first shown an object (‘reward cue’: 3 s), and then had up to 5 s to estimate the ‘resale value of objects in that city at that time’, that is, the average value of objects in that context. After submitting their answer, they saw a different trial-unique object (‘reward outcome’: 3 s) along with the monetary outcome associated with both objects on that trial. Participants were paid 10% of the rewards they received on every trial regardless of their estimates, in line with a Pavlovian conditioning environment.</p><p>The individual rewards associated with the object pairs fluctuated around a fixed mean (the means ranged from 10¢ to 90¢). Once or twice within each reward block, the underlying mean changed, generating large RPEs. These ‘change points’ occurred once in the six-trial blocks, twice in the nine-trial blocks, and were at least three trials apart. The reward variance associated with each context provided a second source of RPEs. The variance in the high-variance context (σ-high-variance = 7¢) was twice that of the low-variance context (σ-low-variance = 3.5¢), leading participants to experience larger RPEs within the high-variance context. Participants were told that the average resale value of the ‘found’ objects could change within each city, but that the inherent variability in reward outcome associated with each city remained constant. Participants were encouraged to remember the rewards associated with the objects, as they were told they would be choosing between objects, and re-earning their associated rewards, later in the task.</p></sec><sec id="s4-1-3-2"><title>Experiment 2 learning task</title><p>Instead of learning the value of a single category (objects) within two reward contexts (as in Experiment 1), participants learned the value of two categories (indoor and outdoor scenes) within one reward context, thereby eliciting RPEs at cue as well as at outcome. They were told that indoor and outdoor scenes were each associated with an average value that does not change during learning, and were asked to estimate the average value of the scene category presented on every trial. As before, participants saw two different trial-unique images at reward cue and outcome, here the cue and the outcome scenes belonged to the same scene category, and images were randomly selected from each scene category.</p><p>The average value of one of the scene categories was higher than the other, and average values, as well as their variance (same for both scene categories; σ = 15.81), remained constant throughout the experiment. In order to test a range of RPEs experienced at cue, participants learned in a reward environment where either (1) the average means of the two scene categories were close to each other (‘40¢−60¢ condition’: μ-low-reward=40¢, μ-high-reward=60¢), or (2) further apart (‘20¢−80¢ condition’: μ-low-reward=20¢, μ-high-reward=80¢). The outcomes were drawn from a predefined range centered at the above means, with the same variance between conditions (‘40¢−60¢ condition’: high-value scene category = 40¢−80¢, low-value scene category = 20¢−60¢; ‘20¢−80¢ condition’: high-value scene category = 60¢−100¢, low-value scene category = 0¢−40¢), and spanned that range uniformly.</p><p>Participants completed 30 trials during learning (15 trials for each scene category; 60 trial-unique scenes). The sequence of scene-value categories (high or low scene-value categories) shown to the participant was pseudo-randomized: participants were assigned to one of eight possible sequences ensuring that no scene category was repeated consecutively more than twice, and controlling (across participants) for the number of high- and low-value scene category trials assigned to each trial number. In other words, across participants, there was a similar amount of data for both value categories on each trial.</p></sec><sec id="s4-1-3-3"><title>Learning measures</title><p>We calculated an empirical trial-by-trial outcome RPE by subtracting participants’ value estimates from the reward outcome on that trial. In Experiment 2, we further calculated a cue RPE by subtracting participant’s value estimates of the present reward category from the other reward category. The ‘unsigned’ outcome and cue RPEs were the absolute values of these measures.</p><p>We also calculated an empirical trial-by-trial learning rate directly from the Rescorla-Wagner update equation (<xref ref-type="bibr" rid="bib47">Rescorla and Wagner, 1972</xref>):<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We tested whether signed cue and unsigned outcome RPEs modulated this empirical learning rate.</p></sec><sec id="s4-1-3-4"><title>Recognition memory</title><p>After completing the risk questionnaire, participants were tested for their memory of the trial-unique images. They were presented with these images and asked to indicate whether they were ‘old’ (previously seen during learning) or ‘new’ (not seen during learning) as well as their confidence level for each memory judgment (from 1 ‘guessing’ to 4 ‘completely certain’). In Experiment 1, the test included 72 trials: 48 old (24 from each context) and 24 new images. In Experiment 2, the memory test included 64 trials (32 old and 32 new images). We did not test memory for every image seen during learning in order to limit fatigue and dwindling attention in online participants. However, across participants, we tested memory for the events of every learning trial by pseudo-randomizing which learning trials were probed during memory. Each participant was randomly assigned to one of four possible lists specifying which learning trials would be selected for memory testing. This ensured that events from each learning trial were probed a similar number of times in the memory test, across participants. Trial-by-trial memory scores were calculated by combining memory performance (hit versus miss) with confidence rating (from 1 = ‘guessing’ to 4 = ‘completely certain’) on old items; the score thus ranged from a ‘completely certain’ miss (1) to a ‘completely certain’ hit (8).</p></sec><sec id="s4-1-3-5"><title>Choice task</title><p>In the final phase, participants were asked to choose the more valuable image between two previously seen images (14 trials). Unbeknownst to the participants, images within each pair were either (1) both cue or outcome events from different reward pairs (in Experiment 1, these events were close in their associated reward but belonged to different variance contexts, and in Experiment 2, the events were associated with different reward outcomes; six trials), or (2) belonged to the same pair and were therefore associated with the exact same value estimate and reward (eight trials; any consistent biases in preference could not be attributable to explicit reward differences in the task).</p></sec></sec></sec><sec id="s4-2"><title>Reinforcement learning models</title><p>We used a simple Rescorla-Wagner model (<xref ref-type="bibr" rid="bib47">Rescorla and Wagner, 1972</xref>) as our baseline model (model: ‘RW’):<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where a static learning rate (α) governs the extent to which the signed RPE at outcome (computed by subtracting the current model value, <inline-formula><mml:math id="inf11"><mml:msub><mml:mi>V</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>, from the reward received on that trial, <inline-formula><mml:math id="inf12"><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>) updates the value of the next trial (<inline-formula><mml:math id="inf13"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>).</p><p>Following attentional models of learning (<xref ref-type="bibr" rid="bib44">Pearce and Mackintosh, 2010</xref>), we investigated whether a dynamic trial-specific learning rate (<inline-formula><mml:math id="inf14"><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>) would better fit learning. We tested three distinct modulators of a trial-by-trial learning rate, separately and in combination with each other. To constrain <inline-formula><mml:math id="inf15"><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> to be in the range of [0–1], for each model, we passed the learning rate through a sigmoid function before updating value (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>).</p><p>First, in line with <xref ref-type="bibr" rid="bib43">Pearce and Hall, 1980</xref>, we used the unsigned (absolute) outcome RPE to modulate learning rate (model: ‘RW-PH’):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Here, the unsigned outcome RPE is calculated as the difference between the reward received and the model value estimate (<inline-formula><mml:math id="inf16"><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>). The learning rate is set as a baseline learning rate, η, plus the unsigned RPE scaled by κ. For positive values of κ, more surprising outcomes therefore lead to higher learning rates, as per the Pearce-Hall model.</p><p>Second, following <xref ref-type="bibr" rid="bib32">Mackintosh, 1975</xref>, we modeled the effect of a cue RPE on learning rate (model: ‘RW-M’). Note that we could only test this effect in Experiment 2 since cue RPEs exist only when there is more than one reward category. The cue RPE is the value of the present reward category (e.g. an indoor scene; <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>) relative to the updated value of the alternative reward category (e.g. an outdoor scene; <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>V</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>). The learning rate in this model is then the scaled cue RPE plus a baseline learning rate η:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn><mml:mi>p</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Therefore, for positive γ, the more one scene category is valued over the other, the higher <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> for trials with the more valued scene category and the lower <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> for trials with the less valued scene category. Since each scene category was sampled an equal number of times (without any runs exceeding two trials), we did not scale the cue RPE by the probability of either scene category occurring.</p><p>Third, given that participants should update their values less (i.e. lower their <inline-formula><mml:math id="inf21"><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula>) once they’ve learned the average values of the reward categories, we tested a model with exponential decay of the learning rate over time (<xref ref-type="bibr" rid="bib57">Sutton and Barto, 1998</xref>; model: ‘RW-D’):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>N</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>N</italic> is the initial value, λ is the decay constant, and <italic>t</italic><sub><italic>c</italic></sub> is the trial number for that reward category (i.e. in Experiment 2 where there were two scene categories, trial number was counted separately for each scene category).</p><p>We further tested models that included each combination of the above three learning-rate modulators. Here, we used a single baseline (η) and added each effect in the learning rate for all of the following models: A model that combines the unsigned outcome RPE and signed cue RPE effects on learning rate (model: ‘RW-PH-M’):<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>A model that combines the unsigned outcome RPE and decay effects on learning rate (model: ‘RW-PH-D’):<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mi>κ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mi>N</mml:mi><mml:mspace width="thinmathspace"/><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:msub><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>A model that combines the signed cue RPE and decay effects on learning rate (model: ‘RW-M-D’):<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>N</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>And finally, a model that combines all three effects (model: ‘RW-PH-M-D’):<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>κ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi>γ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mn>4</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>N</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>λ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mn>10</mml:mn><mml:mo>⁢</mml:mo><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><sec id="s4-2-1"><title>Model fitting and comparison</title><p>All models were fit to each participant’s value estimates by finding parameters that maximize the log likelihood of the participant value estimates. The likelihood was calculated assuming a Gaussian distribution around the model value, with variance equal to the average empirical difference between model values and participant estimates (<inline-formula><mml:math id="inf22"><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:math></inline-formula>). This is equivalent to linear regression of the value estimates on the model values, giving a log likelihood:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mo>⁢</mml:mo><mml:mi>L</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msqrt><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:msqrt><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mn>0.5</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula>where <italic>n</italic> is the number of trials fit. To maximize log likelihood we used MATLAB’s <italic>fmincon</italic> function. We constrained parameter values within the following ranges: <inline-formula><mml:math id="inf23"><mml:mrow><mml:mi>α</mml:mi><mml:mo>∈</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [0,1], <inline-formula><mml:math id="inf24"><mml:mrow><mml:mi>η</mml:mi><mml:mo>∈</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [−10,10], <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi>κ</mml:mi><mml:mo>∈</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [−20,20], <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [−20,20], <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>N</mml:mi><mml:mo>∈</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [−15,15], <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>∈</mml:mo><mml:mi/></mml:mrow></mml:math></inline-formula> [0,10]. Note, however, that the trial-by-trial learning rate was always passed through a sigmoid function (x<sub>t</sub> = input), and was therefore between 0 and 1:<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>Values were initialized to 50¢, and in Experiment 1, were re-initialized at the beginning of each reward context. Each fit was run 30 times with different random initial parameter values.</p><p>Since all our models were nested (with additional parameters further modulating the RW-learning rate), we compared models using the likelihood-ratio test, across subjects (<xref ref-type="bibr" rid="bib45">Pickles, 1985</xref>). To verify that our data can arbitrate between these models, we performed model recovery on simulated data generated by randomly sampling 100 parameter settings from Experiment 2 (including sampling the Gaussian noise translating model value to predicted value). From these simulated data we calculated empirical trial-by-trial learning rates (as in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>). We then tested whether the model generating said learning rates was the best fit for them, by fitting all models to each dataset. We concentrated specifically on modeling learning rates, since the only differences between the models were in how they determined trial-by-trial learning rates. We then compared model recovery using the conservative Bayesian information criterion (BIC; <xref ref-type="bibr" rid="bib54">Schwarz, 1978</xref>), to calculate a confusion matrix demonstrating the proportion of simulations fit best by the true model (<xref ref-type="bibr" rid="bib63">Wilson and Collins, 2019</xref>). The models were sufficiently recovered, validating model comparison (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2B</xref>). Code for model fitting and recovery in ‘models_RL_matlabCode’ at <ext-link ext-link-type="uri" xlink:href="https://github.com/ninarouhani/2021_RouhaniNiv">https://github.com/ninarouhani/2021_RouhaniNiv</ext-link> (<xref ref-type="bibr" rid="bib50">Rouhani, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0d62b7ab882d819b4a903da0b3de1cf4ed4006ed;origin=https://github.com/ninarouhani/2021_RouhaniNiv;visit=swh:1:snp:b0db51330de567674fc9ef3b7648894afc211b65;anchor=swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40/">swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40</ext-link>).</p></sec></sec><sec id="s4-3"><title>Mixed-effects modeling</title><p>We used mixed-effects modeling to test hypotheses throughout the paper (lme4 package in R; <xref ref-type="bibr" rid="bib4">Bates et al., 2015</xref>). We treated participant as a random effect for both the slope and the intercept of each fixed effect; however, if the model did not converge, we incrementally simplified the random effect structure (i.e. by taking out interactions, then the slope of each effect), until convergence was achieved (the simplest structure only modeled participant intercept as a random effect; for model specifications, see ‘analysis&amp;figures.ipynb’ at <ext-link ext-link-type="uri" xlink:href="https://github.com/ninarouhani/2021_RouhaniNiv/">https://github.com/ninarouhani/2021_RouhaniNiv/</ext-link>).</p></sec><sec id="s4-4"><title>Hierarchical model of memory</title><p>We ran a hierarchical regression model to better characterize the effects of unsigned and signed RPES, as well as their relative influence, on memory for cue and outcome events. This model performed full Bayesian inference over the effects of interest with Hamiltonian Monte Carlo sampling, simultaneously estimating subject and group-level posterior distributions (Stan; <xref ref-type="bibr" rid="bib8">Carpenter et al., 2017</xref>). We included all putative RPE signals of interest in predicting memory score: signed RPE signal at outcome, unsigned RPE signal at outcome, as well as an intercept and a nuisance variable that captured overall differences in memory for cue versus outcome events. We also included signed and unsigned RPE signals at cue for Experiment 2. Subject-level parameter distributions were drawn from group-level, standard normal distributions, and scaled by a gamma distribution (1,0.5). The response variable (memory score) was modeled with a normal distribution and fit with a single Gaussian noise parameter across all participants. All RPE regressors were centered and standardized. We report the median (M) of the posterior parameter distributions as a measure of centrality, and the highest density interval (HDI) as a measure of uncertainty around the parameter estimate; by default, HDI returns the 89% credible interval (which is recommended as a more stable interval for sample sizes less than 10,000; <xref ref-type="bibr" rid="bib29">Kruschke, 2014</xref>; <xref ref-type="bibr" rid="bib33">Makowski et al., 2019</xref>). Code for Stan models in ‘models_memory_stanCode’ at <ext-link ext-link-type="uri" xlink:href="https://github.com/ninarouhani/2021_RouhaniNiv/">https://github.com/ninarouhani/2021_RouhaniNiv/</ext-link>.</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>We thank Angela Radulescu and Isabel Berwian for helpful comments. This work was supported by grant W911NF-14-1-0101 from the Army Research Office (YN), grant R01MH098861 from the National Institute for Mental Health (YN), grant R21MH120798 from the National Institute of Health (YN) and the National Science Foundation’s Graduate Research Fellowship Program (NR).</p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Methodology, Writing - original draft, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: We obtained informed consent online; procedures were approved by Princeton University's Institutional Review Board (IRB #4452).</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-61077-transrepform-v2.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>All data files and code for models, analysis and figures are publicly available at <ext-link ext-link-type="uri" xlink:href="https://github.com/ninarouhani/2021_RouhaniNiv">https://github.com/ninarouhani/2021_RouhaniNiv</ext-link> copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40/">https://archive.softwareheritage.org/swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation id="dataset1" publication-type="data" specific-use="isSupplementedBy"><person-group person-group-type="author"><name><surname>Rouhani</surname><given-names>N</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>2021_RouhaniNiv</data-title><source>github</source><pub-id assigning-authority="other" pub-id-type="accession" xlink:href="https://github.com/ninarouhani/2021_RouhaniNiv">2021_RouhaniNiv</pub-id></element-citation></p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Adcock</surname> <given-names>RA</given-names></name><name><surname>Thangavel</surname> <given-names>A</given-names></name><name><surname>Whitfield-Gabrieli</surname> <given-names>S</given-names></name><name><surname>Knutson</surname> <given-names>B</given-names></name><name><surname>Gabrieli</surname> <given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Reward-motivated learning: mesolimbic activation precedes memory formation</article-title><source>Neuron</source><volume>50</volume><fpage>507</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2006.03.036</pub-id><pub-id pub-id-type="pmid">16675403</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Antony</surname> <given-names>JW</given-names></name><name><surname>Hartshorne</surname> <given-names>TH</given-names></name><name><surname>Pomeroy</surname> <given-names>K</given-names></name><name><surname>Gureckis</surname> <given-names>TM</given-names></name><name><surname>Hasson</surname> <given-names>U</given-names></name><name><surname>McDougle</surname> <given-names>SD</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Behavioral, physiological, and neural signatures of surprise during naturalistic sports viewing</article-title><source>Neuron</source><volume>109</volume><fpage>377</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.10.029</pub-id><pub-id pub-id-type="pmid">33242421</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1995">1995</year><chapter-title>Adaptive critic and the basal ganglia</chapter-title><person-group person-group-type="editor"><name><surname>Houk</surname> <given-names>J. C</given-names></name><name><surname>Davis</surname> <given-names>J. L</given-names></name><name><surname>Beiser</surname> <given-names>D. G</given-names></name></person-group><source>Models of Information Processing in the Basal Ganglia</source><publisher-name>University of Massachusetts Amherst</publisher-name><fpage>1</fpage><lpage>215</lpage></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bates</surname> <given-names>D</given-names></name><name><surname>Mächler</surname> <given-names>M</given-names></name><name><surname>Bolker</surname> <given-names>B</given-names></name><name><surname>Walker</surname> <given-names>S</given-names></name><name><surname>Christensen</surname> <given-names>RHB</given-names></name><name><surname>Singmann</surname> <given-names>H</given-names></name><name><surname>Grothendieck</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Fitting linear Mixed-Effects models using lme4</article-title><source>Journal of Statistical Software</source><volume>67</volume><fpage>1</fpage><lpage>48</lpage><pub-id pub-id-type="doi">10.18637/jss.v067.i01</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beesley</surname> <given-names>T</given-names></name><name><surname>Nguyen</surname> <given-names>KP</given-names></name><name><surname>Pearson</surname> <given-names>D</given-names></name><name><surname>Le Pelley</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Uncertainty and predictiveness determine attention to cues during human associative learning</article-title><source>Quarterly Journal of Experimental Psychology</source><volume>68</volume><fpage>2175</fpage><lpage>2199</lpage><pub-id pub-id-type="doi">10.1080/17470218.2015.1009919</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bein</surname> <given-names>O</given-names></name><name><surname>Duncan</surname> <given-names>K</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Mnemonic prediction errors Bias hippocampal states</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3451</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-17287-1</pub-id><pub-id pub-id-type="pmid">32651370</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braun</surname> <given-names>EK</given-names></name><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Retroactive and graded prioritization of memory by reward</article-title><source>Nature Communications</source><volume>9</volume><elocation-id>4886</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-018-07280-0</pub-id><pub-id pub-id-type="pmid">30459310</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname> <given-names>B</given-names></name><name><surname>Gelman</surname> <given-names>A</given-names></name><name><surname>Hoffman</surname> <given-names>MD</given-names></name><name><surname>Lee</surname> <given-names>D</given-names></name><name><surname>Goodrich</surname> <given-names>B</given-names></name><name><surname>Betancourt</surname> <given-names>M</given-names></name><name><surname>Brubaker</surname> <given-names>M</given-names></name><name><surname>Guo</surname> <given-names>J</given-names></name><name><surname>Li</surname> <given-names>P</given-names></name><name><surname>Riddell</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title><italic>Stan</italic>: a probabilistic programming language</article-title><source>Journal of Statistical Software</source><volume>76</volume><fpage>1</fpage><lpage>32</lpage><pub-id pub-id-type="doi">10.18637/jss.v076.i01</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Clewett</surname> <given-names>D</given-names></name><name><surname>Murty</surname> <given-names>VP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Echoes of emotions past: how neuromodulators determine what we recollect</article-title><source>Eneuro</source><volume>6</volume><elocation-id>ENEURO.0108-18.2019</elocation-id><pub-id pub-id-type="doi">10.1523/ENEURO.0108-18.2019</pub-id><pub-id pub-id-type="pmid">30923742</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Davidow</surname> <given-names>JY</given-names></name><name><surname>Foerde</surname> <given-names>K</given-names></name><name><surname>Galván</surname> <given-names>A</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>An upside to reward sensitivity: the Hippocampus supports enhanced reinforcement learning in adolescence</article-title><source>Neuron</source><volume>92</volume><fpage>93</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.031</pub-id><pub-id pub-id-type="pmid">27710793</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>De Loof</surname> <given-names>E</given-names></name><name><surname>Ergo</surname> <given-names>K</given-names></name><name><surname>Naert</surname> <given-names>L</given-names></name><name><surname>Janssens</surname> <given-names>C</given-names></name><name><surname>Talsma</surname> <given-names>D</given-names></name><name><surname>Van Opstal</surname> <given-names>F</given-names></name><name><surname>Verguts</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Signed reward prediction errors drive declarative learning</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0189212</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0189212</pub-id><pub-id pub-id-type="pmid">29293493</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname> <given-names>K</given-names></name><name><surname>Sadanand</surname> <given-names>A</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Memory's penumbra: episodic memory decisions induce lingering mnemonic biases</article-title><source>Science</source><volume>337</volume><fpage>485</fpage><lpage>487</lpage><pub-id pub-id-type="doi">10.1126/science.1221936</pub-id><pub-id pub-id-type="pmid">22837528</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duszkiewicz</surname> <given-names>AJ</given-names></name><name><surname>McNamara</surname> <given-names>CG</given-names></name><name><surname>Takeuchi</surname> <given-names>T</given-names></name><name><surname>Genzel</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Novelty and dopaminergic modulation of memory persistence: a tale of two systems</article-title><source>Trends in Neurosciences</source><volume>42</volume><fpage>102</fpage><lpage>114</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2018.10.002</pub-id><pub-id pub-id-type="pmid">30455050</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname> <given-names>E</given-names></name><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mood as representation of momentum</article-title><source>Trends in Cognitive Sciences</source><volume>20</volume><fpage>15</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.07.010</pub-id><pub-id pub-id-type="pmid">26545853</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eldar</surname> <given-names>E</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Interaction between emotional state and learning underlies mood instability</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>ncomms7149</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7149</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ergo</surname> <given-names>K</given-names></name><name><surname>De Loof</surname> <given-names>E</given-names></name><name><surname>Janssens</surname> <given-names>C</given-names></name><name><surname>Verguts</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Oscillatory signatures of reward prediction errors in declarative learning</article-title><source>NeuroImage</source><volume>186</volume><fpage>137</fpage><lpage>145</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2018.10.083</pub-id><pub-id pub-id-type="pmid">30391561</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ergo</surname> <given-names>K</given-names></name><name><surname>De Loof</surname> <given-names>E</given-names></name><name><surname>Verguts</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reward prediction error and declarative memory</article-title><source>Trends in Cognitive Sciences</source><volume>24</volume><fpage>388</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2020.02.009</pub-id><pub-id pub-id-type="pmid">32298624</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Foerde</surname> <given-names>K</given-names></name><name><surname>Braun</surname> <given-names>EK</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A trade-off between feedback-based learning and episodic memory for feedback events: evidence from Parkinson's disease</article-title><source>Neurodegenerative Diseases</source><volume>11</volume><fpage>93</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1159/000342000</pub-id><pub-id pub-id-type="pmid">23036965</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gluth</surname> <given-names>S</given-names></name><name><surname>Sommer</surname> <given-names>T</given-names></name><name><surname>Rieskamp</surname> <given-names>J</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Effective connectivity between Hippocampus and ventromedial prefrontal cortex controls preferential choices from memory</article-title><source>Neuron</source><volume>86</volume><fpage>1078</fpage><lpage>1090</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.04.023</pub-id><pub-id pub-id-type="pmid">25996135</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname> <given-names>A</given-names></name><name><surname>Cooper</surname> <given-names>E</given-names></name><name><surname>Kaula</surname> <given-names>A</given-names></name><name><surname>Anderson</surname> <given-names>MC</given-names></name><name><surname>Henson</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Does prediction error drive one-shot declarative learning?</article-title><source>Journal of Memory and Language</source><volume>94</volume><fpage>149</fpage><lpage>165</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2016.11.001</pub-id><pub-id pub-id-type="pmid">28579691</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname> <given-names>A</given-names></name><name><surname>Cooper</surname> <given-names>E</given-names></name><name><surname>Tibon</surname> <given-names>R</given-names></name><name><surname>Henson</surname> <given-names>RN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Knowledge is power: prior knowledge aids memory for both congruent and incongruent events, but in different ways</article-title><source>Journal of Experimental Psychology: General</source><volume>148</volume><fpage>325</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1037/xge0000498</pub-id><pub-id pub-id-type="pmid">30394766</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hasselmo</surname> <given-names>ME</given-names></name><name><surname>Bodelón</surname> <given-names>C</given-names></name><name><surname>Wyble</surname> <given-names>BP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A proposed function for hippocampal theta rhythm: separate phases of encoding and retrieval enhance reversal of prior learning</article-title><source>Neural Computation</source><volume>14</volume><fpage>793</fpage><lpage>817</lpage><pub-id pub-id-type="doi">10.1162/089976602317318965</pub-id><pub-id pub-id-type="pmid">11936962</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hauser</surname> <given-names>TU</given-names></name><name><surname>Eldar</surname> <given-names>E</given-names></name><name><surname>Purg</surname> <given-names>N</given-names></name><name><surname>Moutoussis</surname> <given-names>M</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Distinct roles of dopamine and noradrenaline in incidental memory</article-title><source>The Journal of Neuroscience</source><volume>39</volume><fpage>7715</fpage><lpage>7721</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0401-19.2019</pub-id><pub-id pub-id-type="pmid">31405924</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennessee</surname> <given-names>JP</given-names></name><name><surname>Patterson</surname> <given-names>TK</given-names></name><name><surname>Castel</surname> <given-names>AD</given-names></name><name><surname>Knowlton</surname> <given-names>BJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Forget me not: encoding processes in value-directed remembering</article-title><source>Journal of Memory and Language</source><volume>106</volume><fpage>29</fpage><lpage>39</lpage><pub-id pub-id-type="doi">10.1016/j.jml.2019.02.001</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Iigaya</surname> <given-names>K</given-names></name><name><surname>Hauser</surname> <given-names>T</given-names></name><name><surname>Kurth-Nelson</surname> <given-names>Z</given-names></name><name><surname>O’Doherty</surname> <given-names>J</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>He value of what’s to come: neural mechanisms coupling prediction error and reward anticipation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/588699</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jang</surname> <given-names>AI</given-names></name><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Dillon</surname> <given-names>DG</given-names></name><name><surname>Frank</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Positive reward prediction errors during decision-making strengthen memory encoding</article-title><source>Nature Human Behaviour</source><volume>3</volume><fpage>719</fpage><lpage>732</lpage><pub-id pub-id-type="doi">10.1038/s41562-019-0597-3</pub-id><pub-id pub-id-type="pmid">31061490</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kalbe</surname> <given-names>F</given-names></name><name><surname>Schwabe</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Beyond arousal: prediction error related to aversive events promotes episodic memory formation</article-title><source>Journal of Experimental Psychology</source><volume>46</volume><fpage>234</fpage><lpage>246</lpage><pub-id pub-id-type="doi">10.1037/xlm0000728</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kempadoo</surname> <given-names>KA</given-names></name><name><surname>Mosharov</surname> <given-names>EV</given-names></name><name><surname>Choi</surname> <given-names>SJ</given-names></name><name><surname>Sulzer</surname> <given-names>D</given-names></name><name><surname>Kandel</surname> <given-names>ER</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dopamine release from the locus coeruleus to the dorsal Hippocampus promotes spatial learning and memory</article-title><source>PNAS</source><volume>113</volume><fpage>14835</fpage><lpage>14840</lpage><pub-id pub-id-type="doi">10.1073/pnas.1616515114</pub-id><pub-id pub-id-type="pmid">27930324</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kruschke</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Doing Bayesian data analysis: A tutorial with R, JAGS, and Stan</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Le Pelley</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>The role of associative history in models of associative learning: a selective review and a hybrid model</article-title><source>The Quarterly Journal of Experimental Psychology Section B</source><volume>57</volume><fpage>193</fpage><lpage>243</lpage><pub-id pub-id-type="doi">10.1080/02724990344000141</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname> <given-names>JE</given-names></name><name><surname>Grace</surname> <given-names>AA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The Hippocampal-VTA loop: controlling the entry of information into Long-Term memory</article-title><source>Neuron</source><volume>46</volume><fpage>703</fpage><lpage>713</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.05.002</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackintosh</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="1975">1975</year><article-title>A theory of attention: variations in the associability of stimuli with reinforcement</article-title><source>Psychological Review</source><volume>82</volume><fpage>276</fpage><lpage>298</lpage><pub-id pub-id-type="doi">10.1037/h0076778</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Makowski</surname> <given-names>D</given-names></name><name><surname>Ben-Shachar</surname> <given-names>M</given-names></name><name><surname>Lüdecke</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>bayestestR: describing effects and their uncertainty, existence and significance within the bayesian framework</article-title><source>Journal of Open Source Software</source><volume>4</volume><elocation-id>1541</elocation-id><pub-id pub-id-type="doi">10.21105/joss.01541</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marvin</surname> <given-names>CB</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Curiosity and reward: valence predicts choice and information prediction errors enhance learning</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>266</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1037/xge0000140</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Montague</surname> <given-names>PR</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Sejnowski</surname> <given-names>TJ</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A framework for mesencephalic dopamine systems based on predictive Hebbian learning</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>1936</fpage><lpage>1947</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-05-01936.1996</pub-id><pub-id pub-id-type="pmid">8774460</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murty</surname> <given-names>VP</given-names></name><name><surname>LaBar</surname> <given-names>KS</given-names></name><name><surname>Adcock</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2016">2016a</year><article-title>Distinct medial temporal networks encode surprise during motivation by reward versus punishment</article-title><source>Neurobiology of Learning and Memory</source><volume>134</volume><fpage>55</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.nlm.2016.01.018</pub-id><pub-id pub-id-type="pmid">26854903</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murty</surname> <given-names>VP</given-names></name><name><surname>FeldmanHall</surname> <given-names>O</given-names></name><name><surname>Hunter</surname> <given-names>LE</given-names></name><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2016">2016b</year><article-title>Episodic memories predict adaptive value-based decision-making</article-title><source>Journal of Experimental Psychology: General</source><volume>145</volume><fpage>548</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.1037/xge0000158</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murty</surname> <given-names>VP</given-names></name><name><surname>Adcock</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Enriched encoding: reward motivation organizes cortical networks for hippocampal detection of unexpected events</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>2160</fpage><lpage>2168</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht063</pub-id><pub-id pub-id-type="pmid">23529005</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nassar</surname> <given-names>MR</given-names></name><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Heasly</surname> <given-names>B</given-names></name><name><surname>Gold</surname> <given-names>JI</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>An approximately bayesian delta-rule model explains the dynamics of belief updating in a changing environment</article-title><source>Journal of Neuroscience</source><volume>30</volume><fpage>12366</fpage><lpage>12378</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0822-10.2010</pub-id><pub-id pub-id-type="pmid">20844132</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Joel</surname> <given-names>D</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Tonic dopamine: opportunity costs and the control of response vigor</article-title><source>Psychopharmacology</source><volume>191</volume><fpage>507</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1007/s00213-006-0502-4</pub-id><pub-id pub-id-type="pmid">17031711</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Schoenbaum</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Dialogues on prediction errors</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>265</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.03.006</pub-id><pub-id pub-id-type="pmid">18567531</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patil</surname> <given-names>A</given-names></name><name><surname>Murty</surname> <given-names>VP</given-names></name><name><surname>Dunsmoor</surname> <given-names>JE</given-names></name><name><surname>Phelps</surname> <given-names>EA</given-names></name><name><surname>Davachi</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Reward retroactively enhances memory consolidation for related items</article-title><source>Learning &amp; Memory</source><volume>24</volume><fpage>65</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1101/lm.042978.116</pub-id><pub-id pub-id-type="pmid">27980078</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>JM</given-names></name><name><surname>Hall</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>A model for pavlovian learning: variations in the effectiveness of conditioned but not of unconditioned stimuli</article-title><source>Psychological Review</source><volume>87</volume><fpage>532</fpage><lpage>552</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.87.6.532</pub-id><pub-id pub-id-type="pmid">7443916</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Pearce</surname> <given-names>JM</given-names></name><name><surname>Mackintosh</surname> <given-names>NJ</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Attention and associative learning: from brain to behaviour</article-title><conf-name>Two Theories of Attention: A Review and a Possible Integration:</conf-name><fpage>11</fpage><lpage>39</lpage></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pickles</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1985">1985</year><source>An Introduction to Likelihood Analysis</source><publisher-name>Geo Books</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pollack</surname> <given-names>I</given-names></name><name><surname>Norman</surname> <given-names>DA</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>A non-parametric analysis of recognition experiments</article-title><source>Psychonomic Science</source><volume>1</volume><fpage>125</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.3758/BF03342823</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Rescorla</surname> <given-names>R</given-names></name><name><surname>Wagner</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="1972">1972</year><chapter-title>A theory of Pavlovian conditoining: Variations in the effectiveness of reinforcement and nonreinforcement</chapter-title><person-group person-group-type="editor"><name><surname>Black</surname> <given-names>A. H</given-names></name><name><surname>Prokasy</surname> <given-names>W. F</given-names></name></person-group><source>Classical Conditioning II: Current Research and Theory</source><publisher-name>Appleton-Century-Crofts</publisher-name><fpage>64</fpage><lpage>99</lpage></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouhani</surname> <given-names>N</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Dissociable effects of surprising rewards on learning and memory</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>44</volume><fpage>1430</fpage><lpage>1443</lpage><pub-id pub-id-type="doi">10.1037/xlm0000518</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rouhani</surname> <given-names>N</given-names></name><name><surname>Norman</surname> <given-names>KA</given-names></name><name><surname>Niv</surname> <given-names>Y</given-names></name><name><surname>Bornstein</surname> <given-names>AM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reward prediction errors create event boundaries in memory</article-title><source>Cognition</source><volume>203</volume><elocation-id>104269</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2020.104269</pub-id><pub-id pub-id-type="pmid">32563083</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Rouhani</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>2021_RouhaniNiv11</data-title><source>Software Heritage</source><version designator="swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40">swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0d62b7ab882d819b4a903da0b3de1cf4ed4006ed;origin=https://github.com/ninarouhani/2021_RouhaniNiv;visit=swh:1:snp:b0db51330de567674fc9ef3b7648894afc211b65;anchor=swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40/">https://archive.softwareheritage.org/swh:1:dir:0d62b7ab882d819b4a903da0b3de1cf4ed4006ed;origin=https://github.com/ninarouhani/2021_RouhaniNiv;visit=swh:1:snp:b0db51330de567674fc9ef3b7648894afc211b65;anchor=swh:1:rev:fa15d035dc4033ebad03f48dbd5c75b0c4d76c40/</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rutledge</surname> <given-names>RB</given-names></name><name><surname>Skandali</surname> <given-names>N</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Dolan</surname> <given-names>RJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A computational and neural model of momentary subjective well-being</article-title><source>PNAS</source><volume>111</volume><fpage>12252</fpage><lpage>12257</lpage><pub-id pub-id-type="doi">10.1073/pnas.1407535111</pub-id><pub-id pub-id-type="pmid">25092308</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sara</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>The locus coeruleus and noradrenergic modulation of cognition</article-title><source>Nature Reviews Neuroscience</source><volume>10</volume><fpage>211</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1038/nrn2573</pub-id><pub-id pub-id-type="pmid">19190638</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname> <given-names>W</given-names></name><name><surname>Dayan</surname> <given-names>P</given-names></name><name><surname>Montague</surname> <given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schwarz</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Estimating the dimension of a model</article-title><source>The Annals of Statistics</source><volume>6</volume><fpage>461</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1214/aos/1176344136</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shohamy</surname> <given-names>D</given-names></name><name><surname>Adcock</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dopamine and adaptive memory</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>464</fpage><lpage>472</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.08.002</pub-id><pub-id pub-id-type="pmid">20829095</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanek</surname> <given-names>JK</given-names></name><name><surname>Dickerson</surname> <given-names>KC</given-names></name><name><surname>Chiew</surname> <given-names>KS</given-names></name><name><surname>Clement</surname> <given-names>NJ</given-names></name><name><surname>Adcock</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Expected reward value and reward uncertainty have temporally dissociable effects on memory formation</article-title><source>Journal of Cognitive Neuroscience</source><volume>31</volume><fpage>1443</fpage><lpage>1454</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01411</pub-id><pub-id pub-id-type="pmid">30990388</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname> <given-names>RS</given-names></name><name><surname>Barto</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT Press</publisher-name></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Takeuchi</surname> <given-names>T</given-names></name><name><surname>Duszkiewicz</surname> <given-names>AJ</given-names></name><name><surname>Sonneborn</surname> <given-names>A</given-names></name><name><surname>Spooner</surname> <given-names>PA</given-names></name><name><surname>Yamasaki</surname> <given-names>M</given-names></name><name><surname>Watanabe</surname> <given-names>M</given-names></name><name><surname>Smith</surname> <given-names>CC</given-names></name><name><surname>Fernández</surname> <given-names>G</given-names></name><name><surname>Deisseroth</surname> <given-names>K</given-names></name><name><surname>Greene</surname> <given-names>RW</given-names></name><name><surname>Morris</surname> <given-names>RG</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Locus coeruleus and dopaminergic consolidation of everyday memory</article-title><source>Nature</source><volume>537</volume><fpage>357</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1038/nature19325</pub-id><pub-id pub-id-type="pmid">27602521</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaghi</surname> <given-names>MM</given-names></name><name><surname>Luyckx</surname> <given-names>F</given-names></name><name><surname>Sule</surname> <given-names>A</given-names></name><name><surname>Fineberg</surname> <given-names>NA</given-names></name><name><surname>Robbins</surname> <given-names>TW</given-names></name><name><surname>De Martino</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Compulsivity reveals a novel dissociation between action and confidence</article-title><source>Neuron</source><volume>96</volume><fpage>348</fpage><lpage>354</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.09.006</pub-id><pub-id pub-id-type="pmid">28965997</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Villano</surname> <given-names>WJ</given-names></name><name><surname>Otto</surname> <given-names>AR</given-names></name><name><surname>Ezie</surname> <given-names>CEC</given-names></name><name><surname>Gillis</surname> <given-names>R</given-names></name><name><surname>Heller</surname> <given-names>AS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Temporal dynamics of real-world emotion are more strongly linked to prediction error than outcome</article-title><source>Journal of Experimental Psychology: General</source><volume>149</volume><fpage>1755</fpage><lpage>1766</lpage><pub-id pub-id-type="doi">10.1037/xge0000740</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wagatsuma</surname> <given-names>A</given-names></name><name><surname>Okuyama</surname> <given-names>T</given-names></name><name><surname>Sun</surname> <given-names>C</given-names></name><name><surname>Smith</surname> <given-names>LM</given-names></name><name><surname>Abe</surname> <given-names>K</given-names></name><name><surname>Tonegawa</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Locus coeruleus input to hippocampal CA3 drives single-trial learning of a novel context</article-title><source>PNAS</source><volume>115</volume><fpage>E310</fpage><lpage>E316</lpage><pub-id pub-id-type="doi">10.1073/pnas.1714082115</pub-id><pub-id pub-id-type="pmid">29279390</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weber</surname> <given-names>EU</given-names></name><name><surname>Blais</surname> <given-names>AR</given-names></name><name><surname>Betz</surname> <given-names>NE</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A domain-specific risk-attitude scale: measuring risk perceptions and risk behaviors</article-title><source>Journal of Behavioral Decision Making</source><volume>15</volume><fpage>263</fpage><lpage>290</lpage><pub-id pub-id-type="doi">10.1002/bdm.414</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname> <given-names>RC</given-names></name><name><surname>Collins</surname> <given-names>AG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Ten simple rules for the computational modeling of behavioral data</article-title><source>eLife</source><volume>8</volume><elocation-id>e49547</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.49547</pub-id><pub-id pub-id-type="pmid">31769410</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Braun</surname> <given-names>EK</given-names></name><name><surname>Daw</surname> <given-names>ND</given-names></name><name><surname>Shohamy</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Episodic memory encoding interferes with reward learning and decreases striatal prediction errors</article-title><source>Journal of Neuroscience</source><volume>34</volume><fpage>14901</fpage><lpage>14912</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0204-14.2014</pub-id><pub-id pub-id-type="pmid">25378157</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wimmer</surname> <given-names>GE</given-names></name><name><surname>Büchel</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reactivation of Reward-Related patterns from single past episodes supports Memory-Based decision making</article-title><source>Journal of Neuroscience</source><volume>36</volume><fpage>2868</fpage><lpage>2880</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3433-15.2016</pub-id><pub-id pub-id-type="pmid">26961943</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wittmann</surname> <given-names>BC</given-names></name><name><surname>Schott</surname> <given-names>BH</given-names></name><name><surname>Guderian</surname> <given-names>S</given-names></name><name><surname>Frey</surname> <given-names>JU</given-names></name><name><surname>Heinze</surname> <given-names>HJ</given-names></name><name><surname>Düzel</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Reward-related FMRI activation of dopaminergic midbrain is associated with enhanced hippocampus-dependent long-term memory formation</article-title><source>Neuron</source><volume>45</volume><fpage>459</fpage><lpage>467</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2005.01.010</pub-id><pub-id pub-id-type="pmid">15694331</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><boxed-text><sec id="s8" sec-type="appendix"><title>Experiment 2 instructions (initial instructions prior to reward learning/encoding)</title><sec id="s8-1"><title>Differences between instructions for the instructed and incidental memory versions of the task noted below</title><p>Welcome!</p><p>In this experiment, you will be viewing a bunch of photographs (organized in pairs of photos), some of outdoor scenes and some of indoor scenes.</p><p>Each pair of photos has a value between 0 and 1 dollar – you will be presented with the photos, and see (and win) the money value of each.</p><p>One type of photograph - either photos of indoor scenes or photos of outdoor scenes - is worth more money overall making it the winner.</p><p>To learn about the values of indoor and outdoor images, you will get a chance to watch the computer choose pairs of indoor or outdoor photographs.</p><p>For each pair that the computer chooses, you will see one of the photos, and be asked to estimate from 0 to 100 cents, how much you think on average pairs of scenes of its type (indoor or outdoor) are worth. You will have 5 s to enter your estimate.</p><p>You will not know the answer at first, but please make your best guess of what the average value of a pair of scenes of this type is. To submit your answer, press enter.</p><p>After submitting your estimate, the true value of the pair will appear along with the second photo belonging to the chosen pair. Although different pairs of photos have different values, their worth is solely determined by the general value of that type of scene (indoor or outdoor). The average value of each type of scene does not change over this whole task.</p><p>Pay attention to the money reward you get, so you can update your estimate of the average value of outdoor and indoor scenes</p><p>Instructed memory: IMPORTANT: You will be paid a portion of the worth of the photos you see. You will receive approximately 1 cent for every 10 cents rewarded. Additionally, you will be able to use the reward value of each pair to choose between the same photos later in the task, and win their values. The more you win, the more you will be paid at the end of this HIT.</p><p>Incidental memory: IMPORTANT: These outcomes are real. You will be paid a portion of the outcomes of the photos you see. You will receive approximately 1 cent for every 10 cents rewarded.</p><p>The order of the task is as follows: (1) you will first see one of the two photos that the computer chose, and determine whether it's an indoors or outdoors scene, (2) you will give an estimate of how much this type of photo (indoor or outdoor) is worth overall, and after submitting your answer, (3) you will see how much the pair is actually worth along with the second photo belonging to the pair.</p><p>Please pay close attention! You will have 3 s to view the first photo in a pair, 5 s to submit your estimate of its average value and 3 s to see the second photo and the value of the chosen pair.</p><p>After looking through all of the photos, you will then be asked to indicate the ‘winner’. In other words, you will indicate which type of scene (indoor or outdoor) has the higher value on average.</p><p>To receive full payment, you will need to complete this experiment and submit it. In the case of an event that precludes you from completing the experiment, please return the HIT and do not submit it. In that case, please e-mail <ext-link ext-link-type="uri" xlink:href="https://scholar.google.com/citations?user=mH-FwtcAAAAJ&amp;hl=en">nrouhani@princeton.edu</ext-link> to be compensated for the time you did spend on the task.</p><p>Comprehension questions (must be answered correctly before starting the task)</p><p>Before starting the experiment, please answer the following questions and click submit. Once you've answered all the questions correctly, the task will automatically load. if you do not answer correctly, you will see the instructions again.</p><p>Which statement is true about indoor versus outdoor photos?</p><list list-type="alpha-lower"><list-item><p>One type of scene (either indoor or outdoor) is more valuable, on average</p></list-item><list-item><p>Scenes can take on any value, and whether it is indoor or outdoor does not matter</p></list-item><list-item><p>Photos within a pair can belong to different scene types</p></list-item></list><p>Which statement is true about the value of each pair?</p><list list-type="alpha-lower"><list-item><p>The value of each pair dosen't affect your winnings</p></list-item><list-item><p>The value of each pair represents the amount you are winning</p></list-item><list-item><p>The value of each pair can change without warning</p></list-item></list><p>What determines the value of each pair?</p><list list-type="alpha-lower"><list-item><p>The average value of outdoor and indoor scenes (although pair may take on different values from each other)</p></list-item><list-item><p>The quality of the photos in the pair</p></list-item><list-item><p>The attractiveness of the scenes in the pair</p></list-item></list></sec></sec><sec id="s9" sec-type="appendix"><title>Practice instructions</title><p>You will now complete a short practice run before starting the real task.</p><p>Remember:</p><p>1. On each turn, the computer will choose a pair of photos and you will estimate how much, on average, you think the type of scene on the screen (indoor or outdoor) is worth.</p><p>2. After submitting your estimate (by clicking or pressing enter), you will see the second photo and the true value of that pair, and will win a portion of the reward.</p><p>Instructed memory: 3. Note each pair and its value - you will be choosing the same photos later in the task.</p><p>Incidental memory: (this point is deleted)</p><p>4. At the end of this task, you will indicate whether indoor or outdoor scenes were the ‘winner’.</p><p>Please do not take notes, it is not necessary for successful performance, and will ruin the experiment.</p></sec></boxed-text></app></app-group></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61077.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kahnt</surname><given-names>Thorsten</given-names></name><role>Reviewing Editor</role><aff><institution>Northwestern University</institution><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This manuscript is of broad interest for psychologists and neuroscientists. The finding that error signals during reinforcement learning affect not only the rate of learning but also the degree to which events occurring during learning are committed to memory, and that these effects are independent of whether learning is instructed or incidental, has important implications for our understanding of how different learning systems interact. These conclusions are supported by strong behavioral and computational modeling data.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Signed and unsigned reward prediction errors dynamically enhance learning and memory&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by three peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Christian Büchel as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission.</p><p>As the editors have judged that your manuscript is of interest, but as described below that additional experiments are required before it is published, we would like to draw your attention to changes in our revision policy that we have made in response to COVID-19 (https://elifesciences.org/articles/57162). First, because many researchers have temporarily lost access to the labs, we will give authors as much time as they need to submit revised manuscripts. We are also offering, if you choose, to post the manuscript to bioRxiv (if it is not already there) along with this decision letter and a formal designation that the manuscript is &quot;in revision at <italic>eLife</italic>&quot;. Please let us know if you would like to pursue this option. (If your work is more suitable for medRxiv, you will need to post the preprint yourself, as the mechanisms for us to do so are still in development.)</p><p>Summary:</p><p>The computational rules through which the brain prioritizes experiences for storage in episodic memory are an active area of research, and one where there has been considerable disagreement in the literature. Here Rouhani and Niv rectify seemingly contradictory results from previous studies by showing that effects of rewarding and surprising outcomes on memory occur at different times in the course of a decision task, and they go on to link them to longstanding learning signals that have played instrumental roles in behavioral psychology.</p><p>Reviewers agreed that the findings are timely and interesting. There were extensive discussions among reviewers about critical aspects of the current task design that may make it difficult to relate the current results to previous findings. Specifically, the instructions given to participants encourage explicitly evaluating the stimuli and remembering them for later use. The concern is that even though the authors claim to resolve conflicting findings about modulators of memory, this is difficult to achieve with the current design because it is too different from existing work on reward-modulated memory. To better integrate the current findings with previous work, it would be important to replicate experiment 2 using instructions that do not encourage explicit memorization and allow for more incidental memory encoding.</p><p>Essential revisions:</p><p>1) There is a qualitative distinction between the current intentional encoding-for-later-use task and the overwhelmingly more common incidental memory tasks that have been used for decades, and to which the authors are trying to connect to. In general, even though the authors claim to resolve some apparently contradictory predictions about modulators of memory, they cannot do that with this design because it is too different from existing work on reward-modulated memory. Their instructed design makes it more akin to the reward-motivated memory work started by Adcock et al., 2006, where participants were incentivized to remember pictures based on cued value. The closest parallel to the current report is the work by Murty et al., 2016; see their Table 1, Experiment 3). There, participants were instructed to remember stimuli for later in the experiment (although remembering associated outcomes was not explicitly instructed). While not the focus of their study, they report no effect of reward on later memory. The current results could be part of the start of a different research direction (building on null findings from Murty et al.) on the relationship between strategic encoding-for-later-use, reward, and memory. It is unclear whether that direction would shed light on how memory is actually used outside of the lab, and such a re-framing would require a very significant re-framing of the current manuscript. Focusing on what the instructed encoding of pictures and outcomes does to memory, what do the results suggest? In these situations, it may be a rational strategy to pay greater attention to and prioritize the encoding of highly variant outcomes (surprising positive and negative deviations from expectations) if participants are trying to remember these associations for later choices. Thus, the reported modulation of memory by unsigned prediction error may be an expected consequence of instructing participants to remember associations between pictures and outcomes. To overcome these issues, reviewers would like to see an exact replication of experiment 2 without explicit instructions to memorize the items. Given data are collected online, this should be possible despite potential lab closings due to COVID-19.</p><p>2) Reviewers were not completely convinced by the model-fitting, although they felt it plays a relatively minor role for the current study, as it is unclear whether the Mackintosh signal needs to affect decision making behavior in order to affect longer term memory storage. Regardless, the model selection would be more convincing if it included the following additional information: 1) a confusion matrix demonstrating that at least one of the model comparison tools used can reliably recover the correct model, 2) posterior predictive checks showing that the winning model can capture basic learning curves, and 3) parameter fits from the winning model that make clear systematic contributions of the Mackintosh and Pearce Hall terms to positive (or negative) adjustments to the learning rate. If there is no systematic sign for Mackintosh parameter, it would be useful to know whether the sign of fits per subject corresponds to memory difference for images on positive versus negative RPE trials.</p><p>3) An additional concern is whether the proposed cue RPE modulator is actually specific to the cue RPE. On each trial, there are two factors that are highly correlated with cue RPE: the estimated value and the feedback value. In the current data, the benefit of including the Mackintosh effect on the model fits suggests that the effect comes on after values begin to be differentiated, supporting a cue RPE influence. However, an alternative model where the effect exists already at the start of learning was not evaluated. The task is very short – there are only 30 trials (15 per category) in Experiment 2. It is reasonable to assume that some initial period is necessary for participants to adjust to the task requirements / establish a “task-set” guiding their behavior, which here could cover a meaningful proportion of trials. Even if fits are better for a delayed onset of the Mackintosh effect, this could just be due to initial task adjustment adding noise to initial trials. If the participants engaged in an additional round of learning (over a new set of two types of stimuli), or if there was an extended practice session before the onset of the currently unfamiliar task, the authors could determine whether their observed result is actually due to acquisition of a cue value (leading to cue RPEs). If the observed effect is instead related to the relative estimated value or relative feedback magnitude, these would have a rapid influence on learning (once a few events are observed to roughly determine the feedback range, similar to scaling effects on dopamine RPEs). Following from this, if the feedback value is driving the observed effect, this might be due to an influence of memory for specific preceding feedback value. Basing next-trial estimates on memory for preceding trial feedback would lead to a higher estimated learning rate. This memory effect could be due to short-term working memory, given the rapid trial repetition (Collins et al.), or episodic memory.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.61077.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) There is a qualitative distinction between the current intentional encoding-for-later-use task and the overwhelmingly more common incidental memory tasks that have been used for decades, and to which the authors are trying to connect to. In general, even though the authors claim to resolve some apparently contradictory predictions about modulators of memory, they cannot do that with this design because it is too different from existing work on reward-modulated memory. Their instructed design makes it more akin to the reward-motivated memory work started by Adcock et al., 2006, where participants were incentivized to remember pictures based on cued value. The closest parallel to the current report is the work by Murty et al., 2016; see their Table 1, Experiment 3). There, participants were instructed to remember stimuli for later in the experiment (although remembering associated outcomes was not explicitly instructed). While not the focus of their study, they report no effect of reward on later memory. The current results could be part of the start of a different research direction (building on null findings from Murty et al.) on the relationship between strategic encoding-for-later-use, reward, and memory. It is unclear whether that direction would shed light on how memory is actually used outside of the lab, and such a re-framing would require a very significant re-framing of the current manuscript. Focusing on what the instructed encoding of pictures and outcomes does to memory, what do the results suggest? In these situations, it may be a rational strategy to pay greater attention to and prioritize the encoding of highly variant outcomes (surprising positive and negative deviations from expectations) if participants are trying to remember these associations for later choices. Thus, the reported modulation of memory by unsigned prediction error may be an expected consequence of instructing participants to remember associations between pictures and outcomes. To overcome these issues, reviewers would like to see an exact replication of experiment 2 without explicit instructions to memorize the items. Given data are collected online, this should be possible despite potential lab closings due to COVID-19.</p></disp-quote><p>We thank reviewers for this suggestion and have now replicated our main results from Experiment 2 in a task where we did not motivate encoding of the scenes (N = 354). We have further reframed our paper to include both of these Experiment 2 datasets, one encouraging explicit memory (original data) and the other incidental memory (we include instructions for each version of Experiment 2 in Appendix 1). Our approach was to model learning behavior across the instructed and incidental memory versions of the task (i.e., combining datasets) since learning instructions were the same, and separately analyze the memory and choice results between these two versions. We did, however, find unanticipated differences in learning performance between the two versions of Experiment 2, which we report in the manuscript and further link to memory results.</p><p>To begin, we replicated our main findings supporting the putative neurobiological mechanisms we describe in this manuscript: unsigned RPEs at outcome increase memory for trial-unique images presented at reward outcome (<italic>β</italic> = 0.11, <italic>t</italic> = 3.01, <italic>p</italic> = 0.003; Figure 4D) and signed RPEs at cue increase memory for images presented at cue (<italic>β</italic> = 0.09, <italic>t</italic> = 3.04, <italic>p</italic> = 0.002; Figure 4E).</p><p>In other words, in a task where no instructions were given to remember the items, nor to associate them with their value, incidental memory was enhanced for both images associated with more surprising rewarding outcomes and more rewarding cue events. We also found, in the incidental version, that <italic>unsigned</italic>​ RPEs at cue increased memory for cue images (<italic>β</italic> = 0.10, <italic>t</italic> = 2.93, <italic>p</italic> = 0.003; Figure 4C), an effect that had only been trending in the explicit version. This effect shows that the more participants had separated the values of the two reward cues, the better they were remembering the cue image, additionally supporting the increasing memory for cue events over learning. We did not replicate signed cue RPE’s enhancement of outcome items in the incidental version (Figure 4E, purple distribution), possibly because that effect was due to a more explicit encoding strategy where participants were encoding both the cue and outcome items associated with the higher reward category.</p><p>As in the instructed version, we also found an overall increase in memory for cue images over learning in the incidental version. However, when analyzing the two learning conditions of Experiment 2 separately (recall that this experiment included a harder condition where the means of two reward categories were 40¢ and 60¢, and an easier condition where the means were further apart, 20¢ and 80¢), we only found this overall increase in cue memory in the easier condition (Figure 3D, E).</p><p>We note here that although we did not find that the learning behavior from the new experiment influenced our modeling results (i.e., the same model was still the best fitting), we did find learning performance to be generally worse in the incidental memory version of the task (even though the learning instructions were exactly the same). We now report these results under “Learning behavior in the experimental conditions of Experiment 2” (and note that the incidental version was run during the COVID pandemic). We therefore examined whether these learning differences could account for overall memory differences between the incidental and instructed memory versions using an individual difference approach. Across all conditions of Experiment 2, we tested whether individual learning performance predicted the increase in cue memory as a function of trial number during learning. Indeed, we found that better learners also showed a greater increase of memory for cue images, but not outcome images, over learning (Figure 3—figure supplement 1). This relationship was particularly strong in the harder learning condition of the incidental memory task – that is, in the replication experiment, where we did not observe an overall increase in cue memory, suggesting that the latter null result may be driven by the overall prevalence of worse learners (while the few better learners still show the effect). This analysis provided an additional between-subjects link connecting a gradual increase in memory for cue images to more successful learning.</p><p>We also replicated all of the previous choice findings. Even though participants were not instructed to encode the association between images and their outcomes, they still preferred cue and outcome images associated with higher reward outcomes and values at the end-of-experiment surprise choice test. This indicates that they were associating both cue and outcome images with the reward outcome and value experienced on that trial (Figure 5C, D). Moreover, as before, when choosing between a cue and an outcome image from the same pair (where there should be no preference for either image), participants “irrationally” preferred the outcome image the more positive the signed RPE at outcome (Figure 6C).</p><p>Given the change from a short report to a full paper, we now include this new version of the experiment, as well as a fuller analysis of both learning and memory results across all experiments throughout the manuscript. We also provide a Discussion section where we relate these results to the larger literature investigating the effects of reward learning signals on subsequent memory (including all of the papers suggested in this review).</p><disp-quote content-type="editor-comment"><p>2) Reviewers were not completely convinced by the model-fitting, although they felt it plays a relatively minor role for the current study, as it is unclear whether the Mackintosh signal needs to affect decision making behavior in order to affect longer term memory storage. Regardless, the model selection would be more convincing if it included the following additional information: 1) a confusion matrix demonstrating that at least one of the model comparison tools used can reliably recover the correct model, 2) posterior predictive checks showing that the winning model can capture basic learning curves, and 3) parameter fits from the winning model that make clear systematic contributions of the Mackintosh and Pearce Hall terms to positive (or negative) adjustments to the learning rate. If there is no systematic sign for Mackintosh parameter, it would be useful to know whether the sign of fits per subject corresponds to memory difference for images on positive versus negative RPE trials.</p></disp-quote><p>Thank you for these important suggestions. With respect to 1, we validated model comparison by calculating a confusion matrix, <italic>p</italic>​ ​(fit model|true model), using BIC for all models used in Experiment 2 (which also comprises all models used in Experiment 1, Figure 2—figure supplement 2B). Given that all tested models were nested within the learning rate, we simulated trial-by-trial learning rates from the parameters fit to values, and directly fit these learning rates. As indicated by the high proportion of recovered simulations, this procedure validated our model comparison. We include code for the model-validation analysis in “models_RL_matlabCode” in the github repository. Since our experiments afforded measurement of trial-by-trial learning rate, we were also able to compare the models by fitting the empirical learning rates. This analysis resulted in the same pattern of results and winning model as when fitting values (we do not report these results in the paper since for this analysis we could not include trials where the prediction error was 0 leading to a learning rate of infinity, which substantially reduced the number of valid trials for many subjects).​</p><p>With respect to 2, we had represented the model-simulated values (which function as posterior predictive checks) in the original learning plots, in Figure 2A-C of the manuscript. Here, the black dots on the learning curves represent the value estimates generated by the winning model for each experiment.</p><p>Regarding 3, the winning model includes several parameters that interact with each other to predict trial-by-trial values. We plot these individually (<xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>), but believe that showing the parameter distributions in the manuscript would not help readers understand how the model predicts behavior. Instead, we chose to include the trial-by-trial distributions for learning rates generated by the winning model of Experiment 2 (Figure 2—figure supplement 2A). The takeaway here is that dynamically changing learning rates are larger and more variable at the beginning of learning, but decrease in size over the course of learning. Because of the interactions between model parameters, we did not correlate individual parameter estimates with memory for cue or outcome images as such correlations may be difficult to interpret.</p><fig id="sa2fig1"><label>Author response image 1.</label><caption><title>Model​ validation simulations.</title><p>'RW': Rescorla-Wagner, 'PH': Pearce-Hall, 'M': Mackintosh, 'D': Decay. Distribution of parameter estimates from the winning model in Experiment 1 ('RW-PH-D') and Experiment 2 ('RW-PH-M-D').</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-61077-resp-fig1-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>3) An additional concern is whether the proposed cue RPE modulator is actually specific to the cue RPE. On each trial, there are two factors that are highly correlated with cue RPE: the estimated value and the feedback value. In the current data, the benefit of including the Mackintosh effect on the model fits suggests that the effect comes on after values begin to be differentiated, supporting a cue RPE influence. However, an alternative model where the effect exists already at the start of learning was not evaluated. The task is very short – there are only 30 trials (15 per category) in Experiment 2. It is reasonable to assume that some initial period is necessary for participants to adjust to the task requirements / establish a “task-set” guiding their behavior, which here could cover a meaningful proportion of trials. Even if fits are better for a delayed onset of the Mackintosh effect, this could just be due to initial task adjustment adding noise to initial trials. If the participants engaged in an additional round of learning (over a new set of two types of stimuli), or if there was an extended practice session before the onset of the currently unfamiliar task, the authors could determine whether their observed result is actually due to acquisition of a cue value (leading to cue RPEs). If the observed effect is instead related to the relative estimated value or relative feedback magnitude, these would have a rapid influence on learning (once a few events are observed to roughly determine the feedback range, similar to scaling effects on dopamine RPEs). Following from this, if the feedback value is driving the observed effect, this might be due to an influence of memory for specific preceding feedback value. Basing next-trial estimates on memory for preceding trial feedback would lead to a higher estimated learning rate. This memory effect could be due to short-term working memory, given the rapid trial repetition (Collins et al.), or episodic memory.</p></disp-quote><p>Thank you for these critical points. First and foremost, we made a mistake in not including information about the practice block that always preceded learning. In order to familiarize participants with the task-set, as you mention, we had participants complete a practice section identical to the learning block where they learned the means of reward categories, although with different underlying means than the learning task and fewer trials. The same practice trials were given in each experiment to avoid differentially biasing participants between conditions. We now include these details in the Materials and methods section.</p><p>Before reward learning, participants completed a practice task (12 trials) to ensure they had learned the structure of the reward-learning task using different reward contingencies than what would be learned in the experimental task. In the practice trials of Experiment 1, participants experienced one reward change-point, from a mean of 30¢ to 50¢. In the practice trials of Experiment 2, in all conditions, the low-value scene category was worth 30¢ and the high-value scene category was worth 70¢, on average.</p><p>As to your point about participants potentially using their working memory of previous outcomes for subsequent value estimates, we now include empirical learning rate plots for each experiment as a supplementary figure that sheds light on this issue (Figure 2—figure supplement 1). If participants were using the last reward outcome as their subsequent estimate for that reward category, learning rates would be close to 1. We do not see such high learning rates on average (except for learning rates after change points in Experiment 1, where we would expect a very high learning rate based on prior literature). We note that empirical learning rates in Experiment 2, which includes the cue RPE signal (Figure 2—figure supplement 1B, C), were low (~0.5 or less, on average) from the very first trials of the experiment.</p></body></sub-article></article>