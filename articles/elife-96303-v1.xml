<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">96303</article-id><article-id pub-id-type="doi">10.7554/eLife.96303</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.96303.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Geometry and dynamics of representations in a precisely balanced memory network related to olfactory cortex</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meissner-Bernard</surname><given-names>Claire</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0009-0007-2038-8398</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author"><name><surname>Zenke</surname><given-names>Friedemann</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1883-644X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Friedrich</surname><given-names>Rainer W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9107-0482</contrib-id><email>rainer.friedrich@fmi.ch</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01bmjkv45</institution-id><institution>Friedrich Miescher Institute for Biomedical Research</institution></institution-wrap><addr-line><named-content content-type="city">Basel</named-content></addr-line><country>Switzerland</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02s6k3f65</institution-id><institution>University of Basel</institution></institution-wrap><addr-line><named-content content-type="city">Basel</named-content></addr-line><country>Switzerland</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Gjorgjieva</surname><given-names>Julijana</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02kkvpp62</institution-id><institution>Technical University of Munich</institution></institution-wrap><country>Germany</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Poirazi</surname><given-names>Panayiota</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01gzszr18</institution-id><institution>FORTH Institute of Molecular Biology and Biotechnology</institution></institution-wrap><country>Greece</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>13</day><month>01</month><year>2025</year></pub-date><volume>13</volume><elocation-id>RP96303</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2024-01-30"><day>30</day><month>01</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2024-01-18"><day>18</day><month>01</month><year>2024</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.12.12.571272"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-05-09"><day>09</day><month>05</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96303.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-12-31"><day>31</day><month>12</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.96303.2"/></event></pub-history><permissions><copyright-statement>© 2024, Meissner-Bernard et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Meissner-Bernard et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-96303-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-96303-figures-v1.pdf"/><abstract><p>Biological memory networks are thought to store information by experience-dependent changes in the synaptic connectivity between assemblies of neurons. Recent models suggest that these assemblies contain both excitatory and inhibitory neurons (E/I assemblies), resulting in co-tuning and precise balance of excitation and inhibition. To understand computational consequences of E/I assemblies under biologically realistic constraints we built a spiking network model based on experimental data from telencephalic area Dp of adult zebrafish, a precisely balanced recurrent network homologous to piriform cortex. We found that E/I assemblies stabilized firing rate distributions compared to networks with excitatory assemblies and global inhibition. Unlike classical memory models, networks with E/I assemblies did not show discrete attractor dynamics. Rather, responses to learned inputs were locally constrained onto manifolds that ‘focused’ activity into neuronal subspaces. The covariance structure of these manifolds supported pattern classification when information was retrieved from selected neuronal subsets. Networks with E/I assemblies therefore transformed the geometry of neuronal coding space, resulting in continuous representations that reflected both relatedness of inputs and an individual’s experience. Such continuous representations enable fast pattern classification, can support continual learning, and may provide a basis for higher-order learning and cognitive computations.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>autoassociative memory</kwd><kwd>assembly</kwd><kwd>olfactory cortex</kwd><kwd>computational model</kwd><kwd>zebrafish</kwd><kwd>neural manifold</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>31003A_172925/1</award-id><principal-award-recipient><name><surname>Friedrich</surname><given-names>Rainer W</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001711</institution-id><institution>Schweizerischer Nationalfonds zur Förderung der Wissenschaftlichen Forschung</institution></institution-wrap></funding-source><award-id>PCEFP3_202981</award-id><principal-award-recipient><name><surname>Zenke</surname><given-names>Friedemann</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000781</institution-id><institution>European Research Council</institution></institution-wrap></funding-source><award-id>742576</award-id><principal-award-recipient><name><surname>Friedrich</surname><given-names>Rainer W</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Computational modeling revealed that balanced assemblies of excitatory and inhibitory neurons shape representational manifolds in olfactory cortex-like recurrent networks, resulting in joint maps of sensory and semantic information.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Autoassociative memory establishes internal representations of specific inputs that may serve as a basis for higher brain functions including classification and prediction. Representation learning in autoassociative memory networks is thought to involve experience-dependent synaptic plasticity and potentially other mechanisms that enhance connectivity among assemblies of excitatory neurons (<xref ref-type="bibr" rid="bib34">Hebb, 1949</xref>; <xref ref-type="bibr" rid="bib42">Ko et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Miehl et al., 2023</xref>; <xref ref-type="bibr" rid="bib69">Ryan et al., 2015</xref>). Classical theories proposed that assemblies define discrete attractor states and map related inputs onto a common stable output pattern. Hence, neuronal assemblies are thought to encode internal representations, or memories, that classify inputs relative to previous experience via attractor dynamics (<xref ref-type="bibr" rid="bib3">Amit and Tsodyks, 1991</xref>; <xref ref-type="bibr" rid="bib32">Goldman-Rakic, 1995</xref>; <xref ref-type="bibr" rid="bib38">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="bib43">Kohonen, 1984</xref>; <xref ref-type="bibr" rid="bib45">Lagzi and Rotter, 2015</xref>; <xref ref-type="bibr" rid="bib53">Mazzucato et al., 2015</xref>). However, brain areas with memory functions such as the hippocampus or neocortex often exhibit dynamics that is atypical of attractor networks including irregular firing patterns, transient responses to inputs, and high trial-to-trial variability (<xref ref-type="bibr" rid="bib39">Iurilli and Datta, 2017</xref>; <xref ref-type="bibr" rid="bib64">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib77">Shadlen and Newsome, 1994</xref>).</p><p>Irregular, fluctuation-driven firing reminiscent of cortical activity emerges in recurrent networks when neurons receive strong excitatory (E) and inhibitory (I) synaptic input (<xref ref-type="bibr" rid="bib13">Brunel, 2000</xref>; <xref ref-type="bibr" rid="bib77">Shadlen and Newsome, 1994</xref>; <xref ref-type="bibr" rid="bib82">van Vreeswijk and Sompolinsky, 1996</xref>). In such ‘balanced state’ networks, enhanced connectivity among assemblies of E neurons is prone to generate runaway activity unless matched I connectivity establishes co-tuning of E and I inputs in individual neurons. The resulting state of ‘precise’ synaptic balance stabilizes firing rates because inhomogeneities in excitation across the population or temporal variations in excitation are tracked by correlated inhibition (<xref ref-type="bibr" rid="bib36">Hennequin et al., 2017</xref>; <xref ref-type="bibr" rid="bib35">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib47">Lagzi and Fairhall, 2024</xref>; <xref ref-type="bibr" rid="bib65">Rost et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Vogels et al., 2011</xref>). E/I co-tuning has been observed experimentally in cortical brain areas (<xref ref-type="bibr" rid="bib8">Bhatia et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Froemke et al., 2007</xref>; <xref ref-type="bibr" rid="bib60">Okun and Lampl, 2008</xref>; <xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>; <xref ref-type="bibr" rid="bib85">Wehr and Zador, 2003</xref>) and emerged in simulations that included spike-timing-dependent plasticity at I synapses (<xref ref-type="bibr" rid="bib46">Lagzi et al., 2021</xref>; <xref ref-type="bibr" rid="bib50">Litwin-Kumar and Doiron, 2014</xref>; <xref ref-type="bibr" rid="bib83">Vogels et al., 2011</xref>; <xref ref-type="bibr" rid="bib91">Zenke et al., 2015</xref>). In simulations, E/I co-tuning can be established by including I neurons in assemblies, resulting in ‘E/I assemblies’ where I neurons track activity of E neurons (<xref ref-type="bibr" rid="bib7">Barron et al., 2017</xref>; <xref ref-type="bibr" rid="bib21">Eckmann et al., 2024</xref>; <xref ref-type="bibr" rid="bib47">Lagzi and Fairhall, 2024</xref>; <xref ref-type="bibr" rid="bib51">Mackwood et al., 2021</xref>). Exploring the structural basis of E/I co-tuning in biological networks is challenging because it requires the dense reconstruction of large neuronal circuits at synaptic resolution (<xref ref-type="bibr" rid="bib28">Friedrich and Wanner, 2021</xref>).</p><p>Modeling studies started to investigate effects of E/I assemblies on network dynamics (<xref ref-type="bibr" rid="bib15">Chenkov et al., 2017</xref>; <xref ref-type="bibr" rid="bib51">Mackwood et al., 2021</xref>; <xref ref-type="bibr" rid="bib71">Sadeh and Clopath, 2020a</xref>; <xref ref-type="bibr" rid="bib76">Schulz et al., 2021</xref>) but the impact on neuronal computations in the brain remains unclear. Balanced state networks can exhibit a broad range of dynamical behaviors, including chaotic firing, transient responses, and stable states (<xref ref-type="bibr" rid="bib22">Festa et al., 2014</xref>; <xref ref-type="bibr" rid="bib35">Hennequin et al., 2014</xref>; <xref ref-type="bibr" rid="bib49">Litwin-Kumar and Doiron, 2012</xref>; <xref ref-type="bibr" rid="bib59">Murphy and Miller, 2009</xref>; <xref ref-type="bibr" rid="bib66">Roudi and Latham, 2007</xref>), implying that computational consequences of E/I assemblies depend on network parameters. We therefore examined effects of E/I assemblies on autoassociative memory in a spiking network model that was constrained by experimental data from telencephalic area Dp of adult zebrafish, which is homologous to mammalian piriform cortex (<xref ref-type="bibr" rid="bib58">Mueller et al., 2011</xref>).</p><p>Dp and piriform cortex receive direct input from mitral cells in the olfactory bulb (OB) and have been proposed to function as autoassociative memory networks (<xref ref-type="bibr" rid="bib33">Haberly, 2001</xref>; <xref ref-type="bibr" rid="bib87">Wilson and Sullivan, 2011</xref>). Consistent with this hypothesis, manipulations of neuronal activity in piriform cortex affected olfactory memory (<xref ref-type="bibr" rid="bib54">Meissner-Bernard et al., 2019</xref>; <xref ref-type="bibr" rid="bib70">Sacco and Sacchetti, 2010</xref>). In both brain areas, odors evoke temporally structured, spatially distributed activity patterns (<xref ref-type="bibr" rid="bib9">Blazing and Franks, 2020</xref>; <xref ref-type="bibr" rid="bib78">Stettler and Axel, 2009</xref>; <xref ref-type="bibr" rid="bib89">Yaksi et al., 2009</xref>) that are dominated by synaptic inputs from recurrent connections (<xref ref-type="bibr" rid="bib24">Franks et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>) and modified by experience (<xref ref-type="bibr" rid="bib14">Chapuis and Wilson, 2011</xref>; <xref ref-type="bibr" rid="bib23">Frank et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Jacobson et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Pashkovski et al., 2020</xref>). Whole-cell voltage clamp recordings revealed that neurons in posterior Dp (pDp) received large E and I synaptic inputs during odor responses. These inputs were co-tuned in odor space and correlated on fast timescales, demonstrating that pDp enters a transient state of precise synaptic balance during odor stimulation (<xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>).</p><p>We found that network models of pDp with assemblies but without E/I co-tuning generated persistent attractor dynamics and exhibited a biologically unrealistic broadening of the firing rate distribution. Introducing E/I assemblies established E/I co-tuning, stabilized the firing rate distribution, and abolished persistent attractor states. In networks with E/I assemblies, population activity was locally constrained onto manifolds that represented learned and related inputs by ‘focusing’ activity into neuronal subspaces. The covariance structure of manifolds supported pattern classification when information was retrieved from selected neuronal subsets. Furthermore, the continuity of the olfactory coding space provided a metric representing the similarity of inputs to learned stimuli. These results show that autoassociative memory networks constrained by biological data operate in a balanced regime where information is contained in the geometry of neural manifolds. Predictions derived from these analyses may be tested experimentally by measurements of neuronal population activity in zebrafish.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>A spiking network model based on pDp</title><p>To analyze memory-related computational functions of E/I assemblies under biologically realistic constraints we built a spiking neural network model, pDp<sub>sim</sub>, based on experimental data from pDp (<xref ref-type="fig" rid="fig1">Figure 1A</xref>; <xref ref-type="bibr" rid="bib10">Blumhagen et al., 2011</xref>; <xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>; <xref ref-type="bibr" rid="bib89">Yaksi et al., 2009</xref>). pDp<sub>sim</sub> comprised 4000 E neurons and 1000 I neurons, consistent with the estimated total number of 4000–10,000 neurons in adult zebrafish pDp (unpublished observations). The network received afferent input from 1500 mitral cells in the OB with a mean spontaneous firing rate of 6 Hz (<xref ref-type="bibr" rid="bib27">Friedrich and Laurent, 2004</xref>; <xref ref-type="bibr" rid="bib26">Friedrich and Laurent, 2001</xref>; <xref ref-type="bibr" rid="bib80">Tabor and Friedrich, 2008</xref>). Odors were modeled by increasing the firing rates of 10% of mitral cells to a mean of 15 Hz and decreasing the firing rates of 5% of mitral cells to a mean of 2 Hz (Methods, <xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="bibr" rid="bib27">Friedrich and Laurent, 2004</xref>; <xref ref-type="bibr" rid="bib26">Friedrich and Laurent, 2001</xref>). As a result, the mean activity increased only slightly while the variance of firing rates across the mitral cell population increased approximately sevenfold, consistent with experimental observations (<xref ref-type="bibr" rid="bib27">Friedrich and Laurent, 2004</xref>; <xref ref-type="bibr" rid="bib84">Wanner and Friedrich, 2020</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Spiking network model of pDp.</title><p>(<bold>A</bold>) Schematic of pDp<sub>sim</sub>. OB, olfactory bulb; E, excitatory; I, inhibitory neurons. (<bold>B</bold>) Spike raster of a random subset of 50 mitral cells in the OB representing 2 odors (O1 and O2). During odor stimulation, firing rates of 10% of mitral cells were increased and firing rates of 5% of mitral cells were decreased (baseline rate, 6 Hz). (<bold>C</bold>) Spike raster of random subsets of 50 E and I neurons in response to 2 odors. (<bold>D</bold>) Representative membrane potential trace (top) and excitatory (EPSC, black) and inhibitory (IPSC, red) currents (bottom) in one excitatory neuron in response to two odors. Purple trace shows net current (EPSC + IPSC). (<bold>E</bold>) Odor-evoked inhibitory (red) and excitatory (black and blue) currents as measured in a hypothetical voltage clamp experiment (conductance multiplied by 70 mV, the absolute difference between holding potential and reversal potential; <xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>). Representative example of one network, averaged across neurons and odors. (<bold>F–H</bold>) Measured values of the observables used to match pDp<sub>sim</sub> to experimental data. Each dot represents one network (average over 10 odors); <italic>n</italic> = 20 networks. Pink shading shows the experimentally observed range of values. (<bold>F</bold>) Baseline and odor-evoked population firing rate. (<bold>G</bold>) Left: <italic>g</italic><sub>OE</sub> is the synaptic conductance in E neurons contributed by afferents from the OB during odor stimulation. Middle: <italic>g</italic><sub>syn</sub> is the total odor-evoked synaptic conductance. Right: % recurrent input quantifies the percentage of E input contributed by recurrent connections during odor stimulation. (<bold>H</bold>) Correlation coefficient between odor-evoked activity patterns in pDp<sub>sim</sub>. The dotted line indicates the mean correlation between odor patterns in the OB.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig1-v1.tif"/></fig><p>pDp<sub>sim</sub> consisted of sparsely connected integrate-and-fire neurons with conductance-based synapses (connection probability ≤5%, Methods). Model parameters were taken from the literature when available and otherwise determined to reproduce measured observables (<xref ref-type="fig" rid="fig1">Figure 1F–H</xref>; Methods). The mean firing rate was &lt;0.1 Hz in the absence of stimulation and increased to ~1 Hz during odor presentation (<xref ref-type="fig" rid="fig1">Figure 1C, F</xref>; <xref ref-type="bibr" rid="bib10">Blumhagen et al., 2011</xref>; <xref ref-type="bibr" rid="bib68">Rupprecht et al., 2021</xref>; <xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>). Population activity was odor-specific and activity patterns evoked by uncorrelated OB inputs remained uncorrelated in pDp<sub>sim</sub> (<xref ref-type="fig" rid="fig1">Figure 1H</xref>; <xref ref-type="bibr" rid="bib89">Yaksi et al., 2009</xref>). The synaptic conductance during odor presentation substantially exceeded the resting conductance and inputs from other E neurons (recurrent inputs) contributed &gt;80% of the excitatory synaptic conductance (<xref ref-type="fig" rid="fig1">Figure 1G</xref>). Hence, pDp<sub>sim</sub> entered a balanced state during odor stimulation (<xref ref-type="fig" rid="fig1">Figure 1D, E</xref>) with recurrent input dominating over afferent input, as observed in pDp (<xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>). Shuffling spike times of inhibitory neurons resulted in runaway activity with a probability of 0.79 ± 0.20, demonstrating that activity was inhibition-stabilized (<xref ref-type="bibr" rid="bib72">Sadeh and Clopath, 2020b</xref>; <xref ref-type="bibr" rid="bib81">Tsodyks et al., 1997</xref>). These results were robust against parameter variations (Methods). pDp<sub>sim</sub> therefore reproduced key features of pDp.</p></sec><sec id="s2-2"><title>Co-tuning and stability of networks with E/I assemblies</title><p>To create networks with defined neuronal assemblies we rewired a small subset of the connections in randomly connected (<italic>rand</italic>) networks. An assembly was generated by identifying the 100 E neurons that received the most connections from the mitral cells activated by a given odor and increasing the probability of connections between these E neurons by a factor of <italic>α</italic> (<xref ref-type="fig" rid="fig2">Figure 2A</xref> and <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1A</xref>). The number of E input connections per neuron was maintained by randomly eliminating connections from neurons outside the assembly. We thus refer to an odor as ‘learned’ when a network contains a corresponding assembly, and as ‘novel’ when no such assembly is present. In each network, we created 15 assemblies representing memories of uncorrelated odors. As a result, ~30% of E neurons were part of an assembly, with few neurons participating in multiple assemblies. Odor-evoked activity within assemblies was higher than the population mean and increased with <italic>α</italic> (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). When <italic>α</italic> reached a critical value of ~6, networks became unstable and generated runaway activity (<xref ref-type="fig" rid="fig2">Figure 2B</xref>).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Networks with neuronal assemblies (memories).</title><p>(<bold>A</bold>) Schematic of assemblies. Each assembly contains the 100 E neurons that are most densely connected to the mitral cells activated by a given odor. Connection probability between these E neurons is increased by a factor <italic>α</italic>. In <italic>Scaled I</italic> networks, weights of all I-to-E synapses are increased by a factor <italic>χ</italic>. In <italic>Tuned</italic> networks, the 25 I neurons that are most densely connected to the 100 E neurons are included in each assembly. In <italic>Tuned I</italic> networks, the probability of I-to-E connections within the assembly is increased by a factor <italic>β</italic>. In <italic>Tuned E+I</italic> networks, probabilities of I-to-E and E-to-I connectivity within the assembly are increased by factors <italic>β</italic> and <italic>γ</italic>, respectively. <italic>n</italic> = 20 networks with 15 assemblies each were simulated for each group. (<bold>B</bold>) Firing rates averaged over all E or I neurons (full lines) and over all assembly neurons (dashed lines) as a function of <italic>α</italic> (mean ± SD across 20 networks). (<bold>C</bold>) Mean E neuron firing rates of <italic>Scaled</italic> (left) and <italic>Tuned</italic> (right) networks in response to learned odors as a function of connection strength and probability, respectively. Squares depict parameters used in following figures unless stated otherwise. (<bold>D</bold>) Spike raster plots showing responses of 50 E neurons to 2 odors (O1: novel odor; O2: learned odor) in a <italic>Scaled I</italic> and the corresponding <italic>Tuned</italic> networks (same neurons and odors in the corresponding <italic>rand</italic> network are shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). (<bold>E</bold>) Top: Mean firing rates in response to learned odors as a function of time, averaged across assembly or non-assembly neurons. Bottom: Mean correlation between activity patterns evoked by a learned odor at different time points during odor presentation. Correlation coefficients were calculated between pairs of activity vectors composed of the mean firing rates of E neurons in 100 ms time bins. Activity vectors were taken from the same or different trials, except for the diagonal, where only patterns from different trials were considered. The pink bar indicates odor presentation. (<bold>F</bold>) Mean firing rate in response to learned odors or novel odors. Each data point represents one network–odor pair (<italic>n</italic> = 20 networks, 10 odors). (<bold>G</bold>) Amplification within and outside assemblies, calculated as the ratio between the mean firing rates in response to learned odors averaged over the same populations of neurons in a given structured network (<italic>Scaled I</italic>, <italic>Tuned I</italic>, or <italic>Tuned E+I</italic>) and the corresponding <italic>rand</italic> network. Each data point represents one network–odor pair. (<bold>H</bold>) Quantification of co-tuning by the correlation between time-averaged E and I conductances in response to different odors, average across neurons. Each data point corresponds to one network (<italic>n</italic> = 20). Mean ± SD. (<bold>I</bold>) Quantification of co-tuning by the ratio of dispersion of joint conductances along balanced and counter-balanced axes (inset; Methods; <italic>n</italic> = 20 networks).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Structured networks reproduce key features of pDp.</title><p>(<bold>A</bold>) Connection probability between activated (activ.) or non-activated (non-activ.) mitral cells and assembly or non-assembly neurons. (<bold>B</bold>) Connection probability between classes of E and I neurons in a <italic>rand</italic>, a <italic>Tuned I</italic>, and a <italic>Tuned E+I</italic> network. (<bold>C</bold>) EPSCs and IPSCs in a <italic>Tuned I</italic> network as observed in a hypothetical voltage clamp recording, averaged across neurons and odors. An equivalent plot for a <italic>rand</italic> network is shown in <xref ref-type="fig" rid="fig1">Figure 1E</xref>. (<bold>D</bold>, <bold>E</bold>) Values of observables for <italic>rand</italic> networks (same as <xref ref-type="fig" rid="fig1">Figure 1G, H</xref>) and different structured networks (<italic>Scaled I</italic>, <italic>Tuned I</italic>, and <italic>Tuned E+I</italic>). (<bold>F</bold>) Spike raster plots showing responses of 50 I neurons to 2 odors (O1: novel odor; O2: learned odor) in a <italic>Scaled I</italic> and the corresponding <italic>Tuned</italic> networks (same neurons and odors in the corresponding rand network are shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>). (<bold>G</bold>) Network with increased connectivity between the E assembly neurons (<italic>α</italic> = 5) and the 25 I neurons that are most densely connected to the mitral cells activated by a given odor. (<bold>H</bold>) Mean firing rate of the network in (<bold>E</bold>) in response to learned odors as a function of connection probability. Activity could not be stabilized efficiently by selecting I neurons based on their afferent connectivity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Structured networks: additional results.</title><p>(<bold>A</bold>) Raster plots showing responses of assembly or non-assembly neurons to a learned odor. (<bold>B</bold>) Ratio of the average firing rate of (pseudo-)assembly neurons to non-assembly neurons in response to learned odors. Each data point represents one network-odor pair (<italic>n</italic> = 20 networks, 10 odors). Mean ± SD. (<bold>C</bold>) Coefficients of variation of the inter-spike interval (ISI) in assembly neurons. (<bold>D</bold>) Correlation between activity patterns across E neurons evoked by the same novel odor in different trials as a function of time. Pink bar indicates odor presentation. Note that correlations in response to novel odors are similar across networks and different from responses to learned odors in <italic>Scaled</italic> and <italic>Tuned</italic> networks (<xref ref-type="fig" rid="fig2">Figure 2E</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig2-figsupp2-v1.tif"/></fig></fig-group><p>We first set <italic>α</italic> to 5, then uniformly scaled I-to-E connection weights by a factor of <italic>χ</italic> until E population firing rates in response to learned odors matched the corresponding firing rates in <italic>rand</italic> networks, that is, 1 Hz (‘<italic>Scaled I</italic>’ networks; <xref ref-type="fig" rid="fig2">Figure 2A, C, D, F</xref>). Under these conditions, activity within assemblies was amplified substantially in comparison to the corresponding neurons in <italic>rand</italic> networks (pseudo-assembly) whereas activity outside assemblies was substantially reduced (<xref ref-type="fig" rid="fig2">Figure 2E, G</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Hence, non-specific scaling of inhibition resulted in a large and biologically unrealistic divergence of firing rates (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>) that nearly exhausted the dynamic range of individual neurons in the population, indicating that homeostatic global inhibition is insufficient to maintain a stable firing rate distribution. We further observed that neurons within activated assemblies produced regular spike trains (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>), indicating that the balanced regime was no longer maintained.</p><p>In <italic>rand</italic> networks, correlations between E and I synaptic conductances in individual neurons were slightly above zero (<xref ref-type="fig" rid="fig2">Figure 2H</xref>), presumably because of stochastic inhomogeneities in the connectivity (<xref ref-type="bibr" rid="bib62">Pehlevan and Sompolinsky, 2014</xref>). In <italic>Scaled I</italic> networks, correlations remained near zero, indicating that E assemblies by themselves did not enhance E/I co-tuning (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). <italic>Scaled I</italic> networks with structured E but random I connectivity can therefore not account for the stability, synaptic balance, and E/I co-tuning observed experimentally (<xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>).</p><p>We next created structured networks with more precise E/I balance by including I neurons within assemblies. We first selected the 25 I neurons that received the largest number of connections from the 100 E neurons of an assembly. The connectivity between these two sets of neurons was then enhanced by two procedures to generate E/I assemblies: (1) in ‘<italic>Tuned I</italic>’ networks, the probability of I-to-E connections was increased by a factor <italic>β</italic> while E-to-I connections remained unchanged. (2) In ‘<italic>Tuned E+I</italic>’ networks, the probability of I-to-E connections was increased by <italic>β</italic> and the probability of E-to-I connections was increased by <italic>γ</italic> (<xref ref-type="fig" rid="fig2">Figure 2A</xref>; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1B</xref>). As for ‘<italic>Scaled I’</italic> networks, <italic>β</italic> and <italic>γ</italic> were adjusted to obtain mean population firing rates of ~1 Hz in response to learned odors (<xref ref-type="fig" rid="fig2">Figure 2C, D, F</xref>). The other observables used to constrain the <italic>rand</italic> networks remained unaffected (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1C, D, E</xref>). Activity of networks with E assemblies could not be stabilized around 1 Hz by increasing connectivity from subsets of I neurons receiving dense input from activated mitral cells (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1G, H</xref>; <xref ref-type="bibr" rid="bib71">Sadeh and Clopath, 2020a</xref>).</p><p>In <italic>Tuned</italic> networks, correlations between E and I conductances in individual neurons were significantly higher than in <italic>rand</italic> or <italic>Scaled I</italic> networks (<xref ref-type="fig" rid="fig2">Figure 2H</xref>). To further analyze E/I co-tuning we projected synaptic conductances of each neuron onto a line representing the E/I ratio expected in a balanced network (balanced axis) and onto an orthogonal line (‘counter-balanced axis’; <xref ref-type="fig" rid="fig2">Figure 2I</xref>, inset, Methods). The ratio between the standard deviations (SDs) along these axes has been used previously to quantify E/I co-tuning in experimental studies (<xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>). This ratio was close to 1 in <italic>rand</italic> and <italic>Scaled I</italic> networks but significantly higher in <italic>Tuned I</italic> and <italic>Tuned E+I</italic> networks (<xref ref-type="fig" rid="fig2">Figure 2I</xref>). Hence, <italic>Tuned</italic> networks exhibited significant co-tuning along the balanced axis, as observed in pDp (<xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>).</p><p>In <italic>Tuned</italic> networks, activity within assemblies was higher than the mean activity but substantially lower and more irregular than in <italic>Scaled I</italic> networks (<xref ref-type="fig" rid="fig2">Figure 2E, G</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Unlike in <italic>Scaled I</italic> networks, mean firing rates evoked by novel odors were indistinguishable from those evoked by learned odors and from mean firing rates in <italic>rand</italic> networks (<xref ref-type="fig" rid="fig2">Figure 2F</xref>, <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1F</xref>). Hence, E/I co-tuning prevented excessive amplification of activity in assemblies without affecting global network activity, consistent with experimental observations in Dp and piriform cortex (<xref ref-type="bibr" rid="bib14">Chapuis and Wilson, 2011</xref>; <xref ref-type="bibr" rid="bib23">Frank et al., 2019</xref>).</p></sec><sec id="s2-3"><title>Effects of E/I assemblies on attractor dynamics</title><p>We next explored effects of assemblies on network dynamics. In <italic>rand</italic> networks, firing rates increased after stimulus onset and rapidly returned to a low baseline after stimulus offset (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Correlations between activity patterns evoked by the same odor at different time points and in different trials were positive but substantially lower than unity, indicating high variability (<xref ref-type="fig" rid="fig2">Figure 2E</xref> and <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2D</xref>). Hence, <italic>rand</italic> networks showed transient and variable responses to input patterns, consistent with the typical behavior of generic balanced state networks (<xref ref-type="bibr" rid="bib77">Shadlen and Newsome, 1994</xref>; <xref ref-type="bibr" rid="bib82">van Vreeswijk and Sompolinsky, 1996</xref>). <italic>Scaled I</italic> networks responded to learned odors with persistent firing of assembly neurons and high pattern correlations across trials and time, implying attractor dynamics (<xref ref-type="bibr" rid="bib38">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="bib41">Khona and Fiete, 2022</xref>), whereas <italic>Tuned</italic> networks exhibited transient responses and modest pattern correlations similar to <italic>rand</italic> networks (<xref ref-type="fig" rid="fig2">Figure 2E</xref>). Hence, <italic>Tuned</italic> networks did not exhibit stable attractor states, presumably because precise synaptic balance prevented strong recurrent amplification within E/I assemblies.</p><p>In classical memory networks, attractor dynamics drives pattern completion, the retrieval of the whole memory from noisy or corrupted versions of the learned input. We therefore tested whether networks with E/I assemblies performed pattern completion using two different approaches. In computational studies, pattern completion is often assessed by testing whether targeted partial stimulation of an assembly can activate the assembly as a whole (<xref ref-type="bibr" rid="bib73">Sadeh and Clopath, 2021</xref>; <xref ref-type="bibr" rid="bib83">Vogels et al., 2011</xref>). Stimulating subsets of E neurons in an assembly during baseline activity recruited the entire assembly in all structured pDp<sub>sim</sub> networks (<italic>Scaled</italic> and <italic>Tuned</italic>) without a significant increase in the overall population activity (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1A</xref>).</p><p>During partial assembly stimulation, most pDp<sub>sim</sub> neurons do not receive any input, which is unlikely to happen during odor presentation. Therefore, we also assessed pattern completion with more realistic variations of inputs, building on experimental studies in the olfactory system and hippocampus (<xref ref-type="bibr" rid="bib87">Wilson and Sullivan, 2011</xref>; <xref ref-type="bibr" rid="bib90">Yassa and Stark, 2011</xref>). We morphed a novel odor into a learned odor (<xref ref-type="fig" rid="fig3">Figure 3A</xref>), or a learned odor into another learned odor (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>), and quantified the similarity between morphed and learned odors by the Pearson correlation of the OB activity patterns (input correlation). We then compared input correlations to the corresponding pattern correlations among E neurons in pDp<sub>sim</sub> (output correlation). In <italic>rand</italic> networks, output correlations increased linearly with input correlations but did not exceed them (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>), consistent with the absence of pattern completion in generic random networks (<xref ref-type="bibr" rid="bib4">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib52">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib74">Schaffer et al., 2018</xref>; <xref ref-type="bibr" rid="bib86">Wiechert et al., 2010</xref>). <italic>Scaled I</italic> networks, in contrast, showed typical signatures of pattern completion: output correlations increased abruptly as a morphed odor approached a learned odor and eventually exceeded the corresponding input correlations (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). In <italic>Tuned</italic> networks, output correlations were higher than in <italic>rand</italic> networks but remained lower than input correlations and did not increase steeply near the learned odor (<xref ref-type="fig" rid="fig3">Figure 3B</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1B</xref>). Hence, networks with E/I assemblies did not show signatures of pattern completion in response to naturalistic stimuli, consistent with the absence of strong recurrent amplification within assemblies.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Changes of output activity to gradual modifications of inputs.</title><p>(<bold>A</bold>) Morphing of a novel odor N into a learned odor L. Morphed odors were generated by gradually changing the fractions of activated mitral cells defining odors N or L. The <italic>x</italic>-axis indicates the similarity of a morphed odor to odor L, here quantified by the correlation between the olfactory bulb (OB) activity patterns representing the morphed and the learned odors (same <italic>x</italic>-axis in all panels). (<bold>B</bold>) Pearson correlation between activity patterns evoked by a morphed odor and a learned odor in pDp<sub>sim</sub> as a function of the corresponding correlations in the OB (full line). The dashed line shows the correlation to the novel odor instead of the learned odor. A signature of pattern completion is a steep increase in the correlation between activity patterns representing a morphed and a learned odor in pDp<sub>sim</sub>, often exceeding the OB input correlation (gray shaded area). (<bold>C</bold>) Firing rates in response to morphed odors averaged across assembly neurons (learned odor, full line) or pseudo-assembly neurons (novel odor, dashed line) as a function of similarity between the presented odor and learned odor L (see A). (<bold>D</bold>) Excitatory input from the OB averaged over assembly neurons or non-assembly neurons. (<bold>E</bold>) Sum of inhibitory and excitatory currents evoked by other pDp<sub>sim</sub> neurons (recurrent input), averaged over assembly or non-assembly neurons. (<bold>F</bold>) Absolute inhibitory and excitatory recurrent currents averaged over assembly neurons. (<bold>B–F</bold>) Averages over eight networks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Pattern completion: additional results.</title><p>(<bold>A</bold>) Artificial reactivation of E assemblies. During 6 Hz baseline activity of the olfactory bulb (OB), a subset of the assembly neurons was artificially reactivated by current injection (500 ms, 28 pA). Mean firing rates were quantified in the injected assembly neurons (<bold>i</bold>), in the remaining, non-injected assembly or pseudo-assembly neurons (<bold>ii</bold>), and in the non-assembly neurons (<bold>iii</bold>) as a function of time. The orange bar indicates duration of current injection. Average over 10 assemblies. <italic>n</italic> = 8 networks (mean ± SD). (<bold>B</bold>) Correlation between activity patterns across E neurons (output patterns) evoked by a series of input patterns representing a morph of one learned odor into another learned odor. Correlations between output patterns are plotted as a function of the correlation between the corresponding OB patterns. (<bold>C</bold>) Absolute value of the inhibitory and excitatory recurrent currents averaged over non-assembly neurons (see <xref ref-type="fig" rid="fig3">Figure 3F</xref> for the average over assembly neurons).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig3-figsupp1-v1.tif"/></fig></fig-group><p>To further understand the effect of E/I assemblies on network dynamics we examined the synaptic currents underlying the modest amplification of activity in assemblies of <italic>Tuned</italic> networks that occurred as a learned odor was approached (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). We measured the relative contribution of synaptic inputs from afferent and recurrent sources received by assembly and non-assembly neurons. In <italic>rand</italic> networks, activity in pseudo-assembly neurons increased linearly as the corresponding odor was approached (<xref ref-type="fig" rid="fig3">Figure 3C</xref>). This linear increase in activity can be attributed entirely to an increase in afferent input (<xref ref-type="fig" rid="fig3">Figure 3D</xref>) because the net recurrent input remained constant due to E/I balance (<xref ref-type="fig" rid="fig3">Figure 3E</xref>). In <italic>Scaled</italic> networks, assembly firing rates increased massively as the learned odor was approached (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), reflecting a large increase in recurrent excitation (<xref ref-type="fig" rid="fig3">Figure 3D–F</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). In <italic>Tuned</italic> networks, in contrast, assembly firing rates increased only modestly due to a slightly larger increase in E than I currents near the learned odor (<xref ref-type="fig" rid="fig3">Figure 3D–F</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1C</xref>). Hence, the modest amplification by assemblies of <italic>Tuned</italic> networks involves an ‘imperfect’ synaptic balance.</p></sec><sec id="s2-4"><title>Geometry of activity patterns in networks with E/I assemblies</title><p>We next examined how E/I assemblies transform the geometry of neuronal representations, that is their organization in a state space where each axis represents the activity of one neuron or one pattern of neural covariance (<xref ref-type="bibr" rid="bib16">Chung and Abbott, 2021</xref>; <xref ref-type="bibr" rid="bib30">Gallego et al., 2017</xref>; <xref ref-type="bibr" rid="bib48">Langdon et al., 2023</xref>). To address this general question, we created an odor subspace and examined its transformation by pDp<sub>sim</sub>. The subspace consisted of a set of OB activity patterns representing four uncorrelated pure odors and mixtures of these pure odors. Pure odors were assigned to the corners of a square and mixtures were generated by selecting active mitral cells from each of the pure odors with probabilities depending on the relative distances from the corners (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, Methods). Correlations between OB activity patterns representing pure odors and mixtures decreased approximately linearly as a function of distance in the subspace (<xref ref-type="fig" rid="fig4">Figure 4B</xref>). The odor subspace therefore represented a hypothetical olfactory environment with four odor sources at the corners of a square arena and concentration gradients within the arena. Locations in the odor subspace were visualized by the color code depicted in <xref ref-type="fig" rid="fig4">Figure 4A</xref>.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Geometry of odor representations in pDp<sub>sim</sub>.</title><p>(<bold>A</bold>) Odor subspace delineated by two learned (L, M) and two novel (N, O) pure odors at the vertices of a virtual square. Pixels within the square represent odor mixtures. (<bold>B</bold>) Left: Pearson correlations between olfactory bulb (OB) activity patterns defining pure odors. Right: Correlation between one pure odor (L; top left vertex) and all other odors. The odor from one vertex gradually morphs into the odor from another vertex. (<bold>C</bold>) Projection of activity patterns in the OB onto the first two principal components (PCs). Colors represent patterns in the odor subspace as shown in (<bold>A, D</bold>). Projection of activity patterns in pDp<sub>sim</sub> in response to the odor subspace onto the first two PCs. Representative examples of different pDp networks. (<bold>E</bold>) Density plot showing distribution of data points and demonstrating clustering at distinct locations in PC space for <italic>Scaled I</italic> networks. (<bold>F</bold>) Quantification of dimensionality of neural activity by the participation ratio: activity evoked by novel odors and related mixtures (left), activity evoked by learned odors and related mixtures (center), and activity evoked by all stimuli (right). Each data point represents one network (<italic>n</italic> = 8, mean ± SD); dotted line represents the participation ratio of OB activity. (<bold>G</bold>) Variance along the first 40 PCs extracted from activity patterns in different networks. Insets: Variance along PCs 200–400. (<bold>H</bold>) Angles between edges connecting a pure odor response and related versions thereof (inset). The analysis was performed using the first 400 PCs, which explained &gt;75% of the variance in all networks. <italic>n</italic> = 168 angles per pure odor in each of 8 networks (Methods). Similar results were obtained in the full-dimensional space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Transformations and dimensionality of activity patterns: additional results.</title><p>(<bold>A</bold>) Spike raster of 80 E neurons in response to a trajectory within the odor subspace (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) including one pure learned and one pure novel odor. (<bold>B</bold>) Projection of activity patterns representing the odor subspace (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) onto the first two principal components (PCs) of the corresponding <italic>rand</italic> networks (representative examples of one network each). (<bold>C</bold>) Scree plot for the principal component analysis (PCA) results shown in <xref ref-type="fig" rid="fig4">Figure 4D</xref>. (<bold>D</bold>) Error in the reconstruction of odor-evoked activity patterns as a function of the number of PCs. Euclidean distances between all pairs of activity patterns were calculated in the full-dimensional state space (Df) and in reduced-dimensional embedding spaces (Dl). The reconstruction error was defined as 1 – (correlation between Df and Dl). (<bold>E</bold>) Participation ratio of the neural activity sampled from different numbers of neurons (50 iterations). (<bold>F</bold>) Loading plot: Contribution of neurons to the first two PCs of a <italic>rand</italic> and a <italic>Tuned E+I</italic> network (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). Each line represents one neuron and neurons that are part of the assemblies representing the two learned odors are color-coded in magenta. (<bold>G</bold>) For each network and PC, the 100 E neurons with the highest absolute loadings were selected and grouped into three categories: neurons part of the two assemblies representing the learned odors, neurons part of the two ‘pseudo-assemblies’ representing the two novel odors, and the remaining neurons (non-assembly).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Geometry and dynamics in a simple model with equal parameters for excitation and inhibition.</title><p>(<bold>A</bold>) Firing rates averaged over all E neurons as a function of <italic>α</italic> (mean across five networks). (<bold>B</bold>) Mean firing rate in response to learned odors or novel odors. Each data point represents one network-odor pair (<italic>n</italic> = 5 networks, 10 odors). Mean ± SD. (<bold>C</bold>) Spike raster plots showing responses of 50 E neurons to 2 odors (O1: novel odor; O2: learned odor). (<bold>D</bold>) Left: Synaptic conductance contributed by afferents from the olfactory bulb (OB) during odor stimulation (<italic>g</italic><sub>OE</sub>). Middle: Total odor-evoked synaptic conductance <italic>g</italic><sub>syn</sub>. Right: Percentage of E input contributed by recurrent connections during odor stimulation. Each dot represents one network (average over 10 odors); <italic>n</italic> = 5 networks. (<bold>E</bold>) Co-tuning: correlation between time-averaged E and I conductances in response to different odors, average across neurons. (<bold>F</bold>) Amplification within assemblies, calculated as the ratio between mean firing rates in response to learned odors averaged across assembly neurons in a structured network (<italic>Scaled I</italic> or <italic>Tuned E+I</italic>) and the same neurons in the corresponding <italic>rand</italic> network. (<bold>G</bold>) Projection of activity patterns in pDp<sub>sim</sub> in response to the odor subspace described in <xref ref-type="fig" rid="fig4">Figure 4</xref> onto the first two principal components (PCs). Representative examples of different networks. (<bold>H</bold>) Participation ratio (similar to <xref ref-type="fig" rid="fig4">Figure 4F</xref>): activity evoked by novel odors and related mixtures (left), activity evoked by learned odors and related mixtures (center), and activity evoked by all stimuli (right).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig4-figsupp2-v1.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Manipulating amplification within assemblies.</title><p>(<bold>A</bold>) Overview of modified pDp<sub>sim</sub> networks that were generated to manipulate amplification of activity within assemblies. (1) <italic>Tuned[adjust]</italic>: in existing <italic>Tuned E+I</italic> networks, the probability of I-to-E connections (<italic>β</italic>) is further increased until the mean activity within assemblies is similar to the activity of the corresponding pseudo-assemblies in <italic>rand</italic> networks. (2) <italic>Scaled[adjust]</italic>: Parameters <italic>α</italic> and <italic>χ</italic> are set such that the mean activity within assemblies matches the mean activity in assemblies of <italic>Tuned E+I</italic> networks. (3) <italic>Excitability</italic>: in rand networks, during presentation of odor L or M, the resting conductance <italic>g</italic><sub>rest</sub> of the corresponding pseudo-assembly neurons is decreased until their mean activity matches the mean activity of the <italic>Tuned E+I</italic> assembly. During mixture presentation, <italic>g</italic><sub>rest</sub> decreases linearly with the concentration of the odor. (<bold>B</bold>) Mean firing rate in response to learned or novel odors (see <xref ref-type="fig" rid="fig2">Figure 2F</xref>). Each data point represents one network-odor pair (<italic>n</italic> = 8 networks, 10 odors). In <italic>Scaled[adjust],</italic> mean firing rates for novel odors were lower than for learned odors. (<bold>C</bold>) Amplification within assemblies, calculated as the ratio between mean firing rates in response to learned odors averaged across assembly neurons (structured network) and the corresponding pseudo-assembly neurons (<italic>rand</italic> network); see <xref ref-type="fig" rid="fig2">Figure 2G</xref>. (<bold>D</bold>) Quantification of E/I co-tuning. Left: Correlation between time-averaged E and I conductances in response to different odors, average across neurons. Right: Ratio of dispersion of joint conductances along balanced and counter-balanced axes (see <xref ref-type="fig" rid="fig2">Figure 2H, I</xref>). Note that E/I co-tuning remained weak in <italic>Scaled[adjust]</italic> networks, inconsistent with experimental observations. (<bold>E</bold>) Sum of recurrent inhibitory and excitatory currents averaged over assembly or non-assembly neurons (see <xref ref-type="fig" rid="fig3">Figure 3E</xref>). In <italic>Tuned E+I</italic> and <italic>Scaled[adjust]</italic> networks, recurrent E inputs in assemblies slightly exceeded I inputs in the vicinity of learned odors. In <italic>Tuned[adjust],</italic> recurrent E and I inputs canceled out. (<bold>F</bold>) Quantification of dimensionality of neural activity by the participation ratio (see <xref ref-type="fig" rid="fig4">Figure 4F</xref>): activity evoked by the odor subspace. Each data point represents one network (<italic>n</italic> = 8). Mean ± SD. (<bold>G</bold>) Projection of activity patterns in response to the odor subspace onto the first two principal components (PCs) (representative examples of different networks; see <xref ref-type="fig" rid="fig4">Figure 4D</xref>). (<bold>H</bold>) The Mahalanobis distance <italic>d</italic><sub>M</sub> between one activity vector (<italic>v</italic>) and reference classes (<italic>Q</italic>) (Methods and <xref ref-type="fig" rid="fig5">Figure 5B</xref>). <italic>d</italic><sub>M</sub> was computed based on activity across subsets of 80 E neurons drawn from the four (pseudo-) assemblies with equal probability.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig4-figsupp3-v1.tif"/></fig></fig-group><p>To examine how pDp<sub>sim</sub> transforms this odor subspace we projected time-averaged activity patterns onto the first two principal components (PCs). As expected, the distribution of OB activity patterns in PC space closely reflected the square geometry of the subspace (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). This geometry was largely preserved in the output of <italic>rand</italic> networks, consistent with the notion that random networks tend to maintain similarity relationships between input patterns (<xref ref-type="fig" rid="fig4">Figure 4D</xref>; <xref ref-type="bibr" rid="bib4">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib52">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib74">Schaffer et al., 2018</xref>). We next examined outputs of <italic>Scaled</italic> or <italic>Tuned</italic> networks containing 15 assemblies, two of which were aligned with pure odors (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>). The four odors delineating the odor subspace therefore consisted of two learned and two novel odors. In <italic>Scaled I</italic> networks, odor inputs were discretely distributed between three locations in state space representing the two learned odors and residual odors, consistent with the expected discrete attractor states (<xref ref-type="fig" rid="fig4">Figure 4D, E</xref>). <italic>Tuned</italic> networks, in contrast, generated continuous representations of the odor subspace (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). The geometry of these representations was distorted in the vicinity of learned odors, which were more distant from most mixtures than novel odors. These geometric transformations were less obvious when activity patterns of <italic>Tuned</italic> networks were projected onto the first two PCs extracted from <italic>rand</italic> networks (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). Hence, E/I assemblies introduced local curvature into the coding space that partially separated learned from novel odors without disrupting the continuity of the subspace representation.</p><p>The curvature of the activity manifold in <italic>Tuned</italic> networks suggests that E/I assemblies confine population activity along specific dimensions of the state space. To test this hypothesis, we first quantified the dimensionality of odor-evoked activity by the participation ratio, a measure that depends on the eigenvalue distribution of the pairwise neural covariance matrix (<xref ref-type="bibr" rid="bib2">Altan et al., 2021</xref>) (Methods). As expected, dimensionality was highest in <italic>rand</italic> networks and very low in <italic>Scaled I</italic> networks, reflecting the discrete attractor states (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). In <italic>Tuned</italic> networks, dimensionality was high compared to <italic>Scaled I</italic> networks but lower than in <italic>rand</italic> networks (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). The same trend was observed when we sampled data from a limited number of neurons to mimic experimental conditions and when we used other measures of dimensionality (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C–E</xref>). Furthermore, when restraining the analysis to activity evoked by novel odors and related mixtures, dimensionality was similar between <italic>rand</italic> and <italic>Tuned</italic> networks (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). These observations support the hypothesis that E/I assemblies locally constrain neuronal activity onto lower-dimensional subspaces (manifolds), consistent with recent findings showing effects of specific circuit motifs on the dimensionality of neural activity (<xref ref-type="bibr" rid="bib17">Dahmen et al., 2023</xref>; <xref ref-type="bibr" rid="bib63">Recanatesi et al., 2019</xref>).</p><p>We further tested this hypothesis by examining the local geometry of activity patterns around representations of learned and novel odors. If E/I assemblies locally confine activity onto manifolds, small changes of input patterns should modify output patterns along preferred dimensions near representations of learned but not novel odors. To test this prediction, we selected sets of input patterns including each pure odor and the seven most closely related mixtures. We then quantified the variance of the projections of their corresponding output patterns onto the first 40 PCs (<xref ref-type="fig" rid="fig4">Figure 4G</xref>). In <italic>Tuned</italic> networks, this variance decreased only slightly as a function of PC rank for activity patterns related to novel odors, indicating that patterns varied similarly in all directions. For patterns related to learned odors, in contrast, the variance was substantially higher in the direction of the first few PCs, implying variation along preferred dimensions. In addition, we measured the distribution of angles between edges connecting activity patterns representing pure odors and their corresponding related mixtures in PC space (<xref ref-type="fig" rid="fig4">Figure 4H</xref>, inset; Methods; <xref ref-type="bibr" rid="bib75">Schoonover et al., 2021</xref>). Angles were narrowly distributed around 1 rad in <italic>rand</italic> networks but smaller in the vicinity of learned patterns in <italic>Tuned</italic> networks (<xref ref-type="fig" rid="fig4">Figure 4H</xref>).</p><p>Activity may be constrained non-isotropically by amplification along a subset of dimensions, by inhibition along other dimensions, or both. E neurons participating in E/I assemblies had large loadings on the first two PCs (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1F, G</xref>) and responded to learned odors with increased firing rates as compared to the mean rates in <italic>Tuned E+I</italic> and <italic>rand</italic> networks. Firing rates of the remaining neurons, in contrast, were slightly lower than the corresponding mean rates in <italic>rand</italic> networks. Consistent with these observations, the variance of activity projected onto the first few PCs was higher in <italic>Tuned E+I</italic> than in <italic>rand</italic> networks (<xref ref-type="fig" rid="fig4">Figure 4G</xref>) while the variance along higher-order PCs was lower (<xref ref-type="fig" rid="fig4">Figure 4G</xref>, inset). These observations indicate that local activity manifolds are delineated both by amplification of activity along preferred directions and by suppression of activity along other dimensions.</p><p>To obtain first insights into network mechanisms underlying effects of E/I assemblies on manifold geometry we simulated additional networks. Equalizing biophysical parameters of E and I neurons had little impact on network behavior (<xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>; Methods), indicating that observed effects do not depend on specific biophysical differences between E and I neurons. We next explored the hypothesis that the modest amplification of activity within E/I assemblies contributes to the geometry of neural manifolds in pDp<sub>sim</sub>. First, we adjusted the number of I-to-E connections (<italic>β</italic>) within assemblies of <italic>Tuned E+I</italic> networks to prevent amplification of activity within assemblies during presentation of learned odors (<italic>Tuned[adjust]</italic>, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A–E</xref>). In contrast to <italic>Tuned E+I</italic> networks with control parameters, we observed neither a curvature of activity manifolds in PC space nor an asymmetry of the Malalanobis distance <italic>(d</italic><sub>M</sub>; another signature of <italic>Tuned</italic> networks, see below) in these modified networks (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3F–H</xref>). Second, we reduced recurrent amplification in assemblies of <italic>Scaled I</italic> networks by decreasing <italic>α</italic> and <italic>χ</italic> (<italic>Scaled[adjust]</italic>, <xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A–E</xref>). This resulted in distorted activity manifolds and an asymmetric increase in <italic>d</italic><sub>M</sub>, similar to observations in <italic>Tuned</italic> networks (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3F–H</xref>; see below). Third, we artificially amplified activity in pseudo-assemblies of <italic>rand</italic> networks through concentration-dependent modifications of excitability instead of connectivity changes (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3A</xref>). This manipulation increased the mean population activity as compared to control <italic>rand</italic> networks, but it also induced geometric modifications similar to <italic>Tuned</italic> networks, although the manifold curvature in PC space appeared less pronounced (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3B–H</xref>). Hence, different manipulations had consistent effects on manifold geometry despite different effects on mean firing rates: signatures of manifold geometry observed in <italic>Tuned</italic> networks were mimicked by manipulations producing a modest, input-dependent amplification of firing rates in (pseudo-)assemblies but abolished by manipulations that suppressed this amplification. These results indicate that the modest amplification of activity within assemblies contributes to the geometry of odor representations in pDp<sub>sim</sub>. However, further analyses are required to explore potential contributions of other processes and to understand the underlying mechanisms in more detail.</p></sec><sec id="s2-5"><title>Pattern classification by networks with E/I assemblies</title><p>The lack of discrete attractor states raises the question how transformations of activity patterns by <italic>Tuned</italic> networks affect pattern classification. To quantify the association between an activity pattern and a class of patterns representing a given odor we computed the Mahalanobis distance (<italic>d</italic><sub>M</sub>). This measure quantifies the distance between the pattern and the class center, taking into account covariation of neuronal activity within the class. In bidirectional comparisons between patterns from different classes, the mean <italic>d</italic><sub>M</sub> may be asymmetric if neural covariance differs between classes. We first quantified <italic>d</italic><sub>M</sub> between representations of pure odors based on activity patterns across 80 E neurons drawn from the corresponding (pseudo-) assemblies. <italic>d</italic><sub>M</sub> was asymmetrically increased in <italic>Tuned E+I</italic> networks as compared to <italic>rand</italic> networks. Large increases were observed for distances between patterns related to learned odors and reference classes representing novel odors (<xref ref-type="fig" rid="fig5">Figure 5A, B</xref>). In the other direction, increases in <italic>d</italic><sub>M</sub> were smaller. Moreover, distances between patterns related to novel odors were almost unchanged (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). Further analyses showed that increases in <italic>d</italic><sub>M</sub> in <italic>Tuned E+I</italic> networks involved both increases in the Euclidean distance between class centers and non-isotropic scaling of intra-class variability (<xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>). The geometric transformation of odor representations by E/I assemblies therefore particularly enhanced the discriminability of patterns representing learned odors.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Distance relationships and classification of odor representations.</title><p>(<bold>A</bold>) Odor-evoked activity patterns used as class distributions and vectors (<bold>B</bold>) or training and test sets (<bold>C</bold>). Same odor subspace as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. (<bold>B</bold>) Top: Schematic illustration of Mahalanobis distance <italic>d</italic><sub>M</sub>. Bottom: <italic>d</italic><sub>M</sub> between one activity vector (<italic>v</italic>) and reference classes (<italic>Q</italic>) in <italic>rand</italic> and <italic>Tuned E+I</italic> networks. <italic>d</italic><sub>M</sub> was computed based on the activity patterns of 80 E neurons drawn from the four (pseudo-) assemblies. Average of 50 draws. Note that <italic>d</italic><sub>M</sub> between patterns related to a learned odor and non-matching reference classes was higher in <italic>Tuned E+I</italic> networks. (<bold>C</bold>) Pattern classification probability quantified by quadratic discriminant analysis (QDA). <italic>P</italic><sub>Target</sub> quantifies the probability that an activity pattern from the test set (odor mixtures, see <bold>A</bold>) is assigned to a target class from the training set (pure or closely related odor; see <bold>A</bold>). Left: Classification probability as a function of the similarity (Pearson correlation) between the test and target odors in the olfactory bulb (OB) (input patterns). Note enhanced classification probability for patterns evoked by odors similar to learned odors in <italic>Tuned E+I</italic> networks. Right: Classification probability for patterns similar to the training set (see <bold>A</bold>). Each data point represents one network (<italic>n</italic> = 8, mean ± SD; Wilcoxon signed-rank test, **: p &lt; 0.01). (<bold>D</bold>) The odor subspace of <xref ref-type="fig" rid="fig4">Figure 4</xref> was subdivided in classes reflecting different concentrations of one of the pure odors (Methods). Odor concentration is defined here as the percentage of the 150 mitral cells representing a given odor that are activated. (<bold>E</bold>) The concentration of a given odor was regressed against the mean firing rates of a subset of neurons (Methods). Mean square error (squared difference between the actual concentration and the estimated concentration). (<bold>F</bold>) Accuracy of a linear support vector machine (SVM) in predicting the concentration of a novel (left) or learned odor (right) in a mixture. Each data point represents one network (<italic>n</italic> = 8, mean ± SD).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig5-v1.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Further analyses of pattern distances: uncorrelated odors.</title><p>Differences in <italic>d</italic><sub>M</sub> between <italic>rand</italic> and <italic>Tuned</italic> networks may involve differences in the distance between class centers and/or differences in intra-class variability. To dissect the contributions of these effects we compared three distance measures:</p><p>(1) The Euclidean distance between class centers:</p><p><inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">E</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>(2) The mean Euclidean distance between patterns of one class and the center of another class (<inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>):</p><p><inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1..</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>(3) The Mahalanobis distance d<sub>M</sub>:</p><p><inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>n</mml:mi></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1..</mml:mn><mml:mi>n</mml:mi></mml:mrow></mml:munder><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mstyle></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Here, <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>V</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf6"><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>Q</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the centers (average patterns) of classes <italic>V</italic> and <italic>Q</italic>, respectively; <italic>v</italic> is a vector from <italic>V</italic> = [<italic>v</italic><sub>1</sub>, <italic>v</italic><sub>2</sub>, …, <italic>v</italic><sub><italic>n</italic></sub>], and <inline-formula><mml:math id="inf7"><mml:msup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:math></inline-formula> is the inverse of the covariance matrix of the neuronal population within reference class <italic>Q</italic>. Note that <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> corresponds to <italic>d</italic><sub>M</sub> without normalization by variability (covariance). (<bold>A</bold>) Analysis of <italic>d</italic><sub>E</sub>. Left: <italic>d</italic><sub>E</sub> in <italic>rand</italic> and <italic>Tuned E+I</italic> networks based on 80 E neurons drawn from (pseudo-) assemblies. Right: Same based on 80 E neurons drawn from the entire population. Note that <italic>d</italic><sub>E</sub> between learned and other odors was increased in <italic>Tuned E+I</italic> networks as compared to <italic>rand</italic> networks, particularly when neurons were drawn from assemblies. (<bold>B</bold>) Equivalent plots for <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> Note that distances were increased nearly symmetrically, similar to <italic>d</italic><sub>E</sub>. (<bold>C</bold>) Equivalent plots for <italic>d</italic><sub>M</sub> (same plots as in <xref ref-type="fig" rid="fig5">Figure 5B</xref>). Note that <italic>d</italic><sub>M</sub> was increased asymmetrically. These observations show that the changes in <italic>d</italic><sub>M</sub> relative to <italic>rand</italic> networks involved an increase in the distance between class centers (<italic>d</italic><sub>E</sub>) and a non-isotropic change in intra-class variability (comparison between <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <italic>d</italic><sub>M</sub>). These effects were prominent when E neurons were drawn from assemblies. An important contribution to the increase in <italic>d</italic><sub>M</sub> in the direction from learned odors to reference classes representing novel odors was made by the increased distance between class centers. In the other direction, <italic>d</italic><sub>M</sub> was smaller, implying that variability in the reference class was higher. Nonetheless, variability in the relevant direction did not fully counteract the increased distance between class centers in <italic>Tuned E+I</italic> networks. As a consequence, <italic>d</italic><sub>M</sub> was still increased slightly relative to the corresponding <italic>rand</italic> networks. Most of these effects were still observed, albeit weakly, when E neurons were drawn from the whole population.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig5-figsupp1-v1.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 2.</label><caption><title>Classification of odor representations (template matching).</title><p>(<bold>A</bold>) Pattern classification probability quantified by template matching (see Methods). <italic>P</italic><sub>Target</sub> quantifies the probability that an activity pattern from the test set is assigned to a target class from the training set (see <xref ref-type="fig" rid="fig5">Figure 5A</xref>) and is plotted as a function of the similarity (Pearson correlation) between the test and target odor patterns in the olfactory bulb (OB). Unlike quadratic discriminant analysis (QDA), template matching can also be applied to outputs of <italic>Scaled</italic> networks. Note enhanced classification probability for patterns evoked by odors similar to learned odors in <italic>Tuned E+I</italic> networks, and to a lesser extent in <italic>Scaled</italic> networks. (<bold>B</bold>) The dataset was divided into two categories, depending on the trajectory in the odor subspace (insets; <italic>n</italic> = 484 activity patterns each), and template matching was performed again on each of the generated datasets. Note that learned odors were presented further apart in time in the top panels than in the bottom panels. Left: <italic>P</italic><sub>Target</sub> as a function of similarity in the OB, see panel A. Right: Classification probability for patterns similar to the training set (see A). Note that odor classification by <italic>Scaled</italic> networks depends on the trajectories in the odor subspace. (<bold>C</bold>) Projection of activity patterns in response to the odor subspace onto the first two principal components (PCs) in <italic>Scaled</italic> networks. Odor representations depend on stimulus history: note the reduced separation between representations of different odor classes during trajectories morphing one learned odor into another learned odor (arrows). This effect arises from the attractor dynamics of the networks.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig5-figsupp2-v1.tif"/></fig></fig-group><p>To further analyze pattern classification, we performed multi-class quadratic discriminant analysis (QDA), an extension of linear discriminant analysis for classes with unequal covariances. Using QDA, we determined the probabilities that an activity pattern evoked by a mixture is classified as being a member of each of four classes representing the pure odors, thus mimicking a four-way forced choice odor classification task. The four classes were defined by the distributions of activity patterns evoked by the pure and closely related odors. We then examined the classification probability of patterns evoked by mixtures with respect to a given class as a function of the similarity between the mixture and the corresponding pure odor (target) in the OB. As expected, the classification probability increased with similarity. Furthermore, in <italic>Tuned E+I</italic> networks, the classification probability of mixtures similar to a pure odor was significantly higher when the pure odor was learned (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). Hence, E/I assemblies facilitated the classification of inputs related to learned patterns. Pattern classification by QDA could not be applied to outputs of <italic>Scaled</italic> networks because intra-class variability was too low (Methods).</p><p>E/I assemblies also improved pattern classification when activity patterns were classified based on correlations to template vectors representing the four odor classes (Methods; <xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>). This analysis further revealed hysteresis effects in <italic>Scaled</italic> but not in <italic>Tuned</italic> networks that impaired pattern classification (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>).</p><p>When neuronal subsets were randomly drawn not from assemblies but from the entire population, <italic>d</italic><sub>M</sub> was generally lower (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). These results indicate that assembly neurons convey higher-than-average information about learned odors. Together, these observations imply that pDp<sub>sim</sub> did not function as a categorical classifier but nonetheless supported the classification of learned odors, particularly when the readout focused on assembly neurons. Conceivably, classification may be further enhanced by optimizing the readout strategy, for example, by a learning-based approach. However, modeling biologically realistic readout mechanisms requires further experimental insights into the underlying circuitry.</p></sec><sec id="s2-6"><title>Additional computational properties of networks with E/I assemblies</title><p>E/I assemblies may not only support discrete classification but also other tasks. We hypothesized that continuous manifolds provide a distance metric to quantitatively evaluate the relatedness between an input and other stimuli. The geometric modifications induced by E/I assemblies may specifically provide a metric to evaluate distances between a new sensory input and previously learned stimuli. To explore these hypotheses, we first used multilinear regression to predict the concentration of a pure odor in subspace mixtures (<xref ref-type="fig" rid="fig4">Figure 4A</xref>) based on population activity. Highest prediction errors were observed for <italic>Scaled I</italic> networks, presumably because their discontinuous outputs poorly represent gradual concentration differences (<xref ref-type="fig" rid="fig5">Figure 5D, E</xref>). Lowest errors were observed for concentration estimates of learned odors in <italic>Tuned</italic> networks. We further trained support vector machines to estimate the concentrations of pure odors in the subspace mixtures and again found highest accuracy for learned odors in <italic>Tuned</italic> networks (<xref ref-type="fig" rid="fig5">Figure 5F</xref>). These results show that networks with E/I assemblies enable the quantitative analysis of selected stimulus components in the presence of distractors, presumably due to the geometric properties of the underlying neural manifold.</p><p>We next examined whether E/I assemblies can stabilize activity when multiple memories are successively added into a network, as in a continual learning process. In the absence of precise E/I balance, the divergence of firing rates and, thus, the risk of network instability are expected to increase as assemblies accumulate. <italic>Tuned</italic> networks may be more resistant against these risks, particularly when assemblies overlap, because activity is controlled more precisely within each additional assembly. To test this hypothesis, we examined responses of <italic>Tuned E+I</italic> and <italic>Scaled I</italic> networks to an additional odor subspace where four of the six pairwise correlations between the pure odors were clearly positive (range, 0.27–0.44; <xref ref-type="fig" rid="fig6">Figure 6A</xref>). We then compared networks with 15 randomly created assemblies to networks with two additional assemblies representing two of the correlated pure odors. These two additional assemblies had, on average, 16% of neurons in common due to the similarity of the odors. In <italic>Tuned</italic> networks, introducing additional overlapping assemblies selectively increased the discriminability of the corresponding odor representations in PC space (<xref ref-type="fig" rid="fig6">Figure 6B, C</xref>), as observed for uncorrelated assemblies (<xref ref-type="fig" rid="fig4">Figure 4D</xref>), but population firing rates remained almost unchanged (<xref ref-type="fig" rid="fig6">Figure 6B, C</xref>). In <italic>Scaled I</italic> networks, in contrast, creating two additional memories resulted in a convergence of the corresponding representations in PC space and in a substantial increase in firing rates, particularly in response to the learned and related odors (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). These observations are consistent with the assumption that precise balance in E/I assemblies protects networks against instabilities during continual learning, even when memories overlap. We further observed that, in this regime of higher pattern similarity, <italic>d</italic><sub>M</sub> was again increased upon learning, particularly between learned odors and reference classes representing other odors (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>). E/I assemblies therefore consistently increased <italic>d</italic><sub>M</sub> in a directional manner under different conditions.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Representation of correlated patterns and resilience against additional memories.</title><p>(<bold>A</bold>) Subspace delineated by four positively correlated odors (see Methods). Top: Correlations between pure odors. Bottom: Projection of olfactory bulb (OB) activity patterns onto the first two principal components (PCs). (<bold>B</bold>) Firing rates (top) and PC projection of output activity of a <italic>Tuned E+I</italic> network with 15 E/I assemblies that did not represent any of the four pure odors of the subspace. (<bold>C</bold>) Firing rates (top) and PC projection of output activity (bottom) after creation of two additional assemblies representing two of the pure odors (Y and Z). Left: <italic>Tuned E+I</italic> network. Right: <italic>Scaled I</italic> network. Note that in the <italic>Scaled I</italic> network, but not in the <italic>Tuned E+I</italic> network, firing rates evoked by newly learned odors were increased and patterns evoked by these odors were not well separated in PC space.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Further analyses of pattern distances: correlated odors.</title><p>(<bold>A–C</bold>) Pattern distances in <italic>Tuned E+I networks</italic> with (right) and without (left) two additional assemblies representing Y and Z (see <xref ref-type="fig" rid="fig6">Figure 6</xref>) based on 80 E neurons drawn from assemblies. Similar analysis as in <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>, but for the odor subspace described in <xref ref-type="fig" rid="fig6">Figure 6</xref>. (<bold>A</bold>) <italic>d</italic><sub>E</sub>. (<bold>B</bold>) <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi mathvariant="normal">E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>C</bold>) <italic>d</italic><sub>M</sub>. (<bold>D</bold>) Pattern classification probability quantified by quadratic discriminant analysis (QDA). <italic>P</italic><sub>Target</sub> quantifies the probability that an activity pattern from the test set is assigned to a target class from the training set (pure or closely related odor; see also <xref ref-type="fig" rid="fig5">Figure 5A, C</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig6-figsupp1-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><sec id="s3-1"><title>A precisely balanced memory network constrained by pDp</title><p>Autoassociative memory networks map inputs onto output patterns representing learned information. Classical models proposed this mapping to be accomplished by discrete attractor states that are defined by assemblies of E neurons and stabilized by global homeostatic inhibition. However, as seen in <italic>Scaled I</italic> networks, global inhibition is insufficient to maintain a stable, biologically plausible firing rate distribution. This problem can be overcome by including I neurons in assemblies, which leads to more precise synaptic balance. To explore network behavior in this regime under biologically relevant conditions we built a spiking network model constrained by experimental data from pDp. The resulting <italic>Tuned</italic> networks reproduced additional experimental observations that were not used as constraints including irregular firing patterns, lower output than input correlations, and the absence of persistent activity. Hence, pDp<sub>sim</sub> recapitulated characteristic properties of a biological memory network with precise synaptic balance.</p></sec><sec id="s3-2"><title>Neuronal dynamics and representations in precisely balanced memory networks</title><p>Simulated networks with global inhibition showed attractor dynamics and pattern completion, consistent with classical attractor memory. However, the distribution of firing rates broadened as connection density within assemblies increased, resulting in unrealistically high (low) rates inside (outside) assemblies and, consequently, in a loss of synaptic balance. Hence, global inhibition was insufficient to stabilize population activity. In networks with E/I assemblies, in contrast, firing rates remained within a realistic range and the balanced state was maintained. Such <italic>Tuned</italic> networks showed no discrete attractor states but transformed the geometry of the coding space by confining activity to continuous manifolds near representations of learned inputs. This observation is consistent with the hypothesis that E/I co-tuning decreases the probability of multistable attractor states as compared to balanced networks with global inhibition or lateral inhibition between assemblies (<xref ref-type="bibr" rid="bib11">Boerlin et al., 2013</xref>; <xref ref-type="bibr" rid="bib37">Hennequin et al., 2018</xref>; <xref ref-type="bibr" rid="bib88">Wu and Zenke, 2021</xref>).</p><p>Geometric transformations in <italic>Tuned</italic> networks may be considered as intermediate between two extremes: (1) geometry-preserving transformations as, for example, performed by many random networks (<xref ref-type="bibr" rid="bib4">Babadi and Sompolinsky, 2014</xref>; <xref ref-type="bibr" rid="bib52">Marr, 1969</xref>; <xref ref-type="bibr" rid="bib74">Schaffer et al., 2018</xref>), and (2) discrete maps as, for example, generated by discrete attractor networks (<xref ref-type="bibr" rid="bib25">Freeman and Skarda, 1985</xref>; <xref ref-type="bibr" rid="bib38">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="bib41">Khona and Fiete, 2022</xref>; <xref ref-type="fig" rid="fig7">Figure 7</xref>). We found that transformations became more discrete map-like when amplification within assemblies was increased and precision of synaptic balance was reduced. Likewise, decreasing amplification in assemblies of <italic>Scaled</italic> networks changed transformations toward the intermediate behavior, albeit with broader firing rate distributions than in <italic>Tuned</italic> networks (<xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3B</xref>). These observations indicate that a modest amplification of activity within assemblies contributes to geometric modifications of activity manifolds in <italic>Tuned</italic> networks, but other factors such as structured inhibition may also contribute. Hence, further analyses are required to obtain a deeper mechanistic understanding of manifold geometry in networks with E/I assemblies. Nonetheless, our results suggest that precise synaptic balance may generally favor intermediate over discrete transformations because this regime tends to linearize the relationship between the mean input and output firing rates of neuronal populations (<xref ref-type="bibr" rid="bib5">Baker et al., 2020</xref>; <xref ref-type="bibr" rid="bib18">Denève and Machens, 2016</xref>).</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Schematic of geometric transformations.</title><p>(<bold>A</bold>) Randomly connected networks tend to preserve the geometry of coding space. Such networks can support neuronal computations, for example, by projecting activity patterns in a higher-dimensional coding space for pattern classification. (<bold>B</bold>) We found that balanced networks with E/I assemblies transform the geometry of representations by locally restricting activity onto manifolds. These networks stored information about learned inputs while preserving continuity of the coding space. Such a geometry may support fast classification, continual learning and cognitive computations. Note that the true manifold geometry cannot be visualized appropriately in 2D because activity was ‘focused’ in different subsets of dimensions at different locations of coding space. As a consequence, the dimensionality of activity remained substantial. (<bold>C</bold>) Neuronal assemblies without precise balance established discrete attractor states, as observed in memory networks that store information as discrete items. Networks establishing locally defined activity manifolds (<bold>B</bold>) may thus be considered as intermediates between networks generating continuous representations without memories (<bold>A</bold>) and classical memory networks with discrete attractor dynamics (<bold>C</bold>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-96303-fig7-v1.tif"/></fig><p>E/I assemblies increased variability of population activity along preferred directions of state space and reduced its dimensionality in comparison to <italic>rand</italic> networks. Nonetheless, dimensionality remained high compared to <italic>Scaled</italic> networks with discrete attractor states. These observations indicate that geometric transformations in <italic>Tuned</italic> networks involved (1) a modest amplification of activity in one or a few directions aligned to the assembly, and (2) a modest reduction of activity in other directions. E/I assemblies therefore created a local curvature of coding space that ‘focused’ activity in a subset of dimensions and, thus, stored information in the geometry of coding space.</p><p>As E/I assemblies were small relative to the total size of the E neuron population, stored information may be represented predominantly by small neuronal subsets. Consistent with this hypothesis, <italic>d</italic><sub>M</sub> was increased and the classification of learned inputs by QDA was enhanced when activity was read out from subsets of assembly neurons as compared to random neuronal subsets. Moreover, signatures of pattern completion were found in the activity of assemblies but not in global pattern correlations. The retrieval of information from networks with small E/I assemblies therefore depends on the selection of informative neurons for readout. Unlike in networks with global attractor states, signatures of memory storage may thus be difficult to detect experimentally without specific knowledge of assembly memberships.</p></sec><sec id="s3-3"><title>Computational functions of networks with E/I assemblies</title><p>In theory, precisely balanced networks with E/I assemblies may support pattern classification despite high variability of spike trains and the absence of discrete attractor states (<xref ref-type="bibr" rid="bib18">Denève and Machens, 2016</xref>). Indeed, we found in <italic>Tuned E+I networks</italic> that input patterns were classified successfully by generic classifiers, particularly relative to learned inputs. Analyses based on the Mahalanobis distance <italic>d</italic><sub>M</sub> indicate that classification of learned inputs was enhanced by two effects: (1) local manifolds representing learned odors became more distant from representations of other odors due to a modest increase in firing rates within E/I assemblies, and (2) the concomitant increase in variability was not isotropic, remaining sufficiently low in directions that separated novel from learned patterns. Hence, information contained in the geometry of coding space can be retrieved by readout mechanisms aligned to activity manifolds. Efficient readout mechanisms may thus integrate activity primarily from assembly neurons, as mimicked in our QDA-based pattern classification. This notion is consistent with the finding that the integrated activity of E/I assemblies can be highly informative despite variable firing of individual neurons (<xref ref-type="bibr" rid="bib11">Boerlin et al., 2013</xref>; <xref ref-type="bibr" rid="bib19">Denève et al., 2017</xref>; <xref ref-type="bibr" rid="bib18">Denève and Machens, 2016</xref>). It will thus be interesting to explore how the readout of information from local manifolds could be further optimized.</p><p>Networks with E/I assemblies may also perform computations other than pattern classification. A toy model of continual learning indicates that precise balance can stabilize firing rate distributions when new memories are added, helping to prevent catastrophic failures. Furthermore, we found that continuous manifolds in <italic>Tuned</italic> networks support metric analyses, for example, to determine the relatedness of a (potentially complex) input to previously learned stimuli. Such tasks are not supported by discrete attractor states because information about gradual differences between inputs is lost by pattern completion. E/I assemblies can thus enhance the quantitative analysis of relevant information through changes in manifold geometry.</p><p>Conceivably, E/I assemblies may have further consequences for neuronal computation. Unlike discrete attractor networks, <italic>Tuned</italic> networks do not exhibit persistent activity, suggesting that they mediate fast computations rather than short-term memory functions. Fast classification may, for example, be important to interpret dynamical sensory inputs on a moment-to-moment basis. Moreover, the representation of learned inputs by small neuronal subsets, rather than global activity states, raises the possibility that multiple inputs can be classified simultaneously. Generally, the absence of discrete attractor states indicates that information is not stored in the form of distinct items. Rather, E/I assemblies cause geometric modifications of a continuous coding space that result in an overrepresentation of learned (relevant) inputs at the expense of other stimuli. These pattern transformations may thus contribute to different classification and learning processes by re-formatting and extracting relevant information for further processing by distributed networks. Such a function would be loosely related to computations within layers of artificial neuronal networks and consistent with the notion that piriform cortex is embedded in a larger network comprising multiple telencephalic brain areas (<xref ref-type="bibr" rid="bib33">Haberly, 2001</xref>).</p></sec><sec id="s3-4"><title>Balanced state networks with E/I assemblies as models for olfactory cortex</title><p>Piriform cortex and Dp have been proposed to function as attractor-based memory networks for odors. Consistent with this hypothesis, pattern completion and its modulation by learning has been observed in piriform cortex of rodents (<xref ref-type="bibr" rid="bib6">Barnes et al., 2008</xref>; <xref ref-type="bibr" rid="bib14">Chapuis and Wilson, 2011</xref>). However, odor-evoked firing patterns in piriform cortex and Dp are typically irregular, variable, transient and less reproducible than in the OB even after learning (<xref ref-type="bibr" rid="bib40">Jacobson et al., 2018</xref>; <xref ref-type="bibr" rid="bib61">Pashkovski et al., 2020</xref>; <xref ref-type="bibr" rid="bib75">Schoonover et al., 2021</xref>; <xref ref-type="bibr" rid="bib89">Yaksi et al., 2009</xref>), indicating that activity does not converge onto stable attractor states. Balanced networks with E/I assemblies, in contrast, are generally consistent with these experimental observations. Alternative models for pattern classification in the balanced state include networks endowed with short-term plasticity, which respond to stimuli with an initial amplification phase followed by a tonic inhibition-stabilized state (<xref ref-type="bibr" rid="bib88">Wu and Zenke, 2021</xref>), or mechanisms related to ‘balanced amplification’, which typically generate pronounced activity transients (<xref ref-type="bibr" rid="bib1">Ahmadian and Miller, 2021</xref>; <xref ref-type="bibr" rid="bib59">Murphy and Miller, 2009</xref>). However, it has not been explored whether these models can be adapted to reproduce characteristic features of Dp or piriform cortex.</p><p>Our results generate predictions to test the hypothesis that E/I assemblies establish local manifolds in Dp: (1) odor-evoked population activity should be constrained onto manifolds, particularly in response to learned odors. (2) Learning should increase the magnitude and asymmetry of <italic>d</italic><sub>M</sub> between odor representations. (3) Activity evoked by learned and related odors should exhibit lower dimensionality and more directional variability than activity evoked by novel odors. (4) Careful manipulations of inhibition may unmask assemblies by increasing amplification. These predictions may be addressed experimentally by large-scale measurements of odor-evoked activity after learning. The direct detection of E/I assemblies will ultimately require dense reconstructions of large neuronal networks at synaptic resolution. Given the small size of Dp, this challenge may be addressed in zebrafish by connectomics approaches based on volume electron microscopy (<xref ref-type="bibr" rid="bib20">Denk et al., 2012</xref>; <xref ref-type="bibr" rid="bib28">Friedrich and Wanner, 2021</xref>; <xref ref-type="bibr" rid="bib44">Kornfeld and Denk, 2018</xref>).</p><p>The hypothesis that memory networks contain E/I assemblies and operate in a state of precise synaptic balance can be derived from the basic assumptions that (1) synaptic plasticity establishes assemblies and (2) that firing rate distributions remain stable as network structure is modified by experience (<xref ref-type="bibr" rid="bib7">Barron et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Hennequin et al., 2017</xref>). Hence, <italic>Tuned</italic> networks based on Dp may also reproduce features of other recurrently connected brain areas such as hippocampus and neocortex, which also operate in a balanced state (<xref ref-type="bibr" rid="bib64">Renart et al., 2010</xref>; <xref ref-type="bibr" rid="bib72">Sadeh and Clopath, 2020b</xref>; <xref ref-type="bibr" rid="bib77">Shadlen and Newsome, 1994</xref>; <xref ref-type="bibr" rid="bib92">Znamenskiy et al., 2024</xref>). Future experiments may therefore explore representations of learned information by local manifolds also in cortical brain areas.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><table-wrap id="keyresource" position="anchor"><label>Key resources table</label><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Reagent type (species) or resource</th><th align="left" valign="bottom">Designation</th><th align="left" valign="bottom">Source or reference</th><th align="left" valign="bottom">Identifiers</th><th align="left" valign="bottom">Additional information</th></tr></thead><tbody><tr><td align="left" valign="bottom">Software, algorithm</td><td align="left" valign="bottom">Matlab</td><td align="left" valign="bottom">Mathworks</td><td align="left" valign="bottom"><ext-link ext-link-type="uri" xlink:href="https://ch.mathworks.com">https://ch.mathworks.com</ext-link> (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID:SCR_001622">SCR_001622</ext-link>)</td><td align="left" valign="bottom"/></tr></tbody></table></table-wrap><sec id="s4-1"><title>pDp spiking network model</title><p>pDp<sub>sim</sub> consisted of 4000 excitatory (E) and 1000 inhibitory (I) neurons which were modeled as adaptive leaky integrate-and-fire units with conductance-based synapses of strength <inline-formula><mml:math id="inf12"><mml:mi>w</mml:mi></mml:math></inline-formula>.</p><p>A spike emitted at time <italic>t</italic> by the presynaptic neuron <italic>y</italic> from population <italic>Y</italic> triggered an increase in the conductance <inline-formula><mml:math id="inf13"><mml:msub><mml:mrow><mml:mi>g</mml:mi></mml:mrow><mml:mrow><mml:mi>Y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> in the postsynaptic neuron <inline-formula><mml:math id="inf14"><mml:mi>x</mml:mi></mml:math></inline-formula>, connected to <italic>y</italic> with strength <italic>w<sub>yx</sub></italic>:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>Y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">n</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>y</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The synaptic time constant <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mtext>syn</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was set to 10 ms for inhibitory synapses and 30 ms for excitatory synapses, except for the simple model were <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mrow><mml:mtext>syn</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi>Y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> was 10 ms for both synapses.</p><p>Neuron <inline-formula><mml:math id="inf17"><mml:mi>x</mml:mi></mml:math></inline-formula> from population <italic>X</italic> received synaptic inputs from the OB as well as from the different local neuronal populations <italic>P</italic>. Its membrane potential <inline-formula><mml:math id="inf18"><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> evolved according to:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow><mml:mo>,</mml:mo><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">O</mml:mi><mml:mi mathvariant="normal">B</mml:mi></mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi>P</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>When the membrane potential reached a threshold <italic>V</italic><sub>th</sub>, the neuron emitted a spike and its membrane potential was reset to <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mtext>rest</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and clamped to this value during a refractory period <italic>τ</italic><sub>ref</sub>.</p><p>Excitatory neurons were endowed with adaptation with the following dynamics (<xref ref-type="bibr" rid="bib12">Brette and Gerstner, 2005</xref>):<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mrow><mml:mtext>rest</mml:mtext></mml:mrow><mml:mo>,</mml:mo><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">w</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>z</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi>z</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">f</mml:mi><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">h</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">p</mml:mi><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">k</mml:mi><mml:mi mathvariant="normal">e</mml:mi></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In inhibitory neurons, <inline-formula><mml:math id="inf20"><mml:mi>z</mml:mi></mml:math></inline-formula> was set to 0 for simplicity.</p><p>The neuronal parameters of the model are summarized in <xref ref-type="table" rid="table1">Table 1</xref>. The values of the membrane time constant <inline-formula><mml:math id="inf21"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the resting conductance <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and resting potential <italic>E</italic><sub>rest</sub> of excitatory neurons, and the inhibitory and excitatory reversal potential <italic>E</italic><sub>inh</sub> and <italic>E</italic><sub>exc</sub> are in the range of experimentally measured values (<xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>; <xref ref-type="bibr" rid="bib10">Blumhagen et al., 2011</xref>). The remaining parameters were then fitted to fulfill two conditions (derived from unpublished experimental observations): (1) the neuron should not generate action potentials in response to a step current injection of duration 500 ms and amplitude 15 pA, and (2) the mean firing rate should be on the order of tens of Hz when the amplitude of the step current is 100 pA. Furthermore, the firing rates of inhibitory neurons should be higher than the firing rates of excitatory neurons, as observed experimentally (unpublished data).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Values of the neuronal parameters.</title><p>The superscripts indicate the reference where the experimental measurements can be found.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2">Neuronal parameters</th><th align="left" valign="top" rowspan="2">Symbol</th><th align="left" valign="top" colspan="3">Value</th></tr><tr><th align="left" valign="top">Excitatory neuron</th><th align="left" valign="top">Inhibitory neuron</th><th align="left" valign="top">Simple model</th></tr></thead><tbody><tr><td align="left" valign="top">Membrane time constant</td><td align="left" valign="top"><inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="char" char="." valign="top">85 ms<xref ref-type="table-fn" rid="table1fn1">*</xref><sup>,<xref ref-type="table-fn" rid="table1fn2">†</xref></sup></td><td align="char" char="." valign="top">50 ms<sup><xref ref-type="table-fn" rid="table1fn2">†</xref></sup></td><td align="char" char="." valign="top">68 ms</td></tr><tr><td align="left" valign="top">Resting conductance</td><td align="left" valign="top"><inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="top">1.35 nS<sup><xref ref-type="table-fn" rid="table1fn1">*</xref></sup></td><td align="char" char="." valign="top">0.9 nS</td><td align="char" char="." valign="top">1.1 nS</td></tr><tr><td align="left" valign="top">Resting potential</td><td align="left" valign="top"><inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="top">–60 mV<xref ref-type="table-fn" rid="table1fn1"><sup>*</sup></xref><sup>,</sup><xref ref-type="table-fn" rid="table1fn2"><sup>†</sup></xref></td><td align="char" char="." valign="top">–65 mV</td><td align="char" char="." valign="top">–62 mV</td></tr><tr><td align="left" valign="top">Excitatory reversal potential</td><td align="left" valign="top"><inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">x</mml:mi><mml:mi mathvariant="normal">c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="top">0 mV<xref ref-type="table-fn" rid="table1fn1">*</xref></td><td align="char" char="." valign="top">0 mV</td><td align="char" char="." valign="top">0 mV</td></tr><tr><td align="left" valign="top">Inhibitory reversal potential</td><td align="left" valign="top"><inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="normal">i</mml:mi><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="top">–70 mV<xref ref-type="table-fn" rid="table1fn1">*</xref></td><td align="char" char="." valign="top">–70 mV</td><td align="char" char="." valign="top">–70 mV</td></tr><tr><td align="left" valign="top">Spiking threshold</td><td align="left" valign="top"><inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi mathvariant="normal">t</mml:mi><mml:mi mathvariant="normal">h</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="top">–38 mV<sup><xref ref-type="table-fn" rid="table1fn1">*</xref></sup></td><td align="char" char="." valign="top">–45 mV</td><td align="char" char="." valign="top">–41 mV</td></tr><tr><td align="left" valign="top">Reset potential</td><td align="left" valign="top"><inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">s</mml:mi><mml:mi mathvariant="normal">t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="top">–60 mV</td><td align="char" char="." valign="top">–65 mV</td><td align="char" char="." valign="top">–62 mV</td></tr><tr><td align="left" valign="top">Refractory period</td><td align="left" valign="top"><inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">f</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="char" char="." valign="top">8 ms</td><td align="char" char="." valign="top">8 ms</td><td align="char" char="." valign="top">8 ms</td></tr><tr><td align="left" valign="top">Adaptation time constant</td><td align="left" valign="top"><inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>τ</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="char" char="." valign="top">40 ms</td><td align="char" char="." valign="top">/</td><td align="char" char="." valign="top">20 ms</td></tr><tr><td align="left" valign="top">Subthreshold adaptation</td><td align="left" valign="top"><inline-formula><mml:math id="inf32"><mml:mi>a</mml:mi></mml:math></inline-formula></td><td align="char" char="." valign="top">1 nS</td><td align="char" char="." valign="top">/</td><td align="char" char="." valign="top">0.5 nS</td></tr><tr><td align="left" valign="top">Spike-triggered adaptation</td><td align="left" valign="top"><inline-formula><mml:math id="inf33"><mml:mi>b</mml:mi></mml:math></inline-formula></td><td align="char" char="." valign="top">10 pA</td><td align="char" char="." valign="top">/</td><td align="char" char="." valign="top">5 pA</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><label>*</label><p><xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>.</p></fn><fn id="table1fn2"><label>†</label><p><xref ref-type="bibr" rid="bib10">Blumhagen et al., 2011</xref>.</p></fn></table-wrap-foot></table-wrap><p>To verify that the behavior of pDp<sub>sim</sub> was robust, we simulated 20 networks with different connection probabilities <italic>p<sub>YX</sub></italic> and synaptic strengths <italic>w<sub>YX</sub></italic> (<xref ref-type="table" rid="table2">Table 2</xref>). The connections between neurons were drawn from a Bernoulli distribution with a predefined <italic>p<sub>YX</sub></italic> ≤ 0.05. As a consequence, each neuron received the same number of input connections. Care was also taken to ensure that the variation in the number of output connections across neurons was low in pDp<sub>sim</sub>: connections of a presynaptic neuron <italic>y</italic> to postsynaptic neurons <italic>x</italic> were randomly deleted when their total number exceeded the average number of output connections by ≥5%, or added when they were lower by ≥5%.</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Values of the connectivity parameters of different networks {probability <italic>p<sub>YX</sub></italic> and synaptic strength <inline-formula><mml:math id="inf34"><mml:mi>w</mml:mi></mml:math></inline-formula><sub>YX</sub> in pS}.</title><p>In <xref ref-type="fig" rid="fig3">Figures 3</xref>—<xref ref-type="fig" rid="fig5">5</xref>, 2 of the 5 networks from each structure were simulated. In the simple model, each neuron received the same number of inputs from each population.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Network structure</th><th align="left" valign="top">OB → E</th><th align="left" valign="top">OB → I</th><th align="left" valign="top">E → E</th><th align="left" valign="top">E → I</th><th align="left" valign="top">I → E</th><th align="left" valign="top">I → I</th></tr></thead><tbody><tr><td align="left" valign="top">A (<italic>n</italic> = 5)</td><td align="left" valign="top">{0.02, 128}</td><td align="left" valign="top">{0.01, 68}</td><td align="left" valign="top">{0.05, 128}</td><td align="left" valign="top">{0.04, 68}</td><td align="left" valign="top">{0.05, 480}</td><td align="left" valign="top">{0.04, 250}</td></tr><tr><td align="left" valign="top">B (<italic>n</italic> = 5)</td><td align="left" valign="top">{0.02, 128}</td><td align="left" valign="top">{0.01, 66}</td><td align="left" valign="top">{0.05, 128}</td><td align="left" valign="top">{0.04, 66}</td><td align="left" valign="top">{0.05, 450}</td><td align="left" valign="top">{0.04, 210}</td></tr><tr><td align="left" valign="top">C (<italic>n</italic> = 5)</td><td align="left" valign="top">{0.02, 128}</td><td align="left" valign="top">{0.01, 68}</td><td align="left" valign="top">{0.05, 108}</td><td align="left" valign="top">{0.02, 80}</td><td align="left" valign="top">{0.05, 520}</td><td align="left" valign="top">{0.02, 310}</td></tr><tr><td align="left" valign="top">D (<italic>n</italic> = 5)</td><td align="left" valign="top">{0.03, 95}</td><td align="left" valign="top">{0.02, 42}</td><td align="left" valign="top">{0.05, 128}</td><td align="left" valign="top">{0.04, 58}</td><td align="left" valign="top">{0.05, 590}</td><td align="left" valign="top">{0.04, 270}</td></tr><tr><td align="left" valign="top">Simple (<italic>n</italic> = 5)</td><td align="left" valign="top">{0.03, 160}</td><td align="left" valign="top">{0.03, 160}</td><td align="left" valign="top">{0.025, 370}</td><td align="left" valign="top">{0.025, 370}</td><td align="left" valign="top">{0.1, 1010}</td><td align="left" valign="top">{0.1, 1010}</td></tr></tbody></table></table-wrap><p>The connection strengths <italic>w<sub>YX</sub></italic> were then fitted to reproduce experimental observations in pDp (five observables in total, see below and <xref ref-type="fig" rid="fig1">Figure 1</xref>). For this purpose, a lower and an upper bound for <italic>w<sub>YX</sub></italic> were set such that the amplitudes of single EPSPs and IPSPs were in the biologically plausible range of 0.2–2 mV. <italic>w</italic><sub><italic>OE</italic></sub> was then further constrained to maintain the odor-evoked, time-averaged <italic>g</italic><sub><italic>OE</italic></sub> in the range of experimental values (<xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>). Once <italic>w</italic><sub><italic>OE</italic></sub> was fixed, the lower bound of <italic>w</italic><sub><italic>EE</italic></sub> was increased to obtain a network rate &gt;10 Hz in the absence of inhibition. A grid search was then used to refine the remaining <italic>w</italic><sub><italic>YX</italic></sub>.</p></sec><sec id="s4-2"><title>OB input</title><p>Each pDp<sub>sim</sub> neuron received external input from the OB, which consisted of 1500 mitral cells spontaneously active at 6 Hz. Odors were simulated by increasing the firing rate of 150 randomly selected mitral cells. Firing rates of these ‘activated’ mitral cells were drawn from a discrete uniform distribution ranging from 8 to 32 Hz and their onset latencies were drawn from a discrete uniform distribution ranging from 0 to 200 ms. An additional 75 randomly selected mitral cells were inhibited. Firing rates and latencies of these neurons were drawn from discrete uniform distributions ranging from 0 to 5 Hz and from 0 to 200 ms, respectively. After odor onset, firing rates decreased exponentially with a time constant of 1, 2, or 4 s (equally distributed). Spikes were then generated from a Poisson distribution, and this process was repeated to create trial-to-trial variability. As all odors had almost identical firing patterns, the total OB input did not vary much across odors. In <xref ref-type="fig" rid="fig1">Figures 1</xref>—<xref ref-type="fig" rid="fig3">3</xref>, the odor set consisted of 10 novel and/or 10 learned odors, all of which were uncorrelated (pattern correlations close to zero). Odors were presented for 2 s and separated by 1 s of baseline activity.</p><p>Olfactory subspaces comprised 121 OB activity patterns. Each pattern was represented by a pixel in a 11 × 11 square. The pixel at each vertex corresponded to one pure odor with 150 activated and 75 inhibited mitral cells as described above, and the remaining pixels corresponded to mixtures. When pure odors were correlated (<xref ref-type="fig" rid="fig6">Figure 6</xref>), adjacent pure odors shared half of their activated and half of their inhibited mitral cells. We generated eight different trajectories within the square, each visiting all possible virtual odor locations for 1 s. Each trajectory thus consisted of 121 s of odor presentation, and trajectories were separated by 2 s of baseline activity. The dataset for analysis therefore comprised 968 activity patterns (8 trajectories × 121 odors). The total number of activated and inhibited cells at each location in the virtual square remained within the range of 150 ± 10% and 75 ± 10%, respectively. The fraction of activated and inhibited mitral cells from a given pure odor (its concentration) decreased with the distance from the corresponding vertex as shown in <xref ref-type="table" rid="table3">Table 3</xref>. At each location within the square and for each trajectory, mitral cells that remained activated in the mixture were randomly selected from the pool of C mitral cells with the shortest latencies from each pure odor. C decreased with the distance from the vertex as shown in <xref ref-type="table" rid="table4">Table 4</xref>. The firing rate of each selected mitral cell varied ±1 Hz around its rate in response to the pure odor. The identity, but not the firing rate, of the activated mitral cells therefore changed gradually within the odor subspace. This procedure reflects the experimental observation that responses of zebrafish mitral cells to binary odor mixtures often resemble responses to one of the pure components (<xref ref-type="bibr" rid="bib79">Tabor et al., 2004</xref>).</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Percentage of mitral cells out of the 150 mitral cells defining one pure odor (top left corner) that are activated in mixtures.</title><p>Percentages may be interpreted as relative odor concentration. Values given here are for uncorrelated pure odors.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom">100</td><td align="left" valign="bottom">90</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">90</td><td align="char" char="." valign="bottom">90</td><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">70</td><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">40</td><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">80</td><td align="char" char="." valign="bottom">70</td><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">40</td><td align="char" char="." valign="bottom">35</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">70</td><td align="char" char="." valign="bottom">70</td><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">35</td><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">20</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">60</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">40</td><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">20</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">50</td><td align="char" char="." valign="bottom">40</td><td align="char" char="." valign="bottom">35</td><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">40</td><td align="char" char="." valign="bottom">40</td><td align="char" char="." valign="bottom">35</td><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">30</td><td align="char" char="." valign="bottom">25</td><td align="char" char="." valign="bottom">20</td><td align="char" char="." valign="bottom">20</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">20</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">10</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">5</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td><td align="char" char="." valign="bottom">0</td></tr></tbody></table></table-wrap><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Percentage of activated cells available for selection for uncorrelated pure odors.</title><p>C is obtained by multiplying the values by 1.5.</p></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="bottom">100</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">90</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">0</td></tr><tr><td align="left" valign="bottom">100</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">90</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">100</td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">90</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">55</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">90</td><td align="left" valign="bottom">90</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">55</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">80</td><td align="left" valign="bottom">80</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">70</td><td align="left" valign="bottom">70</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">55</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">60</td><td align="left" valign="bottom">60</td><td align="left" valign="bottom">55</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">50</td><td align="left" valign="bottom">50</td><td align="left" valign="bottom">45</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">40</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">40</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">35</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">30</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">30</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">25</td><td align="left" valign="bottom">0</td><td align="char" char="." valign="bottom">0</td></tr><tr><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="left" valign="bottom">0</td><td align="char" char="." valign="bottom">0</td></tr></tbody></table></table-wrap></sec><sec id="s4-3"><title>Assemblies</title><p>Unless noted otherwise, <italic>Scaled</italic> and <italic>Tuned</italic> networks contained 15 assembles (memories). An assembly representing a given odor contained the 100 E neurons that received the highest density of inputs from the corresponding active mitral cells. Hence, the size of assemblies was substantially smaller than the total population, consistent with the observation that only a minority of neurons in Dp or piriform cortex are activated during odor stimulation (<xref ref-type="bibr" rid="bib57">Miura et al., 2012</xref>; <xref ref-type="bibr" rid="bib78">Stettler and Axel, 2009</xref>; <xref ref-type="bibr" rid="bib89">Yaksi et al., 2009</xref>) and upregulate <italic>cfos</italic> during olfactory learning (<xref ref-type="bibr" rid="bib54">Meissner-Bernard et al., 2019</xref>). We then rewired assembly neurons: additional connections were created between assembly neurons, and a matching number of existing connections between non-assembly and assembly neurons were eliminated. The number of input connections per neuron therefore remained unchanged. A new connection between two assembly neurons doubled the synaptic strength <italic>w</italic><sub><italic>EE</italic></sub> if it added to an existing connection. As a result of this rewiring, the connection probability within the assembly increased by a factor <italic>α</italic> relative to the baseline connection probability.</p><p>In <italic>Scaled I</italic> networks, <italic>w</italic><sub><italic>IE</italic></sub> was increased globally by a constant factor <italic>χ</italic>. In <italic>Tuned</italic> networks, connections were modified between the 100 E neurons of an assembly and the 25 I neurons that were most densely connected to these E neurons, using the same procedure as for E-to-E connections. In <italic>Tuned I</italic> networks, only I-to-E connections were rewired, while in <italic>Tuned E+I networks</italic>, both I-to-E and E-to-I connections were rewired (<xref ref-type="table" rid="table5">Table 5</xref>). Whenever possible, parameters <italic>α</italic>, <italic>β</italic>, <italic>γ</italic>, and <italic>χ</italic> that generated networks with less than 15% change in population firing rates compared to the corresponding <italic>rand</italic> network were selected. In <xref ref-type="fig" rid="fig6">Figure 6</xref>, two additional assemblies were created in <italic>Scaled I</italic> or <italic>Tuned</italic> networks without adjusting any parameters.</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Parameters of structured networks.</title></caption><table frame="hsides" rules="groups" id="F2"><thead><tr><th align="left" valign="top"/><th align="left" valign="top"><xref ref-type="fig" rid="fig2">Figures 2</xref>—<xref ref-type="fig" rid="fig6">6</xref></th><th align="left" valign="top"><xref ref-type="fig" rid="fig4s3">Figure 4—figure supplement 3</xref> (mean)</th></tr></thead><tbody><tr><td align="left" valign="top">Scaled I</td><td align="left" valign="top"><italic>α</italic> = 5, <italic>χ</italic> = 1.4</td><td align="left" valign="top">Scaled[adjust]: <italic>α</italic> = 3.7, <italic>χ</italic> = 1.07</td></tr><tr><td align="left" valign="top">Tuned I</td><td align="left" valign="top"><italic>α</italic> = 5, <italic>β</italic> = 18</td><td align="left" valign="top">/</td></tr><tr><td align="left" valign="top">Tuned E+I</td><td align="left" valign="top"><italic>α</italic> = 5, <italic>β</italic> = 4, <italic>γ</italic> = 3</td><td align="left" valign="top">Tuned[adjust]: <italic>α</italic> = 5, <italic>β</italic> = 11.5, <italic>γ</italic> = 3</td></tr></tbody></table></table-wrap></sec><sec id="s4-4"><title>Observables</title><p>All variables were measured in the E population and time-averaged over the first 1.5 s of odor presentations, unless otherwise stated.</p><list list-type="order"><list-item><p>The <italic>firing rate</italic> is the number of spikes in a time interval <italic>T</italic> divided by <italic>T</italic>.</p></list-item><list-item><p><italic>g</italic><sub><italic>OE</italic></sub> is the mean conductance change in E neurons triggered by spikes from the OB.</p></list-item><list-item><p><italic>g</italic><sub>syn</sub> is the total synaptic conductance change due to odor stimulation, calculated as the sum of <italic>g</italic><sub><italic>OE</italic></sub>, <italic>g</italic><sub><italic>EE</italic></sub>, and <italic>g</italic><sub><italic>IE</italic></sub><italic><sub>.</sub> g</italic><sub><italic>EE</italic></sub> and <italic>g</italic><sub><italic>IE</italic></sub> are the conductance changes contributed by E and I synapses, respectively.</p></list-item><list-item><p>The <italic>percentage of recurrent input</italic> quantifies the average contribution of the recurrent excitatory input to the total excitatory input in E neurons. It was defined for each excitatory neuron as the ratio of the time-averaged <italic>g</italic><sub><italic>EE</italic></sub> to the time-averaged total excitatory conductance (<italic>g</italic><sub><italic>EE</italic></sub> <italic>+ g</italic><sub><italic>OE</italic></sub>) multiplied by 100. In (2,3,4), the time-averaged E and I synaptic conductances during the 500 ms before odor presentation were subtracted from the E and I conductances measured during odor presentation for each neuron.</p></list-item><list-item><p>In addition, we required the Pearson correlation between activity patterns to be close to zero in response to uncorrelated inputs. The Pearson correlation between pairs of activity vectors composed of the firing rate of E neurons was averaged over all possible odor pairs.</p></list-item></list></sec><sec id="s4-5"><title>Co-tuning</title><p><italic>Co-tuning</italic> was quantified by two different procedures: (1) For each neuron, we calculated the Pearson correlation between the time-averaged E and I conductances in response to 10 learned odors. (2) As described in <xref ref-type="bibr" rid="bib67">Rupprecht and Friedrich, 2018</xref>, we projected observed pairs of E and I conductances onto a ‘balanced’ and ‘counter-balanced’ axis. The balanced axis was obtained by fitting a linear model without constant to the E and I conductances of 4000×10 neuron-learned odor pairs. The resulting model was a constant I/E ratio (~1.2) that defined a membrane potential close to spike threshold. The counter-balanced axis was orthogonal to the balanced axis. For each neuron, synaptic conductances were projected onto these axes and their dispersions quantified by the standard deviations.</p></sec><sec id="s4-6"><title>Characterization of population activity in state space</title><p><italic>Principal component analysis</italic> (PCA) was applied to the OB activity patterns of the odor subspace or to the corresponding activity patterns across E neurons in pDp<sub>sim</sub> (8 × 121 = 968 patterns, each averaged over 1 s).</p><p>The <italic>participation ratio</italic> (PR) provides an estimate of the maximal number of principal components (PCs) required to recapitulate the observed neuronal activity. It is defined as <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">P</mml:mi><mml:mi mathvariant="normal">R</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:msubsup><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>λ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the eigenvalues obtained from PCA (variance along each PC).</p><p>For angular analyses, we projected activity patterns onto the first 400 PCs, which was the number of PCs required to explain at least 75% of the variance in all networks. We measured the <italic>angle</italic> <inline-formula><mml:math id="inf37"><mml:mi>θ</mml:mi></mml:math></inline-formula> between the edges connecting the pattern <inline-formula><mml:math id="inf38"><mml:mi mathvariant="bold">p</mml:mi></mml:math></inline-formula> evoked by a pure odor to two patterns <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula><inline-formula><mml:math id="inf41"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> were patterns evoked by 2 out of the 7 odors that were most similar to the pure odor (168 angles in total). <inline-formula><mml:math id="inf43"><mml:mi>θ</mml:mi></mml:math></inline-formula> was defined as <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">y</mml:mi><mml:mi mathvariant="normal">z</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="normal">c</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">s</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">p</mml:mi></mml:mrow></mml:mfenced><mml:mo>∙</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">p</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">z</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">p</mml:mi></mml:mrow></mml:mfenced><mml:mfenced open="‖" close="‖" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="bold">y</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi mathvariant="bold">p</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:math></inline-formula> . This metric is sensitive to non-uniform expansion and other non-linear transformations.</p><p>The <italic>Mahalanobis distance d</italic><sub>M</sub> is defined as <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mrow><mml:mi mathvariant="normal">M</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>,</mml:mo><mml:mi>Q</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msqrt><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mi>S</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>v</mml:mi><mml:mo>−</mml:mo><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:msqrt></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf46"><mml:mi>v</mml:mi></mml:math></inline-formula> is a vector representing an activity pattern, and <inline-formula><mml:math id="inf47"><mml:mi>Q</mml:mi></mml:math></inline-formula> a reference class consisting of a distribution of activity patterns with mean <italic>µ</italic> and covariance matrix <inline-formula><mml:math id="inf48"><mml:mi>S</mml:mi></mml:math></inline-formula>.</p></sec><sec id="s4-7"><title>Classification: Four-way forced choice task</title><p>To assess the assignment of odor-evoked patterns to representations of pure odors in the odor subspace, we used <italic>quadratic discriminant analysis</italic> (QDA), a non-linear classifier that takes into account the separation and covariance patterns of different classes (<xref ref-type="bibr" rid="bib31">Ghojogh and Crowley, 2019</xref>). The training set consisted of the population response to multiple trials of each of the four pure odors, averaged over the first and second half of the 1 s odor presentation. To increase the number of training data in each of the four odor classes, the training set also included population response to odors that were closely related to the pure odor (Pearson correlation between OB response patterns &gt;0.6). Analyses were performed using subsets of 80 neurons (similar results were obtained using 50–100 neurons). These neurons were randomly selected (50 iterations) either from the (pseudo-) assemblies representing the pure odors (400 E neurons; pseudo-assemblies in <italic>rand</italic> networks and for novel odors) or from the entire population of E neurons. We verified that the data came from a Gaussian mixture model. The trained classifier was then applied to activity patterns evoked by the remaining odors of the subspace (test set; correlation with pure odors &lt;0.6). Each pattern <italic>x</italic> from the test set was assigned to the class <italic>k</italic> that maximized the discriminant function.</p><p><inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi>δ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mi mathvariant="normal">l</mml:mi><mml:mi mathvariant="normal">o</mml:mi><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:mrow><mml:mo>|</mml:mo><mml:msub><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>|</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mi>T</mml:mi></mml:mrow></mml:msup><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">l</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">o</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">g</mml:mi></mml:mrow><mml:msub><mml:mrow><mml:mi>π</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the neuronal covariance matrix of each odor class <italic>k</italic> and <inline-formula><mml:math id="inf51"><mml:msub><mml:mrow><mml:mi mathvariant="normal">π</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">k</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the prior probability of class <italic>k</italic>. This discriminant function is closely related to <italic>d</italic><sub>M</sub>. The subsampling of neurons ensured the invertibility of <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> in <italic>Tuned</italic> and <italic>rand</italic> networks. In <italic>Scaled</italic> networks, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>S</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is low-rank and therefore not invertible, and QDA cannot be applied.</p><p>Template matching (<xref ref-type="fig" rid="fig5s2">Figure 5—figure supplement 2</xref>) was performed in a similar way except that each test pattern <italic>x</italic> was assigned to the class <italic>k</italic> that maximized the correlation between pattern <italic>x</italic> and a randomly selected pattern in each class (10 iterations).</p></sec><sec id="s4-8"><title>Classification: Odor concentration</title><p>As mentioned above, an odor is represented by 150 activated mitral cells, a subset of which remain activated in mixtures. We define the percentage of remaining activated cells as the concentration of the odor. We performed multiple linear regression to predict the concentration of a given odor based on the firing rate of 80 E neurons averaged over the first second of odor presentation. As for QDA, these 80 neurons were randomly selected (50 iterations) from the (pseudo-) assemblies representing the pure odors (400 E neurons; pseudo-assemblies in <italic>rand</italic> networks and for novel odors). We included four or eight trials to equilibrate the number of population responses across the different concentrations (<italic>n</italic> = 352).</p><p>Using the Matlab function <italic>fitcecoc</italic>, we also trained a linear support vector machine model to assess the accuracy of different networks in predicting the concentration of an odor in a mixture. Mixtures were divided into six different concentration classes, with 15% increments (see <xref ref-type="fig" rid="fig5">Figure 5D</xref>). We included two to six trials to equilibrate the number of population responses within the different classes. The dataset consisted of 162 population responses of 80 randomly selected neurons (20 iterations) and was randomly partitioned in training and testing set (twofold cross-validation, 20 iterations).</p></sec><sec id="s4-9"><title>Simulations</title><p>Simulations were performed using Matlab and Python. Differential equations were solved using the forward Euler method and an integration time step of <italic>dt</italic> = 0.1 ms.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Validation, Visualization, Methodology, Writing – original draft, Writing – review and editing, Investigation</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Methodology, Writing – original draft</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Visualization, Writing – original draft, Project administration, Writing – review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-96303-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Modeling code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/clairemb90/pDp-model">https://github.com/clairemb90/pDp-model</ext-link> (copy archived at <xref ref-type="bibr" rid="bib55">Meissner-Bernard, 2024</xref>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank the Friedrich lab for insightful discussions. This work was supported by the Novartis Research Foundation, by the European Research Council (ERC) under the European Union’s Horizon 2020 research and innovation program (grant agreement no. 742576), and by the Swiss National Science Foundation (grants no. 31003A_172925/1, PCEFP3_202981).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ahmadian</surname><given-names>Y</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>What is the dynamical regime of cerebral cortex?</article-title><source>Neuron</source><volume>109</volume><fpage>3373</fpage><lpage>3391</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.07.031</pub-id><pub-id pub-id-type="pmid">34464597</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Altan</surname><given-names>E</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Perreault</surname><given-names>EJ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Estimating the dimensionality of the manifold underlying multi-electrode neural recordings</article-title><source>PLOS Computational Biology</source><volume>17</volume><elocation-id>e1008591</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008591</pub-id><pub-id pub-id-type="pmid">34843461</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname><given-names>DJ</given-names></name><name><surname>Tsodyks</surname><given-names>MV</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Quantitative study of attractor neural network retrieving at low spike rates: I. substrate—spikes, rates and neuronal gain</article-title><source>Network</source><volume>2</volume><fpage>259</fpage><lpage>273</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_2_3_003</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Babadi</surname><given-names>B</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Sparseness and expansion in sensory representations</article-title><source>Neuron</source><volume>83</volume><fpage>1213</fpage><lpage>1226</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.07.035</pub-id><pub-id pub-id-type="pmid">25155954</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>C</given-names></name><name><surname>Zhu</surname><given-names>V</given-names></name><name><surname>Rosenbaum</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nonlinear stimulus representations in neural circuits with approximate excitatory-inhibitory balance</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008192</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008192</pub-id><pub-id pub-id-type="pmid">32946433</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barnes</surname><given-names>DC</given-names></name><name><surname>Hofacer</surname><given-names>RD</given-names></name><name><surname>Zaman</surname><given-names>AR</given-names></name><name><surname>Rennaker</surname><given-names>RL</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Olfactory perceptual stability and discrimination</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>1378</fpage><lpage>1380</lpage><pub-id pub-id-type="doi">10.1038/nn.2217</pub-id><pub-id pub-id-type="pmid">18978781</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barron</surname><given-names>HC</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Behrens</surname><given-names>TE</given-names></name><name><surname>Ramaswami</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inhibitory engrams in perception and memory</article-title><source>PNAS</source><volume>114</volume><fpage>6666</fpage><lpage>6674</lpage><pub-id pub-id-type="doi">10.1073/pnas.1701812114</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bhatia</surname><given-names>A</given-names></name><name><surname>Moza</surname><given-names>S</given-names></name><name><surname>Bhalla</surname><given-names>US</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Precise excitation-inhibition balance controls gain and timing in the hippocampus</article-title><source>eLife</source><volume>8</volume><elocation-id>e43415</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.43415</pub-id><pub-id pub-id-type="pmid">31021319</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Blazing</surname><given-names>RM</given-names></name><name><surname>Franks</surname><given-names>KM</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>Odor Coding in Piriform Cortex: Mechanistic Insights into Distributed Coding</source><publisher-name>Elsevier Ltd</publisher-name><pub-id pub-id-type="doi">10.1016/j.conb.2020.03.001</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blumhagen</surname><given-names>F</given-names></name><name><surname>Zhu</surname><given-names>P</given-names></name><name><surname>Shum</surname><given-names>J</given-names></name><name><surname>Schärer</surname><given-names>YPZ</given-names></name><name><surname>Yaksi</surname><given-names>E</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Neuronal filtering of multiplexed odour representations</article-title><source>Nature</source><volume>479</volume><fpage>493</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1038/nature10633</pub-id><pub-id pub-id-type="pmid">22080956</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boerlin</surname><given-names>M</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name><name><surname>Denève</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Predictive coding of dynamical variables in balanced spiking networks</article-title><source>PLOS Computational Biology</source><volume>9</volume><elocation-id>e1003258</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003258</pub-id><pub-id pub-id-type="pmid">24244113</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brette</surname><given-names>R</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Adaptive exponential integrate-and-fire model as an effective description of neuronal activity</article-title><source>Journal of Neurophysiology</source><volume>94</volume><fpage>3637</fpage><lpage>3642</lpage><pub-id pub-id-type="doi">10.1152/jn.00686.2005</pub-id><pub-id pub-id-type="pmid">16014787</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons</article-title><source>Journal of Computational Neuroscience</source><volume>8</volume><fpage>183</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1023/a:1008925309027</pub-id><pub-id pub-id-type="pmid">10809012</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chapuis</surname><given-names>J</given-names></name><name><surname>Wilson</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Bidirectional plasticity of cortical pattern recognition and behavioral sensory acuity</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>155</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1038/nn.2966</pub-id><pub-id pub-id-type="pmid">22101640</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chenkov</surname><given-names>N</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Kempter</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Memory replay in balanced recurrent networks</article-title><source>PLOS Computational Biology</source><volume>13</volume><elocation-id>e1005359</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005359</pub-id><pub-id pub-id-type="pmid">28135266</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chung</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural population geometry: An approach for understanding biological and artificial neural networks</article-title><source>Current Opinion in Neurobiology</source><volume>70</volume><fpage>137</fpage><lpage>144</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2021.10.010</pub-id><pub-id pub-id-type="pmid">34801787</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Dahmen</surname><given-names>D</given-names></name><name><surname>Recanatesi</surname><given-names>S</given-names></name><name><surname>Jia</surname><given-names>X</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Campagnola</surname><given-names>L</given-names></name><name><surname>Seeman</surname><given-names>S</given-names></name><name><surname>Jarsky</surname><given-names>T</given-names></name><name><surname>Helias</surname><given-names>M</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Strong and Localized Recurrence Controls Dimensionality of Neural Activity across Brain Areas</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.11.02.365072</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denève</surname><given-names>S</given-names></name><name><surname>Machens</surname><given-names>CK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Efficient codes and balanced networks</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>375</fpage><lpage>382</lpage><pub-id pub-id-type="doi">10.1038/nn.4243</pub-id><pub-id pub-id-type="pmid">26906504</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denève</surname><given-names>S</given-names></name><name><surname>Alemi</surname><given-names>A</given-names></name><name><surname>Bourdoukan</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The brain as an efficient and robust adaptive learner</article-title><source>Neuron</source><volume>94</volume><fpage>969</fpage><lpage>977</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.016</pub-id><pub-id pub-id-type="pmid">28595053</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denk</surname><given-names>W</given-names></name><name><surname>Briggman</surname><given-names>KL</given-names></name><name><surname>Helmstaedter</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Structural neurobiology: missing link to a mechanistic understanding of neural computation</article-title><source>Nature Reviews. Neuroscience</source><volume>13</volume><fpage>351</fpage><lpage>358</lpage><pub-id pub-id-type="doi">10.1038/nrn3169</pub-id><pub-id pub-id-type="pmid">22353782</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eckmann</surname><given-names>S</given-names></name><name><surname>Young</surname><given-names>EJ</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Synapse-type-specific competitive Hebbian learning forms functional recurrent networks</article-title><source>PNAS</source><volume>121</volume><elocation-id>e2305326121</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2305326121</pub-id><pub-id pub-id-type="pmid">38870059</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Festa</surname><given-names>D</given-names></name><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Analog memories in a balanced rate-based network of E-I neurons</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frank</surname><given-names>T</given-names></name><name><surname>Mönig</surname><given-names>NR</given-names></name><name><surname>Satou</surname><given-names>C</given-names></name><name><surname>Higashijima</surname><given-names>S-I</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Associative conditioning remaps odor representations and modifies inhibition in a higher olfactory brain area</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1844</fpage><lpage>1856</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0495-z</pub-id><pub-id pub-id-type="pmid">31591559</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franks</surname><given-names>KM</given-names></name><name><surname>Russo</surname><given-names>MJ</given-names></name><name><surname>Sosulski</surname><given-names>DL</given-names></name><name><surname>Mulligan</surname><given-names>AA</given-names></name><name><surname>Siegelbaum</surname><given-names>SA</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Recurrent circuitry dynamically shapes the activation of piriform cortex</article-title><source>Neuron</source><volume>72</volume><fpage>49</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.08.020</pub-id><pub-id pub-id-type="pmid">21982368</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freeman</surname><given-names>WJ</given-names></name><name><surname>Skarda</surname><given-names>CA</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spatial EEG patterns, non-linear dynamics and perception: the neo-sherringtonian view</article-title><source>Brain Research Reviews</source><volume>10</volume><fpage>147</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0165-0173(85)90022-0</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Dynamic optimization of odor representations by slow temporal patterning of mitral cell activity</article-title><source>Science</source><volume>291</volume><fpage>889</fpage><lpage>894</lpage><pub-id pub-id-type="doi">10.1126/science.291.5505.889</pub-id><pub-id pub-id-type="pmid">11157170</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Laurent</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Dynamics of olfactory bulb input and output activity during odor stimulation in zebrafish</article-title><source>Journal of Neurophysiology</source><volume>91</volume><fpage>2658</fpage><lpage>2669</lpage><pub-id pub-id-type="doi">10.1152/jn.01143.2003</pub-id><pub-id pub-id-type="pmid">14960561</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friedrich</surname><given-names>RW</given-names></name><name><surname>Wanner</surname><given-names>AA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Dense circuit reconstruction to understand neuronal computation: focus on zebrafish</article-title><source>Annual Review of Neuroscience</source><volume>44</volume><fpage>275</fpage><lpage>293</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-110220-013050</pub-id><pub-id pub-id-type="pmid">33730512</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Froemke</surname><given-names>RC</given-names></name><name><surname>Merzenich</surname><given-names>MM</given-names></name><name><surname>Schreiner</surname><given-names>CE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A synaptic memory trace for cortical receptive field plasticity</article-title><source>Nature</source><volume>450</volume><fpage>425</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1038/nature06289</pub-id><pub-id pub-id-type="pmid">18004384</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gallego</surname><given-names>JA</given-names></name><name><surname>Perich</surname><given-names>MG</given-names></name><name><surname>Miller</surname><given-names>LE</given-names></name><name><surname>Solla</surname><given-names>SA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Neural manifolds for the control of movement</article-title><source>Neuron</source><volume>94</volume><fpage>978</fpage><lpage>984</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.05.025</pub-id><pub-id pub-id-type="pmid">28595054</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ghojogh</surname><given-names>B</given-names></name><name><surname>Crowley</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Linear and quadratic discriminant analysis: tutorial</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://doi.org/10.48550/arXiv.1906.02590">https://doi.org/10.48550/arXiv.1906.02590</ext-link></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Goldman-Rakic</surname><given-names>PS</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Cellular basis of working memory</article-title><source>Neuron</source><volume>14</volume><fpage>477</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1016/0896-6273(95)90304-6</pub-id><pub-id pub-id-type="pmid">7695894</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haberly</surname><given-names>LB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Parallel-distributed processing in olfactory cortex: new insights from morphological and physiological analysis of neuronal circuitry</article-title><source>Chemical Senses</source><volume>26</volume><fpage>551</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1093/chemse/26.5.551</pub-id><pub-id pub-id-type="pmid">11418502</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Hebb</surname><given-names>DO</given-names></name></person-group><year iso-8601-date="1949">1949</year><source>The Organization of Behavior; a Neuropsychological Theory</source><publisher-name>Wiley</publisher-name></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Optimal control of transient dynamics in balanced networks supports generation of complex movements</article-title><source>Neuron</source><volume>82</volume><fpage>1394</fpage><lpage>1406</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2014.04.045</pub-id><pub-id pub-id-type="pmid">24945778</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Vogels</surname><given-names>TP</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Inhibitory plasticity: balance, control, and codependence</article-title><source>Annual Review of Neuroscience</source><volume>40</volume><fpage>557</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-072116-031005</pub-id><pub-id pub-id-type="pmid">28598717</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hennequin</surname><given-names>G</given-names></name><name><surname>Ahmadian</surname><given-names>Y</given-names></name><name><surname>Rubin</surname><given-names>DB</given-names></name><name><surname>Lengyel</surname><given-names>M</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The dynamical regime of sensory cortex: stable dynamics around a single stimulus-tuned attractor account for patterns of noise variability</article-title><source>Neuron</source><volume>98</volume><fpage>846</fpage><lpage>860</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.017</pub-id><pub-id pub-id-type="pmid">29772203</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>PNAS</source><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Population coding in an innately relevant olfactory area</article-title><source>Neuron</source><volume>93</volume><fpage>1180</fpage><lpage>1197</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.02.010</pub-id><pub-id pub-id-type="pmid">28238549</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacobson</surname><given-names>GA</given-names></name><name><surname>Rupprecht</surname><given-names>P</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Experience-dependent plasticity of odor representations in the telencephalon of zebrafish</article-title><source>Current Biology</source><volume>28</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.11.007</pub-id><pub-id pub-id-type="pmid">29249662</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Khona</surname><given-names>M</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Attractor and integrator networks in the brain</article-title><source>Nature Reviews. Neuroscience</source><volume>23</volume><fpage>744</fpage><lpage>766</lpage><pub-id pub-id-type="doi">10.1038/s41583-022-00642-0</pub-id><pub-id pub-id-type="pmid">36329249</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ko</surname><given-names>H</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Pichler</surname><given-names>B</given-names></name><name><surname>Buchanan</surname><given-names>KA</given-names></name><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Functional specificity of local synaptic connections in neocortical networks</article-title><source>Nature</source><volume>473</volume><fpage>87</fpage><lpage>91</lpage><pub-id pub-id-type="doi">10.1038/nature09880</pub-id><pub-id pub-id-type="pmid">21478872</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Kohonen</surname><given-names>T</given-names></name></person-group><year iso-8601-date="1984">1984</year><source>Self-Organzation and Associative Memory</source><publisher-name>Springer</publisher-name><pub-id pub-id-type="doi">10.1007/978-3-642-88163-3</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kornfeld</surname><given-names>J</given-names></name><name><surname>Denk</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Progress and remaining challenges in high-throughput volume electron microscopy</article-title><source>Current Opinion in Neurobiology</source><volume>50</volume><fpage>261</fpage><lpage>267</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2018.04.030</pub-id><pub-id pub-id-type="pmid">29758457</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lagzi</surname><given-names>F</given-names></name><name><surname>Rotter</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamics of competition between subnetworks of spiking neuronal networks in the balanced state</article-title><source>PLOS ONE</source><volume>10</volume><elocation-id>e0138947</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0138947</pub-id><pub-id pub-id-type="pmid">26407178</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lagzi</surname><given-names>F</given-names></name><name><surname>Bustos</surname><given-names>MC</given-names></name><name><surname>Oswald</surname><given-names>AM</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Assembly Formation Is Stabilized by Parvalbumin Neurons and Accelerated by Somatostatin Neurons</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.09.06.459211</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lagzi</surname><given-names>F</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Emergence of co-tuning in inhibitory neurons as a network phenomenon mediated by randomness, correlations, and homeostatic plasticity</article-title><source>Science Advances</source><volume>10</volume><elocation-id>eadi4350</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.adi4350</pub-id><pub-id pub-id-type="pmid">38507489</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langdon</surname><given-names>C</given-names></name><name><surname>Genkin</surname><given-names>M</given-names></name><name><surname>Engel</surname><given-names>TA</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>A unifying perspective on neural manifolds and circuits for cognition</article-title><source>Nature Reviews. Neuroscience</source><volume>24</volume><fpage>363</fpage><lpage>377</lpage><pub-id pub-id-type="doi">10.1038/s41583-023-00693-x</pub-id><pub-id pub-id-type="pmid">37055616</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Slow dynamics and high variability in balanced cortical networks with clustered connections</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>1498</fpage><lpage>1505</lpage><pub-id pub-id-type="doi">10.1038/nn.3220</pub-id><pub-id pub-id-type="pmid">23001062</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Litwin-Kumar</surname><given-names>A</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Formation and maintenance of neuronal assemblies through synaptic plasticity</article-title><source>Nature Communications</source><volume>5</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/ncomms6319</pub-id><pub-id pub-id-type="pmid">25395015</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mackwood</surname><given-names>O</given-names></name><name><surname>Naumann</surname><given-names>LB</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning excitatory-inhibitory neuronal assemblies in recurrent networks</article-title><source>eLife</source><volume>10</volume><elocation-id>e59715</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.59715</pub-id><pub-id pub-id-type="pmid">33900199</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>A theory of cerebellar cortex</article-title><source>The Journal of Physiology</source><volume>202</volume><fpage>437</fpage><lpage>470</lpage><pub-id pub-id-type="doi">10.1113/jphysiol.1969.sp008820</pub-id><pub-id pub-id-type="pmid">5784296</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mazzucato</surname><given-names>L</given-names></name><name><surname>Fontanini</surname><given-names>A</given-names></name><name><surname>La Camera</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Dynamics of multistable states during ongoing and evoked cortical activity</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>8214</fpage><lpage>8231</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4819-14.2015</pub-id><pub-id pub-id-type="pmid">26019337</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meissner-Bernard</surname><given-names>C</given-names></name><name><surname>Dembitskaya</surname><given-names>Y</given-names></name><name><surname>Venance</surname><given-names>L</given-names></name><name><surname>Fleischmann</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Encoding of odor fear memories in the mouse olfactory cortex</article-title><source>Current Biology</source><volume>29</volume><fpage>367</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.12.003</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Meissner-Bernard</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>pDp-model</data-title><version designator="swh:1:rev:54b9510473217f9e9d35fa14782a06a3e8416a85">swh:1:rev:54b9510473217f9e9d35fa14782a06a3e8416a85</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:26ebe16b25e95bb7e07c73916a4f0e3e3cfc9c12;origin=https://github.com/clairemb90/pDp-model;visit=swh:1:snp:edb8f9c2a10563b445b00666322dc9ef13d014bf;anchor=swh:1:rev:54b9510473217f9e9d35fa14782a06a3e8416a85">https://archive.softwareheritage.org/swh:1:dir:26ebe16b25e95bb7e07c73916a4f0e3e3cfc9c12;origin=https://github.com/clairemb90/pDp-model;visit=swh:1:snp:edb8f9c2a10563b445b00666322dc9ef13d014bf;anchor=swh:1:rev:54b9510473217f9e9d35fa14782a06a3e8416a85</ext-link></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Onasch</surname><given-names>S</given-names></name><name><surname>Festa</surname><given-names>D</given-names></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Formation and computational implications of assemblies in neural circuits</article-title><source>The Journal of Physiology</source><volume>601</volume><fpage>3071</fpage><lpage>3090</lpage><pub-id pub-id-type="doi">10.1113/JP282750</pub-id><pub-id pub-id-type="pmid">36068723</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miura</surname><given-names>K</given-names></name><name><surname>Mainen</surname><given-names>ZF</given-names></name><name><surname>Uchida</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Odor representations in olfactory cortex: distributed rate coding and decorrelated population activity</article-title><source>Neuron</source><volume>74</volume><fpage>1087</fpage><lpage>1098</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.04.021</pub-id><pub-id pub-id-type="pmid">22726838</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mueller</surname><given-names>T</given-names></name><name><surname>Dong</surname><given-names>Z</given-names></name><name><surname>Berberoglu</surname><given-names>MA</given-names></name><name><surname>Guo</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The dorsal pallium in zebrafish, <italic>Danio rerio</italic> (Cyprinidae, Teleostei)</article-title><source>Brain Research</source><volume>1381</volume><fpage>95</fpage><lpage>105</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2010.12.089</pub-id><pub-id pub-id-type="pmid">21219890</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murphy</surname><given-names>BK</given-names></name><name><surname>Miller</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Balanced amplification: A new mechanism of selective amplification of neural activity patterns</article-title><source>Neuron</source><volume>61</volume><fpage>635</fpage><lpage>648</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.02.005</pub-id><pub-id pub-id-type="pmid">19249282</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okun</surname><given-names>M</given-names></name><name><surname>Lampl</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Instantaneous correlation of excitation and inhibition during ongoing and sensory-evoked activities</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>535</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1038/nn.2105</pub-id><pub-id pub-id-type="pmid">18376400</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pashkovski</surname><given-names>SL</given-names></name><name><surname>Iurilli</surname><given-names>G</given-names></name><name><surname>Brann</surname><given-names>D</given-names></name><name><surname>Chicharro</surname><given-names>D</given-names></name><name><surname>Drummey</surname><given-names>K</given-names></name><name><surname>Franks</surname><given-names>KM</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Datta</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Structure and flexibility in cortical representations of odour space</article-title><source>Nature</source><volume>583</volume><fpage>253</fpage><lpage>258</lpage><pub-id pub-id-type="doi">10.1038/s41586-020-2451-1</pub-id><pub-id pub-id-type="pmid">32612230</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pehlevan</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Selectivity and sparseness in randomly connected balanced networks</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e89992</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0089992</pub-id><pub-id pub-id-type="pmid">24587172</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Recanatesi</surname><given-names>S</given-names></name><name><surname>Ocker</surname><given-names>GK</given-names></name><name><surname>Buice</surname><given-names>MA</given-names></name><name><surname>Shea-Brown</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dimensionality in recurrent spiking networks: Global trends in activity and local origins in connectivity</article-title><source>PLOS Computational Biology</source><volume>15</volume><elocation-id>e1006446</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1006446</pub-id><pub-id pub-id-type="pmid">31299044</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Renart</surname><given-names>A</given-names></name><name><surname>de la Rocha</surname><given-names>J</given-names></name><name><surname>Bartho</surname><given-names>P</given-names></name><name><surname>Hollender</surname><given-names>L</given-names></name><name><surname>Parga</surname><given-names>N</given-names></name><name><surname>Reyes</surname><given-names>A</given-names></name><name><surname>Harris</surname><given-names>KD</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The asynchronous state in cortical circuits</article-title><source>Science</source><volume>327</volume><fpage>587</fpage><lpage>590</lpage><pub-id pub-id-type="doi">10.1126/science.1179850</pub-id><pub-id pub-id-type="pmid">20110507</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rost</surname><given-names>T</given-names></name><name><surname>Deger</surname><given-names>M</given-names></name><name><surname>Nawrot</surname><given-names>MP</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Winnerless competition in clustered balanced networks: inhibitory assemblies do the trick</article-title><source>Biological Cybernetics</source><volume>112</volume><fpage>81</fpage><lpage>98</lpage><pub-id pub-id-type="doi">10.1007/s00422-017-0737-7</pub-id><pub-id pub-id-type="pmid">29075845</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Roudi</surname><given-names>Y</given-names></name><name><surname>Latham</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A balanced memory network</article-title><source>PLOS Computational Biology</source><volume>3</volume><elocation-id>0030141</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.0030141</pub-id><pub-id pub-id-type="pmid">17845070</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rupprecht</surname><given-names>P</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Precise synaptic balance in the zebrafish homolog of olfactory cortex</article-title><source>Neuron</source><volume>100</volume><fpage>669</fpage><lpage>683</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.09.013</pub-id><pub-id pub-id-type="pmid">30318416</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rupprecht</surname><given-names>P</given-names></name><name><surname>Carta</surname><given-names>S</given-names></name><name><surname>Hoffmann</surname><given-names>A</given-names></name><name><surname>Echizen</surname><given-names>M</given-names></name><name><surname>Blot</surname><given-names>A</given-names></name><name><surname>Kwan</surname><given-names>AC</given-names></name><name><surname>Dan</surname><given-names>Y</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Kitamura</surname><given-names>K</given-names></name><name><surname>Helmchen</surname><given-names>F</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A database and deep learning toolbox for noise-optimized, generalized spike inference from calcium imaging</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1324</fpage><lpage>1337</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00895-5</pub-id><pub-id pub-id-type="pmid">34341584</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ryan</surname><given-names>TJ</given-names></name><name><surname>Roy</surname><given-names>DS</given-names></name><name><surname>Pignatelli</surname><given-names>M</given-names></name><name><surname>Arons</surname><given-names>A</given-names></name><name><surname>Tonegawa</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Memory: engram cells retain memory under retrograde amnesia</article-title><source>Science</source><volume>348</volume><fpage>1007</fpage><lpage>1013</lpage><pub-id pub-id-type="doi">10.1126/science.aaa5542</pub-id><pub-id pub-id-type="pmid">26023136</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sacco</surname><given-names>T</given-names></name><name><surname>Sacchetti</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Role of secondary sensory cortices in emotional memory storage and retrieval in rats</article-title><source>Science</source><volume>329</volume><fpage>649</fpage><lpage>656</lpage><pub-id pub-id-type="doi">10.1126/science.1183165</pub-id><pub-id pub-id-type="pmid">20689011</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeh</surname><given-names>S</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Patterned perturbation of inhibition can reveal the dynamical structure of neural processing</article-title><source>eLife</source><volume>9</volume><elocation-id>e52757</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.52757</pub-id><pub-id pub-id-type="pmid">32073400</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeh</surname><given-names>S</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Inhibitory stabilization and cortical computation</article-title><source>Nature Reviews. Neuroscience</source><volume>22</volume><fpage>21</fpage><lpage>37</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-00390-z</pub-id><pub-id pub-id-type="pmid">33177630</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sadeh</surname><given-names>S</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Excitatory-inhibitory balance modulates the formation and dynamics of neuronal assemblies in cortical networks</article-title><source>Science Advances</source><volume>7</volume><elocation-id>eabg8411</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abg8411</pub-id><pub-id pub-id-type="pmid">34731002</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schaffer</surname><given-names>ES</given-names></name><name><surname>Stettler</surname><given-names>DD</given-names></name><name><surname>Kato</surname><given-names>D</given-names></name><name><surname>Choi</surname><given-names>GB</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Odor perception on the two sides of the brain: consistency despite randomness</article-title><source>Neuron</source><volume>98</volume><fpage>736</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.04.004</pub-id><pub-id pub-id-type="pmid">29706585</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schoonover</surname><given-names>CE</given-names></name><name><surname>Ohashi</surname><given-names>SN</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name><name><surname>Fink</surname><given-names>AJP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Representational drift in primary olfactory cortex</article-title><source>Nature</source><volume>594</volume><fpage>541</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03628-7</pub-id><pub-id pub-id-type="pmid">34108681</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schulz</surname><given-names>A</given-names></name><name><surname>Miehl</surname><given-names>C</given-names></name><name><surname>Berry</surname><given-names>MJ</given-names><suffix>II</suffix></name><name><surname>Gjorgjieva</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The generation of cortical novelty responses through inhibitory plasticity</article-title><source>eLife</source><volume>10</volume><elocation-id>e65309</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.65309</pub-id><pub-id pub-id-type="pmid">34647889</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Newsome</surname><given-names>WT</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Noise, neural codes and cortical organization</article-title><source>Current Opinion in Neurobiology</source><volume>4</volume><fpage>569</fpage><lpage>579</lpage><pub-id pub-id-type="doi">10.1016/0959-4388(94)90059-0</pub-id><pub-id pub-id-type="pmid">7812147</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stettler</surname><given-names>DD</given-names></name><name><surname>Axel</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Representations of odor in the piriform cortex</article-title><source>Neuron</source><volume>63</volume><fpage>854</fpage><lpage>864</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.09.005</pub-id><pub-id pub-id-type="pmid">19778513</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabor</surname><given-names>R</given-names></name><name><surname>Yaksi</surname><given-names>E</given-names></name><name><surname>Weislogel</surname><given-names>JM</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Processing of odor mixtures in the zebrafish olfactory bulb</article-title><source>The Journal of Neuroscience</source><volume>24</volume><fpage>6611</fpage><lpage>6620</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1834-04.2004</pub-id><pub-id pub-id-type="pmid">15269273</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tabor</surname><given-names>R</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Pharmacological analysis of ionotropic glutamate receptor function in neuronal circuits of the zebrafish olfactory bulb</article-title><source>PLOS ONE</source><volume>3</volume><elocation-id>e1416</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0001416</pub-id><pub-id pub-id-type="pmid">18183297</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>MV</given-names></name><name><surname>Skaggs</surname><given-names>WE</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Paradoxical effects of external modulation of inhibitory interneurons</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>4382</fpage><lpage>4388</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-11-04382.1997</pub-id><pub-id pub-id-type="pmid">9151754</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van Vreeswijk</surname><given-names>C</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Chaos in neuronal networks with balanced excitatory and inhibitory activity</article-title><source>Science</source><volume>274</volume><fpage>1724</fpage><lpage>1726</lpage><pub-id pub-id-type="doi">10.1126/science.274.5293.1724</pub-id><pub-id pub-id-type="pmid">8939866</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vogels</surname><given-names>TP</given-names></name><name><surname>Sprekeler</surname><given-names>H</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Clopath</surname><given-names>C</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Inhibitory plasticity balances excitation and inhibition in sensory pathways and memory networks</article-title><source>Science</source><volume>334</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1126/science.1212991</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wanner</surname><given-names>AA</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Whitening of odor representations by the wiring diagram of the olfactory bulb</article-title><source>Nature Neuroscience</source><volume>01</volume><fpage>1</fpage><lpage>24</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0576-z</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wehr</surname><given-names>M</given-names></name><name><surname>Zador</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Balanced inhibition underlies tuning and sharpens spike timing in auditory cortex</article-title><source>Nature</source><volume>426</volume><fpage>442</fpage><lpage>446</lpage><pub-id pub-id-type="doi">10.1038/nature02116</pub-id><pub-id pub-id-type="pmid">14647382</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wiechert</surname><given-names>MT</given-names></name><name><surname>Judkewitz</surname><given-names>B</given-names></name><name><surname>Riecke</surname><given-names>H</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Mechanisms of pattern decorrelation by recurrent neuronal circuits</article-title><source>Nature Neuroscience</source><volume>13</volume><fpage>1003</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1038/nn.2591</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>DA</given-names></name><name><surname>Sullivan</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cortical processing of odor objects</article-title><source>Neuron</source><volume>72</volume><fpage>506</fpage><lpage>519</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2011.10.027</pub-id><pub-id pub-id-type="pmid">22099455</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>YK</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Nonlinear transient amplification in recurrent neural networks with short-term plasticity</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.09.447718</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yaksi</surname><given-names>E</given-names></name><name><surname>von Saint Paul</surname><given-names>F</given-names></name><name><surname>Niessing</surname><given-names>J</given-names></name><name><surname>Bundschuh</surname><given-names>ST</given-names></name><name><surname>Friedrich</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Transformation of odor representations in target areas of the olfactory bulb</article-title><source>Nature Neuroscience</source><volume>12</volume><fpage>474</fpage><lpage>482</lpage><pub-id pub-id-type="doi">10.1038/nn.2288</pub-id><pub-id pub-id-type="pmid">19305401</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yassa</surname><given-names>MA</given-names></name><name><surname>Stark</surname><given-names>CEL</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Pattern separation in the hippocampus</article-title><source>Trends in Neurosciences</source><volume>34</volume><fpage>515</fpage><lpage>525</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2011.06.006</pub-id><pub-id pub-id-type="pmid">21788086</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Agnes</surname><given-names>EJ</given-names></name><name><surname>Gerstner</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Diverse synaptic plasticity mechanisms orchestrated to form and retrieve memories in spiking neural networks</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>6922</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms7922</pub-id><pub-id pub-id-type="pmid">25897632</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Znamenskiy</surname><given-names>P</given-names></name><name><surname>Kim</surname><given-names>MH</given-names></name><name><surname>Muir</surname><given-names>DR</given-names></name><name><surname>Iacaruso</surname><given-names>MF</given-names></name><name><surname>Hofer</surname><given-names>SB</given-names></name><name><surname>Mrsic-Flogel</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="2024">2024</year><article-title>Functional specificity of recurrent inhibition in visual cortex</article-title><source>Neuron</source><volume>112</volume><fpage>991</fpage><lpage>1000</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2023.12.013</pub-id><pub-id pub-id-type="pmid">38244539</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96303.3.sa0</article-id><title-group><article-title>eLife Assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gjorgjieva</surname><given-names>Julijana</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Technical University of Munich</institution><country>Germany</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Compelling</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Important</kwd></kwd-group></front-stub><body><p>This <bold>important</bold> study introduces a biologically constrained model of telencephalic area of adult zebrafish to highlight the significance of precisely balanced memory networks in olfactory processing. The authors provide <bold>compelling</bold> evidence that their model performs better in multiple situations (for e.g. in terms of network stability and shaping the geometry of representations), compared to traditional attractor networks and persistent activity. The work supports recent studies reporting functional E/I subnetworks in several sensory cortexes, and will be of interest to both theoretical and experimental neuroscientists studying network dynamics based on structured excitatory and inhibitory interactions.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96303.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>Meissner-Bernard et al present a biologically constrained model of telencephalic area of adult zebrafish, a homologous area to the piriform cortex, and argue for the role of precisely balanced memory networks in olfactory processing.</p><p>This is interesting as it can add to recent evidence on the presence of functional subnetworks in multiple sensory cortices. It is also important in deviating from traditional accounts of memory systems as attractor networks. Evidence for attractor networks has been found in some systems, like in the head direction circuits in the flies. However, the presence of attractor dynamics in other modalities, like sensory systems, and their role in computation has been more contentious. This work contributes to this active line of research in experimental and computational neuroscience by suggesting that, rather than being represented in attractor networks and persistent activity, olfactory memories might be coded by balanced excitation-inhibitory subnetworks.</p><p>Strengths:</p><p>The main strength of the work is in: (1) direct link to biological parameters and measurements, (2) good controls and quantification of the results, and (3) comparison across multiple models.</p><p>(1) The authors have done a good job of gathering the current experimental information to inform a biological-constrained spiking model of the telencephalic area of adult zebrafish. The results are compared to previous experimental measurements to choose the right regimes of operation.</p><p>(2) Multiple quantification metrics and controls are used to support the main conclusions, and to ensure that the key parameters are controlled for - e.g. when comparing across multiple models.</p><p>(3) Four specific models (random, scaled I / attractor, and two variant of specific E-I networks - tuned I and tuned E+I) are compared with different metrics, helping to pinpoint which features emerge in which model.</p><p>In the revised manuscript, the authors have also:</p><p>(a) made a good effort to provide a mechanistic explanation of their results (especially on the mechanism underlying medium amplification in specific E/I network models);</p><p>(b) performed a systematic analysis of the parameter space by changing different parameters of E and I neurons (specifically showing that different time constants of E and I neurons do not change the results and therefore the main effects result from connectivity);</p><p>(c) added further analysis and discussion on the potential functional and computational significance of balanced specific E-I subnetworks.</p><p>These additions substantially strengthen the study, presenting compelling evidence for how networks with specific E-I structure can underpin olfactory processing and memory representations. The findings have potential implications that extend beyond the olfactory system and may be applicable to other neural systems and species.</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96303.3.sa2</article-id><title-group><article-title>Reviewer #2 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>The authors conducted a comparative analysis of four networks, varying in the presence of excitatory assemblies and the architecture of inhibitory cell assembly connectivity. They found that co-tuned E-I assemblies provide network stability and a continuous representation of input patterns (on locally constrained manifolds), contrasting with networks with global inhibition that result in attractor networks.</p><p>Strengths:</p><p>The findings presented in this paper are very interesting and cutting-edge. The manuscript effectively conveys the message and presents a creative way to represent high-dimensional inputs and network responses. Particularly, the result regarding the projection of input patterns onto local manifolds and continuous representation of input/memory is very Intriguing and novel. Both computational and experimental neuroscientists would find value in reading the paper.</p><p>Weaknesses:</p><p>Intuitively, classification (decodability) in discrete attractor networks is much better than in networks with continuous representations. This could also be shown in Figure 5B, along with the performance of the random and tuned E-I networks. The latter networks have the advantage of providing network stability compared to the Scaled I network, but at the cost of reduced network salience and, therefore, reduced input decodability. Thus, tuned E-I networks cannot always perform better than any other network.</p></body></sub-article><sub-article article-type="referee-report" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96303.3.sa3</article-id><title-group><article-title>Reviewer #3 (Public review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary:</p><p>This work investigates computational consequences of assemblies containing both excitatory and inhibitory neurons (E/I assembly) in a model with parameters constrained by experimental data from the telencephalic area Dp of zebrafish. The authors show how this precise E/I balance shapes the geometry of neuronal dynamics in comparison to unstructured networks and networks with more global inhibitory balance. Specifically, E/I assemblies lead to the activity being locally restricted onto manifolds - a dynamical structure in-between high-dimensional representations in unstructured networks and discrete attractors in networks with global inhibitory balance. Furthermore, E/I assemblies lead to smoother representations of mixtures of stimuli while those stimuli can still be reliably classified, and allows for more robust learning of additional stimuli.</p><p>Strengths:</p><p>Since experimental studies do suggest that E/I balance is very precise and E/I assemblies exist, it is important to study the consequences of those connectivity structures on network dynamics. The authors convincingly show that E/I assemblies lead to different geometries of stimulus representation compared to unstructured networks and networks with global inhibition. This finding might open the door for future studies for exploring the functional advantage of these locally defined manifolds, and how other network properties allow to shape those manifolds.</p><p>The authors also make sure that their spiking model is well-constrained by experimental data from the zebrafish pDp. Both, spontaneous and odor stimulus triggered spiking activity is within the range of experimental measurements. But the model is also general enough to be potentially applied to findings in other animal models and brain regions.</p><p>Weaknesses:</p><p>All my previous points have been addressed.</p></body></sub-article><sub-article article-type="author-comment" id="sa4"><front-stub><article-id pub-id-type="doi">10.7554/eLife.96303.3.sa4</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Meissner-Bernard</surname><given-names>Claire</given-names></name><role specific-use="author">Author</role><aff><institution>Friedrich Miescher Institute for Biomedical Research</institution><addr-line><named-content content-type="city">Basel</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Zenke</surname><given-names>Friedemann</given-names></name><role specific-use="author">Author</role><aff><institution>Friedrich Miescher Institute for Biomedical Research</institution><addr-line><named-content content-type="city">Basel</named-content></addr-line><country>Switzerland</country></aff></contrib><contrib contrib-type="author"><name><surname>Friedrich</surname><given-names>Rainer W</given-names></name><role specific-use="author">Author</role><aff><institution>Friedrich Miescher Institute for Biomedical Research</institution><addr-line><named-content content-type="city">Basel</named-content></addr-line><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authors’ response to the original reviews.</p><p>The revised manuscript contains new results and additional text. Major revisions:</p><p>(1) Additional simulations and analyses of networks with different biophysical parameters and with identical time constants for E and I neurons (Methods, Supplementary Fig. 5).</p><p>(2) Additional simulations and analyses of networks with modifications of connectivity parameters to further analyze effects of E/I assemblies on manifold geometry (Supplementary Fig. 6).</p><p>(3) Analysis of synaptic current components (Figure 3 D-F; to analyze mechanism of modest amplification in <italic>Tuned</italic> networks).</p><p>(4) More detailed explanation of pattern completion analysis (Results).</p><p>(5) Analysis of classification performance of <italic>Scaled</italic> networks (Supplementary Fig.8).</p><p>(6) Additional analysis (Figure 5D-F) and discussion (particularly section 'Computational functions of networks with E/I assemblies') of functional benefits of continuous representations in networks with E-I assemblies.</p><disp-quote content-type="editor-comment"><p><bold>Public Reviews:</bold></p><p><bold>Reviewer #1 (Public Review):</bold></p><p>Summary:</p><p>Meissner-Bernard et al present a biologically constrained model of telencephalic area of adult zebrafish, a homologous area to the piriform cortex, and argue for the role of precisely balanced memory networks in olfactory processing.</p><p>This is interesting as it can add to recent evidence on the presence of functional subnetworks in multiple sensory cortices. It is also important in deviating from traditional accounts of memory systems as attractor networks. Evidence for attractor networks has been found in some systems, like in the head direction circuits in the flies. However, the presence of attractor dynamics in other modalities, like sensory systems, and their role in computation has been more contentious. This work contributes to this active line of research in experimental and computational neuroscience by suggesting that, rather than being represented in attractor networks and persistent activity, olfactory memories might be coded by balanced excitation-inhibitory subnetworks.</p><p>Strengths:</p><p>The main strength of the work is in: (1) direct link to biological parameters and measurements, (2) good controls and quantification of the results, and (3) comparison across multiple models.</p><p>(1) The authors have done a good job of gathering the current experimental information to inform a biological-constrained spiking model of the telencephalic area of adult zebrafish. The results are compared to previous experimental measurements to choose the right regimes of operation.</p><p>(2) Multiple quantification metrics and controls are used to support the main conclusions and to ensure that the key parameters are controlled for - e.g. when comparing across multiple models. (3) Four specific models (random, scaled I / attractor, and two variant of specific E-I networks - tuned I and tuned E+I) are compared with different metrics, helping to pinpoint which features emerge in which model.</p><p>Weaknesses:</p><p>Major problems with the work are: (1) mechanistic explanation of the results in specific E-I networks, (2) parameter exploration, and (3) the functional significance of the specific E-I model.</p><p>(1) The main problem with the paper is a lack of mechanistic analysis of the models. The models are treated like biological entities and only tested with different assays and metrics to describe their different features (e.g. different geometry of representation in Fig. 4). Given that all the key parameters of the models are known and can be changed (unlike biological networks), it is expected to provide a more analytical account of why specific networks show the reported results. For instance, what is the key mechanism for medium amplification in specific E/I network models (Fig. 3)? How does the specific geometry of representation/manifolds (in Fig. 4) emerge in terms of excitatory-inhibitory interactions, and what are the main mechanisms/parameters? Mechanistic account and analysis of these results are missing in the current version of the paper.</p></disp-quote><p>We agree that further mechanistic insights would be of interest and addressed this issue at different levels:</p><p>(1) Biophysical parameters: to determine whether network behavior depends on specific choices of biophysical parameters in E and I neurons we equalized biophysical parameters across neuron types. The main observations are unchanged, suggesting that the observed effects depend primarily on network connectivity (see also response to comment [2]).</p><p>(2) Mechanism of modest amplification in E/I assemblies: analyzing the different components of the synaptic currents demonstrate that the modest amplification of activity in <italic>Tuned</italic> networks results from an 'imperfect' balance of <italic>recurrent</italic> excitation and inhibition within assemblies (see new Figures 3D-F and text p.7). Hence, E/I co-tuning substantially reduces the net amplification in <italic>Tuned</italic> networks as compared to <italic>Scaled</italic> networks, thus preventing discrete attractor dynamics and stabilizing network activity, but a modest amplification still occurs, consistent with biological observations.</p><p>(3) Representational geometry: to obtain insights into the network mechanisms underlying effects of E/I assemblies on the geometry of population activity we tested the hypothesis that geometrical changes depend, at least in part, on the modest amplification of activity within E/I assemblies (see Supplementary Figure 6). We changed model parameters to either prevent the modest amplification in <italic>Tuned</italic> networks (increasing I-to-E connectivity within assemblies) or introduce a modest amplification in subsets of neurons by other mechanisms (concentration-dependent increase in the excitability of pseudo-assembly neurons; <italic>Scaled</italic> I networks with reduced connectivity within assemblies). Manipulations that introduced a modest, input-dependent amplification in neuronal subsets had geometrical effects similar to those observed in <italic>Tuned</italic> networks, whereas manipulations that prevented a modest amplification abolished these effects (Supplementary Figure 6). Note however that these manipulations generated different firing rate distributions. These results provide a starting point for more detailed analyses of the relationship between network connectivity and representational geometry (see p.12).</p><p>In summary, our additional analyses indicate that effects of E/I assemblies on representational geometry depend primarily on network connectivity, rather than specific biophysical parameters, and that the resulting modest amplification of activity within assemblies makes an important contribution. Further analyses may reveal more specific relationships between E/I assemblies and representational geometry, but such analyses are beyond the scope of this study.</p><disp-quote content-type="editor-comment"><p>(2) The second major issue with the study is a lack of systematic exploration and analysis of the parameter space. Some parameters are biologically constrained, but not all the parameters. For instance, it is not clear what the justification for the choice of synaptic time scales are (with E synaptic time constants being larger than inhibition: tau_syn_i = 10 ms, tau_syn_E = 30 ms). How would the results change if they are varying these - and other unconstrained - parameters? It is important to show how the main results, especially the manifold localisation, would change by doing a systematic exploration of the key parameters and performing some sensitivity analysis. This would also help to see how robust the results are, which parameters are more important and which parameters are less relevant, and to shed light on the key mechanisms.</p></disp-quote><p>We thank the reviewer for raising this point. We chose a relatively slow time constant for excitatory synapses because experimental data indicate that excitatory synaptic currents in Dp and piriform cortex contain a prominent NMDA component. Nevertheless, to assess whether network behavior depends on specific choices of biophysical parameters in E and I neurons, we have performed additional simulations with equal synaptic time constants and equal biophysical parameters for all neurons. Each neuron also received the same number of inputs from each population (see revised Methods). Results were similar to those observed previously (Supplementary Fig.5 and p.9 of main text). We therefore conclude that the main effects observed in <italic>Tuned</italic> networks cannot be explained by differences in biophysical parameters between E and I neurons but is primarily a consequence of network connectivity.</p><disp-quote content-type="editor-comment"><p>(3) It is not clear what the main functional advantage of the specific E-I network model is compared to random networks. In terms of activity, they show that specific E-I networks amplify the input more than random networks (Fig. 3). But when it comes to classification, the effect seems to be very small (Fig. 5c). Description of different geometry of representation and manifold localization in specific networks compared to random networks is good, but it is more of an illustration of different activity patterns than proving a functional benefit for the network. The reader is still left with the question of what major functional benefits (in terms of computational/biological processing) should be expected from these networks, if they are to be a good model for olfactory processing and learning.</p><p>One possibility for instance might be that the tasks used here are too easy to reveal the main benefits of the specific models - and more complex tasks would be needed to assess the functional enhancement (e.g. more noisy conditions or more combination of odours). It would be good to show this more clearly - or at least discuss it in relation to computation and function.</p></disp-quote><p>In the previous manuscript, the analysis of potential computational benefits other than pattern classification was limited and the discussion of this issue was condensed into a single itemized paragraph to avoid excessive speculation. Although a thorough analysis of potential computational benefits exceeds the scope of a single paper, we agree with the reviewer that this issue is of interest and therefore added additional analyses and discussion.</p><p>In the initial manuscript we analyzed pattern classification primarily to investigate whether <italic>Tuned</italic> networks can support this function at all, given that they do not exhibit discrete attractor states. We found this to be the case, which we consider a first important result.</p><p>Furthermore, we found that precise balance of E/I assemblies can protect networks against catastrophic firing rate instabilities when assemblies are added sequentially, as in continual learning. Results from these simulations are now described and discussed in more detail (see Results p.11 and Discussion p.13).</p><p>In the revised manuscript, we now also examine additional potential benefits of <italic>Tuned</italic> networks and discuss them in more detail (see new Figure 5D-F and text p.11). One hypothesis is that continuous representations provide a distance metric between a given input and relevant (learned) stimuli. To address this hypothesis, we (1) performed regression analysis and (2) trained support vector machines (SVMs) to predict the concentration of a given odor in a mixture based on population activity. In both cases, <italic>Tuned E+I</italic> networks outperformed <italic>Scaled</italic> and _rand n_etworks in predicting the concentration of learned odors across a wide range mixtures (Figure 5D-F). E/I assemblies therefore support the quantification of learned odors within mixtures or, more generally, assessments of how strongly a (potentially complex) input is related to relevant odors stored in memory. Such a metric assessment of stimulus quality is not well supported by discrete attractor networks because inputs are mapped onto discrete network states.</p><p>The observation that <italic>Tuned</italic> networks do not map inputs onto discrete outputs indicates that such networks do not classify inputs as distinct items. Nonetheless, the observed geometrical modifications of continuous representations support the classification of learned inputs or the assessment of metric relationships by hypothetical readout neurons. Geometrical modifications of odor representations may therefore serve as one of multiple steps in multi-layer computations for pattern classification (and/or other computations). In this scenario, the transformation of odor representations in Dp may be seen as related to transformations of representations between different layers in artificial networks, which collectively perform a given task (notwithstanding obvious structural and mechanistic differences between artificial and biological networks). In other words, geometrical transformations of representations in <italic>Tuned</italic> networks may overrepresent learned (relevant) information at the expense of other information and thereby support further learning processes in other brain areas. An obvious corollary of this scenario is that Dp does not perform odor classification <italic>per se</italic> based on inputs from the olfactory bulb but reformats representations of odor space based on experience to support computational tasks as part of a larger system. This scenario is now explicitly discussed (p.14).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Summary:</p><p>The authors conducted a comparative analysis of four networks, varying in the presence of excitatory assemblies and the architecture of inhibitory cell assembly connectivity. They found that co-tuned E-I assemblies provide network stability and a continuous representation of input patterns (on locally constrained manifolds), contrasting with networks with global inhibition that result in attractor networks.</p><p>Strengths:</p><p>The findings presented in this paper are very interesting and cutting-edge. The manuscript effectively conveys the message and presents a creative way to represent high-dimensional inputs and network responses. Particularly, the result regarding the projection of input patterns onto local manifolds and continuous representation of input/memory is very Intriguing and novel. Both computational and experimental neuroscientists would find value in reading the paper.</p><p>Weaknesses:</p><p>that have continuous representations. This could also be shown in Figure 5B, along with the performance of the random and tuned E-I networks. The latter networks have the advantage of providing network stability compared to the Scaled I network, but at the cost of reduced network salience and, therefore, reduced input decodability. The authors may consider designing a decoder to quantify and compare the classification performance of all four networks.</p></disp-quote><p>We have now quantified classification by networks with discrete attractor dynamics (<italic>Scaled</italic>) along with other networks. However, because the neuronal covariance matrix for such networks is low rank and not invertible, pattern classification cannot be analyzed by QDA as in Figure 5B. We therefore classified patterns from the odor subspace by template matching, assigning test patterns to one of the four classes based on correlations (see Supplementary Figure 8). As expected, <italic>Scaled</italic> networks performed well, but they did not outperform <italic>Tuned</italic> networks. Moreover, the performance of <italic>Scaled</italic> networks, but not <italic>Tuned</italic> networks, depended on the order in which odors were presented to the network. This hysteresis effect is a direct consequence of persistent attractor states and decreased the general classification performance of <italic>Scaled</italic> networks (see Supplementary Figure 8 for details). These results confirm the prediction that networks with discrete attractor states can efficiently classify inputs, but also reveal disadvantages arising from attractor dynamics. Moreover, the results indicate that the classification performance of <italic>Tuned</italic> networks is also high under the given task conditions, which simulate a biologically realistic scenario.</p><p>We would also like to emphasize that classification may not be the only task, and perhaps not even a main task, of Dp/piriform cortex or other memory networks with E/I assemblies. Conceivably, other computations could include metric assessments of inputs relative to learned inputs or additional learning-related computations. Please see our response to comment (3) of reviewer 1 for a further discussion of this issue.</p><disp-quote content-type="editor-comment"><p>Networks featuring E/I assemblies could potentially represent multistable attractors by exploring the parameter space for their reciprocal connectivity and connectivity with the rest of the network. However, for co-tuned E-I networks, the scope for achieving multistability is relatively constrained compared to networks employing global or lateral inhibition between assemblies. It would be good if the authors mentioned this in the discussion. Also, the fact that reciprocal inhibition increases network stability has been shown before and should be cited in the statements addressing network stability (e.g., some of the citations in the manuscript, including Rost et al. 2018, Lagzi &amp; Fairhall 2022, and Vogels et al. 2011 have shown this).</p></disp-quote><p>We thank the reviewer for this comment. We now explicitly discuss multistability (see p. 12) and refer to additional references in the statements addressing network stability.</p><disp-quote content-type="editor-comment"><p>Providing raster plots of the pDp network for familiar and novel inputs would help with understanding the claims regarding continuous versus discrete representation of inputs, allowing readers to visualize the activity patterns of the four different networks. (similar to Figure 1B).</p></disp-quote><p>We thank the reviewer for this suggestion. We have added raster plots of responses to both familiar and novel inputs in the revised manuscript (Figure 2D and Supplementary Figure 4A).</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Public Review):</bold></p><p>Summary:</p><p>This work investigates the computational consequences of assemblies containing both excitatory and inhibitory neurons (E/I assembly) in a model with parameters constrained by experimental data from the telencephalic area Dp of zebrafish. The authors show how this precise E/I balance shapes the geometry of neuronal dynamics in comparison to unstructured networks and networks with more global inhibitory balance. Specifically, E/I assemblies lead to the activity being locally restricted onto manifolds - a dynamical structure in between high-dimensional representations in unstructured networks and discrete attractors in networks with global inhibitory balance. Furthermore, E/I assemblies lead to smoother representations of mixtures of stimuli while those stimuli can still be reliably classified, and allow for more robust learning of additional stimuli.</p><p>Strengths:</p><p>Since experimental studies do suggest that E/I balance is very precise and E/I assemblies exist, it is important to study the consequences of those connectivity structures on network dynamics. The authors convincingly show that E/I assemblies lead to different geometries of stimulus representation compared to unstructured networks and networks with global inhibition. This finding might open the door for future studies for exploring the functional advantage of these locally defined manifolds, and how other network properties allow to shape those manifolds.</p><p>The authors also make sure that their spiking model is well-constrained by experimental data from the zebrafish pDp. Both spontaneous and odor stimulus triggered spiking activity is within the range of experimental measurements. But the model is also general enough to be potentially applied to findings in other animal models and brain regions.</p><p>Weaknesses:</p><p>I find the point about pattern completion a bit confusing. In Fig. 3 the authors argue that only the Scaled I network can lead to pattern completion for morphed inputs since the output correlations are higher than the input correlations. For me, this sounds less like the network can perform pattern completion but it can nonlinearly increase the output correlations. Furthermore, in Suppl. Fig. 3 the authors show that activating half the assembly does lead to pattern completion in the sense that also non-activated assembly cells become highly active and that this pattern completion can be seen for Scaled I, Tuned E+I, and Tuned I networks. These two results seem a bit contradictory to me and require further clarification, and the authors might want to clarify how exactly they define pattern completion.</p></disp-quote><p>We believe that this comment concerns a semantic misunderstanding and apologize for any lack of clarity. We added a definition of pattern completion in the text: '…the retrieval of the whole memory from noisy or corrupted versions of the learned input.'. Pattern completion may be assessed using different procedures. In computational studies, it is often analyzed by delivering input to a subset of the assembly neurons which store a given memory (partial activation). Under these conditions, we find recruitment of the entire assembly in all structured networks, as demonstrated in Supplementary Figure 3. However, these conditions are unlikely to occur during odor presentation because the majority of neurons do not receive any input.</p><p>Another more biologically motivated approach to assess pattern completion is to gradually modify a realistic odor input into a learned input, thereby gradually increasing the overlap between the two inputs. This approach had been used previously in experimental studies (references added to the text p.6). In the presence of assemblies, recurrent connectivity is expected to recruit assembly neurons (and thus retrieve the stored pattern) more efficiently as the learned pattern is approached. This should result in a nonlinear increase in the similarity between the evoked and the learned activity pattern. This signature was prominent in <italic>Scaled</italic> networks but not in <italic>Tuned</italic> or <italic>rand</italic> networks. Obviously, the underlying procedure is different from the partial activation of the assembly described above because input patterns target many neurons (including neurons outside assemblies) and exhibit a biologically realistic distribution of activity. However, this approach has also been referred to as 'pattern completion' in the neuroscience literature, which may be the source of semantic confusion here. To clarify the difference between these approaches we have now revised the text and explicitly described each procedure in more detail (see p.6).</p><disp-quote content-type="editor-comment"><p>The authors argue that Tuned E+I networks have several advantages over Scaled I networks. While I agree with the authors that in some cases adding this localized E/I balance is beneficial, I believe that a more rigorous comparison between Tuned E+I networks and Scaled I networks is needed: quantification of variance (Fig. 4G) and angle distributions (Fig. 4H) should also be shown for the Scaled I network. Similarly in Fig. 5, what is the Mahalanobis distance for Scaled I networks and how well can the Scaled I network be classified compared to the Tuned E+I network? I suspect that the Scaled I network will actually be better at classifying odors compared to the E+I network. The authors might want to speculate about the benefit of having networks with both sources of inhibition (local and global) and hence being able to switch between locally defined manifolds and discrete attractor states.</p></disp-quote><p>We agree that a more rigorous comparison of Tuned and Scaled networks would be of interest. We have added the variance analysis (Fig 4G) and angle distributions (Fig. 4H) for both Tuned I and Scaled networks. However, the Mahalanobis distances and Quadratic Discriminant Analysis cannot be applied to Scaled networks because their neuronal covariance matrix is low rank and not invertible_._ To nevertheless compare these networks, we performed template matching by assigning test patterns to one of the four odor classes based on correlations to template patterns (Supplementary Figure 8; see also response to the first comment of reviewer 2). Interestingly, <italic>Scaled</italic> networks performed well at classification but did not outperform <italic>Tuned</italic> networks, and exhibited disadvantages arising from attractor dynamics (Supplementary Figure 8; see also response to the first comment of reviewer 2)<italic>.</italic> Furthermore, in further analyses we found that continuous representational manifolds support metric assessments of inputs relative to learned odors, which cannot be achieved by discrete representations. These results are now shown in Figure 5D-E and discussed explicitly in the text on p.11 (see also response to comment 3 of reviewer 1).</p><p>We preferred not to add a sentence in the Discussion about benefits of networks having both sources of inhibition, as we find this a bit too speculative.</p><disp-quote content-type="editor-comment"><p>At a few points in the manuscript, the authors use statements without actually providing evidence in terms of a Figure. Often the authors themselves acknowledge this, by adding the term &quot;not shown&quot; to the end of the sentence. I believe it will be helpful to the reader to be provided with figures or panels in support of the statements.</p></disp-quote><p>Thank you for this comment. We have provided additional data figures to support the following statements:</p><p>'d<sub>M</sub> was again increased upon learning, particularly between learned odors and reference classes representing other odors (Supplementary Figure 9)'</p><p>'decreasing amplification in assemblies of Scaled networks changed transformations towards the intermediate behavior, albeit with broader firing rate distributions than in Tuned networks (Supplementary Figure 6 B)'</p><disp-quote content-type="editor-comment"><p><bold>Recommendations for the authors:</bold></p><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>Meissner-Bernard et al present a biologically constrained model of telencephalic area of adult zebrafish, a homologous area to the piriform cortex, and argue for the role of precisely balanced memory networks in olfactory processing.</p><p>This is interesting as it can add to recent evidence on the presence of functional subnetworks in multiple sensory cortices. It is also important in deviating from traditional accounts of memory systems as attractor networks. Evidence for attractor networks has been found in some systems, like in the head direction circuits in the flies. However, the presence of attractor dynamics in other modalities, like sensory systems, and their role in computation has been more contentious. This work contributes to this active line of research in experimental and computational neuroscience by suggesting that, rather than being represented in attractor networks and persistent activity, olfactory memories might be coded by balanced excitation-inhibitory subnetworks.</p><p>The paper is generally well-written, the figures are informative and of good quality, and multiple approaches and metrics have been used to test and support the main results of the paper.</p><p>The main strength of the work is in: (1) direct link to biological parameters and measurements, (2) good controls and quantification of the results, and (3) comparison across multiple models.</p><p>(1) The authors have done a good job of gathering the current experimental information to inform a biological-constrained spiking model of the telencephalic area of adult zebrafish. The results are compared to previous experimental measurements to choose the right regimes of operation.</p><p>(2) Multiple quantification metrics and controls are used to support the main conclusions and to ensure that the key parameters are controlled for - e.g. when comparing across multiple models. (3) Four specific models (random, scaled I / attractor, and two variant of specific E-I networks - tuned I and tuned E+I) are compared with different metrics, helping to pinpoint which features emerge in which model.</p><p>Major problems with the work are: (1) mechanistic explanation of the results in specific E-I networks, (2) parameter exploration, and (3) the functional significance of the specific E-I model.</p><p>(1) The main problem with the paper is a lack of mechanistic analysis of the models. The models are treated like biological entities and only tested with different assays and metrics to describe their different features (e.g. different geometry of representation in Fig. 4). Given that all the key parameters of the models are known and can be changed (unlike biological networks), it is expected to provide a more analytical account of why specific networks show the reported results. For instance, what is the key mechanism for medium amplification in specific E/I network models (Fig. 3)? How does the specific geometry of representation/manifolds (in Fig. 4) emerge in terms of excitatory-inhibitory interactions, and what are the main mechanisms/parameters? Mechanistic account and analysis of these results are missing in the current version of the paper.</p><p>Precise balancing of excitation and inhibition in subnetworks would lead to the cancellation of specific dynamical modes responsible for the amplification of responses (hence, deviating from the attractor dynamics with an unstable specific mode). What is the key difference in the specific E/I networks here (tuned I or/and tuned E+I) which make them stand between random and attractor networks? Excitatory and inhibitory neurons have different parameters in the model (Table 1). Time constants of inhibitory and excitatory synapses are also different (P. 13). Are these parameters causing networks to be effectively more excitation dominated (hence deviating from a random spectrum which would be expected from a precisely balanced E/I network, with exactly the same parameters of E and I neurons)? It is necessary to analyse the network models, describe the key mechanism for their amplification, and pinpoint the key differences between E and I neurons which are crucial for this.</p></disp-quote><p>To address these comments we performed additional simulations and analyses at different levels. Please see our reply to comment (1) of the public review (reviewer 1) for a detailed description. We thank the reviewer for these constructive comments.</p><disp-quote content-type="editor-comment"><p>(2) The second major issue with the study is a lack of systematic exploration and analysis of the parameter space. Some parameters are biologically constrained, but not all the parameters. For instance, it is not clear what the justification for the choice of synaptic time scales are (with E synaptic time constants being larger than inhibition: tau_syn_i = 10 ms, tau_syn_E = 30 ms). How would the results change if they are varying these - and other unconstrained - parameters? It is important to show how the main results, especially the manifold localisation, would change by doing a systematic exploration of the key parameters and performing some sensitivity analysis. This would also help to see how robust the results are, which parameters are more important and which parameters are less relevant, and to shed light on the key mechanisms.</p></disp-quote><p>We thank the reviewer for this comment. We have now carried out additional simulations with equal time constants for all neurons. Please see our reply to the public review for more details (comment 2 of reviewer 1).</p><disp-quote content-type="editor-comment"><p>(3) It is not clear what the main functional advantage of the specific E-I network model is compared to random networks. In terms of activity, they show that specific E-I networks amplify the input more than random networks (Fig. 3). But when it comes to classification, the effect seems to be very small (Fig. 5c). Description of different geometry of representation and manifold localization in specific networks compared to random networks is good, but it is more of an illustration of different activity patterns than proving a functional benefit for the network. The reader is still left with the question of what major functional benefits (in terms of computational/biological processing) should be expected from these networks, if they are to be a good model for olfactory processing and learning.</p></disp-quote><p>One possibility for instance might be that the tasks used here are too easy to reveal the main benefits of the specific models - and more complex tasks would be needed to assess the functional enhancement (e.g. more noisy conditions or more combination of odours). It would be good to show this more clearly - or at least discuss it in relation to computation and function.</p><p>Please see our reply to the public review (comment 3 of reviewer 1).</p><disp-quote content-type="editor-comment"><p>Specific comments:</p><p>Abstract: &quot;resulting in continuous representations that reflected both relatedness of inputs and *an individual's experience*&quot;</p><p>It didn't become apparent from the text or the model where the role of &quot;individual's experience&quot; component (or &quot;internal representations&quot; - in the next line) was introduced or shown (apart from a couple of lines in the Discussion)</p></disp-quote><p>We consider the scenario that that assemblies are the outcome of an experience-dependent plasticity process. To clarify this, we have now made a small addition to the text: 'Biological memory networks are thought to store information by experience-dependent changes in the synaptic connectivity between assemblies of neurons.'.</p><disp-quote content-type="editor-comment"><p>P. 2: &quot;The resulting state of &quot;precise&quot; synaptic balance stabilizes firing rates because inhomogeneities or fluctuations in excitation are tracked by correlated inhibition&quot;</p><p>It is not clear what the &quot;inhomogeneities&quot; specifically refers to - they can be temporal, or they can refer to the quenched noise of connectivity, for instance. Please clarify what you mean.</p></disp-quote><p>The statement has been modified to be more precise: '…'precise' synaptic balance stabilizes firing rates because inhomogeneities in excitation across the population or temporal variations in excitation are tracked by correlated inhibition…'.</p><disp-quote content-type="editor-comment"><p>P. 3 (and Methods): When odour stimulus is simulated in the OB, the activity of a fraction of mitral cells is increased (10% to 15 Hz) - but also a fraction of mitral cells is suppressed (5% to 2 Hz). What is the biological motivation or reference for this? It is not provided. Is it needed for the results? Also, it is not explained how the suppressed 5% are chosen (e.g. randomly, without any relation to the increased cells?).</p></disp-quote><p>We thank the reviewer for this comment. These changes in activity directly reflect experimental observations. We apologize that we forgot to include the references reporting these observations (Friedrich and Laurent, 2001 and 2004); this is now fixed.</p><p>In our simulation, OB neurons do not interact with each other, and the suppressed 5% were indeed randomly selected. We changed the text in Methods accordingly to read: 'An additional 75 randomly selected mitral cells were inhibited'</p><disp-quote content-type="editor-comment"><p>P. 4, L. 1-2: &quot;... sparsely connected integrate-and-fire neurons with conductance-based synapses (connection probability {less than or equal to}5%).&quot;</p><p>Specify the connection probability of specific subtypes (EE, EI, IE, II).</p></disp-quote><p>We now refer to the Methods section, where this information can be found.</p><p>'... conductance-based synapses (connection probability ≤5%, Methods)'</p><disp-quote content-type="editor-comment"><p>P. 4, L. 6-7: &quot;Population activity was odor-specific and activity patterns evoked by uncorrelated OB inputs remained uncorrelated in Dp (Figure 1H)&quot;</p><p>What would happen to correlated OB inputs (e.g. as a result of mixture of two overlapping odours) in this baseline state of the network (before memories being introduced to it)? It would be good to know this, as it sheds light on the initial operating regime of the network in terms of E/I balance and decorrelation of inputs.</p></disp-quote><p>This information was present in the original manuscript at (Figure 3) but we improved the writing to further clarify this issue: ' (…) we morphed a novel odor into a learned odor (Figure 3A), or a learned odor into another learned odor (Supplementary Figure 3B), and quantified the similarity between morphed and learned odors by the Pearson correlation of the OB activity patterns (input correlation). We then compared input correlations to the corresponding pattern correlations among E neurons in Dp (output correlation). In <italic>rand</italic> networks, output correlations increased linearly with input correlations but did not exceed them (Figure 3B and Supplementary Figure 3B)'</p><disp-quote content-type="editor-comment"><p>P. 4, L. 12-13: &quot;Shuffling spike times of inhibitory neurons resulted in runaway activity with a probability of ~80%, ..&quot; Where is this shown?</p><p>(There are other occasions too in the paper where references to the supporting figures are missing).</p></disp-quote><p>We now provide the statistics: 'Shuffling spike times of inhibitory neurons resulted in runaway activity with a probability of 0.79 ± 0.20'</p><disp-quote content-type="editor-comment"><p>P. 4: &quot;In each network, we created 15 assemblies representing uncorrelated odors. As a consequence, ~30% of E neurons were part of an assembly ...&quot;</p><p>15 x 100 / 4000 = 37.5% - so it's closer to 40% than 30%. Unless there is some overlap?</p></disp-quote><p>Yes: despite odors being uncorrelated and connectivity being random, some neurons (6 % of E neurons) belong to more than one assembly.</p><disp-quote content-type="editor-comment"><p>P. 4: &quot;When a reached a critical value of ~6, networks became unstable and generated runaway activity (Figure 2B).&quot;</p><p>Can this transition point be calculated or estimated from the network parameters, and linked to the underlying mechanisms causing it?</p></disp-quote><p>We thank the reviewer for this interesting question. The unstability arises when inhibitions fails to counterbalance efficiently the increased recurrent excitation within Dp. The transition point is difficult to estimate, as it can depend on several parameters, including the probability of E to E connections, their strength, assembly size, and others. We have therefore not attempted to estimate it analytically.</p><disp-quote content-type="editor-comment"><p>P. 4: &quot;Hence, non-specific scaling of inhibition resulted in a divergence of firing rates that exhausted the dynamic range of individual neurons in the population, implying that homeostatic global inhibition is insufficient to maintain a stable firing rate distribution.&quot;</p><p>I don't think this is justified based on the results and figures presented here (Fig. 2E) - the interpretation is a bit strong and biased towards the conclusions the authors want to draw.</p></disp-quote><p>To more clearly illustrate the finding that in <italic>Scaled</italic> networks, assembly neurons are highly active (close to maximal realistic firing rates) whereas non-assembly neurons are nearly silent we have now added Supplementary Fig. 2B. Moreover, we have toned down the text: 'Hence, non-specific scaling of inhibition resulted in a large and biologically unrealistic divergence of firing rates (Supplementary Figure 2B) that nearly exhausted the dynamic range of individual neurons in the population, indicating that homeostatic global inhibition is insufficient to maintain a stable firing rate distribution'</p><disp-quote content-type="editor-comment"><p>P. 5, third paragraph: Description of Figure 2I, inset is needed, either in the text or caption.</p></disp-quote><p>The inset is now referred to in the text: 'we projected synaptic conductances of each neuron onto a line representing the E/I ratio expected in a balanced network ('balanced axis') and onto an orthogonal line ('counter-balanced axis'; Figure 2I inset, Methods).'</p><disp-quote content-type="editor-comment"><p>P. 5, last paragraph: another example of writing about results without showing/referring to the corresponding figures:</p><p>&quot;In rand networks, firing rates increased after stimulus onset and rapidly returned to a low baseline after stimulus offset. Correlations between activity patterns evoked by the same odor at different time points and in different trials were positive but substantially lower than unity, indicating high variability ...&quot;</p><p>And the continuation with similar lack of references on P. 6:</p><p>&quot;Scaled networks responded to learned odors with persistent firing of assembly neurons and high pattern correlations across trials and time, implying attractor dynamics (Hopfield, 1982; Khona and Fiete, 2022), whereas Tuned networks exhibited transient responses and modest pattern correlations similar to rand networks.&quot;</p><p>Please go through the Results and fix the references to the corresponding figures on all instances.</p></disp-quote><p>We thank the reviewer for pointing out these overlooked figure references, which are now fixed.</p><disp-quote content-type="editor-comment"><p>P. 8: &quot;These observations further support the conclusion that E/I assemblies locally constrain neuronal dynamics onto manifolds.&quot;</p><p>As discussed in the general major points, mechanistic explanation in terms of how the interaction of E/I dynamics leads to this is missing.</p></disp-quote><p>As discussed in the reply to the public review (comment 3 of reviewer 1), we have now provided more mechanistic analyses of our observations.</p><disp-quote content-type="editor-comment"><p>P. 9: &quot;Hence, E/I assemblies enhanced the classification of inputs related to learned patterns.&quot; The effect seems to be very small. Also, any explanation for why for low test-target correlation the effect is negative (random doing better than tuned E/I)?</p></disp-quote><p>The size of the effect (plearned – pnovel = 0.074; difference of means; Figure 5C) may appear small in terms of absolute probability, but it is substantial relative to the maximum possible increase (1 – p<sub>novel</sub> = 0.133; Figure 5C). The fact that for low test-target correlations the effect is negative is a direct consequence of the positive effect for high test-target correlations and the presence of 2 learned odors in the 4-way forced choice task.</p><disp-quote content-type="editor-comment"><p>P. 9: &quot;In Scaled I networks, creating two additional memories resulted in a substantial increase in firing rates, particularly in response to the learned and related odors&quot; Where is this shown? Please refer to the figure.</p></disp-quote><p>We thank the reviewer again for pointing this out. We forgot to include a reference to the relevant figure which has now been added in the revised manuscript (Figure 6C).</p><disp-quote content-type="editor-comment"><p>P. 10: &quot;The resulting Tuned networks reproduced additional experimental observations that were not used as constraints including irregular firing patterns, lower output than input correlations, and the absence of persistent activity&quot;</p><p>It is difficult to present these as &quot;additional experimental observations&quot;, as all of them are negative, and can exist in random networks too - hence cannot be used as biological evidence in favour of specific E/I networks when compared to random networks.</p></disp-quote><p>We agree with the reviewer that these additional experimental observations cannot be used as biological evidence favouring Tuned E+I networks over random networks. We here just wanted to point out that additional observations which we did not take into account to fit the model are not invalidating the existence of E-I assemblies in biological networks. As assemblies tend to result in persistent activity in other types of networks, we feel that this observation is worth pointing out.</p><disp-quote content-type="editor-comment"><p>Methods:</p><p>P. 13: Describe the parameters of Eq. 2 after the equation.</p></disp-quote><p>Done.</p><disp-quote content-type="editor-comment"><p>P. 13: &quot;The time constants of inhibitory and excitatory synapses were 10 ms and 30 ms, respectively.&quot;</p><p>What is the (biological) justification for the choice of these parameters?</p><p>How would varying them affect the main results (e.g. local manifolds)?</p></disp-quote><p>We chose a relatively slow time constant for excitatory synapses because experimental data indicate that excitatory synaptic currents in Dp and piriform cortex contain a prominent NMDA component. We have now also simulated networks with equal time constants for excitatory and inhibitory synapses and equal biophysical parameters for excitatory and inhibitory neurons, which did not affect the main results (see also reply to the public review: comment 2 of reviewer 1).</p><disp-quote content-type="editor-comment"><p>P. 14: &quot;Care was also taken to ensure that the variation in the number of output connections was low across neurons&quot; How exactly?</p></disp-quote><p>More detailed explanations have now been added in the Methods section: 'connections of a presynaptic neuron <italic>y</italic> to postsynaptic neurons <italic>x</italic> were randomly deleted when their total number exceeded the average number of output connections by ≥5%, or added when they were lower by ≥5%.'</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>Congratulations on the great and interesting work! The results were nicely presented and the idea of continuous encoding on manifolds is very interesting. To improve the quality of the paper, in addition to the major points raised in the public review, here are some more detailed comments for the paper:</p><p>(1) Generally, citations have to improve. Spiking networks with excitatory assemblies and different architectures of inhibitory populations have been studied before, and the claim about improved network stability in co-tuned E-I networks has been made in the following papers that need to be correctly cited:</p><p>• Vogels TP, Sprekeler H, Zenke F, Clopath C, Gerstner W. 2011. Inhibitory Plasticity Balances Excitation and Inhibition in Sensory Pathways and Memory Networks. Science 334:1-7. doi:10.1126/science.1212991 (mentions that emerging precise balance on the synaptic weights can result in the overall network stability)</p><p>• Lagzi F, Bustos MC, Oswald AM, Doiron B. 2021. Assembly formation is stabilized by Parvalbumin neurons and accelerated by Somatostatin neurons. bioRxiv doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2021.09.06.459211">https://doi.org/10.1101/2021.09.06.459211</ext-link> (among other things, contrasts stability and competition which arises from multistable networks with global inhibition and reciprocal inhibition) • Rost T, Deger M, Nawrot MP. 2018. Winnerless competition in clustered balanced networks: inhibitory assemblies do the trick. Biol Cybern 112:81-98. doi:10.1007/s00422-017-0737-7 (compares different architectures of inhibition and their effects on network dynamics)</p><p>• Lagzi F, Fairhall A. 2022. Tuned inhibitory firing rate and connection weights as emergent network properties. bioRxiv 2022.04.12.488114. doi:10.1101/2022.04.12.488114 (here, see the eigenvalue and UMAP analysis for a network with global inhibition and E/I assemblies)</p><p>Additionally, there are lots of pioneering work about tracking of excitatory synaptic inputs by inhibitory populations, that are missing in references. Also, experimental work that show existence of cell assemblies in the brain are largely missing. On the other hand, some references that do not fit the focus of the statements have been incorrectly cited.</p><p>The authors may consider referencing the following more pertinent studies on spiking networks to support the statement regarding attractor dynamics in the first paragraph in the Introduction (the current citations of Hopfield and Kohonen are for rate-based networks):</p><p>• Wong, K.-F., &amp; Wang, X.-J. (2006). A recurrent network mechanism of time integration in perceptual decisions. Journal of Neuroscience, 26(4), 1314-1328. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1523/JNEUROSCI.3733-05.2006">https://doi.org/10.1523/JNEUROSCI.3733-05.2006</ext-link></p><p>• Wang, X.-J. (2008). Decision making in recurrent neuronal circuits. Neuron, 60(2), 215-234. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2008.09.034">https://doi.org/10.1016/j.neuron.2008.09.034</ext-link></p><p>• F. Lagzi, &amp; S. Rotter. (2015). Dynamics of competition between subnetworks of spiking neuronal networks in the balanced state. PloS One.</p><p>• Goldman-Rakic, P. S. (1995). Cellular basis of working memory. Neuron, 14(3), 477-485.</p><p>• Rost T, Deger M, Nawrot MP. 2018. Winnerless competition in clustered balanced networks: inhibitory assemblies do the trick. Biol Cybern 112:81-98. doi:10.1007/s00422-017-0737-7.</p><p>• Amit DJ, Tsodyks M (1991) Quantitative study of attractor neural network retrieving at low spike rates: I. substrate-spikes, rates and neuronal gain. Network 2:259-273.</p><p>• Mazzucato, L., Fontanini, A., &amp; La Camera, G. (2015). Dynamics of Multistable States during Ongoing and Evoked Cortical Activity. Journal of Neuroscience, 35(21), 8214-8231.</p></disp-quote><p>We thank the reviewer for the references suggestions. We have carefully reviewed the reference list and made the following changes, which we hope address the reviewer’s concerns:</p><p>(1) We adjusted References about network stability in co-tuned E-I networks.</p><p>(2) We added the Lagzi &amp; Rotter (2015), Amit et al. (1991), Mazzucato et al. (2015) and GoldmanRakic (1995) papers in the Introduction as studies on attractor dynamics in spiking neural networks. We preferred to omit the two X.J Wang papers, as they describe attractors in decision making rather than memory processes.</p><p>(3) We added the Ko et al. 2011 paper as experimental evidence for assemblies in the brain. In our view, there are few experimental studies showing the existence of cell assemblies in the brain, which we distinguish from cell ensembles, group of coactive neurons.</p><p>(4) We also included Hennequin 2018, Brunel 2000, Lagzi et al. 2021 and Eckmann et al. 2024, which we had not cited in the initial manuscript.</p><p>(5) We removed the Wiechert et al. 2010 reference as it does not support the statement about geometry-preserving transformation by random networks.</p><disp-quote content-type="editor-comment"><p>(2) The gist of the paper is about how the architecture of inhibition (reciprocal vs. global in this case) can determine network stability and salient responses (related to multistable attractors and variations) for classification purposes. It would improve the narrative of the paper if this point is raised in the Introduction and Discussion section. Also see a relevant paper that addresses this point here:</p><p>Lagzi F, Bustos MC, Oswald AM, Doiron B. 2021. Assembly formation is stabilized by Parvalbumin neurons and accelerated by Somatostatin neurons. bioRxiv doi: <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2021.09.06.459211">https://doi.org/10.1101/2021.09.06.459211</ext-link></p></disp-quote><p>Classification has long been proposed to be a function of piriform cortex and autoassociative memory networks in general, and we consider it important. However, the computational function of Dp or piriform cortex is still poorly understood, and we do not focus only on odor classification as a possibility. In fact, continuous representational manifolds also support other functions such as the quantification of distance relationships of an input to previously memorized stimuli, or multi-layer network computations (including classification). In the revised manuscript, we have performed additional analyses to explore these notions in more detail, as explained above (response to public reviews, comment 3 of reviewer 1). Furthermore, we have now expanded the discussion of potential computational functions of <italic>Tuned</italic> networks and explicitly discuss classification but also other potential functions.</p><disp-quote content-type="editor-comment"><p>(3) A plot for the values of the inhibitory conductances in Figure 1 would complete the analysis for that section.</p></disp-quote><p>In Figure 1, we decided to only show the conductances that we use to fit our model, namely the afferent and total synaptic conductances. As the values of the inhibitory conductances can be derived from panel E, we refrained from plotting them separately for the sake of simplicity.</p><disp-quote content-type="editor-comment"><p>(4) How did the authors calculate correlations between activity patterns as a function of time in Figure 2E, bottom row? Does the color represent correlation coefficient (which should not be time dependent) or is it a correlation function? This should be explained in the Methods section.</p></disp-quote><p>The color represents the Pearson correlation coefficient between activity patterns within a narrow time window (100 ms). We updated the Figure legend to clarify this: 'Mean correlation between activity patterns evoked by a learned odor at different time points during odor presentation. Correlation coefficients were calculated between pairs of activity vectors composed of the mean firing rates of E neurons in 100 ms time bins. Activity vectors were taken from the same or different trials, except for the diagonal, where only patterns from different trials were considered.'</p><disp-quote content-type="editor-comment"><p>(5) Figure 3 needs more clarification (both in the main text and the figure caption). It is not clear what the axes are exactly, and why the network responses for familiar and novel inputs are different. The gray shaded area in panel B needs more explanation as well.</p></disp-quote><p>We thank the reviewer for the comment. We have improved Figure 3A, the figure caption, as well as the text (see p.6). We hope that the figure is now clearer.</p><disp-quote content-type="editor-comment"><p>(6) The &quot;scaled I&quot; network, known for representing input patterns in discrete attractors, should exhibit clear separation between network responses in the 2D PC space in the PCA plots. However, Figure 4D and Figure 6D do not reflect this, as all network responses are overlapped. Can the authors explain the overlap in Figure 4D?</p></disp-quote><p>In Figure 4D, activity of Scaled networks is distributed between three subregions in state space that are separated by the first 2 PCs. Two of them indeed correspond to attractor states representing the two learned odors while the third represents inputs that are not associated with these attractor states. To clarify this, please see also the density plot in Figure 4E. The few datapoints between these three subregions are likely outliers generated by the sequential change in inputs, as described in Supplementary Figure 8C.</p><disp-quote content-type="editor-comment"><p>(7) The reason for writing about the ISN networks is not clear. Co-tuned E-I assemblies do not necessarily have to operate in this regime. Also, the results of the paper do not rely on any of the properties of ISNs, but they are more general. Authors should either show the paradoxical effect associated with ISN (i.e., if increasing input to I neurons decreases their responses) or show ISN properties using stability analysis (See computational research conducted at the Allen Institute, namely Millman et al. 2020, eLife). Currently, the paper reads as if being in the ISN regime is a necessary requirement, which is not true. Also, the arguments do not connect with the rest of the paper and never show up again. Since we know it is not a requirement, there is no need to have those few sentences in the Results section. Also, the choice of alpha=5.0 is extreme, and therefore, it would help to judge the biological realism if the raster plots for Figs 2-6 are shown.</p></disp-quote><p>We have toned down the part on ISN and reduced it to one sentence for readers who might be interested in knowing whether activity is inhibition-stabilized or not. We have also added the reference to the Tsodyks et al. 1997 paper from which we derive our stability analysis. The text now reads 'Hence, pDp<sub>sim</sub> entered a balanced state during odor stimulation (Figure 1D, E) with recurrent input dominating over afferent input, as observed in pDp (Rupprecht and Friedrich, 2018). Shuffling spike times of inhibitory neurons resulted in runaway activity with a probability of 0.79 ± 0.20, demonstrating that activity was inhibition-stabilized (Sadeh and Clopath, 2020b, Tsodyks et al., 1997).'</p><p>We have now also added the raster plots as suggested by the reviewer (see Figure 2D, Supplementary Figure 1 G, Supplementary Figure 4). We thank the reviewer for this comment.</p><disp-quote content-type="editor-comment"><p>(8) In the abstract, authors mention &quot;fast pattern classification&quot; and &quot;continual learning,&quot; but in the paper, those issues have not been addressed. The study does not include any synaptic plasticity.</p></disp-quote><p>Concerning 'continual learning' we agree that we do not simulate the learning process itself. However, Figure 6 show results of a simulation where two additional patterns were stored in a network that already contained assemblies representing other odors. We consider this a crude way of exploring the end result of a 'continual learning' process. 'Fast pattern classification' is mentioned because activity in balanced networks can follow fluctuating inputs with high temporal resolution, while networks with stable attractor states tend to be slow. This is likely to account for the occurrence of hysteresis effects in <italic>Scaled</italic> but not <italic>Tuned</italic> networks as shown in Supplementary</p><p>Fig. 8.</p><disp-quote content-type="editor-comment"><p>(9) In the Introduction, the first sentence in the second paragraph reads: &quot;... when neurons receive strong excitatory and inhibitory synaptic input ...&quot;. The word strong should be changed to &quot;weak&quot;.</p><p>Also, see the pioneering work of Brunel 2000.</p></disp-quote><p>In classical balanced networks, strong excitatory inputs are counterbalanced by strong inhibitory inputs, leading to a fluctuation-driven regime. We have added Brunel 2000.</p><disp-quote content-type="editor-comment"><p>(10) In the second paragraph of the introduction, the authors refer to studies about structural co-tuning (e.g., where &quot;precise&quot; synaptic balance is mentioned, and Vogels et al. 2011 should be cited there) and functional co-tuning (which is, in fact, different than tracking of excitation by inhibition, but the authors refer to that as co-tuning). It makes it easier to understand which studies talk about structural co-tuning and which ones are about functional co-tuning. The paper by Znamenski 2018, which showed both structural and functional tuning in experiments, is missing here.</p></disp-quote><p>We added the citation to the now published paper by Znamenskyi et al. (2024).</p><disp-quote content-type="editor-comment"><p>(11) The third paragraph in the Introduction misses some references that address network dynamics that are shaped by the inhibitory architecture in E/I assemblies in spiking networks, like Rost et al 2018 and Lagzi et al 2021.</p></disp-quote><p>These references have been added.</p><disp-quote content-type="editor-comment"><p>(12) The last sentence of the fourth paragraph in the Introduction implies that functional co-tuning is due to structural co-tuning, which is not necessarily true. While structural co-tuning results in functional co-tuning, functional co-tuning does not require structural co-tuning because it could arise from shared correlated input or heterogeneity in synaptic connections from E to I cells.</p></disp-quote><p>We generally agree with the reviewer, but we are uncertain which sentence the reviewer refers to.</p><p>We assume the reviewer refers to the last sentence of the second (rather than the fourth paragraph), which explicitly mentions the '…structural basis of E/I co-tuning…'. If so, we consider this sentence still correct because the 'structural basis' refers not specifically to E/I assemblies, but also includes any other connectivity that may produce co-tuning, including the connectivity underlying the alternative possibilities mentioned by the reviewer (shared correlated input or heterogeneity of synaptic connections).</p><disp-quote content-type="editor-comment"><p>(13) In order to ensure that the comparison between network dynamics is legit, authors should mention up front that for all networks, the average firing rates for the excitatory cells were kept at 1 Hz, and the background input was identical for all E and I cells across different networks.</p></disp-quote><p>We slightly revised the text to make this more clear 'We (…) uniformly scaled I-to-E connection weights by a factor of χ until E population firing rates in response to learned odors matched the corresponding firing rates in <italic>rand</italic> networks, i.e., 1 Hz'</p><disp-quote content-type="editor-comment"><p>(14) In the last paragraph on page 5, my understanding was that an individual odor could target different cells within an assembly in different trials to generate trial to trail variability. If this is correct, this needs to be mentioned clearly.</p></disp-quote><p>This is not correct, an odor consists of 150 activated mitral cells with defined firing rates. As now mentioned in the Methods, 'Spikes were then generated from a Poisson distribution, and this process was repeated to create trial-to-trial variability.'</p><disp-quote content-type="editor-comment"><p>(15) The last paragraph on page 6 mentions that the four OB activity patterns were uncorrelated but if they were designed as in Figure 4A, dues to the existing overlap between the patterns, they cannot be uncorrelated.</p></disp-quote><p>This appears to be a misunderstanding. We mention in the text (and show in Figure 4B) that the four odors which '… were assigned to the corners of a square…' are uncorrelated. The intermediate odors are of course not uncorrelated. We slightly modified the corresponding paragraph (now on page 7) to clarify this: 'The subspace consisted of a set of OB activity patterns representing four uncorrelated pure odors and mixtures of these pure odors. Pure odors were assigned to the corners of a square and mixtures were generated by selecting active mitral cells from each of the pure odors with probabilities depending on the relative distances from the corners (Figure 4A, Methods).'</p><disp-quote content-type="editor-comment"><p>(16) The notion of &quot;learned&quot; and &quot;novel&quot; odors may be misleading as there was no plasticity in the network to acquire an input representation. It would be beneficial for the authors to clarify that by &quot;learned,&quot; they imply the presence of the corresponding E assembly for the odor in the network, with the input solely impacting that assembly. Conversely, for &quot;novel&quot; inputs, the input does not target a predefined assembly. In Figure 2 and Figure 4, it would be especially helpful to have the spiking raster plots of some sample E and I cells.</p></disp-quote><p>As suggested by the reviewer, we have modified the existing spiking raster plots in Figure 2, such that they include examples of responses to both learned and novel odors. We added spiking raster plots showing responses of I neurons to the same odors in Supplementary Figure 1F, as well as spiking raster plots of E neurons in Supplementary Figure 4A. To clarify the usage of 'learned' and 'novel', we have added a sentence in the Results section: 'We thus refer to an odor as 'learned' when a network contains a corresponding assembly, and as 'novel' when no such assembly is present.'.</p><disp-quote content-type="editor-comment"><p>(17) In the last paragraph of page 8, can the authors explain where the asymmetry comes from?</p></disp-quote><p>As mentioned in the text, the asymmetry comes from the difference in the covariance structure of different classes. To clarify, we have rephrased the sentence defining the Mahalanobis distance:</p><p>'This measure quantifies the distance between the pattern and the class center, taking into account covariation of neuronal activity within the class. In bidirectional comparisons between patterns from different classes, the mean dM may be asymmetric if neural covariance differs between classes.'</p><disp-quote content-type="editor-comment"><p>(18) The first paragraph of page 9: random networks are not expected to perform pattern classification, but just pattern representation. It would have been better if the authors compared Scaled I network with E/I co-tuned network. Regardless of the expected poorer performance of the E/I co-tuned networks, the result would have been interesting.</p></disp-quote><p>Please see our reply to the public review (reviewer 2).</p><disp-quote content-type="editor-comment"><p>(19) Second paragraph on page 9, the authors should provide statistical significance test analysis for the statement &quot;... was significantly higher ...&quot;.</p></disp-quote><p>We have performed a Wilcoxon signed-rank test, and reported the p-value in the revised manuscript (p &lt; 0.01).</p><disp-quote content-type="editor-comment"><p>(20) The last sentence in the first paragraph on page 11 is not clear. What do the authors mean by &quot;linearize input-output functions&quot;, and how does it support their claim?</p></disp-quote><p>We have now amended this sentence to clarify what we mean: '…linearize the relationship between the mean input and output firing rates of neuronal populations…'.</p><disp-quote content-type="editor-comment"><p>(21) In the first sentence of the last paragraph on page 11, the authors mentioned 'high variability', but it is not clear compared with which of the other 3 networks they observed high variability.</p><p>Structurally co-tuned E/I networks are expected to diminish network-level variability.</p></disp-quote><p>'High variability' refers to the variability of spike trains, which is now mentioned explicity in the text. We hope this more precise statement clarifies this point.</p><disp-quote content-type="editor-comment"><p>(22) Methods section, page 14: &quot;firing rates decreased with a time constant of 1, 2 or 4 s&quot;. How did they decrease? Was it an implementation algorithm? The time scale of input presentation is 2 s and it overlaps with the decay time constant (particularly with the one with 4 s decrease).</p></disp-quote><p>Firing rates decreased exponentially. We have added this information in the Methods section.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #3 (Recommendations For The Authors):</bold></p><p>In the following, I suggest minor corrections to each section which I believe can improve the manuscript.</p><p>- There was no github link to the code in the manuscript. The code should be made available with a link to github in the final manuscript.</p></disp-quote><p>The code can be found here: https://github.com/clairemb90/pDp-model. The link has been added in the Methods section.</p><disp-quote content-type="editor-comment"><p>Figure 1:</p><p>- Fig. 1A: call it pDp not Dp. Please check if this name is consistent in every figure and the text.</p></disp-quote><p>Thank you for catching this. Now corrected in Figure 1, Figure 2 and in the text.</p><disp-quote content-type="editor-comment"><p>- The authors write: &quot;Hence, pDpsim entered an inhibition-stabilized balanced state (Sadeh and Clopath, 2020b) during odor stimulation (Figure 1D, E).&quot; and then later &quot;Shuffling spike times of inhibitory neurons resulted in runaway activity with a probability of ~80%, demonstrating that activity was indeed inhibition-stabilized. These results were robust against parameter variations (Methods).&quot; I would suggest moving the second sentence before the first sentence, because the fact that the network is in the ISN regime follows from the shuffled spike timing result.</p><p>Also, I'd suggest showing this as a supplementary figure.</p></disp-quote><p>We thank the reviewer for this comment. We have removed 'inhibition-stabilized' in the first sentence as there is no strong evidence of this in Rupprecht and Friedrich, 2018. And removed 'indeed' in the second sentence. We also provided more detailed statistics. The text now reads 'Hence, pDpsim entered a balanced state during odor stimulation (Figure 1D, E) with recurrent input dominating over afferent input, as observed in pDp (Rupprecht and Friedrich, 2018). Shuffling spike times of inhibitory neurons resulted in runaway activity with a probability of 0.79 ± 0.20, demonstrating that activity was inhibition-stabilized (Sadeh and Clopath, 2020b).'</p><disp-quote content-type="editor-comment"><p>Figure 2:</p><p>- &quot;... Scaled I networks (Figure 2H.&quot; Missing)</p></disp-quote><p>Corrected.</p><disp-quote content-type="editor-comment"><p>- The authors write &quot;Unlike in Scaled I networks, mean firing rates evoked by novel odors were indistinguishable from those evoked by learned odors and from mean firing rates in rand networks (Figure 2F).&quot;</p><p>Why is this something you want to see? Isn't it that novel stimuli usually lead to high responses? Eg in the paper Schulz et al., 2021 (eLife) which is also cited by the authors it is shown that novel responses have high onset firing rates. I suggest clarifying this (same in the context of Fig. 3C).</p></disp-quote><p>In Dp and piriform cortex, firing rates evoked by learned odors are not substantially different from firing rates evoked by novel odors. While small differences between responses to learned versus novel odors cannot be excluded, substantial learning-related differences in firing rates, as observed in other brain areas, have not been described in Dp or piriform cortex. We added references in the last paragraph of p.5. Note that the paper by Schulz et al. (2021) models a different type of circuit.</p><disp-quote content-type="editor-comment"><p>- Fig. 2B: Indicate in figure caption that this is the case &quot;Scaled I&quot;</p></disp-quote><p>This is not exactly the case 'Scaled I', as the parameter 𝝌𝝌 (increased I to E strength) is set to 1.</p><disp-quote content-type="editor-comment"><p>- Suppl Fig. 2I: Is E&amp;F ever used in the manuscript? I couldn't find a reference. I suggest removing it if not needed.</p></disp-quote><p>Suppl. Fig 2I E&amp;F is now Suppl Fig.1G&amp;H. We now refer to it in the text: 'Activity of networks with E assemblies could not be stabilized around 1 Hz by increasing connectivity from subsets of I neurons receiving dense feed-forward input from activated mitral cells (Supplementary Figure 1GH; Sadeh and Clopath, 2020).'</p><disp-quote content-type="editor-comment"><p>Figure 3:</p><p>- As mentioned in my comment in the public review section, I find the arguments about pattern completion a little bit confusing. For me it's not clear why an increase of output correlations over input correlations is considered &quot;pattern completion&quot; (this is not to say that I don't find the nonlinear increase of output correlations interesting). For me, to test pattern completion with second-order statistics one would need to do a similar separation as in Suppl Fig. 3, ie measuring the pairwise correlation at cells in the assembly L that get direct input from L OB with cells in the assembly L that do not get direct input from OB. If the pairwise correlations of assembly cells which do not get direct input from OB increase in correlations, I would consider this as pattern completion (similar to the argument that increase in firing rate in cells which are not directly driven by OB are considered a sign of pattern completion).</p><p>Also, for me it now seems like that there are contradictory results, in Fig. 3 only Scaled I can lead to pattern completion while in the context of Suppl. Fig. 3 the authors write &quot;We found that assemblies were recruited by partial inputs in all structured pDpsim networks (Scaled and Tuned) without a significant increase in the overall population activity (Supplementary Figure 3A).&quot; I suggest clarifying what the authors exactly mean by pattern completion, why the increase of output correlations above input correlations can be considered as pattern completion, and why the results differs when looking at firing rates versus correlations.</p></disp-quote><p>Please see our reply to the public review (reviewer 3).</p><disp-quote content-type="editor-comment"><p>- I actually would suggest adding Suppl. Fig. 3 to the main figure. It shows a more intuitive form of pattern completion and in the text there is a lot of back and forth between Fig. 3 and Suppl. Fig. 3</p></disp-quote><p>We feel that the additional explanations and panels in Fig.3 should clarify this issue and therefore prefer to keep Supplementary Figure 3 as part of the Supplementary Figures for simplicity.</p><disp-quote content-type="editor-comment"><p>- In the whole section &quot;We next explored effects of assemblies ... prevented strong recurrent amplification within E/I assemblies.&quot; the authors could provide a link to the respective panel in Fig. 2 after each statement. This would help the reader follow your arguments.</p></disp-quote><p>We thank the reviewer for pointing this out. The references to the appropriate panels have been added.</p><disp-quote content-type="editor-comment"><p>- Fig. 3A: I guess the x-axis has been shifted upwards? Should be at zero.</p></disp-quote><p>We have modified the x-axis to make it consistent with panels B and C.</p><disp-quote content-type="editor-comment"><p>- Fig. 3B: In the figure caption, the dotted line is described as the novel odor but it is actually the unit line. The dashed lines represent the reference to the novel odor.</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>- Fig. 3C: The &quot; is missing for Pseudo-Assembly N</p></disp-quote><p>Fixed.</p><disp-quote content-type="editor-comment"><p>- &quot;...or a learned odor into another learned odor.&quot; Have here a ref to the Supplementary Figure 3B.</p></disp-quote><p>Added.</p><disp-quote content-type="editor-comment"><p>Figure 4:</p><p>- &quot;This geometry was largely maintained in the output of rand networks, consistent with the notion that random networks tend to preserve similarity relationships between input patterns (Babadi and Sompolinsky, 2014; Marr, 1969; Schaffer et al., 2018; Wiechert et al., 2010).&quot; I suggest adding here reference to Fig. 4D (left).</p></disp-quote><p>Added.</p><disp-quote content-type="editor-comment"><p>- Please add a definition of E/I assemblies. How do the authors define E/I assemblies? I think they consider both, Tuned I and Tuned E+I as E/I assemblies? In Suppl. Fig. 2I E it looks like tuned feedforward input is defined as E/I assemblies.</p></disp-quote><p>We thank the reviewer for pointing this out. E/I assemblies are groups of E and I neurons with enhanced connectivity. In other words, in E/I assemblies, connectivity is enhanced not only between subsets of E neurons, but also between these E neurons and a subset of I neurons. This is now clarified in the text: 'We first selected the 25 I neurons that received the largest number of connections from the 100 E neurons of an assembly. To generate E/I assemblies, the connectivity between these two sets of neurons was then enhanced by two procedures.'. We removed 'E/I assemblies' in Suppl. Fig.2, where the term was not used correctly, and apologize for the confusion.</p><disp-quote content-type="editor-comment"><p>- Suppl. Fig. 4: Could the authors please define what they mean by &quot;Loadings&quot;</p></disp-quote><p>The loadings indicate the contribution of each neuron to each principal component, see adjusted legend of Suppl. Fig. 4: 'G. Loading plot: contribution of neurons to the first two PCs of a <italic>rand</italic> and a <italic>Tuned E+I</italic> network (Figure 4D).'</p><disp-quote content-type="editor-comment"><p>- Fig. 4F: The authors might want to normalize the participation ratio by the number of neurons (see e.g. Dahmen et al., 2023 bioRxiv, &quot;relative PR&quot;), so the PR is bound between 0 and 1 and the dependence on N is removed.</p></disp-quote><p>We thank the reviewer for the suggestion, but we prefer to use the non-normalized PR as we find it more easily interpretable (e.g. number of attractor states in Scaled networks).</p><disp-quote content-type="editor-comment"><p>- Fig. 4G&amp;H: as mentioned in the public review, I'd add the case of Scaled I to be able to compare it to the Tuned E+I case.</p></disp-quote><p>As already mentioned in the public review, we thank the reviewer for this suggestion, which we have implemented.</p><disp-quote content-type="editor-comment"><p>- Figure caption Fig. 4H &quot;Similar results were obtained in the full-dimensional space.&quot; I suggest showing this as a supplemental panel.</p></disp-quote><p>Since this only adds little information, we have chosen not to include it as a supplemental panel to avoid overloading the paper with figures.</p><disp-quote content-type="editor-comment"><p>Figure 5:</p><p>- As mentioned in the public review, I suggest that the authors add the Scaled I case to Fig. 5 (it's shown in all figures and also in Fig. 6 again). I guess for Scaled I the separation between L and M will be very good?</p></disp-quote><p>Please see our reply to the public review (reviewer 3).</p><disp-quote content-type="editor-comment"><p>- Fig. 5A&amp;B: I am a bit confused about which neurons are drawn to calculate the Mahalanobis distance. In Fig. 5A, the schematic indicates that the vector B from which the neurons are drawn is distinct from the distribution Q. For the example of odor L, the distribution Q consists of pure odor L with odors that have little mixtures with the other odors. But the vector v for odor L seems to be drawn only from odors that have slightly higher mixtures (as shown in the schematic in Fig. 5A). Is there a reason to choose the vector v from different odors than the distribution Q?</p></disp-quote><p>The distribution Q and the vector v consist of activity patterns across the same neurons in response to different odors. The reason to choose a different odor for v was to avoid having this test datapoint being included in the distribution Q. We also wanted Q to be the same for all test datapoints.</p><disp-quote content-type="editor-comment"><p>What does &quot;drawn from whole population&quot; mean? Does this mean that the vectors are drawn from any neuron in pDp? If yes, then I don't understand how the authors can distinguish between different odors (L,M,O,N) on the y-axis. Or does &quot;whole population&quot; mean that the vector is drawn across all assemblies as shown in the schematic in Fig. 5A and the case &quot;neurons drawn from (pseudo-) assembly&quot; means that the authors choose only one specific assembly? In any case, the description here is a bit confusing, I think it would help the reader to clarify those terms better.</p></disp-quote><p>Yes, 'drawn from whole population' means that we randomly draw 80 neurons from the 4000 E neurons in pDp. The y-axis means that we use the activity patterns of these neurons evoked by one of the 4 odors (L, M, N, O) as reference. We have modified the Figure legend to clarify this: 'd<sub>M</sub> was computed based on the activity patterns of 80 E neurons drawn from the four (pseudo-) assemblies (top) or from the whole population of 4000 E neurons (bottom). Average of 50 draws.'</p><disp-quote content-type="editor-comment"><p>- Suppl Fig. 5A: In the schematic the distance is called <inline-formula><mml:math id="sa4m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>V</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> while the colorbar has <inline-formula><mml:math id="sa4m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>d</mml:mi><mml:mi>E</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mover><mml:mi>Q</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with the Qs in different color. The green Q should be a V.</p></disp-quote><p>We thank the reviewer for spotting this mistake, it is now fixed.</p><disp-quote content-type="editor-comment"><p>- Fig. 5: Could the authors comment on the fact that a random network seems to be very good in classifying patterns on it's own. Maybe in the Discussion?</p></disp-quote><p>The task shown in Figure 5 is a relatively easy one, a forced-choice between four classes which are uncorrelated. In Supplementary Figure 9, we now show classification for correlated classes, which is already much harder.</p><disp-quote content-type="editor-comment"><p>Figure 6:</p><p>- Is the correlation induced by creating mixtures like in the other Figures? Please clarify how the correlations were induced.</p></disp-quote><p>We clarified this point in the Methods section: 'The pixel at each vertex corresponded to one pure odor with 150 activated and 75 inhibited mitral cells (…) and the remaining pixels corresponded to mixtures. In the case of correlated pure odors (Figure 6), adjacent pure odors shared half of their activated and half of their inhibited cells.'. An explicit reference to the Methods section has also been added to the figure legend.</p><disp-quote content-type="editor-comment"><p>- Fig. 6C (right): why don't we see the clear separation in PC space as shown in Fig. 4? Is this related to the existence of correlations? Please clarify.</p></disp-quote><p>Yes. The assemblies corresponding to the correlated odors X and Y overlap significantly, and therefore responses to these odors cannot be well separated, especially for Scaled networks. We added the overlap quantification in the Results section to make this clear. 'These two additional assemblies had on average 16% of neurons in common due to the similarity of the odors.'</p><disp-quote content-type="editor-comment"><p>- &quot;Furthermore, in this regime of higher pattern similarity, dM was again increased upon learning, particularly between learned odors and reference classes representing other odors (not shown).&quot; Please show this (maybe as a supplemental figure).</p></disp-quote><p>We now show the data in Supplementary Figure 9.</p><disp-quote content-type="editor-comment"><p>Discussion:</p><p>- The authors write: &quot;We found that transformations became more discrete map-like when amplification within assemblies was increased and precision of synaptic balance was reduced. Likewise, decreasing amplification in assemblies of Scaled networks changed transformations towards the intermediate behavior, albeit with broader firing rate distributions than in Tuned networks (not shown).&quot;</p><p>Where do I see the first point? I guess when I compare in Fig. 4D the case of Scaled I vs Tuned E+I, but the sentence above sounds like the authors showed this in a more step-wise way eg by changing the strength of \alpha or \beta (as defined in Fig. 1).</p><p>Also I think if the authors want to make the point that decreasing amplification in assemblies changes transformation with a different rate distribution in scaled vs tuned networks, the authors should show it (eg adding a supplemental figure).</p></disp-quote><p>The first point is indeed supported by data from different figures. Please note that the revised manuscript now contains further simulations that reinforce this statement, particularly those shown in Supplementary Figure 6, and that this point is now discussed more extensively in the Discussion. We hope that these revisions clarify this general point.</p><p>The data showing effects of decreasing amplification in assemblies is now shown in Supplementary Figure 6 (Scaled[adjust])</p><disp-quote content-type="editor-comment"><p>- I suggest adding the citation Znamenskiy et al., 2024 (Neuron; <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1016/j.neuron.2023.12.013">https://doi.org/10.1016/j.neuron.2023.12.013</ext-link>), which shows that excitatory and inhibitory (PV) neurons with functional similarities are indeed strongly connected in mouse V1, suggesting the existence of E/I assembly structure also in mammals.</p></disp-quote><p>Done.</p></body></sub-article></article>