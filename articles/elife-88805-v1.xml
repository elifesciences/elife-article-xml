<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">88805</article-id><article-id pub-id-type="doi">10.7554/eLife.88805</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.88805.3</article-id><article-version article-version-type="publication-state">version of record</article-version><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dynamic control of sequential retrieval speed in networks with heterogeneous learning rules</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gillett</surname><given-names>Maxwell</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1937-477X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes"><name><surname>Brunel</surname><given-names>Nicolas</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2272-3248</contrib-id><email>nicolas.brunel@duke.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00py81415</institution-id><institution>Department of Neurobiology, Duke University</institution></institution-wrap><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00py81415</institution-id><institution>Department of Physics, Duke University</institution></institution-wrap><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Bhalla</surname><given-names>Upinder S</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03gf8rp76</institution-id><institution>National Centre for Biological Sciences</institution></institution-wrap><country>India</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Colgin</surname><given-names>Laura L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>University of Texas at Austin</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>28</day><month>08</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP88805</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-05-08"><day>08</day><month>05</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-03-24"><day>24</day><month>03</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.03.22.533836"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-08-07"><day>07</day><month>08</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88805.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-07-11"><day>11</day><month>07</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.88805.2"/></event></pub-history><permissions><copyright-statement>© 2023, Gillett and Brunel</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Gillett and Brunel</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-88805-v1.pdf"/><abstract><p>Temporal rescaling of sequential neural activity has been observed in multiple brain areas during behaviors involving time estimation and motor execution at variable speeds. Temporally asymmetric Hebbian rules have been used in network models to learn and retrieve sequential activity, with characteristics that are qualitatively consistent with experimental observations. However, in these models sequential activity is retrieved at a fixed speed. Here, we investigate the effects of a heterogeneity of plasticity rules on network dynamics. In a model in which neurons differ by the degree of temporal symmetry of their plasticity rule, we find that retrieval speed can be controlled by varying external inputs to the network. Neurons with temporally symmetric plasticity rules act as brakes and tend to slow down the dynamics, while neurons with temporally asymmetric rules act as accelerators of the dynamics. We also find that such networks can naturally generate separate ‘preparatory’ and ‘execution’ activity patterns with appropriate external inputs.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>sequential activity</kwd><kwd>temporal rescaling</kwd><kwd>hebbian learning</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01 EB022891</award-id><principal-award-recipient><name><surname>Gillett</surname><given-names>Maxwell</given-names></name><name><surname>Brunel</surname><given-names>Nicolas</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000006</institution-id><institution>Office of Naval Research</institution></institution-wrap></funding-source><award-id>N00014-16-1-2327</award-id><principal-award-recipient><name><surname>Gillett</surname><given-names>Maxwell</given-names></name><name><surname>Brunel</surname><given-names>Nicolas</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Low-dimensional external inputs are sufficient to control the speed of sequential activity, and to transition between persistent activity, in a network of neurons with both temporally symmetric and asymmetric learning.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Timing is a critical component in the proper planning and execution of temporally extended motor behaviors. In behaviors consisting of a single motor action, it may be desirable to control the duration of its execution. In behaviors composed of multiple actions, the precise time interval between actions can be a key determinant in the success of the behavior. How can the duration of these intervals be flexibly controlled in a network of neurons?</p><p>A simple mechanistic hypothesis for the regulation of motor related timing intervals posits a specialized neural circuit with network dynamics that vary in speed as a consequence of differing levels of constant external input. Several network models utilizing external input as a means of speed control have been proposed to account for cortical and striatal dynamics observed during motor execution (<xref ref-type="bibr" rid="bib27">Murray and Escola, 2017</xref>; <xref ref-type="bibr" rid="bib42">Wang et al., 2018</xref>). To account for speed control in cortex, a recurrent neural network model has been trained to achieve temporal rescaling of network activity as a function of external input (<xref ref-type="bibr" rid="bib42">Wang et al., 2018</xref>). However, this model relies on supervised learning rules that may not be biologically plausible, and cannot generalize to other speeds from training on just one example timing interval. To explain speed control of sequential activity in striatum, a recurrent inhibitory network model has been proposed with a feedforward structure learned through anti-Hebbian plasticity (<xref ref-type="bibr" rid="bib27">Murray and Escola, 2017</xref>). This model demonstrates transient winner-take-all dynamics, with short-term synaptic depression facilitating transitions in activity from one group of neurons to the next, and external input controlling the duration of each group’s transient activation. While experimental evidence for the necessary type of depressive adaptation mechanism exists in the striatum, it may not be present in all cortical areas where rescaling of sequential activity is observed. Whether speed can be controlled in network models constructed using Hebbian learning without this mechanism remains unknown.</p><p>Network models with a connectivity generated by temporally asymmetric synaptic plasticity provide a potential framework for explaining how sequential activity can arise from local biologically plausible learning rules (<xref ref-type="bibr" rid="bib35">Sompolinsky and Kanter, 1986</xref>; <xref ref-type="bibr" rid="bib21">Kleinfeld, 1986</xref>). In both rate and spiking networks, the temporal statistics of sequential activity in networks using this type of rule qualitatively match experimental findings made over both short and long timescales of observation in multiple tasks with timing components (<xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>). However, the speed of sequential dynamics in these models is constrained by the choice of temporal offset in the learning rule and neuronal time constant, and cannot be modulated with external input.</p><p>The Hebbian rules explored in this work and previous studies are approximations of various forms of spike-timing dependent plasticity (STDP). The effects of STDP can be quantified through kernels that measure the change in excitatory postsynaptic potential size at a synapse (as a proxy for synaptic strength), as a function of the timing difference between pre and postsynaptic spikes. Experimentally, a large diversity of STDP kernels have been characterized across cortical, subcortical, and cerebellar structures (<xref ref-type="bibr" rid="bib1">Abbott and Nelson, 2000</xref>; <xref ref-type="bibr" rid="bib36">Suvrathan et al., 2016</xref>). Kernels measured in cortex and hippocampus typically, but not always, exhibit a temporal asymmetry, in which presynaptic activity must precede postsynaptic activity to elicit a positive change in synaptic strength (<xref ref-type="bibr" rid="bib6">Bi and Poo, 1998</xref>; <xref ref-type="bibr" rid="bib10">Egger et al., 1999</xref>). Theoretical studies have shown that this temporal asymmetry can be used to store and retrieve sequences of activity (<xref ref-type="bibr" rid="bib19">Jun and Jin, 2007</xref>; <xref ref-type="bibr" rid="bib24">Liu and Buonomano, 2009</xref>; <xref ref-type="bibr" rid="bib11">Fiete et al., 2010</xref>; <xref ref-type="bibr" rid="bib40">Waddington et al., 2012</xref>; <xref ref-type="bibr" rid="bib46">Zheng and Triesch, 2014</xref>; <xref ref-type="bibr" rid="bib29">Okubo et al., 2015</xref>; <xref ref-type="bibr" rid="bib32">Ravid Tannenbaum and Burak, 2016</xref>; <xref ref-type="bibr" rid="bib27">Murray and Escola, 2017</xref>; <xref ref-type="bibr" rid="bib43">Weissenberger et al., 2017</xref>; <xref ref-type="bibr" rid="bib38">Theodoni et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Pereira and Brunel, 2019</xref>; <xref ref-type="bibr" rid="bib39">Tupikov and Jin, 2020</xref>; <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>). However, symmetric kernels, in which coincident activity leads to strengthening regardless of the order of pre and post-synaptic spikes, have also been observed in multiple contexts - with high frequency plasticity induction protocols in cortex (<xref ref-type="bibr" rid="bib34">Sjöström et al., 2001</xref>), in hippocampal cultures in the presence of dopamine (<xref ref-type="bibr" rid="bib45">Zhang et al., 2009</xref>), and at excitatory-to-excitatory synapses in hippocampal CA3 (<xref ref-type="bibr" rid="bib26">Mishra et al., 2016</xref>). Hebbian learning rules that are temporally symmetric lead instead to the creation of fixed point attractors (<xref ref-type="bibr" rid="bib16">Hopfield, 1982</xref>; <xref ref-type="bibr" rid="bib3">Amit and Brunel, 1997</xref>; <xref ref-type="bibr" rid="bib41">Wang, 2001</xref>; <xref ref-type="bibr" rid="bib7">Brunel, 2005</xref>; <xref ref-type="bibr" rid="bib30">Pereira and Brunel, 2018</xref>). It is not known to what degree temporal asymmetry varies across synapses at the scale of local networks, but analysis of a calcium-based plasticity model demonstrates that the degree of asymmetry can be controlled via adjustment of biophysical parameters (<xref ref-type="bibr" rid="bib15">Graupner and Brunel, 2012</xref>). We hypothesize that variability in the temporal offset expressed at a synapse may be a key ingredient in permitting the control of retrieval speed, suggesting a potential new role for the observed heterogeneity in STDP kernels.</p><p>In this work, we explore a generalization of previously investigated temporally asymmetric learning to multiple temporal offsets that captures this heterogeneity. Specifically, we find that varying the temporal asymmetry of the learning rule across synapses gives rise to network mechanisms that allow for the control of speed as a function of external inputs to the network. We start by considering a network with a bimodal distribution of heterogeneity in the learning rule, resulting in two distinct populations: one with a symmetric learning rule, and one with an asymmetric rule. We characterize the effect of input strength on retrieval speed and quality in these networks with connectivity generated using linear and nonlinear synaptic plasticity rules. We also find that transitions between fixed-point attractor-like ‘preparatory’ periods and sequential ‘execution’ phases can be realized in this model by rescaling the magnitude of external input. Finally, we demonstrate that networks with a uniform distribution of heterogeneity lead to qualitatively similar findings.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Degree of symmetry in learning rule determines retrieval speed</title><p>We explore a network model in which the firing rate dynamics of each neuron <italic>r</italic><sub><italic>i</italic></sub> in a population of size <inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is described by the equation<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the time constant of firing rate dynamics, <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the connectivity matrix, <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a sigmoidal neuronal transfer function (see Methods), and <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> describes the external input provided to each neuron at time <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>We follow a similar learning procedure as in <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>. A sequence of <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> random i.i.d standard Gaussian patterns <inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is presented to the network and stored in network connectivity. This sequence of patterns modifies the strength of synaptic connections <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> from neuron <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> according to a Hebbian learning rule that transforms pre and post synaptic inputs into synaptic weight changes. The resulting connectivity matrix <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a generalization of previously studied rules which combines both temporally symmetric and asymmetric learning (<xref ref-type="bibr" rid="bib30">Pereira and Brunel, 2018</xref>; <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>),<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mfrac><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mi>N</mml:mi><mml:mi>c</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a matrix describing the structural connectivity, whose entries are given by i.i.d. Bernoulli random variables, <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the connection probability; The functions <inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> describe how the synaptic plasticity rule depends on pre and postsynaptic input patterns during learning, respectively; The parameter <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the overall strength of the recurrent connections; And <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> describes the degree of temporal symmetry at synapses of neuron <italic>i</italic>. A neuron with fully temporally symmetric plasticity is described by <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, while <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> indicates a neuron with fully temporally asymmetric plasticity. Note that we focus here to the case of a single sequence stored in synaptic connectivity, but such networks can also store multiple sequences (<xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>).</p><p>We first explore the bilinear learning rule scenario (<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with homogeneous synaptic plasticity, i.e. <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for all <inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. At the two extremes of this variable we can recover previously studied learning rules. When <inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, only the second term in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> is present, resulting in a purely temporally asymmetric rule. Networks with connectivity constructed using such a rule can recall a sequence of stored patterns, and their sequential retrieval dynamics have been extensively characterized (<xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>). When <inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, synaptic plasticity is temporally symmetric, potentially leading to fixed point attractor dynamics (<xref ref-type="bibr" rid="bib30">Pereira and Brunel, 2018</xref>). If <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is instead fixed to a value between 0 and 1, then the asymmetric component in the plasticity rule leads to the retrieval of the whole sequence, but the speed at which the sequence is retrieved strongly depends on <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For instance, in <xref ref-type="fig" rid="fig1">Figure 1b</xref> we demonstrate retrieval for an intermediate value of <inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Retrieval is quantified by plotting the Pearson correlation of the instantaneous firing rate <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with each stored pattern <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of time (see Methods). During sequence retrieval, correlations with individual patterns in the sequence increase, peak and decrease one after the other, indicating the network transiently visit states close to each of the patterns in succession. We find that in such a network, retrieval speed strongly depends on <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the parameters in <xref ref-type="fig" rid="fig1">Figure 1b</xref>, retrieval proceeds nearly twice as slowly as compared to a network with connectivity arising from a purely asymmetric learning rule, where retrieval speed is fixed by the time constant of the firing rate dynamics (<xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>). However, retrieval speed is fixed by the choice of <inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (see <xref ref-type="fig" rid="fig1">Figure 1c</xref> showing a linear dependence of speed on <inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), and cannot be dynamically modulated in response to changes in the external input <inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Network model and sequence retrieval.</title><p>(<bold>a</bold>) Schematic of network connectivity after learning with a plasticity rule that combines temporally symmetric and asymmetric components. The network stores a sequence of patterns that activate non-overlapping sets of neurons (colored according to the pattern that activates them). Note connections both within each set, and from one set to the next. (<bold>b</bold>) Correlation of each stored pattern with network activity following initialization to the first pattern. Retrieval speed is fixed by the balance of symmetry/asymmetry at the synapse. (<bold>c</bold>) Relative retrieval speed as a function of temporal symmetry (<bold>z</bold>), showing linear relationship. Solid line: <inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the speed computed from MFT (see Methods). Black dots: Network simulations. (<bold>d</bold>) Connectivity of a network with two types of neurons, asymmetric (left) and symmetric (right). Note that the connections from left neurons project to neurons active in the next pattern in the sequence, while connections from right neurons project to neurons active in the same pattern as the pre-synaptic neuron. The two types of neurons can be driven differentially by external inputs (<inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively) (<bold>e</bold>) Solid lines: correlations as in (<bold>a</bold>) for two distinct pairs of input strengths (in the range [–1,0] for <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>), demonstrating two different retrieval speeds. Dashed lines: correlations with noisy time-dependent heterogeneous input added to the network (see Methods). In the simulations shown on the center and right panels, <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>80,000</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo>=</mml:mo><mml:mn>0.005</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn><mml:mtext>ms</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo>=</mml:mo><mml:mn>16</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. For simplicity, we depict only 3 of the 16 stored patterns in the left schematics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88805-fig1-v1.tif"/></fig></sec><sec id="s2-2"><title>Heterogeneity in synaptic plasticity temporal asymmetry gives rise to a speed control mechanism</title><p>We next explored whether adding heterogeneity to this learning rule, allowing <italic>z</italic><sub><italic>i</italic></sub> to differ across synapses, can produce networks capable of both recalling stored sequences of patterns and modulating the speed of recall. We initially consider a bimodal distribution of degrees of temporal symmetry across the network. For each neuron, <italic>z</italic><sub><italic>i</italic></sub> was drawn randomly and independently as a Bernoulli random variable with probability <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. As a result, the network of <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons can be divided into two subpopulations of approximately equal sizes <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mi>N</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> neurons, according to the learning rule present at their synapses:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>r</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where the connectivity matrix is given by<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mi>c</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>c</mml:mi></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and where <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> denotes the presynaptic population. Note that the external input <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> now depends on the population. To reduce the space of possible learning rules, we have assumed that the type of learning at a synapse depends only on the identity of the postsynaptic neuron. The bimodal distribution of <italic>z</italic><sub><italic>i</italic></sub> restricts synapses to only one of the two types of plasticity, but in the final section entitled ‘Retrieval with a broad distribution of learning rules’ we relax this constraint.</p><p>In <xref ref-type="fig" rid="fig1">Figure 1e</xref>, we show an example of how the stored sequence can be retrieved under different input conditions. In both the top and bottom panels of 1e, network activity is initialized to the first pattern in the sequence, and a constant external input is provided to each subpopulation (‘asymmetric’ input <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and ‘symmetric’ input <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>). In the top panel, the symmetric population is effectively silenced with strongly negative input, resulting in retrieval that lasts approximately <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, consistent with the dynamics being driven purely by the asymmetric component in the learning rule (<xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>). In the bottom panel, this input is no longer strongly negative, causing retrieval time to more than double, due to the effect of the symmetric population that tends to slow down the dynamics. Retrieval in both conditions is robust to noise, as shown in <xref ref-type="fig" rid="fig1">Figure 1E</xref>, in which noisy inputs to neurons strongly perturb single neuron firing rates but leave sequence retrieval intact at both speeds (see Methods).</p><p>To characterize how retrieval time depends on these two sources of external input, we explored the space of the parameters defining the inputs to the network, <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig2">Figure 2</xref>, we show the dependence of retrieval quality and speed on these variables. Retrieval quality is quantified by measuring the maximal correlation of the final pattern in the retrieved sequence. Retrieval speed is measured in units of the inverse of the neural time constant, <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>. It is computed by measuring the average inverse time between the peaks of consecutive correlations of the network state with consecutive patterns in the sequence. For example, a speed of 0.5 corresponds to an average time difference of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>2</mml:mn><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> between the peaks of the correlations of two consecutive retrieved patterns with network state. In the upper left quadrant of <xref ref-type="fig" rid="fig2">Figure 2b</xref>, speed depends primarily on the strength of input to the symmetric population. Moving away from this region in the direction of increasing symmetric input, retrieval speed slows down to approximately 0.5. In the lower right quadrant, retrieval speed instead depends primarily on the strength of external input provided to the asymmetric population. As this negative input grows, retrieval speed becomes approximately four times slower than the speed of the purely asymmetric network. In <xref ref-type="fig" rid="fig2">Figure 2</xref>, we have focused on the region in which external inputs are negative. This is because in our model external inputs are expressed relative to the threshold, and this region leads to biologically plausible average firing rates that are much smaller than the maximal firing rates (see Methods). While we have focused on negative input in <xref ref-type="fig" rid="fig2">Figure 2</xref>, retrieval speed is also modulated by positive input. Interestingly, it is the magnitude, not sign, of the input that determines retrieval speed. Expanding the phase diagram in panel (b) to positive input shows that the same dependence holds: values for retrieval speed are approximately symmetric about the <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> axes (not shown).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Retrieval properties depend on external inputs.</title><p>(<bold>a</bold>) Retrieval quality, defined as the peak correlation <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>m</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> of the final pattern in the sequence, as a function of external inputs to asymmetric population <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, and to symmetric population <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. The white line bounds the region of successful retrieval. Below this line (black region), retrieval is not possible, regardless of initial condition (see Methods). (<bold>b</bold>) Retrieval speed, measured by averaging the inverse time between consecutive pattern correlation peaks (see Methods). (<bold>c</bold>) Solid lines: firing rates of three randomly selected neurons during retrieval for parameters corresponding to the circle (left) and diamond (right) in panels (<bold>a–b</bold>), which are the same parameters used in <xref ref-type="fig" rid="fig1">Figure 1e</xref>. Note the approximate (but not exact) temporal scaling by a factor ∼ 3 between these two sets of external inputs. Dashed lines: firing rates in response to the same noisy inputs as in <xref ref-type="fig" rid="fig1">Figure 1e</xref>. (<bold>d</bold>) Activity of 125 units (from a total of 80,000), sorted by peak firing rate time, for parameters corresponding to the circle (left) and diamond (right) in panels (<bold>a–b</bold>). All other parameters are as in <xref ref-type="fig" rid="fig1">Figure 1b</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88805-fig2-v1.tif"/></fig></sec><sec id="s2-3"><title>Flexible retrieval with a non-linear plasticity rule</title><p>We next considered the consequences of a nonlinear learning rule implemented by the following presynaptic and postsynaptic functions in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>:<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the Heaviside function. This rule binarizes the activity patterns <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> according to a threshold, and its effects on persistent and sequential network activity have been studied extensively (<xref ref-type="bibr" rid="bib23">Lim et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Pereira and Brunel, 2018</xref>; <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>). The parameter <italic>q</italic><sub><italic>g</italic></sub> is chosen such that <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∫</mml:mo><mml:mi>D</mml:mi><mml:mi>z</mml:mi><mml:mi>g</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, which keeps the mean connection strength at zero. The general dependency of retrieval speed on asymmetric and symmetric inputs in a network utilizing this rule is similar to that of the bilinear rule (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). One key difference is that a much wider range of speeds can be achieved using a nonlinear rule within the same retrieval quality bounds (see Methods). In fact, retrieval speed can now be arbitrarily slowed down, and even completely stopped when the input to the asymmetric population is sufficiently negative (see white dots in <xref ref-type="fig" rid="fig3">Figure 3b</xref>). In this region, persistent activity is stable, and there exists a fixed point attractor correlated with any of the patterns in any stored sequence. There also exists a region in which sequential activity stops in the middle of retrieval and switches to stable persistent activity (see hatched diagonal lines in <xref ref-type="fig" rid="fig3">Figure 3b</xref>). Note that retrieval is not considered to be successful in this region (as the sequence is not fully retrieved), and so it is plotted in black.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Retrieval properties in networks with nonlinear learning rules.</title><p>(<bold>a</bold>) Correlations between stored patterns and network states in asymmetric (left) and symmetric (right) populations, for three different external input combinations (denoted by the inset symbol, see right panel). (<bold>b</bold>) Retrieval speed as a function of parameters describing external inputs, similarly as in <xref ref-type="fig" rid="fig2">Figure 2</xref>. White dots indicate the region in which stable persistent activity of the first pattern is present. Hatched diagonal lines indicate the region in which incomplete sequential activity terminates in stable persistent activity. All parameters are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>, except <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The parameters of the learning rule are <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>g</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.8</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88805-fig3-v1.tif"/></fig></sec><sec id="s2-4"><title>Temporally varying external inputs can lead to transitions between persistent and sequential activity</title><p>We next explored how this heterogeneity might be used not only to control the speed of dynamics, but also to trigger transitions between qualitatively different dynamics. In <xref ref-type="fig" rid="fig4">Figure 4</xref>, we use the same nonlinear model as in the previous section, and present discrete, time-dependent inputs intended to achieve persistent retrieval of a single pattern, followed by sequential retrieval of the remaining patterns at a specified time. To initiate persistent activity, we briefly present the first pattern as an input to the symmetric population. This elicits persistent activity in this population, as reflected by the sustained positive correlation of the symmetric population with the first pattern during the first 200ms (<xref ref-type="fig" rid="fig4">Figure 4b</xref>). This activity does not recruit sequential activity in either population, however, as the asymmetric population responsible for that transition is presented with sufficiently strong negative input during this period. To initiate sequential activity, inhibition to the asymmetric population is released after <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> s, prompting the network to retrieve the stored sequence in both populations.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Transition from persistent activity (‘preparatory’ period) to sequence retrieval (‘execution’ period) mediated by external input.</title><p>(<bold>a</bold>) Inputs provided to the asymmetric (black) and symmetric population (orange) consist of a ‘preparatory period’ input lasting 200ms, followed by an ‘execution period’ input that is fixed for the rest of the interval. During a 200ms preparatory period, a brief input is presented to the symmetric population for the first 10ms, which drives the network to a state which is strongly correlated with the first pattern in a sequence. This input is removed after 10ms, but the network remains in a persistent activity state corresponding the the first pattern, because a strong negative input is presented to the asymmetric population throughout the entire 200ms, which prevents the network from retrieving the sequence. At the end of this period, the input to the symmetric population is decreased, while the asymmetric population is increased, which leads to retrieval of the sequence (‘execution period’). Sequence retrieval can happen at different speeds, depending on the inputs to the asymmetric and symmetric populations. (<bold>b</bold>) Correlations with stored patterns in the sequence in each population, in each input scenario. Note correlations in the slow retrieval case are temporally scaled by a factor ∼ 2.5 compared to the fast retrieval case. (<bold>c</bold>) Example single unit firing rates in each population. Note that for some neurons firing rates do not follow a simple temporal rescaling - for instance the purple neuron in the symmetric population is active at around <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.45</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> in the slow retrieval case, but is not active in the fast retrieval case. All parameters are as in <xref ref-type="fig" rid="fig3">Figure 3</xref>, except <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.07</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88805-fig4-v1.tif"/></fig><p>Note that in this scenario also, a sequence can be retrieved at various speeds, using the same inputs during the persistent period, but changing the level of constant stimulation provided during retrieval (compare left and right panels in <xref ref-type="fig" rid="fig4">Figure 4b</xref>). As in a network with only a single asymmetric population, single neuron activity in this network is temporally sparse, with many neurons being active only at specific time intervals (<xref ref-type="fig" rid="fig4">Figure 4c</xref>).</p><p>In our network, stability of persistent activity requires the dependence of the plasticity rule on pre and/or post synaptic firing rates to be non-linear. With a bilinear learning rule and Gaussian patterns, the network dynamics does not converge to fixed-point attractors that are correlated with a single pattern, but rather to mixed states correlated with multiple patterns (<xref ref-type="bibr" rid="bib2">Amit et al., 1985</xref>).</p><p>The dynamics shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> reproduces some of the landmark features observed in electrophysiological recordings during delayed motor tasks. In such tasks, a preparatory period follows presentation of a cue (e.g. instructing a target direction or a desired response speed), during which the animal can prepare the motor response, but not execute it (<xref ref-type="bibr" rid="bib9">Churchland et al., 2012</xref>). This period is typically characterized by persistent activity of specific groups of neurons, whereas during motor execution those same neurons instead display transient activity (<xref ref-type="bibr" rid="bib37">Svoboda and Li, 2018</xref>).</p></sec><sec id="s2-5"><title>Flexible sequence retrieval in networks with a continuous distribution of degrees of temporal symmetry</title><p>Up to this point, we have analyzed a network model in which neurons are separated in two discrete classes distinguished by their plasticity rule (symmetric or asymmetric). For a given postsynaptic neuron, the learning rule present at all presynaptic synapses was chosen to be either temporally symmetric or asymmetric with equal probability, defining two distinct subpopulations of neurons. Can retrieval speed still be modulated by external input when synapses do not fall into such a binary classification, but have more heterogeneous properties? To model this heterogeneity, we chose to embed a continuum of learning rules. Instead of a bimodal distribution for <italic>z</italic><sub><italic>i</italic></sub> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, we choose a uniform distribution on the interval <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The input <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> provided to each neuron <italic>i</italic> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is a linear combination of symmetric and asymmetric input components: <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>. We also choose to investigate a network with the previously described non-linear plasticity rule. <xref ref-type="fig" rid="fig5">Figure 5</xref> shows that a network with these modifications also exhibits flexible sequence retrieval, and that speed decreases as the asymmetric input component becomes more negative. However, as shown in <xref ref-type="fig" rid="fig5">Figure 5c</xref>, to reach slower speeds a positive <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> is now required. Note that a region of stable persistent activity is no longer present in this scenario, as stable persistent activity requires that a finite fraction of neurons in the network have a symmetric plasticity rule.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Retrieval of sequences in networks with heterogeneous learning rules described by a continuum of degrees of symmetry.</title><p>(<bold>a</bold>) Firing rate dynamics of five representative neurons during retrieval for each external input configuration (see inset symbols in panel c). (<bold>b</bold>) Correlation of network activity with each stored pattern during retrieval for each external input configuration. (<bold>c</bold>) Retrieval speed as described in <xref ref-type="fig" rid="fig2">Figure 2</xref>. All parameters are as in <xref ref-type="fig" rid="fig3">Figure 3</xref> except <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>A</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88805-fig5-v1.tif"/></fig></sec><sec id="s2-6"><title>Learning external input strengths using a reward-based plasticity rule</title><p>The low-dimensional external inputs used to regulate speed are unrelated to the stored sequential input patterns. This suggests that a mapping from external inputs to retrieval speed can be learned independently from a particular set of sequential patterns. We demonstrated that a reinforcement learning rule can be used to converge to external input values implementing a desired speed (<xref ref-type="fig" rid="fig6">Figure 6</xref>). By using a reward signal measuring how similar retrieval is to the desired speed, the rule adjusts initially random external inputs to the appropriate values over the course of multiple trial repetitions (see Methods for details). Critically, once these external input values are learned, they can be used to modulate the retrieval speed of other stored sequences without having to relearn this mapping.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Desired target speeds can be reached by adjusting external inputs using a reward-based learning rule.</title><p>The black and grey lines denote the trajectories for two learning trials targeting different speeds. External inputs start at −0.2 (marked with a circle) and terminate at values implementing desired target speeds of 0.8 and 0.3 (marked with crosses). All parameters are as in <xref ref-type="fig" rid="fig2">Figure 2</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88805-fig6-v1.tif"/></fig></sec><sec id="s2-7"><title>Flexible retrieval of sequences in a spiking network</title><p>We have until now focused exclusively on rate networks that do not obey Dale’s law. We now turn to networks composed of excitatory and inhibitory spiking neurons, as a more realistic model of neurobiological networks. We implemented learning in excitatory to excitatory synaptic connectivity, generalizing the procedure described in <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref> to two excitatory subpopulations. We found that successful speed control can be obtained in such networks using biases in external inputs to symmetric and asymmetric populations, as in the simpler rate model described above. <xref ref-type="fig" rid="fig7">Figure 7</xref> shows network simulations using two different external input configurations, leading to sequence retrieval at two different speeds. Interestingly, small external input biases (<inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>∼</mml:mo><mml:mn>1</mml:mn><mml:mtext>mV</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>) relative to the difference in spiking threshold and resting potential (<inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>20</mml:mn><mml:mtext>mV</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>) are sufficient to generate a temporal rescaling of as large as ∼ 2.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Retrieval in a network of excitatory and inhibitory spiking neurons, in which excitatory neurons are subdivided into asymmetric and symmetric populations.</title><p>(<bold>a</bold>) Fast retrieval when inputs are biased to asymmetric neurons. <italic>Top</italic>: Correlation of network activity with each stored pattern. <italic>Middle</italic>: Voltage traces of three representative neurons. <italic>Bottom</italic>: The bottom two panels show raster plots of excitatory and inhibitory neurons, sorted by the latency of their peak firing rate. (<bold>b</bold>) Slow retrieval when inputs are biased to symmetric neurons. Note that only external inputs differ in (<bold>a</bold>) and (<bold>b</bold>). See Methods for parameters.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-88805-fig7-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>In this paper, we have introduced a new mechanism for flexible control of retrieval speed in networks storing sequences. This mechanism relies on heterogeneity of synaptic plasticity rules across neurons in the network, with different degrees of temporal asymmetry. Neurons with temporally symmetric plasticity act as brakes of the dynamics, as they stabilize network activity in its current state, while neurons with temporally asymmetric plasticity act instead as accelerators, as they push the network toward the next pattern in the sequence. The speed of retrieval can then be modified in a flexible way by changing external inputs driving these two types of neurons. Furthermore, we found that this mechanism can be used to gate transitions between persistent and sequential activity. We showed that appropriate inputs can be learned using a reinforcement learning scheme. Finally, we also showed that networks of spiking neurons can generate the same behavior, provided the excitatory network is subdivided in asymmetric and symmetric neurons.</p><sec id="s3-1"><title>Heterogeneity of synaptic plasticity</title><p>Our findings suggest a potential functional role for the experimentally observed diversity in synaptic plasticity rules (<xref ref-type="bibr" rid="bib6">Bi and Poo, 1998</xref>; <xref ref-type="bibr" rid="bib1">Abbott and Nelson, 2000</xref>; <xref ref-type="bibr" rid="bib34">Sjöström et al., 2001</xref>; <xref ref-type="bibr" rid="bib26">Mishra et al., 2016</xref>; <xref ref-type="bibr" rid="bib36">Suvrathan et al., 2016</xref>). In particular, a wide diversity of spike-timing dependent plasticity (STDP) curves have been reported in various brain structures, and sometimes in the same structure. In the hippocampus, temporally asymmetric STDP is typically observed in cultures (<xref ref-type="bibr" rid="bib6">Bi and Poo, 1998</xref>) or in CA3 to CA1 connections in slices in some conditions, but temporally symmetric STDP is observed in area CA3 (<xref ref-type="bibr" rid="bib26">Mishra et al., 2016</xref>). Interestingly, the degree of temporal symmetry at CA3 to CA1 connections can be modulated by extracellular calcium concentration (<xref ref-type="bibr" rid="bib18">Inglebert et al., 2020</xref>) and post-synaptic bursting (<xref ref-type="bibr" rid="bib44">Wittenberg and Wang, 2006</xref>; <xref ref-type="bibr" rid="bib18">Inglebert et al., 2020</xref>). In the cerebellum, synaptic plasticity rules with diverse temporal requirements on the time difference between parallel fiber and climbing fiber inputs have been found in Purkinje cells in different zones of this structure suvrathan16. While this heterogeneity has been found so far across structures or across different regions in the same structure, this heterogeneity could also be present within local networks, as current experimental methods for probing plasticity only have access to a single delay between pre and post-synaptic spikes in each recorded neuron, and would therefore miss this heterogeneity.</p><p>For simplicity, the degree of temporal asymmetry was chosen in our model to depend only on the identity of the postsynaptic neuron. This is consistent with the observation that a model of synaptic plasticity that depends only on the postsynaptic concentration of calcium can account for a range of experimentally observed STDP curves (<xref ref-type="bibr" rid="bib15">Graupner and Brunel, 2012</xref>). This suggests that heterogeneities in temporal asymmetry could arise due to heterogeneities in biophysical parameters that control calcium dynamics in post-synaptic spines.</p></sec><sec id="s3-2"><title>Comparison with other mechanisms of speed control</title><p>The mechanism investigated here is distinct from previously described models of input-driven speed control. It does not require adaptation mechanisms or delays to slow down retrieval of subsequent patterns (<xref ref-type="bibr" rid="bib35">Sompolinsky and Kanter, 1986</xref>; <xref ref-type="bibr" rid="bib27">Murray and Escola, 2017</xref>). It also does not require presentation of multiple exemplars spanning the desired range of retrieval speeds in order to find the appropriate network structure (<xref ref-type="bibr" rid="bib42">Wang et al., 2018</xref>). However, the mapping between external input strength and retrieval speed must be learned in order for the network to be able to perform retrieval at desired speeds. Unlike the model explored in <xref ref-type="bibr" rid="bib42">Wang et al., 2018</xref>, however, once this mapping is learned, it can be used to control the speed of other stored sequences.</p><p>Another recent study (<xref ref-type="bibr" rid="bib5">Beiran et al., 2023</xref>) has investigated how a recurrent network could flexibly control its temporal dynamics using a different approach. They trained a low-rank recurrent network using back-propagation through time to produce specific dynamics with flexible timing, and showed that the resulting network can then be flexibly controlled by a one-dimensional input. It would be interesting to investigate whether the low-rank structure found in such a manner exhibits similarities with the synaptic connectivity structure in our model.</p><p>Future experimental work could analyze the evolution of neural activity across the training of interval timing tasks, and evaluate whether it is consistent with such a reinforcement-based rule.</p></sec><sec id="s3-3"><title>Experimental predictions</title><p>This mechanism presented here makes several predictions regarding the relationship between plasticity rules, external input, and the speed of network dynamics. One prediction is that retrieval speed could be modified by providing different external inputs to each population (asymmetric and symmetric). In vivo, these populations could be identified using the dependence of mean firing rates on speed of retrieval - neurons who increase their rates with slower/faster retrieval speeds would be predicted to be the symmetric/asymmetric neurons, respectively. Targeting one class of neurons or the other, using holographic techniques (see e.g. <xref ref-type="bibr" rid="bib25">Marshel et al., 2019</xref>) would then be expected to increase or decrease the speed of retrieval. Another prediction is that these cells have distinct profiles of temporal asymmetry in their synaptic plasticity. The model presented here also predicts the existence of ‘null’ input directions, for which no change in retrieval speed is expected as external input is changed. When moving along these ‘null’ directions, single neurons would only be expected to change their temporal firing patterns, but without affecting the speed of retrieval.</p></sec><sec id="s3-4"><title>Transitions between persistent and sequential activity</title><p>Heterogeneity in the learning rule also provides a mechanism that enables input changes to drive transitions in activity states. An example of such a transition is frequently reported in primary motor cortex (M1) during delayed reaching tasks, where a preparatory period with persistent activity or ramping dynamics is followed by an execution period with transient, sequential dynamics (<xref ref-type="bibr" rid="bib33">Riehle and Requin, 1989</xref>; <xref ref-type="bibr" rid="bib22">Li et al., 2016</xref>). We demonstrated how an input change can gate such a transition in a simple network model composed of neurons with two distinct plasticity rules, the first temporally symmetric, and the second temporally asymmetric. At the start of the preparatory period, asymmetric neurons are inhibited, and a transient specific input elicits persistent activity in symmetric neurons. When inhibition is removed, asymmetric neurons become activated and drive a transition to sequential activity in both types of neurons.</p><p>Inhibitory gating has been previously hypothesized as a mechanism to control the initiation of execution period activity. Analysis of M1 activity suggests that local inhibitory interneurons do not engage in this gating, as putative inhibitory neurons do not appear to be preferentially active during the preparatory period compared to the execution period (<xref ref-type="bibr" rid="bib20">Kaufman et al., 2013</xref>). However, this does not rule out the possibility that the necessary inhibition could arise from other external inputs to M1. It is also possible that inhibition may not be required at all. Effective silencing of the asymmetric neurons could occur by a reduction of excitatory input during the preparatory period. Recent work in mice suggests that thalamocortical interactions may be a potential candidate for driving the required transition. Recorded activity in motor thalamus during a reaching task shows that at movement onset, thalamus activity is negatively correlated with premotor activity, but positively correlated with activity in M1 (<xref ref-type="bibr" rid="bib28">Nashef et al., 2021</xref>). In a separate directional licking task, thalamus projections were shown to be required for initiating cued movement, and mimicked presentation of the cue when optogenetically stimulated (<xref ref-type="bibr" rid="bib17">Inagaki et al., 2022</xref>). An alternative model for transitions between preparatory and execution activity has recently been proposed (<xref ref-type="bibr" rid="bib4">Bachschmid-Romano et al., 2023</xref>), in which external inputs trigger a switch between a preparatory state and a nearly orthogonal execution state. However, in the model of <xref ref-type="bibr" rid="bib4">Bachschmid-Romano et al., 2023</xref>, the execution epoch is described by a single pattern, and any temporal dynamics within this epoch is inherited from external inputs, while in the present paper the temporal dynamics during the execution phase is generated by the recurrent connectivity structure.</p></sec><sec id="s3-5"><title>Limitations and future directions</title><p>We have focused here on a simple learning scenario in which a temporally asymmetric plasticity rule imprints a sequence of external input patterns into the recurrent synaptic connectivity. In real neuronal networks, one expects recurrent synaptic inputs to shape the response of a network to external inputs, and therefore how such inputs sculpt recurrent connectivity. Studying such a learning process is outside the scope of this paper, but is an important topic for future work.</p><p>In this paper, we have focused on Hebbian specific synaptic plasticity rules to store a sequence of input patterns. Another fruitful approach to investigate learning and memory in neural circuits was introduced by <xref ref-type="bibr" rid="bib12">Gardner, 1988</xref>. In Gardner’s approach, the idea is to consider the space of all possible connectivity matrices that store a given set of memories as fixed point attractor states. It was later shown that the statistics of the connectivity matrix in attractor networks with sign-constrained synapses optimizing the storage capacity and/or robustness of learning is in striking agreement with cortical data - in particular, the resulting connectivity is sparse, with an overrepresentation of bidirectional motifs in pairs of neurons, compared to random directed Erdos-Renyi networks (<xref ref-type="bibr" rid="bib8">Brunel, 2016</xref>). However, in networks storing sequences, no such overrepresentation exists (<xref ref-type="bibr" rid="bib8">Brunel, 2016</xref>). It will be interesting to investigate the statistics of connectivity in networks with flexibility constraints, such that sequences can be retrieved at different speeds, or with a coexistence of fixed point attractor dynamics with sequential retrieval.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Neuronal transfer function</title><p>The neuronal transfer function is given by the sigmoidal function<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mi>ϕ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>h</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>r</mml:mi><mml:mtext>max</mml:mtext></mml:msub><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mtext>erf</mml:mtext><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mfrac><mml:mrow><mml:mi>h</mml:mi><mml:mo>−</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:msqrt><mml:mn>2</mml:mn></mml:msqrt><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> determines the input at which the neuron fires at half the maximal value <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>σ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is inversely proportional to the gain. This function was chosen for continuity with previous work (<xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>). We expect that using qualitatively similar functions should not alter the results of this paper.</p></sec><sec id="s4-2"><title>Noisy inputs</title><p>We introduce noisy inputs to each neuron in <xref ref-type="fig" rid="fig1">Figures 1e</xref> and <xref ref-type="fig" rid="fig2">2c</xref> through independent realizations of an Ornstein-Uhlenbeck process with a mean equal to either <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mtext>ext</mml:mtext></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively, with standard deviation of 0.3, and a correlation time constant of 4 ms. This noise leads to fluctuations of firing rate that are comparable to rate fluctuations induced by sequence retrieval (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), while leaving sequence retrieval intact (<xref ref-type="fig" rid="fig1">Figure 1e</xref>).</p></sec><sec id="s4-3"><title>Measuring pattern correlations</title><p>To compute the Pearson pattern correlation <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, we compute the overlap of each of the stored patterns <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> with the instantaneous firing rates for the entire population and divide by the standard deviation of firing rate activity: <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>r</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. In <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig4">4</xref>, we compute the correlations separately for each subpopulation.</p></sec><sec id="s4-4"><title>Measuring retrieval speed</title><p>To measure retrieval speed <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig5">5</xref>, we recorded the times at which each pattern correlation attained its peak value, and computed the average time difference between the peaks of successive patterns in a sequence. We then divided the time constant of the rate dynamics by this averaged value in order to convert speed into units of <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>τ</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mi>τ</mml:mi><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>l</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>x</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To account for simulations with dynamics that did not have well-defined correlation peaks (typically observed at extreme storage loads or with persistent activity), we excluded peak time differences that exceeded two standard deviations of the average difference value. If no peak time difference passed this criteria, the sequence was considered not retrieved (black regions in <xref ref-type="fig" rid="fig2">Figures 2</xref>, <xref ref-type="fig" rid="fig3">3</xref> and <xref ref-type="fig" rid="fig5">5</xref>).</p></sec><sec id="s4-5"><title>Mean-field theory of single-population network with variable degree of temporal asymmetry</title><p>In this section we derive a mean-field theory for the single population network with homogeneous synaptic plasticity. This is a generalization of the theory derived for a purely temporally asymmetric network <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>. We define order parameters <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>M</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>r</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, describing the average overlap of network activity with pattern <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and the average squared firing rate, respectively.</p><p>Using <xref ref-type="disp-formula" rid="equ1 equ2">Equations 1 and 2</xref>, we derive equations describing the temporal evolution of the overlaps (for <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>,</mml:mo><mml:mi>P</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>),<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mo>∫</mml:mo><mml:mi>D</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mi>D</mml:mi><mml:mi>x</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msup><mml:mi>ξ</mml:mi><mml:mi>l</mml:mi></mml:msup><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mtext>ext</mml:mtext></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is a ‘noise’ term due to patterns <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>≠</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the sequence (see <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref> for details) By making the following change of variables:<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:mi>v</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:msub><mml:mi>ξ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:msub><mml:mi>R</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula><disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mi>u</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mi>R</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mi>x</mml:mi></mml:mrow><mml:msqrt><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:msqrt></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>in which we have defined <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>z</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>z</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we obtain<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>z</mml:mi></mml:msubsup><mml:mi>G</mml:mi><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mi>z</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>l</mml:mi><mml:mn>2</mml:mn></mml:msubsup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∫</mml:mo><mml:mi>D</mml:mi><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:msqrt><mml:mi>x</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mi>x</mml:mi></mml:msqrt></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Assuming that <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>G</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is the case during successful retrieval (see also <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>), then we can simplify to:<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>z</mml:mi></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>This equation makes it clear that retrieval speed depends linearly on <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>z</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, that is on the balance between the symmetric and asymmetric components of synaptic plasticity.</p></sec><sec id="s4-6"><title>Mean-field theory of heterogeneous network and conditions for retrieval</title><p>Mean-field theory can be used to further analyze retrieval speed dynamics, along the lines of <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>. We define order parameters <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, describing the average overlap of network activity in subpopulation <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> with pattern <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> and the average squared firing rate in subpopulation <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. The equations for the overlaps are given by:<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mo>⋅</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>G</italic>is given, for arbitrary transfer functions <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ϕ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> by:<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>∫</mml:mo><mml:mi>D</mml:mi><mml:mi>v</mml:mi><mml:mi>ϕ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:msqrt><mml:mi>x</mml:mi></mml:msqrt><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mrow><mml:msqrt><mml:mi>x</mml:mi></mml:msqrt></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>For the transfer function used in this paper, <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>, the expression simplifies,<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>X</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≡</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:msqrt></mml:mfrac><mml:mtext>exp</mml:mtext><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>As in the previous section, <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>R</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are ‘noise’ terms due to patterns <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>≠</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in the sequence, which also depends on the average squared firing rates <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>M</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Using <xref ref-type="disp-formula" rid="equ17 equ18">Equations 17 and 18</xref>, we can derive the dynamics of the combined population overlap <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mi>τ</mml:mi><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mi>G</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msub><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>R</mml:mi><mml:mi>l</mml:mi><mml:mi>s</mml:mi></mml:msubsup><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>To compute the boundary for successful retrieval given by the white line in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we analyze this equation when the gains are constant: <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Plugging in and rearranging, we find:<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mfrac><mml:mi>τ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mfrac><mml:msub><mml:mi>G</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>This equation shows that the sequence can only be retrieved if <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>&gt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, otherwise the peak of the overlaps decay to zero with increasing <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus retrieval of an asymptotically long sequence is successful if the gain converges to a value greater or equal to one during retrieval. This condition can only be satisfied if<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mi>x</mml:mi></mml:munder><mml:mrow><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">[</mml:mo></mml:mrow><mml:msub><mml:mi>G</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>G</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mi>s</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">]</mml:mo></mml:mrow></mml:mrow><mml:mo>≥</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>To test for successful sequence retrieval in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we computed the maximal correlation value of the final pattern <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and compared this value to a threshold <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. If the value fell below this threshold, then retrieval was considered unsuccessful, and was denoted by a black square. This threshold criterion was also used in <xref ref-type="fig" rid="fig3">Figures 3</xref> and <xref ref-type="fig" rid="fig5">5</xref>.</p></sec><sec id="s4-7"><title>Reward-driven learning</title><p>A simple perturbation-based reinforcement learning rule is used to demonstrate that external inputs can be generated that produce network dynamics at a desired target speed over the course of multiple trial repetitions. We simulate a series of trials with stochastically varying external inputs. At each trial <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, the external inputs used in the previous trial are perturbed randomly,<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>pert</mml:mtext></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula><disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>pert</mml:mtext></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>where λ is the strength of the perturbation, and <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:msubsup><mml:mi>x</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mstyle></mml:math></inline-formula> are uniformly distributed random variables over the interval <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, drawn independently for each population <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> at each trial <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. If these external inputs lead to an improvement in speed compared to previous trials, then<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>pert</mml:mtext></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula><disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mtext>pert</mml:mtext></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>else,<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula><disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="fig" rid="fig6">Figure 6</xref>, the correlation threshold <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, the target speed <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>r</mml:mi><mml:mi>g</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>0.3</mml:mn><mml:mo>,</mml:mo><mml:mn>0.8</mml:mn><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>λ</mml:mi><mml:mo>=</mml:mo><mml:mn>0.1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. On the first trial <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>n</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the external inputs are taken to be <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.2</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (open circle in <xref ref-type="fig" rid="fig6">Figure 6</xref>).</p></sec><sec id="s4-8"><title>Network of excitatory and inhibitory spiking neurons</title><p>We simulated a network of excitatory and inhibitory leaky integrate-and-fire (LIF) neurons similar to the one described described in the Appendix of <xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref> (sections 3 and 4) with a few differences described below.</p><p>In this network, the dynamics of the membrane potential of neuron <italic>i</italic> (<inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) in population <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) are governed by the following equations:<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>α</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi mathvariant="normal">Θ</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mtext>floor</mml:mtext><mml:mi>α</mml:mi></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mrow><mml:mtext>rest</mml:mtext></mml:mrow><mml:mi>α</mml:mi></mml:msubsup><mml:mo>+</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:munder><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>≠</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:munderover><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:msqrt><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mi>α</mml:mi></mml:msubsup></mml:msqrt><mml:msup><mml:mi>W</mml:mi><mml:mi>α</mml:mi></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>α</mml:mi></mml:msubsup><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:msubsup></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:msubsup><mml:mi>S</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>α</mml:mi><mml:mi>β</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mi>α</mml:mi></mml:msubsup><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:munder><mml:mi>δ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:msubsup><mml:mi>t</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>D</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mi>β</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>E</mml:mi><mml:mo>,</mml:mo><mml:mi>I</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the synaptic delay, <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>(t) controls the time-dependent external input drive, <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> controls the refractory period, <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi mathvariant="normal">Θ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is the Heaviside function, and <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> is a white noise input with zero mean and unit variance density.</p><p>Excitatory neurons are divided into two (asymmetric and symmetric) populations of equal size (<inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>), with connectivity matrices given by the following, where <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ω</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> is the rectified synaptic transfer function defined in the procedure and <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>X</mml:mi><mml:mo>∈</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mi>c</mml:mi></mml:msqrt></mml:mfrac><mml:mi>ω</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>a</mml:mi></mml:msub><mml:mi>c</mml:mi></mml:msqrt></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mtext>f</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mtext>g</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mstyle></mml:mrow></mml:math></disp-formula><disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>J</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>c</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>X</mml:mi></mml:mrow></mml:msubsup><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>c</mml:mi></mml:msqrt></mml:mfrac><mml:mi>ω</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mi>N</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mi>c</mml:mi></mml:msqrt></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>P</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:munderover><mml:mtext>f</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mtext>g</mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>X</mml:mi><mml:mo>,</mml:mo><mml:mi>μ</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>The excitatory populations receive external input that depends on their identity, and on the retrieval configuration. For slow retrieval, we set the input <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>E</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> equal to <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>1.5</mml:mn><mml:mtext>mV</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn><mml:mtext>mV</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> for asymmetric and symmetric neurons, respectively. For fast retrieval, we use <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>a</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn><mml:mtext>mV</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:mn>0.75</mml:mn><mml:mtext>mV</mml:mtext></mml:mrow></mml:mstyle></mml:math></inline-formula>. In inhibitory neurons, we use <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>The learning strength (<inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>E</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>) is set to .25, which result in changes to the following parameters: <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>J</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msub><mml:mi>K</mml:mi><mml:mrow><mml:mi>I</mml:mi><mml:mi>E</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.134</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>E</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>5.123</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>λ</mml:mi><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>I</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mn>3.012</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. All other parameter values are identical to those documented in Table 7c of the referenced Appendix (<xref ref-type="bibr" rid="bib13">Gillett et al., 2020</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Formal analysis, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-88805-mdarchecklist1-v1.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The current manuscript is a computational study, so no data have been generated for this manuscript. Code to run the simulations and build the figures have been uploaded to Github at <ext-link ext-link-type="uri" xlink:href="https://github.com/maxgillett/dynamic_speed_control">https://github.com/maxgillett/dynamic_speed_control</ext-link> (copy archived at <xref ref-type="bibr" rid="bib14">Gillett, 2024</xref>).</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Synaptic plasticity: taming the beast</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>1178</fpage><lpage>1183</lpage><pub-id pub-id-type="doi">10.1038/81453</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname><given-names>DJ</given-names></name><name><surname>Gutfreund</surname><given-names>H</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1985">1985</year><article-title>Spin-glass models of neural networks</article-title><source>Physical Review A</source><volume>32</volume><fpage>1007</fpage><lpage>1018</lpage><pub-id pub-id-type="doi">10.1103/PhysRevA.32.1007</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amit</surname><given-names>DJ</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Model of global spontaneous activity and local structured activity during delay periods in the cerebral cortex</article-title><source>Cerebral Cortex</source><volume>7</volume><fpage>237</fpage><lpage>252</lpage><pub-id pub-id-type="doi">10.1093/cercor/7.3.237</pub-id><pub-id pub-id-type="pmid">9143444</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bachschmid-Romano</surname><given-names>L</given-names></name><name><surname>Hatsopoulos</surname><given-names>NG</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Interplay between external inputs and recurrent dynamics during movement preparation and execution in a network model of motor cortex</article-title><source>eLife</source><volume>12</volume><elocation-id>e77690</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.77690</pub-id><pub-id pub-id-type="pmid">37166452</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beiran</surname><given-names>M</given-names></name><name><surname>Meirhaeghe</surname><given-names>N</given-names></name><name><surname>Sohn</surname><given-names>H</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name><name><surname>Ostojic</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Parametric control of flexible timing through low-dimensional neural manifolds</article-title><source>Neuron</source><volume>111</volume><fpage>739</fpage><lpage>753</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.12.016</pub-id><pub-id pub-id-type="pmid">36640766</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bi</surname><given-names>GQ</given-names></name><name><surname>Poo</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Synaptic modifications in cultured hippocampal neurons: dependence on spike timing, synaptic strength, and postsynaptic cell type</article-title><source>The Journal of Neuroscience</source><volume>18</volume><fpage>10464</fpage><lpage>10472</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.18-24-10464.1998</pub-id><pub-id pub-id-type="pmid">9852584</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2005">2005</year><chapter-title>Course 10 - network models of memory</chapter-title><person-group person-group-type="editor"><name><surname>Chow</surname><given-names>CC</given-names></name><name><surname>Gutkin</surname><given-names>B</given-names></name><name><surname>Hansel</surname><given-names>D</given-names></name><name><surname>Meunier</surname><given-names>C</given-names></name></person-group><source>Les Houches. Methods and Models in Neurophysics</source><publisher-name>Elsevier</publisher-name><fpage>407</fpage><lpage>476</lpage><pub-id pub-id-type="doi">10.1016/S0924-8099(05)80016-2</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Is cortical connectivity optimized for storing information?</article-title><source>Nature Neuroscience</source><volume>19</volume><fpage>749</fpage><lpage>755</lpage><pub-id pub-id-type="doi">10.1038/nn.4286</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Cunningham</surname><given-names>JP</given-names></name><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Foster</surname><given-names>JD</given-names></name><name><surname>Nuyujukian</surname><given-names>P</given-names></name><name><surname>Ryu</surname><given-names>SI</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural population dynamics during reaching</article-title><source>Nature</source><volume>487</volume><fpage>51</fpage><lpage>56</lpage><pub-id pub-id-type="doi">10.1038/nature11129</pub-id><pub-id pub-id-type="pmid">22722855</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egger</surname><given-names>V</given-names></name><name><surname>Feldmeyer</surname><given-names>D</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Coincidence detection and changes of synaptic efficacy in spiny stellate neurons in rat barrel cortex</article-title><source>Nature Neuroscience</source><volume>2</volume><fpage>1098</fpage><lpage>1105</lpage><pub-id pub-id-type="doi">10.1038/16026</pub-id><pub-id pub-id-type="pmid">10570487</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fiete</surname><given-names>IR</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>CZH</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Spike-time-dependent plasticity and heterosynaptic competition organize networks to produce long scale-free sequences of neural activity</article-title><source>Neuron</source><volume>65</volume><fpage>563</fpage><lpage>576</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.02.003</pub-id><pub-id pub-id-type="pmid">20188660</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The space of interactions in neural network models</article-title><source>Journal of Physics A</source><volume>21</volume><fpage>257</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1088/0305-4470/21/1/030</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gillett</surname><given-names>M</given-names></name><name><surname>Pereira</surname><given-names>U</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Characteristics of sequential activity in networks with temporally asymmetric Hebbian learning</article-title><source>PNAS</source><volume>117</volume><fpage>29948</fpage><lpage>29958</lpage><pub-id pub-id-type="doi">10.1073/pnas.1918674117</pub-id><pub-id pub-id-type="pmid">33177232</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Gillett</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2024">2024</year><data-title>Dynamic_speed_control</data-title><version designator="swh:1:rev:462af73bfeb54c0a8b88522988f8b83c002a49d4">swh:1:rev:462af73bfeb54c0a8b88522988f8b83c002a49d4</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:0a08a78367cfaba3f949e7bdaf520eedc61023e3;origin=https://github.com/maxgillett/dynamic_speed_control;visit=swh:1:snp:205b282583b656d28dc27fcbf2a7778c362cf524;anchor=swh:1:rev:462af73bfeb54c0a8b88522988f8b83c002a49d4">https://archive.softwareheritage.org/swh:1:dir:0a08a78367cfaba3f949e7bdaf520eedc61023e3;origin=https://github.com/maxgillett/dynamic_speed_control;visit=swh:1:snp:205b282583b656d28dc27fcbf2a7778c362cf524;anchor=swh:1:rev:462af73bfeb54c0a8b88522988f8b83c002a49d4</ext-link></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graupner</surname><given-names>M</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Calcium-based plasticity model explains sensitivity of synaptic changes to spike pattern, rate, and dendritic location</article-title><source>PNAS</source><volume>109</volume><fpage>3991</fpage><lpage>3996</lpage><pub-id pub-id-type="doi">10.1073/pnas.1109359109</pub-id><pub-id pub-id-type="pmid">22357758</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hopfield</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1982">1982</year><article-title>Neural networks and physical systems with emergent collective computational abilities</article-title><source>PNAS</source><volume>79</volume><fpage>2554</fpage><lpage>2558</lpage><pub-id pub-id-type="doi">10.1073/pnas.79.8.2554</pub-id><pub-id pub-id-type="pmid">6953413</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inagaki</surname><given-names>HK</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Ridder</surname><given-names>MC</given-names></name><name><surname>Sah</surname><given-names>P</given-names></name><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Yang</surname><given-names>Z</given-names></name><name><surname>Hasanbegovic</surname><given-names>H</given-names></name><name><surname>Gao</surname><given-names>Z</given-names></name><name><surname>Gerfen</surname><given-names>CR</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A midbrain-thalamus-cortex circuit reorganizes cortical dynamics to initiate movement</article-title><source>Cell</source><volume>185</volume><fpage>1065</fpage><lpage>1081</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2022.02.006</pub-id><pub-id pub-id-type="pmid">35245431</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Inglebert</surname><given-names>Y</given-names></name><name><surname>Aljadeff</surname><given-names>J</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name><name><surname>Debanne</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Synaptic plasticity rules with physiological calcium levels</article-title><source>PNAS</source><volume>117</volume><fpage>33639</fpage><lpage>33648</lpage><pub-id pub-id-type="doi">10.1073/pnas.2013663117</pub-id><pub-id pub-id-type="pmid">33328274</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>JK</given-names></name><name><surname>Jin</surname><given-names>DZ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Development of neural circuitry for precise temporal sequences through spontaneous activity, axon remodeling, and synaptic plasticity</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e723</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0000723</pub-id><pub-id pub-id-type="pmid">17684568</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufman</surname><given-names>MT</given-names></name><name><surname>Churchland</surname><given-names>MM</given-names></name><name><surname>Shenoy</surname><given-names>KV</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The roles of monkey M1 neuron classes in movement preparation and execution</article-title><source>Journal of Neurophysiology</source><volume>110</volume><fpage>817</fpage><lpage>825</lpage><pub-id pub-id-type="doi">10.1152/jn.00892.2011</pub-id><pub-id pub-id-type="pmid">23699057</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kleinfeld</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Sequential state generation by model neural networks</article-title><source>PNAS</source><volume>83</volume><fpage>9469</fpage><lpage>9473</lpage><pub-id pub-id-type="doi">10.1073/pnas.83.24.9469</pub-id><pub-id pub-id-type="pmid">3467316</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>N</given-names></name><name><surname>Daie</surname><given-names>K</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Robust neuronal dynamics in premotor cortex during motor planning</article-title><source>Nature</source><volume>532</volume><fpage>459</fpage><lpage>464</lpage><pub-id pub-id-type="doi">10.1038/nature17643</pub-id><pub-id pub-id-type="pmid">27074502</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lim</surname><given-names>S</given-names></name><name><surname>McKee</surname><given-names>JL</given-names></name><name><surname>Woloszyn</surname><given-names>L</given-names></name><name><surname>Amit</surname><given-names>Y</given-names></name><name><surname>Freedman</surname><given-names>DJ</given-names></name><name><surname>Sheinberg</surname><given-names>DL</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Inferring learning rules from distributions of firing rates in cortical neurons</article-title><source>Nature Neuroscience</source><volume>18</volume><fpage>1804</fpage><lpage>1810</lpage><pub-id pub-id-type="doi">10.1038/nn.4158</pub-id><pub-id pub-id-type="pmid">26523643</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>JK</given-names></name><name><surname>Buonomano</surname><given-names>DV</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Embedding multiple trajectories in simulated recurrent neural networks in a self-organizing manner</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>13172</fpage><lpage>13181</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2358-09.2009</pub-id><pub-id pub-id-type="pmid">19846705</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marshel</surname><given-names>JH</given-names></name><name><surname>Kim</surname><given-names>YS</given-names></name><name><surname>Machado</surname><given-names>TA</given-names></name><name><surname>Quirin</surname><given-names>S</given-names></name><name><surname>Benson</surname><given-names>B</given-names></name><name><surname>Kadmon</surname><given-names>J</given-names></name><name><surname>Raja</surname><given-names>C</given-names></name><name><surname>Chibukhchyan</surname><given-names>A</given-names></name><name><surname>Ramakrishnan</surname><given-names>C</given-names></name><name><surname>Inoue</surname><given-names>M</given-names></name><name><surname>Shane</surname><given-names>JC</given-names></name><name><surname>McKnight</surname><given-names>DJ</given-names></name><name><surname>Yoshizawa</surname><given-names>S</given-names></name><name><surname>Kato</surname><given-names>HE</given-names></name><name><surname>Ganguli</surname><given-names>S</given-names></name><name><surname>Deisseroth</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cortical layer-specific critical dynamics triggering perception</article-title><source>Science</source><volume>365</volume><elocation-id>eaaw5202</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaw5202</pub-id><pub-id pub-id-type="pmid">31320556</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mishra</surname><given-names>RK</given-names></name><name><surname>Kim</surname><given-names>S</given-names></name><name><surname>Guzman</surname><given-names>SJ</given-names></name><name><surname>Jonas</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Symmetric spike timing-dependent plasticity at CA3-CA3 synapses optimizes storage and recall in autoassociative networks</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>11552</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms11552</pub-id><pub-id pub-id-type="pmid">27174042</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Murray</surname><given-names>JM</given-names></name><name><surname>Escola</surname><given-names>GS</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learning multiple variable-speed sequences in striatum via cortical tutoring</article-title><source>eLife</source><volume>6</volume><elocation-id>e26084</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.26084</pub-id><pub-id pub-id-type="pmid">28481200</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nashef</surname><given-names>A</given-names></name><name><surname>Mitelman</surname><given-names>R</given-names></name><name><surname>Harel</surname><given-names>R</given-names></name><name><surname>Joshua</surname><given-names>M</given-names></name><name><surname>Prut</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Area-specific thalamocortical synchronization underlies the transition from motor planning to execution</article-title><source>PNAS</source><volume>118</volume><elocation-id>e2012658118</elocation-id><pub-id pub-id-type="doi">10.1073/pnas.2012658118</pub-id><pub-id pub-id-type="pmid">33526664</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Okubo</surname><given-names>TS</given-names></name><name><surname>Mackevicius</surname><given-names>EL</given-names></name><name><surname>Payne</surname><given-names>HL</given-names></name><name><surname>Lynch</surname><given-names>GF</given-names></name><name><surname>Fee</surname><given-names>MS</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Growth and splitting of neural sequences in songbird vocal development</article-title><source>Nature</source><volume>528</volume><fpage>352</fpage><lpage>357</lpage><pub-id pub-id-type="doi">10.1038/nature15741</pub-id><pub-id pub-id-type="pmid">26618871</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>U</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Attractor dynamics in networks with learning rules inferred from invivo data</article-title><source>Neuron</source><volume>99</volume><fpage>227</fpage><lpage>238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.05.038</pub-id><pub-id pub-id-type="pmid">29909997</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pereira</surname><given-names>U</given-names></name><name><surname>Brunel</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Unsupervised learning of persistent and sequential activity</article-title><source>Frontiers in Computational Neuroscience</source><volume>13</volume><elocation-id>97</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2019.00097</pub-id><pub-id pub-id-type="pmid">32009924</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ravid Tannenbaum</surname><given-names>N</given-names></name><name><surname>Burak</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Shaping neural circuits by high order synaptic interactions</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005056</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005056</pub-id><pub-id pub-id-type="pmid">27517461</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Riehle</surname><given-names>A</given-names></name><name><surname>Requin</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Monkey primary motor and premotor cortex: single-cell activity related to prior information about direction and extent of an intended movement</article-title><source>Journal of Neurophysiology</source><volume>61</volume><fpage>534</fpage><lpage>549</lpage><pub-id pub-id-type="doi">10.1152/jn.1989.61.3.534</pub-id><pub-id pub-id-type="pmid">2709098</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sjöström</surname><given-names>PJ</given-names></name><name><surname>Turrigiano</surname><given-names>GG</given-names></name><name><surname>Nelson</surname><given-names>SB</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Rate, timing, and cooperativity jointly determine cortical synaptic plasticity</article-title><source>Neuron</source><volume>32</volume><fpage>1149</fpage><lpage>1164</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(01)00542-6</pub-id><pub-id pub-id-type="pmid">11754844</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sompolinsky</surname><given-names>H</given-names></name><name><surname>Kanter</surname><given-names>I</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Temporal association in asymmetric neural networks</article-title><source>Physical Review Letters</source><volume>57</volume><fpage>2861</fpage><lpage>2864</lpage><pub-id pub-id-type="doi">10.1103/PhysRevLett.57.2861</pub-id><pub-id pub-id-type="pmid">10033885</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Suvrathan</surname><given-names>A</given-names></name><name><surname>Payne</surname><given-names>HL</given-names></name><name><surname>Raymond</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Timing rules for synaptic plasticity matched to behavioral function</article-title><source>Neuron</source><volume>92</volume><fpage>959</fpage><lpage>967</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.10.022</pub-id><pub-id pub-id-type="pmid">27839999</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural mechanisms of movement planning: motor cortex and beyond</article-title><source>Current Opinion in Neurobiology</source><volume>49</volume><fpage>33</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2017.10.023</pub-id><pub-id pub-id-type="pmid">29172091</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Theodoni</surname><given-names>P</given-names></name><name><surname>Rovira</surname><given-names>B</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Roxin</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Theta-modulation drives the emergence of connectivity patterns underlying replay in a network model of place cells</article-title><source>eLife</source><volume>7</volume><elocation-id>e37388</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37388</pub-id><pub-id pub-id-type="pmid">30355442</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Tupikov</surname><given-names>Y</given-names></name><name><surname>Jin</surname><given-names>DZ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Addition of New Neurons and the Emergence of a Local Neural Circuit for Precise Timing</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.03.04.977025</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Waddington</surname><given-names>A</given-names></name><name><surname>Appleby</surname><given-names>PA</given-names></name><name><surname>De Kamps</surname><given-names>M</given-names></name><name><surname>Cohen</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Triphasic spike-timing-dependent plasticity organizes networks to produce robust sequences of neural activity</article-title><source>Frontiers in Computational Neuroscience</source><volume>6</volume><elocation-id>88</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2012.00088</pub-id><pub-id pub-id-type="pmid">23162457</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(00)01868-3</pub-id><pub-id pub-id-type="pmid">11476885</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Narain</surname><given-names>D</given-names></name><name><surname>Hosseini</surname><given-names>EA</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Flexible timing by temporal scaling of cortical responses</article-title><source>Nature Neuroscience</source><volume>21</volume><fpage>102</fpage><lpage>110</lpage><pub-id pub-id-type="doi">10.1038/s41593-017-0028-6</pub-id><pub-id pub-id-type="pmid">29203897</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weissenberger</surname><given-names>F</given-names></name><name><surname>Meier</surname><given-names>F</given-names></name><name><surname>Lengler</surname><given-names>J</given-names></name><name><surname>Einarsson</surname><given-names>H</given-names></name><name><surname>Steger</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Long synfire chains emerge by spike-timing dependent plasticity modulated by population activity</article-title><source>International Journal of Neural Systems</source><volume>27</volume><elocation-id>1750044</elocation-id><pub-id pub-id-type="doi">10.1142/S0129065717500447</pub-id><pub-id pub-id-type="pmid">28982282</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wittenberg</surname><given-names>GM</given-names></name><name><surname>Wang</surname><given-names>SS-H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Malleability of spike-timing-dependent plasticity at the CA3-CA1 synapse</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>6610</fpage><lpage>6617</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.5388-05.2006</pub-id><pub-id pub-id-type="pmid">16775149</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>JC</given-names></name><name><surname>Lau</surname><given-names>PM</given-names></name><name><surname>Bi</surname><given-names>GQ</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Gain in sensitivity and loss in temporal contrast of STDP by dopaminergic modulation at hippocampal synapses</article-title><source>PNAS</source><volume>106</volume><fpage>13028</fpage><lpage>13033</lpage><pub-id pub-id-type="doi">10.1073/pnas.0900546106</pub-id><pub-id pub-id-type="pmid">19620735</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zheng</surname><given-names>P</given-names></name><name><surname>Triesch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Robust development of synfire chains from multiple plasticity mechanisms</article-title><source>Frontiers in Computational Neuroscience</source><volume>8</volume><elocation-id>66</elocation-id><pub-id pub-id-type="doi">10.3389/fncom.2014.00066</pub-id><pub-id pub-id-type="pmid">25071537</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.88805.3.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Bhalla</surname><given-names>Upinder S</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>National Centre for Biological Sciences</institution><country>India</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Convincing</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>The authors provide a <bold>valuable</bold> analysis of what neural circuit mechanisms enable varying the speed of retrieval of sequences, which is needed in situations such as reproducing motor patterns. Their use of heterogeneous plasticity rules to allow external currents to control speed of sequence recall is a novel alternative to other mechanisms proposed in the literature. They perform a <bold>convincing</bold> characterization of relevant properties of recall via simulations and theory, though a better mapping to biologically plausible mechanisms is left for future work.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.88805.3.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>While there are many models for sequence retrieval, it has been difficult to find models that vary the speed of sequence retrieval dynamically via simple external inputs. While recent works have proposed some mechanisms, the authors here propose a different one based on heterogeneous plasticity rules. Temporally symmetric plasticity kernels (that do not distinguish between the order of pre and post spikes, but only their time difference) are expected to give rise to attractor states, asymmetric ones to sequence transitions. The authors incorporate a rate-based, discrete-time analog of these spike-based plasticity rules to learn the connections between neurons (leading to connections similar to Hopfield networks for attractors and sequences). They use either a parametric combination of symmetric and asymmetric learning rules for connections into each neuron, or separate subpopulations having only symmetric or asymmetric learning rules on incoming connections. They find that the latter is conducive to enabling external inputs to control the speed of sequence retrieval.</p><p>Comments on revised version:</p><p>The authors have addressed most of the points of the reviewers.</p><p>A major substantive point raised by both reviewers was on the biological plausibility of the learning.</p><p>The authors have added a section in the Discussion. This remains an open question, however the discussion suffices for the current paper.</p></body></sub-article><sub-article article-type="author-comment" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.88805.3.sa2</article-id><title-group><article-title>Author response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Gillett</surname><given-names>Maxwell</given-names></name><role specific-use="author">Author</role><aff><institution>Duke University</institution><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Brunel</surname><given-names>Nicolas</given-names></name><role specific-use="author">Author</role><aff><institution>Duke University</institution><addr-line><named-content content-type="city">Durham</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>Author response:</p><p>The following is the authors’ response to the original reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Public Review):</bold></p><p>While there are many models for sequence retrieval, it has been difficult to find models that vary the speed of sequence retrieval dynamically via simple external inputs. While recent works [1,2] have proposed some mechanisms, the authors here propose a different one based on heterogeneous plasticity rules. Temporally symmetric plasticity kernels (that do not distinguish between the order of pre and post spikes, but only their time difference) are expected to give rise to attractor states, asymmetric ones to sequence transitions. The authors incorporate a rate-based, discrete-time analog of these spike-based plasticity rules to learn the connections between neurons (leading to connections similar to Hopfield networks for attractors and sequences). They use either a parametric combination of symmetric and asymmetric learning rules for connections into each neuron, or separate subpopulations having only symmetric or asymmetric learning rules on incoming connections. They find that the latter is conducive to enabling external inputs to control the speed of sequence retrieval.</p><p>Strengths:</p><p>The authors have expertly characterised the system dynamics using both simulations and theory. How the speed and quality of retrieval varies across phases space has been well-studied. The authors are also able to vary the external inputs to reproduce a preparatory followed by an execution phase of sequence retrieval as seen experimentally in motor control. They also propose a simple reinforcement learning scheme for learning to map the two external inputs to the desired retrieval speed.</p><p>Weaknesses:</p><p>(1) The authors translate spike-based synaptic plasticity rules to a way to learn/set connections for rate units operating in discrete time, similar to their earlier work in [5]. The bio-plausibility issues of learning in [5] carry over here, for e.g. the authors ignore any input due to the recurrent connectivity during learning and effectively fix the pre and post rates to the desired ones. While the learning itself is not fully bio-plausible, it does lend itself to writing the final connectivity matrix in a manner that is easier to analyze theoretically.</p></disp-quote><p>We agree with the reviewer that learning is not `fully bio-plausible’. However, we believe that extending the results to a model in which synaptic plasticity depends on recurrent inputs is beyond the scope of this work. We have added a mention of this issue in the Discussion in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>(2) While the authors learn to map the set of two external input strengths to speed of retrieval, they still hand-wire one external input to the subpopulation of neurons with temporally symmetric plasticity and the other external input to the other subpopulation with temporally asymmetric plasticity. The authors suggest that these subpopulations might arise due to differences in the parameters of Ca dynamics as in their earlier work [29]. How these two external inputs would connect to neurons differentially based on the plasticity kernel / Ca dynamics parameters of the recurrent connections is still an open question which the authors have not touched upon.</p></disp-quote><p>The issue of how external inputs could self-organize to drive the network to retrieve sequences at appropriate speeds is addressed in the Results section, paragraph `Reward-driven learning’. These inputs are not `hand-wired’ - they are initially random and then acquire the necessary strengths to allow the network to retrieve the sequences at different speeds thanks to a simple reinforcement learning scheme. We have rewritten this section to clarify this issue.</p><disp-quote content-type="editor-comment"><p>(3) The authors require that temporally symmetric and asymmetric learning rules be present in the recurrent connections between subpopulations of neurons in the same brain region, i.e. some neurons in the same brain region should have temporally symmetric kernels, while others should have temporally asymmetric ones. The evidence for this seems thin. Though, in the discussion, the authors clarify 'While this heterogeneity has been found so far across structures or across different regions in the same structure, this heterogeneity could also be present within local networks, as current experimental methods for probing plasticity only have access to a single delay between pre and post-synaptic spikes in each recorded neuron, and would therefore miss this heterogeneity'.</p></disp-quote><p>We agree with the reviewer that this is currently an open question. We describe this issue in more detail in the Discussion of the revised manuscript.</p><disp-quote content-type="editor-comment"><p>(4) An aspect which the authors have not connected to is one of the author's earlier work:</p><p>Brunel, N. (2016). Is cortical connectivity optimized for storing information? Nature Neuroscience, 19(5), 749-755 <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1038/nn.4286">https://doi.org/10.1038/nn.4286</ext-link> which suggests that the experimentally observed over-representation of symmetric synapses suggests that cortical networks are optimized for attractors rather github</p><p>han sequences.</p></disp-quote><p>We thank the reviewer for this suggestion. We have added a paragraph in the discussion that discusses work on statistics of synaptic connectivity in optimal networks. We expect that in networks that contain two subpopulations of neurons, the degree of symmetry should be intermediate between a network storing fixed point attractors exclusively, and a network storing sequences exclusively.</p><disp-quote content-type="editor-comment"><p>Despite the above weaknesses, the work is a solid advance in proposing an alternate model for modulating speed of sequence retrieval and extends the use of well-established theoretical tools. This work is expected to spawn further works like extending to a spiking neural network with Dale's law, more realistic learning taking into account recurrent connections during learning, and experimental follow-ups. Thus, I expect this to be an important contribution to the field.</p></disp-quote><p>We thank the reviewer for the insightful comments.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Public Review):</bold></p><p>Sequences of neural activity underlie most of our behavior. And as experience suggests we are (in most cases) able to flexibly change the speed for our learned behavior which essentially means that brains are able to change the speed at which the sequence is retrieved from the memory. The authors here propose a mechanism by which networks in the brain can learn a sequence of spike patterns and retrieve them at variable speed. At a conceptual level I think the authors have a very nice idea: use of symmetric and asymmetric learning rules to learn the sequences and then use different inputs to neurons with symmetric or asymmetric plasticity to control the retrieval speed. The authors have demonstrated the feasibility of the idea in a rather idealized network model. I think it is important that the idea is demonstrated in more biologically plausible settings (e.g. spiking neurons, a network with exc. and inh. neurons with ongoing activity).</p><p>Summary</p><p>In this manuscript authors have addressed the problem of learning and retrieval sequential activity in neuronal networks. In particular, they have focussed on the problem of how sequence retrieval speed can be controlled?</p><p>They have considered a model with excitatory rate-based neurons. Authors show that when sequences are learned with both temporally symmetric and asymmetric Hebbian plasticity, by modulating the external inputs to the network the sequence retrieval speed can be modulated. With the two types of Hebbian plasticity in the network, sequence learning essentially means that the network has both feedforward and recurrent connections related to the sequence. By giving different amounts of input to the feed-forward and recurrent components of the sequence, authors are able to adjust the speed.</p><p>Strengths</p><p>- Authors solve the problem of sequence retrieval speed control by learning the sequence in both feedforward and recurrent connectivity within a network. It is a very interesting idea for two main reasons: 1. It does not rely on delays or short-term dynamics in neurons/synapses 2. It does not require that the animal is presented with the same sequences multiple times at different speeds. Different inputs to the feedforward and recurrent populations are sufficient to alter the speed. However, the work leaves several issues unaddressed as explained below.</p><p>Weaknesses</p><p>- The main weakness of the paper is that it is mostly driven by a motivation to find a computational solution to the problem of sequence retrieval speed. In most cases they have not provided any arguments about the biological plausibility of the solution they have proposed e.g.:</p><p>- Is there any experimental evidence that some neurons in the network have symmetric Hebbian plasticity and some temporally asymmetric? In the references authors have cited some references to support this. But usually the switch between temporally symmetric and asymmetric rules is dependent on spike patterns used for pairing (e.g. bursts vs single spikes). In the context of this manuscript, it would mean that in the same pattern, some neurons burst and some don't and this is the same for all the patterns in the sequence. As far as I see here authors have assumed a binary pattern of activity which is the same for all neurons that participate in the pattern.</p></disp-quote><p>There is currently only weak evidence for heterogeneity of synaptic plasticity rules within a single network, though there is plenty of evidence for such a heterogeneity across networks or across locations within a particular structure (see references in our Discussion). The reviewer suggests another interesting possibility, that the temporal asymmetry could depend on the firing pattern on the post-synaptic neuron. An example of such a behavior can be found in a paper by Wittenberg and Wang in 2006, where they show that pairing single spikes of pre and post-synaptic neurons lead to LTD at all time differences in a symmetric fashion, while pairing a pre-synaptic spike with a burst of post-synaptic spikes lead to temporally asymmetric plasticity, with a LTP window at short positive time differences. We now mention this possibility in the Discussion, but we believe exploring fully this scenario is beyond the scope of the paper.</p><disp-quote content-type="editor-comment"><p>- How would external inputs know that they are impinging on a symmetric or asymmetric neuron? Authors have proposed a mechanism to learn these inputs. But that makes the sequence learning problem a two stage problem -- first an animal has to learn the sequence and then it has to learn to modulate the speed of retrieval. It should be possible to find experimental evidence to support this?</p></disp-quote><p>Our model does not assume that the two processes necessarily occur one after the other. Importantly, once the correct external inputs that can modulate sequence retrieval are learned, sequence retrieval modulation will automatically generalize to arbitrary new sequences that are learned by the network.</p><disp-quote content-type="editor-comment"><p>- Authors have only considered homogeneous DC input for sequence retrieval. This kind of input is highly unnatural. It would be more plausible if the authors considered fluctuating input which is different from each neuron.</p></disp-quote><p>We have modified Figure 1e and Figure 2c to show the effects of fluctuating inputs on pattern correlations and single unit activity. We find that these inputs do not qualitatively affect our results.</p><disp-quote content-type="editor-comment"><p>- All the work is demonstrated using a firing rate based model of only excitatory neurons. I think it is important that some of the key results are demonstrated in a network of both excitatory and inhibitory spiking neurons. As the authors very well know it is not always trivial to extend rate-based models to spiking neurons.</p><p>I think at a conceptual level authors have a very nice idea but it needs to be demonstrated in a more biologically plausible setting (and by that I do not mean biophysical neurons etc.).</p></disp-quote><p>We have included a new section in the discussion with an associated figure (Figure 7) demonstrating that flexible speed control can be achieved in an excitatory-inhibitory (E-I) spiking network containing two excitatory populations with distinct plasticity mechanisms.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1 (Recommendations For The Authors):</bold></p><p>In the introduction, the authors state: 'symmetric kernels, in which coincident activity leads to strengthening regardless of the order of pre and post-synaptic spikes, have also been observed in multiple contexts with high frequency plasticity induction protocols in cortex [21]'. To my understanding, [21]'s final model 3, ignores LTD if the post-spike also participates in LTP, and only considers nearest-neighbour interactions. Thus, the kernel would not be symmetric. Can the authors clarify what they mean and how their conclusion follows, as [21] does not show any kernels either.</p></disp-quote><p>In this statement, we were not referring to the model in [21], but rather the experimentally observed plasticity kernels at different frequencies. In particular, we were referring to the symmetric kernel that appears in the bottom panel of Figure 7c in that paper.</p><disp-quote content-type="editor-comment"><p>The authors should also address the weaknesses mentioned above. They don't need to solve the issues but expand (and maybe indicate resolutions) on these issues in the Discussion.</p><p>For ease of reproducibility, the authors should make their code available as well.</p></disp-quote><p>We intend to publish the code required to reproduce all figures on Github.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #2 (Recommendations For The Authors):</bold></p><p>- Show the ground state of the network before and after learning.</p></disp-quote><p>We have decided not to include such a figure, as we have not analyzed the learning process, but instead a network with a fixed connectivity matrix which is assumed to be the end result of a learning process.</p><disp-quote content-type="editor-comment"><p>- Authors have only considered a network of excitatory neurons. This does not make sense. I think they should demonstrate a network of both exc. and inch. neurons (spiking neurons) exhibiting ongoing activity.</p></disp-quote><p>See our comment to Reviewer #2 in the previous section.</p><disp-quote content-type="editor-comment"><p>- Show how the sequence dynamics unfolds when we assume a non-zero ongoing activity.</p></disp-quote><p>We are not sure what the reviewer means by `non-zero ongoing activity. We show now the dynamics of the network in the presence of noisy inputs, which can represent ongoing activity from other structures (see Fig 1e and 2c).</p><disp-quote content-type="editor-comment"><p>- From the correlation (==quality) alone it is difficult to judge how well the sequence has been recovered. Authors should consider showing some examples so that the reader can get a visual estimate of what 0.6 quality may mean. High speed is not really associated with high quality (Fig 2b). So it is important to show how the sequence retrieval quality is for non-linear and heterogeneous learning rules.</p></disp-quote><p>We believe that some insight into the relationship between speed and quality for the case of non-linear and heterogeneous learning rules is addressed by the correlation plots for chosen input configurations (see Fig. 3a and 5b). We leave a full characterization for future work.</p><disp-quote content-type="editor-comment"><p>- Authors should show how the retrieval and quality of sequences change when they are recovered with positive input, or positive input to one population and negative to another. In the current version sequence retrieval is shown only with negative inputs. This is a somewhat non-biological setting. The inhibitory gating argument (L367-389) is really weak.</p></disp-quote><p>We would like to clarify that with the parameters chosen in this paper, the transfer function has half its maximal rate at zero input. This is due to the fact we chose the threshold to be zero, using the fact that any threshold can be absorbed in the external inputs. Thus, negative inputs really mean sub-threshold inputs, and they are consistent with sub-threshold external excitatory inputs. We have clarified this issue in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>- Authors should demonstrate how the sequence retrieval dynamics is altered when they assume a fluctuating input current for sequence retrieval instead of a homogeneous DC input.</p></disp-quote><p>See our comment to Reviewer #2 in the previous section.</p><disp-quote content-type="editor-comment"><p>- Authors should show what are the differences in synaptic weight distribution for the two types of learning (bi-linear and non-linear). I am curious to know if the difference in the speed in the two cases is related to the weight distribution. In general I think it is a good idea to show the synaptic weight distribution before and after learning.</p></disp-quote><p>As mentioned above, we do not study any learning process, but rather a network with a fixed connectivity matrix, assumed to represent the end result of learning. In this network, the distribution of synaptic weights converges to a Gaussian in the large p and cN limits, independently of the functions f and g, because of the central limit theorem, if there are no sign constraints on weights. In the presence of sign constraints, the distribution is a truncated Gaussian.</p><disp-quote content-type="editor-comment"><p>- I suggest the use of a monochromatic color scale for figure 2b and 3b.</p><p>Figure 3: The sentence describing panel 2 seems incomplete.</p><p>Also explain why there is non-monotonic relationship between I_s and speed for some values of</p><p>I_a in 3b</p></disp-quote><p>There is a non-monotonic relationship for retrieval quality, not speed. We have clarified this in the manuscript text, but don’t currently have an explanation for why this phenomenon occurs for these specific values of I_a.</p></body></sub-article></article>