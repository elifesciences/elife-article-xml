<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">68066</article-id><article-id pub-id-type="doi">10.7554/eLife.68066</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>An oscillating computational model can track pseudo-rhythmic speech by using linguistic predictions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-205180"><name><surname>ten Oever</surname><given-names>Sanne</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7547-5842</contrib-id><email>sanne.tenoever@mpi.nl</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-171158"><name><surname>Martin</surname><given-names>Andrea E</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3395-7234</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>Language and Computation in Neural Systems group, Max Planck Institute for Psycholinguistics</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff2"><label>2</label><institution>Donders Centre for Cognitive Neuroimaging, Radboud University</institution><addr-line><named-content content-type="city">Nijmegen</named-content></addr-line><country>Netherlands</country></aff><aff id="aff3"><label>3</label><institution>Department of Cognitive Neuroscience, Faculty of Psychology and Neuroscience, Maastricht University</institution><addr-line><named-content content-type="city">Maastricht</named-content></addr-line><country>Netherlands</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>King</surname><given-names>Andrew J</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="editor"><name><surname>Kösem</surname><given-names>Anne</given-names></name><role>Reviewing Editor</role><aff><institution>Max Planck Institute for Psycholinguistics; Donders Institute for Brain, Cognition and Behaviour, Radboud University; Lyon Neuroscience Research Center</institution><country>Netherlands</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>02</day><month>08</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e68066</elocation-id><history><date date-type="received" iso-8601-date="2021-03-03"><day>03</day><month>03</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2021-07-16"><day>16</day><month>07</month><year>2021</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2020-12-07"><day>07</day><month>12</month><year>2020</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2020.12.07.414425"/></event></pub-history><permissions><copyright-statement>© 2021, ten Oever and Martin</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>ten Oever and Martin</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-68066-v1.pdf"/><abstract><p>Neuronal oscillations putatively track speech in order to optimize sensory processing. However, it is unclear how isochronous brain oscillations can track pseudo-rhythmic speech input. Here we propose that oscillations can track pseudo-rhythmic speech when considering that speech time is dependent on content-based predictions flowing from internal language models. We show that temporal dynamics of speech are dependent on the predictability of words in a sentence. A computational model including oscillations, feedback, and inhibition is able to track pseudo-rhythmic speech input. As the model processes, it generates temporal phase codes, which are a candidate mechanism for carrying information forward in time. The model is optimally sensitive to the natural temporal speech dynamics and can explain empirical data on temporal speech illusions. Our results suggest that speech tracking does not have to rely only on the acoustics but could also exploit ongoing interactions between oscillations and constraints flowing from internal language models.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>speech</kwd><kwd>oscillations</kwd><kwd>language</kwd><kwd>temporal processing</kwd><kwd>prediction</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max Planck Society</institution></institution-wrap></funding-source><award-id>MaxPlanck Research Group</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Andrea E</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100003246</institution-id><institution>Nederlandse Organisatie voor Wetenschappelijk Onderzoek</institution></institution-wrap></funding-source><award-id>016.Vidi.188.029</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Andrea E</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100004189</institution-id><institution>Max Planck Society</institution></institution-wrap></funding-source><award-id>Lise Meitner Research Group</award-id><principal-award-recipient><name><surname>Martin</surname><given-names>Andrea E</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>An oscillating computational model combined with a predictive internal linguistic model can track naturally timed speech in which pseudo-rhythmicity is related to the predictability of words within their sentence context.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Speech is a biological signal that is characterized by a plethora of temporal information. The temporal relationship between subsequent speech units allows for the online tracking of speech in order to optimize processing at relevant moments in time (<xref ref-type="bibr" rid="bib38">Jones and Boltz, 1989</xref>; <xref ref-type="bibr" rid="bib47">Large and Jones, 1999</xref>; <xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib27">Ghitza and Greenberg, 2009</xref>; <xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib1">Arvaniti, 2009</xref>; <xref ref-type="bibr" rid="bib82">Poeppel, 2003</xref>). Neural oscillations are a putative index of such tracking (<xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib88">Schroeder and Lakatos, 2009</xref>). The existing evidence for neural tracking of the speech envelope is consistent with such a functional interpretation (<xref ref-type="bibr" rid="bib52">Luo et al., 2013</xref>; <xref ref-type="bibr" rid="bib43">Keitel et al., 2018</xref>). In these accounts, the most excitable optimal phase of an oscillation is aligned with the most informative time point within a rhythmic input stream (<xref ref-type="bibr" rid="bib88">Schroeder and Lakatos, 2009</xref>; <xref ref-type="bibr" rid="bib46">Lakatos et al., 2008</xref>; <xref ref-type="bibr" rid="bib33">Henry and Obleser, 2012</xref>; <xref ref-type="bibr" rid="bib34">Herrmann et al., 2013</xref>; <xref ref-type="bibr" rid="bib73">Obleser and Kayser, 2019</xref>). However, the range of onset time difference between speech units seems more variable than fixed oscillations can account for (<xref ref-type="bibr" rid="bib86">Rimmele et al., 2018</xref>; <xref ref-type="bibr" rid="bib70">Nolan and Jeon, 2014</xref>; <xref ref-type="bibr" rid="bib35">Jadoul et al., 2016</xref>). As such, it remains an open question how is it possible that oscillations can track a signal that is at best only pseudo-rhythmic (<xref ref-type="bibr" rid="bib70">Nolan and Jeon, 2014</xref>).</p><p>Oscillatory accounts tend to focus on the prediction in the sense of predicting ‘when’, rather than predicting ‘what’: oscillations function to align the optimal moment of processing given that timing is predictable in a rhythmic input structure. If rhythmicity in the input stream is violated, oscillations must be modulated to retain optimal alignment to incoming information. This can be achieved through phase resets (<xref ref-type="bibr" rid="bib86">Rimmele et al., 2018</xref>; <xref ref-type="bibr" rid="bib62">Meyer, 2018</xref>), direct coupling of the acoustics to oscillations (<xref ref-type="bibr" rid="bib83">Poeppel and Assaneo, 2020</xref>), or the use of many oscillators at different frequencies (<xref ref-type="bibr" rid="bib47">Large and Jones, 1999</xref>). However, the optimal or effective time of processing stimulus input might not only depend on when you predict something to occur, but also depend on what stimulus is actually being processed (<xref ref-type="bibr" rid="bib90">Ten Oever et al., 2013</xref>; <xref ref-type="bibr" rid="bib56">Martin, 2016</xref>; <xref ref-type="bibr" rid="bib87">Rosen, 1992</xref>; <xref ref-type="bibr" rid="bib15">deen et al., 2017</xref>).</p><p>What and when are not independent, and certainly not from the brain’s-eye-view. If continuous input arrives to a node in an oscillatory network, the exact phase at which this node reaches threshold activation does not only depend on the strength of the input, but also depend on how sensitive this node was to start with. Sensitivity of a node in a language network (or any neural network) is naturally affected by predictions in the what domain generated by an internal language model (<xref ref-type="bibr" rid="bib57">Martin, 2020</xref>; <xref ref-type="bibr" rid="bib55">Marslen-Wilson, 1987</xref>; <xref ref-type="bibr" rid="bib48">Lau et al., 2008</xref>; <xref ref-type="bibr" rid="bib69">Nieuwland, 2019</xref>). We define internal language model as the individually acquired statistical and structural knowledge of language stored in the brain. A virtue of such an internal language model is that it can predict the most likely future input based on the currently presented speech information. If a language model creates strong predictions, we call it a strong model. In contrast, a weak model creates no or little predictions about future input (note that the strength of individual predictions depends not only on the capability of the system to create a prediction, but also on the available information). If a node represents a speech unit that is likely to be spoken next, a strong internal language model will sensitize this node and it will therefore be active earlier, that is, on a less excitable phase of the oscillation. In the domain of working memory, this type of phase precession has been shown in rat hippocampus (<xref ref-type="bibr" rid="bib71">O'Keefe and Recce, 1993</xref>; <xref ref-type="bibr" rid="bib54">Malhotra et al., 2012</xref>) and more recently in human electroencephalography (<xref ref-type="bibr" rid="bib4">Bahramisharif et al., 2018</xref>). In speech, phase of activation and perceived content are also associated (<xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>; <xref ref-type="bibr" rid="bib42">Kayser et al., 2016</xref>; <xref ref-type="bibr" rid="bib16">Di Liberto et al., 2015</xref>; <xref ref-type="bibr" rid="bib91">Ten Oever et al., 2016</xref>; <xref ref-type="bibr" rid="bib95">Thézé et al., 2020</xref>), and phase has been implicated in tracking of higher-level linguistic structure (<xref ref-type="bibr" rid="bib62">Meyer, 2018</xref>; <xref ref-type="bibr" rid="bib10">Brennan and Martin, 2020</xref>; <xref ref-type="bibr" rid="bib39">Kaufeld et al., 2020a</xref>). However, the direct link between phase and the predictability flowing from a language model has yet to be established.</p><p>The time of speaking/speed of processing is not only a consequence of how predictable a speech unit is within a stream, but also a cue for the interpretation of this unit. For example, phoneme categorization depends on timing (e.g., voice onsets, difference between voiced and unvoiced phonemes), and there are timing constraints on syllable durations (e.g., the theta syllable <xref ref-type="bibr" rid="bib83">Poeppel and Assaneo, 2020</xref>; <xref ref-type="bibr" rid="bib26">Ghitza, 2013</xref>) that affect intelligibility (<xref ref-type="bibr" rid="bib25">Ghitza, 2012</xref>). Even the delay between mouth movements and speech audio can influence syllabic categorizations (<xref ref-type="bibr" rid="bib90">Ten Oever et al., 2013</xref>). Most oscillatory models use oscillations for parsing, but not as a temporal code for content (<xref ref-type="bibr" rid="bib75">Panzeri et al., 2015</xref>; <xref ref-type="bibr" rid="bib41">Kayser et al., 2009</xref>; <xref ref-type="bibr" rid="bib61">Mehta et al., 2002</xref>; <xref ref-type="bibr" rid="bib51">Lisman and Jensen, 2013</xref>). However, the time or phase of presentation does influence content perception. This is evident from two temporal speech phenomena. In the first phenomena, the interpretation of an ambiguous short /α/ or long vowel /a:/ depends on speech rate (in Dutch; <xref ref-type="bibr" rid="bib85">Reinisch and Sjerps, 2013</xref>; <xref ref-type="bibr" rid="bib45">Kösem et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Bosker and Reinisch, 2015</xref>). Specifically, when speech rates are fast the stimulus is interpreted as a long vowel and vice versa for slow rates. However, modulating the entrainment rate effectively changes the phase at which the target stimulus – which is presented at a constant speech rate – arrives (but this could not be confirmed in <xref ref-type="bibr" rid="bib8">Bosker and Kösem, 2017</xref>). A second speech phenomena shows the direct phase-dependency of content (<xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>; <xref ref-type="bibr" rid="bib91">Ten Oever et al., 2016</xref>). Ambiguous /da/-/ga/ stimuli will be interpreted as a /da/ on one phase and a /ga/ on another phase. This was confirmed in both a EEG and a behavioral study. An oscillatory theory of speech tracking should account for how temporal properties in the input stream can alter what is perceived.</p><p>In the speech production literature, there is strong evidence that the onset times (as well as duration) of an uttered word is modulated by the frequency of that word in the language (<xref ref-type="bibr" rid="bib72">O'Malley and Besner, 2008</xref>; <xref ref-type="bibr" rid="bib67">Monsell, 1991</xref>; <xref ref-type="bibr" rid="bib66">Monsell et al., 1989</xref>; <xref ref-type="bibr" rid="bib84">Powers, 1998</xref>; <xref ref-type="bibr" rid="bib79">Piantadosi, 2014</xref>) showing that internal language models modulate the access to or sensitivity of a word node (<xref ref-type="bibr" rid="bib57">Martin, 2020</xref>; <xref ref-type="bibr" rid="bib31">Hagoort, 2017</xref>). This word-frequency effect relates to the access to a single word. However, it is likely that during ongoing speech internal language models use the full context to estimate upcoming words (<xref ref-type="bibr" rid="bib6">Beattie and Butterworth, 1979</xref>; <xref ref-type="bibr" rid="bib80">Pluymaekers et al., 2005a</xref>; <xref ref-type="bibr" rid="bib49">Lehiste, 1972</xref>). If so, the predictability of a word in context should provide additional modulations on speech time. Therefore, we predict that words with a high predictability in the producer’s language model should be uttered relatively early. In this way, word-to-word onset times map to the predictability level of that word within the internal model. Thus, not only the processing time depends on the predictability of a word (faster processing for predictable words; see <xref ref-type="bibr" rid="bib30">Gwilliams et al., 2020</xref>; <xref ref-type="bibr" rid="bib14">Deacon et al., 1995</xref>, and <xref ref-type="bibr" rid="bib3">Aubanel and Schwartz, 2020</xref> showing that speech time in noise matters), but also the production time (earlier uttering of predicted words).</p><p>Language comprehension involves the mapping of speech units from a producer’s internal model to the speech units of the receiver’s internal model. In other words, one will only understand what someone else is writing or saying if one’s language model is sufficiently similar to the speakers (and if we speak in Dutch, fewer people will understand us). If the producer’s and receiver’s internal language model have roughly matching top-down constrains, they should similarly influence the speed of processing (either in production or perception; <xref ref-type="fig" rid="fig1">Figure 1A–C</xref>). Therefore, if predictable words arrive earlier (due to high predictability in the producer’s internal model), the receiver also expects the content of this word to match one of the more predictable ones from their own internal model (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Thus, the phase of arrival depends on the internal model of the producer and the expected phase of arrival depends on the internal model of the receiver (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). If this is true, pseudo-rhythmicity is fully natural to the brain, and it provides a means to use time or arrival phase as a content indicator. It also allows the receiver to be sensitive to less predictable words when they arrive relatively late. Current oscillatory models of speech parsing do not integrate the constraints flowing from an internal linguistic model into the temporal structure of the brain response. It is therefore an open question whether the oscillatory model the brain employs is actually attuned to the temporal variations in natural speech.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Proposed interaction between speech timing and internal linguistic models.</title><p>(<bold>A</bold>) Isochronous production and expectation when there is a weak internal model (even distribution of node activation). All speech units arrive around the most excitable phase. (<bold>B</bold>) When the internal model of the producer does not align with the model of the receiver temporal alignment and optimal communication fails. (<bold>C</bold>) When both producer and receiver have a strong internal model, speech is non-isochronous and not aligned to the most excitable phase, but fully expected by the brain. (<bold>D</bold>) Expected time is a constraint distribution in which the center can be shifted due to linguistic constraints.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig1-v1.tif"/></fig><p>Here, we propose that neural oscillations can track pseudo-rhythmic speech by taking into account that speech timing is a function of linguistic constrains. As such we need to demonstrate that speech statistics are influenced by linguistic constrains as well as showing how oscillations can be sensitive to this property in speech. We approach this hypothesis as follows: First, we demonstrate that in natural speech timing depends on linguistics predictions (<italic>temporal speech properties</italic>). Then, we model how oscillations can be sensitive to these linguistic predictions (<italic>modeling speech tracking</italic>). Finally, we validate that this model is optimally sensitive to the natural temporal properties in speech and displays temporal speech illusions (<italic>model validation</italic>). Our results reveal that tracking of speech needs to be viewed as an interaction between ongoing oscillations as well as constraints flowing from an internal language model (<xref ref-type="bibr" rid="bib56">Martin, 2016</xref>; <xref ref-type="bibr" rid="bib57">Martin, 2020</xref>). In this way, oscillations do not have to shift their phase after every speech unit and can remain at a relatively stable frequency as long as the internal model of the speaker matches the internal model of the perceiver.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Temporal speech properties</title><sec id="s2-1-1"><title>Word frequency influences word duration</title><p>To extract the temporal properties in naturally spoken speech we used the Corpus Gesproken Nederlands (CGN; [Version 2.0.3; 2014]). This corpus consists of elaborated annotations of over 900 hr of spoken Dutch and Flemish words. We focus here on the subset of the data of which onset and offset timings were manually annotated at the word level in Dutch. Cleaning of the data included removing all dashes and backslashes. Only words were included that were part of a Dutch word2vec embedding (<ext-link ext-link-type="uri" xlink:href="https://github.com/coosto/dutch-word-embeddings">github.com/coosto/dutch-word-embeddings</ext-link>; <xref ref-type="bibr" rid="bib68">Nieuwenhuijse, 2018</xref>; needed for later modeling) and required to have a frequency of at least 10 in the corpus. All other words were replaced with an &lt;unknown&gt; label. This resulted in 574,726 annotated words with 3096 unique words. Two thousand and forty-eight of the words were recognized in the Dutch Wordforms database in CELEX (Version 3.1) in order to extract the word frequency as well as the number of syllables per word (later needed to fit a regression model). Mean word duration was 0.392 s, with an average standard deviation of 0.094 s (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). By splitting up the data in sequences of 10 sequential words, we could extract the average word, syllable, and character rate (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplements 2</xref> and <xref ref-type="fig" rid="fig2s3">3</xref>). The reported rates fall within the generally reported ranges for syllables (5.2 Hz) and words (3.7 Hz; <xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib78">Pellegrino and Coupé, 2011</xref>).</p><p>We predict that knowledge about the language statistics influences the duration of speech units. As such we predict that more prevalent words will have on average a shorter duration (also reported in <xref ref-type="bibr" rid="bib66">Monsell et al., 1989</xref>). In <xref ref-type="fig" rid="fig2">Figure 2A</xref>, the duration of several mono- and bi-syllabic words are listed with their word frequency. From these examples, it seems that words with higher word frequency generally have a shorter duration. To test this statistically we entered word frequency in an ordinary least square regression with number of syllables as control. Both number of syllables (coefficient = 0.1008, t(2843) = 75.47, p&lt;0.001) as well as word frequency (coefficient = −0.022, t(2843) = −13.94, p&lt;0.001) significantly influence the duration of the word. Adding an interaction term did not significantly improve the model (F (1,2843) = 1.320, p=0.251; <xref ref-type="fig" rid="fig2">Figure 2B,C</xref>). The effect is so strong that words with a low frequency can last three times as long as high-frequency words (even within mono-syllabic words). This indicates that word frequency could be an important part of an internal model that influences word duration.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Word frequency modulates word duration.</title><p>(<bold>A</bold>) Example of mono- and bi-syllabic words of different word frequencies in brackets (van=from, zijn=be, snel=fast, stem=voice, hebben=have, eten=eating, volgend=next, toekomst=future). Text in the graph indicates the mean word duration. (<bold>B</bold>) Relationship between word frequency and duration. Darker colors mean more values. (<bold>C</bold>) same as (<bold>B</bold>) but separately for mono- and bi-syllabic words. (<bold>D</bold>) Relationship character amount and word duration. The longer the words, the longer the duration (left). The increase in word duration does not follow a fixed number per character as duration as measured by rate increases. (<bold>E</bold>) same as (<bold>D</bold>) but for number of syllables. Red dots indicate the mean.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Distribution of mean duration (<bold>A</bold>) and of average rate (<bold>B</bold>).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Distribution of mean duration split up for word length (in characters).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Distribution of mean duration split up for syllable length.</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig2-figsupp3-v1.tif"/></fig></fig-group><p>The previous analysis probed us to expand on the relationship between word duration and length of the words. Obviously, there is a strong correlation between word length and mean word duration (number of characters 0.824, p&lt;0.001; number of syllables: ρ = 0.808, p&lt;0.001; for number of syllables already shown above; <xref ref-type="fig" rid="fig2">Figure 2D,E</xref>). In contrast, this correlation is present, but much lower for the standard deviation of word duration (number of characters: ρ = 0.269, p&lt;0.001; number of syllables: ρ = 0.292, p&lt;0.001). Finding a strong correlation does not imply that for every time unit increase in the word length, the duration of the word also increases with the same time unit, i.e., bi-syllabic words do not necessarily have to last twice as long as mono-syllabic words. Therefore, we recalculated word duration to a rate unit considering the number of syllables/characters of the word. Thus, a 250 ms mono- versus bi-syllabic word would have a rate of 4 versus 8 Hz, respectively. Then we correlated character/syllabic rate with word duration. If word duration increases monotonically with character/syllable length, there should be no correlation. We found that the syllabic rate varies between 3 and 8 Hz as previously reported (<xref ref-type="fig" rid="fig2">Figure 2E</xref>, right; <xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib78">Pellegrino and Coupé, 2011</xref>). However, the more syllables there are in a word, the higher this rate (ρ = 0.676, p&lt;0.001). This increase was less strong for the character rate (ρ = 0.499, p&lt;0.001; <xref ref-type="fig" rid="fig2">Figure 2D</xref>, right).</p><p>These results show that the syllabic/character rate depends on the number of characters /syllables within a word and is not an independent temporal unit (<xref ref-type="bibr" rid="bib26">Ghitza, 2013</xref>). This effect is easy to explain when assuming that the prediction strength of an internal model influences word duration: transitional probabilities of syllables are simply more constrained within a word than across words (<xref ref-type="bibr" rid="bib96">Thompson and Newport, 2007</xref>). This will reduce the time it takes to utter/perceive any syllable which is later in a word. In the current model, we focus on words (based on the availability of word2vec embedding used to calculate contextual predictabilities based on a RNN) instead of syllables, so we will not test this prediction for syllables, but instead we can investigate the effect of transitional probabilities and other statistical regularities flowing from internal models across words (see next section and [<xref ref-type="bibr" rid="bib35">Jadoul et al., 2016</xref>] for statistical regularities in syllabic processing).</p></sec><sec id="s2-1-2"><title>Word-by-word predictability predicts word onset differences</title><p>The brain’s internal model likely provides predictions about what linguistic features and representations, and possibly about which specific units, such as words, to expect next when listening to ongoing speech (<xref ref-type="bibr" rid="bib56">Martin, 2016</xref>; <xref ref-type="bibr" rid="bib57">Martin, 2020</xref>). As such, it is also expected that word-by-word onset delays are shorter for words that fit the internal model (i.e., those that are expected; <xref ref-type="bibr" rid="bib6">Beattie and Butterworth, 1979</xref>). To investigate this possibility, we created a simplified version of an internal model predicting the next word using recurrent neural nets (RNN). We trained an RNN to predict the next word from ongoing sentences (<xref ref-type="fig" rid="fig3">Figure 3A</xref>). The model consisted of an embedding layer (pretrained; <ext-link ext-link-type="uri" xlink:href="https://github.com/coosto/dutch-word-embeddings">github.com/coosto/dutch-word-embeddings</ext-link>), a recurrent layer with a tanh activation function, and a dense output layer with a softmax activation. To prevent overfitting, we added a 0.2 dropout to the recurrent layers and the output layer. An Adam optimizer was used at a 0.001 learning rate and a batch size of 32. We investigated four different recurrent layers (GRU and LSTM at either 128 or 300 units; see <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The final model we use here includes a LSTM with 300 units. Input data consistent of 10 sequential words (label encoding) within the corpus (of a single speaker; shifting the sentences by one word at a time), and an output consisted of a single word. A maximum of four unknown labeled words (words not included in the word2vec estimations. Four was chosen as it was &lt;50% of the words) was allowed in the input, but not in output. Validation consisted of a randomly chosen 2% of the data.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>RNN output influence word onset differences.</title><p>(<bold>A</bold>) Sequences of 10 words were entered in an RNN in order to predict the content of the next word. Three examples are provided of input data with the label (bold word) and probability output for three different words. The regression model showed a relation between the duration of last word in the sequence and the predictability of the next word such that words were systematically shorter when the next word was more predictable according to the RNN output (illustrated here with the shorted black boxes). (<bold>B</bold>) Regression line estimated at mean value of word duration and bigram. (<bold>C</bold>) Scatterplot of prediction and onset difference of data within ± 0.5 standard deviation of word duration and bigram. Note that for (<bold>B</bold>) and (<bold>C</bold>), the axes are linear on the transformed values. (<bold>D</bold>) Regression line for the correlation between logarithm of variance of the prediction and theta power. (<bold>E</bold>) None-transformed distribution of variance of the predictions (within a sentence). Translation of the sentences in (<bold>A</bold>) from top to bottom: ‘... that it has for me and while you have no answer [on]’, ‘... the only real hope for us humans is a firm and [sure]’, ‘... a couple of glass doors in front and then it would not have been [in]’.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Recurrent neural network evaluation.</title><p>Probability is defined as the mean of the model output value at the node representing the next word.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>RNN prediction distributions.</title><p>RNN prediction dependent on the current word (left), previous word (middle), or sentence position (right). Words with generally high average RNN prediction were for current words ‘je’,’te’,’ik’,’de’,’van’ (‘you’,’to’,’I’,’the’,’from’) and previous words ‘dan’,’met’,’ook’,’voor’,’op’ (‘than’,’with’,’also’,’for’,’on’). The later the position the stronger the RNN prediction on average.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig3-figsupp2-v1.tif"/></fig></fig-group><p>The output of the RNN reflects a probability distribution in which the values of the RNN sum up to one and each word has its own predicted value (<xref ref-type="fig" rid="fig3">Figure 3A</xref>; see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for differences across words and sentence position). As such we can extract the predicted value of the uttered word and relate the RNN prediction with the stimulus onset delay relative to the previous word. We entered word prediction in a regression using the stimulus onset difference between the current word in the sentence and the previous word (i.e., onset difference of words). We added the control variables bigram (using the NLTK toolbox based on the training data only), frequency of previous word, syllable rate (rate of the full sentence input), and mean duration of previous word (all variables that can account for part of the variance that affects the duration of the last word). We only used the test data (total of 7361 sentences, excluding all words in which the previous word (W-1) was not present in Celex. 4837 sentences). Many of the variables were skewed to the right; therefore, we transformed the data accordingly (see <xref ref-type="table" rid="table1">Table 1</xref>; results were robust to changes in these transformation).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Summary of regression model for logarithm of onset difference of words.</title></caption><table frame="hsides" rules="groups"><thead><tr><th>Variable</th><th>Trans</th><th>B</th><th>β</th><th>SE</th><th>t</th><th>p</th><th>VIF</th></tr></thead><tbody><tr><td>Intercept</td><td>x</td><td>0.9719</td><td/><td>0.049</td><td>19.764</td><td>&lt;0.001</td><td/></tr><tr><td>RNN prediction</td><td>x <sup>(1/6)</sup></td><td>−0.3370</td><td>−0.0862</td><td>0.047</td><td>−7.163</td><td>&lt;0.001</td><td>1.5</td></tr><tr><td>Bigram</td><td>log(x)</td><td>−0.0118</td><td>−0.0316</td><td>0.005</td><td>−2.424</td><td>0.015</td><td>1.8</td></tr><tr><td>Word frequency W-1</td><td>x</td><td>0.0049</td><td>0.0076</td><td>0.009</td><td>0.546</td><td>0.585</td><td>2.0</td></tr><tr><td>Mean duration W-1</td><td>log(x)</td><td>1.1206</td><td>0.7003</td><td>0.022</td><td>50.326</td><td>&lt;0.001</td><td>2.0</td></tr><tr><td>Syllable Rate</td><td>x</td><td>−0.1033</td><td>−0.2245</td><td>0.004</td><td>−23.014</td><td>&lt;0.001</td><td>1.0</td></tr></tbody></table><table-wrap-foot><fn><p>Model R<sup>2</sup> = 0.542. Trans = transformation, W-1 = previous word, B = unstandardized coefficient, β = standardized coefficient, SE = standard error, t = t value, p = p value, VIF = variance inflation factor.</p></fn></table-wrap-foot></table-wrap><p>All predictors except word frequency of the previous word showed a significant effect (<xref ref-type="table" rid="table1">Table 1</xref>). The variance explained by word frequency was likely captured by the mean duration variable of the previous word, which is correlated to word frequency. The RNN predictor could capture more variance than the bigram model, suggesting that word duration is modulated by the level of predictability within a fuller context than just the conditional probability of the current word given the previous word (<xref ref-type="fig" rid="fig3">Figure 3B,C</xref>). Importantly, it was necessary to use the trained RNN model as a predictor; entering the RNN predictions after the first training cycle (of a total of 100) did not results in a significant predictor (t(4837) = −1.191, p=0.234). Also adding the predictor word frequency of the current word did not add significant information to the model (F(1, 4830) = 0.2048, p=0.651). These results suggest that words are systematically lengthened (or pauses are added. However, the same predictors are also significant when excluding sentences containing pauses) when the next word is not strongly predicted by the internal model. We also investigate whether RNN predictions have an influence on the duration of the word that has to be uttered. We found no effect on the duration (Supporting <xref ref-type="table" rid="table1">Table 1</xref>).</p></sec><sec id="s2-1-3"><title>Sentence isochrony depends on prediction variance</title><p>In the previous section, we investigated word-to-word onsets, but did not investigate how this influences the temporal properties within a full sentence. In a regular sentence, predictability values change from word-to-word. Based on the previous results, it is expected that overall sentences with a more stable predictability level (sequential words are equally predictable) should be more isochronous than sentences in which the predictability shifts from high to low. This prediction is based on the observation that when predictions are equal the expected shift is the same, while for varying predictions, temporal shifts vary (<xref ref-type="fig" rid="fig3">Figure 3B,C</xref>).</p><p>To test this hypothesis, we extracted the RNN prediction for 10 subsequent words. Then we extracted the variance of the prediction across those 10 words and extracted the word onset itself. We created a time course at which word onset were set to 1 (at a sampling rate of 100 Hz). Then we performed an fast Fourier transform (FFT) and extracted z-transformed power values over a 0–15 Hz interval. The power at the maximum power value with the theta range (3–8 Hz) was extracted. These max z-scores were correlated with the log transform of the variance (to normalize the skewed variance distribution; <xref ref-type="fig" rid="fig3">Figure 3E</xref>). We found a weak, but significant negative correlation (r = −0.062, p&lt;0.001; <xref ref-type="fig" rid="fig3">Figure 3D</xref>) in line with our hypothesis. This suggests that the more variable the predictions within a sentence, the lower the peak power value is. When we repeated the analysis on the envelope, we did not find a significant effect.</p></sec></sec></sec><sec id="s3" sec-type="materials|methods"><title>Materials and methods</title><sec id="s3-1"><title>Speech Tracking in a Model Constrained Oscillatory Network</title><p>In order to investigate how much of these duration effects can be explained using an oscillator model, we created the model Speech Tracking in a Model Constrained Oscillatory Network (STiMCON). STiMCON in its current form will not be exhaustive; however, it can extract how much an oscillating network can cope with asynchronies by using its own internal model illustrating how the brain’s language model and speech timing interact (<xref ref-type="bibr" rid="bib29">Guest and Martin, 2021</xref>). The current model is capable of explaining how top-down predictions can influence the processing time as well as provide an explanation for two known temporal illusions in speech.</p><p>STiMCON consists of a network of semantic nodes of which the activation A of each level in the model l is governed by:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>→</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>→</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mi>T</mml:mi><mml:mi>a</mml:mi></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula>in which C represents the connectivity patterns between different hierarchical levels, T the time in a sentence, and Ta the vector of times of an individual node in an inhibition function (in milliseconds). The inhibition function is a gate function:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>T</mml:mi><mml:mi>a</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>3</mml:mn><mml:mo>∗</mml:mo><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mtext> </mml:mtext><mml:mo>&lt;</mml:mo><mml:mi>l</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>3</mml:mn><mml:mo>∗</mml:mo><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mn>20</mml:mn><mml:mo>≤</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>100</mml:mn></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>B</mml:mi><mml:mi>a</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>I</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi><mml:mo>,</mml:mo><mml:mi>T</mml:mi><mml:mi>a</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>100</mml:mn></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>in which BaseInhib is a constant for the base level of inhibition (negative value, set to −0.2). As such nodes are by default inhibited, as soon as they get activated above threshold (activation threshold set at 1) Ta sets to zero. Then, the node will have suprathreshold activation, which after 20 ms returns to increased inhibition until the base level of inhibition is returned. These values are set to reflect early excitation and longer lasting inhibition, which are only loosely related to neurophysiological time scales. The oscillation is a constant oscillator:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>m</mml:mi><mml:mo>∗</mml:mo><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>i</mml:mi><mml:mi>ω</mml:mi><mml:mi>T</mml:mi><mml:mo>+</mml:mo><mml:mi>i</mml:mi><mml:mi>φ</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula>in which Am is the amplitude of the oscillator, ω the frequency, and φ the phase offset. As such we assume a stable oscillator which is already aligned to the average speech rate (see <xref ref-type="bibr" rid="bib86">Rimmele et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Poeppel and Assaneo, 2020</xref> for phase alignment models). The model used for the current simulation has one an input layer (l−1 level) and one single layer of semantic word nodes (l level) that receives feedback from a higher level layer (l+1 level). As such only the word (l) level is modeled according to <xref ref-type="disp-formula" rid="equ1 equ2 equ3">Equation 1–3</xref> and the other levels form fixed input and feedback connection patterns. Even though the feedback influences the activity at the word level, it does not cause a phase reset as the phase of the oscillation does not change in response to this feedback.</p></sec><sec id="s3-2"><title>Language models influence time of activation</title><p>To illustrate how STiMCON can explain how processing time depends on the prediction of internal language models, we instantiated a language model that had only seen three sentences and five words presented at different probabilities (I eat cake at 0.5 probability, I eat nice cake at 0.3 probability, I eat very nice cake at 0.2 probability; <xref ref-type="table" rid="table2">Table 2</xref>). While in the brain the prediction should add up to 1, we can assume that the probability is spread across a big number of word nodes of the full language model and therefore neglectable. This language model will serve as the feedback arriving from the l+1-level to the l-level. The l-level consists of five nodes that each represent one of the words and receives proportional feedback from l+1 according to <xref ref-type="table" rid="table2">Table 2</xref> with a delay of 0.9*ω seconds, which then decays at 0.01 unit per millisecond and influences the l-level at a proportion of 1.5. The 0.9*ω was defined as we hypothesized that onset time would be loosely predicted around on oscillatory cycle, but to be prepared for input slightly earlier (which of course happens for predictable stimuli), we set it to 0.9 times the length of the cycle. The decay is needed and set such that the feedback would continue around a full theta cycle. The proportion was set empirically such to ensure that strong feedback did cause suprathreshold activation at the active node. The feedback is only initiated when supra-activation arrives due to l−1-level bottom-up input. Each word at the l−1-level input is modeled as a linearly function to the individual nodes lasting length of 125 ms (half a cycle, ranging from 0 to 1 arbitrary units). As such, the input is not the acoustic input itself but rather reflects a linear increase representing the increasing confidence of a word representing the specific node. φ is set such that the peak of a 4 Hz oscillation aligns to the peak of sensory input of the first word. Sensory input is presented at a base stimulus onset asynchrony of 250 ms (i.e., 4 Hz).</p><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Example of a language model.</title><p>This model has seen three sentences at different probabilities. Rows represent the prediction for the next word, e.g., /I/ predicts /eat/ at a probability of 1, but after /eat/ there is a wider distribution.</p></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">I</th><th valign="top">Eat</th><th valign="top">Very</th><th valign="top">Nice</th><th valign="top">Cake</th></tr></thead><tbody><tr><td valign="top">I</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td></tr><tr><td valign="top">eat</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0.2</td><td valign="top">0.3</td><td valign="top">0.5</td></tr><tr><td valign="top">very</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">1</td><td valign="top">0</td></tr><tr><td valign="top">nice</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">1</td></tr><tr><td valign="top">cake</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td><td valign="top">0</td></tr></tbody></table></table-wrap><p>When we present this model with different sensory inputs at an isochronous rhythm of 4 Hz, it is evident that the timing at which different nodes reach activation depends on the level of feedback that is provided (<xref ref-type="fig" rid="fig4">Figure 4</xref>). For example, while the /I/-node needs a while to get activated after the initial sensory input, the /eat/-node is activated earlier as it is pre-activated due to feedback. After presenting /eat/, the feedback arrives at three different nodes and the activation timing depends on the stimulus that is presented (earlier activation for /cake/ compared to /very/).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Model output for different sentences.</title><p>For the supra-threshold activation dark red indicates activation which included input from l+1 as well as l1, orange indicates activation due to l+1 input. Feedback at different strengths causes phase dependent activation (left). Suprathreshold activation is reached earlier when a highly predicted stimulus (right) arrives, compared to a mid-level predicted stimulus (middle).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig4-v1.tif"/></fig></sec><sec id="s3-3"><title>Time of presentation influences processing efficiency</title><p>To investigate how the time of presentation influences the processing efficiency, we presented the model with /I eat XXX/ in which the last word was varied in content (<xref ref-type="fig" rid="fig5">Figure 5A</xref>; either /I/, /very/, /nice/, or /cake/), intensity (linearly ranging from 0 to 1), and onset delay (ranging between −125 and +125 ms relative to isochronous presentation). We extracted the time at which the node matching the stimulus presentation reached activation threshold first (relative to stimulus onset and relative to isochronous presentation).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Model output on processing efficiency.</title><p>(<bold>A</bold>) Input given to the model. Sensory input is varied in intensity and timing. We extract the time of activation relative to stimulus onset (supra-time) and relative to isochrony onset. (<bold>B</bold>) Time of presentation influences efficiency. Outcome variable is the time at which the node reached threshold activation (supra-time). The dashed line is presented to ease comparison between the four content types. White indicates that threshold is never reached. (C) Same as (B), but estimated at a threshold of 0.53 showing that oscillations regulate feedforward timing. Panel (C) shows that the earlier the stimuli are presented (on a weaker point of the ongoing oscillation), the longer it takes until supra-threshold activation is reached. This figure shows that timing relative to the ongoing oscillation is regulated such that the stimulus activation timing is closer to isochronous. Line discontinuities are a consequence of stimuli never reaching threshold for a specific node.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig5-v1.tif"/></fig><p><xref ref-type="fig" rid="fig5">Figure 5B</xref> shows the output. When there is no feedback (i.e., at the first word /I/ presentation), a classical efficiency map can be found in which processing is most optimal (possible at lowest stimulus intensities) at isochronous (in phase with the stimulus rate) presentation and then drops to either side. For nodes that have feedback, input processing is possible at earlier times relative to isochronous presentation and parametrically varies with prediction strength (earlier for /cake/ at 0.5 probability, then /very/ at 0.2 probability). Additionally, the activation function is asymmetric. This is a consequence of the interaction between the supra-activation caused by the feedback and the sensory input. As soon as supra-activation is reached due to the feedback, sensory input at any intensity will reach supra-activity (thus at early stages of the linearly increasing confidence of the input). This is why for the /very/ stimulus activation is still reached at later delays compared to /nice/ and /cake/ as the /very/-node reaches supra-activation due to feedback at a later time point. In regular circumstances, we would of course always want to process speech, also when it arrives at a less excitable phase. Note, however, that the current stimulus intensities were picked to exactly extract the threshold responses. When we increase our intensity range above 2.1, nodes will always get activated even on the lowest excitable phase of the oscillation.</p><p>When we investigate timing differences in stimulus presentation, it is important to also consider what this means for the timing in the brain. Before, we showed that the amount of prediction can influence timing in our model. It is also evident that the earlier a stimulus was presented the more time it took (relative to the stimulus) for the nodes to reach threshold (more yellow colors for earlier delays). This is a consequence of the oscillation still being at a relatively low excitability point at stimulus onset for stimuli that are presented early during the cycle. However, when we translate these activation threshold timing to the timing of the ongoing oscillation, the variation is strongly reduced (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). A stimulus timing that varies between 130 ms (e.g., from −59 to +72 in the /cake/ line; excluding the non-linear section of the line) only reaches the first supra-threshold response with 19 ms variation in the model (translating to a reduction of 53–8% of the cycle of the ongoing oscillation, i.e., a 1:6.9 ratio). This means that within this model (and any oscillating model) the activation of nodes is robust to some timing variation in the environment. This effect seemed weaker when no prediction was present (for the /I/ stimulus this ratio was around 1:3.5. Note that when determining the /cake/ range using the full line the ratio would be 1:3.4).</p></sec><sec id="s3-4"><title>Top-down interactions can provide rhythmic processing for non-isochronous stimulus input</title><p>The previous simulation demonstrate that oscillations provide a temporal filter and the processing at the word layer can actually be closer to isochronous than what can be solely extracted from the stimulus input. Next, we investigated whether dependent on changes in top-down prediction, processing within the model will be more or less rhythmic. To do this, we create stimulus input of 10 sequential words at a base rate of 4 Hz to the model with constant (<xref ref-type="fig" rid="fig6">Figure 6A</xref>; low at 0 and high at 0.8 predictability) or alternating word-to-word predictability. For the alternating conditions, word-to-word predictability alternates between low and high (sequences which word are predicted at 0 or 0.8 predictability, respectively) or shift from high to low. For this simulation, we used Gaussian sensory input (with a standard deviation of 42 ms aligning the mean at the peak of the ongoing oscillation; see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> for output with linear sensory input). Then, we vary the onset time of the odd words in the sequence (shifting from −100 up to +100 ms) and the stimulus intensity (from 0.2 to 1.5). We extracted the overall activity of the model and computed the FFT of the created time course (using a Hanning taper only including data from 0.5 to 2.5 s to exclude the onset responses). From this FFT, we extracted the peak activation at the stimulation rate of 4 Hz.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Model output on rhythmicity.</title><p>(<bold>A</bold>) We presented the model with repeating (<bold>A, B</bold>) stimuli with varying internal models. We extracted the power spectra and peak activity at various odd stimulus offsets and stimulus intensities. (<bold>B</bold>) Strength of 4 Hz power depends on predictability in the stream. When predictability is alternated between low and high, activation is more rhythmic when the predictable odd stimulus arrives earlier and vice versa. (<bold>C</bold>) Power across different internal models at intensity of 0.8 and 1.0 (different visualization than <bold>B</bold>). (<bold>D</bold>) Magnitude spectra at three different odd word offsets at 1.0 intensity. To more clearly illustrate the differences, the magnitude to the power of 20 is plotted.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Power at 4 Hz using linearly increasing sensory input.</title><p>Conventions of panel A, B are the same as in <xref ref-type="fig" rid="fig6">Figure 6C,D</xref>, respectively.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig6-figsupp1-v1.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Example of overall activation at threshold 0.8 (Gaussian shaped input).</title><p>(<bold>A</bold>) Overall activation for individual nodes and summed activation. (<bold>B</bold>) Overall summed activation as time course. (<bold>C</bold>) Power spectra of the different delays and conditions. Dependent on the internal language model, the power is stronger at the -33 ms or 33 ms delay condition.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig6-figsupp2-v1.tif"/></fig></fig-group><p>The first thing that is evident is that the model with no content predictions has overall lowest power, but has the strongest 4 Hz response around isochronous presentation (odd word offset of 0 ms) at high stimulus intensities (<xref ref-type="fig" rid="fig6">Figure 6B–D</xref>) following closely the acoustic input. Adding overall high predictability increases the power, but also here the power seems symmetric around zero. The spectra of the alternating predictability conditions look different. For the low to high predictability condition, the curve seems to be shifted to the left such that 4 Hz power is strongest when the predictable odd stimulus is shifted to an earlier time point (low–high condition). This is reversed for the high–low condition. At middle stimulus intensities, there is a specific temporal specificity window at which the 4 Hz power is particularly strong. This window is earlier for the low–high than the high–low alternation (<xref ref-type="fig" rid="fig6">Figure 6C,D</xref> and <xref ref-type="fig" rid="fig6s2">Figure 6—figure supplement 2</xref>). The effect only occurs at specific middle-intensity combination as at high intensities the stimulus dominates the responses and at low intensities the stimulus does not reach threshold activation. These results show that even though stimulus input is non-isochronous, the interaction with the internal model can still create a potential isochronous structure in the brain (see <xref ref-type="bibr" rid="bib63">Meyer et al., 2019</xref>; <xref ref-type="bibr" rid="bib64">Meyer et al., 2020</xref>). Note that the direction in which the brain response is more isochronous matches with the natural onset delays in speech (shorter onset delays for more predictable stimuli).</p></sec><sec id="s3-5"><title>Model validation</title><sec id="s3-5-1"><title>STiMCON’s sinusoidal modulations of RNN predictions is optimally sensitive to natural onset delays</title><p>Next, we aimed to investigated whether STiMCON would be optimally sensitive to speech input timings found naturally in speech. Therefore, we tried to fit STIMCON’s expected word-to-word onset differences to the word-to-word onset differences we found in the CGN. At a stable level of intensity of the input and inhibition, the only aspect that changes the timing of the interaction between top-down predictions and bottom-up input within STiMCON is the ongoing oscillation. Considering that we only want to model for individual words how much the prediction <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mi>C</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>→</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mi>*</mml:mi><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) influences the expected timing we can set the contribution of the other factors from <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref> to zero remaining with the relative contribution of prediction:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">→</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>t</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>o</mml:mi><mml:mi>w</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>=</mml:mo><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mo>−</mml:mo><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>c</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mi>T</mml:mi><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>We can solve this formula in order to investigate the expected relative time shift (T) in processing that is a consequence of the strength of the prediction (ignoring that in the exact timing will also depend on the strength of the input and inhibition):<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>m</mml:mi><mml:mi>e</mml:mi><mml:mspace width="thinmathspace"/><mml:mspace width="thinmathspace"/><mml:mi>s</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mrow><mml:mi>π</mml:mi><mml:mi>ω</mml:mi></mml:mrow></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>arcsin</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">→</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>∗</mml:mo><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>l</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi>T</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo>−</mml:mo><mml:mi>A</mml:mi><mml:mi>m</mml:mi></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>−</mml:mo><mml:mi>φ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></disp-formula></p><p>ω was set as the syllable rate for each sentence, and Am and φ were systematically varied. We fitted a linear model between the STiMCON’s expected time and the actual word-to-word onset differences. This model was similar to the model described in the section <italic>Word-by-word predictability predicts word onset differences</italic> and included the predictor syllable rate and duration of the previous word. However, as we were interested in how well non-transformed data matches the natural onset timings, we did not perform any normalization besides <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref>. As this might involve violating some of the assumptions of the ordinary least square fit, we estimate model performance by repeating the regression 1000 times fitting it on 90% of the data (only including the test data from the RNN) and extracting R<sup>2</sup> from the remaining 10%.</p><p>Results show a modulation of the R<sup>2</sup> dependent on the amplitude and phase offset of the oscillation (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). This was stronger than a model in which transformation in <xref ref-type="disp-formula" rid="equ5">Equation (5)</xref> was not applied (R<sup>2</sup>for a model with no transfomation was 0.389). This suggests that STiMCON expected time durations matches the actual word-by-word duration. This was even more strongly so for specific oscillatory alignments (around −0.25π offset), suggesting an optimal alignment phase relative to the ongoing oscillation is needed for optimal tracking (<xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib88">Schroeder and Lakatos, 2009</xref>). Interestingly, the optimal transformation seemed to automatically alter a highly skewed prediction distribution (<xref ref-type="fig" rid="fig7">Figure 7B</xref>) toward a more normal distribution of relative time shifts (<xref ref-type="fig" rid="fig7">Figure 7C</xref>). Note that the current prediction only operated on the word node (to which we have the RNN predictions), while full temporal shifts are probably better explained by word, syllabic, and phrasal predictions.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Fit between real and expected time shift dependent on predictability.</title><p>(<bold>A</bold>) Phase offset and amplitude of the oscillation modulate the fit to the word-to-word onset durations. (<bold>B</bold>) Histogram of the predictions created by the deep neural net. (<bold>C</bold>) Histogram of the relative time shift transformation at phase of −0.15π and amplitude of 1.5.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig7-v1.tif"/></fig></sec><sec id="s3-5-2"><title>STiMCON can explain perceptual effects in speech processing</title><p>Due to the differential feedback strength and the inhibition after suprathreshold feedback stimulation, STiMCON is more sensitive to lower predictable stimuli at phases later in the oscillatory cycle. This property can explain two illusions that have been reported in the literature, specifically, the observation that the interpretation of ambiguous input depends on the phase of presentation (<xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>; <xref ref-type="bibr" rid="bib42">Kayser et al., 2016</xref>; <xref ref-type="bibr" rid="bib92">Ten Oever et al., 2020</xref>) and on speech rate (<xref ref-type="bibr" rid="bib9">Bosker and Reinisch, 2015</xref>). The only assumption that has to be made is that there is an uneven base prediction balance between the ways the ambiguous stimulus can be interpreted.</p><p>The empirical data we aim to model comprises an experiment in which ambiguous syllables, which could either be interpreted as /da/ or /ga/, were presented (<xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>). In one of the experiments in this study, broadband simuli were presented at specific rates to entrain ongoing oscillations. After the last entrainment stimulus, an ambiguous /daga/ stimulus was presented at different delays (covering two cycles of the presentation rate at 12 different steps), putatively reflecting different oscillatory phases. Dependent on the delay of stimulation participants perceived either /da/ or /ga/, suggesting that phase modulates the percept of the participants. Besides this behavioral experiment, the authors also demonstrated that the same temporal dynamics were present when looking at ongoing EEG data, showing that the phase of ongoing oscillations at the onset of ambiguous stimulus presentation determined the percept (<xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>).</p><p>To illustrate that STiMCON is capable of showing a phase (or delay) dependent effect, we use an internal language model similar to our original model (<xref ref-type="table" rid="table2">Table 2</xref>). The model consists of four nodes (N1, N2, Nda, and Nga). N1 and N2 represent nodes responsive to two stimulus S1 and S2 that function as entrainment stimuli. N1 activation predicts a second unspecific stimulus (S2) represented by N2 at a predictability of 1. N2 activation predicts either da or ga at 0.2 and 0.1 probability, respectively. This uneven prediction of /da/ and /ga/ is justified as /da/ is more prevalent in the Dutch language as /ga/ (<xref ref-type="bibr" rid="bib99">Zuidema, 2010</xref>), and it thus has a higher predicted level of occurring. Then, we present STiMCON (same parameters as before) with /S1 S2 XXX/. XXX is varied to have different proportion of the stimulus /da/ and /ga/ (ranging from 0% /da/ to 100% /ga/ in 12 times steps; these reflect relative proportions that sum up to one such that at 30% the intensity of /da/ would be at max 0.3 and of /ga/ 0.7) and is the onset is varied relate to the second to last word. We extract the time that a node reaches suprathreshold activity after stimulus onset. If both nodes were active at the same time, the node with the highest total activation was chosen. Results showed that for some ambiguous stimuli, the delay determines which node is activated first, modulating the ultimate percept of the participant (<xref ref-type="fig" rid="fig8">Figure 8A</xref>, also see <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1A</xref>). The same type of simulation can explain how speech rate can influence perception (<xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1B</xref>; but see <xref ref-type="bibr" rid="bib8">Bosker and Kösem, 2017</xref>).</p><fig-group><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Results for /daga/ illusions.</title><p>(<bold>A</bold>) Modulations due to ambiguous input at different times. Illustration of the node that is active first. Different proportions of the /da/ stimulus show activation timing modulations at different delays. (<bold>B</bold>) Summary of the model and the parameters altered for the empirical fits in (<bold>C</bold>) and (<bold>D</bold>). (<bold>C</bold>). R2 for the grid search fit of the full model using the first active node as outcome variable, a model without inhibition (no inhib), without uneven feedback (no fb), or without an oscillation (no os). The right panel shows the fit of the full model on the rectified behavioral data of <xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>. Blue crossed indicate rectified data and red lines indicate the fit. (<bold>D</bold>) is the same as (<bold>C</bold>) but using the average activity instead of the first active node. Removing the oscillation results in an R<sup>2</sup> less than 0.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig8-v1.tif"/></fig><fig id="fig8s1" position="float" specific-use="child-fig"><label>Figure 8—figure supplement 1.</label><caption><title>Explaining speech timing illusions.</title><p>(<bold>A</bold>) Model activation of two example delays for the fitting (<xref ref-type="fig" rid="fig8">Figure 8A</xref>). (<bold>B</bold>) Modulations due to ambiguous input at different speech rates. Illustration of the node that is active first. Different proportions of the /da/ stimulus show activation timing modulations at different speech rates. Conventions are the same as <xref ref-type="fig" rid="fig8">Figure 8</xref>.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig8-figsupp1-v1.tif"/></fig></fig-group><p>To further scrutinize on this effect, we fitted our model to the behavioral data of <xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>. As we used an iterative approach in the simulations of the model, we optimized the model using a grid search. We varied the parameters of proportion of the stimulus being /da/ or /ga/ (ranging between 10:5:80%), the onset time of the feedback (0.1:0.1:1.0 cycle), the speed of the feedback decay (0:0.01:0.1), and a temporal offset of the final sound to account for the time it takes to interpret a specific ambiguous syllable (ranging between −0.05:0.01:0.05 s). Our first outcome variable was the node that show the first suprathreshold activation (Nda = 1, Nga = 0). If both nodes were active at the same time, the node with the highest total activates was chosen. If both nodes had equal activation or never reached threshold activation, we coded the outcome to 0.5 (i.e., fully ambiguous). These outcomes were fitted to the behavioral data of the 6.25 Hz and 10 Hz presentation rate (the two rates showing a significant modulation of the percept). This data was normalized to have a range between 0 and 1 to account for the model outcomes being binary (0, 0.5, or 1). As a second outcome measure, we also extracted the relative activity of the /da/ and /ga/ nodes by subtracting their activity and dividing by the summed activity. The activity was calculated as the average activity over a window of 500 ms after stimulus onset and the final time course was normalized between 0 and 1.</p><p>For the first node activation analysis, we found that our model could fit the data at an average explained variance of 43% (30% and 58% for 6.25 Hz and 10 Hz, respectively; <xref ref-type="fig" rid="fig8">Figure 8C,D</xref>). For the average activity analysis, we found a fit with 83% explained variance. Compared to the original sinus fit, this explained variance was higher for the average activation analysis (40% for three parameter sinus fit [amplitude, phase offset, and mean]). Note that for the first node activation analysis, our fit cannot account for variance ranging between 0–0.5 and 0.5–1, while the sinus fit can do this. If we correct for this (by setting the sinus fit to the closest 0, 0.5, or 1 value and doing a grid search to optimize the fitting), the average fit of the sinus is 21%. Comparing the fits of the rectified sinus versus the first node activation reveals an average Akaike information criterion of the model and sinus fits of −27.0 and −24.1, respectively. For the average activation analysis, this was −41.5 versus −27.8, respectively. This overall suggests that the STiMCON model has the better fit. Thus, STiMCON does better than a fixed-frequency sinus fit. This is a likely consequence of the sinus fit not being able to explain the dampening of the oscillation later (i.e., the perception bias is stronger for shorter compared to longer delays).</p><p>Finally, we investigated the relevance of the three key features of our model for this fit: inhibition, feedback, and oscillations (<xref ref-type="fig" rid="fig8">Figure 8B</xref>). We repeated the grid search fit but set either the inhibition to zero, the feedback matrix equal for both /da/ and /ga/ (both 0.15), or the oscillation at an amplitude of zero. Results showed for both outcome measures that the full model showed the best performance. Without the oscillation, the models could not even fit better than the mean of the model (R<sup>2</sup> &lt; 0). Removing the feedback had a negative influence on both the outcome measures, dropping the performance. Removing the inhibition reduced performance for both outcome measures, but more strongly on the average activation compared to the first active node model. This suggest that all features (with potentially to a lesser extend the inhibition) are required to model the data, suggesting that oscillatory tracking is dependent on linguistic constrains flowing from the internal language model.</p></sec></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>In the current paper, we combined an oscillating computational model with a proxy for linguistic knowledge, an internal language model, in order to investigate the model’s processing capacity for onset timing differences in natural speech. We show that word-to-word speech onset differences in natural speech are indeed related to predictions flowing from the internal language model (estimated through an RNN). Fixed oscillations aligned to the mean speech rate are robust against natural temporal variations and even optimized for temporal variations that match the predictions flowing from the internal model. Strikingly, when the pseudo-rhythmicity in speech matches the predictions of the internal model, responses were more rhythmic for matched pseudo-rhythmic compared to isochronous speech input. Our model is optimally sensitive to natural speech variations, can explain phase-dependent speech categorization behavior (<xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>; <xref ref-type="bibr" rid="bib95">Thézé et al., 2020</xref>; <xref ref-type="bibr" rid="bib85">Reinisch and Sjerps, 2013</xref>; <xref ref-type="bibr" rid="bib92">Ten Oever et al., 2020</xref>), and naturally comprises a neural phase code (<xref ref-type="bibr" rid="bib75">Panzeri et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Mehta et al., 2002</xref>; <xref ref-type="bibr" rid="bib51">Lisman and Jensen, 2013</xref>). These results show that part of the pseudo-rhythmicity of speech is expected by the brain and it is even optimized to process it in this manner, but only when it follows the internal model.</p><p>Speech timing is variable, and in order to understand how the brain tracks this pseudo-rhythmic signal, we need a better understanding of how this variability arises. Here, we isolated one of the components explaining speech time variation, namely, constraints that are posed by an internal language model. This goes beyond extracting the average speech rate (<xref ref-type="bibr" rid="bib17">Ding et al., 2017</xref>; <xref ref-type="bibr" rid="bib83">Poeppel and Assaneo, 2020</xref>; <xref ref-type="bibr" rid="bib78">Pellegrino and Coupé, 2011</xref>) and might be key to understanding how a predictive brain uses temporal cues. We show that speech timing depends on the predictions made from an internal language model, even when those predictions are highly reduced to be as simple as word predictability. While syllables generally follow a theta rhythm, there is a systematic increase in syllabic rate as soon as more syllables are in a word. This is likely a consequence of the higher close probability of syllables within a word which reduces the onset differences of the later uttered syllables (<xref ref-type="bibr" rid="bib96">Thompson and Newport, 2007</xref>). However, an oscillatory model constrained by an internal language model is sensitive to these temporal variations, it is actually capable of processing them optimally.</p><p>The oscillatory model we here pose has three components: oscillations, feedback, and inhibition. The oscillations allow for the parsing of speech and provide windows in which information is processed (<xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib25">Ghitza, 2012</xref>; <xref ref-type="bibr" rid="bib77">Peelle and Davis, 2012</xref>; <xref ref-type="bibr" rid="bib58">Martin and Doumas, 2017</xref>). Importantly, the oscillation acts as a temporal filter, such that the activation time of any incoming signal will be confined to the high excitable window and thereby is relatively robust against small temporal variations (<xref ref-type="fig" rid="fig5">Figure 5C</xref>). The feedback allows for differential activation time dependent on the sensory input (<xref ref-type="fig" rid="fig5">Figure 5B</xref>). As a consequence, the model is more sensitive to higher predictable speech input and therefore active earlier on the duty cycle (this also means that oscillations are less robust against temporal variations when the feedback is very strong). The inhibition allows for the network to be more sensitive to less predictable speech units when they arrive later (the higher predictable nodes get inhibited at some point on the oscillation; best illustrated by the simulation in <xref ref-type="fig" rid="fig8">Figure 8A</xref>). In this way, speech is ordered along the duty cycle according to its predictability (<xref ref-type="bibr" rid="bib51">Lisman and Jensen, 2013</xref>; <xref ref-type="bibr" rid="bib37">Jensen et al., 2012</xref>). The feedback in combination with an oscillatory model can explain speech rate and phase-dependent content effects. Moreover, it is an automatic temporal code that can use time of activation as a cue for content (<xref ref-type="bibr" rid="bib61">Mehta et al., 2002</xref>). Note that previously we have interpreted the /daga/ phase-dependent effect as a mapping of differences between natural audio-visual onset delays of the two syllabic types onto oscillatory phase (<xref ref-type="bibr" rid="bib90">Ten Oever et al., 2013</xref>; <xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>). However, the current interpretation is not mutually exclusive with this delay-to-phase mapping as audio-visual delays could be bigger for less frequent syllables. The three components in the model are common brain mechanisms (<xref ref-type="bibr" rid="bib54">Malhotra et al., 2012</xref>; <xref ref-type="bibr" rid="bib61">Mehta et al., 2002</xref>; <xref ref-type="bibr" rid="bib11">Buzsáki and Draguhn, 2004</xref>; <xref ref-type="bibr" rid="bib5">Bastos et al., 2012</xref>; <xref ref-type="bibr" rid="bib65">Michalareas et al., 2016</xref>; <xref ref-type="bibr" rid="bib50">Lisman, 2005</xref>) and follow many previously proposed organization principles (e.g., temporal coding and parsing of information). While we implement these components on an abstract level (not veridical to the exact parameters of neuronal interactions), they illustrate how oscillations, feedback, and inhibition interact to optimize sensitivity to natural pseudo-rhythmic speech.</p><p>The current model is not exhaustive and does not provide a complete explanation of all the details of speech processing in the brain. For example, it is likely that the primary auditory cortex is still mostly modulated by the acoustic pseudo-rhythmic input and only later brain areas follow more closely the constraints posed by the language model of the brain. Moreover, we now focus on the word level, while many tracking studies have shown the importance of syllabic temporal structure (<xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib25">Ghitza, 2012</xref>; <xref ref-type="bibr" rid="bib53">Luo and Poeppel, 2007</xref>) as well as the role of higher order linguistic temporal dynamics (<xref ref-type="bibr" rid="bib63">Meyer et al., 2019</xref>; <xref ref-type="bibr" rid="bib40">Kaufeld et al., 2020b</xref>). It is likely that predictive mechanisms also operate on these higher linguistic levels as well as on syllabic levels. It is known, for example, that syllables are shortened when the following syllabic content is known versus producing syllables in isolation (<xref ref-type="bibr" rid="bib80">Pluymaekers et al., 2005a</xref>; <xref ref-type="bibr" rid="bib49">Lehiste, 1972</xref>). Interactions also occur as syllables part of more frequent words are generally shortened (<xref ref-type="bibr" rid="bib81">Pluymaekers et al., 2005b</xref>). Therefore, more hierarchical levels need to be added to the current model (but this is possible following <xref ref-type="disp-formula" rid="equ1">Equation (1)</xref>). Moreover, the current model does not allow for phase or frequency shifts. This was intentional in order to investigate how much a fixed oscillator could explain. We show that onset times matching the predictions from the internal model can be explained by a fixed oscillator processing pseudo-rhythmic input. However, when the internal model and the onset timings do not match, the internal model phase and/or frequency shift are still required and need to be incorporated (see e.g. <xref ref-type="bibr" rid="bib86">Rimmele et al., 2018</xref>; <xref ref-type="bibr" rid="bib83">Poeppel and Assaneo, 2020</xref>).</p><p>We aimed to show that a stable oscillator can be sensitive to temporal pseudo-rhythmicities when these shifts match predictions from an internal linguistic model (causing higher sensitivity to these nodes). In this way, we show that temporal dynamics in speech and the brain cannot be isolated from processing the content of speech. This is in contrast with other models that try to explain how the brain deals with pseudo-rhythmicity in speech (<xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib86">Rimmele et al., 2018</xref>; <xref ref-type="bibr" rid="bib18">Doelling et al., 2019</xref>). While some of these models discuss that higher-level linguistic processing can modulate the timing of ongoing oscillations (<xref ref-type="bibr" rid="bib86">Rimmele et al., 2018</xref>), they typically do not consider that in the speech signal itself the content or predictability of a word relates to the timing of this word. Phase resetting models typically deal with pseudo-rhythmicity by shifting the phase of ongoing oscillations in response to a word that is offset to the mean frequency of the input (<xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib18">Doelling et al., 2019</xref>). We believe that this cannot explain how the brain uses what/when dependencies in the environment to infer the content of the word (e.g., later words are likely a less predictable word). Our current model does not have an explanation of how the brain can actually entrain to an average speech rate. This is much better described in dynamical systems theories in which this is a consequence of the coupling strength between internal oscillations and speech acoustics (<xref ref-type="bibr" rid="bib18">Doelling et al., 2019</xref>; <xref ref-type="bibr" rid="bib2">Assaneo et al., 2021</xref>). However, these models do not take top-down predictive processing into account. Therefore, the best way forward is likely to extend coupling between brain oscillations and speech acoustics (<xref ref-type="bibr" rid="bib83">Poeppel and Assaneo, 2020</xref>), with the coupling of brain oscillations to brain activity patterns of internal models (<xref ref-type="bibr" rid="bib13">Cumin and Unsworth, 2007</xref>).</p><p>In the current paper, we use an RNN to represent the internal model of the brain. However, it is unlikely that the RNN captures the wide complexities of the language model in the brain. The decades-long debates about the origin of a language model in the brain remains ongoing and controversial. Utilizing the RNN as a proxy for our internal language model makes a tacit assumption that language is fundamentally statistical or associative in nature, and does not posit the derivation or generation of knowledge of grammar from the input (<xref ref-type="bibr" rid="bib12">Chater, 2001</xref>; <xref ref-type="bibr" rid="bib60">McClelland and Elman, 1986</xref>). In contrast, our brain could as well store knowledge of language that functions as fundamental interpretation principles to guide our understanding of language input (<xref ref-type="bibr" rid="bib56">Martin, 2016</xref>; <xref ref-type="bibr" rid="bib57">Martin, 2020</xref>; <xref ref-type="bibr" rid="bib31">Hagoort, 2017</xref>; <xref ref-type="bibr" rid="bib58">Martin and Doumas, 2017</xref>; <xref ref-type="bibr" rid="bib24">Friederici, 2011</xref>). Knowledge of language and linguistic structure could be acquired through an internal self-supervised comparison process extracted from environmental invariants and statistical regularities from the stimulus input (<xref ref-type="bibr" rid="bib59">Martin and Doumas, 2019</xref>; <xref ref-type="bibr" rid="bib19">Doumas et al., 2008</xref>; <xref ref-type="bibr" rid="bib20">Doumas and Martin, 2018</xref>). Future research should investigate which language model can better account for the temporal variations found in speech.</p><p>A natural feature of our model is that time can act as a cue for content implemented as a phase code (<xref ref-type="bibr" rid="bib51">Lisman and Jensen, 2013</xref>; <xref ref-type="bibr" rid="bib37">Jensen et al., 2012</xref>). This code unravels as an ordered list of predictability strength of the internal model. This idea diverges from the idea that entrainment should align to the most excitable phase of the oscillation with the highest energy in the acoustics (<xref ref-type="bibr" rid="bib28">Giraud and Poeppel, 2012</xref>; <xref ref-type="bibr" rid="bib86">Rimmele et al., 2018</xref>). Instead, this type of phase coding could increase the brain representational space to separate information content (<xref ref-type="bibr" rid="bib51">Lisman and Jensen, 2013</xref>; <xref ref-type="bibr" rid="bib74">Panzeri et al., 2001</xref>). We predict that if speech nodes have a different base activity, ambiguous stimulus interpretation should dependent on the time/phase of presentation (see <xref ref-type="bibr" rid="bib93">Ten Oever and Sack, 2015</xref>; <xref ref-type="bibr" rid="bib92">Ten Oever et al., 2020</xref>). Indeed, we could model two temporal speech illusions (<xref ref-type="fig" rid="fig8">Figure 8</xref>, <xref ref-type="fig" rid="fig8s1">Figure 8—figure supplement 1</xref>). There have also been null results regarding the influence of phase on ambiguous stimulus interpretation (<xref ref-type="bibr" rid="bib8">Bosker and Kösem, 2017</xref>; <xref ref-type="bibr" rid="bib44">Kösem et al., 2016</xref>). For the speech rate effect, when modifying the time of presentation with a neutral entrainer (summed sinusoidals with random phase), no obvious phase effect was reported (<xref ref-type="bibr" rid="bib8">Bosker and Kösem, 2017</xref>). A second null result relates to a study where participants were specifically instructed to maintain a specific perception in different blocks which likely increases the pre-activation and thereby the phase (<xref ref-type="bibr" rid="bib44">Kösem et al., 2016</xref>). Future studies need to investigate the use of temporal/phase codes to disambiguate speech input and specifically use predictions in their design.</p><p>The temporal dynamics of speech signals needs to be integrated with the temporal dynamics of brain signals. However, it is unnecessary (and unlikely) that the exact duration of speech matches with the exact duration of brain processes. Temporal expansion or squeezing of stimulus inputs occur regularly in the brain (<xref ref-type="bibr" rid="bib21">Eagleman et al., 2005</xref>; <xref ref-type="bibr" rid="bib76">Pariyadath and Eagleman, 2007</xref>), and this temporal morphing also maps to duration (<xref ref-type="bibr" rid="bib22">Eagleman, 2008</xref>; <xref ref-type="bibr" rid="bib94">Terao et al., 2008</xref>; <xref ref-type="bibr" rid="bib97">Ulrich et al., 2006</xref>) or order illusions (<xref ref-type="bibr" rid="bib98">Vroomen and Keetels, 2010</xref>). Our model predicts increased rhythmic responses for non-isochronous speech matching the internal model. The perceived rhythmicity of speech could therefore also be an illusion generated by a rhythmic brain signal somewhere in the brain.</p><p>When investigating the pseudo-rhythmicity in speech, it is important to identify situations where speech is actually more isochronous. Two examples are the production of lists (<xref ref-type="bibr" rid="bib36">Jefferson, 1990</xref>) and infant-directed speech (<xref ref-type="bibr" rid="bib23">Fernald, 2000</xref>). In both these examples, it is clear that a strong internal predictive language model is lacking either on the producer’s or on the receiver’s side, respectively. The infant-directed speech also illustrates that a producer might proactively adapt its speech rhythm to the expectations of the internal model of the receiver to align better with the predictions from the receiver’s model (<xref ref-type="fig" rid="fig9">Figure 9B</xref>; similar to when you are speaking to somebody that is just learning a new language). Other examples in which speech is more isochronous is during poems, during emotional conversation (<xref ref-type="bibr" rid="bib32">Hawkins, 2014</xref>), and in noisy situations (<xref ref-type="bibr" rid="bib7">Bosker and Cooke, 2018</xref>). While speculative, it is conceivable that in these circumstances one puts more weight on a different level of hierarchy than the internal linguistic model. In the case of poems and emotional conversation, an emotional route might get more weight in processing. In the case of noisy situations, stimulus input has to pass the first hierarchical level of the primary auditory cortex which effectively gets more weight than the internal model.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Predictions of the model.</title><p>(<bold>A</bold>) Acoustics signals will be more isochronous when a producer has a weak versus a strong internal model (top right). When the producer’s strong model matches the receiver’s model, the brain response will be more isochronous for less isochronous acoustic input. (<bold>B</bold>) When a producer realizes the model of the receiver is weak, it might transform its model and thereby their speech timing to match the receiver’s expectations.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-fig9-v1.tif"/></fig><sec id="s4-1"><title>Conclusions</title><p>We argued that pseudo-rhythmicity in speech is in part a consequence of top-down predictions flowing from an internal model of language. This pseudo-rhythmicity is created by a speaker and expected by a receiver if they have overlapping internal language models. Oscillatory tracking of this signal does not need to be hampered by the pseudo-rhythmicity, but can use temporal variations as a cue to extract content information since the phase of activation parametrically relates to the likelihood of an input relative to the internal model. Brain responses can even be more isochronous to pseudo-rhythmic compared to isochronous speech if they follow the temporal delays imposed by the internal model. This account provides various testable predictions which, we list in <xref ref-type="table" rid="table3">Table 3</xref> and <xref ref-type="fig" rid="fig9">Figure 9</xref>. We believe that by integrating neuroscientific explanations of speech tracking with linguistic models of language processing (<xref ref-type="bibr" rid="bib56">Martin, 2016</xref>; <xref ref-type="bibr" rid="bib57">Martin, 2020</xref>), we can improve to explain temporal speech dynamics. This will ultimately aid our understanding of language in the brain and provide a means to improve temporal properties in speech synthesis.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Predictions from the current model.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td valign="top">When there is a flat constraint distribution over an utterance (e.g., when probabilities are uniform over the utterance), the acoustics of speech should naturally be more isochronous (<xref ref-type="fig" rid="fig9">Figures 9A</xref> and <xref ref-type="fig" rid="fig3">3D,E</xref>).</td></tr><tr><td valign="top">If speech timing matches the internal language model, brain responses should be more isochronous even if the acoustics are not (<xref ref-type="fig" rid="fig9">Figure 9A</xref>).</td></tr><tr><td valign="top">The more similar the internal language models of two speakers, the more effective they are in ‘entraining’ each other’s brain.</td></tr><tr><td valign="top">If speakers suspect their listener to have a flatter constraint distribution than themselves (e.g., the environment is noisy, or the speakers are in a second language context), they adjust to the distribution by speaking more isochronous (<xref ref-type="fig" rid="fig9">Figure 9B</xref>).</td></tr><tr><td valign="top">One adjusts the weight of the constraint distribution to a hierarchical level when needed. For example, when there is noise, participants adjust to the rhythm of primary auditory cortex instead of higher order language models. As a consequence, they speak more isochronous.</td></tr><tr><td valign="top">The theoretical account provides various predictions that are listed in this table.</td></tr></tbody></table></table-wrap></sec><sec id="s4-2"><title>Code availability statement</title><p>Code for the creation of the main figures is available on <ext-link ext-link-type="uri" xlink:href="https://github.com/sannetenoever/STiMCON">GitHub</ext-link> (<xref ref-type="bibr" rid="bib89">Ten Oever &amp; Martin, 2021</xref>; copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:cf831eabfe75473deb3aafac084e8af91398ae29;origin=https://github.com/sannetenoever/STiMCON;visit=swh:1:snp:fbce7be5ac6a1486f21dcc28e7a79b952d3e1c92;anchor=swh:1:rev:873a2bf5c79fe2f828e72e14ef74db409d387854">swh:1:rev:873a2bf5c79fe2f828e72e14ef74db409d387854</ext-link>).</p></sec></sec></body><back><ack id="ack"><title>Acknowledgements</title><p>AEM was supported by the Max Planck Research Group and Lise Meitner Research Group ‘Language and Computation in Neural Systems’ from the Max Planck Society, and by the Netherlands Organization for Scientific Research (grant 016.Vidi.188.029 to AEM). <xref ref-type="fig" rid="fig1">Figure 1</xref> and <xref ref-type="fig" rid="fig9">9</xref> were created in collaboration with scientific illustrator Jan-Karen Campbell (<ext-link ext-link-type="uri" xlink:href="http://www.jankaren.com">http://www.jankaren.com</ext-link>). </p></ack><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Resources, Supervision, Funding acquisition, Validation, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Summary of regression model for logarithm of word duration.</title></caption><media mime-subtype="docx" mimetype="application" xlink:href="elife-68066-supp1-v1.docx"/></supplementary-material><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="pdf" mimetype="application" xlink:href="elife-68066-transrepform-v1.pdf"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Data used in the dataset relate to the corpus gesproken nederlands. Information about this dataset can be found here: <ext-link ext-link-type="uri" xlink:href="http://lands.let.ru.nl/cgn/">http://lands.let.ru.nl/cgn/</ext-link>. Access to the dataset can be requested here: <ext-link ext-link-type="uri" xlink:href="https://taalmaterialen.ivdnt.org/download/tstc-corpus-gesproken-nederlands/">https://taalmaterialen.ivdnt.org/download/tstc-corpus-gesproken-nederlands/</ext-link>. Data regarding the simulations in Figure 8 are based on data from Ten Oever &amp; Sack (2015). As this data regards a closed database owned by Maastricht University it is not openly available. However, the data is available upon request without any restrictions via sanne.tenoever@mpi.nl or datamanagement-fpn@maastrichtuniversity.nl.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Arvaniti</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Rhythm, timing and the timing of rhythm</article-title><source>Phonetica</source><volume>66</volume><fpage>46</fpage><lpage>63</lpage><pub-id pub-id-type="doi">10.1159/000208930</pub-id><pub-id pub-id-type="pmid">19390230</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assaneo</surname> <given-names>MF</given-names></name><name><surname>Rimmele</surname> <given-names>JM</given-names></name><name><surname>Sanz Perl</surname> <given-names>Y</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Speaking rhythmically can shape hearing</article-title><source>Nature Human Behaviour</source><volume>5</volume><fpage>71</fpage><lpage>82</lpage><pub-id pub-id-type="doi">10.1038/s41562-020-00962-0</pub-id><pub-id pub-id-type="pmid">33046860</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Aubanel</surname> <given-names>V</given-names></name><name><surname>Schwartz</surname> <given-names>J-L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The role of isochrony in speech perception in noise</article-title><source>Scientific Reports</source><volume>10</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1038/s41598-020-76594-1</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bahramisharif</surname> <given-names>A</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Jacobs</surname> <given-names>J</given-names></name><name><surname>Lisman</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Serial representation of items during working memory maintenance at letter-selective cortical sites</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2003805</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2003805</pub-id><pub-id pub-id-type="pmid">30110320</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bastos</surname> <given-names>AM</given-names></name><name><surname>Usrey</surname> <given-names>WM</given-names></name><name><surname>Adams</surname> <given-names>RA</given-names></name><name><surname>Mangun</surname> <given-names>GR</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name><name><surname>Friston</surname> <given-names>KJ</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Canonical microcircuits for predictive coding</article-title><source>Neuron</source><volume>76</volume><fpage>695</fpage><lpage>711</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2012.10.038</pub-id><pub-id pub-id-type="pmid">23177956</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beattie</surname> <given-names>GW</given-names></name><name><surname>Butterworth</surname> <given-names>BL</given-names></name></person-group><year iso-8601-date="1979">1979</year><article-title>Contextual probability and word frequency as determinants of pauses and errors in spontaneous speech</article-title><source>Language and Speech</source><volume>22</volume><fpage>201</fpage><lpage>211</lpage><pub-id pub-id-type="doi">10.1177/002383097902200301</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Cooke</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Talkers produce more pronounced amplitude modulations when speaking in noise</article-title><source>The Journal of the Acoustical Society of America</source><volume>143</volume><fpage>EL121</fpage><lpage>EL126</lpage><pub-id pub-id-type="doi">10.1121/1.5024404</pub-id><pub-id pub-id-type="pmid">29495684</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Kösem</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>An entrained rhythm's frequency, not phase, influences temporal sampling of speech</article-title><conf-name>Interspeech</conf-name><pub-id pub-id-type="doi">10.21437/Interspeech.2017-73</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Reinisch</surname> <given-names>E</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Normalization for Speechrate in Native and Nonnative Speech. 18th International Congress of Phonetic Sciences (ICPhS 2015)</source><publisher-name>International Phonetic Association</publisher-name></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brennan</surname> <given-names>JR</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Phase synchronization varies systematically with linguistic structure composition</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>375</volume><elocation-id>20190305</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2019.0305</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Buzsáki</surname> <given-names>G</given-names></name><name><surname>Draguhn</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Neuronal oscillations in cortical networks</article-title><source>Science</source><volume>304</volume><fpage>1926</fpage><lpage>1929</lpage><pub-id pub-id-type="doi">10.1126/science.1099745</pub-id><pub-id pub-id-type="pmid">15218136</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Chater</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Connectionist Psycholinguistics</source><publisher-name>Greenwood Publishing Group</publisher-name></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cumin</surname> <given-names>D</given-names></name><name><surname>Unsworth</surname> <given-names>CP</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Generalising the Kuramoto model for the study of neuronal synchronisation in the brain</article-title><source>Physica D: Nonlinear Phenomena</source><volume>226</volume><fpage>181</fpage><lpage>196</lpage><pub-id pub-id-type="doi">10.1016/j.physd.2006.12.004</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Deacon</surname> <given-names>D</given-names></name><name><surname>Mehta</surname> <given-names>A</given-names></name><name><surname>Tinsley</surname> <given-names>C</given-names></name><name><surname>Nousak</surname> <given-names>JM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Variation in the latencies and amplitudes of N400 and NA as a function of semantic priming</article-title><source>Psychophysiology</source><volume>32</volume><fpage>560</fpage><lpage>570</lpage><pub-id pub-id-type="doi">10.1111/j.1469-8986.1995.tb01232.x</pub-id><pub-id pub-id-type="pmid">8524990</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>deen</surname> <given-names>V</given-names></name><name><surname>Kochs</surname> <given-names>S</given-names></name><name><surname>Smulders</surname> <given-names>F</given-names></name><name><surname>De Weerd</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Learned interval time facilitates associate memory retrieval</article-title><source>Learn Memory</source><volume>24</volume><fpage>158</fpage><lpage>161</lpage><pub-id pub-id-type="doi">10.1101/lm.044404.116</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Di Liberto</surname> <given-names>GM</given-names></name><name><surname>O'Sullivan</surname> <given-names>JA</given-names></name><name><surname>Lalor</surname> <given-names>EC</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Low-Frequency cortical entrainment to speech reflects Phoneme-Level processing</article-title><source>Current Biology</source><volume>25</volume><fpage>2457</fpage><lpage>2465</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2015.08.030</pub-id><pub-id pub-id-type="pmid">26412129</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ding</surname> <given-names>N</given-names></name><name><surname>Patel</surname> <given-names>AD</given-names></name><name><surname>Chen</surname> <given-names>L</given-names></name><name><surname>Butler</surname> <given-names>H</given-names></name><name><surname>Luo</surname> <given-names>C</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Temporal modulations in speech and music</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>81</volume><fpage>181</fpage><lpage>187</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.02.011</pub-id><pub-id pub-id-type="pmid">28212857</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doelling</surname> <given-names>KB</given-names></name><name><surname>Assaneo</surname> <given-names>MF</given-names></name><name><surname>Bevilacqua</surname> <given-names>D</given-names></name><name><surname>Pesaran</surname> <given-names>B</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>An oscillator model better predicts cortical entrainment to music</article-title><source>PNAS</source><volume>116</volume><fpage>10113</fpage><lpage>10121</lpage><pub-id pub-id-type="doi">10.1073/pnas.1816414116</pub-id><pub-id pub-id-type="pmid">31019082</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doumas</surname> <given-names>LA</given-names></name><name><surname>Hummel</surname> <given-names>JE</given-names></name><name><surname>Sandhofer</surname> <given-names>CM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A theory of the discovery and predication of relational concepts</article-title><source>Psychological Review</source><volume>115</volume><fpage>1</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.115.1.1</pub-id><pub-id pub-id-type="pmid">18211183</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doumas</surname> <given-names>LA</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Learning structured representations from experience</article-title><source>Psychology of Learning and Motivation</source><volume>69</volume><fpage>165</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1016/BS.PLM.2018.10.002</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eagleman</surname> <given-names>DM</given-names></name><name><surname>Tse</surname> <given-names>PU</given-names></name><name><surname>Buonomano</surname> <given-names>D</given-names></name><name><surname>Janssen</surname> <given-names>P</given-names></name><name><surname>Nobre</surname> <given-names>AC</given-names></name><name><surname>Holcombe</surname> <given-names>AO</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Time and the brain: how subjective time relates to neural time</article-title><source>Journal of Neuroscience</source><volume>25</volume><fpage>10369</fpage><lpage>10371</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3487-05.2005</pub-id><pub-id pub-id-type="pmid">16280574</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eagleman</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Human time perception and its illusions</article-title><source>Current Opinion in Neurobiology</source><volume>18</volume><fpage>131</fpage><lpage>136</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2008.06.002</pub-id><pub-id pub-id-type="pmid">18639634</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fernald</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Speech to infants as hyperspeech: knowledge-driven processes in early word recognition</article-title><source>Phonetica</source><volume>57</volume><fpage>242</fpage><lpage>254</lpage><pub-id pub-id-type="doi">10.1159/000028477</pub-id><pub-id pub-id-type="pmid">10992144</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friederici</surname> <given-names>AD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The brain basis of language processing: from structure to function</article-title><source>Physiological Reviews</source><volume>91</volume><fpage>1357</fpage><lpage>1392</lpage><pub-id pub-id-type="doi">10.1152/physrev.00006.2011</pub-id><pub-id pub-id-type="pmid">22013214</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>On the role of theta-driven syllabic parsing in decoding speech: intelligibility of speech with a manipulated modulation spectrum</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>238</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00238</pub-id><pub-id pub-id-type="pmid">22811672</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The theta-syllable: a unit of speech information defined by cortical function</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>138</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00138</pub-id><pub-id pub-id-type="pmid">23519170</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghitza</surname> <given-names>O</given-names></name><name><surname>Greenberg</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>On the possible role of brain rhythms in speech perception: intelligibility of time-compressed speech with periodic and aperiodic insertions of silence</article-title><source>Phonetica</source><volume>66</volume><fpage>113</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1159/000208934</pub-id><pub-id pub-id-type="pmid">19390234</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Cortical oscillations and speech processing: emerging computational principles and operations</article-title><source>Nature Neuroscience</source><volume>15</volume><fpage>511</fpage><lpage>517</lpage><pub-id pub-id-type="doi">10.1038/nn.3063</pub-id><pub-id pub-id-type="pmid">22426255</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guest</surname> <given-names>O</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>How computational modeling can force theory building in psychological science</article-title><source>Perspectives on Psychological Science</source><volume>16</volume><fpage>789</fpage><lpage>802</lpage><pub-id pub-id-type="doi">10.1177/1745691620970585</pub-id><pub-id pub-id-type="pmid">33482070</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Gwilliams</surname> <given-names>L</given-names></name><name><surname>King</surname> <given-names>J-R</given-names></name><name><surname>Marantz</surname> <given-names>A</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neural dynamics of phoneme sequencing in real speech jointly encode order and invariant content</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.04.04.025684</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hagoort</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The core and beyond in the language-ready brain</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>81</volume><fpage>194</fpage><lpage>204</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.01.048</pub-id><pub-id pub-id-type="pmid">28193452</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hawkins</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Situational influences on rhythmicity in speech, music, and their interaction</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130398</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0398</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Henry</surname> <given-names>MJ</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Frequency modulation entrains slow neural oscillations and optimizes human listening behavior</article-title><source>PNAS</source><volume>109</volume><fpage>20095</fpage><lpage>20100</lpage><pub-id pub-id-type="doi">10.1073/pnas.1213390109</pub-id><pub-id pub-id-type="pmid">23151506</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Herrmann</surname> <given-names>B</given-names></name><name><surname>Henry</surname> <given-names>MJ</given-names></name><name><surname>Grigutsch</surname> <given-names>M</given-names></name><name><surname>Obleser</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Oscillatory phase dynamics in neural entrainment underpin illusory percepts of time</article-title><source>Journal of Neuroscience</source><volume>33</volume><fpage>15799</fpage><lpage>15809</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1434-13.2013</pub-id><pub-id pub-id-type="pmid">24089487</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jadoul</surname> <given-names>Y</given-names></name><name><surname>Ravignani</surname> <given-names>A</given-names></name><name><surname>Thompson</surname> <given-names>B</given-names></name><name><surname>Filippi</surname> <given-names>P</given-names></name><name><surname>de Boer</surname> <given-names>B</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Seeking temporal predictability in speech: comparing statistical approaches on 18 world languages</article-title><source>Frontiers in Human Neuroscience</source><volume>10</volume><elocation-id>586</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2016.00586</pub-id><pub-id pub-id-type="pmid">27994544</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jefferson</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>List construction as a task and resource</article-title><source>Interaction Competence</source><volume>63</volume><elocation-id>92</elocation-id><pub-id pub-id-type="doi">10.1016/j.pragma.2006.07.008</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Bonnefond</surname> <given-names>M</given-names></name><name><surname>VanRullen</surname> <given-names>R</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>An oscillatory mechanism for prioritizing salient unattended stimuli</article-title><source>Trends in Cognitive Sciences</source><volume>16</volume><fpage>200</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2012.03.002</pub-id><pub-id pub-id-type="pmid">22436764</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jones</surname> <given-names>MR</given-names></name><name><surname>Boltz</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Dynamic attending and responses to time</article-title><source>Psychological Review</source><volume>96</volume><fpage>459</fpage><lpage>491</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.96.3.459</pub-id><pub-id pub-id-type="pmid">2756068</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kaufeld</surname> <given-names>G</given-names></name><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Alday</surname> <given-names>PM</given-names></name><name><surname>Meyer</surname> <given-names>AS</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>Linguistic structure and meaning organize neural oscillations into a content-specific hierarchy</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.02.05.935676</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kaufeld</surname> <given-names>G</given-names></name><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Ten Oever</surname> <given-names>S</given-names></name><name><surname>Alday</surname> <given-names>PM</given-names></name><name><surname>Meyer</surname> <given-names>AS</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>Linguistic structure and meaning organize neural oscillations into a Content-Specific hierarchy</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>9467</fpage><lpage>9475</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0302-20.2020</pub-id><pub-id pub-id-type="pmid">33097640</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>C</given-names></name><name><surname>Montemurro</surname> <given-names>MA</given-names></name><name><surname>Logothetis</surname> <given-names>NK</given-names></name><name><surname>Panzeri</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Spike-phase coding boosts and stabilizes information carried by spatial and temporal spike patterns</article-title><source>Neuron</source><volume>61</volume><fpage>597</fpage><lpage>608</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2009.01.008</pub-id><pub-id pub-id-type="pmid">19249279</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kayser</surname> <given-names>SJ</given-names></name><name><surname>McNair</surname> <given-names>SW</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prestimulus influences on auditory perception from sensory representations and decision processes</article-title><source>PNAS</source><volume>113</volume><fpage>4842</fpage><lpage>4847</lpage><pub-id pub-id-type="doi">10.1073/pnas.1524087113</pub-id><pub-id pub-id-type="pmid">27071110</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Keitel</surname> <given-names>A</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004473</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004473</pub-id><pub-id pub-id-type="pmid">29529019</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname> <given-names>A</given-names></name><name><surname>Basirat</surname> <given-names>A</given-names></name><name><surname>Azizi</surname> <given-names>L</given-names></name><name><surname>van Wassenhove</surname> <given-names>V</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>High-frequency neural activity predicts word parsing in ambiguous speech streams</article-title><source>Journal of Neurophysiology</source><volume>116</volume><fpage>2497</fpage><lpage>2512</lpage><pub-id pub-id-type="doi">10.1152/jn.00074.2016</pub-id><pub-id pub-id-type="pmid">27605528</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kösem</surname> <given-names>A</given-names></name><name><surname>Bosker</surname> <given-names>HR</given-names></name><name><surname>Takashima</surname> <given-names>A</given-names></name><name><surname>Meyer</surname> <given-names>A</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name><name><surname>Hagoort</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neural entrainment determines the words we hear</article-title><source>Current Biology</source><volume>28</volume><fpage>2867</fpage><lpage>2875</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.07.023</pub-id><pub-id pub-id-type="pmid">30197083</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakatos</surname> <given-names>P</given-names></name><name><surname>Karmos</surname> <given-names>G</given-names></name><name><surname>Mehta</surname> <given-names>AD</given-names></name><name><surname>Ulbert</surname> <given-names>I</given-names></name><name><surname>Schroeder</surname> <given-names>CE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Entrainment of neuronal oscillations as a mechanism of attentional selection</article-title><source>Science</source><volume>320</volume><fpage>110</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1126/science.1154735</pub-id><pub-id pub-id-type="pmid">18388295</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Large</surname> <given-names>EW</given-names></name><name><surname>Jones</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>The dynamics of attending: how people track time-varying events</article-title><source>Psychological Review</source><volume>106</volume><fpage>119</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.106.1.119</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lau</surname> <given-names>EF</given-names></name><name><surname>Phillips</surname> <given-names>C</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>A cortical network for semantics: (de)constructing the N400</article-title><source>Nature Reviews Neuroscience</source><volume>9</volume><fpage>920</fpage><lpage>933</lpage><pub-id pub-id-type="doi">10.1038/nrn2532</pub-id><pub-id pub-id-type="pmid">19020511</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lehiste</surname> <given-names>I</given-names></name></person-group><year iso-8601-date="1972">1972</year><article-title>The timing of utterances and linguistic boundaries</article-title><source>The Journal of the Acoustical Society of America</source><volume>51</volume><fpage>2018</fpage><lpage>2024</lpage><pub-id pub-id-type="doi">10.1121/1.1913062</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname> <given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>The theta/gamma discrete phase code occuring during the hippocampal phase precession may be a more general brain coding scheme</article-title><source>Hippocampus</source><volume>15</volume><fpage>913</fpage><lpage>922</lpage><pub-id pub-id-type="doi">10.1002/hipo.20121</pub-id><pub-id pub-id-type="pmid">16161035</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lisman</surname> <given-names>JE</given-names></name><name><surname>Jensen</surname> <given-names>O</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The Theta-Gamma neural code</article-title><source>Neuron</source><volume>77</volume><fpage>1002</fpage><lpage>1016</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.03.007</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Tian</surname> <given-names>X</given-names></name><name><surname>Song</surname> <given-names>K</given-names></name><name><surname>Zhou</surname> <given-names>K</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Neural response phase tracks how listeners learn new acoustic representations</article-title><source>Current Biology</source><volume>23</volume><fpage>968</fpage><lpage>974</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.031</pub-id><pub-id pub-id-type="pmid">23664974</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Luo</surname> <given-names>H</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex</article-title><source>Neuron</source><volume>54</volume><fpage>1001</fpage><lpage>1010</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2007.06.004</pub-id><pub-id pub-id-type="pmid">17582338</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname> <given-names>S</given-names></name><name><surname>Cross</surname> <given-names>RWA</given-names></name><name><surname>van der Meer</surname> <given-names>MAA</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Theta phase precession beyond the Hippocampus</article-title><source>Reviews in the Neurosciences</source><volume>23</volume><fpage>39</fpage><lpage>65</lpage><pub-id pub-id-type="doi">10.1515/revneuro-2011-0064</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marslen-Wilson</surname> <given-names>WD</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Functional parallelism in spoken word-recognition</article-title><source>Cognition</source><volume>25</volume><fpage>71</fpage><lpage>102</lpage><pub-id pub-id-type="doi">10.1016/0010-0277(87)90005-9</pub-id><pub-id pub-id-type="pmid">3581730</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Language processing as cue integration: grounding the psychology of language in perception and neurophysiology</article-title><source>Frontiers in Psychology</source><volume>7</volume><elocation-id>120</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2016.00120</pub-id><pub-id pub-id-type="pmid">26909051</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A compositional neural architecture for language</article-title><source>Journal of Cognitive Neuroscience</source><volume>32</volume><fpage>1407</fpage><lpage>1427</lpage><pub-id pub-id-type="doi">10.1162/jocn_a_01552</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>AE</given-names></name><name><surname>Doumas</surname> <given-names>LA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A mechanism for the cortical computation of hierarchical linguistic structure</article-title><source>PLOS Biology</source><volume>15</volume><elocation-id>e2000663</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2000663</pub-id><pub-id pub-id-type="pmid">28253256</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Martin</surname> <given-names>AE</given-names></name><name><surname>Doumas</surname> <given-names>LAA</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Predicate learning in neural systems: using oscillations to discover latent structure</article-title><source>Current Opinion in Behavioral Sciences</source><volume>29</volume><fpage>77</fpage><lpage>83</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2019.04.008</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McClelland</surname> <given-names>JL</given-names></name><name><surname>Elman</surname> <given-names>JL</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>The TRACE model of speech perception</article-title><source>Cognitive Psychology</source><volume>18</volume><fpage>1</fpage><lpage>86</lpage><pub-id pub-id-type="doi">10.1016/0010-0285(86)90015-0</pub-id><pub-id pub-id-type="pmid">3753912</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mehta</surname> <given-names>MR</given-names></name><name><surname>Lee</surname> <given-names>AK</given-names></name><name><surname>Wilson</surname> <given-names>MA</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Role of experience and oscillations in transforming a rate code into a temporal code</article-title><source>Nature</source><volume>417</volume><fpage>741</fpage><lpage>746</lpage><pub-id pub-id-type="doi">10.1038/nature00807</pub-id><pub-id pub-id-type="pmid">12066185</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The neural oscillations of speech processing and language comprehension: state of the art and emerging mechanisms</article-title><source>European Journal of Neuroscience</source><volume>48</volume><fpage>2609</fpage><lpage>2621</lpage><pub-id pub-id-type="doi">10.1111/ejn.13748</pub-id><pub-id pub-id-type="pmid">29055058</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Synchronous, but not entrained: exogenous and endogenous cortical rhythms of speech and language processing</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>1089</fpage><lpage>1099</lpage><pub-id pub-id-type="doi">10.1080/23273798.2019.1693050</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Meyer</surname> <given-names>L</given-names></name><name><surname>Sun</surname> <given-names>Y</given-names></name><name><surname>Martin</surname> <given-names>AE</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>“Entraining” to speech, generating language?</article-title><source>Language, Cognition and Neuroscience</source><volume>35</volume><fpage>1138</fpage><lpage>1148</lpage><pub-id pub-id-type="doi">10.1080/23273798.2020.1827155</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Michalareas</surname> <given-names>G</given-names></name><name><surname>Vezoli</surname> <given-names>J</given-names></name><name><surname>van Pelt</surname> <given-names>S</given-names></name><name><surname>Schoffelen</surname> <given-names>JM</given-names></name><name><surname>Kennedy</surname> <given-names>H</given-names></name><name><surname>Fries</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Alpha-Beta and gamma rhythms subserve feedback and feedforward influences among human visual cortical Areas</article-title><source>Neuron</source><volume>89</volume><fpage>384</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.12.018</pub-id><pub-id pub-id-type="pmid">26777277</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Monsell</surname> <given-names>S</given-names></name><name><surname>Doyle</surname> <given-names>MC</given-names></name><name><surname>Haggard</surname> <given-names>PN</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Effects of frequency on visual word recognition tasks: where are they?</article-title><source>Journal of Experimental Psychology: General</source><volume>118</volume><fpage>43</fpage><lpage>71</lpage><pub-id pub-id-type="doi">10.1037/0096-3445.118.1.43</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Monsell</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1991">1991</year><source>The Nature and Locus of Word Frequency Effects in Reading</source><publisher-name>Routledge</publisher-name></element-citation></ref><ref id="bib68"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Nieuwenhuijse</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><data-title>Dutch Word2Vec Model</data-title><source>GitHub</source><version designator="4014bf0">4014bf0</version><ext-link ext-link-type="uri" xlink:href="https://github.com/coosto/dutch-word-embeddings">https://github.com/coosto/dutch-word-embeddings</ext-link></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nieuwland</surname> <given-names>MS</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Do 'early' brain responses reveal word form prediction during language comprehension? A critical review</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>96</volume><fpage>367</fpage><lpage>400</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.11.019</pub-id><pub-id pub-id-type="pmid">30621862</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nolan</surname> <given-names>F</given-names></name><name><surname>Jeon</surname> <given-names>H-S</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Speech rhythm: a metaphor?</article-title><source>Philosophical Transactions of the Royal Society B: Biological Sciences</source><volume>369</volume><elocation-id>20130396</elocation-id><pub-id pub-id-type="doi">10.1098/rstb.2013.0396</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Keefe</surname> <given-names>J</given-names></name><name><surname>Recce</surname> <given-names>ML</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Phase relationship between hippocampal place units and the EEG theta rhythm</article-title><source>Hippocampus</source><volume>3</volume><fpage>317</fpage><lpage>330</lpage><pub-id pub-id-type="doi">10.1002/hipo.450030307</pub-id><pub-id pub-id-type="pmid">8353611</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O'Malley</surname> <given-names>S</given-names></name><name><surname>Besner</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reading aloud: qualitative differences in the relation between stimulus quality and word frequency as a function of context</article-title><source>Journal of Experimental Psychology: Learning, Memory, and Cognition</source><volume>34</volume><fpage>1400</fpage><lpage>1411</lpage><pub-id pub-id-type="doi">10.1037/a0013084</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Obleser</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Neural entrainment and attentional selection in the listening brain</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>913</fpage><lpage>926</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.08.004</pub-id><pub-id pub-id-type="pmid">31606386</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Petersen</surname> <given-names>RS</given-names></name><name><surname>Schultz</surname> <given-names>SR</given-names></name><name><surname>Lebedev</surname> <given-names>M</given-names></name><name><surname>Diamond</surname> <given-names>ME</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>The role of spike timing in the coding of stimulus location in rat somatosensory cortex</article-title><source>Neuron</source><volume>29</volume><fpage>769</fpage><lpage>777</lpage><pub-id pub-id-type="doi">10.1016/S0896-6273(01)00251-3</pub-id><pub-id pub-id-type="pmid">11301035</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Panzeri</surname> <given-names>S</given-names></name><name><surname>Macke</surname> <given-names>JH</given-names></name><name><surname>Gross</surname> <given-names>J</given-names></name><name><surname>Kayser</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural population coding: combining insights from microscopic and mass signals</article-title><source>Trends in Cognitive Sciences</source><volume>19</volume><fpage>162</fpage><lpage>172</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2015.01.002</pub-id><pub-id pub-id-type="pmid">25670005</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pariyadath</surname> <given-names>V</given-names></name><name><surname>Eagleman</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The effect of predictability on subjective duration</article-title><source>PLOS ONE</source><volume>2</volume><elocation-id>e1264</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0001264</pub-id><pub-id pub-id-type="pmid">18043760</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peelle</surname> <given-names>JE</given-names></name><name><surname>Davis</surname> <given-names>MH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Neural oscillations carry speech rhythm through to comprehension</article-title><source>Frontiers in Psychology</source><volume>3</volume><elocation-id>320</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2012.00320</pub-id><pub-id pub-id-type="pmid">22973251</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pellegrino</surname> <given-names>F</given-names></name><name><surname>Coupé</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>A cross-language perspective on speech information rate</article-title><source>Language</source><volume>87</volume><fpage>539</fpage><lpage>558</lpage><pub-id pub-id-type="doi">10.2307/23011654</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piantadosi</surname> <given-names>ST</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Zipf's word frequency law in natural language: a critical review and future directions</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>21</volume><fpage>1112</fpage><lpage>1130</lpage><pub-id pub-id-type="doi">10.3758/s13423-014-0585-6</pub-id><pub-id pub-id-type="pmid">24664880</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pluymaekers</surname> <given-names>M</given-names></name><name><surname>Ernestus</surname> <given-names>M</given-names></name><name><surname>Baayen</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2005">2005a</year><article-title>Articulatory planning is continuous and sensitive to informational redundancy</article-title><source>Phonetica</source><volume>62</volume><fpage>146</fpage><lpage>159</lpage><pub-id pub-id-type="doi">10.1159/000090095</pub-id><pub-id pub-id-type="pmid">16391500</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pluymaekers</surname> <given-names>M</given-names></name><name><surname>Ernestus</surname> <given-names>M</given-names></name><name><surname>Baayen</surname> <given-names>RH</given-names></name></person-group><year iso-8601-date="2005">2005b</year><article-title>Lexical frequency and acoustic reduction in spoken dutch</article-title><source>The Journal of the Acoustical Society of America</source><volume>118</volume><fpage>2561</fpage><lpage>2569</lpage><pub-id pub-id-type="doi">10.1121/1.2011150</pub-id><pub-id pub-id-type="pmid">16266176</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>The analysis of speech in different temporal integration windows: cerebral lateralization as ‘asymmetric sampling in time’</article-title><source>Speech Communication</source><volume>41</volume><fpage>245</fpage><lpage>255</lpage><pub-id pub-id-type="doi">10.1016/S0167-6393(02)00107-3</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Assaneo</surname> <given-names>MF</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Speech rhythms and their neural foundations</article-title><source>Nature Reviews Neuroscience</source><volume>21</volume><fpage>322</fpage><lpage>334</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0304-4</pub-id><pub-id pub-id-type="pmid">32376899</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Powers</surname> <given-names>DM</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Editor applications and explanations of zipf’s law</article-title><conf-name>New Methods in Language Processing and Computational Natural Language Learning</conf-name></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reinisch</surname> <given-names>E</given-names></name><name><surname>Sjerps</surname> <given-names>MJ</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The uptake of spectral and temporal cues in vowel perception is rapidly influenced by context</article-title><source>Journal of Phonetics</source><volume>41</volume><fpage>101</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1016/j.wocn.2013.01.002</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rimmele</surname> <given-names>JM</given-names></name><name><surname>Morillon</surname> <given-names>B</given-names></name><name><surname>Poeppel</surname> <given-names>D</given-names></name><name><surname>Arnal</surname> <given-names>LH</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Proactive sensing of periodic and aperiodic auditory patterns</article-title><source>Trends in Cognitive Sciences</source><volume>22</volume><fpage>870</fpage><lpage>882</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2018.08.003</pub-id><pub-id pub-id-type="pmid">30266147</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosen</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Temporal information in speech: acoustic, auditory and linguistic aspects</article-title><source>Philosophical Transactions of the Royal Society of London. Series B, Biological Sciences</source><volume>336</volume><fpage>367</fpage><lpage>373</lpage><pub-id pub-id-type="doi">10.1098/rstb.1992.0070</pub-id><pub-id pub-id-type="pmid">1354376</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schroeder</surname> <given-names>CE</given-names></name><name><surname>Lakatos</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Low-frequency neuronal oscillations as instruments of sensory selection</article-title><source>Trends in Neurosciences</source><volume>32</volume><fpage>9</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2008.09.012</pub-id><pub-id pub-id-type="pmid">19012975</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Ten Oever &amp; Martin</collab></person-group><year iso-8601-date="2021">2021</year><data-title>STiMCON</data-title><source>Software Heritage</source><version designator="swh:1:rev:873a2bf5c79fe2f828e72e14ef74db409d387854">swh:1:rev:873a2bf5c79fe2f828e72e14ef74db409d387854</version><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:cf831eabfe75473deb3aafac084e8af91398ae29;origin=https://github.com/sannetenoever/STiMCON;visit=swh:1:snp:fbce7be5ac6a1486f21dcc28e7a79b952d3e1c92;anchor=swh:1:rev:873a2bf5c79fe2f828e72e14ef74db409d387854">https://archive.softwareheritage.org/swh:1:dir:cf831eabfe75473deb3aafac084e8af91398ae29;origin=https://github.com/sannetenoever/STiMCON;visit=swh:1:snp:fbce7be5ac6a1486f21dcc28e7a79b952d3e1c92;anchor=swh:1:rev:873a2bf5c79fe2f828e72e14ef74db409d387854</ext-link></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ten Oever</surname> <given-names>S</given-names></name><name><surname>Sack</surname> <given-names>AT</given-names></name><name><surname>Wheat</surname> <given-names>KL</given-names></name><name><surname>Bien</surname> <given-names>N</given-names></name><name><surname>van Atteveldt</surname> <given-names>N</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Audio-visual onset differences are used to determine syllable identity for ambiguous audio-visual stimulus pairs</article-title><source>Frontiers in Psychology</source><volume>4</volume><elocation-id>331</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2013.00331</pub-id><pub-id pub-id-type="pmid">23805110</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ten Oever</surname> <given-names>S</given-names></name><name><surname>Hausfeld</surname> <given-names>L</given-names></name><name><surname>Correia</surname> <given-names>JM</given-names></name><name><surname>Van Atteveldt</surname> <given-names>N</given-names></name><name><surname>Formisano</surname> <given-names>E</given-names></name><name><surname>Sack</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A 7T fMRI study investigating the influence of oscillatory phase on syllable representations</article-title><source>NeuroImage</source><volume>141</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.011</pub-id><pub-id pub-id-type="pmid">27395392</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ten Oever</surname> <given-names>S</given-names></name><name><surname>Meierdierks</surname> <given-names>T</given-names></name><name><surname>Duecker</surname> <given-names>F</given-names></name><name><surname>De Graaf</surname> <given-names>TA</given-names></name><name><surname>Sack</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Phase-Coded oscillatory ordering promotes the separation of closely matched representations to optimize perceptual discrimination</article-title><source>iScience</source><volume>23</volume><elocation-id>101282</elocation-id><pub-id pub-id-type="doi">10.1016/j.isci.2020.101282</pub-id><pub-id pub-id-type="pmid">32604063</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ten Oever</surname> <given-names>S</given-names></name><name><surname>Sack</surname> <given-names>AT</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Oscillatory phase shapes syllable perception</article-title><source>PNAS</source><volume>112</volume><fpage>15833</fpage><lpage>15837</lpage><pub-id pub-id-type="doi">10.1073/pnas.1517519112</pub-id><pub-id pub-id-type="pmid">26668393</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Terao</surname> <given-names>M</given-names></name><name><surname>Watanabe</surname> <given-names>J</given-names></name><name><surname>Yagi</surname> <given-names>A</given-names></name><name><surname>Nishida</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Reduction of stimulus visibility compresses apparent time intervals</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>541</fpage><lpage>542</lpage><pub-id pub-id-type="doi">10.1038/nn.2111</pub-id><pub-id pub-id-type="pmid">18408716</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thézé</surname> <given-names>R</given-names></name><name><surname>Giraud</surname> <given-names>AL</given-names></name><name><surname>Mégevand</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The phase of cortical oscillations determines the perceptual fate of visual cues in naturalistic audiovisual speech</article-title><source>Science Advances</source><volume>6</volume><elocation-id>eabc6348</elocation-id><pub-id pub-id-type="doi">10.1126/sciadv.abc6348</pub-id><pub-id pub-id-type="pmid">33148648</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thompson</surname> <given-names>SP</given-names></name><name><surname>Newport</surname> <given-names>EL</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Statistical learning of syntax: the role of transitional probability</article-title><source>Language Learning and Development</source><volume>3</volume><fpage>1</fpage><lpage>42</lpage><pub-id pub-id-type="doi">10.1080/15475440709336999</pub-id></element-citation></ref><ref id="bib97"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ulrich</surname> <given-names>R</given-names></name><name><surname>Nitschke</surname> <given-names>J</given-names></name><name><surname>Rammsayer</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perceived duration of expected and unexpected stimuli</article-title><source>Psychological Research Psychologische Forschung</source><volume>70</volume><fpage>77</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1007/s00426-004-0195-4</pub-id><pub-id pub-id-type="pmid">15609031</pub-id></element-citation></ref><ref id="bib98"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vroomen</surname> <given-names>J</given-names></name><name><surname>Keetels</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Perception of intersensory synchrony: a tutorial review</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>72</volume><fpage>871</fpage><lpage>884</lpage><pub-id pub-id-type="doi">10.3758/APP.72.4.871</pub-id><pub-id pub-id-type="pmid">20436185</pub-id></element-citation></ref><ref id="bib99"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Zuidema</surname> <given-names>W</given-names></name></person-group><year iso-8601-date="2010">2010</year><source>A Syllable Frequency List for Dutch</source><publisher-name>Taalportaal</publisher-name></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68066.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Kösem</surname><given-names>Anne</given-names></name><role>Reviewing Editor</role><aff><institution>Max Planck Institute for Psycholinguistics; Donders Institute for Brain, Cognition and Behaviour, Radboud University; Lyon Neuroscience Research Center</institution><country>Netherlands</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Kösem</surname><given-names>Anne</given-names> </name><role>Reviewer</role><aff><institution>Lyon Neuroscience Research Center</institution><country>France</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Rimmele</surname><given-names>Johanna</given-names> </name><role>Reviewer</role><aff><institution>Max-Planck-Institute for Empirical Aesthetics</institution><country>Germany</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Doelling</surname><given-names>Keith</given-names> </name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2020.12.07.414425">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2020.12.07.414425v2">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>The manuscript is of broad interest to readers in the field of speech recognition and neural oscillations. The authors provide a computational model which, in addition to feedforward acoustic input, incorporates linguistic predictions as feedback, allowing a fixed oscillator to process non-isochronous speech. The model is tested extensively by applying it to a linguistic corpus, EEG and behavioral data. The article gives new insights to the ongoing debate about the role of neural oscillations and predictability in speech recognition.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Oscillatory tracking of pseudo-rhythmic speech is constrained by linguistic predictions&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, including Anne Kösem as the Reviewing Editor and Reviewer #1, and the evaluation has been overseen by Andrew King as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Johanna Rimmele (Reviewer #2); Keith Doelling (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>All reviewers had a very positive assessment of the manuscript. They find the described work highly novel and interesting. However, they also find the manuscript quite dense and complex, and suggest some clarifications in the description of the model and in the methods.</p><p>Please find a list of the recommendations below.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>1. First of all, I think that the concept of &quot;internal language model&quot; should be defined and described in more detail in the introduction. What does it mean when it is weak and when it is strong, for the producer and for the receiver?</p><p>2. It is still not fully clear to me what kind of information the internal oscillation is entraining to in the model. From Figure 1, it seems that the oscillation is driven by the acoustics only, but the phase of processing of linguistic units depends on their predictability.</p><p>3. If acoustic information arrives at the most excitable phase of the neural oscillation (as described in figure 1), and if predictability makes words arrive earlier, does it entail that more predictable words arrive at less excitable phases of the neural oscillation? What would be the computational advantage of this mechanism?</p><p>4. What is &quot;stimulus intensity&quot; in figure 5? Does it reflect volume or SNR?</p><p>5. Similarly what is &quot;amplitude&quot; in Figure 6?</p><p>6. L. 376-L. 439 &quot;N2 activation predicts either da or ga at 0.2 and 0.1 probability respectively.&quot; Please explain why the probabilities are not equal in the model, same for l. 445 &quot;intensity of /da/ would be at max 0.3. and of /ga/ 0.7&quot;.</p><p>7. Table 3: I feel that the first prediction is not actually a prediction, but a result of the article, as the first data shows that &quot;The more predictable a word, the earlier this word is uttered.&quot;</p><p>8. Table 3 and Figure 8A: I think that the second prediction that &quot;When there is a flat constraint distribution over an utterance (e.g., when probabilities are uniform over the utterance) the acoustics of speech should naturally be more rhythmic (Figure 8A).&quot; could be tested with the current data. In the speech corpus, are sentences with lower linguistic constraints more rhythmic?</p><p>9. Table 3: &quot;If speech timing matches the internal language model, brain responses should be more rhythmic even if the acoustics are not (Figure 8A).&quot; What do the authors mean by &quot;more rhythmic&quot;? Does it mean the brain follows more accurately the acoustics? Does it generate stronger internal rhythms that are distinct from the acoustic temporal structure?</p><p>10. Figure 5 C: &quot;Strength of 4 Hz power&quot;: what is the frequency bandwidth?</p><p>11. Figure 5 D: &quot; Slice of D&quot;, Slice of C?</p><p>12. L 442: &quot;propotions » -&gt; proportions.</p><p>13. Abstract: &quot;Our results reveal that speech tracking does not only rely on the input acoustics but instead entails an interaction between oscillations and constraints flowing from internal language model &quot; I think this claim is too strong, considering that the article does not present direct electrophysiological evidence.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>1. In the model the predictability is computed at the word-level, while the oscillator operates at the syllable level. The authors show different duration effects for syllables within words, likely related to predictability. Is there any consequence of this mismatch of scales?</p><p>2. Furthermore, could the authors clarify whether or not and how they think the model mechanism is different from top-down phase reset (e.g. l. 41). It seems that the excitability cycle at the intermediate word-level is shifted from being aligned to the 4 Hz oscillator though the linguistic feedback from layer l+1. Would that indicate a phase resetting at the word-level layer through the feedback?</p><p>3. The model shows how linguistic predictability can affect neuronal excitability in an oscillatory model, allowing to improve the processing of non-isochronous speech. I do not fully understand the claim that the linguistic predictability makes the processing (at the word-level) more isochronous, and why such isochronicity is crucial.</p><p>4. The authors showed that word frequency affects the duration of a word. Now the RNN model relates the predictability of a word (output) to the duration of the previous word W-1 (l. 187). Didn't one expect from Figure 1B that the duration of the actually predicted word is affected? How are these two effects related?</p><p>5. Title: is &quot;constrained&quot; the right word here, rather &quot;modulated&quot;? As we can process non-predictable speech.</p><p>6. See l. 129: &quot;In this way, oscillations do not have to shift their phase after every speech unit and can remain at a relatively stable frequency as long as the internal model of the speaker matches the internal model of the perceiver.&quot; It seems to me that in the model the authors introduce, the phase-shifting still occurs. Even though the oscillator component is fixed, the activation threshold fluctuations at the word-level are &quot;shifted&quot; due to the feedback. So there is no feedforward phase-reset, however, a phase-reset due to feedback?</p><p>7. l. 219: why was bigrams added as control variable?</p><p>8. l. 233 in l. 142 it says that only 2848 words were present in CELEX. Where the 4837 sentences consisting of the 2848 words?</p><p>9. Figure 2 D,E the labeling with ρ and p is confusing, I'd at least state consistently both, so one sees the difference.</p><p>10. Table 1 legend: could you add why the specific transformations were performed?</p><p>11. l. 204: the β coefficient is rather small compared to the duration of W-1 effect. The dependent variable onset-to-onset should be strongly correlated with the W-1 duration. I wonder if this is a problem?</p><p>12. l. 249: what is meant with &quot;after the first epoch&quot;?</p><p>13. l. 254: how local were these lengthening effects? Did the predictability based on the trained RNN strongly vary across words or rather vary on a larger scale i.e. full sentences being less predictable than others?</p><p>14. l. 268: Could you explain where the constants are coming from: like the 20 and 100 ms windows for inhibition and the values -0.2 and -3. The function inhibit(ta) is not clear to me. What is the output when Ta is 0 versus 1?</p><p>15. Figure 4: the legend is very short, adding some description what the figure illustrates would make it easier to follow. The small differences in early/late activation are hard to see, particularly for the 4th row. Maybe it would help to add lines?</p><p>16. Figure 5 B: could you clarify the effect at late stim times relative to isochronous, i.e. why the supra time relative to isochronous decreases for highly predictable stimuli. I assume this is to the inhibition function?</p><p>17. How is the connectivity between layers defined? Is it symmetric for feedforward and feedback?</p><p>18. l. 294/l. 205: &quot;with a delay of 0.9*ω seconds, which then decays at 0.01 unit per millisecond and influences the l-level at a proportion of 1.5.&quot; where are the constants coming from?</p><p>19. l. 347: &quot;the processing itself can actually be closer to isochronous than what can be solely extracted from the stimulus&quot;. This refers to Figure 5 D I assume. Did you directly compare the acoustics and the model output with respect to isochrony?</p><p>20. l. 437-438: I am not fully understanding these choices: why is N1 represented by N2? Why is the probability of da and ga uneaven, and why are there nodes for da and ga (Nda, Nga) plus a node N2 which predicts both with different probability?</p><p>21. Figure 5: why is the power of the high-high predictable condition the lowest. Is this an artifact of the oscillator in the model being fixed at 4 Hz or related to the inhibition function? High-high should like low-low result in rather regular, but faster acoustics?</p><p>22. l. 600: &quot;The perceived rhythmicity&quot; In my view speech has been suggested to be quasi-rhythmic, as (1) some consistency in syllable duration has been observed within/across languages, and (2) as (quasi-)rhythmicity seemed a requirement to explain how segmentation of speech based on oscillations could work in the absence of simple segmentation cues (i.e. pauses between syllables). While one can ask when something is &quot;rhythmic enough&quot; to be called rhythmic, I don't understand why this is related to &quot;perceived rhythmicity&quot;.</p><p>23. l. 604: interesting thought!</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>1. An important question is how the authors relate these findings to the Giraud and Poeppel, 2012 proposal which really focuses on the syllable. Would you alter the hypothesis to focus on the word level? Or remain at the syllable level and speed up and low down the oscillator depending on the predictability of each word? It would be interesting to hear the authors thoughts on how to manage the juxtaposition of syllable and word processing in this framework.</p><p>2. The authors describe the STiMCON model as having an oscillator with frequency set to the average stimulus rate of the sentence. But how an oscillator can achieve this on its own (without the hand of its overloads) is unclear particularly given a pseudo-rhythmic input. The authors freely accept this limitation. However, it is worth noting that the ability for an oscillator mechanism to do this under pseudorhythmic context is more complicated than it might seem, particularly once we include that the stimulus rate might change from the beginning to the end of a sentence and across an entire discourse.</p><p>3. The analysis of the naturalistic dataset shows a nice correlation between the estimated time shifts predicted by the model and the true naturalistic deviations. However, I find it surprising that there is so little deviation across the parameters of the oscillator (Figure 6A). What should we take from the fact that an oscillator aligned in anti-phase from the with the stimulus (which would presumably show the phase code only stimulus offsets), still shows a near equal correlation with true timing deviations. Furthermore, while the R2 shows that the predictions of the model co-vary with the true values, I'm curious to know how accurately they are predicted overall (in terms of mean squared error for example). Does the model account for deviations from rhythmicity of the right magnitude?</p><p>4. Lastly, it is unclear to what extent the oscillator is necessary to find this relative time shift. A model comparison between the predictions of the STiMCON and the RNN predictions on their own (à la Figure 3) would help to show how much the addition of the oscillation improves our predictions. Perhaps this is what is meant by the &quot;non-transformed R2&quot; but this is unclear.</p><p>5. Figure 7 shows a striking result demonstrating how the model can be used to explain an interesting finding that phase of an oscillation can bias perception towards da or ga. The initial papers consider this result to be explained by delays in onset between visual and auditory stimuli whereas this result explains it in terms of the statistical likelihood each syllable. It is a nice reframing which helps me to better understand the previous result.</p><p>6. The authors show that syllable lengths are determined in part by the predictability of the word it is a part of. While the authors have reasonably restricted themselves to a single hierarchical level, the point invites the question as to whether all hierarchical levels are governed by similar processes. Should syllables accelerate from beginning to end of a word? Or in more or less predictable phrases?</p><p>7. Figure 5 shows how an oscillator mechanism can force pseudo-rhythmic stimuli into a more rhythmic code. The authors note that this can be done either by slowing responses to early stimuli and quickening responses to later ones, or by dropping (nodes don't reach threshold) stimuli too far outside the range of the oscillation. The first is an interesting mechanism, the second is potentially detrimental to processing (although it could be used as a means for filtering out noise). The authors should make clear how much deviation is required to invoke the dropping out mechanism and how this threshold relates to the naturalistic case. This would give the reader a clearer view of the flexibility of this model.</p><p>8. I found Figure 5 very difficult to understand and had to read and read it multiple times to feel like I could get a handle on it. I struggled to get a handle on why supra time was shorter and shorter the later the stimulus was activated. It should reverse at some point as the phase goes back into lower excitability, right? The current wording is very unclear on this point. In addition, the low-high, high-low analysis is unclear because the nature of the stimuli is unclear. I think an added figure panel to show how these stimuli are generated and manipulated would go a long way here.</p><p>9. The prediction of behavioral data in Figure 7 is striking but the methods could be improved. Currently, the authors bin the output of the model to be 0, 0.5 or 1 which requires some maneuvering to effectively compare it with the sinewave model. They could instead use a continuous measure (either lag of activation between da and ga, or activation difference) as a feature in a logistic regression to predict the human subject behavior.</p><p>10. I'm not sure but I think there is a typo in line 383-384. The parameter for feedback should read Cl+1◊ l * Al+1,T. Note the + sign instead of the -. Or I have misunderstood something important.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Tracking of pseudo-rhythmic speech is modulated by linguistic predictions in an oscillating computational model&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Andrew King (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been greatly improved, and only these issues need to be addressed, as outlined below:</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>I want to thank the authors for the great effort revising the manuscript. The manuscript has much improved. I only have some final small comments.</p><p>Detailed comments</p><p>l. 273-275: In my opinion: This is because the oscillator is set as a rigid oscillator in the model that is not affected by the word level layer activation; however, as the authors already discuss this topic, this is just a comment.</p><p>l. 344: &quot;the processing itself&quot; I'd specify: &quot;the processing at the word layer&quot;.</p><p>l. 557/558: Rimmele et al., (2018) do discuss that besides the motor system, predictions from higher-level linguistic processing might affect auditory cortex neuronal oscillations through phase resetting. Top-down predictions affecting auditory cortex oscillations is one of the main claims of the paper. Thus, this paper seems not a good example for proposals that exclude when-to-what interactions. In my view the claims are rather consistent with the ones proposed here, although Rimmele et al., do not detail the mechanism and differ from the current proposal in that they suggest phase resetting. Could you clarify?</p><p>l 584 ff.: &quot;This idea diverges from the idea that entrainment should per definition occur on the most excitable phase of the oscillation [3,15].&quot; Maybe rephrase: &quot;This idea diverges from the idea that entrainment should align the most excitable phase of the oscillation with the highest energy in the acoustics [3,15].&quot;</p><p>l. 431: &quot;The model consists of four nodes (N1, N2, Nda, and Nga) at which N1 activation predicts a second unspecific stimulus (S2) represented by N2 at a predictability of 1. N2 activation predicts either da or ga at 0.2 and 0.1 probability respectively.&quot;</p><p>This is still hard to understand for me. E.g. What is S2, is this either da or ga, wouldn't their probability have to add up to 1?</p><p>Wording</p><p>l. 175/176: sth is wrong with the sentence.</p><p>l. 544: &quot;higher and syllabic&quot;? (sounds like sth is wrong in the wording)</p><p>l. 546: &quot;within more frequency&quot; (more frequent or higher frequency?)</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.68066.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>All reviewers had a very positive assessment of the manuscript. They find the described work highly novel and interesting. However, they also find the manuscript quite dense and complex, and suggest some clarifications in the description of the model and in the methods.</p><p>Please find a list of the recommendations below.</p><p>Reviewer #1 (Recommendations for the authors):</p><p>1. First of all, I think that the concept of &quot;internal language model&quot; should be defined and described in more detail in the introduction. What does it mean when it is weak and when it is strong, for the producer and for the receiver?</p></disp-quote><p>We define internal language model as the individually acquired statistical and structural knowledge of language stored in the brain. A virtue of such an internal language model is that it can predict the most likely future input based on the currently presented speech information. If a language model creates strong predictions, we call it a strong model. In contrast, a weak model creates no or little predictions about future input (note that the strength of individual predictions depends not only on the capability of the system to create a prediction, but also on the available information). If a node represents a speech unit that is likely to be spoken next, a strong internal language model will sensitize this node and it will therefore be active earlier, that is, on a less excitable phase of the oscillation.</p><p>The above explanation has been included in the introduction.</p><disp-quote content-type="editor-comment"><p>2. It is still not fully clear to me what kind of information the internal oscillation is entraining to in the model. From Figure 1, it seems that the oscillation is driven by the acoustics only, but the phase of processing of linguistic units depends on their predictability.</p></disp-quote><p>The entrainment proper is indeed still the acoustics. However, compared to other models in which oscillations are very strongly coupled to this acoustic envelope by aligning the most excitable phase as a consequence of the acoustic phase shifts (Doelling et al., 2019) or proactively dependent on temporal predictions (Rimmele et al., 2018), we propose that it is not necessary to change the phase of the ongoing oscillation in response to the acoustics to optimally process pseudo-rhythmic speech. As such, we view the model as weakly coupled relatively to more strongly coupled oscillator models. The phase of the oscillation does not need to change to every phase shift in the acoustics. We propose that the model can entrain to the average speech rate, however, we acknowledge in the updated manuscript we do not answer how this can be done. We have added a discussion on this point in the discussion which now reads: “We aimed to show that a stable oscillator can be sensitive to temporal pseudo-rhythmicities when these shifts match predictions from an internal linguistic model (causing higher sensitivity to these nodes). In this way we show that temporal dynamics in speech and the brain cannot be isolated from processing the content of speech. This is contrast with other models that try to explain how the brain deals with pseudo-rhythmicity in speech (Giraud and Poeppel, 2012, Rimmele et al., 2018, Doelling et al., 2019), which typically do not take into account that the content of a word can influence the timing. Phase resetting models can only deal with pseudo-rhythmicity by shifting the phase of ongoing oscillations in response to a word that is off-set to the mean frequency of the input (Giraud and Poeppel, 2012, Doelling et al., 2019). We believe that this goes beyond the temporal and content information the brain can extract from the environment which has what/when interactions. However, our current model does not have an explanation of how the brain can actually entrain to an average speech rate. This is much better described in dynamical systems theories in which this is a consequence of the coupling strength between internal oscillations and speech acoustics (Doelling et al., 2019, Assaneo et al., 2021). However, these models do not take top-down predictive processing into account. Therefore, the best way forward is likely to extend coupling between brain oscillations and speech acoustics (Poeppel and Assaneo, 2020) with the coupling of brain oscillations to brain activity patterns of internal models (Cumin and Unsworth, 2007).”</p><disp-quote content-type="editor-comment"><p>3. If acoustic information arrives at the most excitable phase of the neural oscillation (as described in figure 1), and if predictability makes words arrive earlier, does it entail that more predictable words arrive at less excitable phases of the neural oscillation? What would be the computational advantage of this mechanism?</p></disp-quote><p>Indeed, more predictable words arrive at a less excitable phase of the oscillation. This is an automatic consequence of the statistics in the environment in which more predictable words are uttered earlier. The brain could utilize these statistical patterns (we can infer that earlier uttered words are more predictable). If we indeed do this, temporal information in the brain contains content information. But how would the brain code for this? We propose that this can be done by phase-of-firing, such that the phase at which neurons are active is relevant for the content of the item to be processed. Computationally it would be advantageous to be able to separate different representations in the brain by virtue of phase coding. As such, you don’t only have a spatial code of information (which neuron is active), but also a temporal code of information (when is a neuron active). This leads to a redundancy of coding which has a lot of computational advantages in a noisy world (e.g. Barlow, 2001).</p><p>The idea of phase coding has been proposed in the past (Jensen and Lisman, 2005; Kayser et al., 2009; Panzeri et al., 2001 Hopfield, 1995). Indeed, there is evidence that time or phase of firing contains content information (Siegel et al., 2012; O’Keefe and Recce, 1993). In some of these theoretical accounts, the first active node actually provides more information that the less-specific activation occurring later (Hopfield, 1995). One might say a neuron that is active already at a low excitable phase contains more information about the relevance of the activation than many neurons that are going to be active at a very excitable phase. Indeed, neurons in rest are found to be active at excitable phases (Haegens et al., 2011). In a similar vein, it has also been suggested that α power/phase modulates not the sensitivity, but mostly the bias to detect something (Iemi et al., 2016), suggesting that high excitable points do not improve sensitivity, but merely the idea that something was perceived. In sum, computational advantages of this model relate to having information about time relate to information on content by virtue of a phase code. To make this clear we have added a section in the discussion. It now reads: “A natural feature of our model is that time can act as a cue for content implemented as a phase code (Jensen et al., 2012, Lisman and Jensen, 2013). This code unravels as an ordered list of predictability strength of the internal model. This idea diverges from the idea that entrainment should per definition occur on the most excitable phase of the oscillation (Giraud and Poeppel, 2012, Rimmele et al., 2018). Instead, this type of phase could increase the brain representational space to separate information content (Panzeri, Petersen et al. 2001, Lisman and Jensen, 2013). We predict that if speech nodes have a different base activity, ambiguous stimulus interpretation should be dependent on the time/phase of presentation (see (Ten Oever and Sack, 2015, Ten Oever et al., 2020))”.</p><disp-quote content-type="editor-comment"><p>4. What is &quot;stimulus intensity&quot; in figure 5? Does it reflect volume or SNR?</p></disp-quote><p>We have added this information in the figure legend. It reflects the overall amplitude of the input in the model. See Figure 5A. We now consistently refer to intensity when referring to the amplitude of the input.</p><disp-quote content-type="editor-comment"><p>5. Similarly what is &quot;amplitude&quot; in Figure 6?</p></disp-quote><p>Amplitude refers to the amplitude of the sinus in the model. This information is now in the figure legend.</p><disp-quote content-type="editor-comment"><p>6. L. 376-L. 439 &quot;N2 activation predicts either da or ga at 0.2 and 0.1 probability respectively.&quot; Please explain why the probabilities are not equal in the model, same for l. 445 &quot;intensity of /da/ would be at max 0.3. and of /ga/ 0.7&quot;.</p></disp-quote><p>We regret that this was unclear. Phase coding of information only occurs when the internal model of STiMCON has different probabilities of predicting the content of the next word. Otherwise, the nodes will be active at the same time. The assumption for the /da/ and /ga/ having different probabilities is reasonable as the /d/ and /g/ consonant have a different overall proportion in the Dutch language (with /d/ being more frequent than the /g/). As such, we would expect the overall /d/ representation in the brain to be active at lower thresholds than then the /g/ representation. We have clarified this now in the manuscript: “N2 activation predicts either da or ga at 0.2 and 0.1 probability respectively. This uneven prediction of /da/ and /ga/ is justified as /da/ is more prevalent in the Dutch language as /ga/ (Zuidema, 2010) and it thus has a higher predicted level of occurring.”</p><p>l. 445 refers to what input we gave to the model. We gave the model the input for a fully unambiguous /da/, a fully unambiguous /ga/, and morphs in between. This was to demonstrate the behavior of the model to show that only at the ambiguous stimulation we would find a phase code of information.</p><disp-quote content-type="editor-comment"><p>7. Table 3: I feel that the first prediction is not actually a prediction, but a result of the article, as the first data shows that &quot;The more predictable a word, the earlier this word is uttered.&quot;</p></disp-quote><p>We agree and have removed it from the table.</p><disp-quote content-type="editor-comment"><p>8. Table 3 and Figure 8A: I think that the second prediction that &quot;When there is a flat constraint distribution over an utterance (e.g., when probabilities are uniform over the utterance) the acoustics of speech should naturally be more rhythmic (Figure 8A).&quot; could be tested with the current data. In the speech corpus, are sentences with lower linguistic constraints more rhythmic?</p></disp-quote><p>We thank the reviewer for the interesting suggestion. We indeed do have the data to investigate whether acoustics are more rhythmic when they are less predictable across the sentence. To investigate this question, we extracted the RNN prediction for 10 subsequent words. Then we extracted the variance of the prediction across those 10 words and extracted the word onset itself. We created a time course at which word onset were set to 1 (at a sampling rate of 100 Hz). Then we performed an FFT and extracted z-transformed power values over a 0-15 Hz interval. The power at the maximum power value with the theta range (3-8Hz) was extracted. These max z-scores were correlated with the log transform of the variance (to normalize the skewed variance distribution; Figure 3E). We found a weak, but significant negative correlation (r = -0.062, p &lt; 0.001; Figure 3D) in line with our hypothesis. This suggests that the more variable the predictions within a sentence, the lower the peak power value is. When we repeated the analysis on the envelope, we did not find a significant effect. We have added this analysis in the main manuscript and added two panels to figure 2.</p><p>We do believe that a full answer to this question requires more experimental work and therefore keep it as a prediction in the table.</p><disp-quote content-type="editor-comment"><p>9. Table 3: &quot;If speech timing matches the internal language model, brain responses should be more rhythmic even if the acoustics are not (Figure 8A).&quot; What do the authors mean by &quot;more rhythmic&quot;? Does it mean the brain follows more accurately the acoustics? Does it generate stronger internal rhythms that are distinct from the acoustic temporal structure?</p></disp-quote><p>This would refer to the brain having a stronger isochronous response for non-isochronous than isochronous acoustics. This is based on the results in figure 6 (previous figure 5). In figure 6 we show that when the internal model predicts the next word at alternatingly high or low predictabilities, the model’s response is not most isochronous (strongest 4 Hz response) when the acoustics are isochronous, but rather when the acoustics are shifted in line with the internal model (more predictable words occurring earlier). We would expect the same in the brain’s responses. We think the wording rhythmic is not correct in this context and should rather refer to isochronous. We have updated the text to now refer to isochrony.</p><disp-quote content-type="editor-comment"><p>10. Figure 5 C: &quot;Strength of 4 Hz power&quot;: what is the frequency bandwidth?</p></disp-quote><p>This is the peak activity. We clarified this now in the text. You can see from (current) Figure 6A+D that the response overall is also very peaky (by nature of the stimulation and the isochrony of the oscillation that we enter in the model).</p><disp-quote content-type="editor-comment"><p>11. Figure 5 D: &quot; Slice of D&quot;, Slice of C?</p></disp-quote><p>This is the peak activity. We clarified this now in the text. You can see from (current) Figure 6A+D that the response overall is also very peaky (by nature of the stimulation and the isochrony of the oscillation that we enter in the model).</p><disp-quote content-type="editor-comment"><p>12. L 442: &quot;propotions » -&gt; proportions.</p></disp-quote><p>Adjusted accordingly.</p><disp-quote content-type="editor-comment"><p>13. Abstract: &quot;Our results reveal that speech tracking does not only rely on the input acoustics but instead entails an interaction between oscillations and constraints flowing from internal language model &quot; I think this claim is too strong, considering that the article does not present direct electrophysiological evidence.</p></disp-quote><p>We agree and regret this strong claim. We have updated the text and it now reads: “Our results suggest that speech tracking does not have to rely only on the acoustics but could also entail an interaction between oscillations and constraints flowing from internal language models.”</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>1. In the model the predictability is computed at the word-level, while the oscillator operates at the syllable level. The authors show different duration effects for syllables within words, likely related to predictability. Is there any consequence of this mismatch of scales?</p></disp-quote><p>The current model does indeed operate on the word level, while oscillatory models operate on the syllabic level. We do not claim by this that predictions per see only work on a word level. In contrary, we believe that ultimately also syllabic level predictions as well as higher level linguistic predictions can be made to influence speech processing. Therefore, our model is incomplete, but serves the purpose to demonstrate how internal language models can influence speech timing as well as perceptual tracking.</p><p>Our choice of the word level was mostly practical. We choose in the current manuscript to start with a word level prediction as this is the starting point commonly available and applied for RNNs. RNNs often work on the word level and not on the syllabic level. For example, this allowed use to use highly trained word level embeddings as a starting point for our LSTM. We are not aware of pre-trained syllabic embeddings that can achieve the same thing. As mentioned above, the temporal shift in STiMCON would also be predicted based on syllabic prediction. Therefore, the only results that are really affected by this notion are the results of figure 6 (or current figure 7). Predicting the temporal shift would likely have been benefitted from also adding predictions in temporal shifts based on syllabic and higher order linguistic predictions.</p><p>We now add a note about the level of processing that might affect the results of current figure 7 as well as a paragraph in the discussion that mentions that the model should have multiple levels operating at different levels of the linguistic hierarchy.</p><p>The result section now reads: “Note that the current prediction only operated on the word node (to which we have the RNN predictions), while full temporal shifts are probably better explained by word, syllabic and phrasal predictions.”</p><p>The discussion now reads: “The current model is not exhaustive and does not provide a complete explanation of all the details of speech processing in the brain. For example, it is likely that the primary auditory cortex is still mostly modulated by the acoustic pseudo-rhythmic input and only later brain areas follow more closely the constraints posed by the language model of the brain. Moreover, we now focus on the word level, while many tracking studies have shown the importance of syllabic temporal structure (Luo and Poeppel, 2007, Ghitza, 2012, Giraud and Poeppel, 2012) as well as the role of higher order linguistic temporal dynamics (Meyer, Sun et al. 2019, Kaufeld et al., 2020).”</p><disp-quote content-type="editor-comment"><p>2. Furthermore, could the authors clarify whether or not and how they think the model mechanism is different from top-down phase reset (e.g. l. 41). It seems that the excitability cycle at the intermediate word-level is shifted from being aligned to the 4 Hz oscillator though the linguistic feedback from layer l+1. Would that indicate a phase resetting at the word-level layer through the feedback?</p></disp-quote><p>The model is different as it does not assume that the top-down influence needs to reflect a phase reset. A phase reset would indicate a shift in the ongoing phase of the oscillator (Winfree 2001). However, the feedback in our model does not shift the phase of the oscillator. The phase of the oscillator remains stable, only the phase at which a node in a model is active changes. The feedback is implemented as a constant input that decays over time, it does not actively interfere with the phase of the oscillator at the word level. We propose here that if the internal model of the perceiver and receiver perfectly align the oscillator can remain entrained to the average speaker rate without shifting phase at every word, but instead can take the linguistic predictions into account.</p><p>We don’t claim phase resetting does not occur. We have acknowledged that our model does not account for this (yet) in the discussion. However, in our model phase resetting would be required when the predictions of perceiver and receiver don’t match through which the expected timing don’t match. To clarify the difference with phase reset models we have added a section in the discussion. It now reads: “We aimed to show that a stable oscillator can be sensitive to temporal pseudo-rhythmicities when these shifts match predictions from an internal linguistic model (causing higher sensitivity to these nodes). In this way we show that temporal dynamics in speech and the brain cannot be isolated from processing the content of speech. This is contrast with other models that try to explain how the brain deals with pseudo-rhythmicity in speech (Giraud and Poeppel 2012, Rimmele et al., 2018, Doelling et al., 2019), which typically do not take into account that the content of a word can influence the timing. Phase resetting models can only deal with pseudo-rhythmicity by shifting the phase of ongoing oscillations in response to a word that is off-set to the mean frequency of the input (Giraud and Poeppel, 2012, Doelling et al., 2019). We believe that this goes beyond the temporal and content information the brain can extract from the environment which has what/when interactions. However, our current model does not have an explanation of how the brain can actually entrain to an average speech rate. This is much better described in dynamical systems theories in which this is a consequence of the coupling strength between internal oscillations and speech acoustics (Doelling et al., 2019, Assaneo et al., 2021). However, these models do not take top-down predictive processing into account. Therefore, the best way forward is likely to extend coupling between brain oscillations and speech acoustics (Poeppel and Assaneo, 2020) with the coupling of brain oscillations to brain activity patterns of internal models (Cumin and Unsworth, 2007).”</p><disp-quote content-type="editor-comment"><p>3. The model shows how linguistic predictability can affect neuronal excitability in an oscillatory model, allowing to improve the processing of non-isochronous speech. I do not fully understand the claim that the linguistic predictability makes the processing (at the word-level) more isochronous, and why such isochronicity is crucial.</p></disp-quote><p>The main point of figure 5 (current figure 6) is to show that acoustic time and brain time do not necessarily have to have the same relation. We show that isochronously presented acoustic input does not need to lead to the most isochronous brain responses. Why does the model behave this way? Effectively the linguistic feedback changes the time at which different word nodes are active and thereby does not make it directly linked to the acoustic input. This creates an interesting dynamic that causes the model’s response to follow a stronger 4 Hz response for non-isochronous acoustic input. This dynamic was not necessarily predicted by us in advance, but is what came out of the model. The exact dynamics can be seen in Figure 6 —figure supplement 2. It seems that due to the interactions between acoustic input and feedback, higher peak activation at a 4 Hz rhythm is reached at an acoustic offset that matches the linguistic predictions (earlier presentation when words are more predictable).</p><p>We don’t necessarily believe isochronicity is crucial for the brain to operate and it can deviate from isochronicity. But isochronicity provides two clear benefits: (1) strong temporal predictions and (2) higher processing efficiency (Schroeder and Lakatos, 2009). If it is possible for the brain to maintain a stable oscillator, this would increase the efficiency relative to changing its phase after the presentation of every word. If the brain needs to shift its phase after every word, we even wonder what the computational benefit of an oscillator is. If simply the acoustics are followed, why would we need an oscillator at all?</p><p>We claim that the brain shows a more isochronous response when the linguistic predictions match the temporal shifts (earlier onsets for more predictable words). If this is not the case the isochrony is also lower. As said above, this provides the benefit that there is higher processing efficiency. But in reality, the predictions of a receiver will not always match the exact timing of the production of the producer. Therefore, some phase shifts are likely still needed (not modelled here).</p><disp-quote content-type="editor-comment"><p>4. The authors showed that word frequency affects the duration of a word. Now the RNN model relates the predictability of a word (output) to the duration of the previous word W-1 (l. 187). Didn't one expect from Figure 1B that the duration of the actually predicted word is affected? How are these two effects related?</p></disp-quote><p>We expect that when words are easier accessible, they are uttered early. This is why we created the RNN based on prediction the onset of the next word (see prediction in Figure 1B). However, based on previous literature in deed it could also be expected that word duration itself is affected by the predictability (Lehiste, 1972, Pluymaekers et al., 2005). However, most of these effects have been shown on mean duration of words and not that the duration of words within a sentence is modulated in the context of the full sentence above what can be explained by word frequency. Indeed, we replicate the findings of individual words showing that word frequency (likely affecting accessibility to a word) affect the mean duration of this word. To test if this extends within the context of predictability within a sentence, we reran our linear regression model, but used word duration as our dependent variable. As other control variables we included the bigram, the frequency of the current word, the mean duration of the current word and the syllable rate. Note that only effects will be significant in which the effects are stronger than can be expected based on the mean duration. Results can be found in Table 1.</p><p>We found no effect of the RNN prediction on the overall duration of word (above what can be explained by the other control factors). Other factors did show effects, such as the word frequency and bigram (p &lt; 0.001). Interesting and unexpected, these resulted in longer duration, not shorter durations (positive t-values). At this moment we do not have an explanation for this effect, but it could be that this lengthening is a consequence of the earlier onset of the individual word by which a speaker tries to keep to the average rate. Alternatively, it is possible that this positive relation is a consequence of the predictability of the word following, such that words get shorter if the word after it is more predictable (as we showed in the manuscript). However, in this case we would also expect the RNN prediction to be significant. We have now reported on these effects in the supplementary materials, but do not have a direct explanation for the direction of this effect.</p><disp-quote content-type="editor-comment"><p>5. Title: is &quot;constrained&quot; the right word here, rather &quot;modulated&quot;? As we can process non-predictable speech.</p></disp-quote><p>This is fair and we changed it to modulated. Indeed, we can perfectly well process non-predictable speech as well.</p><disp-quote content-type="editor-comment"><p>6. See l. 129: &quot;In this way, oscillations do not have to shift their phase after every speech unit and can remain at a relatively stable frequency as long as the internal model of the speaker matches the internal model of the perceiver.&quot; It seems to me that in the model the authors introduce, the phase-shifting still occurs. Even though the oscillator component is fixed, the activation threshold fluctuations at the word-level are &quot;shifted&quot; due to the feedback. So there is no feedforward phase-reset, however, a phase-reset due to feedback?</p></disp-quote><p>This is fair and we changed it to modulated. Indeed, we can perfectly well process non-predictable speech as well.</p><disp-quote content-type="editor-comment"><p>7. l. 219: why was bigrams added as control variable?</p></disp-quote><p>We regret this was unclear. We were interested to investigate if a bigger context (as measured with the RNN) provides more information than a bigram which only investigates the statistics regarding two previous words.</p><disp-quote content-type="editor-comment"><p>8. l. 233 in l. 142 it says that only 2848 words were present in CELEX. Where the 4837 sentences consisting of the 2848 words?</p></disp-quote><p>We did two rounds of checking our dictionary of words. In the first round we investigated if the words were present in the word2vec embeddings (otherwise we couldn’t use them for the RNN). If not, they were marked with a &lt;unknown&gt; label. The RNN was ran with all these words. This refers to the 4837 sentences. In total there were 3096 unique words.</p><p>For the regression analyses, we further wanted to extract parameters of the individual words. Thus, we investigated if the words were present in CELEX. This was the case of 2848 of 3096 words. For the regression analyses we only included the words so that we could estimate the relevant parameters. This was when the W-1 word was in CELEX (for the analysis in 4 this was related to the current word). For the regression we therefore didn’t include all the sentences going into the RNN.</p><disp-quote content-type="editor-comment"><p>9. Figure 2 D,E the labeling with ρ and p is confusing, I'd at least state consistently both, so one sees the difference.</p></disp-quote><p>We now also report on the p-value in the figure.</p><disp-quote content-type="editor-comment"><p>10. Table 1 legend: could you add why the specific transformations were performed?</p></disp-quote><p>The transformations were performed to ensure that our factors were close-to-normal in order to enter in the regression analyses. This information is now in the main manuscript. We would like to note that our analysis is robust against changes in the transformations. If we don’t perform any transformation the same results hold.</p><disp-quote content-type="editor-comment"><p>11. l. 204: the β coefficient is rather small compared to the duration of W-1 effect. The dependent variable onset-to-onset should be strongly correlated with the W-1 duration. I wonder if this is a problem?</p></disp-quote><p>Indeed, the word duration has the strongest relation to the onset-to-onset difference (as is of course intuitive, but also evident from the β coefficient). To capture this variance, we added this variable into the regression analyses. When performing a regression analyses it is useful to include factors that explain variance in your model. This ensure that the factor of interest (here RNN prediction) captures any variance that cannot be related to variance already explained by the other factors. Therefore, we don’t feel that this is a big problem, but actually an intended and expected effect.</p><disp-quote content-type="editor-comment"><p>12. l. 249: what is meant with &quot;after the first epoch&quot;?</p></disp-quote><p>We have clarified it. RNN are normally trained with different steps (or epochs). After every epoch the weights in the model are adjusted to reduce the overall error of the fit. But this term might be specific to keras, and not to machine learning in general. We now state: “entering the RNN predictions after the first training cycle (of a total of 100)”.</p><disp-quote content-type="editor-comment"><p>13. l. 254: how local were these lengthening effects? Did the predictability based on the trained RNN strongly vary across words or rather vary on a larger scale i.e. full sentences being less predictable than others?</p></disp-quote><p>To answer this question, we investigated for individual words and sentence positions whether the RNN prediction was generally higher. Indeed, for some words average prediction was higher. Both for the word that was predicted as well as for the last word before the prediction. For sentence position it seemed that for words very early in the sentence there was a lower overall prediction than for words later in the sentence. We have added a figure as a supporting figure to the manuscript.</p><disp-quote content-type="editor-comment"><p>14. l. 268: Could you explain where the constants are coming from: like the 20 and 100 ms windows for inhibition and the values -0.2 and -3. The function inhibit(ta) is not clear to me. What is the output when Ta is 0 versus 1?</p></disp-quote><p>The inhibition function reflects the excitation and inhibition of individual nodes in the model and is always relative to the time of activation of that node (Ta). When Ta is less than 20 (in ms), the node is activated (-3* inhibition factor), when Ta is between 20 and 100 there is strong inhibition, and after that, there is a base inhibition on the node. So, for Ta 0 versus 1 the output is both -3*BaseInhibition. The point of this activation function is to have a nonlinear activation, loosely resembling the nonlinear activation pattern in the brain.</p><p>The values for inhibition reflect rather early excitation (20 ms) and longer lasting inhibition (100 ms). We acknowledge that these numbers are only loosely related to neurophysiological time scales and is of course highly dependent on the region of interest and the local and distant connections. However, the exact timing is not critical to the main output factors of the model (phase coding and temporal sensitivity for higher predictable scales) as long as there is excitation and inhibition. We have added the rationale behind our parameter choice in the manuscript.</p><disp-quote content-type="editor-comment"><p>15. Figure 4: the legend is very short, adding some description what the figure illustrates would make it easier to follow. The small differences in early/late activation are hard to see, particularly for the 4th row. Maybe it would help to add lines?</p></disp-quote><p>We have clarified the description and have added a zoom-in on the relevant early/late activations for the critical word including relevant lines. We hope that this has improved the readability of this figure.</p><disp-quote content-type="editor-comment"><p>16. Figure 5 B: could you clarify the effect at late stim times relative to isochronous, i.e. why the supra time relative to isochronous decreases for highly predictable stimuli. I assume this is to the inhibition function?</p></disp-quote><p>This is indeed related to the inhibition function. As soon as high enough activation reaches a node, the node will reach suprathreshold activation due to the feedback (when Ta &lt; 20). While the node is at suprathreshold activation, very low sensory input will push the node to reach suprathreshold as a consequence due to sensory input. See <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>.</p><fig id="sa2fig1"><label>Author response image 1.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-resp-fig1-v1.tif"/></fig><p>Here the ‘nice’ node is already activated due to the feedback (supra threshold activation panel is orange), then the sensory input arrives, and the node immediately reaches a 2 activation (red color. Activation due to sensory input). This of course never happens for stimuli that have no feedback (the ‘I’ node) and happens later for nodes that have weaker feedback (later for ‘very’ compared to ‘cake’). Moreover, after the supra threshold due to feedback is finished, the inhibition sets in reducing the activation and the nodes never (or at least at intensities used for the simulation) reaches threshold for later stimulus times during inhibition.</p><disp-quote content-type="editor-comment"><p>17. How is the connectivity between layers defined? Is it symmetric for feedforward and feedback?</p></disp-quote><p>In the current model the feedforward layers only connect to nodes representation the same items (i.e. a ‘I’ L-1 (stimulus level) node connects to a ‘I’ L node which connects to a ‘I’ L+1 node). Only the feedback nodes (L+1) are fully connected with the active level (L) but with different connection strengths which are defined by the internal model (defined in table 2).</p><disp-quote content-type="editor-comment"><p>18. l. 294/l. 205: &quot;with a delay of 0.9*ω seconds, which then decays at 0.01 unit per millisecond and influences the l-level at a proportion of 1.5.&quot; where are the constants coming from?</p></disp-quote><p>These are informed choices. The 0.9*ω was defined as we hypothesized that onset time would be loosely predicted around on oscillatory cycle, but to be prepared for input slightly earlier (which of course happens for predictable stimuli), we set it to 0.9 times the length of the cycle. The decay is needed and set such that the feedback would continue around a full theta cycle. The proportion was set empirically such to ensure that strong feedback did cause suprathreshold activation at the active node. We added this explanation to the manuscript.</p><disp-quote content-type="editor-comment"><p>19. l. 347: &quot;the processing itself can actually be closer to isochronous than what can be solely extracted from the stimulus&quot;. This refers to Figure 5 D I assume. Did you directly compare the acoustics and the model output with respect to isochrony?</p></disp-quote><p>We can compare the relative distribution (so when is the peak strongest across delays), but not the absolute values as the stimulus intensity and the activation are not on the same unit scale. In order to promote this comparison, we now also show the power of the stimulus input distribution across stimulus intensities and delays (Figure 6B). It is evident that the stimulus has a symmetrical 4 Hz power spectral which is strongest at a delay of 0 (isochrony). This is not strange as we defined it a-priory this way.</p><disp-quote content-type="editor-comment"><p>20. l. 437-438: I am not fully understanding these choices: why is N1 represented by N2? Why is the probability of da and ga uneaven, and why are there nodes for da and ga (Nda, Nga) plus a node N2 which predicts both with different probability?</p></disp-quote><p>N1 represent N1. We regret the confusing, we meant to state that N1 predicts N2. N2 is represented as N2. We have rephrased this sentence to clarify it.</p><p>The probabilities of /da/ and /ga/ are uneven as the model only shows phase coding of information when the internal model of STiMCON has different probabilities of predicting the content of the next word. Otherwise, the nodes will be active at the same time. The assumption for the /da/ and /ga/ having different probabilities is reasonable as the /d/ and /g/ consonant have a different overall proportion in the Dutch language (with /d/ being more frequent than the /g/). As such, we would expect the overall /d/ representation in the brain to be active at lower thresholds than then the /g/ representation. We have clarified this now in the manuscript. It now reads: “N2 activation predicts either da or ga at 0.2 and 0.1 probability respectively. This uneven prediction of /da/ and /ga/ is justified as /da/ is more prevalent in the Dutch language as /ga/ [64] and it thus has a higher predicted level of occurring.”</p><disp-quote content-type="editor-comment"><p>21. Figure 5: why is the power of the high-high predictable condition the lowest. Is this an artifact of the oscillator in the model being fixed at 4 Hz or related to the inhibition function? High-high should like low-low result in rather regular, but faster acoustics?</p></disp-quote><p>We thank the reviewer and regret an unfortunate mistake. The labeling of the low-low and high-high was reversed. So actually, the high-high has the stronger activation. We have updated the labels and text accordingly.</p><p>Regarding the interpretation of the reviewer, we do indeed predict in natural situations for high-high that the acoustics to be slightly faster. However, in the simulations in figure 5 (current figure 6), the speed of the acoustics is never modulated, but decided by us in the simulation. Therefore, only the response of the model is estimated to varying internal models is estimated.</p><disp-quote content-type="editor-comment"><p>22. l. 600: &quot;The perceived rhythmicity&quot; In my view speech has been suggested to be quasi-rhythmic, as (1) some consistency in syllable duration has been observed within/across languages, and (2) as (quasi-)rhythmicity seemed a requirement to explain how segmentation of speech based on oscillations could work in the absence of simple segmentation cues (i.e. pauses between syllables). While one can ask when something is &quot;rhythmic enough&quot; to be called rhythmic, I don't understand why this is related to &quot;perceived rhythmicity&quot;.</p></disp-quote><p>We regret our terminology. We mean perceived isochronicity. Indeed, rhythmicity occurs without isochrony (Obleser, Henry and Lakatos, 2017) and we don’t intend to state that natural speech timing do not have rhythm and pushing away from isochrony does not reflect rhythm. We mere meant to state that when the brain response is more isochronous (relative to acoustic stimulation), than likely the perception is also more isochronous.</p><disp-quote content-type="editor-comment"><p>23. l. 604: interesting thought!</p></disp-quote><p>We hope to pursue these ideas in the future.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>1. An important question is how the authors relate these findings to the Giraud and Poeppel, 2012 proposal which really focuses on the syllable. Would you alter the hypothesis to focus on the word level? Or remain at the syllable level and speed up and low down the oscillator depending on the predictability of each word? It would be interesting to hear the authors thoughts on how to manage the juxtaposition of syllable and word processing in this framework.</p></disp-quote><p>The current model does indeed operate on the word level, while oscillatory models operate on the syllabic level. We do not claim by this that predictions per see only work on a word level. In contrary, we believe that ultimately also syllabic level predictions as well as higher level linguistic predictions can be made to influence speech processing. Therefore, our model is incomplete, but serves the purpose to demonstrate how internal language models can influence speech timing as well as perceptual tracking.</p><p>Our choice of the word level was mostly practical. We choose in the current manuscript to start with a word level prediction as this is the starting point commonly available and applied for RNNs. RNNs often work on the word level and not on the syllabic level. For example, this allowed use to use highly trained word level embeddings as a starting point for our LSTM. We are not aware of pre-trained syllabic embeddings that can achieve the same thing. As mentioned above, the temporal shift in STiMCON would also be predicted based on syllabic prediction. Therefore, the only results that are really affected by this notion are the results of figure 6 (or current figure 7). Predicting the temporal shift would likely have been benefitted from also adding predictions in temporal shifts based on syllabic and higher order linguistic predictions.</p><p>We would predict that linguistic inference involves integrating knowledge at different hierarchical levels including predictions concerning which syllable comes next, based on syllabic regularities, word-to-word regularities as well as syntactic context. This is definitely on our to-do list to extrapolate the model to include predictions at these different layers. To ensure this is clear to the reader, we now add a note about the level of processing that might affect the results of figure 7 as well as a paragraph in the discussion that mentions that the model should have multiple levels operating at different levels of the linguistic hierarchy.</p><p>The result section now reads: “Note that the current prediction only operated on the word node (to which we have the RNN predictions), while full temporal shifts are probably better explained by word, syllabic and phrasal predictions.”</p><p>The discussion now reads: “The current model is not exhaustive and does not provide a complete explanation of all the details of speech processing in the brain. For example, it is likely that the primary auditory cortex is still mostly modulated by the acoustic pseudo-rhythmic input and only later brain areas follow more closely the constraints posed by the language model of the brain. Moreover, we now focus on the word level, while many tracking studies have shown the importance of syllabic temporal structure (Luo and Poeppel, 2007, Ghitza, 2012, Giraud and Poeppel, 2012) as well as the role of higher order linguistic temporal dynamics (Meyer et al., 2019, Kaufeld et al., 2020).”</p><disp-quote content-type="editor-comment"><p>2. The authors describe the STiMCON model as having an oscillator with frequency set to the average stimulus rate of the sentence. But how an oscillator can achieve this on its own (without the hand of its overloads) is unclear particularly given a pseudo-rhythmic input. The authors freely accept this limitation. However, it is worth noting that the ability for an oscillator mechanism to do this under pseudorhythmic context is more complicated than it might seem, particularly once we include that the stimulus rate might change from the beginning to the end of a sentence and across an entire discourse.</p></disp-quote><p>This is a clear limitation, but to be fair also a limitation of any proposal on entrainment not often addressed (Giraud and Poeppel, 2012; Ghitza, 2012). The reviewer himself has done great work on investigating oscillatory entrainment based on ideas on dynamical systems and weakly coupled oscillators (Doelling et al., 2019). These simulations show that oscillations can shift their frequency when the coupling to the acoustics is strong enough. If the coupling is very strong, this could even happen from word to word (as the intrinsic oscillator in that case almost one-to-one follows the phase of the acoustics). However, our contribution here is that none of these models take top-down influences about word predictability into account on these dynamics. Moreover, it is difficult to explain how these models can explain how temporal information can inform about the content of a word (Ten Oever et al., 2013; Ten Oever and Sack, 2015). Specifically, the effect on how syllable identity depends on oscillatory phase (Ten Oever and Sack, 2015). We added a section on this in the discussion clarifying this limitation and our contribution relating to this type of research.</p><p>It now reads: “We aimed to show that a stable oscillator can be sensitive to temporal pseudo-rhythmicities when these shifts match predictions from an internal linguistic model (causing higher sensitivity to these nodes). In this way we show that temporal dynamics in speech and the brain cannot be isolated from processing the content of speech. This is contrast with other models that try to explain how the brain deals with pseudo-rhythmicity in speech (Giraud and Poeppel, 2012, Rimmele et al., 2018, Doelling et al., 2019), which typically do not take into account that the content of a word can influence the timing. Phase resetting models can only deal with pseudo-rhythmicity by shifting the phase of ongoing oscillations in response to a word that is off-set to the mean frequency of the input (Giraud and Poeppel, 2012, Doelling et al., 2019). We believe that this goes beyond the temporal and content information the brain can extract from the environment which has what/when interactions. However, our current model does not have an explanation of how the brain can actually entrain to an average speech rate. This is much better coupling in dynamical systems theories in which this is a consequence of the coupling strength between internal oscillations and speech acoustics (Doelling et al., 2019, Assaneo et al., 2021). However, these models do not take top-down predictive processing into account. Therefore, the best way forward is likely to extend coupling between brain oscillations and speech acoustics (Poeppel and Assaneo, 2020) with the coupling of brain oscillations to brain activity patterns of internal models (Cumin and Unsworth, 2007).”</p><disp-quote content-type="editor-comment"><p>3. The analysis of the naturalistic dataset shows a nice correlation between the estimated time shifts predicted by the model and the true naturalistic deviations. However, I find it surprising that there is so little deviation across the parameters of the oscillator (Figure 6A). What should we take from the fact that an oscillator aligned in anti-phase from the with the stimulus (which would presumably show the phase code only stimulus offsets), still shows a near equal correlation with true timing deviations. Furthermore, while the R2 shows that the predictions of the model co-vary with the true values, I'm curious to know how accurately they are predicted overall (in terms of mean squared error for example). Does the model account for deviations from rhythmicity of the right magnitude?</p></disp-quote><p>We agree that the differences in R2 depending on the parameters of the oscillation might seem slightly underwhelming. This is likely due to the nature of our fitting. We fit the same parameter (RNN prediction), but use different transformation on this predictor (all of which maintain the same order of the prediction parameter). Therefore, the difference between the model can be viewed as a difference in a transformation applied to the data (the same as a difference between doing for example doing a log transform or an arcsin transformation). In general, OLS is robust to slight variations in these transformations and therefore will still fit at a similar explained variance with only slight variations. Therefore, we believe that these small differences are meaningful but should rather be viewed as a relative comparison than absolute explanation of our different oscillatory parameters. Indeed, this is also why we set all our other parameters of our model to zero (equation 4) and don’t actually simulate stimulus processing, but merely the relative expected shift. We agree with the reviewer that is unlikely that processing at an anti-phase manner would lead to good performance. We demonstrate that in the other figures this is not the case (e.g. processing at anti-phase is not even possible at low stimulus intensities in Figure 5). For this specific comparison we did choose for the OLS and transformation shift and not a brute force fitting as in Figure 8 as (1) we can enter control variables to account for different variance we need to control for in the natural dataset, and (2) on this big dataset will result in a much more efficient code.</p><p>To further answer the reviewer’s question, we extracted the mean square error of the model (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>). See Figure 7 for the original R2 for comparisons. It is evident that the MSE is directly related to the R2 of the model. This is not strange as the lower the error variance, the more the explained variance. Maybe we misunderstood what the reviewer was asking for, but we do not see the direct added benefit of including the MSE value.</p><fig id="sa2fig2"><label>Author response image 2.</label><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-68066-resp-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>4. Lastly, it is unclear to what extent the oscillator is necessary to find this relative time shift. A model comparison between the predictions of the STiMCON and the RNN predictions on their own (à la Figure 3) would help to show how much the addition of the oscillation improves our predictions. Perhaps this is what is meant by the &quot;non-transformed R2&quot; but this is unclear.</p></disp-quote><p>We regret that this was unclear. Indeed, we meant with the non-transformed R2 a model with the same factors and same permutations as for the models show figure 6, but not performing the transformation as described in equation 5. This has now been clarified in the manuscript. It now reads: “Results show a modulation of the R2 dependent on the amplitude and phase offset of the oscillation (Figure 7A). This was stronger than a model in which transformation in equation (5) was not applied (R2 was there 0.389).”</p><disp-quote content-type="editor-comment"><p>5. Figure 7 shows a striking result demonstrating how the model can be used to explain an interesting finding that phase of an oscillation can bias perception towards da or ga. The initial papers consider this result to be explained by delays in onset between visual and auditory stimuli whereas this result explains it in terms of the statistical likelihood each syllable. It is a nice reframing which helps me to better understand the previous result.</p></disp-quote><p>We agree that this argument is more parsimonious than our original interpretation. Of course, the two are not mutually exclusive (AV delays could also be bigger for less likely syllables). We have mentioned this now in the result section. It now reads: “Note that previously we have interpreted the /daga/ phase-dependent effect as a mapping of differences between natural audio-visual onset delays of the two syllabic types onto oscillatory phase (Ten Oever et al., 2013, Ten Oever and Sack, 2015). However, the current interpretation is not mutually exclusive with this delay-to-phase mapping as audio-visual delays could be bigger for less frequent syllables.”</p><disp-quote content-type="editor-comment"><p>6. The authors show that syllable lengths are determined in part by the predictability of the word it is a part of. While the authors have reasonably restricted themselves to a single hierarchical level, the point invites the question as to whether all hierarchical levels are governed by similar processes. Should syllables accelerate from beginning to end of a word? Or in more or less predictable phrases?</p></disp-quote><p>We would predict that different level word on similar operations. However, there has not been a great deal of research on syllabic predictions and lengthening or shortening of syllables. In figure 2E we show that indeed words with more syllables are shorten than would be expected by summing up the average syllabic duration expected from mono-syllabic words. However, this does not have to be because later syllables are shortened. Note however that this in not directly what we would predict. We predict that the more predictable the next syllable, the shorter the syllable-to-syllable onsets. So if the next word is very unpredictable, the last syllable would actually be longer. Indeed, often it is found that the last syllable of a word is lengthened (Lindblom, 1968). Other studies show that for individual phonemes word frequency can affect the duration of the syllable (Pluymaekers et al., 2005). But also, higher predictability within the word or based on the next word can influence shortening (Luymakers et al., 2005b). In linguistic studies it has been found that also initial syllables of a word are shortened when words are longer (Lehiste, 1972). While this initially seems against our prediction of higher predictability (within a word) leading to shortening, note that these were only a bi- or tri-syllabic words. For these words we would predict that the first syllable would be shortened when the next syllable is predictable which it the case when a producer can access the full word. In sum, whenever transitional boundary to the next syllable are weaker (across than within words) this can lead to (relative) lengthening (as predicted from Figure 3 and Table 2). This needs to be investigated across syllabic, word, and phrasal levels. However, to fully answer this question we would need to investigate the transitional probabilities of the syllables in the corpus. We have added a section on this point in the discussion. It now reads: “It is likely that predictive mechanisms also operate on these higher and syllabic levels. It is known for example that syllables are shortened when the following syllabic content is known versus producing syllables in isolation (Lehiste ,1972, Pluymaekers et al., 2005). Interactions also occur as syllables within more frequency words are generally shortened (Pluymaekers et al., 2005). Therefore, more hierarchical levels need to be added to the current model (but this is possible following equation (1)).”</p><disp-quote content-type="editor-comment"><p>7. Figure 5 shows how an oscillator mechanism can force pseudo-rhythmic stimuli into a more rhythmic code. The authors note that this can be done either by slowing responses to early stimuli and quickening responses to later ones, or by dropping (nodes don't reach threshold) stimuli too far outside the range of the oscillation. The first is an interesting mechanism, the second is potentially detrimental to processing (although it could be used as a means for filtering out noise). The authors should make clear how much deviation is required to invoke the dropping out mechanism and how this threshold relates to the naturalistic case. This would give the reader a clearer view of the flexibility of this model.</p></disp-quote><p>The dropping mechanism will only every occur when the stimulus intensity is not high enough. For this specific demonstration we kept the stimulus intensity low to demonstrate the change in sensitivity based on feedback. As the current implementation of the model an intensity of 2.1 would be sufficient to always reach activation also at the least excitable point of the oscillation. We have now clarified this in the manuscript. It now reads: “In regular circumstances we would of course always want to process speech, also when it arrives at a less excitable phase. Note however, that the current stimulus intensities were picked to exactly extract the threshold responses. When we increase our intensity range above 2.1 nodes will always get activated even on the lowest excitable phase of the oscillation.”</p><disp-quote content-type="editor-comment"><p>8. I found Figure 5 very difficult to understand and had to read and read it multiple times to feel like I could get a handle on it. I struggled to get a handle on why supra time was shorter and shorter the later the stimulus was activated. It should reverse at some point as the phase goes back into lower excitability, right? The current wording is very unclear on this point. In addition, the low-high, high-low analysis is unclear because the nature of the stimuli is unclear. I think an added figure panel to show how these stimuli are generated and manipulated would go a long way here.</p><p>9. The prediction of behavioral data in Figure 7 is striking but the methods could be improved. Currently, the authors bin the output of the model to be 0, 0.5 or 1 which requires some maneuvering to effectively compare it with the sinewave model. They could instead use a continuous measure (either lag of activation between da and ga, or activation difference) as a feature in a logistic regression to predict the human subject behavior.</p></disp-quote><p>We have split up the figure in two figure, one relation to Figure 5A+B and the other to C-D-E. In this way we could add clarifying figures on what the model is doing exactly (Figure 5A and Figure 6A). We hope this is now clearer.</p><p>Regarding the clarification points.</p><p>1) I struggled to get a handle on why supra time was shorter and shorter the later the stimulus was activated. It should reverse at some point as the phase goes back into lower excitability, right?</p><p>Indeed, at some point the supra time should reverse, however, we didn’t go so far out as also the feedback would be faded by that time and there would be no difference among the different stimuli.</p><p>2) The low-high, high-low analysis is unclear because the nature of the stimuli is unclear.</p><p>The nature of the stimuli is now clarified in panel 6A. The stimuli are all the same, but vary in the underlying internal model. For the low-high the stimuli alternate between a highly predicted and not-predicted stimulus and vice versa for high-low.</p><disp-quote content-type="editor-comment"><p>10. I'm not sure but I think there is a typo in line 383-384. The parameter for feedback should read Cl+1◊ l * Al+1,T. Note the + sign instead of the -. Or I have misunderstood something important.</p></disp-quote><p>The lag analysis is difficult as often one of the nodes is not activated at all and we would have to set an arbitrary value to this latency then. But we are able to look at the mean activation for the nodes to get a continuous variable. Therefore, we repeated the analysis but using the relative activation between /da/ and /ga/ nodes over an interval of 500 ms post-stimulus. The results of this analysis are shown in figure 8D. Firstly, the explained variance increases up to 83% compared to the analyses using the first active node as outcome measure. For the rest the pattern is very similar, but for the mean activation, the inhibition function was more important for the strength of the first than using the first active node as outcome. We have decided to keep both types of fitting in our manuscript as we are not sure yet what could be the more relevant neuronal feature for identifying the stimulus. Is it the first time the node is active or the average activation? But as both analyses point to the same direction we are confident that all features of the model are important to fit the data.</p><p>References</p><p>Assaneo, M. F., et al. (2021). &quot;Speaking rhythmically can shape hearing.&quot; Nature human behaviour 5(1): 71-82.</p><p>Cumin, D. and C. Unsworth (2007). &quot;Generalising the Kuramoto model for the study of neuronal synchronisation in the brain.&quot; Physica D: Nonlinear Phenomena 226(2): 181-196.</p><p>Doelling, K. B., et al. (2019). &quot;An oscillator model better predicts cortical entrainment to music.&quot; Proceedings of the National Academy of Sciences 116(20): 10113-10121.</p><p>Ghitza, O. (2012). &quot;On the role of theta-driven syllabic parsing in decoding speech: intelligibility of speech with a manipulated modulation spectrum.&quot; Frontiers in Psychology 3.</p><p>Giraud, A. L. and D. Poeppel (2012). &quot;Cortical oscillations and speech processing: emerging computational principles and operations.&quot; Nature Neuroscience 15(4): 511-517.</p><p>Jensen, O., et al. (2012). &quot;An oscillatory mechanism for prioritizing salient unattended stimuli.&quot; Trends in Cognitive Sciences 16(4): 200-206.</p><p>Kaufeld, G., et al. (2020). &quot;Linguistic structure and meaning organize neural oscillations into a content-specific hierarchy.&quot; Journal of Neuroscience 40(49): 9467-9475.</p><p>Lehiste (1972). &quot;The timing of utterances and linguistic boundaries.&quot; The Journal of the Acoustical Society of America 51.</p><p>Lisman, J. E. and O. Jensen (2013). &quot;The theta-gamma neural code.&quot; Neuron 77(6): 1002-1016.</p><p>Luo, H. and D. Poeppel (2007). &quot;Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex.&quot; Neuron 54(6): 1001-1010.</p><p>Meyer, L., et al. (2019). &quot;Synchronous, but not entrained: Exogenous and endogenous cortical rhythms of speech and language processing.&quot; Language, Cognition and Neuroscience: 1-11.</p><p>Panzeri, S., et al. (2001). &quot;The role of spike timing in the coding of stimulus location in rat somatosensory cortex.&quot; Neuron 29(3): 769-777.</p><p>Pluymaekers, M., et al. (2005). &quot;Articulatory planning is continuous and sensitive to informational redundancy.&quot; Phonetica 62(2-4): 146-159.</p><p>Pluymaekers, M., et al. (2005). &quot;Lexical frequency and acoustic reduction in spoken Dutch.&quot; The Journal of the Acoustical Society of America 118(4): 2561-2569.</p><p>Poeppel, D. and M. F. Assaneo (2020). &quot;Speech rhythms and their neural foundations.&quot; Nature Reviews Neuroscience: 1-13.</p><p>Rimmele, J. M., et al. (2018). &quot;Proactive sensing of periodic and aperiodic auditory patterns.&quot; Trends in Cognitive Sciences 22(10): 870-882.</p><p>Schroeder, C. E. and P. Lakatos (2009). &quot;Low-frequency neuronal oscillations as instruments of sensory selection.&quot; Trends in Neurosciences 32(1): 9-18.</p><p>Ten Oever, S., et al. (2020). &quot;Phase-coded oscillatory ordering promotes the separation of closely matched representations to optimize perceptual discrimination.&quot; iScience: 101282.</p><p>Ten Oever, S. and A. T. Sack (2015). &quot;Oscillatory phase shapes syllable perception.&quot; Proceedings of the National Academy of Sciences 112(52): 15833-15837.</p><p>Ten Oever, S., et al. (2013). &quot;Audio-visual onset differences are used to determine syllable identity for ambiguous audio-visual stimulus pairs.&quot; Frontiers in Psychology 4.</p><p>Winfree, A. T. (2001). The geometry of biological time, Springer Science and Business Media.</p><p>Zuidema, W. (2010). &quot;A syllable frequency list for Dutch.&quot;</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>I want to thank the authors for the great effort revising the manuscript. The manuscript has much improved. I only have some final small comments.</p><p>Detailed comments</p><p>l. 273-275: In my opinion: This is because the oscillator is set as a rigid oscillator in the model that is not affected by the word level layer activation; however, as the authors already discuss this topic, this is just a comment.</p><p>l. 344: &quot;the processing itself&quot; I'd specify: &quot;the processing at the word layer&quot;.</p></disp-quote><p>We have changed the phrasing accordingly.</p><disp-quote content-type="editor-comment"><p>l. 557/558: Rimmele et al., (2018) do discuss that besides the motor system, predictions from higher-level linguistic processing might affect auditory cortex neuronal oscillations through phase resetting. Top-down predictions affecting auditory cortex oscillations is one of the main claims of the paper. Thus, this paper seems not a good example for proposals that exclude when-to-what interactions. In my view the claims are rather consistent with the ones proposed here, although Rimmele et al., do not detail the mechanism and differ from the current proposal in that they suggest phase resetting. Could you clarify?</p></disp-quote><p>We regret that it seemed that we overlooked that Rimmele et al., (2018) discuss that linguistic predictions can influence auditory cortex. We believe this is a misunderstanding. While Rimmele et al., (2018) indeed discuss how timing in the brain can change due to top-down linguistic predictions, they do not discuss that timing in the speech input itself is also dependent on the content and relates by itself to linguistic predictions. One core question to us is how the brain can extract the statistical information from what-when dependencies in the environment. For example, it is unclear how phase resetting would account for inferring that a less predictable word typically occurs later. This is a core difference between those models and the current model. We realize now that the current phrasing is rather ambiguous talking about when-what in the brain or in the stimulus statistics and the argument was not complete. We rephrase this part of the manuscript. It now reads: “While some of these models discuss that higher-level linguistic processing can modulate the timing of ongoing oscillations [15], they typically do not consider that in the speech signal itself the content or predictability of a word relates to the timing of this word. Phase resetting models typically deal with pseudo-rhythmicity by shifting the phase of ongoing oscillations in response to a word that is off-set to the mean frequency of the input [3, 77]. We believe that this cannot explain how the brain uses what/when dependencies present in the environment to infer the content of the word (e.g. later words are likely a less predictable word).”</p><disp-quote content-type="editor-comment"><p>l 584 ff.: &quot;This idea diverges from the idea that entrainment should per definition occur on the most excitable phase of the oscillation [3,15].&quot; Maybe rephrase: &quot;This idea diverges from the idea that entrainment should align the most excitable phase of the oscillation with the highest energy in the acoustics [3,15].&quot;</p></disp-quote><p>We have changed the phrasing accordingly.</p><disp-quote content-type="editor-comment"><p>l. 431: &quot;The model consists of four nodes (N1, N2, Nda, and Nga) at which N1 activation predicts a second unspecific stimulus (S2) represented by N2 at a predictability of 1. N2 activation predicts either da or ga at 0.2 and 0.1 probability respectively.&quot;</p><p>This is still hard to understand for me. E.g. What is S2, is this either da or ga, wouldn't their probability have to add up to 1?</p></disp-quote><p>N1 and N2 represent nodes that are responsive to two stimuli S1 and S2 (just as Nda and Nga are responsive to the stimulus /da/ and /ga/). S1 and S2 are two unspecific stimuli that are in the simulation to model the entrainment stimuli. (manuscript now reads: “N1 and N2 represent nodes responsive to two stimulus S1 and S2 that function as entrainment stimuli.”)</p><p>Indeed, in the brain it would make sense that the prediction adds up to 1. However, we here only model a small proportion of all the possible word nodes in the brain. To clarify this, we added the following: “While in the brain the prediction should add up to 1, we can assume that the probability is spread across a big number of word nodes of the full language model and therefore neglectable.”</p><disp-quote content-type="editor-comment"><p>Wording</p><p>l. 175/176: sth is wrong with the sentence.</p><p>l. 544: &quot;higher and syllabic&quot;? (sounds like sth is wrong in the wording)</p><p>l. 546: &quot;within more frequency&quot; (more frequent or higher frequency?)</p></disp-quote><p>We have updated the wording of these sentences accordingly.</p></body></sub-article></article>