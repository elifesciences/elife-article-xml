<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">75600</article-id><article-id pub-id-type="doi">10.7554/eLife.75600</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Genetics and Genomics</subject></subj-group></article-categories><title-group><article-title>Rapid, Reference-Free human genotype imputation with denoising autoencoders</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-270188"><name><surname>Dias</surname><given-names>Raquel</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-261911"><name><surname>Evans</surname><given-names>Doug</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-261912"><name><surname>Chen</surname><given-names>Shang-Fu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-261913"><name><surname>Chen</surname><given-names>Kai-Yu</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-288980"><name><surname>Loguercio</surname><given-names>Salvatore</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-261914"><name><surname>Chan</surname><given-names>Leslie</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" corresp="yes" id="author-188477"><name><surname>Torkamani</surname><given-names>Ali</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0232-8053</contrib-id><email>atorkama@scripps.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con7"/><xref ref-type="fn" rid="conf2"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02dxx6824</institution-id><institution>Scripps Research Translational Institute, Scripps Research Institute</institution></institution-wrap><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution>Department of Integrative Structural and Computational Biology, Scripps Research</institution><addr-line><named-content content-type="city">La Jolla</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02y3ad647</institution-id><institution>Department of Microbiology and Cell Science, University of Florida</institution></institution-wrap><addr-line><named-content content-type="city">Gainesville</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Stephens</surname><given-names>Matthew</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Przeworski</surname><given-names>Molly</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj8s172</institution-id><institution>Columbia University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>23</day><month>09</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e75600</elocation-id><history><date date-type="received" iso-8601-date="2021-11-16"><day>16</day><month>11</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-09-19"><day>19</day><month>09</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2021-12-02"><day>02</day><month>12</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.12.01.470739"/></event></pub-history><permissions><copyright-statement>Â© 2022, Dias et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Dias et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-75600-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-75600-figures-v2.pdf"/><abstract><p>Genotype imputation is a foundational tool for population genetics. Standard statistical imputation approaches rely on the co-location of large whole-genome sequencing-based reference panels, powerful computing environments, and potentially sensitive genetic study data. This results in computational resource and privacy-risk barriers to access to cutting-edge imputation techniques. Moreover, the accuracy of current statistical approaches is known to degrade in regions of low and complex linkage disequilibrium. Artificial neural network-based imputation approaches may overcome these limitations by encoding complex genotype relationships in easily portable inference models. Here, we demonstrate an autoencoder-based approach for genotype imputation, using a large, commonly used reference panel, and spanning the entirety of human chromosome 22. Our autoencoder-based genotype imputation strategy achieved superior imputation accuracy across the allele-frequency spectrum and across genomes of diverse ancestry, while delivering at least fourfold faster inference run time relative to standard imputation tools.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>imputation</kwd><kwd>deep learning</kwd><kwd>artifitial intelligence</kwd><kwd>population genetics</kwd><kwd>genomics</kwd><kwd>autoencoder</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01HG010881</award-id><principal-award-recipient><name><surname>Dias</surname><given-names>Raquel</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>KL2TR002552</award-id><principal-award-recipient><name><surname>Dias</surname><given-names>Raquel</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U24TR002306</award-id><principal-award-recipient><name><surname>Evans</surname><given-names>Doug</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>UL1TR002550</award-id><principal-award-recipient><name><surname>Evans</surname><given-names>Doug</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A new autoencoder-based genotype imputation method shows superior accuracy across human genomes of diverse ancestry and across the allele-frequency spectrum, while delivering significantly faster inference run times relative to standard imputation tools.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>The human genome is inherited in large blocks from parental genomes, generated through a DNA-sequence-dependent shuffling process called recombination. The non-uniform nature of recombination breakpoints producing these genomic blocks results in correlative genotype relationships across genetic variants, known as linkage disequilibrium. Thus, genotypes for a small subset (1â10%) of observed common genetic variants can be used to infer the genotype status of unobserved but known genetic variation sites across the genome (on the order of ~1 M of &gt;10 M sites; <xref ref-type="bibr" rid="bib27">Li et al., 2009</xref>; <xref ref-type="bibr" rid="bib31">Marchini and Howie, 2010</xref>). This process, called genotype imputation, allows for the generation of nearly the full complement of known common genetic variation at a fraction of the cost of direct genotyping or sequencing. Given the massive scale of genotyping required for genome-wide association studies or implementation of genetically informed population health initiatives, genotype imputation is an essential approach in population genetics.</p><p>Standard approaches to genotype imputation utilize Hidden Markov Models (HMM) (<xref ref-type="bibr" rid="bib8">Browning et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Das et al., 2016</xref>; <xref ref-type="bibr" rid="bib35">Rubinacci et al., 2020</xref>) distributed alongside large WGS-based reference panels (<xref ref-type="bibr" rid="bib7">Browning and Browning, 2016</xref>). In general terms, these imputation algorithms use genetic variants shared between to-be-imputed genomes and the reference panel and apply Hidden Markov Models (HMM) to impute the missing genotypes per sample (<xref ref-type="bibr" rid="bib14">Das et al., 2018</xref>). The hidden states in the HMMs represent the haplotype in a reference panel that is most closely related to the haplotype being imputed. The HMM parameter estimation also depends on recombination rates, mutation rates, and/or genotype error rates that must be fit by Markov Chain Monte Carlo Algorithm (MCMC) or an expectation-maximization algorithm. Thus, HMM-based imputation is a computationally intensive process, requiring access to both high-performance computing environments and large, privacy-sensitive, WGS reference panels (<xref ref-type="bibr" rid="bib26">Kowalski et al., 2019</xref>). Often, investigators outside of large consortia will resort to submitting genotype data to imputation servers (<xref ref-type="bibr" rid="bib13">Das et al., 2016</xref>), resulting in privacy and scalability concerns (<xref ref-type="bibr" rid="bib36">Sarkar et al., 2021</xref>).</p><p>Recently, artificial neural networks, especially autoencoders, have attracted attention in functional genomics for their ability to fill-in missing data for image restoration and inpainting (<xref ref-type="bibr" rid="bib10">Chaitanya et al., 2017</xref>; <xref ref-type="bibr" rid="bib22">Ghosh et al., 2020</xref>; <xref ref-type="bibr" rid="bib30">Mao et al., 2016</xref>; <xref ref-type="bibr" rid="bib41">Xie et al., 2012</xref>). Autoencoders are neural networks tasked with the problem of simply reconstructing the original input data, with constraints applied to the network architecture or transformations applied to the input data in order to achieve a desired goal like dimensionality reduction or compression, and de-noising or de-masking (<xref ref-type="bibr" rid="bib1">Abouzid et al., 2019</xref>; <xref ref-type="bibr" rid="bib29">Liu et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Voulodimos et al., 2018</xref>). Stochastic noise or masking is used to modify or remove data inputs, training the autoencoder to reconstruct the original uncorrupted data from corrupted inputs (<xref ref-type="bibr" rid="bib39">Tian et al., 2020</xref>). Autoencoders that receive corrupted or masked data as input and are trained to predict the original uncorrupted data as the output are also known as denoising autoencoders. These autoencoder characteristics are well-suited for genotype imputation and may address some of the limitations of HMM-based imputation by eliminating the need for dissemination of reference panels and allowing the capture of non-linear relationships in genomic regions with complex linkage disequilibrium structures. Some attempts at genotype imputation using neural networks have been previously reported, though for specific genomic contexts (<xref ref-type="bibr" rid="bib34">Naito et al., 2021</xref>) at genotype masking levels (5â20%) not applicable in typical real-world population genetics scenarios (<xref ref-type="bibr" rid="bib11">Chen and Shi, 2019</xref>; <xref ref-type="bibr" rid="bib23">Islam et al., 2021</xref>; <xref ref-type="bibr" rid="bib37">Sun and Kardia, 2008</xref>), or in contexts where the neural network must be trained for imputation with specific input variant sets, i.e. the model must be re-trained for imputation from different genotyping array (<xref ref-type="bibr" rid="bib25">Kojima et al., 2020</xref>).</p><p>Here, we present a generalized approach to unphased human genotype imputation using sparse, denoising autoencoders capable of highly accurate genotype imputation at genotype masking levels (98+%) appropriate for array-based genotyping and low-pass sequencing-based population genetics initiatives. We describe the initial training and implementation of autoencoders spanning all of human chromosome 22, achieving equivalent to superior accuracy relative to modern HMM-based methods, and dramatically improving computational efficiency at deployment without the need to distribute reference panels.</p></sec><sec id="s2" sec-type="materials|methods"><title>Materials and methods</title><sec id="s2-1"><title>Overview</title><p>Sparse, de-noising autoencoders spanning all bi-allelic SNPs observed in the Haplotype Reference Consortium were developed and optimized. Each autoencoder receives masked data as input and is trained to predict the original uncorrupted data as the output. Each bi-allelic SNP was encoded as two binary input nodes, representing the presence or absence of each allele (<xref ref-type="fig" rid="fig1">Figure 1B, E</xref>). This encoding allows for the straightforward extension to multi-allelic architectures and non-binary allele presence probabilities. A data augmentation approach using modeled recombination events and offspring formation coupled with random masking at an escalating rate drove our autoencoder training strategy (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). Because of the extreme skew of the allele frequency distribution for rarely present alleles (<xref ref-type="bibr" rid="bib3">Auton et al., 2015</xref>), a focal-loss-based approach was essential to genotype imputation performance. The basic architecture of the template fully-connected autoencoder before optimization to each genomic segment is depicted in <xref ref-type="fig" rid="fig1">Figure 1D</xref>. Individual autoencoders were designed to span genomic segments with boundaries defined by computationally identified recombination hotspots (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). The starting point for model hyperparameters were randomly selected from a grid of possible combinations and were further tuned from a battery of features describing the complexity of the linkage-disequilibrium structure of each genomic segment.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Schematic overview of the autoencoder training workflow.</title><p>(<bold>A</bold>) Tiling of autoencoders across the genome is achieved by (<bold>A.1</bold>) calculating a <italic>n x n</italic> matrix of pairwise SNP correlations, thresholding them at 0.45 (selected values are shown in red background, excluded values in gray), (<bold>A.2</bold>) quantifying the overall local LD strength centered at each SNP by computing their local correlation box counts and splitting the genome into approximately independent segments by identifying local minima (recombination hotspots). The red arrow illustrates minima between strong LD regions. For reducing computational complexity, we calculated the correlations in a fixed sliding box size of 500x500 common variants (MAF â¥ 0.5%). Thus, the memory utilization for calculating correlations will be the same regardless of genomic density. (<bold>B</bold>) Ground truth whole genome sequencing data is encoded as binary values representing the presence (1) or absence (0) of the reference allele (blue) and alternative allele (red). (<bold>C</bold>) Variant masking (setting both alleles as absent, represented by 0, corrupts data inputs at a gradually increasing masking rate). Example masked variants are outlined. (<bold>D</bold>) Fully-connected autoencoders spanning segments defined as shown in panel (<bold>A</bold>), are then trained to reconstruct the original uncorrupted data from corrupted inputs; (<bold>E</bold>) the reconstructed outputs (imputed data) are compared to the ground truth states for loss calculation and are decoded back to genotypes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig1-v2.tif"/></fig></sec><sec id="s2-2"><title>Genotype encoding</title><p>Genotypes for all bi-allelic SNPs were converted to binary values representing the presence (1) or absence (0) of the reference allele A and alternative allele B, respectively, as shown in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>.<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="{" close="}" separators="|"><mml:mrow><mml:mtable><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi><mml:mo>]</mml:mo></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>[</mml:mo><mml:mn>1,0</mml:mn><mml:mo>]</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>1,1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>B</mml:mi><mml:mo>,</mml:mo><mml:mi>B</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mrow><mml:maligngroup/><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">G</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mi>u</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced><mml:mo>:</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced open="[" close="]" separators="|"><mml:mrow><mml:mn>0,0</mml:mn></mml:mrow></mml:mfenced></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:mfenced></mml:math></disp-formula></p><p>where <italic>x</italic> is a vector containing the two allele presence input nodes to the autoencoder and their encoded allele presence values derived from the original genotype, <italic>G,</italic> of variant <italic>i</italic>. The output nodes of the autoencoder are similarly rescaled to 0â1 by a sigmoid function, split into three genotype outputs (homozygous reference, homozygous alternate, and heterozygous), and normalized using the Softmax function. The normalized outputs can also be regarded as probabilities and can be combined for the calculation of alternative allele dosage and as a measure of imputation quality. This representation is extensible to other classes of genetic variation, and allows for the use of probabilistic loss functions.</p></sec><sec id="s2-3"><title>Training data, masking, and data augmentation</title><sec id="s2-3-1"><title>Training data</title><p>Whole-genome sequence data from the Haplotype Reference Consortium (HRC) was used for training and as the reference panel for comparison to HMM-based imputation (<xref ref-type="bibr" rid="bib32">McCarthy et al., 2016</xref>). The dataset consists of 27,165 samples and 39,235,157 biallelic SNPs generated using whole-genome sequence data from 20 studies of predominantly European ancestry (HRC Release 1.1): 83.92% European, 2.33% East Asian, 1.63% Native American, 2.17% South Asian, 2.96% African, and 6.99% admixed ancestry individuals. Genetic ancestry was determined using continental population classification from the 1000 Genomes Phase3 v5 (1000 G) reference panel and a 95% cutoff using Admixture software (<xref ref-type="bibr" rid="bib2">Alexander et al., 2009</xref>). Genotype imputation autoencoders were trained for all 510,442 unique SNPs observed in HRC on human chromosome 22. For additional comparisons, whole-genome sequence data from 31 studies available through the NHLBI Trans-Omics for Precision Medicine (TOPMed) program were used as an alternative reference panel for HMM-based imputation tools (<xref ref-type="bibr" rid="bib38">Taliun et al., 2021</xref>). We downloaded Freeze 8 of TOPMed, which is the latest version with all consent groups genotyped across the same set of jointly called variants. GRCh38 TOPMed cohorts were converted to hg19 with Picard 2.25 (âPicard toolkitâ, 2019), and multi allelic SNPs removed with bcftools v.1.10.2 (<xref ref-type="bibr" rid="bib12">Danecek et al., 2021</xref>). Any variants with missing genotypes were excluded as well, yielding a final reference panel for chr22 consisting of 73,586 samples and 11,089,826 biallelic SNPs. Since the ARIC and MESA cohorts are used for model selection and validation, they were excluded from the TOPMed reference panel. Relatedness analysis using the KING robust kinship estimator revealed significant data leakage boosting HMM-based imputation performance through individuals directly participating in the MESA and other TOPMed cohorts, as well as through numerous first- and second-degree familial relationships spanning MESA individuals and individuals in other TOPMed cohorts.</p></sec><sec id="s2-3-2"><title>Validation and testing data</title><p>A balanced (50%:50% European and African genetic ancestry) subset of 796 whole genome sequences from the Atherosclerosis Risk in Communities cohort (ARIC) (<xref ref-type="bibr" rid="bib33">Mou et al., 2018</xref>), was used for model validation and selection. The Wellderly (<xref ref-type="bibr" rid="bib20">Erikson et al., 2016</xref>), Human Genome Diversity Panel (HGDP) (<xref ref-type="bibr" rid="bib9">Cann et al., 2002</xref>), and Multi-Ethnic Study of Atherosclerosis (MESA) (<xref ref-type="bibr" rid="bib5">Bild et al., 2002</xref> ) cohorts were used for model testing. The Wellderly cohort consisted of 961 whole genomes of predominantly European genetic ancestry. HGDP consisted of 929 individuals across multiple ancestries: 11.84% European, 14.64% East Asian, 6.57% Native American, 10.98% African, and 55.97% admixed. MESA consisted of 5370 whole genomes across multiple ancestries: 27.62% European, 11.25<bold>%</bold> East Asian<bold>,</bold> 4.99<bold>%</bold> Native American, 5.53% African, and 50.61% admixed. MESA, Wellderly, and HGDP are all independent datasets, not used for autoencoder training, nor model selection, whereas HRC and ARIC were utilized for training and model selection, respectively.</p><p>GRCh38 mapped cohorts (HGDP and MESA) were converted to hg19 using Picard v2.25 (<xref ref-type="bibr" rid="bib6">Broad Institute, 2022</xref>). All other datasets were originally mapped and called against hg19. Multi-allelic SNPs, SNPS with &gt;10% missingness, and SNPs not observed in HRC were removed with bcftools v1.10.2 (<xref ref-type="bibr" rid="bib12">Danecek et al., 2021</xref>). Mock genotype array data was generated from these WGS cohorts by restricting genotypes to those present on commonly used genotyping arrays (Affymetrix 6.0, UKB Axiom, and Omni 1.5 M). For chromosome 22, intersection with HRC and this array-like masking respectively resulted in: 9025, 10,615, and 14,453 out of 306,812 SNPs observed in ARIC; 8630, 10,325, and 12,969 out of 195,148 SNPs observed in the Wellderly; 10,176, 11,086, and 14,693 out of 341,819 SNPs observed in HGDP; 9237, 10,428, and 13,677 out of 445,839 SNPs observed in MESA. All input genotypes from all datasets utilized in this work are unphased, and no pre-phasing was performed.</p></sec><sec id="s2-3-3"><title>Data augmentation</title><p>We employed two strategies for data augmentation â random variant masking and simulating further recombination with offspring formation. During training, random masking of input genotypes was performed at escalating rates, starting with a relatively low masking rate (80% of variants) that is gradually incremented in subsequent training rounds until up to only five variants remain unmasked per autoencoder. Masked variants are encoded as the <italic>null</italic> case in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. During finetuning we used sim1000G (<xref ref-type="bibr" rid="bib19">Dimitromanolakis et al., 2019</xref>) to simulate of offspring formation using the default genetic map and HRC genomes as parents. A total of 30,000 offspring genomes were generated and merged with the original HRC dataset, for a total of 57,165 genomes.</p></sec></sec><sec id="s2-4"><title>Loss function</title><p>In order to account for the overwhelming abundance of rare variants, the accuracy of allele presence reconstruction was scored using an adapted version of focal loss (<italic>FL</italic>) (<xref ref-type="bibr" rid="bib28">Lin et al., 2017</xref>), shown in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>.<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:msub><mml:mrow><mml:mo>-</mml:mo><mml:mi>Î±</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Î³</mml:mi></mml:mrow></mml:msup><mml:mi mathvariant="normal"> </mml:mi><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>â¡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mrow><mml:mrow><mml:mi mathvariant="normal">log</mml:mi></mml:mrow><mml:mo>â¡</mml:mo><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:math></disp-formula></p><p>where the classic cross entropy (shown as binary log loss in brackets) of the truth class (<italic>x<sub>t</sub></italic>) predicted probability (<italic>p<sub>t</sub></italic>) is weighted by the class imbalance factor Î±<sub>t</sub> and a modulating factor (1 - <italic>p<sub>t</sub></italic>)<sup>Î³</sup>. <italic>t</italic> represents the index of each allele in a genomic segment. The modulating factor is the standard focal loss factor with hyperparameter, Î³, which amplifies the focal loss effect by down-weighting the contributions of well-classified alleles to the overall loss (especially abundant reference alleles for rare variant sites)(<xref ref-type="bibr" rid="bib28">Lin et al., 2017</xref>). Î±<sub>t</sub> is an additional balancing hyperparameter set to the truth class frequency.</p><p>This base focal loss function is further penalized and regularized to encourage simple and sparse models in terms of edge-weight and hidden layer activation complexity. These additional penalties result in our final loss function as shown in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>.<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mi>S</mml:mi><mml:mi>F</mml:mi><mml:mi>L</mml:mi><mml:mo>=</mml:mo><mml:mo>â</mml:mo><mml:msub><mml:mi>Î±</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mi>Î³</mml:mi></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>log</mml:mi><mml:mo>â¡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>Î²</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>Ï</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo symmetric="true">â</mml:mo><mml:mi>W</mml:mi><mml:mo symmetric="true">â</mml:mo></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>Î»</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mo symmetric="true">â</mml:mo><mml:mi>W</mml:mi><mml:mo symmetric="true">â</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf1"><mml:msub><mml:mrow><mml:mfenced open="â" close="â" separators="|"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf2"><mml:msub><mml:mrow><mml:mfenced open="â" close="â" separators="|"><mml:mrow><mml:mi>W</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> are the standard <italic>L</italic>1 and <italic>L</italic>2 norms of the autoencoder weight matrix (W), with their contributions mediated by the hyperparameters Î»<sub>1</sub> and Î»<sub>2</sub>. S is a sparsity penalty, with its contribution mediated by the hyperparameter Î², which penalizes deviation from a target hidden node activation set by the hyperparameter vs the observed mean activation <inline-formula><mml:math id="inf3"><mml:mover accent="true"><mml:mrow><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:math></inline-formula> over a training batch <italic>j</italic> summed over total batches <italic>n</italic>, as shown in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">S</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>Ï</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow><mml:mrow><mml:mover><mml:mi>Ï</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo movablelimits="false">â</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msubsup><mml:mi>Ï</mml:mi><mml:mo>â</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>Ï</mml:mi><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>Ï</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>â</mml:mo><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:mi>Ï</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>â</mml:mo><mml:msub><mml:mrow><mml:mrow><mml:mover><mml:mi>Ï</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p></sec><sec id="s2-5"><title>Genome tiling</title><p>All model training tasks were distributed across a diversified set of NVIDIA graphical processing units (GPUs) with different video memory limits: 5 x Titan Vs (12 GB), 8x A100s (40 GB), 60x V100s (32 GB). Given computational complexity and GPU memory limitations, individual autoencoders were designed to span approximately independent genomic segments with boundaries defined by computationally identified recombination hotspots (<xref ref-type="fig" rid="fig1">Figure 1E</xref>). These segments were defined using an adaptation of the LDetect algorithm (<xref ref-type="bibr" rid="bib4">Berisa and Pickrell, 2016</xref>). First, we calculated a <italic>n x n</italic> matrix of pairwise SNP correlations using all common genetic variation (â¥0.5% minor allele frequency) from HRC. Correlation values were thresholded at 0.45, which is the threshold that returns the minimum number of segments spanning chromosome 22 with an average size per segment that fits into the video memory of GPUs. While developing the tiling algorithm, we tested lower thresholds, which made the segments smaller and more abundant, and thus made the GPU memory workload less efficient (e.g. many tiles resulted in many autoencoders per GPU, which thus caused a CPU-GPU communication overhead). Due to the obstacles related to computational inefficiency, CPU-GPU communication overhangs, and GPU memory limits, we did not proceed with model training on segments generated with other correlation thresholds. For each SNP, we calculated a box count of all pairwise SNP correlations spanning 500 common SNPs upstream and downstream of the index SNP. This moving box count quantifies the overall local LD strength centered at each SNP. Local minima in this moving box count were used to split the genome into approximately independent genomic segments of two types â large segments of high LD interlaced with short segments of weak LD corresponding to recombination hotspot regions. Individual autoencoders were designed to span the entirety of a single high LD segment plus its adjacent upstream and downstream weak LD regions. Thus, adjacent autoencoders overlap at their weak LD ends. If an independent genomic segment exceeded the threshold number of SNPs amenable to deep learning given GPU memory limitations, internal local minima within the high LD regions were used to split the genomic segments further to a maximum of 6000 SNPs per autoencoder. Any remaining genomic segments still exceeding 6000 SNPs were further split into 6000 SNP segments with large overlaps of 2500 SNPs given the high degree of informative LD split across these regions. This tiling process resulted in 256 genomic segments spanning chromosome 22: 188 independent LD segments, 32 high LD segments resulting from internal local minima splits, and 36 segments further split due to GPU memory limitations.</p></sec><sec id="s2-6"><title>Hyperparameter initialization and grid search</title><p>We first used a random grid search approach to define initial hyperparameter combinations producing generally accurate genotype imputation results. The hyperparameters and their potential starting values are listed in <xref ref-type="table" rid="table1">Table 1</xref>. This coarse-grain grid search was performed on all genomic segments of chromosome 22 (256 genomic segments), each tested with 100 randomly selected hyperparameter combinations per genomic segment, with a batch size of 256 samples, training for 500 epochs without any stop criteria, and validating on an independent dataset (ARIC). To evaluate the performance of each hyperparameter combination, we calculated the average coefficient of determination (r-squared) comparing the predicted and observed alternative allele dosages per variant. Concordance and F1-score were also calculated to screen for anomalies but were not ultimately used for model selection.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Description and values of hyperparameters tested in grid search.</title><p>Î»<sub>1</sub>: scaling factor for Least Absolute Shrinkage and Selection Operator (LASSO or L1) regularization; Î»<sub>2</sub>: scaling factor for Ridge (L2) regularization; Î²: scaling factor for sparsity penalty described in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>; Ï: target hidden layer activation described in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>; Activation function type: defines how the output of a hidden neuron will be computed given a set of inputs; Learning rate: step size at each learning iteration while moving toward the minimum of the loss function; Î³: amplifying factor for focal loss described in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>; Optimizer type: algorithms utilized to minimize the loss function and update the model weights in backpropagation; Loss type: algorithms utilized to calculate the model error (<xref ref-type="disp-formula" rid="equ2">Equation 2</xref>); Number of hidden layers: how many layers of artificial neurons to be implemented between input layer and output layer; Hidden layer size ratio: scaling factor to resize the next hidden layer with reference to the size of Its previous layer; Learning rate decay ratio: scaling factor for updating the learning rate value on every 500 epochs.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom">Hyperparameter description</th><th align="left" valign="bottom">Tested values (coarse-grid search)</th></tr></thead><tbody><tr><td align="left" valign="bottom">Î»<sub>1</sub> for L1 regularization</td><td align="left" valign="bottom">[1e-3, 1e-4, 1e-5, 1e-6, 1e-1, 1e-2, 1e-7, 1e-8]</td></tr><tr><td align="left" valign="bottom">Î»<sub>2</sub> for L2 regularization</td><td align="left" valign="bottom">[1e-3, 1e-4, 1e-5, 1e-6, 1e-1, 1e-2, 1e-7, 1e-8]</td></tr><tr><td align="left" valign="bottom">Sparsity scaling factor (Î²)</td><td align="left" valign="bottom">[0, 0.001, 0.01, 0.05, 1, 5, 10]</td></tr><tr><td align="left" valign="bottom">Target average hidden layer activation (Ï)</td><td align="left" valign="bottom">[0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1.0]</td></tr><tr><td align="left" valign="bottom">Activation function type</td><td align="left" valign="bottom">[âsigmoidâ, âtanhâ, âreluâ, âsoftplusâ]</td></tr><tr><td align="left" valign="bottom">Learning rate</td><td align="left" valign="bottom">[0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]</td></tr><tr><td align="left" valign="bottom">Amplifying factor for focal loss (Î³)</td><td align="left" valign="bottom">[0, 0.5, 1, 2, 3, 5]</td></tr><tr><td align="left" valign="bottom">Optimizer type</td><td align="left" valign="bottom">[âAdamâ, âRMS Propagationâ, âGradient Descentâ]</td></tr><tr><td align="left" valign="bottom">Loss type</td><td align="left" valign="bottom">[âBinary Cross Entropyâ, âCustom Focal Lossâ]</td></tr><tr><td align="left" valign="bottom">Number of hidden layers</td><td align="left" valign="bottom">[1, 2, 4, 6, 8]</td></tr><tr><td align="left" valign="bottom">Hidden layer size ratio</td><td align="left" valign="bottom">[0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]</td></tr><tr><td align="left" valign="bottom">Learning rate decay ratio</td><td align="left" valign="bottom">[ 0.0, 0.25, 0.5, 0.75, 0.95, 0.99, 0.999, 0.9999]</td></tr></tbody></table></table-wrap></sec><sec id="s2-7"><title>Hyperparameter tuning</title><p>In order to avoid local optimal solutions and reduce the hyperparameter search space, we used an ensemble-based machine learning approach (Extreme Gradient BoostingâXGBoost) to predict the expected performance (r-squared) of each hyperparameter combination per genomic segment using the results of the coarse-grid search and predictive features calculated for each genomic segment. These features include the number of variants, average recombination rate and average pairwise Pearson correlation across all SNPs, proportion of rare and common variants across multiple minor allele frequency (MAF) bins, number of principal components necessary to explain at least 90% of variance, and the total variance explained by the first two principal components. The observed accuracies of the coarse-grid search, numbering 25,600 training inputs, were used to predict the accuracy of 500,000 new hyperparameter combinations selected from <xref ref-type="table" rid="table1">Table 1</xref> without training. All categorical predictors (activation function name, optimizer type, loss function type) were one-hot encoded. The model was implemented using XGBoost package v1.4.1 in Python v3.8.3 with 10-fold cross-validation and default settings.</p><p>We then ranked all hyperparameter combinations by their predicted performance and selected the top 10 candidates per genomic segment along with the single best initially tested hyperparameter combination per genomic segments for further consideration. All other hyperparameter combinations were discarded. Genomic segments with sub-optimal performance relative to Minimac were subjected to tuning with simulated offspring formation. For tuning, the maximum number of epochs was increased (35,000) with automatic stop criteria: if there is no improvement in average loss value of the current masking/training cycle versus the previous one, the training is interrupted, otherwise training continues until the maximum epoch limit is reached. Each masking/training cycle consisted of 500 epochs. Final hyperparameter selection was based on performance on the validation dataset (ARIC).</p><p>This process results in 256 unique autoencoders spanning the genomic segments of chromosome 22. Each genomic segment consists of a different number of input variables (genetic variants), sparsity, and correlation structure. Thus, 256 unique autoencoder models span the entirety of chromosome 22 (e.g.: each autoencoder has different edge weights, number of layers, loss function, as well as regularization and optimization parameters).</p></sec><sec id="s2-8"><title>Performance testing and comparisons</title><p>Performance was compared to Minimac4 (<xref ref-type="bibr" rid="bib13">Das et al., 2016</xref>), Beagle5 (<xref ref-type="bibr" rid="bib8">Browning et al., 2018</xref>), and Impute5 (<xref ref-type="bibr" rid="bib35">Rubinacci et al., 2020</xref>) using default parameters. We utilized HRC as reference panel for the HMM-based imputation tools, which is the same dataset used for training the autoencoders, and we applied the same quality control standards for both HMM-based and autoencoder-based imputation. We also provide additional comparisons to HMM-based imputation using the TOPMed cohort. No post-imputation quality control was applied. Population level reconstruction accuracy is quantified by measuring r-squared across multiple strata of data: per genomic segment, at whole chromosome level, and stratified across multiple minor allele frequency bins: [0.001â0.005], [0.005â0.01], [0.01â0.05], [0.05â0.1], [0.1â0.2], [0.2â0.3], [0.3â0.4], [0.4â0.5]. While r-squared is our primary comparison metric, sample-level and population-level model performance is also evaluated with concordance and the F1-score. Wilcoxon rank-sum testing was used to assess the significance of accuracy differences observed. Spearman correlations were used to evaluate the relationships between genomic segment features and observed imputation accuracy differences. Standard errors for per variant imputation accuracy r-squared is equal or less than 0.001 where not specified. Performance is reported only for the independent test datasets (Wellderly, MESA, and HGDP). Note that MESA ultimately is not independent of the TOPMed cohort when used for HMM-based imputation.</p><p>We used the MESA cohort for inference runtime comparisons. Runtime was determined using the average and standard error of three imputation replicates. Two hardware configurations were used for the tests: (1) a low-end environment: 16-core Intel Xeon CPU (E5-2640 v2 2.00 GHz), 250 GB RAM, and one GPU (NVIDIA GTX 1080); (2) a high-end environment: 24-Core AMD CPU (EPYC 7352 2.3 GHz), 250 GB RAM, using one NVIDIA A100 GPU. We report computation time only, input/output (I/O) reading/writing times are excluded as separately optimized functions. Since the computational burden of training the models remains on the developer side, the runtime results refer to the task of imputing the missing genotypes given a pre-trained autoencoder set.</p></sec><sec id="s2-9"><title>Data availability</title><p>The data that support the findings of this study are available from dbGAP and European Genome-phenome Archive (EGA), but restrictions apply to the availability of these data, which were used under ethics approval for the current study, and so are not openly available to the public. The computational pipeline for autoencoder training and validation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/Imputation_Autoencoder/tree/master/autoencoder_tuning_pipeline">https://github.com/TorkamaniLab/Imputation_Autoencoder/tree/master/autoencoder_tuning_pipeline</ext-link>; <xref ref-type="bibr" rid="bib18">Dias et al., 2022</xref>. The python script for calculating imputation accuracy is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/imputation_accuracy_calculator">https://github.com/TorkamaniLab/imputation_accuracy_calculator</ext-link>; <xref ref-type="bibr" rid="bib16">Dias, 2021</xref>. Instructions on how to access the unique information on the parameters and hyperparameters of each one of the 256 autoencoders is shared through our source code repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/imputator_inference">https://github.com/TorkamaniLab/imputator_inference</ext-link>, copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bcdf526c7102b44428af0a8edc41c95c449c7713;origin=https://github.com/TorkamaniLab/imputator_inference;visit=swh:1:snp:1f1e9662e49b6476f0475c52ca54929ae422184d;anchor=swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c">swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c</ext-link>; <xref ref-type="bibr" rid="bib17">Dias, 2022</xref>. We also shared the pre-trained autoencoders and instructions on how to use them for imputation at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/imputator_inference">https://github.com/TorkamaniLab/imputator_inference</ext-link>; <xref ref-type="bibr" rid="bib17">Dias, 2022</xref>.</p></sec><sec id="s2-10"><title>Imputation data format</title><p>The imputation results are exported in variant calling format (VCF) containing the imputed genotypes and imputation quality scores in the form of class probabilities for each one of the three possible genotypes (homozygous reference, heterozygous, and homozygous alternate allele). The probabilities can be used for quality control of the imputation results.</p></sec></sec><sec id="s3" sec-type="results"><title>Results</title><sec id="s3-1"><title>Untuned performance and model optimization</title><p>A preliminary comparison of the best performing autoencoder per genomic segment vs HMM-based imputation was made after the initial grid search (Minimac4: <xref ref-type="fig" rid="fig2">Figure 2</xref>, Beagle5 and Eagle5: <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplements 1</xref>â<xref ref-type="fig" rid="fig2s2">2</xref>). Untuned autoencoder performance was generally inferior to all tested HMM-based methods except when tested on the European ancestry-rich Wellderly dataset when masked using the Affymetrix 6.0 and UKB Axiom marker sets, but not Omni 1.5 M markers. HMM-based imputation was consistently superior across the more ancestrally diverse test datasets (MESA and HGDP) (two proportion test, pâ¤8.77 Ã 10<sup>â6</sup>). Overall, when performance across genomic segments, test datasets, and test array marker sets was combined, the autoencoders exhibited an average r-squared per variant of 0.352Â±0.008 in reconstruction of WGS ground truth genotypes versus an average r-squared per variant of 0.374Â±0.007, 0.364Â±0.007, and 0.357Â±0.007 for HMM-based imputation methods (Minimac4, Beagle5, and Impute5, respectively) (<xref ref-type="table" rid="table2">Table 2</xref>). This difference was statistically significant only relative to Minimac4 (Minimac4: Wilcoxon rank-sum test p=0.037, Beagle5 and Eagle5: pâ¥0.66).</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>HMM-based (y-axis) versus autoencoder-based (x-axis) imputation accuracy prior to tuning.</title><p>Minimac4 and untuned autoencoders were tested across three independent datasetsâ- MESA (top), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) and across three genotyping array platformsâ- Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (average r-squared per variant) for an individual genomic segment relative to its WGS-based ground truth. The numerical values presented on the left side and below the identity line (dashed line) indicate the number of genomic segments in which Minimac4 outperformed the untuned autoencoder (left of identity line) and the number of genomic segments in which the untuned autoencoder surpassed Minimac4 (below the identity line). Statistical significance was assessed through two-proportion Z-test p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>Beagle5 (y-axis) versus autoencoder-based (x-axis) imputation accuracy prior to tuning.</title><p>Beagle5 and untuned autoencoders were tested across three independent datasets - MESA (top), Wellderly (middle), and HGDP (bottom) and across three genotyping array platforms - Affymetrix 6.0 (left), UKB Axiom (middle), Omni1.5M (right). Each data point represents the imputation accuracy (average r-squared per variant) for an individual genomic segment relative to its WGS-based ground truth. The numerical values presented on the left side and below the identity line (dashed line) indicate the number of genomic segments in which Beagle5 outperformed the untuned autoencoder (left of identity line) and the number of genomic segments in which the untuned autoencoder surpassed Beagle5 (below the identity line). Statistical significance was assessed through two-proportion Z-test p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 2.</label><caption><title>Impute5 (y-axis) versus autoencoder-based (x-axis) imputation accuracy prior to tuning.</title><p>Impute5 and untuned autoencoders were tested across three independent datasets - MESA (top), Wellderly (middle), and HGDP (bottom) and across three genotyping array platforms - Affymetrix 6.0 (left), UKB Axiom (middle), Omni1.5M (right). Each data point represents the imputation accuracy (average r-squared per variant) for an individual genomic segment relative to its WGS-based ground truth. The numerical values presented on the left side and below the identity line (dashed line) indicate the number of genomic segments in which Impute5 outperformed the untuned autoencoder (left of identity line) and the number of genomic segments in which the untuned autoencoder surpassed Impute5 (below the identity line). Statistical significance was assessed through two-proportion Z-test p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 3.</label><caption><title>Relationship between genomic segment features and autoencoder performance.</title><p>Spearman correlations (Ï) between genomic segment features and autoencoder performance metrics are presented. An <italic>âXâ</italic> denotes Spearman correlations that are not statistically significant (p&gt;0.05). The performance metrics include the mean validation accuracy of Minimac4 and autoencoder (R2_AE_MINUS_MINIMAC), the autoencoderâs improvement in accuracy observed after offspring formation (AE_IMPROVEMENT_SIM) and the autoencoderâs improvement in accuracy after fine tuning of hyperparameters (AE_IMPROVEMENT_TUNING). The genomic features include the total number of variants per genomic segment in HRC (NVAR_HRC), proportion of rare variants at MAF â¤0.5% threshold (RARE_VAR_PROP), proportion of common variants at MAF &gt;0.5% threshold (COMMON_VAR_PROP), number of components needed to explain at least 90% of variance after running Principal Component Analysis (NCOMP), proportion of heterozygous genotypes (PROP_HET), proportion of unique haplotypes (PROP_UNIQUE_HAP) and diplotypes (PROP_UNIQUE_DIP), sum of ratios of explained variance from first two (EXP_RATIO_C1_C2) and three (EXP_RATIO_C1_C2_C3) components from Principal Component Analysis, recombination per variant per variant (REC_PER_SITE), mean pairwise correlation across all variants in each genomic segment (MEAN_LD), mean MAF (MEAN_MAF), GC content of reference alleles (GC_CONT_REF), GC content of alternate alleles (GC_CONT_ALT).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig2-figsupp3-v2.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 4.</label><caption><title>Projecting autoencoder performance from hyperparameters and genomic features.</title><p>We developed an ensemble-based machine learning approach (Extreme Gradient Boosting - XGBoost) to predict the expected performance (r-squared) of each hyperparameter combination per genomic segment using the results of the coarse-grid search and predictive features calculated for each genomic segment (see Materials and methods). We plot the observed accuracy of trained autoencoders versus the accuracy predicted by the XGBoost model after 10-fold cross-validation. Each subplot shows one iteration of the 10-fold validation process and its respective Pearson correlation between the predicted and observed accuracy values in the ARIC validation dataset.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig2-figsupp4-v2.tif"/></fig></fig-group><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Performance comparisons between untuned autoencoder (AE) and HMM-based imputation tools (Minimac4, Beagle5, and Impute5).</title><p>Average r-squared per variant was extracted from each genomic segment of chromosome 22. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the reference tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top"/><th align="left" valign="top">MESA</th><th align="left" valign="top">Wellderly</th><th align="left" valign="top">HGDP</th><th align="left" valign="top">Affymetrix 6.0</th><th align="left" valign="top">UKB Axiom</th><th align="left" valign="top">Omni 1.5 M</th><th align="left" valign="top">Combined</th></tr></thead><tbody><tr><td align="left" valign="top">AE (untuned)</td><td align="char" char="plusmn" valign="top">0.303Â±0.008</td><td align="char" char="plusmn" valign="top">0.470Â±0.009</td><td align="char" char="plusmn" valign="top">0.285Â±0.006</td><td align="char" char="plusmn" valign="top">0.339Â±0.008</td><td align="char" char="plusmn" valign="top">0.356Â±0.007</td><td align="char" char="plusmn" valign="top">0.362Â±0.008</td><td align="char" char="plusmn" valign="top">0.352Â±0.008</td></tr><tr><td align="left" valign="top">Minimac4</td><td align="char" char="plusmn" valign="top">0.337Â±0.007<sup>*</sup></td><td align="char" char="plusmn" valign="top">0.471Â±0.008</td><td align="char" char="plusmn" valign="top">0.314Â±0.006<sup>**</sup></td><td align="char" char="plusmn" valign="top">0.352Â±0.008</td><td align="char" char="plusmn" valign="top">0.370Â±0.006</td><td align="char" char="plusmn" valign="top">0.400Â±0.007<sup>**</sup></td><td align="char" char="plusmn" valign="top">0.374Â±0.007<sup>*</sup></td></tr><tr><td align="left" valign="top">Beagle5</td><td align="char" char="plusmn" valign="top">0.336Â±0.007<sup>*</sup></td><td align="char" char="plusmn" valign="top">0.460Â±0.008</td><td align="char" char="plusmn" valign="top">0.296Â±0.005</td><td align="char" char="plusmn" valign="top">0.342Â±0.007</td><td align="char" char="plusmn" valign="top">0.367Â±0.006</td><td align="char" char="plusmn" valign="top">0.384Â±0.007<sup>*</sup></td><td align="char" char="plusmn" valign="top">0.364Â±0.007</td></tr><tr><td align="left" valign="top">Impute5</td><td align="char" char="plusmn" valign="top">0.326Â±0.007<sup>*</sup></td><td align="char" char="plusmn" valign="top">0.458Â±0.008</td><td align="char" char="plusmn" valign="top">0.289Â±0.006</td><td align="char" char="plusmn" valign="top">0.336Â±0.008</td><td align="char" char="plusmn" valign="top">0.354Â±0.006</td><td align="char" char="plusmn" valign="top">0.383Â±0.008<sup>*</sup></td><td align="char" char="plusmn" valign="top">0.358Â±0.007</td></tr></tbody></table></table-wrap><p>In order to understand the relationship between genomic segment features, hyperparameter values, and imputation performance, we calculated predictive features (see Materials and methods) for each genomic segment and determined their Spearman correlation with the differences in r-squared observed for the autoencoder vs Minimac4 (<xref ref-type="fig" rid="fig2s3">Figure 2âfigure supplement 3</xref>). We observed that the autoencoder had superior performance when applied to the genomic segments with the most complex LD structures: those with larger numbers of observed unique haplotypes, unique diplotypes, and heterozygosity, as well as high average MAF, and low average pairwise Pearson correlation across all SNPs (average LD) (Spearman correlation Ïâ¥0.22, pâ¤9.8 Ã 10<sup>â04</sup>). Similarly, we quantified genomic segment complexity by the proportion of variance explained by the first two principal components as well as the number of principal components needed to explain at least 90% of the variance of HRC genotypes from each genomic segment. Concordantly, superior autoencoder performance was associated with a low proportion explained by the first two components and positively correlated with the number of components required to explained 90% of variance (Spearman Ïâ¥0.22, pâ¤8.3 Ã 10<sup>â04</sup>). These observations, with predictive features determined in the HRC training dataset and performance determined in the ARIC validation dataset, informed our tuning strategy.</p><p>We then used the genomic features significantly correlated with imputation performance in the ARIC validation dataset to predict the performance of and select the hyperparameter values to advance to fine-tuning. An ensemble model inference approach was able to predict the genomic segment-specific performance of hyperparameter combinations with high accuracy (<xref ref-type="fig" rid="fig2s4">Figure 2âfigure supplement 4</xref>, mean r-squared=0.935 Â± 0.002 of predicted vs observed autoencoders accuracies via 10-fold cross validation). The top 10 best performing hyperparameter combinations were advanced to fine-tuning (<xref ref-type="table" rid="table3">Table 3</xref>). Autoencoder tuning with simulated offspring formation was then executed as described in Materials and methods.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Top 10 best performing hyperparameter combinations that advanced to fine-tuning.</title><p>See Materials and methods and <xref ref-type="table" rid="table1">Table 1</xref> for a detailed description of the hyperparameters.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top">Î»<sub>1</sub></th><th align="left" valign="top">Î»<sub>2</sub></th><th align="left" valign="top">Î²</th><th align="left" valign="top">Ï</th><th align="left" valign="top">Activation</th><th align="left" valign="top">Learn rate</th><th align="left" valign="top">Î³</th><th align="left" valign="top">Optimizer</th><th align="left" valign="top">Loss type</th><th align="left" valign="top">Hidden layers</th><th align="left" valign="top">Size ratio</th><th align="left" valign="top">Decay</th></tr></thead><tbody><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.01</td><td align="char" char="." valign="top">0.01</td><td align="left" valign="top">tanh</td><td align="char" char="ndash" valign="top">1.0*10<sup>â4</sup></td><td align="char" char="." valign="top">0</td><td align="left" valign="top">adam</td><td align="left" valign="top">CE</td><td align="char" char="." valign="top">4</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.95</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.5</td><td align="left" valign="top">sigmoid</td><td align="char" char="ndash" valign="top">1.0*10<sup>â4</sup></td><td align="char" char="." valign="top">1</td><td align="left" valign="top">adam</td><td align="left" valign="top">CE</td><td align="char" char="." valign="top">2</td><td align="char" char="." valign="top">0.9</td><td align="char" char="." valign="top">0.95</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">5</td><td align="char" char="." valign="top">0.5</td><td align="left" valign="top">sigmoid</td><td align="char" char="ndash" valign="top">1.0*10<sup>â1</sup></td><td align="char" char="." valign="top">4</td><td align="left" valign="top">adam</td><td align="left" valign="top">CE</td><td align="char" char="." valign="top">2</td><td align="char" char="." valign="top">0.5</td><td align="char" char="." valign="top">0</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.005</td><td align="left" valign="top">relu</td><td align="char" char="ndash" valign="top">1.0*10<sup>â1</sup></td><td align="char" char="." valign="top">4</td><td align="left" valign="top">adam</td><td align="left" valign="top">FL</td><td align="char" char="." valign="top">6</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.25</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">5</td><td align="char" char="." valign="top">0.01</td><td align="left" valign="top">relu</td><td align="char" char="ndash" valign="top">1.0*10<sup>â5</sup></td><td align="char" char="." valign="top">5</td><td align="left" valign="top">adam</td><td align="left" valign="top">FL</td><td align="char" char="." valign="top">4</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.95</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.01</td><td align="char" char="." valign="top">0.1</td><td align="left" valign="top">leakyrelu</td><td align="char" char="ndash" valign="top">1.0*10<sup>â5</sup></td><td align="char" char="." valign="top">0</td><td align="left" valign="top">adam</td><td align="left" valign="top">FL</td><td align="char" char="." valign="top">8</td><td align="char" char="." valign="top">0.9</td><td align="char" char="." valign="top">0.95</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.01</td><td align="left" valign="top">tanh</td><td align="char" char="ndash" valign="top">1.0*10<sup>â4</sup></td><td align="char" char="." valign="top">0</td><td align="left" valign="top">adam</td><td align="left" valign="top">CE</td><td align="char" char="." valign="top">6</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.95</td></tr><tr><td align="char" char="." valign="top">0</td><td align="char" char="ndash" valign="top">1.0*10<sup>â8</sup></td><td align="char" char="." valign="top">0.001</td><td align="char" char="." valign="top">0.05</td><td align="left" valign="top">relu</td><td align="char" char="ndash" valign="top">1.0*10<sup>â5</sup></td><td align="char" char="." valign="top">4</td><td align="left" valign="top">adam</td><td align="left" valign="top">CE</td><td align="char" char="." valign="top">8</td><td align="char" char="." valign="top">0.6</td><td align="char" char="." valign="top">0.95</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.01</td><td align="left" valign="top">relu</td><td align="char" char="ndash" valign="top">1.0*10<sup>â1</sup></td><td align="char" char="." valign="top">5</td><td align="left" valign="top">adam</td><td align="left" valign="top">FL</td><td align="char" char="." valign="top">8</td><td align="char" char="." valign="top">0.9</td><td align="char" char="." valign="top">0</td></tr><tr><td align="char" char="." valign="top">0.1</td><td align="char" char="." valign="top">0</td><td align="char" char="." valign="top">0.01</td><td align="char" char="." valign="top">0.01</td><td align="left" valign="top">tanh</td><td align="char" char="ndash" valign="top">1.0*10<sup>â3</sup></td><td align="char" char="." valign="top">5</td><td align="left" valign="top">adam</td><td align="left" valign="top">CE</td><td align="char" char="." valign="top">2</td><td align="char" char="." valign="top">1</td><td align="char" char="." valign="top">0.95</td></tr></tbody></table></table-wrap></sec><sec id="s3-2"><title>Tuned performance</title><p>After tuning, autoencoder performance surpassed HMM-based imputation performance across all imputation methods, independent test datasets, and genotyping array marker sets. At a minimum, autoencoders surpassed HMM-based imputation performance in &gt;62% of chromosome 22 genomic segments (two proportion test p=1.02 Ã 10<sup>â11</sup>) (Minimac4: <xref ref-type="fig" rid="fig3">Figure 3</xref>, Beagle5 and Eagle5: <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplements 1</xref>â<xref ref-type="fig" rid="fig3s2">2</xref>). Overall, the optimized autoencoders exhibited superior performance with an average r-squared of 0.395Â±0.007 vs 0.374Â±0.007 for Minimac4 (Wilcoxon rank sum test p=0.007), 0.364Â±0.007 for Beagle5 (Wilcoxon rank sum test p=1.53*10<sup>â4</sup>), and 0.358Â±0.007 for Impute5 (Wilcoxon rank sum test p=2.01*10<sup>â5</sup>) (<xref ref-type="table" rid="table4">Table 4</xref>). This superiority was robust to the marker sets tested, with the mean r-squared per genomic segment for autoencoders being 0.373Â±0.008, 0.399Â±0.007, and 0.414Â±0.008 vs 0.352Â±0.008, 0.370Â±0.006, and 0.400Â±0.007 for Minimac4 using Affymetrix 6.0, UKB Axiom, and Omni 1.5 M marker sets (Wilcoxon rank-sums test <italic>P</italic>-value = 0.029, 1.99*10<sup>â4</sup>, and 0.087, respectively). Detailed comparisons to Beagle5 and Eagle5 are presented in <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplements 1</xref>â<xref ref-type="fig" rid="fig3s2">2</xref>.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>HMM-based (y-axis) versus autoencoder-based (axis) imputation accuracy after tuning.</title><p>Minimac4 and tuned autoencoders were validated across three independent datasetsâ- MESA (top), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) and across three genotyping array platformsâ- Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (average r-squared per variant) for an individual genomic segment relative to its WGS-based ground truth. The numerical values presented on the left side and below the identity line (dashed line) indicate the number of genomic segments in which Minimac4 outperformed the untuned autoencoder (left of identity line) and the number of genomic segments in which the untuned autoencoder surpassed Minimac4 (below the identity line). Statistical significance was assessed through two-proportion Z-test p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Beagle5 (y-axis) versus autoencoder-based (axis) imputation accuracy after tuning.</title><p>Beagle5 and tuned autoencoders were validated across three independent datasets - MESA (top), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (average r-squared per variant) for an individual genomic segment relative to its WGS-based ground truth. The numerical values presented on the left side and below the identity line (dashed line) indicate the number of genomic segments in which Beagle5 outperformed the untuned autoencoder (left of identity line) and the number of genomic segments in which the untuned autoencoder surpassed Beagle5 (below the identity line). Statistical significance was assessed through two-proportion Z-test p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 2.</label><caption><title>Impute5 (y-axis) versus autoencoder-based (axis) imputation accuracy after tuning.</title><p>Impute5 and tuned autoencoders were validated across three independent datasets - MESA (<bold>top</bold>), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (average r-squared per variant) for an individual genomic segment relative to its WGS-based ground truth. The numerical values presented on the left side and below the identity line (dashed line) indicate the number of genomic segments in which Impute5 outperformed the untuned autoencoder (left of identity line) and the number of genomic segments in which the untuned autoencoder surpassed Impute5 (below the identity line). Statistical significance was assessed through two-proportion Z-test p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 3.</label><caption><title>Imputation accuracy as a function of unique haplotype abundance.</title><p>Minimac4 and tuned and untuned autoencoders (AE) were tested across three independent datasets - MESA (top), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). âManyâ vs âFewâ haplotypes are defined by splitting genomic segments into those with greater than vs less than the median number of unique haplotypes per genomic segment. We applied Wilcoxon rank-sum tests to compare the untuned and tuned autoencoder to Minimac4. The validation datasets consist of: (<bold>A</bold>) MESA Affymetrix 6.0; (<bold>B</bold>) MESA UKB Axiom; (<bold>C</bold>) MESA Omni 1.5 M; (<bold>D</bold>) Wellderly Affymetrix 6.0; (<bold>E</bold>) Wellderly UKB Axiom; (<bold>F</bold>) Wellderly Omni 1.5 M; (<bold>G</bold>) HGDP Affymetrix 6.0; (<bold>H</bold>) HGDP UKB Axiom; (<bold>I</bold>) HGDP Omni 1.5 M.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-figsupp3-v2.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 4.</label><caption><title>Imputation accuracy as a function of unique diplotype abundance.</title><p>Minimac4 and tuned and untuned autoencoders (AE) were tested across three independent datasets - MESA (top), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) - and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). âManyâ vs âFewâ diplotypes are defined by splitting genomic segments into those with greater than vs less than the median number of unique diplotypes per genomic segment. We applied Wilcoxon rank-sum tests to compare the untuned and tuned autoencoder to Minimac4. The validation datasets consist of: (<bold>A</bold>) MESA Affymetrix 6.0; (<bold>B</bold>) MESA UKB Axiom; (<bold>C</bold>) MESA Omni 1.5 M; (<bold>D</bold>) Wellderly Affymetrix 6.0; (<bold>E</bold>) Wellderly UKB Axiom; (<bold>F</bold>) Wellderly Omni 1.5 M; (<bold>G</bold>) HGDP Affymetrix 6.0; (<bold>H</bold>) HGDP UKB Axiom; (<bold>I</bold>) HGDP Omni 1.5 M.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-figsupp4-v2.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 5.</label><caption><title>Imputation accuracy as a function of linkage disequilibrium (LD).</title><p>Minimac4 and tuned and untuned autoencoders (AE) were tested across three independent datasets - MESA (<bold>top</bold>), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) - and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). âHighâ vs âLowâ LD is defined by splitting genomic segments into those with greater than vs less than the average pairwise LD strength per genomic segment. We applied Wilcoxon rank-sum tests to compare the untuned and tuned autoencoder to Minimac4. The validation datasets consist of: (A) MESA Affymetrix 6.0; (<bold>B</bold>) MESA UKB Axiom; (<bold>C</bold>) MESA Omni 1.5 M; (<bold>D</bold>) Wellderly Affymetrix 6.0; (<bold>E</bold>) Wellderly UKB Axiom; (<bold>F</bold>) Wellderly Omni 1.5 M; (<bold>G</bold>) HGDP Affymetrix 6.0; (<bold>H</bold>) HGDP UKB Axiom; (<bold>I</bold>) HGDP Omni 1.5 M.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-figsupp5-v2.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 6.</label><caption><title>Imputation accuracy as a function of data complexity.</title><p>Minimac4 and tuned and untuned autoencoders (AE) were tested across three independent datasets - MESA (top), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) - and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). âHighâ vs âLowâ data complexity is defined by splitting genomic segments into those with greater than vs less than the median proportion of variance explained by first two components of Principal Component Analysis per genomic segment (PCA C1+C2). We applied Wilcoxon rank-sum tests to compare the untuned and tuned autoencoder to Minimac4. The validation datasets consist of: (<bold>A</bold>) MESA Affymetrix 6.0; (<bold>B</bold>) MESA UKB Axiom; (<bold>C</bold>) MESA Omni 1.5 M; (<bold>D</bold>) Wellderly Affymetrix 6.0; (<bold>E</bold>) Wellderly UKB Axiom; (<bold>F</bold>) Wellderly Omni 1.5 M; (<bold>G</bold>) HGDP Affymetrix 6.0; (<bold>H</bold>) HGDP UKB Axiom; (<bold>I</bold>) HGDP Omni 1.5 M.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-figsupp6-v2.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 7.</label><caption><title>Imputation accuracy as a function of recombination rate.</title><p>Minimac4 and tuned and untuned autoencoders (AE) were tested across three independent datasets - MESA (<bold>top</bold>), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) - and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). âHighâ vs âLowâ recombination rate is defined by splitting genomic segments in those with greater than vs less than the median recombination rate per variant per genomic segment. We applied Wilcoxon rank-sum tests to compare the untuned and tuned autoencoder to Minimac4. The validation datasets consist of: (<bold>A</bold>) MESA Affymetrix 6.0; (<bold>B</bold>) MESA UKB Axiom; (<bold>C</bold>) MESA Omni 1.5 M; (<bold>D</bold>) Wellderly Affymetrix 6.0; (<bold>E</bold>) Wellderly UKB Axiom; (<bold>F</bold>) Wellderly Omni 1.5 M; (<bold>G</bold>) HGDP Affymetrix 6.0; (<bold>H</bold>) HGDP UKB Axiom; (<bold>I</bold>) HGDP Omni 1.5 M.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig3-figsupp7-v2.tif"/></fig></fig-group><table-wrap id="table4" position="float"><label>Table 4.</label><caption><title>Performance comparisons between tuned autoencoder (AE) and HMM-based imputation tools (Minimac4, Beagle5, and Impute5).</title><p>Average r-squared per variant was extracted from each genomic segment of chromosome 22. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the reference untuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom"/><th align="left" valign="bottom">MESA</th><th align="left" valign="bottom">Wellderly</th><th align="left" valign="bottom">HGDP</th><th align="left" valign="bottom">Affymetrix 6.0</th><th align="left" valign="bottom">UKB Axiom</th><th align="left" valign="bottom">Omni 1.5 M</th><th align="left" valign="bottom">Combined</th></tr></thead><tbody><tr><td align="left" valign="bottom">AE (tuned)</td><td align="char" char="plusmn" valign="bottom">0.355Â±0.007</td><td align="char" char="plusmn" valign="bottom">0.505Â±0.008</td><td align="char" char="plusmn" valign="bottom">0.327Â±0.006</td><td align="char" char="plusmn" valign="bottom">0.373Â±0.008</td><td align="char" char="plusmn" valign="bottom">0.399Â±0.007</td><td align="char" char="plusmn" valign="bottom">0.414Â±0.008</td><td align="char" char="plusmn" valign="bottom">0.396Â±0.007</td></tr><tr><td align="left" valign="bottom">AE (untuned)</td><td align="char" char="plusmn" valign="bottom">0.303Â±0.008<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.470Â±0.009<sup>*</sup></td><td align="char" char="plusmn" valign="bottom">0.285Â±0.006<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.339Â±0.008<sup>*</sup></td><td align="char" char="plusmn" valign="bottom">0.356Â±0.007<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.362Â±0.008<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.352Â±0.008<sup>***</sup></td></tr><tr><td align="left" valign="bottom">Minimac4</td><td align="char" char="plusmn" valign="bottom">0.337Â±0.007<sup>*</sup></td><td align="char" char="plusmn" valign="bottom">0.471Â±0.008<sup>**</sup></td><td align="char" char="plusmn" valign="bottom">0.314Â±0.006</td><td align="char" char="plusmn" valign="bottom">0.352Â±0.008<sup>*</sup></td><td align="char" char="plusmn" valign="bottom">0.370Â±0.006<sup>**</sup></td><td align="char" char="plusmn" valign="bottom">0.400Â±0.007</td><td align="char" char="plusmn" valign="bottom">0.374Â±0.007<sup>*</sup></td></tr><tr><td align="left" valign="bottom">Beagle5</td><td align="char" char="plusmn" valign="bottom">0.336Â±0.007<sup>*</sup></td><td align="char" char="plusmn" valign="bottom">0.460Â±0.008<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.296Â±0.005<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.342Â±0.007<sup>**</sup></td><td align="char" char="plusmn" valign="bottom">0.367Â±0.006<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.384Â±0.007<sup>**</sup></td><td align="char" char="plusmn" valign="bottom">0.364Â±0.007<sup>**</sup></td></tr><tr><td align="left" valign="bottom">Impute5</td><td align="char" char="plusmn" valign="bottom">0.326Â±0.007<sup>*</sup></td><td align="char" char="plusmn" valign="bottom">0.458Â±0.008<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.289Â±0.006<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.336Â±0.008<sup>**</sup></td><td align="char" char="plusmn" valign="bottom">0.354Â±0.006<sup>***</sup></td><td align="char" char="plusmn" valign="bottom">0.383Â±0.008<sup>**</sup></td><td align="char" char="plusmn" valign="bottom">0.358Â±0.007<sup>***</sup></td></tr></tbody></table></table-wrap><p>Tuning improved performance of the autoencoders across all genomic segments, generally improving the superiority of autoencoders relative to HMM-based approaches in genomic segments with complex haplotype structures while equalizing performance relative to HMM-based approaches in genomic segments with more simple LD structures (as described in Materials and methods, by the number of unique haplotypes: <xref ref-type="fig" rid="fig3s3">Figure 3âfigure supplement 3</xref>, diplotypes: <xref ref-type="fig" rid="fig3s4">Figure 3âfigure supplement 4</xref>, average pairwise LD: <xref ref-type="fig" rid="fig3s5">Figure 3âfigure supplement 5</xref>, proportion variance explained: <xref ref-type="fig" rid="fig3s6">Figure 3âfigure supplement 6</xref>). Concordantly, genomic segments with higher recombination rates exhibited the largest degree of improvement with tuning (<xref ref-type="fig" rid="fig3s7">Figure 3âfigure supplement 7</xref>). Use of the augmented reference panel did not improve HMM-based imputation, having no influence on Minimac4 performance (original overall r-squared of 0.374Â±0.007 vs 0.363Â±0.007 after augmentation, Wilcoxon rank-sum test p=0.0917), and significantly degrading performance of Beagle5 and Impute5 (original r-squared of 0.364Â±0.007 and 0.358Â±0.007 vs 0.349Â±0.006 and 0.324Â±0.007 after augmentation, p=0.026 and p=1.26*10<sup>â4</sup>, respectively). Summary statistics for these comparisons are available in <xref ref-type="supplementary-material" rid="supp1">Supplementary file 1</xref>.</p></sec><sec id="s3-3"><title>Overall chromosome 22 imputation accuracy</title><p>After merging the results from all genomic segments, the whole chromosome accuracy of autoencoder-based imputation remained superior to all HMM-based imputation tools, across all independent test datasets, and all genotyping array marker sets (Wilcoxon rank-sums test pâ¤5.55 Ã 10<sup>â67</sup>). The autoencoderâs mean r-squared per variant ranged from 0.363 for HGDP to 0.605 for the Wellderly vs 0.340â0.557 for Minimac4, 0.326â0.549 for Beagle5, and 0.314â0.547 for Eagle5, respectively. Detailed comparisons are presented in in <xref ref-type="table" rid="table5">Table 5</xref> and <xref ref-type="supplementary-material" rid="supp2">Supplementary file 2</xref>.</p><table-wrap id="table5" position="float"><label>Table 5.</label><caption><title>Whole chromosome level comparisons between autoencoder (AE) and HMM-based imputation tools (Minimac4, Beagle5, and Impute5).</title><p>Average r-squared per variant was extracted at whole chromosome level. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the reference tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001. Standard errors that are equal or less than 0.001 are not shown.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2"/><th align="left" valign="bottom" colspan="3">MESA</th><th align="left" valign="bottom" colspan="3">Wellderly</th><th align="left" valign="bottom" colspan="3">HGDP</th></tr><tr><th align="left" valign="bottom">Affymetrix 6.0</th><th align="left" valign="bottom">UKB Axiom</th><th align="left" valign="bottom">Omni 1.5 M</th><th align="left" valign="bottom">Affymetrix 6.0</th><th align="left" valign="bottom">UKB Axiom</th><th align="left" valign="bottom">Omni 1.5 M</th><th align="left" valign="bottom">Affymetrix 6.0</th><th align="left" valign="bottom">UKB Axiom</th><th align="left" valign="bottom">Omni 1.5 M</th></tr></thead><tbody><tr><td align="left" valign="bottom">AE (tuned)</td><td align="char" char="." valign="bottom">0.410</td><td align="char" char="." valign="bottom">0.395</td><td align="char" char="." valign="bottom">0.452</td><td align="char" char="." valign="bottom">0.537</td><td align="char" char="." valign="bottom">0.605</td><td align="char" char="." valign="bottom">0.586</td><td align="char" char="." valign="bottom">0.363</td><td align="char" char="." valign="bottom">0.364</td><td align="char" char="." valign="bottom">0.392</td></tr><tr><td align="left" valign="bottom">Minimac4</td><td align="char" char="." valign="bottom">0.390<sup>***</sup></td><td align="char" char="." valign="bottom">0.364<sup>***</sup></td><td align="char" char="." valign="bottom">0.436<sup>***</sup></td><td align="char" char="." valign="bottom">0.500<sup>***</sup></td><td align="char" char="." valign="bottom">0.557<sup>***</sup></td><td align="char" char="." valign="bottom">0.551<sup>***</sup></td><td align="char" char="." valign="bottom">0.350<sup>***</sup></td><td align="char" char="." valign="bottom">0.340<sup>***</sup></td><td align="char" char="." valign="bottom">0.385<sup>***</sup></td></tr><tr><td align="left" valign="bottom">Beagle5</td><td align="char" char="." valign="bottom">0.383<sup>***</sup></td><td align="char" char="." valign="bottom">0.379<sup>***</sup></td><td align="char" char="." valign="bottom">0.420<sup>***</sup></td><td align="char" char="." valign="bottom">0.484<sup>***</sup></td><td align="char" char="." valign="bottom">0.549<sup>***</sup></td><td align="char" char="." valign="bottom">0.534<sup>***</sup></td><td align="char" char="." valign="bottom">0.326<sup>***</sup></td><td align="char" char="." valign="bottom">0.328<sup>***</sup></td><td align="char" char="." valign="bottom">0.353<sup>***</sup></td></tr><tr><td align="left" valign="bottom">Impute5</td><td align="char" char="." valign="bottom">0.384<sup>***</sup></td><td align="char" char="." valign="bottom">0.356<sup>***</sup></td><td align="char" char="." valign="bottom">0.429<sup>***</sup></td><td align="char" char="." valign="bottom">0.485<sup>***</sup></td><td align="char" char="." valign="bottom">0.547<sup>***</sup></td><td align="char" char="." valign="bottom">0.539<sup>***</sup></td><td align="char" char="." valign="bottom">0.328<sup>***</sup></td><td align="char" char="." valign="bottom">0.314<sup>***</sup></td><td align="char" char="." valign="bottom">0.359<sup>***</sup></td></tr></tbody></table></table-wrap><p>Further, when imputation accuracy is stratified by MAF bins, the autoencoders maintain superiority across all MAF bins by nearly all test dataset and genotyping array marker sets (<xref ref-type="fig" rid="fig4">Figure 4</xref>, and <xref ref-type="supplementary-material" rid="supp3">Supplementary file 3</xref>). Concordantly, autoencoder imputation accuracy is similarly superior when measured with F1-scores (<xref ref-type="fig" rid="fig4s1">Figure 4âfigure supplement 1</xref>) and concordance (<xref ref-type="fig" rid="fig4s2">Figure 4âfigure supplement 2</xref>), though these metrics are less sensitive at capturing differences in rare variant imputation accuracy.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>HMM-based versus autoencoder-based imputation accuracy across MAF bins.</title><p>Autoencoder-based (<bold>red</bold>) and HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation accuracy was validated across three independent datasetsâ- MESA (<bold>top</bold>), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) and across three genotyping array platformsâ- Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (average r-squared per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 1.</label><caption><title>HMM-based versus autoencoder-based imputation accuracy across MAF bins (F1 score).</title><p>Autoencoder-based (<bold>red</bold>) and HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation accuracy was validated across three independent datasets - MESA (<bold>top</bold>), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) - and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (mean F1-score per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values. Please note that F1 scores are high for rare variations given the high degree of class imbalance, most alternative alleles are not present for rare variants, leading to high accuracy in the negative class. R-squared depicted in <xref ref-type="fig" rid="fig4">Figure 4</xref> provides a more accurate picture of balanced class accuracy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 2.</label><caption><title>HMM-based versus autoencoder-based imputation accuracy across MAF bins (concordance).</title><p>Autoencoder-based (<bold>red</bold>) and HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation accuracy was validated across three independent datasets - MESA (<bold>top</bold>), Wellderly (<bold>middle</bold>), and HGDP (<bold>bottom</bold>) - and across three genotyping array platforms - Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (mean concordance per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values. Please note that F1 scores are high for rare variations given the high degree of class imbalance, most alternative alleles are not present for rare variants, leading to high accuracy in the negative class. R-squared depicted in <xref ref-type="fig" rid="fig4">Figure 4</xref> provides a more accurate picture of balanced class accuracy.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4âfigure supplement 3.</label><caption><title>TOPMed cohort HMM-based imputation versus HRC cohort autoencoder-based imputation accuracy across MAF bins.</title><p>Autoencoder-based imputation using the HRC reference panel (<bold>red</bold>) was compared to HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation accuracy using the upgraded TOPMed cohort. Accuracy was determined across three datasetsâ- MESA (<bold>top â</bold> not independent), Wellderly (<bold>middle</bold> - independent), and HGDP (<bold>bottom</bold> - independent) and across three genotyping array platformsâ- Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>). Each data point represents the imputation accuracy (average r-squared per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig4-figsupp3-v2.tif"/></fig></fig-group><p>When we upgraded the reference panel of the HMM-based tools with the more expansive TOPMed cohort, the superior performance of the HRC-trained autoencoder was still sustained across all datasets except for MESA (<xref ref-type="fig" rid="fig4s3">Figure 4âfigure supplement 3</xref>). Given that MESA is a sub-cohort of the TOPMed cohort, we evaluated the possibility of residual data leakage after the removal of MESA from the TOPMed cohort and found that 44 MESA individuals were duplicated in other TOPMed cohorts, 182 MESA individuals had a first degree relative in other TOPMed cohorts, and &gt;92% of MESA individuals had at least one second degree relative in other TOPMed cohorts, resulting in improved imputation performance. Notably, across the most diverse and truly independent HGDP validation dataset, the autoencoder displays superior performance despite only being exposed to training on the less diverse HRC reference cohort.</p></sec><sec id="s3-4"><title>Ancestry-specific chromosome 22 imputation accuracy</title><p>Finally, we evaluated ancestry-specific imputation accuracy. As before, overall autoencoder-based imputation maintains superiority across all continental populations present in MESA (<xref ref-type="fig" rid="fig5">Figure 5</xref>, Wilcoxon rank-sums test p=5.39 Ã 10<sup>â19</sup>). The autoencodersâ mean r-squared ranged from 0.357 for African ancestry to 0.614 for East Asian ancestry vs 0.328â0.593 for Minimac4, 0.330â0.544 for Beagle5, and 0.324â0.586 for Impute5, respectively. Note, East Asian ancestry exhibits a slightly higher overall imputation accuracy relative to European ancestry due to improved rare variant imputation. Autoencoder superiority replicates when HGDP is split into continental populations (<xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>).</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>HMM-based versus autoencoder-based imputation accuracy across ancestry groups.</title><p>Autoencoder-based (<bold>red</bold>) and HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation accuracy was validated across individuals of diverse ancestry from MESA cohort (EUR: European (<bold>top</bold>); EAS: East Asian (<bold>2<sup>nd</sup> row</bold>); AMR: Native American (<bold>3<sup>rd</sup> row</bold>); AFR: African (<bold>bottom</bold>)) and multiple genotype array platforms (Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>)). Each data point represents the imputation accuracy (average r-squared per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 1.</label><caption><title>HMM-based versus autoencoder-based imputation accuracy across ancestry groups.</title><p>Autoencoder-based (<bold>red</bold>) and HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation accuracy was validated across individuals of diverse ancestry from HGDP cohort (EUR: European (<bold>top</bold>); EAS: East Asian (<bold>2<sup>nd</sup> row</bold>); AMR: Native American (<bold>3<sup>rd</sup> row</bold>); AFR: African (<bold>bottom</bold>)) and multiple genotype array platforms (Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>)). Each data point represents the imputation accuracy (average r-squared per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig5-figsupp1-v2.tif"/></fig><fig id="fig5s2" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 2.</label><caption><title>TOPMed cohort HMM-based versus HRC cohort autoencoder-based imputation accuracy across ancestry groups.</title><p>Autoencoder-based imputation using the HRC reference panel (<bold>red</bold>) was compared to HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation using the TOPMed reference panel. Accuracy was determined across individuals of diverse ancestry from the HGDP cohort (EUR: European (<bold>top</bold>); EAS: East Asian (<bold>2<sup>nd</sup> row</bold>); AMR: Native American (<bold>3<sup>rd</sup> row</bold>); AFR: African (<bold>bottom</bold>)) and multiple genotype array platforms (Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>)). Each data point represents the imputation accuracy (average r-squared per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig5-figsupp2-v2.tif"/></fig><fig id="fig5s3" position="float" specific-use="child-fig"><label>Figure 5âfigure supplement 3.</label><caption><title>TOPMed cohort HMM-based versus HRC cohort autoencoder-based imputation accuracy across ancestry groups.</title><p>Autoencoder-based imputation using the HRC reference panel (<bold>red</bold>) was compared to HMM-based (Minimac4 (<bold>blue</bold>), Beagle5 (<bold>green</bold>), and Impute5 (<bold>purple</bold>)) imputation using the TOPMed reference panel. Accuracy was determined across individuals of diverse ancestry from the MESA cohort (EUR: European (<bold>top</bold>); EAS: East Asian (<bold>2<sup>nd</sup> row</bold>); AMR: Native American (<bold>3<sup>rd</sup> row</bold>); AFR: African (<bold>bottom</bold>)) and multiple genotype array platforms (Affymetrix 6.0 (<bold>left</bold>), UKB Axiom (<bold>middle</bold>), Omni1.5M (<bold>right</bold>)). Each data point represents the imputation accuracy (average r-squared per variant) relative to WGS-based ground truth across MAF bins. Error bars represent standard errors. We applied Wilcoxon rank-sum tests to compare the HMM-based tools to the tuned autoencoder (AE). * represents p-values â¤0.05, ** indicates p-values â¤0.001, and *** indicates p-values â¤0.0001, ns represents non-significant p-values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig5-figsupp3-v2.tif"/></fig></fig-group><p>Further stratification of ancestry-specific imputation accuracy results by MAF continues to support autoencoder superiority across all ancestries, MAF bins, and nearly all test datasets, and genotyping array marker sets (<xref ref-type="fig" rid="fig5">Figure 5</xref>, <xref ref-type="fig" rid="fig5s1">Figure 5âfigure supplement 1</xref>). Minimum and maximum accuracies across MAF by ancestry bins ranged between 0.177â0.937 for the autoencoder, 0.132â0.907 for Minimac4, 0.147â0.909 for Beagle5, and 0.115â0.903 for Impute5, with a maximum standard error of Â±0.004.</p><p>Thus, with training on equivalent reference cohorts, autoencoder performance was superior across all variant allele frequencies and ancestries with the primary source of superiority arising from hard to impute regions with complex LD structures. When the reference panel of the HMM-based tools is upgraded to the more diverse TOPMed dataset, the HRC-trained autoencoder remains superior across all ancestry groups of HGDP (<xref ref-type="fig" rid="fig5s2">Figure 5âfigure supplement 2</xref>), as well as in the MESA ancestries well represented in HRC (European and East Asian) but not in MESA ancestries where representation is significantly enhanced by the TOPMed reference panel (American and African) with additional imputation performance deriving from a significant degree of familial relationships spanning the TOPMed reference panel and MESA test cohort (<xref ref-type="fig" rid="fig5s3">Figure 5âfigure supplement 3</xref>).</p></sec><sec id="s3-5"><title>Inference speed</title><p>Inference runtimes for the autoencoder vs HMM-based methods were compared in a low-end and high-end computational environment as described in <italic>Methods</italic>. In the low-end environment, the autoencoderâs inference time is at least ~4 X faster than all HMM-based inference times (summing all inference times from all genomic segments of chromosome 22, the inference time for the autoencoder was 2.4Â±1.1*10<sup>â3</sup> seconds versus 1,754Â±3.2, 583.3Â±0.01, and 8.4Â±4.3*10<sup>â3</sup> s for Minimac4, Beagle5, and Impute5, respectively (<xref ref-type="fig" rid="fig6">Figure 6A</xref>)). In the high-end environment, this difference narrows to a~3 X advantage of the autoencoder vs HMM-based methods (2.1Â±8.0*10<sup>â4</sup> versus 374.3Â±1.2, 414.3Â±0.01, and 6.1Â±2.1*10<sup>â4</sup>) seconds for Minimac4, Beagle5, and Impute5, respectively (<xref ref-type="fig" rid="fig6">Figure 6B</xref>). These unoptimized results indicate that autoencoder-based imputation can be executed rapidly, without a reference cohort, and without the need for a high-end server or high-performance computing (HPC) infrastructure. However, we must note that to deploy the autoencoder-based imputation to production, the autoencoders must be pre-trained separately across all segments of all chromosomes in the human genome. This initial pre-training can require months of computation time, depending upon the GPU resources available, whereas HMM-based imputation does not require any pre-training after initial parameters are defined. Thus, the HMM-based approach is more flexible to the de-novo use of alternative reference panels â though recent cohorts have revealed scaling limitations. On the other hand, unlike HMM-based imputation tools, pre-trained autoencoders retain the information learned from pre-training and can be continuously fine-tuned with additional genomes and reference panels as they become available. Thus, once pre-trained, autoencoders may be incrementally upgraded using newly available reference panels.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>HMM-based versus autoencoder-based inference runtimes.</title><p>We plot the average time and standard error of three imputation replicates. Two hardware configurations were used for the tests: (<bold>A</bold>) a low-end environment: 16-core Intel Xeon CPU (E5-2640 v2 2.00 GHz), 250 GB RAM, and one GPU (NVIDIA GTX 1080); (<bold>B</bold>) a high-end environment: 24-Core AMD CPU (EPYC 7352 2.3 GHz), 250 GB RAM, using one NVIDIA A100 GPU.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-fig6-v2.tif"/></fig></sec></sec><sec id="s4" sec-type="discussion"><title>Discussion</title><p>Artificial neural network-based data mining techniques are revolutionizing biomedical informatics and analytics (<xref ref-type="bibr" rid="bib15">Dias and Torkamani, 2019</xref>; <xref ref-type="bibr" rid="bib24">Jumper et al., 2021</xref>). Here, we have demonstrated the potential for these techniques to execute a fundamental analytical task in population genetics, genotype imputation, producing superior results in a computational efficient and portable framework. The trained autoencoders can be transferred easily, and execute their functions rapidly, even in modest computing environments, obviating the need to transfer private genotype data to external imputation servers or services. Furthermore, our fully trained autoencoders robustly surpass the performance of all modern HMM-based imputation approaches across all tested independent datasets, genotyping array marker sets, minor allele frequency spectra, and diverse ancestry groups. This superiority was most apparent in genomic regions with low LD and/or high complexity in their linkage disequilibrium structure.</p><p>Superior imputation accuracy is expected to improve GWAS power, enable more complete coverage in meta-analyses, and improve causal variant identification through fine-mapping. Moreover, superior imputation accuracy in low LD regions may enable the more accurate interrogation of specific classes of genes under a greater degree of selective pressure and involved in environmental sensing. For example, promoter regions of genes associated with inflammatory immune responses, response to pathogens, environmental sensing, and neurophysiological processes (including sensory perception genes) are often located in regions of low LD (<xref ref-type="bibr" rid="bib15">Dias and Torkamani, 2019</xref>; <xref ref-type="bibr" rid="bib21">Frazer et al., 2007</xref>). These known disease-associated biological processes that are critical to interrogate accurately in GWAS. Thus, the autoencoder-based imputation approach both improves statistical power and biological coverage of individual GWASâ and downstream meta-analyses.</p><p>HMM-based imputation tools depend on end-user access to large reference panels or datasets to impute a single genome whereas pre-trained autoencoder models eliminate that dependency. However, further development is required to actualize this approach in practice for broad adoption. Autoencoders must be pre-trained and validated across all segments of the human genome â a computationally expensive task. Here we performed training only for chromosome 22. Autoencoder training is computationally intensive, shifting the computational burden to model trainers, and driving performance gains for end-users. As a result, inference time scales only with the number of variants to be imputed, whereas HMM-based inference time depends on both reference panel and the number of variants to be imputed. This allows for autoencoder-based imputation to extend to millions of genomes but introduces some challenges in the continuous re-training and fine-tuning of the pre-trained models as larger reference panels are made available. In addition, our current encoding approach lacks phasing information and no pre-phasing was performed. Pre-phasing can lead to substantial improvements in imputation accuracy. Future models will need to address the need for phasing and continuous fine-tuning of models for application to modern, ever-growing, genomic datasets.</p><sec id="s4-1"><title>Ideas and speculation</title><p>After expanding this approach across the whole genome, our work will provide a more efficient genotype imputation platform on whole genome scale and thus benefit genomic research especially in contexts where the computational power required for modern HMM-based imputation is not accessible. In addition to the speed, cost, and accuracy benefits, our proposed approach can potentially improve automation for downstream analyses. The autoencoder naturally generates a hidden encoding with latent features representative of the original data. This latent representation of the original data acts as an automatic feature extraction and dimensionality reduction technique for downstream tasks such as genetic risk prediction. Moreover, the autoencoder-based imputation approach only requires a reference panel during training â only the neural network needs to be distributed for implementation. Thus, the neural network is portable and avoids privacy issues associated with standard statistical imputation. This privacy-preserving feature will allow developers to deploy real-time data-driven algorithms on personal devices (edge computing). These new features will expand the clinical applications of genomic imputation, as well as its role in preventive healthcare. Another point related to data privacy is that the autoencoders segment the genome, making reconstruction of an individual genome impossible even if reference data were somehow recoverable from the neural networks. Nevertheless, while there are no official data sharing restrictions on deep learning model weights generated from genomic data, future privacy risks may be discovered, necessitating further research into privacy concerns and differential privacy techniques for autoencoder-based genotype imputation.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Validation, Investigation, Visualization, Methodology, Writing â original draft, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Software, Visualization, Methodology, Writing â review and editing</p></fn><fn fn-type="con" id="con3"><p>Data curation, Software, Validation, Methodology</p></fn><fn fn-type="con" id="con4"><p>Data curation, Software, Validation</p></fn><fn fn-type="con" id="con5"><p>Data curation, Formal analysis, Validation, Visualization, Writing â review and editing</p></fn><fn fn-type="con" id="con6"><p>Software, Validation, Methodology</p></fn><fn fn-type="con" id="con7"><p>Conceptualization, Resources, Supervision, Funding acquisition, Investigation, Project administration, Writing â review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-75600-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material><supplementary-material id="supp1"><label>Supplementary file 1.</label><caption><title>Performance comparisons between tuned autoencoder (AE) and HMM-based imputation tools (Minimac4, Beagle5, and Impute5) after applying data augmentation to HMM-based tools.</title></caption><media xlink:href="elife-75600-supp1-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp2"><label>Supplementary file 2.</label><caption><title>Detailed performance comparisons between tuned autoencoder (AE) and HMM-based imputation tools (Minimac4, Beagle5, and Impute5).</title></caption><media xlink:href="elife-75600-supp2-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material><supplementary-material id="supp3"><label>Supplementary file 3.</label><caption><title>Detailed performance comparisons between tuned autoencoder (AE) and HMM-based imputation tools (Minimac4, Beagle5, and Impute5).</title></caption><media xlink:href="elife-75600-supp3-v2.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data that support the findings of this study are available from dbGAP and European Genome-phenome Archive (EGA), but restrictions apply to the availability of these data, which were used under ethics approval for the current study, and so are not openly available to the public. The computational pipeline for autoencoder training and validation is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/Imputation_Autoencoder/tree/master/autoencoder_tuning_pipeline">https://github.com/TorkamaniLab/Imputation_Autoencoder/tree/master/autoencoder_tuning_pipeline</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:20c922e4cce40c9c9f017de70cbed0dafea410ec;origin=https://github.com/TorkamaniLab/Imputation_Autoencoder;visit=swh:1:snp:7604f13a3ae5a1471b1c6620b00dd37d16a6b33f;anchor=swh:1:rev:35d2e292e786ebc41e71f27809dad56b1e1933c4;path=/autoencoder_tuning_pipeline/">swh:1:rev:35d2e292e786ebc41e71f27809dad56b1e1933c4</ext-link>). The python script for calculating imputation accuracy is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/imputation_accuracy_calculator">https://github.com/TorkamaniLab/imputation_accuracy_calculator</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:27393c4be42545b487fe4f32cf34c200cd1e9d99;origin=https://github.com/TorkamaniLab/imputation_accuracy_calculator;visit=swh:1:snp:c520059cc24989cbe62e6e82c890d0aa1e14fcf0;anchor=swh:1:rev:e01229e3f245e8bb95b29d4f4f1e547fcff70ae4">swh:1:rev:e01229e3f245e8bb95b29d4f4f1e547fcff70ae4</ext-link>). Instructions on how to access the unique information on the parameters and hyperparameters of each one of the 256 autoencoders is shared through our source code repository at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/imputator_inference">https://github.com/TorkamaniLab/imputator_inference</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bcdf526c7102b44428af0a8edc41c95c449c7713;origin=https://github.com/TorkamaniLab/imputator_inference;visit=swh:1:snp:1f1e9662e49b6476f0475c52ca54929ae422184d;anchor=swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c">swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c</ext-link>). We also shared the pre-trained autoencoders and instructions on how to use them for imputation at <ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/imputator_inference">https://github.com/TorkamaniLab/imputator_inference</ext-link>. Imputation data format. The imputation results are exported in variant calling format (VCF) containing the imputed genotypes and imputation quality scores in the form of class probabilities for each one of the three possible genotypes (homozygous reference, heterozygous, and homozygous alternate allele). The probabilities can be used for quality control of the imputation results.</p><p>The following previously published datasets were used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>McCarthy</surname><given-names>S</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Kretzschmar</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><data-title>Haplotype Reference Consortium</data-title><source>EGA European Genome-Phenome Archive</source><pub-id pub-id-type="accession" xlink:href="https://ega-archive.org/studies/EGAS00001001710">EGAS00001001710</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset2"><person-group person-group-type="author"><collab>1000 Genomes Project Consortium</collab></person-group><year iso-8601-date="2015">2015</year><data-title>The 1000 Genomes Project Consortium</data-title><source>Internationalgenome</source><pub-id pub-id-type="accession" xlink:href="https://www.internationalgenome.org/data-portal/data-collection/phase-3">phase-3</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset3"><person-group person-group-type="author"><name><surname>Bild</surname><given-names>DE</given-names></name><name><surname>Bluemke</surname><given-names>DA</given-names></name><name><surname>Burke</surname><given-names>GL</given-names></name></person-group><year iso-8601-date="2002">2002</year><data-title>MESA (Multi-Ethnic Study of Atherosclerosis) study</data-title><source>NCBI Gene Expression Omnibus</source><pub-id pub-id-type="accession" xlink:href="https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs001416.v2.p1">phs001416.v2.p1</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset4"><person-group person-group-type="author"><collab>The ARIC investigators consortium</collab></person-group><year iso-8601-date="1989">1989</year><data-title>Atherosclerosis Risk in Communities (ARIC)</data-title><source>NCBI Gene Expression Omnibus</source><pub-id pub-id-type="accession" xlink:href="https://www.ncbi.nlm.nih.gov/projects/gap/cgi-bin/study.cgi?study_id=phs001211.v4.p3">phs001211.v4.p3</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset5"><person-group person-group-type="author"><name><surname>BergstrÃ¶m</surname><given-names>A</given-names></name><name><surname>McCarthy</surname><given-names>SA</given-names></name><name><surname>Hui</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>Human Genome Diversity Project (HGDP)</data-title><source>Internationalgenome</source><pub-id pub-id-type="accession" xlink:href="https://www.internationalgenome.org/data-portal/data-collection/hgdp">hgdp</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="references" id="dataset6"><person-group person-group-type="author"><name><surname>Taliun</surname><given-names>D</given-names></name><name><surname>Harris</surname><given-names>DN</given-names></name><name><surname>Kessler</surname><given-names>MD</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>TOPMed Cohort</data-title><source>Multiple</source><pub-id pub-id-type="accession" xlink:href="https://topmed.nhlbi.nih.gov/">topmed</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work is supported by R01HG010881 to AT, KL2TR002552 to RD, as well as grants U24TR002306 and UL1TR002550. We would like to thank JC Ducom and Lisa Dong from the Scripps High Performance Computing center, as well as Fernanda Foertter, Johnny Israeli, Ohad Mosafi, and Joyjit Daw from NVIDIA for their technical support and collaboration in this project. A portion of this research was conducted using a startup account at the Summit supercomputer from Oak Ridge National Laboratory (ORNL).</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abouzid</surname><given-names>H</given-names></name><name><surname>Chakkor</surname><given-names>O</given-names></name><name><surname>Reyes</surname><given-names>OG</given-names></name><name><surname>Ventura</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Signal speech reconstruction and noise removal using convolutional denoising audioencoders with neural deep learning</article-title><source>Analog Integrated Circuits and Signal Processing</source><volume>100</volume><fpage>501</fpage><lpage>512</lpage><pub-id pub-id-type="doi">10.1007/s10470-019-01446-6</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Alexander</surname><given-names>DH</given-names></name><name><surname>Novembre</surname><given-names>J</given-names></name><name><surname>Lange</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Fast model-based estimation of ancestry in unrelated individuals</article-title><source>Genome Research</source><volume>19</volume><fpage>1655</fpage><lpage>1664</lpage><pub-id pub-id-type="doi">10.1101/gr.094052.109</pub-id><pub-id pub-id-type="pmid">19648217</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Auton</surname><given-names>A</given-names></name><name><surname>Brooks</surname><given-names>LD</given-names></name><name><surname>Durbin</surname><given-names>RM</given-names></name><name><surname>Garrison</surname><given-names>EP</given-names></name><name><surname>Kang</surname><given-names>HM</given-names></name><name><surname>Korbel</surname><given-names>JO</given-names></name><name><surname>Marchini</surname><given-names>JL</given-names></name><name><surname>McCarthy</surname><given-names>S</given-names></name><name><surname>McVean</surname><given-names>GA</given-names></name><name><surname>Abecasis</surname><given-names>GR</given-names></name><collab>1000 Genomes Project Consortium</collab></person-group><year iso-8601-date="2015">2015</year><article-title>A global reference for human genetic variation</article-title><source>Nature</source><volume>526</volume><fpage>68</fpage><lpage>74</lpage><pub-id pub-id-type="doi">10.1038/nature15393</pub-id><pub-id pub-id-type="pmid">26432245</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berisa</surname><given-names>T</given-names></name><name><surname>Pickrell</surname><given-names>JK</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Approximately independent linkage disequilibrium blocks in human populations</article-title><source>Bioinformatics</source><volume>32</volume><fpage>283</fpage><lpage>285</lpage><pub-id pub-id-type="doi">10.1093/bioinformatics/btv546</pub-id><pub-id pub-id-type="pmid">26395773</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bild</surname><given-names>DE</given-names></name><name><surname>Bluemke</surname><given-names>DA</given-names></name><name><surname>Burke</surname><given-names>GL</given-names></name><name><surname>Detrano</surname><given-names>R</given-names></name><name><surname>Diez Roux</surname><given-names>AV</given-names></name><name><surname>Folsom</surname><given-names>AR</given-names></name><name><surname>Greenland</surname><given-names>P</given-names></name><name><surname>Jacob</surname><given-names>DR</given-names><suffix>Jr</suffix></name><name><surname>Kronmal</surname><given-names>R</given-names></name><name><surname>Liu</surname><given-names>K</given-names></name><name><surname>Nelson</surname><given-names>JC</given-names></name><name><surname>OâLeary</surname><given-names>D</given-names></name><name><surname>Saad</surname><given-names>MF</given-names></name><name><surname>Shea</surname><given-names>S</given-names></name><name><surname>Szklo</surname><given-names>M</given-names></name><name><surname>Tracy</surname><given-names>RP</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Multi-Ethnic study of atherosclerosis: objectives and design</article-title><source>American Journal of Epidemiology</source><volume>156</volume><fpage>871</fpage><lpage>881</lpage><pub-id pub-id-type="doi">10.1093/aje/kwf113</pub-id><pub-id pub-id-type="pmid">12397006</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="software"><person-group person-group-type="author"><collab>Broad Institute</collab></person-group><year iso-8601-date="2022">2022</year><data-title>Picard</data-title><version designator="5db8017">5db8017</version><source>GitHub</source><ext-link ext-link-type="uri" xlink:href="https://github.com/broadinstitute/picard">https://github.com/broadinstitute/picard</ext-link></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>BL</given-names></name><name><surname>Browning</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Genotype imputation with millions of reference samples</article-title><source>American Journal of Human Genetics</source><volume>98</volume><fpage>116</fpage><lpage>126</lpage><pub-id pub-id-type="doi">10.1016/j.ajhg.2015.11.020</pub-id><pub-id pub-id-type="pmid">26748515</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Browning</surname><given-names>BL</given-names></name><name><surname>Zhou</surname><given-names>Y</given-names></name><name><surname>Browning</surname><given-names>SR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A one-penny imputed genome from next-generation reference panels</article-title><source>American Journal of Human Genetics</source><volume>103</volume><fpage>338</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.ajhg.2018.07.015</pub-id><pub-id pub-id-type="pmid">30100085</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cann</surname><given-names>HM</given-names></name><name><surname>de Toma</surname><given-names>C</given-names></name><name><surname>Cazes</surname><given-names>L</given-names></name><name><surname>Legrand</surname><given-names>M-F</given-names></name><name><surname>Morel</surname><given-names>V</given-names></name><name><surname>Piouffre</surname><given-names>L</given-names></name><name><surname>Bodmer</surname><given-names>J</given-names></name><name><surname>Bodmer</surname><given-names>WF</given-names></name><name><surname>Bonne-Tamir</surname><given-names>B</given-names></name><name><surname>Cambon-Thomsen</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Chu</surname><given-names>J</given-names></name><name><surname>Carcassi</surname><given-names>C</given-names></name><name><surname>Contu</surname><given-names>L</given-names></name><name><surname>Du</surname><given-names>R</given-names></name><name><surname>Excoffier</surname><given-names>L</given-names></name><name><surname>Ferrara</surname><given-names>GB</given-names></name><name><surname>Friedlaender</surname><given-names>JS</given-names></name><name><surname>Groot</surname><given-names>H</given-names></name><name><surname>Gurwitz</surname><given-names>D</given-names></name><name><surname>Jenkins</surname><given-names>T</given-names></name><name><surname>Herrera</surname><given-names>RJ</given-names></name><name><surname>Huang</surname><given-names>X</given-names></name><name><surname>Kidd</surname><given-names>J</given-names></name><name><surname>Kidd</surname><given-names>KK</given-names></name><name><surname>Langaney</surname><given-names>A</given-names></name><name><surname>Lin</surname><given-names>AA</given-names></name><name><surname>Mehdi</surname><given-names>SQ</given-names></name><name><surname>Parham</surname><given-names>P</given-names></name><name><surname>Piazza</surname><given-names>A</given-names></name><name><surname>Pistillo</surname><given-names>MP</given-names></name><name><surname>Qian</surname><given-names>Y</given-names></name><name><surname>Shu</surname><given-names>Q</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Zhu</surname><given-names>S</given-names></name><name><surname>Weber</surname><given-names>JL</given-names></name><name><surname>Greely</surname><given-names>HT</given-names></name><name><surname>Feldman</surname><given-names>MW</given-names></name><name><surname>Thomas</surname><given-names>G</given-names></name><name><surname>Dausset</surname><given-names>J</given-names></name><name><surname>Cavalli-Sforza</surname><given-names>LL</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>A human genome diversity cell line panel</article-title><source>Science</source><volume>296</volume><fpage>261</fpage><lpage>262</lpage><pub-id pub-id-type="doi">10.1126/science.296.5566.261b</pub-id><pub-id pub-id-type="pmid">11954565</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaitanya</surname><given-names>CRA</given-names></name><name><surname>Kaplanyan</surname><given-names>AS</given-names></name><name><surname>Schied</surname><given-names>C</given-names></name><name><surname>Salvi</surname><given-names>M</given-names></name><name><surname>Lefohn</surname><given-names>A</given-names></name><name><surname>Nowrouzezahrai</surname><given-names>D</given-names></name><name><surname>Aila</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Interactive reconstruction of Monte Carlo image sequences using a recurrent denoising autoencoder</article-title><source>ACM Transactions on Graphics</source><volume>36</volume><fpage>1</fpage><lpage>12</lpage><pub-id pub-id-type="doi">10.1145/3072959.3073601</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Shi</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sparse convolutional denoising autoencoders for genotype imputation</article-title><source>Genes</source><volume>10</volume><elocation-id>652</elocation-id><pub-id pub-id-type="doi">10.3390/genes10090652</pub-id><pub-id pub-id-type="pmid">31466333</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Danecek</surname><given-names>P</given-names></name><name><surname>Bonfield</surname><given-names>JK</given-names></name><name><surname>Liddle</surname><given-names>J</given-names></name><name><surname>Marshall</surname><given-names>J</given-names></name><name><surname>Ohan</surname><given-names>V</given-names></name><name><surname>Pollard</surname><given-names>MO</given-names></name><name><surname>Whitwham</surname><given-names>A</given-names></name><name><surname>Keane</surname><given-names>T</given-names></name><name><surname>McCarthy</surname><given-names>SA</given-names></name><name><surname>Davies</surname><given-names>RM</given-names></name><name><surname>Li</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Twelve years of samtools and bcftools</article-title><source>GigaScience</source><volume>10</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1093/gigascience/giab008</pub-id><pub-id pub-id-type="pmid">33590861</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Forer</surname><given-names>L</given-names></name><name><surname>SchÃ¶nherr</surname><given-names>S</given-names></name><name><surname>Sidore</surname><given-names>C</given-names></name><name><surname>Locke</surname><given-names>AE</given-names></name><name><surname>Kwong</surname><given-names>A</given-names></name><name><surname>Vrieze</surname><given-names>SI</given-names></name><name><surname>Chew</surname><given-names>EY</given-names></name><name><surname>Levy</surname><given-names>S</given-names></name><name><surname>McGue</surname><given-names>M</given-names></name><name><surname>Schlessinger</surname><given-names>D</given-names></name><name><surname>Stambolian</surname><given-names>D</given-names></name><name><surname>Loh</surname><given-names>PR</given-names></name><name><surname>Iacono</surname><given-names>WG</given-names></name><name><surname>Swaroop</surname><given-names>A</given-names></name><name><surname>Scott</surname><given-names>LJ</given-names></name><name><surname>Cucca</surname><given-names>F</given-names></name><name><surname>Kronenberg</surname><given-names>F</given-names></name><name><surname>Boehnke</surname><given-names>M</given-names></name><name><surname>Abecasis</surname><given-names>GR</given-names></name><name><surname>Fuchsberger</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Next-Generation genotype imputation service and methods</article-title><source>Nature Genetics</source><volume>48</volume><fpage>1284</fpage><lpage>1287</lpage><pub-id pub-id-type="doi">10.1038/ng.3656</pub-id><pub-id pub-id-type="pmid">27571263</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Abecasis</surname><given-names>GR</given-names></name><name><surname>Browning</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Genotype imputation from large reference panels</article-title><source>Annual Review of Genomics and Human Genetics</source><volume>19</volume><fpage>73</fpage><lpage>96</lpage><pub-id pub-id-type="doi">10.1146/annurev-genom-083117-021602</pub-id><pub-id pub-id-type="pmid">29799802</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dias</surname><given-names>R</given-names></name><name><surname>Torkamani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Artificial intelligence in clinical and genomic diagnostics</article-title><source>Genome Medicine</source><volume>11</volume><elocation-id>70</elocation-id><pub-id pub-id-type="doi">10.1186/s13073-019-0689-8</pub-id><pub-id pub-id-type="pmid">31744524</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dias</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Imputation_accuracy_calculator</data-title><version designator="swh:1:rev:e01229e3f245e8bb95b29d4f4f1e547fcff70ae4">swh:1:rev:e01229e3f245e8bb95b29d4f4f1e547fcff70ae4</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:27393c4be42545b487fe4f32cf34c200cd1e9d99;origin=https://github.com/TorkamaniLab/imputation_accuracy_calculator;visit=swh:1:snp:c520059cc24989cbe62e6e82c890d0aa1e14fcf0;anchor=swh:1:rev:e01229e3f245e8bb95b29d4f4f1e547fcff70ae4">https://archive.softwareheritage.org/swh:1:dir:27393c4be42545b487fe4f32cf34c200cd1e9d99;origin=https://github.com/TorkamaniLab/imputation_accuracy_calculator;visit=swh:1:snp:c520059cc24989cbe62e6e82c890d0aa1e14fcf0;anchor=swh:1:rev:e01229e3f245e8bb95b29d4f4f1e547fcff70ae4</ext-link></element-citation></ref><ref id="bib17"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dias</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Imputator_inference</data-title><version designator="swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c">swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bcdf526c7102b44428af0a8edc41c95c449c7713;origin=https://github.com/TorkamaniLab/imputator_inference;visit=swh:1:snp:1f1e9662e49b6476f0475c52ca54929ae422184d;anchor=swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c">https://archive.softwareheritage.org/swh:1:dir:bcdf526c7102b44428af0a8edc41c95c449c7713;origin=https://github.com/TorkamaniLab/imputator_inference;visit=swh:1:snp:1f1e9662e49b6476f0475c52ca54929ae422184d;anchor=swh:1:rev:2fbd203acf8aaf320a520c6374d6f4d57f068a7c</ext-link></element-citation></ref><ref id="bib18"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Dias</surname><given-names>R</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Rogers</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Imputation_Autoencoder</data-title><version designator="swh:1:rev:35d2e292e786ebc41e71f27809dad56b1e1933c4; path=/autoencoder_tuning_pipeline/">swh:1:rev:35d2e292e786ebc41e71f27809dad56b1e1933c4; path=/autoencoder_tuning_pipeline/</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:20c922e4cce40c9c9f017de70cbed0dafea410ec;origin=https://github.com/TorkamaniLab/Imputation_Autoencoder;visit=swh:1:snp:7604f13a3ae5a1471b1c6620b00dd37d16a6b33f;anchor=swh:1:rev:35d2e292e786ebc41e71f27809dad56b1e1933c4;path=/autoencoder_tuning_pipeline/">https://archive.softwareheritage.org/swh:1:dir:20c922e4cce40c9c9f017de70cbed0dafea410ec;origin=https://github.com/TorkamaniLab/Imputation_Autoencoder;visit=swh:1:snp:7604f13a3ae5a1471b1c6620b00dd37d16a6b33f;anchor=swh:1:rev:35d2e292e786ebc41e71f27809dad56b1e1933c4;path=/autoencoder_tuning_pipeline/</ext-link></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dimitromanolakis</surname><given-names>A</given-names></name><name><surname>Xu</surname><given-names>J</given-names></name><name><surname>Krol</surname><given-names>A</given-names></name><name><surname>Briollais</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sim1000G: a user-friendly genetic variant simulator in R for unrelated individuals and family-based designs</article-title><source>BMC Bioinformatics</source><volume>20</volume><elocation-id>26</elocation-id><pub-id pub-id-type="doi">10.1186/s12859-019-2611-1</pub-id><pub-id pub-id-type="pmid">30646839</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Erikson</surname><given-names>GA</given-names></name><name><surname>Bodian</surname><given-names>DL</given-names></name><name><surname>Rueda</surname><given-names>M</given-names></name><name><surname>Molparia</surname><given-names>B</given-names></name><name><surname>Scott</surname><given-names>ER</given-names></name><name><surname>Scott-Van Zeeland</surname><given-names>AA</given-names></name><name><surname>Topol</surname><given-names>SE</given-names></name><name><surname>Wineinger</surname><given-names>NE</given-names></name><name><surname>Niederhuber</surname><given-names>JE</given-names></name><name><surname>Topol</surname><given-names>EJ</given-names></name><name><surname>Torkamani</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Whole-Genome sequencing of a healthy aging cohort</article-title><source>Cell</source><volume>165</volume><fpage>1002</fpage><lpage>1011</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2016.03.022</pub-id><pub-id pub-id-type="pmid">27114037</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Frazer</surname><given-names>KA</given-names></name><name><surname>Ballinger</surname><given-names>DG</given-names></name><name><surname>Cox</surname><given-names>DR</given-names></name><name><surname>Hinds</surname><given-names>DA</given-names></name><name><surname>Stuve</surname><given-names>LL</given-names></name><name><surname>Gibbs</surname><given-names>RA</given-names></name><name><surname>Belmont</surname><given-names>JW</given-names></name><name><surname>Boudreau</surname><given-names>A</given-names></name><name><surname>Hardenbol</surname><given-names>P</given-names></name><name><surname>Leal</surname><given-names>SM</given-names></name><name><surname>Pasternak</surname><given-names>S</given-names></name><name><surname>Wheeler</surname><given-names>DA</given-names></name><name><surname>Willis</surname><given-names>TD</given-names></name><name><surname>Yu</surname><given-names>F</given-names></name><name><surname>Yang</surname><given-names>H</given-names></name><name><surname>Zeng</surname><given-names>C</given-names></name><name><surname>Gao</surname><given-names>Y</given-names></name><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>Hu</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>C</given-names></name><name><surname>Lin</surname><given-names>W</given-names></name><name><surname>Liu</surname><given-names>S</given-names></name><name><surname>Pan</surname><given-names>H</given-names></name><name><surname>Tang</surname><given-names>X</given-names></name><name><surname>Wang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>W</given-names></name><name><surname>Yu</surname><given-names>J</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Zhang</surname><given-names>Q</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Zhou</surname><given-names>J</given-names></name><name><surname>Gabriel</surname><given-names>SB</given-names></name><name><surname>Barry</surname><given-names>R</given-names></name><name><surname>Blumenstiel</surname><given-names>B</given-names></name><name><surname>Camargo</surname><given-names>A</given-names></name><name><surname>Defelice</surname><given-names>M</given-names></name><name><surname>Faggart</surname><given-names>M</given-names></name><name><surname>Goyette</surname><given-names>M</given-names></name><name><surname>Gupta</surname><given-names>S</given-names></name><name><surname>Moore</surname><given-names>J</given-names></name><name><surname>Nguyen</surname><given-names>H</given-names></name><name><surname>Onofrio</surname><given-names>RC</given-names></name><name><surname>Parkin</surname><given-names>M</given-names></name><name><surname>Roy</surname><given-names>J</given-names></name><name><surname>Stahl</surname><given-names>E</given-names></name><name><surname>Winchester</surname><given-names>E</given-names></name><name><surname>Ziaugra</surname><given-names>L</given-names></name><name><surname>Altshuler</surname><given-names>D</given-names></name><name><surname>Shen</surname><given-names>Y</given-names></name><name><surname>Yao</surname><given-names>Z</given-names></name><name><surname>Huang</surname><given-names>W</given-names></name><name><surname>Chu</surname><given-names>X</given-names></name><name><surname>He</surname><given-names>Y</given-names></name><name><surname>Jin</surname><given-names>L</given-names></name><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Shen</surname><given-names>Y</given-names></name><name><surname>Sun</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>Y</given-names></name><name><surname>Xiong</surname><given-names>X</given-names></name><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Waye</surname><given-names>MMY</given-names></name><name><surname>Tsui</surname><given-names>SKW</given-names></name><name><surname>Xue</surname><given-names>H</given-names></name><name><surname>Wong</surname><given-names>JTF</given-names></name><name><surname>Galver</surname><given-names>LM</given-names></name><name><surname>Fan</surname><given-names>JB</given-names></name><name><surname>Gunderson</surname><given-names>K</given-names></name><name><surname>Murray</surname><given-names>SS</given-names></name><name><surname>Oliphant</surname><given-names>AR</given-names></name><name><surname>Chee</surname><given-names>MS</given-names></name><name><surname>Montpetit</surname><given-names>A</given-names></name><name><surname>Chagnon</surname><given-names>F</given-names></name><name><surname>Ferretti</surname><given-names>V</given-names></name><name><surname>Leboeuf</surname><given-names>M</given-names></name><name><surname>Olivier</surname><given-names>JF</given-names></name><name><surname>Phillips</surname><given-names>MS</given-names></name><name><surname>Roumy</surname><given-names>S</given-names></name><name><surname>SallÃ©e</surname><given-names>C</given-names></name><name><surname>Verner</surname><given-names>A</given-names></name><name><surname>Hudson</surname><given-names>TJ</given-names></name><name><surname>Kwok</surname><given-names>PY</given-names></name><name><surname>Cai</surname><given-names>D</given-names></name><name><surname>Koboldt</surname><given-names>DC</given-names></name><name><surname>Miller</surname><given-names>RD</given-names></name><name><surname>Pawlikowska</surname><given-names>L</given-names></name><name><surname>Taillon-Miller</surname><given-names>P</given-names></name><name><surname>Xiao</surname><given-names>M</given-names></name><name><surname>Tsui</surname><given-names>LC</given-names></name><name><surname>Mak</surname><given-names>W</given-names></name><name><surname>Song</surname><given-names>YQ</given-names></name><name><surname>Tam</surname><given-names>PKH</given-names></name><name><surname>Nakamura</surname><given-names>Y</given-names></name><name><surname>Kawaguchi</surname><given-names>T</given-names></name><name><surname>Kitamoto</surname><given-names>T</given-names></name><name><surname>Morizono</surname><given-names>T</given-names></name><name><surname>Nagashima</surname><given-names>A</given-names></name><name><surname>Ohnishi</surname><given-names>Y</given-names></name><name><surname>Sekine</surname><given-names>A</given-names></name><name><surname>Tanaka</surname><given-names>T</given-names></name><name><surname>Tsunoda</surname><given-names>T</given-names></name><name><surname>Deloukas</surname><given-names>P</given-names></name><name><surname>Bird</surname><given-names>CP</given-names></name><name><surname>Delgado</surname><given-names>M</given-names></name><name><surname>Dermitzakis</surname><given-names>ET</given-names></name><name><surname>Gwilliam</surname><given-names>R</given-names></name><name><surname>Hunt</surname><given-names>S</given-names></name><name><surname>Morrison</surname><given-names>J</given-names></name><name><surname>Powell</surname><given-names>D</given-names></name><name><surname>Stranger</surname><given-names>BE</given-names></name><name><surname>Whittaker</surname><given-names>P</given-names></name><name><surname>Bentley</surname><given-names>DR</given-names></name><name><surname>Daly</surname><given-names>MJ</given-names></name><name><surname>de Bakker</surname><given-names>PIW</given-names></name><name><surname>Barrett</surname><given-names>J</given-names></name><name><surname>Chretien</surname><given-names>YR</given-names></name><name><surname>Maller</surname><given-names>J</given-names></name><name><surname>McCarroll</surname><given-names>S</given-names></name><name><surname>Patterson</surname><given-names>N</given-names></name><name><surname>Peâer</surname><given-names>I</given-names></name><name><surname>Price</surname><given-names>A</given-names></name><name><surname>Purcell</surname><given-names>S</given-names></name><name><surname>Richter</surname><given-names>DJ</given-names></name><name><surname>Sabeti</surname><given-names>P</given-names></name><name><surname>Saxena</surname><given-names>R</given-names></name><name><surname>Schaffner</surname><given-names>SF</given-names></name><name><surname>Sham</surname><given-names>PC</given-names></name><name><surname>Varilly</surname><given-names>P</given-names></name><name><surname>Altshuler</surname><given-names>D</given-names></name><name><surname>Stein</surname><given-names>LD</given-names></name><name><surname>Krishnan</surname><given-names>L</given-names></name><name><surname>Smith</surname><given-names>AV</given-names></name><name><surname>Tello-Ruiz</surname><given-names>MK</given-names></name><name><surname>Thorisson</surname><given-names>GA</given-names></name><name><surname>Chakravarti</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>PE</given-names></name><name><surname>Cutler</surname><given-names>DJ</given-names></name><name><surname>Kashuk</surname><given-names>CS</given-names></name><name><surname>Lin</surname><given-names>S</given-names></name><name><surname>Abecasis</surname><given-names>GR</given-names></name><name><surname>Guan</surname><given-names>W</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Munro</surname><given-names>HM</given-names></name><name><surname>Qin</surname><given-names>ZS</given-names></name><name><surname>Thomas</surname><given-names>DJ</given-names></name><name><surname>McVean</surname><given-names>G</given-names></name><name><surname>Auton</surname><given-names>A</given-names></name><name><surname>Bottolo</surname><given-names>L</given-names></name><name><surname>Cardin</surname><given-names>N</given-names></name><name><surname>Eyheramendy</surname><given-names>S</given-names></name><name><surname>Freeman</surname><given-names>C</given-names></name><name><surname>Marchini</surname><given-names>J</given-names></name><name><surname>Myers</surname><given-names>S</given-names></name><name><surname>Spencer</surname><given-names>C</given-names></name><name><surname>Stephens</surname><given-names>M</given-names></name><name><surname>Donnelly</surname><given-names>P</given-names></name><name><surname>Cardon</surname><given-names>LR</given-names></name><name><surname>Clarke</surname><given-names>G</given-names></name><name><surname>Evans</surname><given-names>DM</given-names></name><name><surname>Morris</surname><given-names>AP</given-names></name><name><surname>Weir</surname><given-names>BS</given-names></name><name><surname>Tsunoda</surname><given-names>T</given-names></name><name><surname>Mullikin</surname><given-names>JC</given-names></name><name><surname>Sherry</surname><given-names>ST</given-names></name><name><surname>Feolo</surname><given-names>M</given-names></name><name><surname>Skol</surname><given-names>A</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Zeng</surname><given-names>C</given-names></name><name><surname>Zhao</surname><given-names>H</given-names></name><name><surname>Matsuda</surname><given-names>I</given-names></name><name><surname>Fukushima</surname><given-names>Y</given-names></name><name><surname>Macer</surname><given-names>DR</given-names></name><name><surname>Suda</surname><given-names>E</given-names></name><name><surname>Rotimi</surname><given-names>CN</given-names></name><name><surname>Adebamowo</surname><given-names>CA</given-names></name><name><surname>Ajayi</surname><given-names>I</given-names></name><name><surname>Aniagwu</surname><given-names>T</given-names></name><name><surname>Marshall</surname><given-names>PA</given-names></name><name><surname>Nkwodimmah</surname><given-names>C</given-names></name><name><surname>Royal</surname><given-names>CDM</given-names></name><name><surname>Leppert</surname><given-names>MF</given-names></name><name><surname>Dixon</surname><given-names>M</given-names></name><name><surname>Peiffer</surname><given-names>A</given-names></name><name><surname>Qiu</surname><given-names>R</given-names></name><name><surname>Kent</surname><given-names>A</given-names></name><name><surname>Kato</surname><given-names>K</given-names></name><name><surname>Niikawa</surname><given-names>N</given-names></name><name><surname>Adewole</surname><given-names>IF</given-names></name><name><surname>Knoppers</surname><given-names>BM</given-names></name><name><surname>Foster</surname><given-names>MW</given-names></name><name><surname>Clayton</surname><given-names>EW</given-names></name><name><surname>Watkin</surname><given-names>J</given-names></name><name><surname>Gibbs</surname><given-names>RA</given-names></name><name><surname>Belmont</surname><given-names>JW</given-names></name><name><surname>Muzny</surname><given-names>D</given-names></name><name><surname>Nazareth</surname><given-names>L</given-names></name><name><surname>Sodergren</surname><given-names>E</given-names></name><name><surname>Weinstock</surname><given-names>GM</given-names></name><name><surname>Wheeler</surname><given-names>DA</given-names></name><name><surname>Yakub</surname><given-names>I</given-names></name><name><surname>Gabriel</surname><given-names>SB</given-names></name><name><surname>Onofrio</surname><given-names>RC</given-names></name><name><surname>Richter</surname><given-names>DJ</given-names></name><name><surname>Ziaugra</surname><given-names>L</given-names></name><name><surname>Birren</surname><given-names>BW</given-names></name><name><surname>Daly</surname><given-names>MJ</given-names></name><name><surname>Altshuler</surname><given-names>D</given-names></name><name><surname>Wilson</surname><given-names>RK</given-names></name><name><surname>Fulton</surname><given-names>LL</given-names></name><name><surname>Rogers</surname><given-names>J</given-names></name><name><surname>Burton</surname><given-names>J</given-names></name><name><surname>Carter</surname><given-names>NP</given-names></name><name><surname>Clee</surname><given-names>CM</given-names></name><name><surname>Griffiths</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>MC</given-names></name><name><surname>McLay</surname><given-names>K</given-names></name><name><surname>Plumb</surname><given-names>RW</given-names></name><name><surname>Ross</surname><given-names>MT</given-names></name><name><surname>Sims</surname><given-names>SK</given-names></name><name><surname>Willey</surname><given-names>DL</given-names></name><name><surname>Chen</surname><given-names>Z</given-names></name><name><surname>Han</surname><given-names>H</given-names></name><name><surname>Kang</surname><given-names>L</given-names></name><name><surname>Godbout</surname><given-names>M</given-names></name><name><surname>Wallenburg</surname><given-names>JC</given-names></name><name><surname>LâArchevÃªque</surname><given-names>P</given-names></name><name><surname>Bellemare</surname><given-names>G</given-names></name><name><surname>Saeki</surname><given-names>K</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>An</surname><given-names>D</given-names></name><name><surname>Fu</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Wang</surname><given-names>Z</given-names></name><name><surname>Wang</surname><given-names>R</given-names></name><name><surname>Holden</surname><given-names>AL</given-names></name><name><surname>Brooks</surname><given-names>LD</given-names></name><name><surname>McEwen</surname><given-names>JE</given-names></name><name><surname>Guyer</surname><given-names>MS</given-names></name><name><surname>Wang</surname><given-names>VO</given-names></name><name><surname>Peterson</surname><given-names>JL</given-names></name><name><surname>Shi</surname><given-names>M</given-names></name><name><surname>Spiegel</surname><given-names>J</given-names></name><name><surname>Sung</surname><given-names>LM</given-names></name><name><surname>Zacharia</surname><given-names>LF</given-names></name><name><surname>Collins</surname><given-names>FS</given-names></name><name><surname>Kennedy</surname><given-names>K</given-names></name><name><surname>Jamieson</surname><given-names>R</given-names></name><name><surname>Stewart</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A second generation human haplotype map of over 3.1 million SNPs</article-title><source>Nature</source><volume>449</volume><fpage>851</fpage><lpage>861</lpage><pub-id pub-id-type="doi">10.1038/nature06258</pub-id><pub-id pub-id-type="pmid">17943122</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ghosh</surname><given-names>SK</given-names></name><name><surname>Biswas</surname><given-names>B</given-names></name><name><surname>Ghosh</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Restoration of mammograms by using deep convolutional denoising auto-encoders</article-title><source>Advances in Intelligent Systems and Computing</source><volume>990</volume><fpage>435</fpage><lpage>447</lpage><pub-id pub-id-type="doi">10.1007/978-981-13-8676-3_38</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Islam</surname><given-names>T</given-names></name><name><surname>Kim</surname><given-names>CH</given-names></name><name><surname>Iwata</surname><given-names>H</given-names></name><name><surname>Shimono</surname><given-names>H</given-names></name><name><surname>Kimura</surname><given-names>A</given-names></name><name><surname>Zaw</surname><given-names>H</given-names></name><name><surname>Raghavan</surname><given-names>C</given-names></name><name><surname>Leung</surname><given-names>H</given-names></name><name><surname>Singh</surname><given-names>RK</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A Deep Learning Method to Impute Missing Values and Compress Genome-ide Polymorphism Data in Rice In</article-title><conf-name>Proceedings of the 14th International Joint Conference on Biomedical Engineering Systems and Technologies</conf-name><fpage>11</fpage><lpage>13</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jumper</surname><given-names>J</given-names></name><name><surname>Evans</surname><given-names>R</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Figurnov</surname><given-names>M</given-names></name><name><surname>Ronneberger</surname><given-names>O</given-names></name><name><surname>Tunyasuvunakool</surname><given-names>K</given-names></name><name><surname>Bates</surname><given-names>R</given-names></name><name><surname>Å½Ã­dek</surname><given-names>A</given-names></name><name><surname>Potapenko</surname><given-names>A</given-names></name><name><surname>Bridgland</surname><given-names>A</given-names></name><name><surname>Meyer</surname><given-names>C</given-names></name><name><surname>Kohl</surname><given-names>SAA</given-names></name><name><surname>Ballard</surname><given-names>AJ</given-names></name><name><surname>Cowie</surname><given-names>A</given-names></name><name><surname>Romera-Paredes</surname><given-names>B</given-names></name><name><surname>Nikolov</surname><given-names>S</given-names></name><name><surname>Jain</surname><given-names>R</given-names></name><name><surname>Adler</surname><given-names>J</given-names></name><name><surname>Back</surname><given-names>T</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Reiman</surname><given-names>D</given-names></name><name><surname>Clancy</surname><given-names>E</given-names></name><name><surname>Zielinski</surname><given-names>M</given-names></name><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Pacholska</surname><given-names>M</given-names></name><name><surname>Berghammer</surname><given-names>T</given-names></name><name><surname>Bodenstein</surname><given-names>S</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Vinyals</surname><given-names>O</given-names></name><name><surname>Senior</surname><given-names>AW</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Kohli</surname><given-names>P</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Highly accurate protein structure prediction with alphafold</article-title><source>Nature</source><volume>596</volume><fpage>583</fpage><lpage>589</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03819-2</pub-id><pub-id pub-id-type="pmid">34265844</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kojima</surname><given-names>K</given-names></name><name><surname>Tadaka</surname><given-names>S</given-names></name><name><surname>Katsuoka</surname><given-names>F</given-names></name><name><surname>Tamiya</surname><given-names>G</given-names></name><name><surname>Yamamoto</surname><given-names>M</given-names></name><name><surname>Kinoshita</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A genotype imputation method for de-identified haplotype reference information by using recurrent neural network</article-title><source>PLOS Computational Biology</source><volume>16</volume><elocation-id>e1008207</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1008207</pub-id><pub-id pub-id-type="pmid">33001993</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kowalski</surname><given-names>MH</given-names></name><name><surname>Qian</surname><given-names>H</given-names></name><name><surname>Hou</surname><given-names>Z</given-names></name><name><surname>Rosen</surname><given-names>JD</given-names></name><name><surname>Tapia</surname><given-names>AL</given-names></name><name><surname>Shan</surname><given-names>Y</given-names></name><name><surname>Jain</surname><given-names>D</given-names></name><name><surname>Argos</surname><given-names>M</given-names></name><name><surname>Arnett</surname><given-names>DK</given-names></name><name><surname>Avery</surname><given-names>C</given-names></name><name><surname>Barnes</surname><given-names>KC</given-names></name><name><surname>Becker</surname><given-names>LC</given-names></name><name><surname>Bien</surname><given-names>SA</given-names></name><name><surname>Bis</surname><given-names>JC</given-names></name><name><surname>Blangero</surname><given-names>J</given-names></name><name><surname>Boerwinkle</surname><given-names>E</given-names></name><name><surname>Bowden</surname><given-names>DW</given-names></name><name><surname>Buyske</surname><given-names>S</given-names></name><name><surname>Cai</surname><given-names>J</given-names></name><name><surname>Cho</surname><given-names>MH</given-names></name><name><surname>Choi</surname><given-names>SH</given-names></name><name><surname>Choquet</surname><given-names>H</given-names></name><name><surname>Cupples</surname><given-names>LA</given-names></name><name><surname>Cushman</surname><given-names>M</given-names></name><name><surname>Daya</surname><given-names>M</given-names></name><name><surname>de Vries</surname><given-names>PS</given-names></name><name><surname>Ellinor</surname><given-names>PT</given-names></name><name><surname>Faraday</surname><given-names>N</given-names></name><name><surname>Fornage</surname><given-names>M</given-names></name><name><surname>Gabriel</surname><given-names>S</given-names></name><name><surname>Ganesh</surname><given-names>SK</given-names></name><name><surname>Graff</surname><given-names>M</given-names></name><name><surname>Gupta</surname><given-names>N</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>Heckbert</surname><given-names>SR</given-names></name><name><surname>Hidalgo</surname><given-names>B</given-names></name><name><surname>Hodonsky</surname><given-names>CJ</given-names></name><name><surname>Irvin</surname><given-names>MR</given-names></name><name><surname>Johnson</surname><given-names>AD</given-names></name><name><surname>Jorgenson</surname><given-names>E</given-names></name><name><surname>Kaplan</surname><given-names>R</given-names></name><name><surname>Kardia</surname><given-names>SLR</given-names></name><name><surname>Kelly</surname><given-names>TN</given-names></name><name><surname>Kooperberg</surname><given-names>C</given-names></name><name><surname>Lasky-Su</surname><given-names>JA</given-names></name><name><surname>Loos</surname><given-names>RJF</given-names></name><name><surname>Lubitz</surname><given-names>SA</given-names></name><name><surname>Mathias</surname><given-names>RA</given-names></name><name><surname>McHugh</surname><given-names>CP</given-names></name><name><surname>Montgomery</surname><given-names>C</given-names></name><name><surname>Moon</surname><given-names>JY</given-names></name><name><surname>Morrison</surname><given-names>AC</given-names></name><name><surname>Palmer</surname><given-names>ND</given-names></name><name><surname>Pankratz</surname><given-names>N</given-names></name><name><surname>Papanicolaou</surname><given-names>GJ</given-names></name><name><surname>Peralta</surname><given-names>JM</given-names></name><name><surname>Peyser</surname><given-names>PA</given-names></name><name><surname>Rich</surname><given-names>SS</given-names></name><name><surname>Rotter</surname><given-names>JI</given-names></name><name><surname>Silverman</surname><given-names>EK</given-names></name><name><surname>Smith</surname><given-names>JA</given-names></name><name><surname>Smith</surname><given-names>NL</given-names></name><name><surname>Taylor</surname><given-names>KD</given-names></name><name><surname>Thornton</surname><given-names>TA</given-names></name><name><surname>Tiwari</surname><given-names>HK</given-names></name><name><surname>Tracy</surname><given-names>RP</given-names></name><name><surname>Wang</surname><given-names>T</given-names></name><name><surname>Weiss</surname><given-names>ST</given-names></name><name><surname>Weng</surname><given-names>LC</given-names></name><name><surname>Wiggins</surname><given-names>KL</given-names></name><name><surname>Wilson</surname><given-names>JG</given-names></name><name><surname>Yanek</surname><given-names>LR</given-names></name><name><surname>ZÃ¶llner</surname><given-names>S</given-names></name><name><surname>North</surname><given-names>KE</given-names></name><name><surname>Auer</surname><given-names>PL</given-names></name><name><surname>Raffield</surname><given-names>LM</given-names></name><name><surname>Reiner</surname><given-names>AP</given-names></name><name><surname>Li</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Use of &gt; 100,000 NHLBI trans-omics for precision medicine (topmed) Consortium whole genome sequences improves imputation quality and detection of rare variant associations in admixed African and Hispanic/Latino populations</article-title><source>PLOS Genetics</source><volume>15</volume><elocation-id>e1008500</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pgen.1008500</pub-id><pub-id pub-id-type="pmid">31869403</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Y</given-names></name><name><surname>Willer</surname><given-names>C</given-names></name><name><surname>Sanna</surname><given-names>S</given-names></name><name><surname>Abecasis</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Genotype imputation</article-title><source>Annual Review of Genomics and Human Genetics</source><volume>10</volume><fpage>387</fpage><lpage>406</lpage><pub-id pub-id-type="doi">10.1146/annurev.genom.9.081307.164242</pub-id><pub-id pub-id-type="pmid">19715440</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>TY</given-names></name><name><surname>Goyal</surname><given-names>P</given-names></name><name><surname>Girshick</surname><given-names>R</given-names></name><name><surname>He</surname><given-names>K</given-names></name><name><surname>Dollar</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Focal Loss for Dense Object Detection</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1708.02002">https://arxiv.org/abs/1708.02002</ext-link></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>Y</given-names></name><name><surname>Gu</surname><given-names>J</given-names></name><name><surname>Goyal</surname><given-names>N</given-names></name><name><surname>Li</surname><given-names>X</given-names></name><name><surname>Edunov</surname><given-names>S</given-names></name><name><surname>Ghazvininejad</surname><given-names>M</given-names></name><name><surname>Lewis</surname><given-names>M</given-names></name><name><surname>Zettlemoyer</surname><given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Multilingual denoising pre-training for neural machine translation</article-title><source>Transactions of the Association for Computational Linguistics</source><volume>8</volume><fpage>726</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1162/tacl_a_00343</pub-id><pub-id pub-id-type="pmid">32695773</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mao</surname><given-names>XJ</given-names></name><name><surname>Shen</surname><given-names>C</given-names></name><name><surname>Yang</surname><given-names>YB</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Advances in Neural Information Processing Systems 29</source><publisher-loc>New york, USA</publisher-loc><publisher-name>Curran Associates, Inc</publisher-name></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Marchini</surname><given-names>J</given-names></name><name><surname>Howie</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Genotype imputation for genome-wide association studies</article-title><source>Nature Reviews. Genetics</source><volume>11</volume><fpage>499</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.1038/nrg2796</pub-id><pub-id pub-id-type="pmid">20517342</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McCarthy</surname><given-names>S</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Kretzschmar</surname><given-names>W</given-names></name><name><surname>Delaneau</surname><given-names>O</given-names></name><name><surname>Wood</surname><given-names>AR</given-names></name><name><surname>Teumer</surname><given-names>A</given-names></name><name><surname>Kang</surname><given-names>HM</given-names></name><name><surname>Fuchsberger</surname><given-names>C</given-names></name><name><surname>Danecek</surname><given-names>P</given-names></name><name><surname>Sharp</surname><given-names>K</given-names></name><name><surname>Luo</surname><given-names>Y</given-names></name><name><surname>Sidore</surname><given-names>C</given-names></name><name><surname>Kwong</surname><given-names>A</given-names></name><name><surname>Timpson</surname><given-names>N</given-names></name><name><surname>Koskinen</surname><given-names>S</given-names></name><name><surname>Vrieze</surname><given-names>S</given-names></name><name><surname>Scott</surname><given-names>LJ</given-names></name><name><surname>Zhang</surname><given-names>H</given-names></name><name><surname>Mahajan</surname><given-names>A</given-names></name><name><surname>Veldink</surname><given-names>J</given-names></name><name><surname>Peters</surname><given-names>U</given-names></name><name><surname>Pato</surname><given-names>C</given-names></name><name><surname>van Duijn</surname><given-names>CM</given-names></name><name><surname>Gillies</surname><given-names>CE</given-names></name><name><surname>Gandin</surname><given-names>I</given-names></name><name><surname>Mezzavilla</surname><given-names>M</given-names></name><name><surname>Gilly</surname><given-names>A</given-names></name><name><surname>Cocca</surname><given-names>M</given-names></name><name><surname>Traglia</surname><given-names>M</given-names></name><name><surname>Angius</surname><given-names>A</given-names></name><name><surname>Barrett</surname><given-names>JC</given-names></name><name><surname>Boomsma</surname><given-names>D</given-names></name><name><surname>Branham</surname><given-names>K</given-names></name><name><surname>Breen</surname><given-names>G</given-names></name><name><surname>Brummett</surname><given-names>CM</given-names></name><name><surname>Busonero</surname><given-names>F</given-names></name><name><surname>Campbell</surname><given-names>H</given-names></name><name><surname>Chan</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>S</given-names></name><name><surname>Chew</surname><given-names>E</given-names></name><name><surname>Collins</surname><given-names>FS</given-names></name><name><surname>Corbin</surname><given-names>LJ</given-names></name><name><surname>Smith</surname><given-names>GD</given-names></name><name><surname>Dedoussis</surname><given-names>G</given-names></name><name><surname>Dorr</surname><given-names>M</given-names></name><name><surname>Farmaki</surname><given-names>A-E</given-names></name><name><surname>Ferrucci</surname><given-names>L</given-names></name><name><surname>Forer</surname><given-names>L</given-names></name><name><surname>Fraser</surname><given-names>RM</given-names></name><name><surname>Gabriel</surname><given-names>S</given-names></name><name><surname>Levy</surname><given-names>S</given-names></name><name><surname>Groop</surname><given-names>L</given-names></name><name><surname>Harrison</surname><given-names>T</given-names></name><name><surname>Hattersley</surname><given-names>A</given-names></name><name><surname>Holmen</surname><given-names>OL</given-names></name><name><surname>Hveem</surname><given-names>K</given-names></name><name><surname>Kretzler</surname><given-names>M</given-names></name><name><surname>Lee</surname><given-names>JC</given-names></name><name><surname>McGue</surname><given-names>M</given-names></name><name><surname>Meitinger</surname><given-names>T</given-names></name><name><surname>Melzer</surname><given-names>D</given-names></name><name><surname>Min</surname><given-names>JL</given-names></name><name><surname>Mohlke</surname><given-names>KL</given-names></name><name><surname>Vincent</surname><given-names>JB</given-names></name><name><surname>Nauck</surname><given-names>M</given-names></name><name><surname>Nickerson</surname><given-names>D</given-names></name><name><surname>Palotie</surname><given-names>A</given-names></name><name><surname>Pato</surname><given-names>M</given-names></name><name><surname>Pirastu</surname><given-names>N</given-names></name><name><surname>McInnis</surname><given-names>M</given-names></name><name><surname>Richards</surname><given-names>JB</given-names></name><name><surname>Sala</surname><given-names>C</given-names></name><name><surname>Salomaa</surname><given-names>V</given-names></name><name><surname>Schlessinger</surname><given-names>D</given-names></name><name><surname>Schoenherr</surname><given-names>S</given-names></name><name><surname>Slagboom</surname><given-names>PE</given-names></name><name><surname>Small</surname><given-names>K</given-names></name><name><surname>Spector</surname><given-names>T</given-names></name><name><surname>Stambolian</surname><given-names>D</given-names></name><name><surname>Tuke</surname><given-names>M</given-names></name><name><surname>Tuomilehto</surname><given-names>J</given-names></name><name><surname>Van den Berg</surname><given-names>LH</given-names></name><name><surname>Van Rheenen</surname><given-names>W</given-names></name><name><surname>Volker</surname><given-names>U</given-names></name><name><surname>Wijmenga</surname><given-names>C</given-names></name><name><surname>Toniolo</surname><given-names>D</given-names></name><name><surname>Zeggini</surname><given-names>E</given-names></name><name><surname>Gasparini</surname><given-names>P</given-names></name><name><surname>Sampson</surname><given-names>MG</given-names></name><name><surname>Wilson</surname><given-names>JF</given-names></name><name><surname>Frayling</surname><given-names>T</given-names></name><name><surname>de Bakker</surname><given-names>PIW</given-names></name><name><surname>Swertz</surname><given-names>MA</given-names></name><name><surname>McCarroll</surname><given-names>S</given-names></name><name><surname>Kooperberg</surname><given-names>C</given-names></name><name><surname>Dekker</surname><given-names>A</given-names></name><name><surname>Altshuler</surname><given-names>D</given-names></name><name><surname>Willer</surname><given-names>C</given-names></name><name><surname>Iacono</surname><given-names>W</given-names></name><name><surname>Ripatti</surname><given-names>S</given-names></name><name><surname>Soranzo</surname><given-names>N</given-names></name><name><surname>Walter</surname><given-names>K</given-names></name><name><surname>Swaroop</surname><given-names>A</given-names></name><name><surname>Cucca</surname><given-names>F</given-names></name><name><surname>Anderson</surname><given-names>CA</given-names></name><name><surname>Myers</surname><given-names>RM</given-names></name><name><surname>Boehnke</surname><given-names>M</given-names></name><name><surname>McCarthy</surname><given-names>MI</given-names></name><name><surname>Durbin</surname><given-names>R</given-names></name><collab>Haplotype Reference Consortium</collab></person-group><year iso-8601-date="2016">2016</year><article-title>A reference panel of 64,976 haplotypes for genotype imputation</article-title><source>Nature Genetics</source><volume>48</volume><fpage>1279</fpage><lpage>1283</lpage><pub-id pub-id-type="doi">10.1038/ng.3643</pub-id><pub-id pub-id-type="pmid">27548312</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mou</surname><given-names>L</given-names></name><name><surname>Norby</surname><given-names>FL</given-names></name><name><surname>Chen</surname><given-names>LY</given-names></name><name><surname>OâNeal</surname><given-names>WT</given-names></name><name><surname>Lewis</surname><given-names>TT</given-names></name><name><surname>Loehr</surname><given-names>LR</given-names></name><name><surname>Soliman</surname><given-names>EZ</given-names></name><name><surname>Alonso</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Lifetime risk of atrial fibrillation by race and socioeconomic status: ARIC study (atherosclerosis risk in communities)</article-title><source>Circulation. Arrhythmia and Electrophysiology</source><volume>11</volume><elocation-id>e006350</elocation-id><pub-id pub-id-type="doi">10.1161/CIRCEP.118.006350</pub-id><pub-id pub-id-type="pmid">30002066</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naito</surname><given-names>T</given-names></name><name><surname>Suzuki</surname><given-names>K</given-names></name><name><surname>Hirata</surname><given-names>J</given-names></name><name><surname>Kamatani</surname><given-names>Y</given-names></name><name><surname>Matsuda</surname><given-names>K</given-names></name><name><surname>Toda</surname><given-names>T</given-names></name><name><surname>Okada</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>A deep learning method for HLA imputation and trans-ethnic MHC fine-mapping of type 1 diabetes</article-title><source>Nature Communications</source><volume>12</volume><fpage>1</fpage><lpage>14</lpage><pub-id pub-id-type="doi">10.1038/s41467-021-21975-x</pub-id><pub-id pub-id-type="pmid">33712626</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rubinacci</surname><given-names>S</given-names></name><name><surname>Delaneau</surname><given-names>O</given-names></name><name><surname>Marchini</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Genotype imputation using the positional Burrows wheeler transform</article-title><source>PLOS Genetics</source><volume>16</volume><elocation-id>e1009049</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pgen.1009049</pub-id><pub-id pub-id-type="pmid">33196638</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sarkar</surname><given-names>E</given-names></name><name><surname>Chielle</surname><given-names>E</given-names></name><name><surname>GÃ¼rsoy</surname><given-names>G</given-names></name><name><surname>Mazonka</surname><given-names>O</given-names></name><name><surname>Gerstein</surname><given-names>M</given-names></name><name><surname>Maniatakos</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Fast and scalable private genotype imputation using machine learning and partially homomorphic encryption</article-title><source>IEEE Access: Practical Innovations, Open Solutions</source><volume>9</volume><fpage>93097</fpage><lpage>93110</lpage><pub-id pub-id-type="doi">10.1109/access.2021.3093005</pub-id><pub-id pub-id-type="pmid">34476144</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sun</surname><given-names>YV</given-names></name><name><surname>Kardia</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Imputing missing genotypic data of single-nucleotide polymorphisms using neural networks</article-title><source>European Journal of Human Genetics</source><volume>16</volume><fpage>487</fpage><lpage>495</lpage><pub-id pub-id-type="doi">10.1038/sj.ejhg.5201988</pub-id><pub-id pub-id-type="pmid">18197192</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Taliun</surname><given-names>D</given-names></name><name><surname>Harris</surname><given-names>DN</given-names></name><name><surname>Kessler</surname><given-names>MD</given-names></name><name><surname>Carlson</surname><given-names>J</given-names></name><name><surname>Szpiech</surname><given-names>ZA</given-names></name><name><surname>Torres</surname><given-names>R</given-names></name><name><surname>Taliun</surname><given-names>SAG</given-names></name><name><surname>Corvelo</surname><given-names>A</given-names></name><name><surname>Gogarten</surname><given-names>SM</given-names></name><name><surname>Kang</surname><given-names>HM</given-names></name><name><surname>Pitsillides</surname><given-names>AN</given-names></name><name><surname>LeFaive</surname><given-names>J</given-names></name><name><surname>Lee</surname><given-names>S-B</given-names></name><name><surname>Tian</surname><given-names>X</given-names></name><name><surname>Browning</surname><given-names>BL</given-names></name><name><surname>Das</surname><given-names>S</given-names></name><name><surname>Emde</surname><given-names>A-K</given-names></name><name><surname>Clarke</surname><given-names>WE</given-names></name><name><surname>Loesch</surname><given-names>DP</given-names></name><name><surname>Shetty</surname><given-names>AC</given-names></name><name><surname>Blackwell</surname><given-names>TW</given-names></name><name><surname>Smith</surname><given-names>AV</given-names></name><name><surname>Wong</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Conomos</surname><given-names>MP</given-names></name><name><surname>Bobo</surname><given-names>DM</given-names></name><name><surname>Aguet</surname><given-names>F</given-names></name><name><surname>Albert</surname><given-names>C</given-names></name><name><surname>Alonso</surname><given-names>A</given-names></name><name><surname>Ardlie</surname><given-names>KG</given-names></name><name><surname>Arking</surname><given-names>DE</given-names></name><name><surname>Aslibekyan</surname><given-names>S</given-names></name><name><surname>Auer</surname><given-names>PL</given-names></name><name><surname>Barnard</surname><given-names>J</given-names></name><name><surname>Barr</surname><given-names>RG</given-names></name><name><surname>Barwick</surname><given-names>L</given-names></name><name><surname>Becker</surname><given-names>LC</given-names></name><name><surname>Beer</surname><given-names>RL</given-names></name><name><surname>Benjamin</surname><given-names>EJ</given-names></name><name><surname>Bielak</surname><given-names>LF</given-names></name><name><surname>Blangero</surname><given-names>J</given-names></name><name><surname>Boehnke</surname><given-names>M</given-names></name><name><surname>Bowden</surname><given-names>DW</given-names></name><name><surname>Brody</surname><given-names>JA</given-names></name><name><surname>Burchard</surname><given-names>EG</given-names></name><name><surname>Cade</surname><given-names>BE</given-names></name><name><surname>Casella</surname><given-names>JF</given-names></name><name><surname>Chalazan</surname><given-names>B</given-names></name><name><surname>Chasman</surname><given-names>DI</given-names></name><name><surname>Chen</surname><given-names>Y-DI</given-names></name><name><surname>Cho</surname><given-names>MH</given-names></name><name><surname>Choi</surname><given-names>SH</given-names></name><name><surname>Chung</surname><given-names>MK</given-names></name><name><surname>Clish</surname><given-names>CB</given-names></name><name><surname>Correa</surname><given-names>A</given-names></name><name><surname>Curran</surname><given-names>JE</given-names></name><name><surname>Custer</surname><given-names>B</given-names></name><name><surname>Darbar</surname><given-names>D</given-names></name><name><surname>Daya</surname><given-names>M</given-names></name><name><surname>de Andrade</surname><given-names>M</given-names></name><name><surname>DeMeo</surname><given-names>DL</given-names></name><name><surname>Dutcher</surname><given-names>SK</given-names></name><name><surname>Ellinor</surname><given-names>PT</given-names></name><name><surname>Emery</surname><given-names>LS</given-names></name><name><surname>Eng</surname><given-names>C</given-names></name><name><surname>Fatkin</surname><given-names>D</given-names></name><name><surname>Fingerlin</surname><given-names>T</given-names></name><name><surname>Forer</surname><given-names>L</given-names></name><name><surname>Fornage</surname><given-names>M</given-names></name><name><surname>Franceschini</surname><given-names>N</given-names></name><name><surname>Fuchsberger</surname><given-names>C</given-names></name><name><surname>Fullerton</surname><given-names>SM</given-names></name><name><surname>Germer</surname><given-names>S</given-names></name><name><surname>Gladwin</surname><given-names>MT</given-names></name><name><surname>Gottlieb</surname><given-names>DJ</given-names></name><name><surname>Guo</surname><given-names>X</given-names></name><name><surname>Hall</surname><given-names>ME</given-names></name><name><surname>He</surname><given-names>J</given-names></name><name><surname>Heard-Costa</surname><given-names>NL</given-names></name><name><surname>Heckbert</surname><given-names>SR</given-names></name><name><surname>Irvin</surname><given-names>MR</given-names></name><name><surname>Johnsen</surname><given-names>JM</given-names></name><name><surname>Johnson</surname><given-names>AD</given-names></name><name><surname>Kaplan</surname><given-names>R</given-names></name><name><surname>Kardia</surname><given-names>SLR</given-names></name><name><surname>Kelly</surname><given-names>T</given-names></name><name><surname>Kelly</surname><given-names>S</given-names></name><name><surname>Kenny</surname><given-names>EE</given-names></name><name><surname>Kiel</surname><given-names>DP</given-names></name><name><surname>Klemmer</surname><given-names>R</given-names></name><name><surname>Konkle</surname><given-names>BA</given-names></name><name><surname>Kooperberg</surname><given-names>C</given-names></name><name><surname>KÃ¶ttgen</surname><given-names>A</given-names></name><name><surname>Lange</surname><given-names>LA</given-names></name><name><surname>Lasky-Su</surname><given-names>J</given-names></name><name><surname>Levy</surname><given-names>D</given-names></name><name><surname>Lin</surname><given-names>X</given-names></name><name><surname>Lin</surname><given-names>K-H</given-names></name><name><surname>Liu</surname><given-names>C</given-names></name><name><surname>Loos</surname><given-names>RJF</given-names></name><name><surname>Garman</surname><given-names>L</given-names></name><name><surname>Gerszten</surname><given-names>R</given-names></name><name><surname>Lubitz</surname><given-names>SA</given-names></name><name><surname>Lunetta</surname><given-names>KL</given-names></name><name><surname>Mak</surname><given-names>ACY</given-names></name><name><surname>Manichaikul</surname><given-names>A</given-names></name><name><surname>Manning</surname><given-names>AK</given-names></name><name><surname>Mathias</surname><given-names>RA</given-names></name><name><surname>McManus</surname><given-names>DD</given-names></name><name><surname>McGarvey</surname><given-names>ST</given-names></name><name><surname>Meigs</surname><given-names>JB</given-names></name><name><surname>Meyers</surname><given-names>DA</given-names></name><name><surname>Mikulla</surname><given-names>JL</given-names></name><name><surname>Minear</surname><given-names>MA</given-names></name><name><surname>Mitchell</surname><given-names>BD</given-names></name><name><surname>Mohanty</surname><given-names>S</given-names></name><name><surname>Montasser</surname><given-names>ME</given-names></name><name><surname>Montgomery</surname><given-names>C</given-names></name><name><surname>Morrison</surname><given-names>AC</given-names></name><name><surname>Murabito</surname><given-names>JM</given-names></name><name><surname>Natale</surname><given-names>A</given-names></name><name><surname>Natarajan</surname><given-names>P</given-names></name><name><surname>Nelson</surname><given-names>SC</given-names></name><name><surname>North</surname><given-names>KE</given-names></name><name><surname>OâConnell</surname><given-names>JR</given-names></name><name><surname>Palmer</surname><given-names>ND</given-names></name><name><surname>Pankratz</surname><given-names>N</given-names></name><name><surname>Peloso</surname><given-names>GM</given-names></name><name><surname>Peyser</surname><given-names>PA</given-names></name><name><surname>Pleiness</surname><given-names>J</given-names></name><name><surname>Post</surname><given-names>WS</given-names></name><name><surname>Psaty</surname><given-names>BM</given-names></name><name><surname>Rao</surname><given-names>DC</given-names></name><name><surname>Redline</surname><given-names>S</given-names></name><name><surname>Reiner</surname><given-names>AP</given-names></name><name><surname>Roden</surname><given-names>D</given-names></name><name><surname>Rotter</surname><given-names>JI</given-names></name><name><surname>Ruczinski</surname><given-names>I</given-names></name><name><surname>Sarnowski</surname><given-names>C</given-names></name><name><surname>Schoenherr</surname><given-names>S</given-names></name><name><surname>Schwartz</surname><given-names>DA</given-names></name><name><surname>Seo</surname><given-names>J-S</given-names></name><name><surname>Seshadri</surname><given-names>S</given-names></name><name><surname>Sheehan</surname><given-names>VA</given-names></name><name><surname>Sheu</surname><given-names>WH</given-names></name><name><surname>Shoemaker</surname><given-names>MB</given-names></name><name><surname>Smith</surname><given-names>NL</given-names></name><name><surname>Smith</surname><given-names>JA</given-names></name><name><surname>Sotoodehnia</surname><given-names>N</given-names></name><name><surname>Stilp</surname><given-names>AM</given-names></name><name><surname>Tang</surname><given-names>W</given-names></name><name><surname>Taylor</surname><given-names>KD</given-names></name><name><surname>Telen</surname><given-names>M</given-names></name><name><surname>Thornton</surname><given-names>TA</given-names></name><name><surname>Tracy</surname><given-names>RP</given-names></name><name><surname>Van Den Berg</surname><given-names>DJ</given-names></name><name><surname>Vasan</surname><given-names>RS</given-names></name><name><surname>Viaud-Martinez</surname><given-names>KA</given-names></name><name><surname>Vrieze</surname><given-names>S</given-names></name><name><surname>Weeks</surname><given-names>DE</given-names></name><name><surname>Weir</surname><given-names>BS</given-names></name><name><surname>Weiss</surname><given-names>ST</given-names></name><name><surname>Weng</surname><given-names>L-C</given-names></name><name><surname>Willer</surname><given-names>CJ</given-names></name><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Zhao</surname><given-names>X</given-names></name><name><surname>Arnett</surname><given-names>DK</given-names></name><name><surname>Ashley-Koch</surname><given-names>AE</given-names></name><name><surname>Barnes</surname><given-names>KC</given-names></name><name><surname>Boerwinkle</surname><given-names>E</given-names></name><name><surname>Gabriel</surname><given-names>S</given-names></name><name><surname>Gibbs</surname><given-names>R</given-names></name><name><surname>Rice</surname><given-names>KM</given-names></name><name><surname>Rich</surname><given-names>SS</given-names></name><name><surname>Silverman</surname><given-names>EK</given-names></name><name><surname>Qasba</surname><given-names>P</given-names></name><name><surname>Gan</surname><given-names>W</given-names></name><collab>NHLBI Trans-Omics for Precision Medicine (TOPMed) Consortium</collab><name><surname>Papanicolaou</surname><given-names>GJ</given-names></name><name><surname>Nickerson</surname><given-names>DA</given-names></name><name><surname>Browning</surname><given-names>SR</given-names></name><name><surname>Zody</surname><given-names>MC</given-names></name><name><surname>ZÃ¶llner</surname><given-names>S</given-names></name><name><surname>Wilson</surname><given-names>JG</given-names></name><name><surname>Cupples</surname><given-names>LA</given-names></name><name><surname>Laurie</surname><given-names>CC</given-names></name><name><surname>Jaquish</surname><given-names>CE</given-names></name><name><surname>Hernandez</surname><given-names>RD</given-names></name><name><surname>OâConnor</surname><given-names>TD</given-names></name><name><surname>Abecasis</surname><given-names>GR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Sequencing of 53,831 diverse genomes from the NHLBI topmed program</article-title><source>Nature</source><volume>590</volume><fpage>290</fpage><lpage>299</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-03205-y</pub-id><pub-id pub-id-type="pmid">33568819</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tian</surname><given-names>C</given-names></name><name><surname>Fei</surname><given-names>L</given-names></name><name><surname>Zheng</surname><given-names>W</given-names></name><name><surname>Xu</surname><given-names>Y</given-names></name><name><surname>Zuo</surname><given-names>W</given-names></name><name><surname>Lin</surname><given-names>CW</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep learning on image denoising: an overview</article-title><source>Neural Networks</source><volume>131</volume><fpage>251</fpage><lpage>275</lpage><pub-id pub-id-type="doi">10.1016/j.neunet.2020.07.025</pub-id><pub-id pub-id-type="pmid">32829002</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voulodimos</surname><given-names>A</given-names></name><name><surname>Doulamis</surname><given-names>N</given-names></name><name><surname>Doulamis</surname><given-names>A</given-names></name><name><surname>Protopapadakis</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Deep learning for computer vision: a brief review</article-title><source>Computational Intelligence and Neuroscience</source><volume>2018</volume><elocation-id>7068349</elocation-id><pub-id pub-id-type="doi">10.1155/2018/7068349</pub-id><pub-id pub-id-type="pmid">29487619</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>J</given-names></name><name><surname>Xu</surname><given-names>L</given-names></name><name><surname>Chen</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Advances in Neural Information Processing Systems 25 (NIPS 2012)</source><publisher-loc>Newyork, United States</publisher-loc><publisher-name>curran associates inc</publisher-name></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75600.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Stephens</surname><given-names>Matthew</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.12.01.470739" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.12.01.470739"/></front-stub><body><p>The paper describes a novel neural-network-based strategy for imputing unmeasured genotypes, which is a standard part of most association testing pipelines. The method is computationally intensive to train, but once training is complete the imputation is fast and accurate and does not require further access to a reference panel. It has the potential to be a practically-appealing alternative to existing methods. although further work (eg training of models) is required before this new approach can be applied genome-wide.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75600.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Stephens</surname><given-names>Matthew</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/024mw5h28</institution-id><institution>University of Chicago</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.12.01.470739">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.12.01.470739v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Rapid, Reference-Free Human Genotype Imputation with Denoising Autoencoders&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 4 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Molly Przeworski as the Senior Editor. The reviewers have opted to remain anonymous.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>1) More precise technical details need to be provided about the method. Code used to train the method should be made available.</p><p>2) Accuracy assessments should be made on a new dataset not used at any point in the training.</p><p>3) Comparison with HMM should use a more diverse reference panel and with standard quality controls in place, with separate comparisons for rare vs common variants.</p><p>4) The reason for the surprisingly low average accuracy of all methods, compared with</p><p>previous studies, needs to be identified and, probably, corrected. (Reported imputation accuracies in previous studies are typically R2&gt;0.8, whereas in this paper reported R2 is mostly in range 0.2-0.6).</p><p>5) Software for applying the pre-trained network to impute genotypes needs to be publicly available.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Supplemental Figures 12 and 13 â I appreciate the authors using various metrics for comparing imputation accuracies, but I think these figures are a bit confusing because it shows that SNPs with smaller MAF somehow have better imputation accuracy.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>Two particular concerns for the authors:</p><p>1) The code is not yet available. Once it is made available I'll be able to more thoroughly evaluate what the authors have done.</p><p>2) As mentioned above, there is a bit of work to do on cleaning up the run time performance evaluations.</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>Besides the points mentioned in the public review, I have the following comments/suggestions:</p><p>â While I am somewhat familiar with autoencoders, I found it quite hard to follow the description of &quot;denoising&quot; autoencoders. Part of the problem was that I did not appreciate that these are different from a regular autoencoder until I did a bit more reading. I'm not quite sure how to make the description more accessible to the audience who are not familiar with denoising autoencoders, but I think it would have helped me to have an equation giving the training objective function that is being optimized to supplement Figure 1.</p><p>â The description of the HMM methods on p3 is inaccurate. For example, the hidden states are not the &quot;to-be-imputed&quot; variants (a better high-level description would be that the hidden states represent the haplotype in the reference panel that is most closely related to the haplotype being imputed, although different HMMs may have slightly different interpretations for the hidden states) and the genotyped variants are not the &quot;observed states&quot; (they are observed, but they are not the HMM states). In any case it needs rewriting.</p><p>â p4 the analogy with single-cell &quot;dropout&quot; is misleading because the zeros in single cell data are not &quot;missing data&quot; in the same sense as the missing genotypes in genotype imputation. (See https://www.nature.com/articles/s41588-021-00873-4 for example).</p><p>â p6: although ultimately performance is what matters, the genotype encoding seems a bit weird to me: specifically the way that 0 is used for both missing data and absence of the allele (which are different things!) Is this standard in the denoising autoencoder literature? Also, can you clarify what you mean by &quot;scaled outputs can also be regarded as probabilities&quot; and &quot;This representation maintains the interdepencies among classes&quot;. Indeed, I don't see why this last is true: for example, the &quot;true&quot; genotype can never be (0,0), so if one of the pair is 0 the other must be 1, and this interdependency does not seem to be captured by the encoding.</p><p><italic>Reviewer #4 (Recommendations for the authors):</italic></p><p>Specific comments are given below.</p><p>1. The authors trained their models using HRC reference panel, which consists of predominantly European ancestry individuals. They didn't mention the reference panel used in HMM-based imputations, and I assume it was also HRC. As we know, performance of HMM-based imputation depends on the reference panels. There are larger and more diverse reference panels publicly available (e.g. TOPMed freeze 8 reference panel through the TOPMed imputation server). Does AE still have values (whatever training datasets they are using) over the TOPMed imputation server? Evaluations with diverse reference panel is particularly important for at least two reasons: (1) diverse panels are widely used (if not the most widely used); (2) AE relies more on pretrained model that no longer retains individual haplotype level information than HMM-based methods, which is conceptually susceptible to heterogeneity in reference panel. The authors may not have access to individual-level TOPMed data for training their AE models. In that case, they should at least evaluate the performance with the 1000 Genomes Project as reference.</p><p>2. The authors didn't mention any post-imputation QC steps, thus it's not clear to me which variants are included in their evaluations/comparisons. As we know, not all markers attempted (that is, markers in the reference panel) can be well imputed. It is essential to have post-imputation QC to filter out poorly imputed markers. The manuscript does not touch this important topic at all, rendering the method practically not useful. Also rather puzzlingly, the R2 showed in all the figures are largely in the range 0.2-0.6, which is inconsistent from what's reported in the literature. For common variants, HMM-based imputation methods have been reported to attain R2 &gt; 0.8 (if not even much higher); and for rare variants, the mode is close to zero. The expected bimodal distribution is not observed. Please clarify. For similar reason, quality assessment should be performed for common and rare variants separately.</p><p>3. Have the authors evaluated the computationally identified hotspots? Are they largely consistent with the &quot;true&quot; recombination hotspots? Based on Figure 1E, these hotspots are calculated according to LD. What if the reference panel contains diverse populations? The LD calculated based on heterogeneous individuals would be meaningless.</p><p>[Editorsâ note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Rapid, Reference-Free Human Genotype Imputation with Denoising Autoencoders&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by two of the original Reviewers, Molly Przeworski (Senior Editor) and a Reviewing Editor.</p><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>1. You need to clearly report the (very considerable) training time required for the autoencoder. This relates to comments from both Reviewers which I repeat here:</p><p>Reviewer 2: &quot;I don't think the authors are dealing enough with performance issues and the need to share training times for tuning. First, in the revision the authors have edited/added Figure 6 which shows a runtime comparison, however, the methods section description (lines 308-315) does not mention how only prediction time was taken into account, as the authors seem to be explaining in the Response to Reviews document. More importantly, given that software release is mostly a proof of principle- pre-trained models are available for chr22 only -further, genome-wide inference a user would have to (as I understand it): download a reference set, train tiled auto-encoders for the genome on their own hardware, and then do inference on their data of interest. Surely then it's important to profile the performance of this bit of the pipeline for the user.&quot;</p><p>Reviewer 4. That &quot;the training process is going to take approximately one year to finish.&quot; would severely limit the application of the method. If the authors, as developers, do not release pre-trained models, we would be relying on extremely motivated and computationally savvy users to perform the pre-training. Along the line, reporting only the inference speed (i.e., computing time needed to APPLY the pre-trained models) becomes insufficient since the users would need to pre-train the models for all other chromosomes before they can apply the models to their target data.</p><p>2. You need to address the original request to compare against HMMs from a diverse reference. This request was made in the original decision, but the response instead focussed on the diversity of the target samples, which is a different issue. (It seems that running the autoencoder with a new diverse reference sample would require considerable new computation, but running the HMMs with a more diverse reference should be relatively straightforward; this does seem to highlight a disadvantage of the long training time of autoencoders if they are to be retrained when new larger reference samples become available.)</p><p>This relates to the following comment from Reviewer 4:</p><p>For major comment 1, I was not referring to a diverse target but referred to a diverse reference. I know that HRC includes the samples from the 1000 Genomes Project but the aggressive variant filtering and the predominant European ancestry individuals (~30,000 versus ~2,000 non-European from the 1000 Genomes Project) make it a lousy example as a diverse reference.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.75600.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>1) More precise technical details need to be provided about the method. Code used to train the method should be made available.</p></disp-quote><p>We have made the source code and detailed instructions for use available publicly at Github.</p><p>The computational pipeline for autoencoder training and validation is available at:</p><p>https://github.com/TorkamaniLab/Imputation_Autoencoder/tree/master/autoencoder_tuning_pipeline.</p><p>The python script for calculating imputation accuracy is available at: https://github.com/TorkamaniLab/imputation_accuracy_calculator.</p><disp-quote content-type="editor-comment"><p>2) Accuracy assessments should be made on a new dataset not used at any point in the training.</p></disp-quote><p>The MESA, Wellderly, and HGDP datasets are all independent datasets, never used for training, nor model selection. Only HRC was used as reference panel or for training, and ARIC was used for model tuning. We included a statement in the methods clarifying this point.</p><disp-quote content-type="editor-comment"><p>3) Comparison with HMM should use a more diverse reference panel and with standard quality controls in place, with separate comparisons for rare vs common variants.</p></disp-quote><p>Two of our independent testing datasets (MESA, but especially HGDP), are from highly diversified multi-ethnic cohorts. HGDP includes 828 samples from 54 different populations representing all continental populations and including remote populations like Siberia, Oceania, etc. This reference panel is described in more detail in the reference below and likely represents the most diverse human genome dataset available.</p><p>BergstrÃ¶m A, et al., Insights into human genetic variation and population history from 929 diverse genomes. Science. 2020 Mar 20;367(6484):eaay5012.</p><disp-quote content-type="editor-comment"><p>4) The reason for the surprisingly low average accuracy of all methods, compared with</p><p>previous studies, needs to be identified and, probably, corrected. (Reported imputation accuracies in previous studies are typically R2&gt;0.8, whereas in this paper reported R2 is mostly in range 0.2-0.6).</p></disp-quote><p>We show average accuracy of 0.2-0.6 in Table 4, but that is the average aggregate R2 per variant across all variants (no minor allele frequency filter or binning applied). The reviewer points that the accuracy should be R2&gt;0.8, but this R2&gt;0.8 expectation applies only to common variants (allele frequency &gt;1%). Our results reflect this level of accuracy (R2&gt;0.8) for common variants as plotted in Figure 4. The aggregate accuracy we report in Table 4 is substantially lower because the vast majority of genetic variants are rare, falling below the 1% allele frequency threshold and pulling down the overall average. The parent HRC reference publication below, as well as other supporting references we provide, demonstrate this is expected and conforms with our results.</p><p>References:</p><p>Rubinacci S, Delaneau O, Marchini J. Genotype imputation using the positional burrows wheeler transform. PLoS genetics. 2020 Nov 16;16(11):e1009049.</p><p>Our curves in Figure 4 replicate this graded accuracy across allele frequencies with higher accruacy above 1% allele frequency. Other examples:</p><p>McCarthy S, Das S, Kretzschmar W, Delaneau O, Wood AR, Teumer A, Kang HM, Fuchsberger C, Danecek P, Sharp K, Luo Y. A reference panel of 64,976 haplotypes for genotype imputation. Nature genetics. 2016 Oct;48(10):1279.</p><p>Vergara C, Parker MM, Franco L, Cho MH, Valencia-Duarte AV, Beaty TH, Duggal P. Genotype imputation performance of three reference panels using African ancestry individuals. Human genetics. 2018 Apr;137(4):281-92.</p><disp-quote content-type="editor-comment"><p>5) Software for applying the pre-trained network to impute genotypes needs to be publicly available.</p></disp-quote><p>We now share the pre-trained autoencoders (including model weights and inference source code), as well as instructions on how to use them for imputation.</p><p>These resources are publicly available at https://github.com/TorkamaniLab/imputator_inference<ext-link ext-link-type="uri" xlink:href="https://github.com/TorkamaniLab/imputator_inference">.</ext-link> We have added this information to the Data Availability subsection of the Methods.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Supplemental Figures 12 and 13 â I appreciate the authors using various metrics for comparing imputation accuracies, but I think these figures are a bit confusing because it shows that SNPs with smaller MAF somehow have better imputation accuracy.</p></disp-quote><p>Yes, this is a bit of an accuracy paradox: since most of the genotypes in rare variants are homozygous reference, concordances and F-scores are inflated due to the overwhelming abundance of the negative class. In order to address this source of confusion, we have added a note to the captions of these figures explaining this phenomenon and pointing the reader to Figure S14 (and R-squared in general) for a more accurate picture of balanced accuracy.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>Two particular concerns for the authors:</p><p>1) The code is not yet available. Once it is made available I'll be able to more thoroughly evaluate what the authors have done.</p></disp-quote><p>We made both pre-trained models and the source code available now.</p><disp-quote content-type="editor-comment"><p>2) As mentioned above, there is a bit of work to do on cleaning up the run time performance evaluations.</p></disp-quote><p>We have clarified the major concerns on the performance comparisons, including further clarification that all the testing results shown are from independent datasets not used for training, nor tuning; and that the runtime comparisons extracted from HMM refer to the prediction step only, excluding the time for I/O and HMM iterations.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>Besides the points mentioned in the public review, I have the following comments/suggestions:</p><p>â While I am somewhat familiar with autoencoders, I found it quite hard to follow the description of &quot;denoising&quot; autoencoders. Part of the problem was that I did not appreciate that these are different from a regular autoencoder until I did a bit more reading. I'm not quite sure how to make the description more accessible to the audience who are not familiar with denoising autoencoders, but I think it would have helped me to have an equation giving the training objective function that is being optimized to supplement Figure 1.</p></disp-quote><p>We edited/included a couple of sentences in the introduction and methods, defining what is the most basic difference between typical autoencoders and the denoising autoencoder. We hope this helps to further clarify this issue.</p><disp-quote content-type="editor-comment"><p>â The description of the HMM methods on p3 is inaccurate. For example, the hidden states are not the &quot;to-be-imputed&quot; variants (a better high-level description would be that the hidden states represent the haplotype in the reference panel that is most closely related to the haplotype being imputed, although different HMMs may have slightly different interpretations for the hidden states) and the genotyped variants are not the &quot;observed states&quot; (they are observed, but they are not the HMM states). In any case it needs rewriting.</p></disp-quote><p>Thanks for catching this. We corrected the HMM definition in the introduction.</p><disp-quote content-type="editor-comment"><p>â p4 the analogy with single-cell &quot;dropout&quot; is misleading because the zeros in single cell data are not &quot;missing data&quot; in the same sense as the missing genotypes in genotype imputation. (See https://www.nature.com/articles/s41588-021-00873-4 for example).</p></disp-quote><p>While there is some subtlety to this point, single-cell âdropoutâ includes both molecules that actually do not exist in the cell at the time of sampling as well as molecules that do exist but are not captured by the single-cell assay. Our own experience with scRNAseq data confirms this, taking the same library and sequencing it more deeply (or applying other molecular techniques to deplete abundant species) leads to the discovery of a larger number of genes per cell.</p><p>These are true missing data.</p><p>Despite this, we agree the scRNAseq example is complex and perhaps confusing â a simpler example more true to the nature of missing data in genotype data would be more appropriate. Therefore weâve replaced those examples with the more classic image applications.</p><disp-quote content-type="editor-comment"><p>â p6: although ultimately performance is what matters, the genotype encoding seems a bit weird to me: specifically the way that 0 is used for both missing data and absence of the allele (which are different things!) Is this standard in the denoising autoencoder literature? Also, can you clarify what you mean by &quot;scaled outputs can also be regarded as probabilities&quot; and &quot;This representation maintains the interdepencies among classes&quot;. Indeed, I don't see why this last is true: for example, the &quot;true&quot; genotype can never be (0,0), so if one of the pair is 0 the other must be 1, and this interdependency does not seem to be captured by the encoding.</p></disp-quote><p>We rescale the outputs by the following formula:</p><p>AE output example [xâref, xâalt]: [0.55,0.90]</p><p>xâ²[ref,ref] = xâ²ref â (1 â xâ²alt) = 0.55 â 0.10 = 0.055</p><p>xâ²[alt,alt] = (1 â xâ²ref) â xâ²alt = 0.45 â 0.90 = 0.405</p><p>xâ²[ref,alt] = xâ²ref â xâ²alt = 0.55 â 0.90 = 0.495</p><p>where xâ is the reconstructed outputs from the autoencoders for reference (ref) and alternate (alt) alleles.</p><p>Next, we convert the rescaled outputs into probabilities ( P[ref,ref], P[alt,alt], P[alt,alt] ) by using a softmax function:<inline-formula><mml:math id="sa2m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mtext>Â </mml:mtext><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mtext>alt.alt</mml:mtext><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.252</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula><inline-formula><mml:math id="sa2m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mrow><mml:mo>â²</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.357</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula><inline-formula><mml:math id="sa2m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mi>e</mml:mi><mml:mi>f</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:msup><mml:mi>x</mml:mi><mml:mo>â²</mml:mo></mml:msup></mml:mrow></mml:msup><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mn>0.391</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> Corrupting the data can be done by using any randomly chosen value as replacement for the true data. Either 0.0 or 0.5 is typically utilized for representing fixed noise or masking noise. Even in an image, RGB 0, 0, 0 is often used but is not actually âmissingâ but black. True âmissingâ is not used in autoencoders. We didnât find any significant differences by using either of these values for masking.</p><p>We have removed the statement about interdependencies to avoid confusion. However, as a point of interest for the reviewer â while we have yet to test this point â 0,0 genotypes may actually be possible â when the position containing the SNP locus is deleted in the genome. This gets a bit complex and weâre not absolutely sure whether we would be able to capture this sort of event with our encoding, so weâve simply removed the statement.</p><disp-quote content-type="editor-comment"><p>Reviewer #4 (Recommendations for the authors):</p><p>Specific comments are given below.</p><p>1. The authors trained their models using HRC reference panel, which consists of predominantly European ancestry individuals. They didn't mention the reference panel used in HMM-based imputations, and I assume it was also HRC. As we know, performance of HMM-based imputation depends on the reference panels. There are larger and more diverse reference panels publicly available (e.g. TOPMed freeze 8 reference panel through the TOPMed imputation server). Does AE still have values (whatever training datasets they are using) over the TOPMed imputation server?</p></disp-quote><p>We included further clarification in the methods section:</p><p>âWe utilized HRC as reference panel for the HMM-based imputation tools, which is the same dataset used for training the autoencoders, and we applied the same quality control standards for both HMM-based and autoencoder-based imputation.â</p><p>Until recently, HRC has been the standard for imputation (and is still often used). Both the HMM-method and our autoencoders would improve with the use of an expanded reference. The fairest comparison is to use the same reference for both approaches.</p><disp-quote content-type="editor-comment"><p>Evaluations with diverse reference panel is particularly important for at least two reasons: (1) diverse panels are widely used (if not the most widely used); (2) AE relies more on pretrained model that no longer retains individual haplotype level information than HMM-based methods, which is conceptually susceptible to heterogeneity in reference panel. The authors may not have access to individual-level TOPMed data for training their AE models. In that case, they should at least evaluate the performance with the 1000 Genomes Project as reference.</p></disp-quote><p>We have evaluated our models on data that is significantly more diverse than the 1000 Genomes (note that 1000 Genomes is also not independent from HRC). Two of our independent testing datasets (MESA, but especially HGDP), are from highly diversified multi-ethnic cohorts. HGDP includes 828 samples from 54 different populations representing all continental populations and including remote populations like Siberia, Oceania, etc. This reference panel is described in more detail in the reference below and likely represents the most diverse human genome dataset available â more diverse (though smaller) than TopMed.</p><p>BergstrÃ¶m A, et al., Insights into human genetic variation and population history from 929 diverse genomes. Science. 2020 Mar 20;367(6484):eaay5012.</p><disp-quote content-type="editor-comment"><p>2. The authors didn't mention any post-imputation QC steps, thus it's not clear to me which variants are included in their evaluations/comparisons. As we know, not all markers attempted (that is, markers in the reference panel) can be well imputed. It is essential to have post-imputation QC to filter out poorly imputed markers. The manuscript does not touch this important topic at all, rendering the method practically not useful. Also rather puzzlingly, the R2 showed in all the figures are largely in the range 0.2-0.6, which is inconsistent from what's reported in the literature. For common variants, HMM-based imputation methods have been reported to attain R2 &gt; 0.8 (if not even much higher); and for rare variants, the mode is close to zero. The expected bimodal distribution is not observed. Please clarify. For similar reason, quality assessment should be performed for common and rare variants separately.</p></disp-quote><p>We included further clarification in the methods explaining that no post-imputation QC was applied. However, we bin performance by allele frequency which relates to imputability. And the output of our model includes genotype probabilities which can be used as a measure of post-imputation QC for end-users. This point is now explained in the Genotype Encoding section of Methods, and in the documentation of the imputation software repository available at:</p><p>https://github.com/TorkamaniLab/imputator_inference</p><p>As for overall accuracy, we show average accuracy of 0.2-0.6 in Table 4, but that is the average aggregate R2 per variant across all variants (no MAF filter or binning applied). The reviewer points that the accuracy should be R2&gt;0.8, but this R2&gt;0.8 expectation applies only to common variants (allele frequency &gt;1%). Our results reflect this level of accuracy (r2&gt;0.8) for common variants as plotted in Figure 4. The aggregate accuracy we report in Table 4 is substantially lower because the vast majority of genetic variants are rare, falling below the 1% allele frequency threshold and pulling down the average. The parent HRC reference, as well as other supporting references we provide, demonstrate this is expected and conforms with our results.</p><p>References:</p><p>Rubinacci S, Delaneau O, Marchini J. Genotype imputation using the positional burrows wheeler transform. PLoS genetics. 2020 Nov 16;16(11):e1009049.</p><p>McCarthy S, Das S, Kretzschmar W, Delaneau O, Wood AR, Teumer A, Kang HM, Fuchsberger C, Danecek P, Sharp K, Luo Y. A reference panel of 64,976 haplotypes for genotype imputation. Nature genetics. 2016 Oct;48(10):1279.</p><p>Vergara C, Parker MM, Franco L, Cho MH, Valencia-Duarte AV, Beaty TH, Duggal P. Genotype imputation performance of three reference panels using African ancestry individuals. Human genetics. 2018 Apr;137(4):281-92.</p><disp-quote content-type="editor-comment"><p>3. Have the authors evaluated the computationally identified hotspots? Are they largely consistent with the &quot;true&quot; recombination hotspots? Based on Figure 1E, these hotspots are calculated according to LD. What if the reference panel contains diverse populations? The LD calculated based on heterogeneous individuals would be meaningless.</p></disp-quote><p>Although we have not manually verified this across the whole genome, we have done several comparisons across regions of known recombination hotspots versus our LD based tiling method. One of the examples is shown in <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, where the LD-based tile boundaries identified by our method are defined by the vertical red lines, and the recombination hotspots defined by PRDM9 binding sites are represented in green.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-sa2-fig1-v2.tif"/></fig><p>Similarly, we have compared our LD-based tiling boundaries of HRC versus 1000G. The plot (<xref ref-type="fig" rid="sa2fig2">Author response image 2</xref>) shows an example where the HRC scores and 1000G scores are plotted and overlaid for some example regions from chromosome 10. The black line is HRC LD scores, the brown plot is the 1000G LD scores. The blue and red vertical lines are the genomic segment boundaries found by our method.</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-75600-sa2-fig2-v2.tif"/></fig><p>However, we had to further split certain segments that were too large due to GPU memory limitations (see Methods).As expected, the sub-segments resulting from this further split did not match to known recombination sites.</p><p>[Editorsâ note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been improved but there are some remaining issues that need to be addressed, as outlined below:</p><p>1. You need to clearly report the (very considerable) training time required for the autoencoder. This relates to comments from both Reviewers which I repeat here:</p><p>Reviewer 2: &quot;I don't think the authors are dealing enough with performance issues and the need to share training times for tuning. First, in the revision the authors have edited/added Figure 6 which shows a runtime comparison, however, the methods section description (lines 308-315) does not mention how only prediction time was taken into account, as the authors seem to be explaining in the Response to Reviews document. More importantly, given that software release is mostly a proof of principle- pre-trained models are available for chr22 only -further, genome-wide inference a user would have to (as I understand it): download a reference set, train tiled auto-encoders for the genome on their own hardware, and then do inference on their data of interest. Surely then it's important to profile the performance of this bit of the pipeline for the user.&quot;</p><p>Reviewer 4. That &quot;the training process is going to take approximately one year to finish.&quot; would severely limit the application of the method. If the authors, as developers, do not release pre-trained models, we would be relying on extremely motivated and computationally savvy users to perform the pre-training. Along the line, reporting only the inference speed (i.e., computing time needed to APPLY the pre-trained models) becomes insufficient since the users would need to pre-train the models for all other chromosomes before they can apply the models to their target data.</p></disp-quote><p>We have now expanded the description of the implications of the inference time at the end of the Results section with a discussion of the considerable training time required to generate pre-trained models that achieve our reported inference time. We contrast this with HMM-based imputation tools, statin that they are more easily adaptable to the de-novo application of novel reference panels (lines 541 â 551).</p><disp-quote content-type="editor-comment"><p>2. You need to address the original request to compare against HMMs from a diverse reference. This request was made in the original decision, but the response instead focussed on the diversity of the target samples, which is a different issue. (It seems that running the autoencoder with a new diverse reference sample would require considerable new computation, but running the HMMs with a more diverse reference should be relatively straightforward; this does seem to highlight a disadvantage of the long training time of autoencoders if they are to be retrained when new larger reference samples become available.)</p><p>This relates to the following comment from Reviewer 4:</p><p>For major comment 1, I was not referring to a diverse target but referred to a diverse reference. I know that HRC includes the samples from the 1000 Genomes Project but the aggressive variant filtering and the predominant European ancestry individuals (~30,000 versus ~2,000 non-European from the 1000 Genomes Project) make it a lousy example as a diverse reference.</p></disp-quote><p>We have acquired the TOPMed reference cohort and performed HMM-based imputation using this reference panel. A description of this reference panel is provided in methods (lines 147 â 160). Imputation accuracy comparisons are provided at the cohort level (line 476 â 485; new Figure 4âfigure supplement 3) and broken down by ancestry (lines 521 â 528; new Figure 5âfigure supplement 2 and 3). The autoencoder retains superior performance even when the HMM has access to a more diverse cohort except in the case of MESA â which is a component of TOPMed and contains a significant rate of familial relationship to the rest of the TOPMed cohort. Additional minor edits to support the addition of this analysis are made throughout the manuscript.</p></body></sub-article></article>