<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.1 20151215//EN"  "JATS-archivearticle1.dtd"><article article-type="research-article" dtd-version="1.1" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">62101</article-id><article-id pub-id-type="doi">10.7554/eLife.62101</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Feature Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Medicine</subject></subj-group><subj-group subj-group-type="sub-display-channel"><subject>Science Forum</subject></subj-group></article-categories><title-group><article-title>Improving preclinical studies through replications</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes" id="author-204935"><name><surname>Drude</surname><given-names>Natascha Ingrid</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-7153-2894</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Natascha Ingrid Drude</bold> is in the Department of Experimental Neurology, Charité–Universitätsmedizin Berlin and the BIH QUEST Center for Transforming Biomedical Research, Berlin Institute of Health, Berlin, Germany</p></bio></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-204933"><name><surname>Martinez Gamboa</surname><given-names>Lorena</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Lorena Martinez Gamboa</bold> is in the Department of Experimental Neurology, Charité–Universitätsmedizin Berlin and the BIH QUEST Center for Transforming Biomedical Research, Berlin Institute of Health, Berlin, Germany</p></bio></contrib><contrib contrib-type="author" id="author-204934"><name><surname>Danziger</surname><given-names>Meggie</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-9224-1722</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Meggie Danziger</bold> is in the Department of Experimental Neurology, Charité–Universitätsmedizin Berlin and the BIH QUEST Center for Transforming Biomedical Research, Berlin Institute of Health, Berlin, Germany</p></bio></contrib><contrib contrib-type="author" id="author-210320"><name><surname>Dirnagl</surname><given-names>Ulrich</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-0755-6119</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Ulrich Dirnagl</bold> is in the Department of Experimental Neurology, Charité–Universitätsmedizin Berlin and the BIH QUEST Center for Transforming Biomedical Research, Berlin Institute of Health, Berlin, Germany</p></bio></contrib><contrib contrib-type="author" corresp="yes" id="author-204932"><name><surname>Toelch</surname><given-names>Ulf</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-8731-3530</contrib-id><email>ulf.toelch@bihealth.de</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/><bio><p><bold>Ulf Toelch</bold> is in the BIH QUEST Center for Transforming Biomedical Research, Berlin Institute of Health, Berlin, Germany</p></bio></contrib><aff id="aff1"><label>1</label><institution>Department of Experimental Neurology, Charité–Universitätsmedizin</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff2"><label>2</label><institution>BIH QUEST Center for Transforming Biomedical Research, Berlin Institute of Health</institution><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="senior_editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Senior Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date date-type="publication" publication-format="electronic"><day>12</day><month>01</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e62101</elocation-id><history><date date-type="received" iso-8601-date="2020-08-13"><day>13</day><month>08</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-01-12"><day>12</day><month>01</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Drude et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Drude et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-62101-v2.pdf"/><abstract><p>The purpose of preclinical research is to inform the development of novel diagnostics or therapeutics, and the results of experiments on animal models of disease often inform the decision to conduct studies in humans. However, a substantial number of clinical trials fail, even when preclinical studies have apparently demonstrated the efficacy of a given intervention. A number of large-scale replication studies are currently trying to identify the factors that influence the robustness of preclinical research. Here, we discuss replications in the context of preclinical research trajectories, and argue that increasing validity should be a priority when selecting experiments to replicate and when performing the replication. We conclude that systematically improving three domains of validity – internal, external and translational – will result in a more efficient allocation of resources, will be more ethical, and will ultimately increase the chances of successful translation.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>replication</kwd><kwd>preclinical research</kwd><kwd>validity</kwd><kwd>reproducibility</kwd><kwd>translation</kwd><kwd>science forum</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Bundesministerium für Bildung und Forschung</institution></institution-wrap></funding-source><award-id>01KC1901A</award-id><principal-award-recipient><name><surname>Drude</surname><given-names>Natascha Ingrid</given-names></name><name><surname>Martinez Gamboa</surname><given-names>Lorena</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Increasing validity should be a priority when deciding which preclinical experiments to replicate and when performing replications.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>Template</meta-name><meta-value>5</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Translation from preclinical research to patients is challenging for many reasons (<xref ref-type="bibr" rid="bib13">Denayer et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Pound and Ritskes-Hoitinga, 2018</xref>). Biological complexity and the disparity between animal models of disease and humans accounts for some failures in translation, but not all (<xref ref-type="bibr" rid="bib25">Kimmelman and London, 2011</xref>; <xref ref-type="bibr" rid="bib34">Mullane and Williams, 2019</xref>; <xref ref-type="bibr" rid="bib58">van der Worp et al., 2010</xref>). Other reasons include the fact that the evidence generated in preclinical efficacy studies is often weak due to the low numbers of experimental units or entities receiving a treatment (<xref ref-type="bibr" rid="bib6">Bonapersona et al., 2019</xref>; <xref ref-type="bibr" rid="bib10">Carneiro et al., 2018</xref>; <xref ref-type="bibr" rid="bib21">Howells et al., 2014</xref>). Moreover, analyses are often not reported in full, leading to the selective reporting of outcomes and the exploitation of researcher degrees of freedom (<xref ref-type="bibr" rid="bib33">Motulsky, 2014</xref>). False positives abound in such studies, and reported effects are often inflated (<xref ref-type="bibr" rid="bib14">Dirnagl, 2020</xref>; <xref ref-type="bibr" rid="bib24">Kimmelman et al., 2014</xref>; <xref ref-type="bibr" rid="bib56">Turner and Barbee, 2019</xref>). Moreover, a strong bias against the publication of non-significant results augments this problem (<xref ref-type="bibr" rid="bib51">Sena et al., 2010</xref>) and makes meta-analytic assessment of preclinical evidence difficult (<xref ref-type="bibr" rid="bib52">Sena et al., 2014</xref>).</p><p>An obvious way to address some of these problem would be for other research groups to reproduce and then replicate preclinical studies before starting experiments on humans. By reproduce we mean to be in principle able to repeat the original study through in depth understanding of methods, protocols, and analytical pipelines used by the original research group. By replicate we mean to actually perform a study to see if the findings of the original study still hold. This potentially involves adapting some methods, protocols, and analytical pipelines (<xref ref-type="bibr" rid="bib37">Nosek and Errington, 2020a</xref>; <xref ref-type="bibr" rid="bib40">Patil et al., 2016</xref>): for example, when different animal strains are used or when environmental factors are changed (<xref ref-type="bibr" rid="bib60">Voelkl et al., 2020</xref>). Reproducibility is thus a prerequisite for engaging in replications that will increase our confidence in a finding through its wider validity.</p><p>Here, we consider the role of replications in the context of the preclinical research trajectory for a potential treatment: such a trajectory is a series of experiments designed to generate evidence that will inform any decision about testing the treatment in humans. The experiments in a preclinical research trajectory typically include exploratory studies, toxicity studies, positive and negative controls, pharmacodynamics and kinetics, and are intended to generate evidence to support an inferential claim and refute possible alternatives. They can be performed on animal models (including invertebrates, zebrafish, nonhuman primates and, quite often, rodents) or with replacement methods (such as cell cultures and organoids).</p><p>Within this framework, replications strengthen two key characteristics of preclinical experimental evidence: validity and reliability. Validity refers to the degree to which an inference is true, and reliability refers to the quality and accuracy of the data supporting an inferential claim. In this article we describe strategies for preclinical research trajectories in which replications balance reliability and validity to foster preclinical and translational research in a way that is ethical and efficient. For example, consider a researcher who hypothesizes that a disease is caused by a metabolic product. A potential drug candidate will inhibit an enzyme that is involved in the relevant metabolic process. In an exploratory study, applying the drug in a knockout mouse model of the disease reduced the metabolic product and the health condition of the animals improved. A within-lab replication confirms these initial findings. The findings are reliable as initial data and replication support the inferential claim that the drug improves health conditions. However, this does not necessarily mean that the inference is valid, particularly when extrapolated to humans: as we will outline below, the validity of such an inference can be threatened on several levels during the preclinical research trajectory.</p></sec><sec id="s2"><title>What to replicate and how to replicate</title><p>A number of large-scale replication projects have been conducted in psychology and social sciences (<xref ref-type="bibr" rid="bib8">Camerer et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Camerer et al., 2018</xref>; <xref ref-type="bibr" rid="bib39">Open Science Collaboration, 2015</xref>). Those studies were performed with healthy subjects with little to no harm anticipated through participation. Consequently, increasing the number of tested subjects was not ethically problematic or overly expensive. The same is not true for projects that involve animals, so there have been relatively few large-scale replication projects in biomedical research. Moreover, the projects that have been started – such as the Reproducibility Project: Cancer Biology, the Brazilian Reproducibility Initiative, and the Confirmatory Preclinical Studies project – all take different approaches to identifying the studies to be replicated and to performing the replications (see <xref ref-type="table" rid="table1">Table 1</xref> and <xref ref-type="box" rid="box1">Box 1</xref>).</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Overview of three large-scale replication projects in biomedical research: Reproducibility Project: Cancer Biology (RPCB); Brazilian Reproducibility Initiative (BRI); Confirmatory Preclinical Studies (CPS).</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="top"/><th valign="top">RPCB</th><th valign="top">BRI</th><th valign="top">CPS</th></tr></thead><tbody><tr><td valign="top">Selection of samples to be replicated</td><td valign="top">Main findings from 50 high impact citations/publications in cancer research</td><td valign="top">Replication of 60–100 experiments from research articles of Brazilian studies in different clinical areas</td><td valign="top">Two-step review process of proposals results in twelve projects</td></tr><tr><td valign="top">Selection of experiment</td><td valign="top">Main finding from published studies</td><td valign="top">Experiments using five pre-defined methods</td><td valign="top">Own experiments</td></tr><tr><td valign="top">Replicate own results</td><td valign="top">No</td><td valign="top">No</td><td valign="top">Yes</td></tr><tr><td valign="top">Exact Protocols</td><td valign="top">Yes (consulting original authors)</td><td valign="top">No</td><td valign="top">Yes</td></tr><tr><td valign="top">Blind to initial results</td><td valign="top">No</td><td valign="top">Yes</td><td valign="top">No</td></tr><tr><td valign="top">Pre-registration</td><td valign="top">Pre-registered study and individual Replication Protocols</td><td valign="top">Yes</td><td valign="top">Yes</td></tr><tr><td valign="top">Multi-site replication</td><td valign="top">No</td><td valign="top">Yes</td><td valign="top">Yes</td></tr></tbody></table></table-wrap><boxed-text id="box1"><label>Box 1.</label><caption><p>Three approaches to large-scale replication projects in biomedical research.</p><p>The Reproducibility Project: Cancer Biology (RPCB) started with the aim of reproducing selected findings from 50 high-impact articles published between 2010 and 2012 in the field of cancer biology (<xref ref-type="bibr" rid="bib16">Errington et al., 2014</xref>; <xref ref-type="bibr" rid="bib32">Morrison, 2014</xref>). The plan was to publish a peer-reviewed Registered Report that outlined the protocols for each attempted reproduction – based on information contained in the original paper and, if necessary, additional information obtained from the original authors – before any experiments were performed (<xref ref-type="bibr" rid="bib38">Nosek and Errington, 2020b</xref>). The experiments were to be conducted by commercial contract research organizations and academic core facilities from the Science Exchange network, and the results were to be published in a separate peer-reviewed Replication Study. The researchers performing the experiments were not blinded with regard to the original results. In the end, due to various problems, only 29 Registered Reports and 18 Replication Studies were published, and the overall conclusions of the project are currently being written up. The aim of the Brazilian Reproducibility Initiative is to assess the reproducibility of biomedical science published by researchers based in Brazil (<xref ref-type="bibr" rid="bib2">Amaral et al., 2019</xref>; <xref ref-type="bibr" rid="bib35">Neves et al., 2020</xref>). The studies selected had to use one of five experimental techniques, including behavioural and wet lab methods, on certain widely-used model organisms. The BRI researchers assume that protocols will never be reproduced exactly so they employ a ‘naturalistic approach’ in which the teams repeating the experiments can supplement the published protocols based on their best judgement and experience. Moreover, three teams will attempt to repeat each study selected, and will preregister their protocols before starting experiments. Furthermore, the researchers performing the experiments will be blinded to the identity of the original authors and the results of the paper. Recently, the Federal Ministry of Education and Research in Germany invited research groups to apply for funding to attempt to confirm promising results from their own preclinical studies (<xref ref-type="bibr" rid="bib5">BMBF-DLR, 2018</xref>). After being screened and reviewed by a panel of international experts, 12 groups have received funding under the CPS (Confirmatory Preclinical Studies) project: one condition of the project is that the groups funded have to collaborate with other groups (of their choosing) in a multi-centre approach with a view to harmonising protocols across sites. Again the groups will have to pre-register their protocols: however, as the researchers are repeating their own experiments, they will not be blinded to the original results.</p></caption></boxed-text><p>Based on the results from the study being replicated and the stage in the preclinical research trajectory, the question is: what additional evidence is needed to ultimately decide to start trials in human subjects? Throughout a sequence of preclinical experiments, validity and reliability have to be adapted at each stage, and criteria are set so that we know whether to continue, to revise, or even completely break off the experiments (<xref ref-type="fig" rid="fig1">Figure 1</xref>). How, for example, could the previous experiments be improved? How many animals should be tested? Are additional controls needed? Should additional labs be involved? At all stages the aim should be to increase the validity of replications and test the reliability of the initial finding (<xref ref-type="bibr" rid="bib14">Dirnagl, 2020</xref>; <xref ref-type="bibr" rid="bib24">Kimmelman et al., 2014</xref>; <xref ref-type="bibr" rid="bib43">Piper et al., 2019</xref>).</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Increasing different forms of validity (internal, external and translational) during a preclinical research trajectory.</title><p>This schematic shows that internal validity (green line) is higher than external validity (orange) and translational validity (blue) at the start of a preclinical research trajectory (left), and that evidence from different types of experiments can increase different types of validity. For example, evidence from exploratory studies can increase internal and external validity, and evidence from between-lab replications can increase translational validity. C1, C2 and C3 are decision points where researchers can decide to refine the current experiment (yellow arrow), stop the trajectory (red arrow), or proceed to the next experiment (green arrow).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62101-fig1-v2.tif"/></fig><p>Researchers usually start off with a study in exploratory mode. As not all details and confounders in such a study can be known upfront, the reliability and validity are potentially not fully optimized. However, even at these early stages researchers should implement strategies to mitigate risks of bias (<xref ref-type="fig" rid="fig1">Figure 1</xref>). After an initial phase, the question is: should we continue to test a particular claim? The criteria used to answer this question should be lenient: standard p-value criteria are potentially too strict and identifying the range of possible effect sizes is a viable goal in early phases. Particularly in fields with a low prevalence of true hypotheses, this will prevent discarding promising treatments too early (<xref ref-type="bibr" rid="bib1">Albers and Lakens, 2018</xref>; <xref ref-type="bibr" rid="bib27">Lakens, 2014</xref>). More stringent criteria should be applied in later experiments. Standard p-values will then identify true effects in high powered replications, which will reduce Type I errors and increase the predictive value of a set of experiments.</p><p>As we move along the research trajectory it also becomes helpful to think in terms of three types of validity: internal validity, external validity, and translation validity (<xref ref-type="fig" rid="fig1">Figure 1</xref>). To illustrate this, we return to the example of a disease caused by a metabolic product. An experiment in which the inhibition of a metabolic pathway leads to an improvement in animal health is internally valid if the measured effect (improved health) is caused by the experimental manipulation (administration of the inhibitor). Such an experiment is also externally valid if the effect is observed in other animal models and/or can be replicated in other labs. And if the effect is also observed in humans it is translationally valid. As we shall discuss, different strategies are needed to improve these three types of validity.</p><sec id="s2-1"><title>Internal validity</title><p>Poor study design and the lack of control for biases are major contributors towards low internal validity (<xref ref-type="bibr" rid="bib4">Bailoo et al., 2014</xref>; <xref ref-type="bibr" rid="bib45">Pound and Ritskes-Hoitinga, 2018</xref>; <xref ref-type="bibr" rid="bib64">Würbel, 2017</xref>). To increase internal validity, controlling for selection and detection bias is essential. For this, methods of randomization and blinding need to be clearly specified, and inclusion and exclusion criteria need to be defined, before data are collected (<xref ref-type="bibr" rid="bib64">Würbel, 2017</xref>). Replications can be leveraged to improve this further. Imagine, for example, that an exploratory study has found that a physiological factor (such as animal weight) influences the primary outcome variable: in the next round of experiments, animals could be randomized and stratified by weight. Particularly for replications, all these choices (along with detailed methods and analysis plans) should be communicated before conducting the experiment, ideally via pre-registration at a platform such as <ext-link ext-link-type="uri" xlink:href="http://www.animalstudyregistry.org">http://www.animalstudyregistry.org</ext-link> or <ext-link ext-link-type="uri" xlink:href="http://www.preclinicaltrials.eu">http://www.preclinicaltrials.eu</ext-link>.</p><p>Methods and analyses also need to be reported transparently and completely (<xref ref-type="bibr" rid="bib62">Vollert et al., 2020</xref>; <xref ref-type="bibr" rid="bib41">Percie du Sert et al., 2020</xref>). However, completely reported experiments can still have low internal validity because following a reporting guideline will not safeguard against suboptimal experimental design: that is, experiments need high internal validity and complete reporting to prevent research waste and fruitless animal testing (<xref ref-type="bibr" rid="bib29">Macleod et al., 2014</xref>).</p><p>The three replication projects described in <xref ref-type="box" rid="box1">Box 1</xref> all aim at high internal validity through exact specifications of replication protocols. Within the RPCB, for example, protocols were pre-registered, peer reviewed and published before replication experiments were performed. However, in some cases where results have been difficult to interpret, exact pre-specification of replication experiments has limited the possibility of performing the experiments in another, potentially improved way (<xref ref-type="bibr" rid="bib15">eLife, 2017</xref>). The BRI describes a more naturalistic approach where each participating lab will fill the gaps in papers as best as they can without consulting primary authors. Nevertheless, protocols will still be preregistered and undergo a round of internal peer review among collaborating labs. In summary, high levels of methodological rigour are necessary to ensure high internal validity and to make replications meaningful.</p></sec><sec id="s2-2"><title>External validity</title><p>For the assessment of external validity, research findings from one setting need to generalise to other settings (<xref ref-type="bibr" rid="bib45">Pound and Ritskes-Hoitinga, 2018</xref>). One way to increase external validity is to conduct replications at multiple sites, emulating an approach already applied in clinical trials (<xref ref-type="bibr" rid="bib12">Dechartres et al., 2011</xref>; <xref ref-type="bibr" rid="bib17">Friedman et al., 2015</xref>). Multi-centre studies (or between-lab replications at a single centre) can index known and unknown differences that define boundary conditions for investigated effects (<xref ref-type="bibr" rid="bib18">Glasgow et al., 2006</xref>; <xref ref-type="bibr" rid="bib45">Pound and Ritskes-Hoitinga, 2018</xref>). Regarding generalizability, external validity can often be improved by including aged or comorbid animals, and by performing multimodal studies with animals of different sex and/or varying strains. This systematically introduced heterogeneity strengthens external validity and explores the extent to which standardization is introducing unwanted idiosyncrasies that may limit external validity and prevent successful replication (<xref ref-type="bibr" rid="bib47">Richter et al., 2010</xref>; <xref ref-type="bibr" rid="bib59">Voelkl et al., 2018</xref>). Additionally negative and positive control groups that are added to replications can further foster external validity (<xref ref-type="bibr" rid="bib23">Kafkafi et al., 2018</xref>).</p><p>Again, the three replication projects described in <xref ref-type="box" rid="box1">Box 1</xref> take different approaches. The BRI and CPS projects take multi-centre approaches, whereas the RPCB does not, which may result in lower external validity. However, multi-centre studies have their own limitations and shortcomings as they come with an organizational overhead that includes decisions on which parts of experiments should be standardised and safeguarding adherence to the agreed protocols throughout a study (<xref ref-type="bibr" rid="bib30">Maysami et al., 2016</xref>). This can be challenging, given different infrastructure and/or resources across laboratories with different budget or resource constraints.</p><p>Difference in equipment can be one reason for variation in performing a certain intervention (e.g. surgery). Moreover, if the centres are in different countries, ethics boards and local regulations will most likely differ, complicating and potentially delaying ethics approval (<xref ref-type="bibr" rid="bib22">Hunniford et al., 2019</xref>; <xref ref-type="bibr" rid="bib28">Llovera et al., 2015</xref>; <xref ref-type="bibr" rid="bib30">Maysami et al., 2016</xref>). For example, regulations for analgesic regimes differ between jurisdictions, so that it is difficult to follow the same protocol across sites. Nevertheless, multi-centre studies are characterised by high quality standards with cross validation of results, larger sample size, lower risk of bias as compared to single-centre studies, and higher completeness of reporting (<xref ref-type="bibr" rid="bib22">Hunniford et al., 2019</xref>). This strongly suggests that a multi-centre approach will be an important component in enabling decisions about clinical trial initiation (<xref ref-type="bibr" rid="bib46">Prohaska and Etkin, 2010</xref>).</p></sec><sec id="s2-3"><title>Translational validity</title><p>Translational validity is used here as an umbrella term for factors that putatively contribute to the translation from animal models to humans. In particular, it pertains to how well measurements and animal models represent a certain disease and its underlying pathomechanisms in humans, as it is common for only a limited number of disease characteristics to present in animal models. In models of Alzheimer’s disease, for example, the focus on familial early onset genes in mouse models has potentially led to translational failures as the majority of diagnoses of Alzheimer’s disease in humans are classified as sporadic late onset form (<xref ref-type="bibr" rid="bib34">Mullane and Williams, 2019</xref>; <xref ref-type="bibr" rid="bib50">Sasaguri et al., 2017</xref>). Translational validity thus reflects whether measured parameters in animal models are diagnostic for human conditions and consequently, to what extent the observed outcomes will predict outcomes in humans (<xref ref-type="bibr" rid="bib13">Denayer et al., 2014</xref>; <xref ref-type="bibr" rid="bib34">Mullane and Williams, 2019</xref>).</p><p>Ideally, auxiliary measures are collected alongside the primary outcome variable of the initial study. While such secondary outcomes might not be recorded in early experiments, replications are ideally suited to including these additional measures. Clinical biomarkers that are diagnostic for a disease in humans can provide information on the translational potential if collected also in replications in animal models (<xref ref-type="bibr" rid="bib31">Metselaar and Lammers, 2020</xref>; <xref ref-type="bibr" rid="bib61">Volk et al., 2015</xref>). In this context, if an imaging method like MRI is used in the diagnosis of humans, the same method applied in animals can reveal whether physiological parameters and disease location are comparable. This can be combined with experiments that gather converging and discriminant evidence to identify mechanistic underpinnings of an intervention to increase translational validity and identify limitations and boundary conditions. For example, studies of pharmacodynamics and kinetics in preclinical models support in-depth understanding of physiological processes and allow comparison with human pharmacological processes (<xref ref-type="bibr" rid="bib49">Salvadori et al., 2019</xref>; <xref ref-type="bibr" rid="bib55">Tuntland et al., 2014</xref>).</p><p>Replication studies can also be performed in a more complex animal model. In cancer research, for example, the initial study might be performed in an animal model with a subcutaneous tumour, while the replication could be conducted in a more advanced tumour model, in which the development of an organ-specific tumour microenvironment more closely mimics the clinical reality (<xref ref-type="bibr" rid="bib19">Guerin et al., 2020</xref>). The decision on which additional information will be helpful should be based on an exchange between preclinical researchers and clinicians.</p><p>Therefore, translational validity needs to be considered at each stage during preclinical research.</p></sec></sec><sec id="s3"><title>Ethical conduct of replications</title><p>To optimize evidence from experiments on animals it is necessary to balance the different types of validity and reliability of experiments and replications. Early on, internal validity needs to be established with high priority. As knowledge about the animal model and disease mechanisms increases, external validity needs to be strengthened through within-lab replications and, for core results, to multiple centres. Systematic heterogeneity (additional strains, similar animal models, different sexes) will further strengthen external validity, and such heterogeneity should be introduced at the early stages of the work if this is feasible.</p><p>Replications at such later stages should also include secondary outcomes that directly link to clinically relevant parameters. However, even in high-validity experiments, reliability can be low when the number of experimental units is not sufficient to detect existing effects. For replications at this stage, reliability should be increased by increasing sample sizes or refining measurement procedures. As true preclinical effect sizes are frequently small and associated with considerable variance between experimental units, increased numbers of experimental units are needed to obtain reliable results (<xref ref-type="bibr" rid="bib7">Bonapersona et al., 2020</xref>; <xref ref-type="bibr" rid="bib10">Carneiro et al., 2018</xref>). According to the 3R principles (<xref ref-type="bibr" rid="bib48">Russell and Burch, 1959</xref>), the number of animals tested needs to reflect the current stage in the preclinical trajectory (<xref ref-type="bibr" rid="bib53">Sneddon et al., 2017</xref>; <xref ref-type="bibr" rid="bib54">Strech and Dirnagl, 2019</xref>).</p><p>There is, however, no consensus yet on how to balance ethical and statistical power considerations in replications in animal experiments. Standard approaches where power calculations are based on the point estimate of the initial study will often yield too small animal numbers (<xref ref-type="bibr" rid="bib1">Albers and Lakens, 2018</xref>; <xref ref-type="bibr" rid="bib43">Piper et al., 2019</xref>). This potentially inflates false negatives, running the risk of missing important effects and wrongfully failing a replication. Alternatives like safeguard power analysis (<xref ref-type="bibr" rid="bib42">Perugini et al., 2014</xref>), sceptical p-value (<xref ref-type="bibr" rid="bib20">Held, 2020</xref>), or adjusting for uncertainty (<xref ref-type="bibr" rid="bib3">Anderson and Maxwell, 2017</xref>) have been proposed mainly for psychological experiments with human subjects. These approaches will often yield high enough sample sizes to ensure sufficient power for replications. Due to ethical and resource constraints, preclinical replications are seldom able to test such high numbers, which may be one reason why such approaches have not yet been implemented widely in preclinical research design. Here, we see clear room for improvement and research opportunities.</p><p>Regarding the number of experimental units, the RPCB aimed at achieving at least 80% statistical power, based on the effect size measured in the original study. However, the 'winner's curse' means that published effect sizes (p&lt;0.05) tend to be larger due to random sample variability. So basing the design of a replication on the effect size reported in the original study could result in the replication being underpowered (<xref ref-type="bibr" rid="bib11">Colquhoun, 2014</xref>).</p><p>The BRI team calculated sample sizes to achieve a statistical power of 95% to detect the original effect in each of the three replications (which will be conducted in different labs). Through this, the BRI team tried to compensate for a possible inflation of the original results due to publication bias and winner’s curse. Furthermore, they planned to include additional positive and/or negative controls to ensure interpretation of the outcomes (<xref ref-type="bibr" rid="bib36">Neves and Amaral, 2020</xref>). The different approaches taken by RPCB and BRI also confirm that there is, as yet, no consensus on how to calculate animal numbers towards an ethical conduct of replications.</p></sec><sec id="s4"><title>Summary and recommendations</title><p>The goal of a preclinical research trajectory is to enable the decision to engage in clinical studies. In a simplified scheme, decision options include to discontinue experiments because of futility (hypothesis apparently not true or effects not biologically significant), to gather more evidence to resolve ambiguity (with increased validity and reliability), or to engage in a clinical study (when enough evidence is collected). Most studies in preclinical research, however, are targeted at initial findings that are often exploratory (<xref ref-type="bibr" rid="bib21">Howells et al., 2014</xref>). Systematic replication efforts that are decision-enabling are rare in academic preclinical research and have only recently begun to be conducted (<xref ref-type="bibr" rid="bib24">Kimmelman et al., 2014</xref>). For the decision to finally engage in a clinical trial, a systematic review of all experiments is needed. In such a review, evidence should be judged on the validity and reliability criteria discussed here. Ideally, this will form the basis for informative investigator brochures that are currently lacking such decision enabling information (<xref ref-type="bibr" rid="bib63">Wieschowski et al., 2018</xref>).</p><p>Our proposed framework is of course not applicable in all cases. If prior knowledge about a mechanism is already available and models are established, internal validity may already be high from the onset. Moreover, our simplified proposal may not generalise across all fields, or to academic and industry settings alike. Nonetheless, replications (and ideally every preclinical experiment) need to be framed in terms of validity and reliability. This is even more pressing as replications constitute an important foundation for successful translation. To enable the establishment of preclinical research trajectories, we see a need for action for funders and researchers.</p><p>The surprising lack of systematic replication in preclinical research also stems from lack of funding opportunities. Contrary to clinical trials, across-lab multi-centre replications are rare in preclinical research. Funders should thus design specific calls aimed at replications in the broad sense described here. Funding schemes should take the structure of preclinical research trajectories into account and may specifically be tailored towards the different experimental stages.</p><p>For replications, researchers need to specify how validity is improved by the replication compared to an initial study. Detailing how internal, external and translational validity increase in replications will emphasise the new evidence that is generated beyond the initial study and provide an ethical justification for the replication. Sample-size calculations should consider how reliability and validity are balanced against each other and define clear criteria for decisions to advance to the next stage in the trajectory. This will require a deviation from standard one-size-fits-all sample-size calculations. Researchers need guidance on how to adjust sample-size calculations and decision criteria, starting from an initial exploratory study that will serve as a proof of concept and gradually moving towards decision-enabling studies that will define whether a clinical trial is warranted.</p><p>The scientific endeavour is not limited to a single study and simple null-hypothesis testing. Even though the prevailing statistical test framework may suggest this, researchers are operating in a larger framework where evidence is accumulated over several levels with several competing alternative hypotheses (<xref ref-type="bibr" rid="bib44">Platt, 1964</xref>). Replications are an important building block, where research priorities are transparently updated according to current knowledge. This includes proper reporting at all stages and registration of research at critical stages to avoid biases (<xref ref-type="bibr" rid="bib54">Strech and Dirnagl, 2019</xref>). Currently, the literature on preclinical research and associated decisions is scarce (see, for example, <xref ref-type="bibr" rid="bib57">van der Staay et al., 2010</xref>). Research on successful – and also on less successful – research lines will inform about best practices and yield important insights how biases potentially distort evidence collection (<xref ref-type="bibr" rid="bib26">Kiwanuka et al., 2018</xref>).</p><p>In conclusion, systematically improving scientific validity in replications will improve trustworthiness and usefulness of preclinical studies and thus allow for a responsible conduct of animal experiments.</p></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Writing - original draft, Project administration, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Visualization, Writing - original draft, Writing - review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="data-availability"><title>Data availability</title><p>No data was generated.</p></sec><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albers</surname> <given-names>C</given-names></name><name><surname>Lakens</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>When power analyses based on pilot data are biased: inaccurate effect size estimators and follow-up bias</article-title><source>Journal of Experimental Social Psychology</source><volume>74</volume><fpage>187</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1016/j.jesp.2017.09.004</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amaral</surname> <given-names>OB</given-names></name><name><surname>Neves</surname> <given-names>K</given-names></name><name><surname>Wasilewska-Sampaio</surname> <given-names>AP</given-names></name><name><surname>Carneiro</surname> <given-names>CF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The Brazilian Reproducibility Initiative</article-title><source>eLife</source><volume>8</volume><elocation-id>e41602</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.41602</pub-id><pub-id pub-id-type="pmid">30720433</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Anderson</surname> <given-names>SF</given-names></name><name><surname>Maxwell</surname> <given-names>SE</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Addressing the &quot;Replication Crisis&quot;: Using original studies to design replication studies with appropriate statistical power</article-title><source>Multivariate Behavioral Research</source><volume>52</volume><fpage>305</fpage><lpage>324</lpage><pub-id pub-id-type="doi">10.1080/00273171.2017.1289361</pub-id><pub-id pub-id-type="pmid">28266872</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bailoo</surname> <given-names>JD</given-names></name><name><surname>Reichlin</surname> <given-names>TS</given-names></name><name><surname>Würbel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Refinement of experimental design and conduct in laboratory animal research</article-title><source>ILAR Journal</source><volume>55</volume><fpage>383</fpage><lpage>391</lpage><pub-id pub-id-type="doi">10.1093/ilar/ilu037</pub-id><pub-id pub-id-type="pmid">25541540</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="web"><person-group person-group-type="author"><collab>BMBF-DLR</collab></person-group><year iso-8601-date="2018">2018</year><article-title>Confirmatory preclinical studies (Förderung von konfirmatorischen präklinischen studien)</article-title><source>German Federal Ministry of Education and Research</source><ext-link ext-link-type="uri" xlink:href="https://www.gesundheitsforschung-bmbf.de/de/8344.php">https://www.gesundheitsforschung-bmbf.de/de/8344.php</ext-link><date-in-citation iso-8601-date="2021-01-14">January 14, 2021</date-in-citation></element-citation></ref><ref id="bib6"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Bonapersona</surname> <given-names>V</given-names></name><name><surname>Hoijtink</surname> <given-names>H</given-names></name><name><surname>Relacs</surname> <given-names>S</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>RePAIR: a power solution to animal experimentation</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/864652</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonapersona</surname> <given-names>V</given-names></name><name><surname>Hoijtink</surname> <given-names>H</given-names></name><name><surname>Joëls</surname> <given-names>M</given-names></name><name><surname>Sarabdjitsingh</surname> <given-names>RA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>P.201 reduction by prior animal informed research (RePAIR): a power solution to animal experimentation</article-title><source>European Neuropsychopharmacology</source><volume>31</volume><fpage>S19</fpage><lpage>S20</lpage><pub-id pub-id-type="doi">10.1016/j.euroneuro.2019.12.027</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camerer</surname> <given-names>CF</given-names></name><name><surname>Dreber</surname> <given-names>A</given-names></name><name><surname>Forsell</surname> <given-names>E</given-names></name><name><surname>Ho</surname> <given-names>TH</given-names></name><name><surname>Huber</surname> <given-names>J</given-names></name><name><surname>Johannesson</surname> <given-names>M</given-names></name><name><surname>Kirchler</surname> <given-names>M</given-names></name><name><surname>Almenberg</surname> <given-names>J</given-names></name><name><surname>Altmejd</surname> <given-names>A</given-names></name><name><surname>Chan</surname> <given-names>T</given-names></name><name><surname>Heikensten</surname> <given-names>E</given-names></name><name><surname>Holzmeister</surname> <given-names>F</given-names></name><name><surname>Imai</surname> <given-names>T</given-names></name><name><surname>Isaksson</surname> <given-names>S</given-names></name><name><surname>Nave</surname> <given-names>G</given-names></name><name><surname>Pfeiffer</surname> <given-names>T</given-names></name><name><surname>Razen</surname> <given-names>M</given-names></name><name><surname>Wu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Evaluating replicability of laboratory experiments in economics</article-title><source>Science</source><volume>351</volume><fpage>1433</fpage><lpage>1436</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0918</pub-id><pub-id pub-id-type="pmid">26940865</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Camerer</surname> <given-names>CF</given-names></name><name><surname>Dreber</surname> <given-names>A</given-names></name><name><surname>Holzmeister</surname> <given-names>F</given-names></name><name><surname>Ho</surname> <given-names>TH</given-names></name><name><surname>Huber</surname> <given-names>J</given-names></name><name><surname>Johannesson</surname> <given-names>M</given-names></name><name><surname>Kirchler</surname> <given-names>M</given-names></name><name><surname>Nave</surname> <given-names>G</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Pfeiffer</surname> <given-names>T</given-names></name><name><surname>Altmejd</surname> <given-names>A</given-names></name><name><surname>Buttrick</surname> <given-names>N</given-names></name><name><surname>Chan</surname> <given-names>T</given-names></name><name><surname>Chen</surname> <given-names>Y</given-names></name><name><surname>Forsell</surname> <given-names>E</given-names></name><name><surname>Gampa</surname> <given-names>A</given-names></name><name><surname>Heikensten</surname> <given-names>E</given-names></name><name><surname>Hummer</surname> <given-names>L</given-names></name><name><surname>Imai</surname> <given-names>T</given-names></name><name><surname>Isaksson</surname> <given-names>S</given-names></name><name><surname>Manfredi</surname> <given-names>D</given-names></name><name><surname>Rose</surname> <given-names>J</given-names></name><name><surname>Wagenmakers</surname> <given-names>EJ</given-names></name><name><surname>Wu</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Evaluating the replicability of social science experiments in Nature and Science between 2010 and 2015</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>637</fpage><lpage>644</lpage><pub-id pub-id-type="doi">10.1038/s41562-018-0399-z</pub-id><pub-id pub-id-type="pmid">31346273</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carneiro</surname> <given-names>CFD</given-names></name><name><surname>Moulin</surname> <given-names>TC</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name><name><surname>Amaral</surname> <given-names>OB</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Effect size and statistical power in the rodent fear conditioning literature - A systematic review</article-title><source>PLOS ONE</source><volume>13</volume><elocation-id>e0196258</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0196258</pub-id><pub-id pub-id-type="pmid">29698451</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Colquhoun</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An investigation of the false discovery rate and the misinterpretation of p-values</article-title><source>Royal Society Open Science</source><volume>1</volume><elocation-id>140216</elocation-id><pub-id pub-id-type="doi">10.1098/rsos.140216</pub-id><pub-id pub-id-type="pmid">26064558</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dechartres</surname> <given-names>A</given-names></name><name><surname>Boutron</surname> <given-names>I</given-names></name><name><surname>Trinquart</surname> <given-names>L</given-names></name><name><surname>Charles</surname> <given-names>P</given-names></name><name><surname>Ravaud</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Single-center trials show larger treatment effects than multicenter trials: evidence from a meta-epidemiologic study</article-title><source>Annals of Internal Medicine</source><volume>155</volume><fpage>39</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.7326/0003-4819-155-1-201107050-00006</pub-id><pub-id pub-id-type="pmid">21727292</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Denayer</surname> <given-names>T</given-names></name><name><surname>Stöhr</surname> <given-names>T</given-names></name><name><surname>Van Roy</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Animal models in translational medicine: validation and prediction</article-title><source>New Horizons in Translational Medicine</source><volume>2</volume><fpage>5</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1016/j.nhtm.2014.08.001</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Dirnagl</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2020">2020</year><chapter-title>Resolving the tension between exploration and confirmation in preclinical biomedical research</chapter-title><person-group person-group-type="editor"><name><surname>Bespalov</surname> <given-names>A</given-names></name><name><surname>Michel</surname> <given-names>M. C</given-names></name><name><surname>Steckler</surname> <given-names>T</given-names></name></person-group><source>Good Research Practice in Non-Clinical Pharmacology and Biomedicine</source><publisher-name>Springer International Publishing</publisher-name><fpage>71</fpage><lpage>79</lpage><pub-id pub-id-type="doi">10.1007/978-3-030-33656-1</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>eLife</collab></person-group><year iso-8601-date="2017">2017</year><article-title>The challenges of replication</article-title><source>eLife</source><volume>6</volume><elocation-id>e23693</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.23693</pub-id><pub-id pub-id-type="pmid">28182866</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Errington</surname> <given-names>TM</given-names></name><name><surname>Iorns</surname> <given-names>E</given-names></name><name><surname>Gunn</surname> <given-names>W</given-names></name><name><surname>Tan</surname> <given-names>FE</given-names></name><name><surname>Lomax</surname> <given-names>J</given-names></name><name><surname>Nosek</surname> <given-names>BA</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>An open investigation of the reproducibility of cancer biology research</article-title><source>eLife</source><volume>3</volume><elocation-id>e04333</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.04333</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Friedman</surname> <given-names>LM</given-names></name><name><surname>Furberg</surname> <given-names>CD</given-names></name><name><surname>DeMets</surname> <given-names>DL</given-names></name><name><surname>Reboussin</surname> <given-names>DM</given-names></name><name><surname>Granger</surname> <given-names>CB</given-names></name></person-group><year iso-8601-date="2015">2015</year><chapter-title>Multicenter Trials</chapter-title><person-group person-group-type="editor"><name><surname>Friedman</surname> <given-names>L. M</given-names></name><name><surname>Furberg</surname> <given-names>C. D</given-names></name><name><surname>DeMets</surname> <given-names>D. L</given-names></name><name><surname>Reboussin</surname> <given-names>D. M</given-names></name><name><surname>Granger</surname> <given-names>C. B</given-names></name></person-group><source>Fundamentals of Clinical Trials</source><publisher-name>Springer</publisher-name><fpage>501</fpage><lpage>518</lpage><pub-id pub-id-type="doi">10.1007/978-1-4419-1586-3</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasgow</surname> <given-names>RE</given-names></name><name><surname>Green</surname> <given-names>LW</given-names></name><name><surname>Klesges</surname> <given-names>LM</given-names></name><name><surname>Abrams</surname> <given-names>DB</given-names></name><name><surname>Fisher</surname> <given-names>EB</given-names></name><name><surname>Goldstein</surname> <given-names>MG</given-names></name><name><surname>Hayman</surname> <given-names>LL</given-names></name><name><surname>Ockene</surname> <given-names>JK</given-names></name><name><surname>Orleans</surname> <given-names>CT</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>External validity: we need to do more</article-title><source>Annals of Behavioral Medicine</source><volume>31</volume><fpage>105</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1207/s15324796abm3102_1</pub-id><pub-id pub-id-type="pmid">16542124</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerin</surname> <given-names>MV</given-names></name><name><surname>Finisguerra</surname> <given-names>V</given-names></name><name><surname>Van den Eynde</surname> <given-names>BJ</given-names></name><name><surname>Bercovici</surname> <given-names>N</given-names></name><name><surname>Trautmann</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Preclinical murine tumor models: a structural and functional perspective</article-title><source>eLife</source><volume>9</volume><elocation-id>e50740</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.50740</pub-id><pub-id pub-id-type="pmid">31990272</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Held</surname> <given-names>L</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A new standard for the analysis and design of replication studies</article-title><source>Journal of the Royal Statistical Society: Series A</source><volume>183</volume><fpage>431</fpage><lpage>448</lpage><pub-id pub-id-type="doi">10.1111/rssa.12493</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Bringing rigour to translational medicine</article-title><source>Nature Reviews Neurology</source><volume>10</volume><fpage>37</fpage><lpage>43</lpage><pub-id pub-id-type="doi">10.1038/nrneurol.2013.232</pub-id><pub-id pub-id-type="pmid">24247324</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Hunniford</surname> <given-names>VT</given-names></name><name><surname>Grudniewicz</surname> <given-names>A</given-names></name><name><surname>Fergusson</surname> <given-names>DA</given-names></name><name><surname>Grigor</surname> <given-names>E</given-names></name><name><surname>Lansdell</surname> <given-names>C</given-names></name><name><surname>Lalu</surname> <given-names>MM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Multicenter preclinical studies as an innovative method to enhance translation: a systematic review of published studies</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/591289</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kafkafi</surname> <given-names>N</given-names></name><name><surname>Agassi</surname> <given-names>J</given-names></name><name><surname>Chesler</surname> <given-names>EJ</given-names></name><name><surname>Crabbe</surname> <given-names>JC</given-names></name><name><surname>Crusio</surname> <given-names>WE</given-names></name><name><surname>Eilam</surname> <given-names>D</given-names></name><name><surname>Gerlai</surname> <given-names>R</given-names></name><name><surname>Golani</surname> <given-names>I</given-names></name><name><surname>Gomez-Marin</surname> <given-names>A</given-names></name><name><surname>Heller</surname> <given-names>R</given-names></name><name><surname>Iraqi</surname> <given-names>F</given-names></name><name><surname>Jaljuli</surname> <given-names>I</given-names></name><name><surname>Karp</surname> <given-names>NA</given-names></name><name><surname>Morgan</surname> <given-names>H</given-names></name><name><surname>Nicholson</surname> <given-names>G</given-names></name><name><surname>Pfaff</surname> <given-names>DW</given-names></name><name><surname>Richter</surname> <given-names>SH</given-names></name><name><surname>Stark</surname> <given-names>PB</given-names></name><name><surname>Stiedl</surname> <given-names>O</given-names></name><name><surname>Stodden</surname> <given-names>V</given-names></name><name><surname>Tarantino</surname> <given-names>LM</given-names></name><name><surname>Tucci</surname> <given-names>V</given-names></name><name><surname>Valdar</surname> <given-names>W</given-names></name><name><surname>Williams</surname> <given-names>RW</given-names></name><name><surname>Würbel</surname> <given-names>H</given-names></name><name><surname>Benjamini</surname> <given-names>Y</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reproducibility and replicability of rodent phenotyping in preclinical studies</article-title><source>Neuroscience &amp; Biobehavioral Reviews</source><volume>87</volume><fpage>218</fpage><lpage>232</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2018.01.003</pub-id><pub-id pub-id-type="pmid">29357292</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimmelman</surname> <given-names>J</given-names></name><name><surname>Mogil</surname> <given-names>JS</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Distinguishing between exploratory and confirmatory preclinical research will improve translation</article-title><source>PLOS Biology</source><volume>12</volume><elocation-id>e1001863</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001863</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kimmelman</surname> <given-names>J</given-names></name><name><surname>London</surname> <given-names>AJ</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Predicting harms and benefits in translational trials: ethics, evidence, and uncertainty</article-title><source>PLOS Medicine</source><volume>8</volume><elocation-id>e1001010</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1001010</pub-id><pub-id pub-id-type="pmid">21423344</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kiwanuka</surname> <given-names>O</given-names></name><name><surname>Bellander</surname> <given-names>BM</given-names></name><name><surname>Hånell</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The case for introducing pre-registered confirmatory pharmacological pre-clinical studies</article-title><source>Journal of Cerebral Blood Flow &amp; Metabolism</source><volume>38</volume><fpage>749</fpage><lpage>754</lpage><pub-id pub-id-type="doi">10.1177/0271678X18760109</pub-id><pub-id pub-id-type="pmid">29480040</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Performing high-powered studies efficiently with sequential analyses</article-title><source>European Journal of Social Psychology</source><volume>44</volume><fpage>701</fpage><lpage>710</lpage><pub-id pub-id-type="doi">10.1002/ejsp.2023</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Llovera</surname> <given-names>G</given-names></name><name><surname>Hofmann</surname> <given-names>K</given-names></name><name><surname>Roth</surname> <given-names>S</given-names></name><name><surname>Salas-Pérdomo</surname> <given-names>A</given-names></name><name><surname>Ferrer-Ferrer</surname> <given-names>M</given-names></name><name><surname>Perego</surname> <given-names>C</given-names></name><name><surname>Zanier</surname> <given-names>ER</given-names></name><name><surname>Mamrak</surname> <given-names>U</given-names></name><name><surname>Rex</surname> <given-names>A</given-names></name><name><surname>Party</surname> <given-names>H</given-names></name><name><surname>Agin</surname> <given-names>V</given-names></name><name><surname>Fauchon</surname> <given-names>C</given-names></name><name><surname>Orset</surname> <given-names>C</given-names></name><name><surname>Haelewyn</surname> <given-names>B</given-names></name><name><surname>De Simoni</surname> <given-names>MG</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name><name><surname>Grittner</surname> <given-names>U</given-names></name><name><surname>Planas</surname> <given-names>AM</given-names></name><name><surname>Plesnila</surname> <given-names>N</given-names></name><name><surname>Vivien</surname> <given-names>D</given-names></name><name><surname>Liesz</surname> <given-names>A</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Results of a preclinical randomized controlled multicenter trial (pRCT): Anti-CD49d treatment for acute brain ischemia</article-title><source>Science Translational Medicine</source><volume>7</volume><elocation-id>299ra121</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aaa9853</pub-id><pub-id pub-id-type="pmid">26246166</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macleod</surname> <given-names>MR</given-names></name><name><surname>Michie</surname> <given-names>S</given-names></name><name><surname>Roberts</surname> <given-names>I</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name><name><surname>Chalmers</surname> <given-names>I</given-names></name><name><surname>Ioannidis</surname> <given-names>JPA</given-names></name><name><surname>Salman</surname> <given-names>RA-S</given-names></name><name><surname>Chan</surname> <given-names>A-W</given-names></name><name><surname>Glasziou</surname> <given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Biomedical research: Increasing value, reducing waste</article-title><source>The Lancet</source><volume>383</volume><fpage>101</fpage><lpage>104</lpage><pub-id pub-id-type="doi">10.1016/S0140-6736(13)62329-6</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maysami</surname> <given-names>S</given-names></name><name><surname>Wong</surname> <given-names>R</given-names></name><name><surname>Pradillo</surname> <given-names>JM</given-names></name><name><surname>Denes</surname> <given-names>A</given-names></name><name><surname>Dhungana</surname> <given-names>H</given-names></name><name><surname>Malm</surname> <given-names>T</given-names></name><name><surname>Koistinaho</surname> <given-names>J</given-names></name><name><surname>Orset</surname> <given-names>C</given-names></name><name><surname>Rahman</surname> <given-names>M</given-names></name><name><surname>Rubio</surname> <given-names>M</given-names></name><name><surname>Schwaninger</surname> <given-names>M</given-names></name><name><surname>Vivien</surname> <given-names>D</given-names></name><name><surname>Bath</surname> <given-names>PM</given-names></name><name><surname>Rothwell</surname> <given-names>NJ</given-names></name><name><surname>Allan</surname> <given-names>SM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A cross-laboratory preclinical study on the effectiveness of interleukin-1 receptor antagonist in stroke</article-title><source>Journal of Cerebral Blood Flow &amp; Metabolism</source><volume>36</volume><fpage>596</fpage><lpage>605</lpage><pub-id pub-id-type="doi">10.1177/0271678X15606714</pub-id><pub-id pub-id-type="pmid">26661169</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Metselaar</surname> <given-names>JM</given-names></name><name><surname>Lammers</surname> <given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Challenges in nanomedicine clinical translation</article-title><source>Drug Delivery and Translational Research</source><volume>10</volume><fpage>721</fpage><lpage>725</lpage><pub-id pub-id-type="doi">10.1007/s13346-020-00740-5</pub-id><pub-id pub-id-type="pmid">32166632</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrison</surname> <given-names>SJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Time to do something about reproducibility</article-title><source>eLife</source><volume>3</volume><elocation-id>e03981</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.03981</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Motulsky</surname> <given-names>HJ</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Common misconceptions about data analysis and statistics</article-title><source>Journal of Pharmacology and Experimental Therapeutics</source><volume>351</volume><fpage>200</fpage><lpage>205</lpage><pub-id pub-id-type="doi">10.1124/jpet.114.219170</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mullane</surname> <given-names>K</given-names></name><name><surname>Williams</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Preclinical models of Alzheimer's disease: Relevance and translational validity</article-title><source>Current Protocols in Pharmacology</source><volume>84</volume><elocation-id>e57</elocation-id><pub-id pub-id-type="doi">10.1002/cpph.57</pub-id><pub-id pub-id-type="pmid">30802363</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Neves</surname> <given-names>K</given-names></name><name><surname>Carneiro</surname> <given-names>CFD</given-names></name><name><surname>Wasilewska-Sampaio</surname> <given-names>AP</given-names></name><name><surname>Abreu</surname> <given-names>M</given-names></name><name><surname>Gomes</surname> <given-names>BV</given-names></name><name><surname>Tan</surname> <given-names>PB</given-names></name><name><surname>Amaral</surname> <given-names>OB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Two years into the Brazilian Reproducibility Initiative: Reflections on conducting a large-scale replication of Brazilian biomedical science</article-title><source>MetaArXiv</source><pub-id pub-id-type="doi">10.31222/osf.io/njs6k</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neves</surname> <given-names>K</given-names></name><name><surname>Amaral</surname> <given-names>OB</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Addressing selective reporting of experiments through predefined exclusion criteria</article-title><source>eLife</source><volume>9</volume><elocation-id>e56626</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56626</pub-id><pub-id pub-id-type="pmid">32441650</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Errington</surname> <given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020a</year><article-title>What is replication?</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000691</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000691</pub-id><pub-id pub-id-type="pmid">32218571</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nosek</surname> <given-names>BA</given-names></name><name><surname>Errington</surname> <given-names>TM</given-names></name></person-group><year iso-8601-date="2020">2020b</year><article-title>The best time to argue about what a replication means? Before you do it</article-title><source>Nature</source><volume>583</volume><fpage>518</fpage><lpage>520</lpage><pub-id pub-id-type="doi">10.1038/d41586-020-02142-6</pub-id><pub-id pub-id-type="pmid">32694846</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Open Science Collaboration</collab></person-group><year iso-8601-date="2015">2015</year><article-title>Estimating the reproducibility of psychological science</article-title><source>Science</source><volume>349</volume><elocation-id>aac4716</elocation-id><pub-id pub-id-type="doi">10.1126/science.aac4716</pub-id><pub-id pub-id-type="pmid">26315443</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Patil</surname> <given-names>P</given-names></name><name><surname>Peng</surname> <given-names>RD</given-names></name><name><surname>Leek</surname> <given-names>JT</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>What should researchers expect when they replicate studies? A statistical view of replicability in psychological science</article-title><source>Perspectives on Psychological Science</source><volume>11</volume><fpage>539</fpage><lpage>544</lpage><pub-id pub-id-type="doi">10.1177/1745691616646366</pub-id><pub-id pub-id-type="pmid">27474140</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Percie du Sert</surname> <given-names>N</given-names></name><name><surname>Hurst</surname> <given-names>V</given-names></name><name><surname>Ahluwalia</surname> <given-names>A</given-names></name><name><surname>Alam</surname> <given-names>S</given-names></name><name><surname>Avey</surname> <given-names>MT</given-names></name><name><surname>Baker</surname> <given-names>M</given-names></name><name><surname>Browne</surname> <given-names>WJ</given-names></name><name><surname>Clark</surname> <given-names>A</given-names></name><name><surname>Cuthill</surname> <given-names>IC</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name><name><surname>Emerson</surname> <given-names>M</given-names></name><name><surname>Garner</surname> <given-names>P</given-names></name><name><surname>Holgate</surname> <given-names>ST</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Karp</surname> <given-names>NA</given-names></name><name><surname>Lazic</surname> <given-names>SE</given-names></name><name><surname>Lidster</surname> <given-names>K</given-names></name><name><surname>MacCallum</surname> <given-names>CJ</given-names></name><name><surname>Macleod</surname> <given-names>M</given-names></name><name><surname>Pearl</surname> <given-names>EJ</given-names></name><name><surname>Petersen</surname> <given-names>OH</given-names></name><name><surname>Rawle</surname> <given-names>F</given-names></name><name><surname>Reynolds</surname> <given-names>P</given-names></name><name><surname>Rooney</surname> <given-names>K</given-names></name><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>Silberberg</surname> <given-names>SD</given-names></name><name><surname>Steckler</surname> <given-names>T</given-names></name><name><surname>Würbel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The ARRIVE guidelines 2.0: updated guidelines for reporting animal research</article-title><source>PLOS Biology</source><volume>18</volume><elocation-id>e3000410</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000410</pub-id><pub-id pub-id-type="pmid">32663219</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Perugini</surname> <given-names>M</given-names></name><name><surname>Gallucci</surname> <given-names>M</given-names></name><name><surname>Costantini</surname> <given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Safeguard power as a protection against imprecise power estimates</article-title><source>Perspectives on Psychological Science</source><volume>9</volume><fpage>319</fpage><lpage>332</lpage><pub-id pub-id-type="doi">10.1177/1745691614528519</pub-id><pub-id pub-id-type="pmid">26173267</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Piper</surname> <given-names>SK</given-names></name><name><surname>Grittner</surname> <given-names>U</given-names></name><name><surname>Rex</surname> <given-names>A</given-names></name><name><surname>Riedel</surname> <given-names>N</given-names></name><name><surname>Fischer</surname> <given-names>F</given-names></name><name><surname>Nadon</surname> <given-names>R</given-names></name><name><surname>Siegerink</surname> <given-names>B</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Exact replication: foundation of science or game of chance?</article-title><source>PLOS Biology</source><volume>17</volume><elocation-id>e3000188</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.3000188</pub-id><pub-id pub-id-type="pmid">30964856</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Platt</surname> <given-names>JR</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Strong inference: certain systematic methods of scientific thinking may produce much more rapid progress than others</article-title><source>Science</source><volume>146</volume><fpage>347</fpage><lpage>353</lpage><pub-id pub-id-type="doi">10.1126/science.146.3642.347</pub-id><pub-id pub-id-type="pmid">17739513</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pound</surname> <given-names>P</given-names></name><name><surname>Ritskes-Hoitinga</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Is it possible to overcome issues of external validity in preclinical animal research? Why most animal models are bound to fail</article-title><source>Journal of Translational Medicine</source><volume>16</volume><elocation-id>304</elocation-id><pub-id pub-id-type="doi">10.1186/s12967-018-1678-1</pub-id><pub-id pub-id-type="pmid">30404629</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Prohaska</surname> <given-names>T</given-names></name><name><surname>Etkin</surname> <given-names>C</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>External validity and translation from research to implementation</article-title><source>Generations</source><volume>34</volume><fpage>59</fpage><lpage>65</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Richter</surname> <given-names>SH</given-names></name><name><surname>Garner</surname> <given-names>JP</given-names></name><name><surname>Auer</surname> <given-names>C</given-names></name><name><surname>Kunert</surname> <given-names>J</given-names></name><name><surname>Würbel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Systematic variation improves reproducibility of animal experiments</article-title><source>Nature Methods</source><volume>7</volume><fpage>167</fpage><lpage>168</lpage><pub-id pub-id-type="doi">10.1038/nmeth0310-167</pub-id><pub-id pub-id-type="pmid">20195246</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Russell</surname> <given-names>WMS</given-names></name><name><surname>Burch</surname> <given-names>RL</given-names></name></person-group><year iso-8601-date="1959">1959</year><source>The Principles of Humane Experimental Technique</source><publisher-name>Methuen</publisher-name></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Salvadori</surname> <given-names>M</given-names></name><name><surname>Cesari</surname> <given-names>N</given-names></name><name><surname>Murgia</surname> <given-names>A</given-names></name><name><surname>Puccini</surname> <given-names>P</given-names></name><name><surname>Riccardi</surname> <given-names>B</given-names></name><name><surname>Dominici</surname> <given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dissecting the pharmacodynamics and pharmacokinetics of MSCs to overcome limitations in their clinical translation</article-title><source>Molecular Therapy - Methods &amp; Clinical Development</source><volume>14</volume><fpage>1</fpage><lpage>15</lpage><pub-id pub-id-type="doi">10.1016/j.omtm.2019.05.004</pub-id><pub-id pub-id-type="pmid">31236426</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sasaguri</surname> <given-names>H</given-names></name><name><surname>Nilsson</surname> <given-names>P</given-names></name><name><surname>Hashimoto</surname> <given-names>S</given-names></name><name><surname>Nagata</surname> <given-names>K</given-names></name><name><surname>Saito</surname> <given-names>T</given-names></name><name><surname>De Strooper</surname> <given-names>B</given-names></name><name><surname>Hardy</surname> <given-names>J</given-names></name><name><surname>Vassar</surname> <given-names>R</given-names></name><name><surname>Winblad</surname> <given-names>B</given-names></name><name><surname>Saido</surname> <given-names>TC</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>APP mouse models for Alzheimer’s disease preclinical studies</article-title><source>The EMBO Journal</source><volume>36</volume><fpage>2473</fpage><lpage>2487</lpage><pub-id pub-id-type="doi">10.15252/embj.201797397</pub-id><pub-id pub-id-type="pmid">28768718</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>van der Worp</surname> <given-names>HB</given-names></name><name><surname>Bath</surname> <given-names>PM</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Publication bias in reports of animal stroke studies leads to major overstatement of efficacy</article-title><source>PLOS Biology</source><volume>8</volume><elocation-id>e1000344</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1000344</pub-id><pub-id pub-id-type="pmid">20361022</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>Currie</surname> <given-names>GL</given-names></name><name><surname>McCann</surname> <given-names>SK</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Systematic reviews and meta-analysis of preclinical studies: why perform them and how to appraise them critically</article-title><source>Journal of Cerebral Blood Flow &amp; Metabolism</source><volume>34</volume><fpage>737</fpage><lpage>742</lpage><pub-id pub-id-type="doi">10.1038/jcbfm.2014.28</pub-id><pub-id pub-id-type="pmid">24549183</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sneddon</surname> <given-names>LU</given-names></name><name><surname>Halsey</surname> <given-names>LG</given-names></name><name><surname>Bury</surname> <given-names>NR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Considering aspects of the 3Rs principles within experimental animal biology</article-title><source>The Journal of Experimental Biology</source><volume>220</volume><fpage>3007</fpage><lpage>3016</lpage><pub-id pub-id-type="doi">10.1242/jeb.147058</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strech</surname> <given-names>D</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>3Rs missing: animal research without scientific value is unethical</article-title><source>BMJ Open Science</source><volume>3</volume><elocation-id>bmjos-2018-000048</elocation-id><pub-id pub-id-type="doi">10.1136/bmjos-2018-000048</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuntland</surname> <given-names>T</given-names></name><name><surname>Ethell</surname> <given-names>B</given-names></name><name><surname>Kosaka</surname> <given-names>T</given-names></name><name><surname>Blasco</surname> <given-names>F</given-names></name><name><surname>Zang</surname> <given-names>RX</given-names></name><name><surname>Jain</surname> <given-names>M</given-names></name><name><surname>Gould</surname> <given-names>T</given-names></name><name><surname>Hoffmaster</surname> <given-names>K</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Implementation of pharmacokinetic and pharmacodynamic strategies in early research phases of drug discovery and development at Novartis Institute of Biomedical Research</article-title><source>Frontiers in Pharmacology</source><volume>5</volume><elocation-id>174</elocation-id><pub-id pub-id-type="doi">10.3389/fphar.2014.00174</pub-id><pub-id pub-id-type="pmid">25120485</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner</surname> <given-names>PV</given-names></name><name><surname>Barbee</surname> <given-names>RW</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Responsible science and research animal use</article-title><source>ILAR Journal</source><volume>60</volume><fpage>1</fpage><lpage>4</lpage><pub-id pub-id-type="doi">10.1093/ilar/ilz020</pub-id><pub-id pub-id-type="pmid">31930313</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Staay</surname> <given-names>FJ</given-names></name><name><surname>Arndt</surname> <given-names>SS</given-names></name><name><surname>Nordquist</surname> <given-names>RE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The standardization–generalization dilemma: A way out</article-title><source>Genes, Brain, and Behavior</source><volume>9</volume><fpage>849</fpage><lpage>855</lpage><pub-id pub-id-type="doi">10.1111/j.1601-183X.2010.00628.x</pub-id><pub-id pub-id-type="pmid">20662940</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Worp</surname> <given-names>HB</given-names></name><name><surname>Howells</surname> <given-names>DW</given-names></name><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>Porritt</surname> <given-names>MJ</given-names></name><name><surname>Rewell</surname> <given-names>S</given-names></name><name><surname>O'Collins</surname> <given-names>V</given-names></name><name><surname>Macleod</surname> <given-names>MR</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Can animal models of disease reliably inform human studies?</article-title><source>PLOS Medicine</source><volume>7</volume><elocation-id>e1000245</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.1000245</pub-id><pub-id pub-id-type="pmid">20361020</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voelkl</surname> <given-names>B</given-names></name><name><surname>Vogt</surname> <given-names>L</given-names></name><name><surname>Sena</surname> <given-names>ES</given-names></name><name><surname>Würbel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Reproducibility of preclinical animal research improves with heterogeneity of study samples</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2003693</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2003693</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Voelkl</surname> <given-names>B</given-names></name><name><surname>Altman</surname> <given-names>NS</given-names></name><name><surname>Forsman</surname> <given-names>A</given-names></name><name><surname>Forstmeier</surname> <given-names>W</given-names></name><name><surname>Gurevitch</surname> <given-names>J</given-names></name><name><surname>Jaric</surname> <given-names>I</given-names></name><name><surname>Karp</surname> <given-names>NA</given-names></name><name><surname>Kas</surname> <given-names>MJ</given-names></name><name><surname>Schielzeth</surname> <given-names>H</given-names></name><name><surname>Van de Casteele</surname> <given-names>T</given-names></name><name><surname>Würbel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Reproducibility of animal research in light of biological variation</article-title><source>Nature Reviews Neuroscience</source><volume>21</volume><fpage>384</fpage><lpage>393</lpage><pub-id pub-id-type="doi">10.1038/s41583-020-0313-3</pub-id><pub-id pub-id-type="pmid">32488205</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Volk</surname> <given-names>HD</given-names></name><name><surname>Stevens</surname> <given-names>MM</given-names></name><name><surname>Mooney</surname> <given-names>DJ</given-names></name><name><surname>Grainger</surname> <given-names>DW</given-names></name><name><surname>Duda</surname> <given-names>GN</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Key elements for nourishing the translational research environment</article-title><source>Science Translational Medicine</source><volume>7</volume><elocation-id>282cm2</elocation-id><pub-id pub-id-type="doi">10.1126/scitranslmed.aaa2049</pub-id><pub-id pub-id-type="pmid">25855490</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vollert</surname> <given-names>J</given-names></name><name><surname>Schenker</surname> <given-names>E</given-names></name><name><surname>Macleod</surname> <given-names>M</given-names></name><name><surname>Bespalov</surname> <given-names>A</given-names></name><name><surname>Wuerbel</surname> <given-names>H</given-names></name><name><surname>Michel</surname> <given-names>M</given-names></name><name><surname>Dirnagl</surname> <given-names>U</given-names></name><name><surname>Potschka</surname> <given-names>H</given-names></name><name><surname>Waldron</surname> <given-names>A-M</given-names></name><name><surname>Wever</surname> <given-names>K</given-names></name><name><surname>Steckler</surname> <given-names>T</given-names></name><name><surname>van de Casteele</surname> <given-names>T</given-names></name><name><surname>Altevogt</surname> <given-names>B</given-names></name><name><surname>Sil</surname> <given-names>A</given-names></name><name><surname>Rice</surname> <given-names>ASC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Systematic review of guidelines for internal validity in the design, conduct and analysis of preclinical biomedical experiments involving laboratory animals</article-title><source>BMJ Open Science</source><volume>4</volume><elocation-id>e100046</elocation-id><pub-id pub-id-type="doi">10.1136/bmjos-2019-100046</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wieschowski</surname> <given-names>S</given-names></name><name><surname>Chin</surname> <given-names>WWL</given-names></name><name><surname>Federico</surname> <given-names>C</given-names></name><name><surname>Sievers</surname> <given-names>S</given-names></name><name><surname>Kimmelman</surname> <given-names>J</given-names></name><name><surname>Strech</surname> <given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Preclinical efficacy studies in investigator brochures: do they enable risk-benefit assessment?</article-title><source>PLOS Biology</source><volume>16</volume><elocation-id>e2004879</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.2004879</pub-id><pub-id pub-id-type="pmid">29621228</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Würbel</surname> <given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>More than 3Rs: the importance of scientific validity for harm-benefit analysis of animal research</article-title><source>Lab Animal</source><volume>46</volume><fpage>164</fpage><lpage>166</lpage><pub-id pub-id-type="doi">10.1038/laban.1220</pub-id><pub-id pub-id-type="pmid">28328898</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62101.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group><contrib contrib-type="editor"><name><surname>Rodgers</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution>eLife</institution><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Winchester</surname><given-names>Catherine</given-names> </name><role>Reviewer</role><aff><institution>Beatson Institute</institution><country>United Kingdom</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Wuerbel</surname><given-names>Hanno</given-names> </name><role>Reviewer</role><aff><institution>University of Bern</institution><country>Switzerland</country></aff></contrib></contrib-group></front-stub><body><boxed-text><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p>Thank you for submitting your article &quot;Improving the predictiveness and ethics of preclinical studies through replications&quot; to <italic>eLife</italic> for consideration as a Feature Article. Your article has been reviewed by three peer reviewers, and the evaluation has been overseen by the <italic>eLife</italic> Features Editor (Peter Rodgers). The following individuals involved in review of your submission have agreed to reveal their identity: Catherine Winchester (Reviewer #1); Hanno Wuerbel (Reviewer #2).</p><p>The reviewers and editors have discussed the reviews and we have drafted this decision letter to help you prepare a revised submission.</p><p>Summary</p><p>This paper presents an informal framework to guide preclinical research programmes, with the aim to systematically and gradually improve the reliability and validity of the research findings. According to the authors, such a procedure would not only increase chances for translational success but also lead to a more efficient allocation of resources and ethical use of animals. While the overall message is clear – most experiments are part of larger research programmes, and experimental designs need to be adjusted to the specific questions asked at any stage of a research programme – the authors need to be more precise in their definition/use of terms like replication, and to provide more details on how their proposals would work in practice.</p><p>Essential revisions</p><p>1) Definition of replication</p><p>The definition of &quot;replication&quot; used in the article seems almost redundant with the term &quot;experiment&quot;. For example, the authors state: &quot;Here, we define replications as experiments that aim to generate evidence that supports a previously made inferential claim (Nosek and Errington, 2020a)&quot;.</p><p>First, I am not sure this is equivalent to definition given in Nosek and Errington, 2020a: &quot;Replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research&quot;. This should be clarified.</p><p>However, even if so, I am still not convinced it would be a useful definition of &quot;replication&quot;. Nosek and Errington conclude: &quot;Theories make predictions; replications test those predictions. Outcomes from replications are fodder for refining, altering, or extending theory to generate new predictions. Replication is a central part of the iterative maturing cycle of description, prediction, and explanation.&quot;</p><p>This is in fact a concise description of the &quot;scientific method&quot;. Thus, Wikipedia (sorry about that….) describes the &quot;scientific method&quot; as: &quot;It [the scientific method] involves formulating hypotheses, via induction, based on such observations; experimental and measurement-based testing of deductions drawn from the hypotheses; and refinement (or elimination) of the hypotheses based on the experimental findings.&quot;</p><p>I don't think it makes sense to replace well established terms such as &quot;scientific method&quot; or &quot;experiment&quot; by &quot;replication&quot;, which for most scientists has a much more specific meaning. The authors should clarify this point and clearly state how they distinguish their term &quot;replication&quot; from the term &quot;experiment&quot; (every experiment is a test of a claim from prior research and therefore fits Nosek and Errington's definition of replication) and how they distinguish their framework from the &quot;scientific method&quot; in general.</p><p>This becomes even more explicit in the following paragraph, where they state: &quot;Besides within and between laboratory replications, experiments in such series include initial exploratory studies, toxicity studies, positive and negative controls, pharmacodynamics and kinetics that all aim to generate evidence to support an inferential claim and refute possible alternatives&quot;. Thus, according to the authors, all of these experiments (toxicity studies, pharmacodynamics, etc.) are nothing but replications – this is stretching the term replication extremely far, and I am not convinced that readers will follow them. And it would not be necessary; they could present the exact same framework by using conventional terms.</p><p>Related to this, please explain the difference/relationship between replication and reproducibility.</p><p>2) The section &quot;What to replicate?&quot;</p><p>The description of the three projects in the section is too long, and needs to be better integrated into the article.</p><p>Note from the Editor: since this point is largely an editorial matter, dealing with it can be deferred until later.</p><p>3) The section &quot;How to replicate?&quot;</p><p>The stepwise procedure presented in the text and in Figure 1 is not well developed. The trade-offs between the different dimensions of validity are not presented in any detail, and the ethical and scientific implications of different decisions at different stages in a research programme are not discussed. In particular:</p><p>– &quot;At the start, reliability and validity are potentially low due to biased assessment of outcomes, unblinded conduct of experiments, undisclosed researcher degrees of freedom, low sample sizes, etc.&quot;. This reads like a recommendation: start your research line with quick and dirty studies and only refine methods once you have discovered something interesting. I am sure that this is not what the authors wanted to say but it is how it may come across. The fact that these problems are prevalent throughout past (and current) preclinical research does not justify them. Even at early, exploratory stages of a research programme, scientific rigour is essential for both scientific and ethical reasons. Therefore, I do not agree with the authors that internal validity may be low initially and should only be improved in the course of a research programme. Studies may be smaller (using smaller sample sizes) initially, but there is no excuse for ignoring any other measure against risks of bias (randomization, blinding, complete reporting of all measured outcome variables, etc.) as these are simply part of good research practice!</p><p>– This brings me to another point: I am not convinced that exploratory research should be conducted under highly restricted conditions. In fact, including variation of conditions (e.g. genotype, environment, measurement) right from the beginning may be much more productive a strategy for generating valuable hypotheses (see e.g. Voelkl et al., 2020). Furthermore, designing larger and more complex confirmatory or replication studies (covering a larger inference space) may reduce the need for additional studies to assess convergent and discriminant validity and external validity. Thus, there is a trade-off between more but simpler vs. fewer but more complex studies that is not at all represented in the framework presented here (and in Figure 1).</p><p>– &quot;At this stage, criteria to decide whether to conduct additional experiments and replications should be lenient&quot;. Here it seems that the authors do make a difference between &quot;experiments&quot; and &quot;replications&quot;, but as discussed above, it is unclear on what grounds.</p><p>– The definition of &quot;translational validity&quot; needs attention. They state that it covers &quot;construct validity&quot; and &quot;predictive validity&quot;, but not &quot;concurrent validity&quot; and &quot;content validity&quot;, which are the four common dimensions of test validity. Thus, it is unclear whether they actually mean &quot;test validity&quot; (but consider concurrent validity and content validity less important) or whether they consider &quot;translational validity&quot; to be different from &quot;test validity&quot;. Importantly, however, &quot;construct validity&quot; is made up by &quot;convergent validity&quot; and &quot;discriminant validity&quot;. This means that assessing &quot;construct validity&quot; requires tests of convergent and discriminant validity that cannot – according to common terminology – be considered as replications as they explicitly test and compare measures of different constructs that should or should not be related.</p><p>– &quot;In studies with low internal validity that stand at the beginning of a new research line, e.g. in an exploratory phase, low numbers of animals may be acceptable.&quot; Again, I don't think low internal validity is ever acceptable, certainly not if animals are involved. The number of animals always needs to be justified by the specific question to be answered – that in exploratory research often low numbers of animals are used does not necessarily mean that this is acceptable.</p><p>– The recommendations in this section are rather vague. For example, the authors write: &quot;For replications, researchers need to specify how validity is improved by the replication compared to the initial study.&quot; Yet, after this general demand, no further details or suggestions are given how this can be achieved. Similarly, they write &quot;Researchers need guidance on how to adjust sample size calculations...&quot; but no guidance is offered here nor are any hints given where to find such guidance (references might be useful) on what principles this guidance should be based etc.</p><p>– Please comment on how much evidence (how many replication studies/ different approaches exploring the same question) is needed? And who should collate this body of evidence – researchers, clinicians, funders, a committee?</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62101.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions</p><p>1) Definition of replication</p><p>The definition of &quot;replication&quot; used in the article seems almost redundant with the term &quot;experiment&quot;. For example, the authors state: &quot;Here, we define replications as experiments that aim to generate evidence that supports a previously made inferential claim (Nosek and Errington, 2020a)&quot;.</p><p>First, I am not sure this is equivalent to definition given in Nosek and Errington, 2020a: &quot;Replication is a study for which any outcome would be considered diagnostic evidence about a claim from prior research&quot;. This should be clarified.</p><p>However, even if so, I am still not convinced it would be a useful definition of &quot;replication&quot;. Nosek and Errington conclude: &quot;Theories make predictions; replications test those predictions. Outcomes from replications are fodder for refining, altering, or extending theory to generate new predictions. Replication is a central part of the iterative maturing cycle of description, prediction, and explanation.&quot;</p><p>This is in fact a concise description of the &quot;scientific method&quot;. Thus, Wikipedia (sorry about that….) describes the &quot;scientific method&quot; as: &quot;It [the scientific method] involves formulating hypotheses, via induction, based on such observations; experimental and measurement-based testing of deductions drawn from the hypotheses; and refinement (or elimination) of the hypotheses based on the experimental findings.&quot;</p><p>I don't think it makes sense to replace well established terms such as &quot;scientific method&quot; or &quot;experiment&quot; by &quot;replication&quot;, which for most scientists has a much more specific meaning. The authors should clarify this point and clearly state how they distinguish their term &quot;replication&quot; from the term &quot;experiment&quot; (every experiment is a test of a claim from prior research and therefore fits Nosek and Errington's definition of replication) and how they distinguish their framework from the &quot;scientific method&quot; in general.</p><p>This becomes even more explicit in the following paragraph, where they state: &quot;Besides within and between laboratory replications, experiments in such series include initial exploratory studies, toxicity studies, positive and negative controls, pharmacodynamics and kinetics that all aim to generate evidence to support an inferential claim and refute possible alternatives&quot;. Thus, according to the authors, all of these experiments (toxicity studies, pharmacodynamics, etc.) are nothing but replications – this is stretching the term replication extremely far, and I am not convinced that readers will follow them. And it would not be necessary; they could present the exact same framework by using conventional terms.</p><p>Related to this, please explain the difference/relationship between replication and reproducibility.</p></disp-quote><p>We agree that our definition was not exactly matching the Nosek and Errington definition. We now clarify that replications are not solely about supporting evidence but works in both ways also to refute a claim. Starting from the admittedly very broad Nosek and Errington definition, we now specify that a replication is indeed based on a specific previous experiment. We thus rewrote the passage mentioned above to clarify this important issue. We now distinguish between experiments that test the same claim but with different design and approach and replications that are closely modelled after an initial experiment, but are changed to improve validity and reliability.</p><p>“Here, we define replications as experiments that are based on previous studies that aim to distinguish false from true claims (Nosek and Errington, 2020a). For this, previous experiments need to be reproducible with all methods and analytical pipelines unambiguously described. Reproducibility is thus a necessary prerequisite to engage in a contrastable replication (Patil et al., 2016; Plesser, 2018). Replications can deviate from previous experimental protocols by e.g. introducing different animal strains or changing environmental factors. Introducing such systematic heterogeneity between studies potentially strengthens generated evidence about inferential claims (Voelkl et al., 2020). “</p><p>With regard to the scientific method, we now emphasise that even tough preclinical research is of course based on a scientific method we now emphasise distinct features (measuring human conditions entirely in model systems) that sets it apart from others disciplines.</p><p>“With its goal to closely model human disease conditions, preclinical research differs from other scientific disciplines as there is a biological gap between experimentally studied models and patients as the ultimate beneficent. This affects the role of replications as they should not only confirm previous results but ideally also increase predictive power for the human case to enable successful translation.”</p><disp-quote content-type="editor-comment"><p>2) The section &quot;What to replicate?&quot;</p><p>The description of the three projects in the section is too long, and needs to be better integrated into the article.</p><p>Note from the Editor: since this point is largely an editorial matter, dealing with it can be deferred until later.</p></disp-quote><p>We make a suggestion how to shorten this paragraph. We reduced the number of words from 1027 to 807. We look forward to discussing this paragraph further if necessary.</p><disp-quote content-type="editor-comment"><p>3) The section &quot;How to replicate?&quot;</p><p>The stepwise procedure presented in the text and in Figure 1 is not well developed. The trade-offs between the different dimensions of validity are not presented in any detail, and the ethical and scientific implications of different decisions at different stages in a research programme are not discussed. In particular:</p><p>– &quot;At the start, reliability and validity are potentially low due to biased assessment of outcomes, unblinded conduct of experiments, undisclosed researcher degrees of freedom, low sample sizes, etc.&quot;. This reads like a recommendation: start your research line with quick and dirty studies and only refine methods once you have discovered something interesting. I am sure that this is not what the authors wanted to say but it is how it may come across. The fact that these problems are prevalent throughout past (and current) preclinical research does not justify them. Even at early, exploratory stages of a research programme, scientific rigour is essential for both scientific and ethical reasons. Therefore, I do not agree with the authors that internal validity may be low initially and should only be improved in the course of a research programme. Studies may be smaller (using smaller sample sizes) initially, but there is no excuse for ignoring any other measure against risks of bias (randomization, blinding, complete reporting of all measured outcome variables, etc.) as these are simply part of good research practice!</p></disp-quote><p>We fully agree with the reviewers here and admit that we introduced some ambiguity here that we now clarify. In particular, we stress that internal validity should be already high in early stages. We also adjusted Figure 1 to indicate that internal validity should be already considered at early stages.</p><p>“At the start, researchers usually start off with an explorative study. As not all details and confounders in such a study can be known upfront reliability and validity are potentially not fully optimized. However, even at these early stages researchers should implement e.g. strategies to mitigate risks of bias (Figure 1).”</p><disp-quote content-type="editor-comment"><p>– This brings me to another point: I am not convinced that exploratory research should be conducted under highly restricted conditions. In fact, including variation of conditions (e.g. genotype, environment, measurement) right from the beginning may be much more productive a strategy for generating valuable hypotheses (see e.g. Voelkl et al., 2020). Furthermore, designing larger and more complex confirmatory or replication studies (covering a larger inference space) may reduce the need for additional studies to assess convergent and discriminant validity and external validity. Thus, there is a trade-off between more but simpler vs. fewer but more complex studies that is not at all represented in the framework presented here (and in Figure 1).</p></disp-quote><p>We agree with the reviewers that the standardisation fallacy is an important topic, particularly to be considered in preclinical research. As our focus is on the role of replications, we now extend our description on within and between laboratory replications to consider heterogenization. We emphasise this at various locations now. We further point readers to the mentioned paper by Voelkl et al., 2020 as a reference for a more general discussion. At this stage, we feel that introducing also a discussion on how to start a preclinical research process and negotiate simpler vs more complex studies would stray too far from our core message. To accommodate this important thought nonetheless we now state:</p><p>“Systematic heterogeneity (additional strains, similar animal models, different sex) will further strengthen external validity. If feasible heterogenization should be introduced at early stages already.”</p><disp-quote content-type="editor-comment"><p>– &quot;At this stage, criteria to decide whether to conduct additional experiments and replications should be lenient&quot;. Here it seems that the authors do make a difference between &quot;experiments&quot; and &quot;replications&quot;, but as discussed above, it is unclear on what grounds.</p></disp-quote><p>We adjusted the Introduction (see above) to make the distinction between a replication and additional experiments in preclinical research clear.</p><disp-quote content-type="editor-comment"><p>– The definition of &quot;translational validity&quot; needs attention. They state that it covers &quot;construct validity&quot; and &quot;predictive validity&quot;, but not &quot;concurrent validity&quot; and &quot;content validity&quot;, which are the four common dimensions of test validity. Thus, it is unclear whether they actually mean &quot;test validity&quot; (but consider concurrent validity and content validity less important) or whether they consider &quot;translational validity&quot; to be different from &quot;test validity&quot;. Importantly, however, &quot;construct validity&quot; is made up by &quot;convergent validity&quot; and &quot;discriminant validity&quot;. This means that assessing &quot;construct validity&quot; requires tests of convergent and discriminant validity that cannot – according to common terminology – be considered as replications as they explicitly test and compare measures of different constructs that should or should not be related.</p></disp-quote><p>We rewrote the paragraph on translational validity to address these points. Importantly, we removed all references to other types of validity than translational validity. The types of validity cited are derived mainly from psychological test theory. They have specific meanings in this context and it is (after lengthy discussion amongst the authors) not clear whether they relate to preclinical research in the same way they relate to psychological test theory. Even in psychology, construct validity is still a hotly debated topic where Meehl/Cronbach accounts clash with Messick for example. As this paper is addressing preclinical researchers who will most likely not be familiar with these terms and their theoretical framing, we removed the terms. We retained the original implications though. That is, translational validity is about the appropriateness of the disease model on a mechanistic level and how well it predicts the human condition. We also introduced as suggested by the reviewers converging and discriminant evidence without however resorting to calling this a type of validity. We clarify also that experiments generating converging and discriminant evidence are separate from replications.</p><p>“The term translational validity is used here as an umbrella term for factors that putatively contribute to translational success. It pertains to how well measurements and animal models represent a certain disease and its underlying pathomechanisms in humans. To assess this, complementary experiments evaluate the bounds of a model to discriminate it from other very similar diseases and collect converging evidence from different approaches. Often only parts of a disease are present in animal models. In models of neurodegenerative diseases like Alzheimer’s disease (AD) (Sasaguri et al., 2017), the focus on familial early onset genes in mouse models has potentially led to translational failures as the majority of human AD diagnoses are classified as sporadic late onset form (Mullane and Williams, 2019). Translational validity thus reflects whether measured parameters in animal models are diagnostic for human conditions and consequently, to what extent the observed outcomes will predict outcomes in humans (Denayer et al., 2014; Mullane and Williams, 2019).”</p><disp-quote content-type="editor-comment"><p>– &quot;In studies with low internal validity that stand at the beginning of a new research line, e.g. in an exploratory phase, low numbers of animals may be acceptable.&quot; Again, I don't think low internal validity is ever acceptable, certainly not if animals are involved. The number of animals always needs to be justified by the specific question to be answered – that in exploratory research often low numbers of animals are used does not necessarily mean that this is acceptable.</p><p>– The recommendations in this section are rather vague. For example, the authors write: &quot;For replications, researchers need to specify how validity is improved by the replication compared to the initial study.&quot; Yet, after this general demand, no further details or suggestions are given how this can be achieved. Similarly, they write &quot;Researchers need guidance on how to adjust sample size calculations...&quot; but no guidance is offered here nor are any hints given where to find such guidance (references might be useful) on what principles this guidance should be based etc.</p></disp-quote><p>Here, we want to point out that the number of animals is in our view not so much related to validity as to reliability. We now address this point in more detail (the reviewers note themselves above: “Studies may be smaller (using smaller sample sizes) initially.”).</p><p>“However, even in high-validity experiments, reliability can be low when the number of experimental units is not sufficient to detect existing effects. For replications at this stage, reliability should be increased by increasing sample sizes or refining measurement procedures. As true preclinical effect sizes are frequently small and associated with considerable variance between experimental units, increased numbers of experimental units are needed to obtain reliable results (Bonapersona et al., 2020; Carneiro et al., 2018). “</p><p>We further provide concrete examples how to increase internal validity, external validity and translational validity.</p><p>Guidance on sample size calculations in replications is scarce and a theme that we are currently researching ourselves. We nonetheless now point to recent approaches that were developed mainly for psychology and thus may not transfer easily.</p><p>“Under consideration of a reduction of animals according to the 3R principles (Russell and Burch, 1959), the number of animals tested needs to reflect the current stage in the preclinical trajectory (Sneddon et al., 2017; Strech and Dirnagl, 2019). For replications, this estimation involves consideration of effect sizes from previous studies. A power calculation based on the point estimate of the initial study will often yield too small animal numbers (Albers and Lakens, 2018; Piper et al., 2019). This potentially inflates false negatives, running the risk of missing important effects and wrongfully failing a replication. Alternatives like safeguard power analysis (Perugini et al., 2014), sceptical p-value (Held, 2020), or adjusting for uncertainty (Anderson and Maxwell, 2017) have been proposed mainly for psychological experiments with human subjects. Sample sizes in animal experiments are, however, much lower than estimated by the above methods due to ethical concerns and resource constraints. This may be one reason why such approaches have not yet been implemented widely in preclinical research.“</p><disp-quote content-type="editor-comment"><p>– Please comment on how much evidence (how many replication studies/ different approaches exploring the same question) is needed? And who should collate this body of evidence – researchers, clinicians, funders, a committee?</p></disp-quote><p>This is indeed an interesting and pressing question. Researchers from the emerging field of meta-analysis in preclinical research will be well suited to collate such evidence for example in systematic reviews. Such analyses could in turn form the basis for investigator brochures that are required for early clinical trials. We outline this as one scenario and admit that there may be different routes how to arrive at the conclusion that there is enough data to start a clinical trial.</p><p>“Systematic replication efforts that are decision-enabling are rare in academic preclinical research and have only recently begun to be conducted (Kimmelman et al., 2014). For the decision to finally engage in a clinical trial a systematic review of all experiments is needed. In such a review, evidence should be judged on the validity and reliability criteria discussed here. Ideally, this will form the basis for informative investigator brochures that are currently lacking such decision enabling information (Wieschowski et al., 2018).”</p></body></sub-article></article>