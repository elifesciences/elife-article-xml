<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">79824</article-id><article-id pub-id-type="doi">10.7554/eLife.79824</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Normative decision rules in changing environments</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-278047"><name><surname>Barendregt</surname><given-names>Nicholas W</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3268-9426</contrib-id><email>nicholas.barendregt@colorado.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-17965"><name><surname>Gold</surname><given-names>Joshua I</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6018-0483</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-252277"><name><surname>Josić</surname><given-names>Krešimir</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-1975-3913</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-93959"><name><surname>Kilpatrick</surname><given-names>Zachary P</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2835-9416</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02ttsq026</institution-id><institution>Department of Applied Mathematics, University of Colorado Boulder</institution></institution-wrap><addr-line><named-content content-type="city">Boulder</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00b30xv10</institution-id><institution>Department of Neuroscience, University of Pennsylvania</institution></institution-wrap><addr-line><named-content content-type="city">Philadelphia</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/048sx0r50</institution-id><institution>Department of Mathematics, University of Houston</institution></institution-wrap><addr-line><named-content content-type="city">Houston</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/052gg0110</institution-id><institution>University of Oxford</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>25</day><month>10</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e79824</elocation-id><history><date date-type="received" iso-8601-date="2022-05-03"><day>03</day><month>05</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2022-10-20"><day>20</day><month>10</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at .</event-desc><date date-type="preprint" iso-8601-date="2022-04-29"><day>29</day><month>04</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.04.27.489722"/></event></pub-history><permissions><copyright-statement>© 2022, Barendregt et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Barendregt et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-79824-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-79824-figures-v2.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.08825" id="ra1"/><abstract><p>Models based on normative principles have played a major role in our understanding of how the brain forms decisions. However, these models have typically been derived for simple, stable conditions, and their relevance to decisions formed under more naturalistic, dynamic conditions is unclear. We previously derived a normative decision model in which evidence accumulation is adapted to fluctuations in the evidence-generating process that occur during a single decision (Glaze et al., 2015), but the evolution of commitment rules (e.g. thresholds on the accumulated evidence) under dynamic conditions is not fully understood. Here, we derive a normative model for decisions based on changing contexts, which we define as changes in evidence quality or reward, over the course of a single decision. In these cases, performance (reward rate) is maximized using decision thresholds that respond to and even anticipate these changes, in contrast to the static thresholds used in many decision models. We show that these adaptive thresholds exhibit several distinct temporal motifs that depend on the specific predicted and experienced context changes and that adaptive models perform robustly even when implemented imperfectly (noisily). We further show that decision models with adaptive thresholds outperform those with constant or urgency-gated thresholds in accounting for human response times on a task with time-varying evidence quality and average reward. These results further link normative and neural decision-making while expanding our view of both as dynamic, adaptive processes that update and use expectations to govern both deliberation and commitment.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>How do we make good choices? Should I have cake or yoghurt for breakfast? The strategies we use to make decisions are important not just for our daily lives, but also for learning more about how the brain works.</p><p>Decision-making strategies have two components: first, a deliberation period (when we gather information to determine which choice is ‘best’); and second, a decision ‘rule’ (which tells us when to stop deliberating and commit to a choice). Although deliberation is relatively well-understood, less is known about the decision rules people use, or how those rules produce different outcomes.</p><p>Another issue is that even the simplest decisions must sometimes adapt to a changing world. For example, if it starts raining while you are deciding which route to walk into town, you would probably choose the driest route – even if it did not initially look the best. However, most studies of decision strategies have assumed that the decision-maker’s environment does not change during the decision process.</p><p>In other words, we know much less about the decision rules used in real-life situations, where the environment changes. Barendregt et al. therefore wanted to extend the approaches previously used to study decisions in static environments, to determine which decision rules might be best suited to more realistic environments that change over time.</p><p>First, Barendregt et al. constructed a computer simulation of decision-making with environmental changes built in. These changes were either alterations in the quality of evidence for or against a particular choice, or the ‘reward’ from a choice, i.e., feedback on how good the decision was. They then used the computer simulation to model single decisions where these changes took place.</p><p>These virtual experiments showed that the best performance – for example, the most accurate decisions – resulted when the threshold for moving from deliberation (i.e., considering the evidence) to selecting an option could respond to, or even anticipate, the changing situations. Importantly, the simulations’ results also predicted real-world choices made by human participants when given a decision-making task with similar variations in evidence and reward over time. In other words, the virtual decision-making rules could explain real behavior.</p><p>This study sheds new light on how we make decisions in a changing environment. In the future, Barendregt et al. hope that this will contribute to a broader understanding of decision-making and behavior in a wide range of contexts, from psychology to economics and even ecology.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>decision-making</kwd><kwd>normative modeling</kwd><kwd>dynamic programming</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-MH-115557</award-id><principal-award-recipient><name><surname>Barendregt</surname><given-names>Nicholas W</given-names></name><name><surname>Gold</surname><given-names>Joshua I</given-names></name><name><surname>Josić</surname><given-names>Krešimir</given-names></name><name><surname>Kilpatrick</surname><given-names>Zachary P</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>R01-EB029847-01</award-id><principal-award-recipient><name><surname>Barendregt</surname><given-names>Nicholas W</given-names></name><name><surname>Kilpatrick</surname><given-names>Zachary P</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF-DMS-1853630</award-id><principal-award-recipient><name><surname>Barendregt</surname><given-names>Nicholas W</given-names></name><name><surname>Kilpatrick</surname><given-names>Zachary P</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000001</institution-id><institution>National Science Foundation</institution></institution-wrap></funding-source><award-id>NSF-DBI-1707400</award-id><principal-award-recipient><name><surname>Josić</surname><given-names>Krešimir</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>In environments that fluctuate over the course of deliberation, optimal decision strategies display novel dynamics that can explain human response behaviors better than commonly used alternatives.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Even simple decisions can require us to adapt to a changing world. Should you go through the park or through town on your walk? The answer can depend on conditions that could be changing while you deliberate, such as an unexpected shower that would send you hurrying down the faster route (<xref ref-type="fig" rid="fig1">Figure 1A</xref>) or a predictable sunrise that would nudge you toward the route with better views. Despite the ubiquity of such dynamics in the real world, they are often neglected in models used to understand how the brain makes decisions. For example, many commonly used models assume that decision commitment occurs when the accumulated evidence for an option reaches a fixed, predefined value or threshold (<xref ref-type="bibr" rid="bib60">Wald, 1945</xref>; <xref ref-type="bibr" rid="bib46">Ratcliff, 1978</xref>; <xref ref-type="bibr" rid="bib8">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib32">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib35">Kilpatrick et al., 2019</xref>). The value of this threshold can account for inherent trade-offs between decision speed and accuracy found in many tasks: lower thresholds generate faster, but less accurate decisions, whereas higher thresholds generate slower, but more accurate decisions (<xref ref-type="bibr" rid="bib32">Gold and Shadlen, 2007</xref>; <xref ref-type="bibr" rid="bib15">Chittka et al., 2009</xref>; <xref ref-type="bibr" rid="bib9">Bogacz et al., 2010</xref>). However, these classical models do not adequately describe decisions made in environments with potentially changing contexts (<xref ref-type="bibr" rid="bib54">Thura et al., 2014</xref>; <xref ref-type="bibr" rid="bib55">Thura and Cisek, 2016</xref>; <xref ref-type="bibr" rid="bib42">Palestro et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib53">Thura et al., 2012</xref>; <xref ref-type="bibr" rid="bib51">Tajima et al., 2019</xref>; <xref ref-type="bibr" rid="bib30">Glickman et al., 2022</xref>). Efforts to model decision-making thresholds under dynamic conditions have focused largely on heuristic strategies that aim to account for contexts that change between each decision. For instance, a common class of heuristic models is ‘urgency-gating models’ (UGMs). UGMs filter accumulated evidence through a low-pass filter and use thresholds that collapse monotonically over time (equivalent to dilating the belief in time) to explain decisions based on time-varying evidence quality (<xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib13">Carland et al., 2015</xref>; <xref ref-type="bibr" rid="bib26">Evans et al., 2020</xref>). Although collapsing decision thresholds are optimal in some cases, they do not always account for changes that occur during decision deliberation, and they are sometimes implemented ad-hoc without a proper derivation from first principles. Such derivations typically assume that individuals set decision thresholds to maximize trial-averaged reward rate (<xref ref-type="bibr" rid="bib48">Simen et al., 2009</xref>; <xref ref-type="bibr" rid="bib2">Balci et al., 2011</xref>; <xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Tajima et al., 2016</xref>; <xref ref-type="bibr" rid="bib40">Malhotra et al., 2018</xref>; <xref ref-type="bibr" rid="bib7">Boehm et al., 2020</xref>), which can result in adaptive, time-varying thresholds similar to those assumed by heuristic UGMs. However, as in fixed-threshold models, these time-varying thresholds are typically defined before the evidence is accumulated, preceding the formative stages of the decision, and thus cannot account for environmental changes that may occur during deliberation.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Simple decisions may require complex strategies.</title><p>(<bold>A</bold>) When choosing where to walk, environmental fluctuations (e.g., weather changes) may necessitate changes in decision bounds (black line) adapted to changes in the conditions (cloudy to sunny). (<bold>B</bold>) Schematic of a dynamic programming. By assigning the best action to each moment in time, dynamic programming optimizes trial-averaged reward rate to produce the normative thresholds for a given decision.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig1-v2.tif"/></fig><p>To identify how environmental changes during the course of a single deliberative decision impact optimal decision rules, we developed normative models of decision-making that adapt to and anticipate two specific types of context changes: changes in reward expectation and changes in evidence quality. Specifically, we used Bellman’s equation (<xref ref-type="bibr" rid="bib4">Bellman, 1957</xref>; <xref ref-type="bibr" rid="bib38">Mahadevan, 1996</xref>; <xref ref-type="bibr" rid="bib49">Sutton and Barto, 1998</xref>; <xref ref-type="bibr" rid="bib6">Bertsekas, 2012</xref>; <xref ref-type="bibr" rid="bib23">Drugowitsch, 2015</xref>) to identify decision strategies that maximize trial-averaged reward rate when conditions can change during decision deliberation. We show that for simple tasks that include sudden, expected within-trial changes in the reward or the quality of observed evidence, these normative decision strategies involve non-trivial, time-dependent changes in decision thresholds. These rules take several different forms that outperform their heuristic counterparts, are identifiable from behavior, and have performance that is robust to noisy implementations. We also show that, compared to fixed-threshold models or UGMs, these normative, adaptive threshold models provide a better account of human behavior on a ‘tokens task’, in which the value of commitment changes gradually at predictable times and the quality of evidence changes unpredictably within each trial (<xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib54">Thura et al., 2014</xref>). These results provide new insights into the behavioral relevance of a diverse set of adaptive decision thresholds in dynamic environments and tightly link the details of such environmental changes to threshold adaptations.</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Normative theory for dynamic context 2AFC tasks</title><p>To determine potential deliberation and commitment strategies used by human subjects, we begin by identifying normative decision rules for two-alternative forced choice (2AFC) tasks with dynamic contexts. Normative decision rules that maximize trial-averaged reward rate can be obtained by solving an optimization problem using dynamic programming (<xref ref-type="bibr" rid="bib4">Bellman, 1957</xref>; <xref ref-type="bibr" rid="bib49">Sutton and Barto, 1998</xref>; <xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib50">Tajima et al., 2016</xref>). We define this trial-averaged reward rate, <inline-formula><mml:math id="inf1"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, as (<xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>; <xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>)<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>R</mml:mi><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>C</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow><mml:mrow><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mo>+</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf2"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is the average reward for a decision, <inline-formula><mml:math id="inf3"><mml:msub><mml:mi>T</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula> is the decision time, <inline-formula><mml:math id="inf4"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:msub><mml:mi>T</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:msubsup><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the average total accumulated cost given an incremental cost function <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf6"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is the average trial length, and <inline-formula><mml:math id="inf7"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> is the average inter-trial interval (<xref ref-type="bibr" rid="bib23">Drugowitsch, 2015</xref>). Note that all averages in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> are taken over trials. To find the normative decision thresholds that maximize <inline-formula><mml:math id="inf8"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, we assign specific values (i.e., economic utilities) to correct and incorrect choices (reward and/or punishment) and the time required to arrive at each choice (i.e., evidence cost). The incremental evidence function <inline-formula><mml:math id="inf9"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents both explicit time costs, such as a price for gathering evidence, and implicit costs, such as opportunity cost. While there are many forms of this cost function, we make the simplifying assumption that it is constant, <inline-formula><mml:math id="inf10"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula>. Because more complex cost functions can influence decision threshold dynamics (<xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>), restricting the cost function to a constant ensures that the threshold dynamics we identify are governed purely by changes in the (external) task conditions and not the (internal) cost function. To represent the structure of a 2AFC tasks, we assume a decision environment for an observer with an initially unknown environmental state, <inline-formula><mml:math id="inf11"><mml:mrow><mml:mi>s</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, that uniquely determines which of two alternatives is correct. To infer the environmental state, this observer makes measurements, <inline-formula><mml:math id="inf12"><mml:mi>ξ</mml:mi></mml:math></inline-formula>, that follow a distribution <inline-formula><mml:math id="inf13"><mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>ξ</mml:mi><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mo>±</mml:mo></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that depends on the state. Determining the correct choice is thus equivalent to determining the generating distribution, <inline-formula><mml:math id="inf14"><mml:msub><mml:mi>f</mml:mi><mml:mo>±</mml:mo></mml:msub></mml:math></inline-formula>. An ideal Bayesian observer uses the log-likelihood ratio (LLR), <inline-formula><mml:math id="inf15"><mml:mi>y</mml:mi></mml:math></inline-formula>, to track their ‘belief’ over the correct choice (<xref ref-type="bibr" rid="bib60">Wald, 1945</xref>; <xref ref-type="bibr" rid="bib8">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib59">Veliz-Cuba et al., 2016</xref>). After <inline-formula><mml:math id="inf16"><mml:mi>n</mml:mi></mml:math></inline-formula> discrete observations <inline-formula><mml:math id="inf17"><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that are independent across time, the discrete-time LLR belief <italic>y</italic><sub><italic>n</italic></sub> is given by:<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mo movablelimits="true" form="prefix">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Given this defined task structure, we discretize the time during which the decision is formed and define the observer’s actions during each timestep. The observer gathers evidence (measurements) during each timestep prior to a decision and uses each increment of evidence to update their belief about the correct choice. Then, the observer has the option to either commit to a choice or make another measurement at the next timestep. By assigning a utility to each of these actions (i.e., a value <inline-formula><mml:math id="inf18"><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> for choosing <inline-formula><mml:math id="inf19"><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>, a value <inline-formula><mml:math id="inf20"><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula> for choosing <inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, and a value <inline-formula><mml:math id="inf22"><mml:msub><mml:mi>V</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:math></inline-formula> for sampling again), we can construct the value function for the observer given their current belief:<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>sample again</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For a full derivation of this equation, see Materials and methods. In <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, <inline-formula><mml:math id="inf23"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> is the state likelihood at time <inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf25"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the reward for a correct choice, <inline-formula><mml:math id="inf26"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the reward for an incorrect choice, and <inline-formula><mml:math id="inf27"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> is the timestep between observations. We choose generating distributions to be symmetric Gaussian distributions <inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> to allow us to compute the conditional distribution function <inline-formula><mml:math id="inf29"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> needed for the average future value explicitly:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, <inline-formula><mml:math id="inf30"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the conditional probability of the future state likelihood <inline-formula><mml:math id="inf31"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> given the current state likelihood <inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. For the case of Gaussian-distributed evidence, this conditional probability is given by <xref ref-type="disp-formula" rid="equ22">Equation 16</xref> in Materials and methods. Using <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, we find the specific belief values where the optimal action changes from gathering evidence to commitment, defining thresholds on the ideal observer’s belief that trigger decisions. <xref ref-type="fig" rid="fig1">Figure 1B</xref> shows a schematic of this process.</p><p>To understand how normative decision thresholds adapt to changing conditions, we derived them for several different forms of two-alternative forced-choice (2AFC) tasks in which we controlled changes in evidence or reward. Even for such simple tasks, there is a broad set of possible changing contexts. In the next section, we analyze a task in which context changes gradually (the tokens task). Here, we focus on tasks in which the context changes abruptly. For each task, an ideal observer was shown evidence generated from a Gaussian distribution <inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> with signal-to-noise ratio (SNR) <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). The SNR measures evidence quality: a smaller (larger) <inline-formula><mml:math id="inf35"><mml:mi>m</mml:mi></mml:math></inline-formula> implies that evidence is of lower (higher) quality, resulting in harder (easier) decisions. The observer’s goal was to determine which of the two means (i.e., which distribution, <inline-formula><mml:math id="inf36"><mml:msub><mml:mi>f</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf37"><mml:msub><mml:mi>f</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula>) were used to generate the observations. We introduced changes in the reward for a correct decision (‘reward-change task’) or the SNR (‘SNR-change task’) within a single decision, where the time and magnitude of the changes are known in advance to the observer (<xref ref-type="fig" rid="fig1">Figure 1A</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). For example, changes in SNR arise naturally throughout a day as animals choose when to forage and hunt given variations in light levels and therefore target-acquisition difficulty (<xref ref-type="bibr" rid="bib17">Combes et al., 2012</xref>; <xref ref-type="bibr" rid="bib24">Einfalt et al., 2012</xref>).</p><p>Under these dynamic conditions, dynamic programming produces normative thresholds with rich non-monotonic dynamics (<xref ref-type="fig" rid="fig2">Figure 2A and B</xref>, <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Environments with multiple reward changes during a single decision lead to complex threshold dynamics that we summarize in terms of several threshold change “motifs.” These motifs occur on shorter intervals and tend to emerge from simple monotonic changes in context parameters (<xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). To better understand the range of possible threshold motifs, we focused on environments with single changes in task parameters. For the reward-change task, we set punishment <inline-formula><mml:math id="inf38"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and assumed reward <inline-formula><mml:math id="inf39"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> changes abruptly, so that its dynamics are described by a Heaviside function:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Thus, the reward switches from the pre-change reward <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to the post-change reward <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> at <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Normative decision rules are characterized by non-monotonic task-dependent motifs.</title><p>(<bold>A,B</bold>) Example reward time series for a reward-change task (black lines in A), with corresponding thresholds found by dynamic programming (black lines in B). The colored lines in B show sample realizations of the observer’s belief. (<bold>C</bold>) To understand the diversity of threshold dynamics, we consider the simple case of a single change in the reward schedule. The panel shows a colormap of normative threshold dynamics for these conditions. Distinct threshold motifs are color-coded, corresponding to examples shown in panels i-v. (<bold>i-v</bold>): Representative thresholds (top) and empirical response distributions (bottom) from each region in C. During times at which thresholds in the upper panels are not shown (e.g., <inline-formula><mml:math id="inf43"><mml:mrow><mml:mi>t</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in i), the thresholds are infinite and the observer will never respond. For all simulations, we take the incremental cost function <inline-formula><mml:math id="inf44"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, punishment <inline-formula><mml:math id="inf45"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, evidence quality <inline-formula><mml:math id="inf46"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, and inter-trial interval <inline-formula><mml:math id="inf47"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig2-v2.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Impact of evidence quality on belief and difficulty.</title><p>Evidence quality impacts observation distinguishability and task difficulty. (<bold>A</bold>) Likelihood functions for environmental states (i.e., possible choices) <inline-formula><mml:math id="inf48"><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> (blue) and <inline-formula><mml:math id="inf49"><mml:msub><mml:mi>s</mml:mi><mml:mo>-</mml:mo></mml:msub></mml:math></inline-formula> (red) in a low evidence quality task (<inline-formula><mml:math id="inf50"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>), where we define <inline-formula><mml:math id="inf51"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></inline-formula>, a scaled signal-to-noise ratio, as the evidence quality. (<bold>B</bold>) Observer belief (LLR of accumulated evidence) in a low evidence quality task, with several belief realizations superimposed. <bold>C,D</bold>: Same as A and B, but for a high evidence quality task (<inline-formula><mml:math id="inf52"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:mrow></mml:math></inline-formula>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig2-figsupp1-v2.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Normative thresholds for reward-change task with multiple changes.</title><p>Normative decision thresholds exhibit multiple motifs for multiple reward changes. (<bold>A,B</bold>) Example reward time series for a reward-change task (black lines in <bold>A</bold>), with corresponding thresholds found by dynamic programming (black lines in <bold>B</bold>). The colored lines in B show sample realizations of the observer’s belief. Similar changes in reward (boxed regions) produce similar motifs in threshold dynamics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig2-figsupp2-v2.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Threshold dynamics in the inferred reward-change task.</title><p>Threshold dynamics in the inferred reward change task track piecewise constant dynamics. (<bold>A</bold>) Markov process governing reward states with rewards <inline-formula><mml:math id="inf53"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>≤</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and symmetric transition (hazard) rate <inline-formula><mml:math id="inf54"><mml:mi>h</mml:mi></mml:math></inline-formula> between states. (<bold>B</bold>) Change point-triggered average of normative thresholds for a high-to-low reward change. Several values of reward inference difficulty <inline-formula><mml:math id="inf55"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:math></inline-formula> are superimposed (legend). Colored dotted line corresponds to the thresholds for an infinite <inline-formula><mml:math id="inf56"><mml:msub><mml:mi>m</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:math></inline-formula> task, and vertical black dashed line shows the time of the reward switch. (<bold>C</bold>) Same as B, but for a low-to-high reward change. For both B and C, the rewards are,<inline-formula><mml:math id="inf57"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>10</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> the state evidence quality is,<inline-formula><mml:math id="inf58"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> and the hazard rate is <inline-formula><mml:math id="inf59"><mml:mrow><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig2-figsupp3-v2.tif"/></fig></fig-group><p>For this single-change task, normative threshold dynamics exhibited several motifs that in some cases resembled fixed or collapsing thresholds characteristic of previous decision models but in other cases exhibited novel dynamics. Specifically, we characterized five different dynamic motifs in response to single, expected changes in reward contingencies for different combinations of pre- and post-change reward values (<xref ref-type="fig" rid="fig2">Figure 2C and i–v</xref>). For tasks in which reward is initially very low, thresholds are infinite until the reward increases, ensuring that the observer waits for the larger payout regardless of how strong their belief is (<xref ref-type="fig" rid="fig2">Figure 2i</xref>). The region where thresholds are infinite corresponds to when <inline-formula><mml:math id="inf60"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, which is the value associated with waiting to gather more information, is maximal for all values of <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. In contrast, when reward is initially very high, thresholds collapse to zero just before the reward decreases, ensuring that all responses occur while payout is high (<xref ref-type="fig" rid="fig2">Figure 2v</xref>). Between these two extremes, optimal thresholds exhibit rich, non-monotonic dynamics (<xref ref-type="fig" rid="fig2">Figure 2ii,iv</xref>), promoting early decisions in the high-reward regime, or preventing early, inaccurate decisions in the low-reward regime. <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows the regions in pre- and post-change reward space where each motif is optimal, including broad regions with non-monotonic thresholds. Thus, even simple context dynamics can evoke complex decision strategies in ideal observers that differ from those predicted by constant decision-thresholds and heuristic UGMs.</p><p>We also formulated an ‘inferred reward-change task’, in which reward fluctuates between a high value <inline-formula><mml:math id="inf62"><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:math></inline-formula> and low value <inline-formula><mml:math id="inf63"><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> governed by a two-state Markov process with known transition rate <inline-formula><mml:math id="inf64"><mml:mi>h</mml:mi></mml:math></inline-formula> and state <inline-formula><mml:math id="inf65"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> that the observer must infer on-line. For this task, the observer receives two independent sets of evidence: the evidence of the state <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> and the evidence of the current reward <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The observer must then track their beliefs about both the state and the current reward and take both sources of information into account when determining the optimal decision thresholds. We found that these thresholds always changed monotonically with monotonic shifts in expected reward (see <xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). These results contrast with our findings from the reward-change task in which changes can be anticipated and monotonic changes in reward can produce non-monotonic changes in decision thresholds.</p><p>For the SNR-change task, optimal strategies for environments with multiple changes in evidence quality are characterized by threshold dynamics that adapt to these changes in a way similar to how they adapt to changes in reward (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). To study the range of possible threshold motifs, we again considered environments with single changes in the evidence quality <inline-formula><mml:math id="inf68"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></inline-formula> by taking µ to be a Heaviside function:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>For this single-change task, we again found similar threshold motifs to those in the reward-change task (<xref ref-type="fig" rid="fig3">Figure 3A and B</xref>). However, in this case monotonic changes in evidence quality always produce monotonic changes in response behavior. This observation holds across all of parameter space for evidence-quality schedules with single change points (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), with only three optimal behavioral motifs (<xref ref-type="fig" rid="fig3">Figure 3i–iii</xref>). This contrasts with our findings in the reward-change task, where monotonic changes in reward can produce non-monotonic changes in decision thresholds. Strategies arising from known dynamical changes in context tend to produce sharper response distributions around reward changes than around quality changes, which may be measurable in psychophysical studies. These findings suggest that changes in reward can have a larger impact on the normative strategy thresholds than changes in evidence quality.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Dynamic-quality task does not exhibit non-monotonic motifs.</title><p>(<bold>A,B</bold>) Example quality time series for the SNR-change task (<bold>A</bold>), with corresponding thresholds found by dynamic programming (<bold>B</bold>). Colored lines in B show sample realizations of the observer’s belief. As in <xref ref-type="fig" rid="fig2">Figure 2</xref>, we characterize motifs in the threshold dynamics and response distributions based on single changes in SNR. (<bold>C</bold>) Colormap of normative threshold dynamics for a known reward schedule task with a single quality change. Distinct dynamics are color-coded, corresponding to examples shown in panels i-iii. (<bold>i-iii</bold>): Representative thresholds (top) and empirical response distributions (bottom) from each region in C. For all simulations, we take the incremental cost function <inline-formula><mml:math id="inf69"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, reward <inline-formula><mml:math id="inf70"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>, punishment <inline-formula><mml:math id="inf71"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and inter-trial interval <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Normative thresholds for SNR-change task with multiple changes.</title><p>Normative decision thresholds change monotonically when anticipating SNR changes. A,B: Example quality time series for the SNR-change task with multiple changes (<bold>A</bold>), with corresponding thresholds found by dynamic programming (<bold>B</bold>). Colored lines in (<bold>B</bold>) show sample realizations of the observer’s belief. Similar changes in quality (boxed regions) produce similar motifs in threshold dynamics.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig3-figsupp1-v2.tif"/></fig></fig-group><sec id="s2-1-1"><title>Performance and robustness of non-monotonic normative thresholds</title><p>The normative solutions that we derived for dynamic-context tasks by definition maximize reward rate. This maximization assumes that the normative solutions are implemented perfectly. However, a perfect implementation may not be possible, given the complexity of the underlying computations, biological constraints on computation time and energy (<xref ref-type="bibr" rid="bib36">Louie et al., 2015</xref>), and the synaptic and neural variability of cortical circuits (<xref ref-type="bibr" rid="bib37">Ma and Jazayeri, 2014</xref>; <xref ref-type="bibr" rid="bib27">Faisal et al., 2008</xref>). Given these constraints, subjects may employ heuristic strategies like the UGM over the normative model if noisy or mistuned versions of both models result in similar reward rates. We used synthetic data to better understand the relative benefits of different imperfectly implemented strategies. Specifically, we corrupted the internal belief state and simulated response times with additive Gaussian noise with zero mean and variance <inline-formula><mml:math id="inf73"><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula> (See <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>) for three models:</p><list list-type="order"><list-item><p>The continuous-time normative model with time-varying thresholds <inline-formula><mml:math id="inf74"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and belief that evolves according to the stochastic differential equation<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mo>±</mml:mo><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>drift</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>sample noise</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>sensory noise</mml:mtext></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a standard increment of a Wiener process, the sign of the drift <inline-formula><mml:math id="inf76"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>m</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is given by the correct choice <inline-formula><mml:math id="inf77"><mml:msub><mml:mi>s</mml:mi><mml:mo>±</mml:mo></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is an independent Wiener process with strength <inline-formula><mml:math id="inf79"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>. The addition of the additional noise process <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> makes this a noisy Bayesian (NB) model.</p></list-item><list-item><p>A constant-threshold (Const) model, which uses the same belief <inline-formula><mml:math id="inf81"><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> as the normative model but a constant, non-adaptive decision threshold <inline-formula><mml:math id="inf82"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1A</xref>).</p></list-item><list-item><p>The UGM, which uses the output of a low-pass filter as the belief,<disp-formula id="equ8"><label>(7)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>drift \§amp; sample noise</mml:mtext></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:mrow><mml:munder><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>⏟</mml:mo></mml:munder></mml:mrow><mml:mrow><mml:mtext>sensory noise</mml:mtext></mml:mrow></mml:munder><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and commits to a decision when this output crosses a hyperbolically collapsing threshold <inline-formula><mml:math id="inf83"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mfrac><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1B</xref>). In <xref ref-type="disp-formula" rid="equ8">Equation 7</xref>, <inline-formula><mml:math id="inf84"><mml:mi>E</mml:mi></mml:math></inline-formula> is the filter’s output that serves as the UGM’s belief, <inline-formula><mml:math id="inf85"><mml:mi>τ</mml:mi></mml:math></inline-formula> is a relaxation time constant, and the optimal observer’s belief <inline-formula><mml:math id="inf86"><mml:mi>y</mml:mi></mml:math></inline-formula> is the filter’s input. Note that the filter’s input can also be written in terms of the state likelihood <inline-formula><mml:math id="inf87"><mml:mi>p</mml:mi></mml:math></inline-formula>,<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mi>p</mml:mi><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>which is the form first proposed by <xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>.</p></list-item></list><p>For more details about these three models, see Materials and methods. We compared their performance in terms of reward rate achieved on the same set of reward-change tasks shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. To ensure the average total reward in each trial was the same, we restricted the pre-change reward <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and post-change reward <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> so that <inline-formula><mml:math id="inf90"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>When all three models were implemented without additional noise, the relative benefits of the normative model depended on the exact task condition. The performance differential between models was highest when reward changed from low to high values (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, dotted line; <xref ref-type="fig" rid="fig4">Figure 4</xref>). Under these conditions, normative thresholds are initially infinite and become finite after the reward increases, ensuring that most responses occur immediately once the high reward becomes available (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). In contrast, response times generated by the constant-threshold and UGM models tend to not follow this pattern. For the constant-threshold model, many responses occur early, when the reward is low (<xref ref-type="fig" rid="fig4">Figure 4E</xref>). For the UGM, a substantial fraction of responses are late, leading to higher time costs however, it is possible to tune the UGM’s thresholds rate of collapse to prevent any early responses while the reward is low (<xref ref-type="fig" rid="fig4">Figure 4F</xref>). In contrast, when the reward changes from high to low values, all models exhibit similar response distributions and reward rates (<xref ref-type="fig" rid="fig4">Figure 4A</xref>, dashed line; <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). This result is not surprising, given that the constant-threshold model produces early peaks in the reaction time distribution, and the UGM was designed to mimic collapsing bounds that hasten decisions in response to imminent decreases in reward (<xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>). We therefore focused on the robustness of each strategy when corrupted by noise and responding to low-to-high reward switches – the regime differentiating strategy performance in ways that could be identified in subject behavior.</p><p>Adding noise to the internal belief state (which tends to trigger earlier responses) and simulated response distributions (which tends to smooth out the distributions) without re-tuning the models to account for the additional noise does not alter the advantage of the normative model: across a range of added noise strengths, which we define as <inline-formula><mml:math id="inf91"><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:math></inline-formula>, where <inline-formula><mml:math id="inf92"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf93"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are the maximum possible strengths of sensory and motor noise, respectively, the normative model outperforms the other two when encountering low-to-high reward switches (<xref ref-type="fig" rid="fig4">Figure 4C</xref>). This robustness arises because, prior to the reward change, the normative model uses infinite decision thresholds that prevent early noise-triggered responses when reward is low (<xref ref-type="fig" rid="fig4">Figure 4D</xref>). In contrast, the heuristic models have finite collapsing or constant thresholds and thus produce more suboptimal early responses as belief noise is increased (<xref ref-type="fig" rid="fig4">Figure 4E and F</xref>). Thus, adaptive decision strategies can result in considerably higher reward rates than heuristic alternatives even when implemented imperfectly, suggesting subjects may be motivated to learn such strategies.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Benefits of adaptive normative thresholds compared to heuristics.</title><p>(<bold>A</bold>) Reward rate <inline-formula><mml:math id="inf94"><mml:mi>ρ</mml:mi></mml:math></inline-formula> for the noisy Bayesian (NB) model, constant-threshold (Const) model, and UGM for the reward-change task, where all models are tuned to maximize performance with zero sensory noise (<inline-formula><mml:math id="inf95"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>) and zero motor noise (<inline-formula><mml:math id="inf96"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>); in this case, the NB model is equivalent to the optimal normative model. Model reward rates are shown for different pre-change rewards <inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, with post-change reward <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> set so that <inline-formula><mml:math id="inf99"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math></inline-formula> to keep the average total reward fixed (see Materials and methods for details). Low-to-high reward changes (dotted line) produce larger performance differentials than high-to-low changes (dashed line). (<bold>B</bold>) Absolute reward rate differential between NB and alternative models, given by <inline-formula><mml:math id="inf100"><mml:mrow><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>NB</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mtext>alt</mml:mtext><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> for different pre-change rewards. Legend shows which alternate model was used to produce each curve. (<bold>C</bold>) Reward rates of all models for reward-change task with <inline-formula><mml:math id="inf101"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> as both observation and response-time noise is increased. Noise strength for each model is given by <inline-formula><mml:math id="inf102"><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mover accent="true"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>¯</mml:mo></mml:mover></mml:mrow></mml:mfrac></mml:math></inline-formula>, are <inline-formula><mml:math id="inf103"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf104"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> were the maximum strengths of <inline-formula><mml:math id="inf105"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf106"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> we considered (See <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref> for reference). Filled markers correspond to no noise, moderate noise, and high noise strengths. D,E,F: Response distributions for (<bold>D</bold>) NB; (<bold>E</bold>) Const; and (<bold>F</bold>) UGM models in a low-to-high reward environment with <inline-formula><mml:math id="inf107"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>3</mml:mn><mml:mo>,</mml:mo><mml:mn>8</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In each panel, results derived for several noise strengths, corresponding with filled markers in C, are superimposed, with lighter distributions denoting higher noise. Inset in D shows normative thresholds obtained from dynamic programming. Dashed line shows time of reward increase. For all simulations, we take the incremental cost function <inline-formula><mml:math id="inf108"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, punishment <inline-formula><mml:math id="inf109"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, and evidence quality <inline-formula><mml:math id="inf110"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Heuristic model and noise schematics.</title><p>Heuristic observer models and noise filters. (<bold>A</bold>) Constant threshold (Const) model in belief-space (left), with associated RT distributions (right). Several versions of model are superimposed (gradient). (<bold>B</bold>) Threshold dynamics of the UGM (left), with associated RT distributions (right). The UGM uses a low-pass filtered version of the normative LLR as its belief and dilates this belief linearly in time; this is equivalent to hyperbolically-collapsing decision thresholds (see Materials and methods). (<bold>C</bold>) Schematic of noise generation processes, describing possible sources of performance decreases. Gaussian noise with standard deviation <inline-formula><mml:math id="inf111"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> is added to belief (red) and with standard deviation <inline-formula><mml:math id="inf112"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (motor noise) added to response times (orange).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig4-figsupp1-v2.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Model performance for high-to-low reward switch.</title><p>All models perform similarly in high-to-low reward switches. (<bold>A</bold>) Reward rates of all models for low-to-high reward changes, with <inline-formula><mml:math id="inf113"><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>8</mml:mn><mml:mo>,</mml:mo><mml:mn>3</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, (dashed line in <xref ref-type="fig" rid="fig4">Figure 4A</xref>) at different noise strengths. Filled markers correspond to no noise, moderate noise, and high noise strengths. (<bold>B,C,D</bold>) Response distribution for (<bold>B</bold>) NB, with inset showing normative thresholds obtained from dynamic programming; (<bold>C</bold>) Const; and (<bold>D</bold>) UGM model in a high-to-low reward environment; several noise strengths, corresponding to filled markers in A, are superimposed, with lighter distributions denoting higher noise.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig4-figsupp2-v2.tif"/></fig><fig id="fig4s3" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 3.</label><caption><title>Model performance for decomposed noise strengths.</title><p>Effects of belief and motor noise on model performance. (<bold>A</bold>) Reward rates of NB model (left), Const model (center), and UGM (right) for different values of belief and motor noise strengths in a low-to-high reward switch. Increasing belief noise strength causes performance to decrease substantially, while increasing motor noise strength has little effect on performance. To better visualize performance decreases, we take a slice through the performance surface at a fixed motor noise strength (arrow label in far left panel). (<bold>B</bold>) Reward rates of each model for different values of belief noise strength and motor noise strength fixed at 0.125 (arrow label in A). (<bold>C,D</bold>) Same as A,B, but for a high-to-low reward switch.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig4-figsupp3-v2.tif"/></fig></fig-group></sec></sec><sec id="s2-2"><title>Adaptive normative strategies in the tokens task</title><p>To determine the relevance of the normative model to human decision-making, we analyzed previously collected data from a ‘tokens task’ (<xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>). For this task, human subjects were shown 15 tokens inside a center target flanked by two empty targets (see <xref ref-type="fig" rid="fig5">Figure 5A</xref> for a schematic). Every 200ms, a token moved from the center target to one of the neighboring targets with equal probability. Subjects were tasked with predicting which flanking target would contain more tokens by the time all 15 moved from the center. Subjects could respond at any time before all 15 tokens had moved. Once the subject made the prediction, the remaining tokens would finish their movements to indicate the correct alternative. Given this task structure, one can show using a combinatorial argument (<xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>) that the state likelihood function <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mtext>top</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, the probability the top target will hold more tokens at the end of the trial, is given by:<disp-formula id="equ10"><label>(8)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>top</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>!</mml:mo></mml:mrow><mml:msup><mml:mn>2</mml:mn><mml:mrow><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mfrac><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mo movablelimits="true" form="prefix">min</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mrow></mml:munderover><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>k</mml:mi><mml:mo>!</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>!</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf115"><mml:msub><mml:mi>U</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf116"><mml:msub><mml:mi>L</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf117"><mml:msub><mml:mi>C</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> are the number of tokens in the upper, lower, and center targets after token movement <inline-formula><mml:math id="inf118"><mml:mi>n</mml:mi></mml:math></inline-formula>, respectively. The token movements are Markovian because each token has an equal chance of moving to the upper/lower target. However, the probability that a target will contain more tokens at the end of the trial is history dependent, and the evolution of these probabilities is thus non-Markovian. As such, the quality of evidence possible from each token draw changes dynamically and gradually. In addition, the task included two different post-decision token movement speeds, ‘slow’ and ‘fast’: once the subject committed to a choice, the tokens finished out their animation, moving either once every 170ms (slow task) or once every 20ms (fast task). This post-decision movement acceleration changed the value associated with commitment by making the average inter-trial interval (<inline-formula><mml:math id="inf119"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>) decrease over time. Because of this modulation, we can interpret the tokens task as a multi-change reward task, where commitment value is controlled through <inline-formula><mml:math id="inf120"><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:math></inline-formula> rather than through reward <inline-formula><mml:math id="inf121"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>. Our dynamic-programming framework for generating adaptive decision rules can handle the gradual changes in task context emerging in the tokens task. Given that costs and rewards can be subjective, we quantified how normative decision thresholds change with different combinations of rewards <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> and costs <inline-formula><mml:math id="inf123"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> for fixed punishment <inline-formula><mml:math id="inf124"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, for both the slow (<xref ref-type="fig" rid="fig5">Figure 5B</xref>) and fast (<xref ref-type="fig" rid="fig5">Figure 5C</xref>) versions of the task.</p><fig-group><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Normative strategies for the tokens task exhibit various distinct decision threshold motifs with sharp, non-monotonic changes.</title><p>(<bold>A</bold>) Schematic of the tokens task. The subject must predict which target (top or bottom) will have the most tokens once all tokens have left the center target (see text for details). (<bold>B</bold>) Colormap of normative threshold dynamics for the ‘slow’ version of the tokens task in reward-evidence cost parameter space (i.e., as a function of <inline-formula><mml:math id="inf125"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf126"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> from <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>, with punishment <inline-formula><mml:math id="inf127"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> set to –1). Distinct dynamics are color-coded, with different motifs shown in i-iv. (<bold>C</bold>) Same as B, but for the ‘fast’ version of the tokens task. (<bold>i-iv</bold>): Representative thresholds (top) and empirical response distributions (bottom) from each region in (<bold>B,C</bold>). Thresholds are plotted in the LLR-belief space <inline-formula><mml:math id="inf128"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is the state likelihood given by <xref ref-type="disp-formula" rid="equ10">Equation 8</xref>. Note that we distinguish iii and iv by the presence of either one (iii) or multiple (iv) consecutive threshold increases. In regions where thresholds are not displayed (e.g., <inline-formula><mml:math id="inf130"><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mi mathvariant="normal">…</mml:mi><mml:mo>,</mml:mo><mml:mn>7</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in ii), the thresholds are infinite.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig5-v2.tif"/></fig><fig id="fig5s1" position="float" specific-use="child-fig"><label>Figure 5—figure supplement 1.</label><caption><title>Tokens task thresholds in token lead space.</title><p>Normative thresholds for tokens task plotted in token lead space. <bold>i-iv</bold>: Same as <xref ref-type="fig" rid="fig5">Figure 5i-iv</xref>, but plotted in “token lead” space instead of LLR space. Here, thresholds are measured as the number of tokens the top target must be ahead of the bottom target to commit to a decision. In this space, non-monotonicity of thresholds in iii and iv is more apparent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig5-figsupp1-v2.tif"/></fig></fig-group><p>We identified four distinct motifs of normative decision threshold dynamics for the tokens task (<xref ref-type="fig" rid="fig5">Figure 5i-iv</xref>). Some combinations of rewards and costs produced collapsing thresholds (<xref ref-type="fig" rid="fig5">Figure 5ii</xref>) similar to the UGM developed by <xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref> for this task. In contrast, large regions of task parameter space produced rich non-monotonic threshold dynamics (<xref ref-type="fig" rid="fig5">Figure 5iii,iv</xref>) that differed from any found in the UGM. In particular, as in the case of reward-change tasks, normative thresholds were often infinite for the first several token movements, preventing early and weakly informed responses. These motifs are similar to those produced by low-to-high reward switches in the reward-change task, but here resulting from the low relative cost of early observations. These non-monotonic dynamics also appear if we measure belief in terms of the difference in tokens between the top and bottom target, which we call ‘token lead space’ (see <xref ref-type="fig" rid="fig5s1">Figure 5—figure supplement 1</xref>).</p><sec id="s2-2-1"><title>Adaptive normative strategies best fit subject response data</title><p>To determine the relevance of these adaptive decision strategies to human behavior, we fit discrete-time versions of the noisy Bayesian (four free parameters), constant-threshold (three free parameters), and urgency-gating (five free parameters) models to response-time data from the tokens task collected by <xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>; see Table 1 in Materials and methods for a table of parameters for each model. All models included belief and motor noise, as in our analysis of the dynamic-context tasks (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1C</xref>). The normative model tended to fit the data better than the heuristic models (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), based on three primary analyses. First, both corrected AIC (AICc), which accounts for goodness-of-fit and model degrees-of-freedom, and average root-mean-squared error (RMSE) between the predicted and actual trial-by-trial response times, favored the noisy Bayesian model for most subjects for both the slow (<xref ref-type="fig" rid="fig6">Figure 6A</xref>) and fast (<xref ref-type="fig" rid="fig6">Figure 6D</xref>) versions of the task. Second, when considering only the best-fitting model for each subject and task condition, the noisy Bayesian model tended to better predict subject’s response times (<xref ref-type="fig" rid="fig6">Figure 6B and E</xref>). Third, most subjects whose data were best described by the noisy Bayesian model had best-fit parameters that corresponded to non-monotonic decision thresholds, which cannot be produced by either of the other two models (<xref ref-type="fig" rid="fig6">Figure 6C and F</xref>). This result also shows that, assuming subjects used a normative model, they used distinct model parameters, and thus different strategies, for both the fast and slow task conditions. This finding is clearer when looking at the posterior parameter distribution for each subject and model parameter (see <xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref> for an example). We speculate that the higher estimated value of reward in the slow task may arise due to subjects valuing frequent rewards more favorably. Together, our results strongly suggest that these human subjects tended to use an adaptive, normative strategy instead of the kinds of heuristic strategies often used to model response data from dynamic context tasks.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Adaptive normative strategies provide the best fit to subject behavior in the tokens task.</title><p>(<bold>A</bold>) Number of subjects from the slow version of the tokens task whose reponses were best described by each model (legend) identified using corrected AIC (left) and average trial-by-trial RMSE (right). (<bold>B</bold>) Comparison of mean RT from subject data in the slow version of the tokens task (<inline-formula><mml:math id="inf131"><mml:mi>x</mml:mi></mml:math></inline-formula>-axis) to mean RT of each fit model (<inline-formula><mml:math id="inf132"><mml:mi>y</mml:mi></mml:math></inline-formula>-axis) at maximum-likelihood parameters. Each symbol is color-coded to agree with its associated model. Darker symbols correspond to the model that best describes the responses of a subject selected using corrected AIC. The NB model had the lowest variance in the difference between predicted and measured mean RT (NB var: 0.13, Const var: 3.11, UGM var: 5.39). (<bold>C</bold>) Scatter plot of maximum-likelihood parameters <inline-formula><mml:math id="inf133"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf134"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:math></inline-formula> for the noisy Bayesian model for each subject in the slow version of the task. Each symbol is color-coded to match the threshold dynamics heatmap from <xref ref-type="fig" rid="fig5">Figure 5B</xref>. Darker symbols correspond to subjects whose responses were best described by the noisy Bayesian model using corrected AIC. (<bold>D-F</bold>) Same as A-C, but for the fast version of the tokens task. The NB model had the lowest variance in the difference between predicted and measured mean RT in this version of the task (NB var: 0.22, Const var: 0.82, UGM var: 5.32).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig6-v2.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 1.</label><caption><title>Summary of model fits.</title><p>Best- and worst-quality fits for each model. (<bold>A</bold>) Best fits, measured using AICc, for the NB model (left), Const model (center), and UGM (right). Black trace shows subject data, and colored trace shows the maximum-likelihood model fit. Each plot shows the Kullback-Leibeler (KL) divergence between the subject data and the fitted response distribution (see Materials and methods for details). (<bold>B</bold>) Same as A, but for the worst fits for each model.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig6-figsupp1-v2.tif"/></fig><fig id="fig6s2" position="float" specific-use="child-fig"><label>Figure 6—figure supplement 2.</label><caption><title>Fitted posterior for each model parameter for NB model.</title><p>Example NB posterior for subject tokens task data. (<bold>A–D</bold>) Fitted posterior from the NB model for reward (<bold>A</bold>), cost (<bold>B</bold>), sensory noise (<bold>C</bold>), and motor noise (<bold>D</bold>) for a single representative subject. The top row (red) shows the model posterior for slow task, whereas the bottom row (blue) shows the model posterior for the fast task. Note the different scales for the same parameter in different versions of the task. Shaded regions show the approximate 95% credible interval for each marginal posterior. Vertical dashed lines show the MLE values for each parameter, calculated from the full joint posterior.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-79824-fig6-figsupp2-v2.tif"/></fig></fig-group></sec></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The goal of this study was to build on previous work showing that in dynamic environments, the most effective decision processes do not necessarily use relatively simple, pre-defined computations as in many decision models (<xref ref-type="bibr" rid="bib8">Bogacz et al., 2006</xref>; <xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>; <xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>), but instead adapt to learned or predicted features of the environmental dynamics (<xref ref-type="bibr" rid="bib21">Drugowitsch et al., 2014a</xref>). Specifically, we used new ‘dynamic context’ task structures to demonstrate that normative decision commitment rules (i.e., decision thresholds, or bounds, in ‘accumulate-to-bound’ models) adapt to reward and evidence-quality switches in complex, but predictable, ways. Comparing the performance of these normative decision strategies to the performance of classic heuristic models, we found that the advantage of normative models is maintained when computations are noisy. We extended these modeling results to include the ‘tokens task’, in which evidence quality changes in a way that depends on stimulus history and the utility of commitment increases over time. We found that the normative decision thresholds for the tokens task are also non-monotonic and robust to noise. By reanalyzing human subject data from this task, we found most subjects’ response times were best-explained by a noisy normative model with non-monotonic decision thresholds. Taken collectively, these results show that ideal observers and human subjects use adaptive and robust normative decision strategies in relatively simple decision environments.</p><p>Our results can aid experimentalists investigating the nuances of complex decision-making in several ways. First, we demonstrated that normative behavior varies substantially across task parameters for relatively simple tasks. For example, the reward-change task structure produces five distinct behavioral motifs, such as waiting until reward increases (<xref ref-type="fig" rid="fig2">Figure 2i</xref>) and responding before reward decreases unless the accumulated evidence is ambiguous (<xref ref-type="fig" rid="fig2">Figure 2iv</xref>). Using these kinds of modeling results to inform experimental design can help us understand the possible behaviors to expect in subject data. Second, extending our work and considering the sensitivity of performance to both model choice and task parameters (<xref ref-type="bibr" rid="bib3">Barendregt et al., 2019</xref>; <xref ref-type="bibr" rid="bib44">Radillo et al., 2019</xref>) will help to identify regions of task parameter space where models are most identifiable from observables like response time and choice. Third, and more generally, our work provides evidence that for tasks with gradual changes in evidence quality and reward, human behavior is more consistent with normative principles than with previously proposed heuristic models. However, more work is needed to determine if and how people follow normative principles for other dynamic-context tasks, such as those involving abrupt changes in evidence or reward contingencies, by using normative theory to determine which subject strategies are plausible, the nature of tasks needed to identify them, and the relationship between task dynamics and decision rules.</p><p>Model-driven experimental design can aid in identification of adaptive decision rules in practice. People commonly encounter unpredictable (e.g. an abrupt thunderstorm) and predictable (e.g. sunset) context changes when making decisions. Natural extensions of common perceptual decision tasks (e.g. random-dot motion discrimination [<xref ref-type="bibr" rid="bib31">Gold and Shadlen, 2002</xref>]) could include within-trial changes in stimulus signal-to-noise ratio (evidence quality) or anticipated reward payout. Task-relevant variability can also arise from internal sources, including noise in neural processing of sensory input and motor output (<xref ref-type="bibr" rid="bib37">Ma and Jazayeri, 2014</xref>; <xref ref-type="bibr" rid="bib27">Faisal et al., 2008</xref>). We assumed subjects do not have precise knowledge of the strength or nature of these noise sources, and thus they could not optimize their strategy accordingly. However, people may be capable of rapidly estimating performance error that results from such internal noise processes and adjusting on-line (<xref ref-type="bibr" rid="bib10">Bonnen et al., 2015</xref>). To extend the models we considered, we could therefore assume that subjects can estimate the magnitude of their own sensory and motor noise, and use this information to adapt their decision strategies to improve performance.</p><p>Real subjects likely do not rely on a single strategy when performing a sequence of trials (<xref ref-type="bibr" rid="bib1">Ashwood et al., 2022</xref>) and instead rely on a mix of near-normative, sub-normative, and heuristic strategies. In fitting subject data, experimentalists are thus presented with the difficult task of constructing a library of possible models to use in their analysis. More general approaches have been developed for fitting response data to a broad class of models (<xref ref-type="bibr" rid="bib47">Shinn et al., 2020</xref>), but these model libraries are typically built on pre-existing assumptions of how subjects accumulate evidence and make decisions. Because the potential library of decision strategies is theoretically limitless, a normative analyses can both expand and provide insights into the range of possible subject behaviors in a systematic and principled way. Understanding this scope will assist in developing a well-groomed candidate list of near-normative and heuristic models. For example, if a normative analysis of performance on a dynamic reward task produces threshold dynamics similar to those in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, then the fitting library should include a piecewise-constant threshold (or urgency signal) model. Combining these model-based investigations with model-free approaches, such as rate-distortion theory (<xref ref-type="bibr" rid="bib5">Berger, 2003</xref>; <xref ref-type="bibr" rid="bib25">Eissa et al., 2021</xref>), can also aid in identifying commonalities in performance and resource usage within and across model classes without the need for pilot experiments.</p><p>Our work complements the existing literature on optimal decision thresholds by demonstrating the diversity of forms those thresholds can take under different dynamic task conditions. Several early normative theories were, like ours, based on dynamic programming (<xref ref-type="bibr" rid="bib45">Rapoport and Burkheimer, 1971</xref>; <xref ref-type="bibr" rid="bib12">Busemeyer and Rapoport, 1988</xref>) and in some cases models fit to experimental data (<xref ref-type="bibr" rid="bib19">Ditterich, 2006</xref>). For example, dynamic programming was used to show that certain optimal decisions can require non-constant decision boundaries similar to those of our normative models in dynamic reward tasks (<xref ref-type="bibr" rid="bib28">Frazier and Yu, 2007</xref>; <xref ref-type="fig" rid="fig2">Figure 2</xref>). More recently, dynamic programming (<xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>; <xref ref-type="bibr" rid="bib22">Drugowitsch et al., 2014b</xref>; <xref ref-type="bibr" rid="bib50">Tajima et al., 2016</xref>) or policy iteration (<xref ref-type="bibr" rid="bib39">Malhotra et al., 2017</xref>; <xref ref-type="bibr" rid="bib40">Malhotra et al., 2018</xref>) have been used to identify normative strategies in dynamic environments that can have monotonically collapsing decision thresholds that in some cases can be implemented using an urgency signal (<xref ref-type="bibr" rid="bib51">Tajima et al., 2019</xref>). These strategies include dynamically changing decision thresholds when signal-to-noise ratios of evidence streams vary according to a Cox-Ingersoll-Ross process (<xref ref-type="bibr" rid="bib21">Drugowitsch et al., 2014a</xref>) and non-monotonic thresholds when the evidence quality varies unpredictably across trials but is fixed within each trial <xref ref-type="bibr" rid="bib40">Malhotra et al., 2018</xref>. Other recent work has started to generalize notions of urgency-gating behavior (<xref ref-type="bibr" rid="bib58">Trueblood et al., 2021</xref>). However, these previous studies tended to focus on environments with a fixed structure, in which dynamic decision thresholds are adapted as the observer acquires knowledge of the environment. Here we have characterized in more detail how both expected and unexpected changes in context within trials relate to changes in decision thresholds over time.</p><p>Perceptual decision-making tasks provide a readily accessible route for validating our normative theory, especially considering the ease with which task difficulty can be parameterized to identify parameter ranges in which strategies can best be differentiated (<xref ref-type="bibr" rid="bib43">Philiastides et al., 2006</xref>). There is ample evidence already that people can tune the timescale of leaky evidence accumulation processes to the switching rate of an unpredictably changing state governing the statistics of a visual stimulus, to efficiently integrate observations and make a decision about the state (<xref ref-type="bibr" rid="bib41">Ossmy et al., 2013</xref>; <xref ref-type="bibr" rid="bib29">Glaze et al., 2015</xref>). We thus speculate that adaptive decision rules could be identified similarly in the strategies people use to make decisions about perceptual stimuli in dynamic contexts.</p><p>The neural mechanisms responsible for implementing and controlling decision thresholds are not well understood. Recent work has identified several cortical regions that may contribute to threshold formation, such as prefrontal cortex (<xref ref-type="bibr" rid="bib33">Hanks et al., 2015</xref>), dorsal premotor area (<xref ref-type="bibr" rid="bib57">Thura and Cisek, 2020</xref>), and superior colliculus (<xref ref-type="bibr" rid="bib18">Crapse et al., 2018</xref>; <xref ref-type="bibr" rid="bib34">Jun et al., 2021</xref>). Urgency signals are a complementary way of dynamically changing decision thresholds via a commensurate scale in belief, which <xref ref-type="bibr" rid="bib56">Thura and Cisek, 2017</xref> suggest are detectable in recordings from basal ganglia. The normative decision thresholds we derived do not employ urgency signals, but analogous UGMs may involve non-monotonic signals. For example, the switch from an infinite-to-constant decision threshold typical of low-to-high reward switches would correspond to a signal that suppresses responses until a reward change. Measurable signals predicted by our normative models would therefore correspond to zero mean activity during low reward, followed by constant mean activity during high reward. While more experimental work is needed to test this hypothesis, our work has expanded the view of normative and neural decision making as dynamic processes for both deliberation and commitment.</p></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Normative decision thresholds from dynamic programming</title><p>Here we detail the dynamic programming tools required to find normative decision thresholds. For the free-response tasks we consider, an observer gathers a sample of evidence <inline-formula><mml:math id="inf135"><mml:mi>ξ</mml:mi></mml:math></inline-formula>, uses the log-likelihood ratio (LLR) <inline-formula><mml:math id="inf136"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>ξ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> as their ‘belief’, and sets potentially time-dependent decision thresholds, <inline-formula><mml:math id="inf137"><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></inline-formula> that determine when they will stop accumulating evidence and commit to a choice. When <inline-formula><mml:math id="inf138"><mml:mrow><mml:mi>y</mml:mi><mml:mo>≥</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> (<inline-formula><mml:math id="inf139"><mml:mrow><mml:mi>y</mml:mi><mml:mo>≤</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>), the observer chooses the state <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (<inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>). In general, an observer is free to set <inline-formula><mml:math id="inf142"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> any way they wish. However, a normative observer sets these thresholds to optimize an objective function, which we assume throughout this study to be the trial-averaged reward rate, <inline-formula><mml:math id="inf143"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, which is given by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. In this definition of reward rate, the incremental cost function <inline-formula><mml:math id="inf144"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> accounts for both explicit costs (e.g. paying for observed evidence, metabolic costs of storing belief in working memory) and implicit costs (e.g. opportunity cost). We assume symmetry in the problem (in terms of prior, rewards, etc.) that guarantees the thresholds are symmetric about <inline-formula><mml:math id="inf145"><mml:mrow><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf146"><mml:mrow><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. We derive the optimal threshold policy for a general incremental cost function <inline-formula><mml:math id="inf147"><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, but in our results we consider only constant costs functions <inline-formula><mml:math id="inf148"><mml:mi>c</mml:mi></mml:math></inline-formula>. Although the space of possible cost functions is large, restricting to a constant value ensures that threshold dynamics are governed purely by task and reward structure and not by an arbitrary evidence cost function.</p><p>To find the thresholds <inline-formula><mml:math id="inf149"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> that optimize the reward rate given by <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>, we start with a discrete-time task where observations are made every <inline-formula><mml:math id="inf150"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> time units, and we simplify the problem so the length of each trial is fixed and independent of the decision time <inline-formula><mml:math id="inf151"><mml:msub><mml:mi>T</mml:mi><mml:mi>d</mml:mi></mml:msub></mml:math></inline-formula>. This simplification makes the denominator of <inline-formula><mml:math id="inf152"><mml:mi>ρ</mml:mi></mml:math></inline-formula> constant with respect to trial-to-trial variability, meaning we can optimize reward rate by maximizing the numerator <inline-formula><mml:math id="inf153"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">⟩</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Under this simplified task structure, we suppose the observer has just drawn a sample <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> and updated their state likelihood to <inline-formula><mml:math id="inf155"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf156"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> is the discrete-time LLR given by <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>. At this moment, the observer takes one of three possible actions:</p><list list-type="order"><list-item><p>Stop accumulating evidence and commit to choice <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This action has value equal to the average reward for choosing <inline-formula><mml:math id="inf158"><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula>, which is given by:<disp-formula id="equ11"><label>(9)</label><mml:math id="m11"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></list-item></list><list list-type="simple"><list-item><p>where <inline-formula><mml:math id="inf159"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> is the value for a correct choice and <inline-formula><mml:math id="inf160"><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the value for an incorrect choice.</p></list-item></list><list list-type="order"><list-item><p>Stop accumulating evidence and commit to choice <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. By assuming the reward for correctly (or incorrectly) choosing <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> is the same as choosing <inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the value of this action is obtained by symmetry from:<disp-formula id="equ12"><label>(10)</label><mml:math id="m12"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></list-item><list-item><p>Wait to commit to a choice and draw an additional piece of evidence. Choosing this action means the observer expects their future overall value <inline-formula><mml:math id="inf164"><mml:mi>V</mml:mi></mml:math></inline-formula> to be greater than their current value, less the cost incurred by waiting for additional evidence. Therefore, the value of this choice is given by:<disp-formula id="equ13"><label>(11)</label><mml:math id="m13"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p></list-item></list><list list-type="simple"><list-item><p>where <inline-formula><mml:math id="inf165"><mml:mi>c</mml:mi></mml:math></inline-formula> is the incremental evidence cost function; because we assume that the incremental cost is constant, this simplifies <inline-formula><mml:math id="inf166"><mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></list-item></list><p>Given the action values from <xref ref-type="disp-formula" rid="equ11 equ12 equ13">Equations 9–11</xref>, the observer takes the action with maximal value, resulting in their overall value function<disp-formula id="equ14"><label>(12)</label><mml:math id="m14"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>sample again</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Because the value-maximizing action depends on the state likelihood, <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, the regions of likelihood space where each action is optimal divide the space into three disjoint regions. The boundaries of these regions are exactly the optimal decision thresholds, which can be mapped to LLR-space to obtain <inline-formula><mml:math id="inf168"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>. To find these thresholds numerically, we started by discretizing the state likelihood space <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Because the state likelihood <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is restricted to values between 0 and 1, whereas the log-likelihood ratio <inline-formula><mml:math id="inf171"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> is unbounded, we chose to formulate all the components of Bellman’s equation in terms of <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to minimize truncation errors. We then proceeded by using backward induction in time, starting at the total trial length <inline-formula><mml:math id="inf173"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. At this moment in time, it impossible to wait for more evidence, so the value function in <xref ref-type="disp-formula" rid="equ14">Equation 12</xref> does not depend on the future. This approach implies that the value function is:<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Once the value is calculated at this time point, it can be used as the future value at time point <inline-formula><mml:math id="inf174"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>To find the decision thresholds for the desired tasks where <inline-formula><mml:math id="inf175"><mml:msub><mml:mi>T</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:math></inline-formula> is not fixed, we must optimize both the numerator and denominator of <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. To account for the variable trial length, we adopt techniques from average reward reinforcement learning (<xref ref-type="bibr" rid="bib38">Mahadevan, 1996</xref>) and penalize the waiting time associated with each action by the waiting time itself scaled by the reward rate <inline-formula><mml:math id="inf176"><mml:mi>ρ</mml:mi></mml:math></inline-formula> (i.e., <inline-formula><mml:math id="inf177"><mml:mrow><mml:mrow><mml:mo stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo stretchy="false">⟩</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:math></inline-formula> for committing to <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> or <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf180"><mml:mrow><mml:mi>ρ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> for waiting). This modification makes all trials effectively the same length and allows us to use the same approach used to derive <xref ref-type="disp-formula" rid="equ14">Equation 12</xref> (<xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>). The new overall value function is given by <xref ref-type="disp-formula" rid="equ3">Equation 3</xref>:<disp-formula id="equ16"><label>(13)</label><mml:math id="m16"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mrow><mml:mtext>choose </mml:mtext><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mi>ρ</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo></mml:mtd><mml:mtd><mml:mtext>sample again</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To use this new value function to numerically find the decision thresholds, we must note two new complications that arise from moving away from fixed-length trials. First, we no longer have a natural end time from which to start backward induction. We remedy this issue by following the approach of <xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref> and artificially setting a final trial time <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>T</mml:mi><mml:mi>f</mml:mi></mml:msub></mml:math></inline-formula> that is far enough in the future so that decision times of this length are highly unlikely and do not impact the response distributions. If we desire accurate thresholds up to a time <inline-formula><mml:math id="inf182"><mml:mi>t</mml:mi></mml:math></inline-formula>, we set <inline-formula><mml:math id="inf183"><mml:mrow><mml:msub><mml:mi>T</mml:mi><mml:mi>f</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>5</mml:mn><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula>, which produces an accurate solution while avoiding a large numerical overhead incurred from a longer simulation time. In our simulations, we set <inline-formula><mml:math id="inf184"><mml:mi>t</mml:mi></mml:math></inline-formula> based on when we expect most decisions to be made. Second, the value function now depends on the unknown quantity <inline-formula><mml:math id="inf185"><mml:mi>ρ</mml:mi></mml:math></inline-formula>, resulting in a co-optimization problem. To address this complication, note that when <inline-formula><mml:math id="inf186"><mml:mi>ρ</mml:mi></mml:math></inline-formula> is maximized, our derivation requires <inline-formula><mml:math id="inf187"><mml:mrow><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for a consistent Bellman’s equation (<xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref>). We exploit this consistency requirement by fixing an initial reward rate <inline-formula><mml:math id="inf188"><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:math></inline-formula>, solving the value function through backward induction, calculating <inline-formula><mml:math id="inf189"><mml:mrow><mml:mi>V</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>;</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and updating the value of <inline-formula><mml:math id="inf190"><mml:mi>ρ</mml:mi></mml:math></inline-formula> via a root finding scheme. For more details on numerical implementation, see <ext-link ext-link-type="uri" xlink:href="https://github.com/nwbarendregt/AdaptNormThresh">https://github.com/nwbarendregt/AdaptNormThresh</ext-link>; <xref ref-type="bibr" rid="bib52">Thresh, 2022</xref>.</p></sec><sec id="s4-2"><title>Dynamic context 2AFC tasks</title><p>For all dynamic context tasks, we assume that observations follow a Gaussian distribution with so that <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>±</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Using the Functional Central Limit Theorem, one can show (<xref ref-type="bibr" rid="bib8">Bogacz et al., 2006</xref>) that in the continuous-time limit, the belief <inline-formula><mml:math id="inf192"><mml:mi>y</mml:mi></mml:math></inline-formula> evolves according to a stochastic differential equation:<disp-formula id="equ17"><label>(14)</label><mml:math id="m17"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="disp-formula" rid="equ17">Equation 14</xref>, <inline-formula><mml:math id="inf193"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></inline-formula> is the scaled signal-to-noise ratio (SNR) given by the observation distribution function <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>±</mml:mo><mml:mi>μ</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mi>t</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> is a standard increment of a Wiener process, and the sign of the drift <inline-formula><mml:math id="inf196"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>m</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is given by the sign of the correct choice <inline-formula><mml:math id="inf197"><mml:msub><mml:mi>s</mml:mi><mml:mo>±</mml:mo></mml:msub></mml:math></inline-formula>. To construct Bellman’s equation for this task, we start by discretizing time <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> and determine the average value gained by waiting and collecting another observation given by <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>:<disp-formula id="equ18"><mml:math id="m18"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msubsup><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf199"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability the environment is in state <inline-formula><mml:math id="inf200"><mml:msub><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msub></mml:math></inline-formula> given <inline-formula><mml:math id="inf201"><mml:mi>n</mml:mi></mml:math></inline-formula> pieces of evidence. The main difficulty in computing this expectation is computing the conditional probability distribution <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, which we call the likelihood transfer function. Once we construct the likelihood transfer function, we can use our discretization of the state likelihood space <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to evaluate the integral in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref> using any standard numerical quadrature scheme. To compute this transfer function, we can start by using the definition of the LLR <inline-formula><mml:math id="inf204"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and leveraging the relationship between <inline-formula><mml:math id="inf205"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf206"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to find <inline-formula><mml:math id="inf207"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and a function of the observation <inline-formula><mml:math id="inf208"><mml:msub><mml:mi>ξ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ19"><label>(15)</label><mml:math id="m19"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac></mml:mrow></mml:msup><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup></mml:mrow></mml:mfrac></mml:mtd></mml:mtr><mml:mtr><mml:mtd/><mml:mtd><mml:mo>=</mml:mo><mml:mrow/><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>μ</mml:mi></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Note that we used the fact that in discrete-time with a time step <inline-formula><mml:math id="inf209"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>, the observations <inline-formula><mml:math id="inf210"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>ξ</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow><mml:mo>±</mml:mo></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>±</mml:mo><mml:mi>μ</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The relationship between <inline-formula><mml:math id="inf211"><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf212"><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ19">Equation 15</xref> can be inverted to obtain:<disp-formula id="equ20"><mml:math id="m20"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>With this relationship established, we can find the likelihood transfer function <inline-formula><mml:math id="inf213"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by finding the observation transfer function <inline-formula><mml:math id="inf214"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mrow><mml:mi>ξ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and performing a change of variables, which by independence of the sample is simply <inline-formula><mml:math id="inf215"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mi>ξ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. With probability <inline-formula><mml:math id="inf216"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf217"><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> will be drawn from the normal distribution <inline-formula><mml:math id="inf218"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, and with probability <inline-formula><mml:math id="inf219"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf220"><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> will be drawn from the normal distribution <inline-formula><mml:math id="inf221"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. This immediately provides the observation transfer function by marginalizing:<disp-formula id="equ21"><mml:math id="m21"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>ξ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>:</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msqrt><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>μ</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msqrt><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>μ</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Performing the change of variables using the derivative <inline-formula><mml:math id="inf222"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>d</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi>μ</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:msubsup><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> yields the transfer function<disp-formula id="equ22"><label>(16)</label><mml:math id="m22"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mi>μ</mml:mi><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:msqrt><mml:mi>σ</mml:mi></mml:mrow></mml:mfrac><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">[</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mstyle><mml:mspace linebreak="newline"/><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mi>δ</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mrow><mml:mn>2</mml:mn><mml:mi>μ</mml:mi></mml:mrow></mml:mfrac><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mi>μ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Note that <xref ref-type="disp-formula" rid="equ22">Equation 16</xref> is equivalent to the likelihood transfer function given by Equation 16 in <xref ref-type="bibr" rid="bib20">Drugowitsch et al., 2012</xref> for the case of <inline-formula><mml:math id="inf223"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>. Combining <xref ref-type="disp-formula" rid="equ17">Equation 14</xref> and <xref ref-type="disp-formula" rid="equ22">Equation 16</xref>, we can construct Bellman’s equation for any dynamic context task.</p><sec id="s4-2-1"><title>Reward-change task thresholds</title><p>For the reward-change task, we fixed punishment <inline-formula><mml:math id="inf224"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> and allowed the reward <inline-formula><mml:math id="inf225"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> to be a Heaviside function given by <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>:<disp-formula id="equ23"><mml:math id="m23"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>, there is a single switch in rewards between pre-change reward <inline-formula><mml:math id="inf226"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and post-change reward <inline-formula><mml:math id="inf227"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. This change occurs at <inline-formula><mml:math id="inf228"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. Substituting this reward function into <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> allows us to find the normative thresholds for this task as a function of <inline-formula><mml:math id="inf229"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf230"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>For the inferred reward change task, we allowed the reward value <inline-formula><mml:math id="inf231"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to be controlled by a continuous-time two-state Markov process with transition (hazard) rate <inline-formula><mml:math id="inf232"><mml:mi>h</mml:mi></mml:math></inline-formula> between rewards <inline-formula><mml:math id="inf233"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo>≥</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>. The hazard rate <inline-formula><mml:math id="inf234"><mml:mi>h</mml:mi></mml:math></inline-formula> governs the probability of switching between <inline-formula><mml:math id="inf235"><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ24"><mml:math id="m24"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo movablelimits="true" form="prefix">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>h</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">↓</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mstyle><mml:mspace linebreak="newline"/><mml:mo movablelimits="true" form="prefix">Pr</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mi>R</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>h</mml:mi><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>o</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mi>δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">↓</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf237"><mml:mrow><mml:mi>o</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> represents a function <inline-formula><mml:math id="inf238"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> with the property <inline-formula><mml:math id="inf239"><mml:mrow><mml:mrow><mml:msub><mml:mo>lim</mml:mo><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>↓</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>g</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> (i.e., all other terms are of smaller order than <inline-formula><mml:math id="inf240"><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula>). In addition, the state of this Markov process must be inferred from evidence <inline-formula><mml:math id="inf241"><mml:mi>η</mml:mi></mml:math></inline-formula> that is independent of the environment’s state evidence <inline-formula><mml:math id="inf242"><mml:mi>ξ</mml:mi></mml:math></inline-formula> (i.e., the correct choice). For simplicity, we assume that the reward-evidence source is also Gaussian-distributed such that <inline-formula><mml:math id="inf243"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>η</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">N</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>±</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> with quality <inline-formula><mml:math id="inf244"><mml:mrow><mml:msub><mml:mi>m</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>μ</mml:mi><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mrow><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:mfrac></mml:mrow></mml:math></inline-formula>. <xref ref-type="bibr" rid="bib29">Glaze et al., 2015</xref>; <xref ref-type="bibr" rid="bib59">Veliz-Cuba et al., 2016</xref>; <xref ref-type="bibr" rid="bib3">Barendregt et al., 2019</xref> have shown that the belief <inline-formula><mml:math id="inf245"><mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mi>R</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mfrac><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:mi>Pr</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo lspace="2.5pt" rspace="2.5pt" stretchy="false">|</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula> for such a dynamic state inference process is given by the modified DDM<disp-formula id="equ25"><mml:math id="m25"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>h</mml:mi><mml:mi>sinh</mml:mi><mml:mo>⁡</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:msub><mml:mi>m</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msub></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf246"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∈</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> is a telegraph process that mirrors the state of the reward process (i.e., <inline-formula><mml:math id="inf247"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf248"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>H</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf249"><mml:mrow><mml:mrow><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula> when <inline-formula><mml:math id="inf250"><mml:mrow><mml:mrow><mml:mi>R</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula>). With this belief over reward state, we must also modify the values <inline-formula><mml:math id="inf251"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>+</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf252"><mml:mrow><mml:msub><mml:mi>V</mml:mi><mml:mo>-</mml:mo></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> to account for the uncertainty in <inline-formula><mml:math id="inf253"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula>. Defining <inline-formula><mml:math id="inf254"><mml:mrow><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:msup><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:msub><mml:mi>y</mml:mi><mml:mi>R</mml:mi></mml:msub></mml:msup></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> as the reward likelihood gives<disp-formula id="equ26"> <mml:math id="m26"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>V</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow/><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>H</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>q</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mrow><mml:mo>⟨</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>⟩</mml:mo></mml:mrow><mml:mi>ρ</mml:mi><mml:mo>,</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where we have fixed <inline-formula><mml:math id="inf255"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> for simplicity.</p></sec><sec id="s4-2-2"><title>SNR-change task thresholds</title><p>For the SNR-change task, we allowed the task difficulty <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mfrac></mml:mrow></mml:math></inline-formula> to vary over a single trial by making <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> a time-dependent step function given by <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>:<disp-formula id="equ27"><mml:math id="m27"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="disp-formula" rid="equ6">Equation 6</xref>, there is a single switch in evidence quality between pre-change quality <inline-formula><mml:math id="inf258"><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and post-change quality <inline-formula><mml:math id="inf259"><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. This change occurs at <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula>. Substituting this quality time series into the likelihood transfer function in <xref ref-type="disp-formula" rid="equ22">Equation 16</xref> allows us to find the normative thresholds for this task as a function of <inline-formula><mml:math id="inf261"><mml:msub><mml:mi>μ</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf262"><mml:msub><mml:mi>μ</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:math></inline-formula>. This modification necessitates that the transfer function <inline-formula><mml:math id="inf263"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> also be a function of time; however, because the quality change points are known in advance to the observer, we can simply change between different transfer functions at the specified quality changes.</p></sec></sec><sec id="s4-3"><title>Reward-change task model performance</title><p>Here we detail the three models used to compare observer performance in the reward-change task, as well as the noise filtering process used to generate synthetic data. For the noisy Bayesian model, the observer uses the thresholds <inline-formula><mml:math id="inf264"><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> obtained via dynamic programming, thus making the observer a noisy ideal observer. For the constant-threshold model, the observer uses a constant threshold <inline-formula><mml:math id="inf265"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo>±</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>, which is predicted to be optimal only in very simple, static decision environments with only two states <inline-formula><mml:math id="inf266"><mml:mi>s</mml:mi></mml:math></inline-formula>. Both the noisy Bayesian and constant-threshold models also use a noisy perturbation of the LLR <inline-formula><mml:math id="inf267"><mml:mrow><mml:mover accent="true"><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mrow><mml:mi>y</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mi>Z</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> as their belief, where <inline-formula><mml:math id="inf268"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> is the strength of the noise and <inline-formula><mml:math id="inf269"><mml:mi>Z</mml:mi></mml:math></inline-formula> is a sample from a standard normal distribution. In continuous-time, this perturbation involves adding an independent Wiener process to <xref ref-type="disp-formula" rid="equ17">Equation 14</xref>:<disp-formula id="equ28"><mml:math id="m28"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext> </mml:mtext><mml:mi>d</mml:mi><mml:mrow><mml:mover><mml:mi>y</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mo>±</mml:mo><mml:mi>m</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msqrt><mml:mn>2</mml:mn><mml:mi>m</mml:mi></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msubsup><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf270"><mml:mrow><mml:mi>d</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mi>t</mml:mi><mml:mo>′</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> is an independent Wiener process with strength <inline-formula><mml:math id="inf271"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>. The UGM, being a phenomenological model, behaves differently from the other models. The UGM belief <inline-formula><mml:math id="inf272"><mml:mi>E</mml:mi></mml:math></inline-formula> is the output of the noisy low-pass filter given by <xref ref-type="disp-formula" rid="equ8">Equation 7</xref>:<disp-formula id="equ29"><mml:math id="m29"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>τ</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>−</mml:mo><mml:mi>E</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mo>−</mml:mo><mml:mi>y</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mfrac><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mspace width="thinmathspace"/><mml:mi>d</mml:mi><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>To add additional noise to the UGM’s belief variable <inline-formula><mml:math id="inf273"><mml:mi>E</mml:mi></mml:math></inline-formula>, we simply allowed <inline-formula><mml:math id="inf274"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> in the low-pass filter in <xref ref-type="disp-formula" rid="equ8">Equation 7</xref>.</p><p>In addition to the inference noise with strength <inline-formula><mml:math id="inf275"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>, we also filtered each process through a Gaussian response-time filter with zero mean and standard deviation <inline-formula><mml:math id="inf276"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. Under this response-time filter, if the model predicted a response time <inline-formula><mml:math id="inf277"><mml:mi>T</mml:mi></mml:math></inline-formula>, the measured response time <inline-formula><mml:math id="inf278"><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> was drawn from a normal distribution centered at <inline-formula><mml:math id="inf279"><mml:mi>T</mml:mi></mml:math></inline-formula> with standard deviation <inline-formula><mml:math id="inf280"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. If the response time <inline-formula><mml:math id="inf281"><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> was drawn outside of the simulation’s time discretization (i.e., if <inline-formula><mml:math id="inf282"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf283"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow><mml:mo>&gt;</mml:mo><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mn>5</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>), we redrew <inline-formula><mml:math id="inf284"><mml:mover accent="true"><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:math></inline-formula> until it fell within the discretization. This filter was chosen to represent both “early responses” caused by attentional lapses, as well as ‘late responses’ caused by motor processing delays between the formation of a choice in the brain and the physical response. We have chosen to add these two sources of noise after optimizing each model to maximize average reward rate, rather than reoptimizing each model after adding these additional noise sources. Although we could have reoptimized each model to maximize performance across noise realizations, we were interested in how the models responded to perturbations that drove their performance to be sub-optimal (but possibly near-optimal).</p><p>To compare model performance on the reward-change task, we first fixed the value of pre-change reward <inline-formula><mml:math id="inf285"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> (and set <inline-formula><mml:math id="inf286"><mml:mrow><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:mrow></mml:math></inline-formula>) to find the post-change reward and tuned each model to achieve optimal reward rate with no additional noise in both the inference and response processes. Bellman’s equation outputs both the optimal normative thresholds and reward rate. For the constant threshold model and the UGM, we approximated the maximal performance of each model by using a grid search over each models parameters to find the model tuning that yielded the highest average reward rate. After tuning all models for a given reward structure, we filtered them through both the sensory (<inline-formula><mml:math id="inf287"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>) and motor (<inline-formula><mml:math id="inf288"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>) noise sources without re-turning the models to account for this additional noise. When generating noisy synthetic data from these models, we generated 100 synthetic subjects, each with sampled values of <inline-formula><mml:math id="inf289"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf290"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>. For each synthetic subject with noise parameter sample (<inline-formula><mml:math id="inf291"><mml:msub><mml:mi>σ</mml:mi><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula>, <inline-formula><mml:math id="inf292"><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), we defined the “noise strength” of that subject’s noise to be the ratio<disp-formula id="equ30"><mml:math id="m30"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mfrac><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msub><mml:mover><mml:mi>σ</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mover><mml:mi>σ</mml:mi><mml:mo accent="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf293"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf294"><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> are the maximum values of belief noise and motor noise considered, respectively. Using this metric, noise strength is defined between 0 and 1. Additionally, the maximum noise levels <inline-formula><mml:math id="inf295"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mi>y</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf296"><mml:msub><mml:mover accent="true"><mml:mi>σ</mml:mi><mml:mo>¯</mml:mo></mml:mover><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> where chosen such that a noise strength of 0.5 is approximately equivalent to the fitted noise strength obtained from tokens task subject data. We plot the response distributions using noise strengths of 0, 0.5, and 1 in our results. To compare the performance of each model after being corrupted by noise, we then generated 1000 trials for each subject and had each simulated subject repeat the same block of trials three times, one for each model. This process ensured that the only difference between model performance would come from their distinct threshold behaviors, because each model was taken to be equally noisy and was run using the same stimuli.</p></sec><sec id="s4-4"><title>Tokens task</title><sec id="s4-4-1"><title>Normative model for the tokens task</title><p>For the tokens task, observations in the form of token movements are Bernoulli distributed with parameter <inline-formula><mml:math id="inf297"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> that occur every 200ms. Once a subject committed to a decision, the token movements continued at a faster rate until the entire animation had finished. This post-decision token acceleration was 170ms per movement in the ‘slow’ version of the task and 20ms per movement for the ‘fast’ version of the task. Because of the stimulus structure, one can show using a combinatorial argument (<xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>) that the likelihood function <inline-formula><mml:math id="inf298"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is given by <xref ref-type="disp-formula" rid="equ10">Equation 8</xref>. Constructing the likelihood transfer function <inline-formula><mml:math id="inf299"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> required for Bellman’s equation is also simplified from the Gaussian 2AFC tasks, as there are only two possible likelihoods that one can transition two after observing a token movement:<disp-formula id="equ31"> <label>(17)</label><mml:math id="m31"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>top</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo fence="false" stretchy="false">|</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mtext>top</mml:mtext><mml:mspace width="thinmathspace"/><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>U</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext>otherwise</mml:mtext></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Combining <xref ref-type="disp-formula" rid="equ10">Equation 8</xref> and <xref ref-type="disp-formula" rid="equ31">Equation 17</xref>, we can fully construct Bellman’s equation for the tokens task. While the timings of the token movements, post-decision token acceleration, and inter-trial interval are fixed, we let the reward <inline-formula><mml:math id="inf300"><mml:msub><mml:mi>R</mml:mi><mml:mi>c</mml:mi></mml:msub></mml:math></inline-formula> and cost function <inline-formula><mml:math id="inf301"><mml:mi>c</mml:mi></mml:math></inline-formula> be free parameters to control the different threshold dynamics of the model.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>List of model parameters used for analyzing tokens task response time data.</title></caption><table frame="hsides" rules="groups"><tbody><tr><td align="left" valign="top">NB:</td><td align="left" valign="top">Reward <inline-formula><mml:math id="inf302"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Cost <inline-formula><mml:math id="inf303"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>c</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Sensory Noise <inline-formula><mml:math id="inf304"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Motor Noise <inline-formula><mml:math id="inf305"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">Const:</td><td align="left" valign="top">Threshold <inline-formula><mml:math id="inf306"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Sensory Noise <inline-formula><mml:math id="inf307"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Motor Noise <inline-formula><mml:math id="inf308"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="top">UGM:</td><td align="left" valign="top">Threshold Scale <inline-formula><mml:math id="inf309"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Gain <inline-formula><mml:math id="inf310"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>a</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Sensory Noise <inline-formula><mml:math id="inf311"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Time Constant <inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>τ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula><break/>Motor Noise <inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula></td></tr></tbody></table></table-wrap></sec><sec id="s4-4-2"><title>Model fitting and comparison</title><p>We used three models to fit the subject response data provided by <xref ref-type="bibr" rid="bib16">Cisek et al., 2009</xref>: the noisy Bayesian model (<inline-formula><mml:math id="inf314"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn></mml:mrow></mml:math></inline-formula> parameters), the constant threshold model (<inline-formula><mml:math id="inf315"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math></inline-formula> parameters), and the UGM (<inline-formula><mml:math id="inf316"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>) parameters (<xref ref-type="table" rid="table1">Table 1</xref>). To adapt the continuous-time models to this discrete-time task, we simply changed the time step to match the time between token movements (<inline-formula><mml:math id="inf317"><mml:mrow><mml:mrow><mml:mi>δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> ms). To fit each model, we took the subject response time distributions as our objective function and used Markov Chain Monte Carlo (MCMC) with a standard Gaussian proposal distribution to generate an approximate posterior made up of 10,000 samples. For more details as to our specific implementation of MCMC for this data, see the MATLAB code available at <ext-link ext-link-type="uri" xlink:href="https://github.com/nwbarendregt/AdaptNormThresh">https://github.com/nwbarendregt/AdaptNormThresh</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bbb89f81d744a0e8835a6f6791e8b653b7d431be;origin=https://github.com/nwbarendregt/AdaptNormThresh;visit=swh:1:snp:a568cc5cff39965992895669038ed8c43a59eedd;anchor=swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2">swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2</ext-link>; <xref ref-type="bibr" rid="bib52">Thresh, 2022</xref>). We held out 2 of the 22 subjects to use as training data when tuning the covariance matrix of the proposal distribution for each model, and performed the model fitting and comparison analysis on the remaining 20 subjects. Using the approximate posterior obtained via MCMC for each subject and model, we used calculated AICc using the formula<disp-formula id="equ32"><label>(18)</label><mml:math id="m32"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>AICc</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>2</mml:mn><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mover><mml:mi>L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>k</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:mn>2</mml:mn><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="disp-formula" rid="equ32">Equation 18</xref>, <inline-formula><mml:math id="inf318"><mml:mi>k</mml:mi></mml:math></inline-formula> is the number of parameters of the model, <inline-formula><mml:math id="inf319"><mml:mover accent="true"><mml:mi>L</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:math></inline-formula> is the likelihood of the model evaluated at the maximum-likelihood parameters, and <inline-formula><mml:math id="inf320"><mml:mi>n</mml:mi></mml:math></inline-formula> is the number of responses in the subject data (<xref ref-type="bibr" rid="bib14">Cavanaugh, 1997</xref>; <xref ref-type="bibr" rid="bib11">Brunham and Anderson, 2002</xref>). Because each subject performed different numbers of trials, using AICc allowed us to normalize results to account for the different data sizes; note that for many responses (i.e., for large <inline-formula><mml:math id="inf321"><mml:mi>n</mml:mi></mml:math></inline-formula>), AICc converges to the standard definition of AIC. For the second model selection metric, we measured how well each fitted model predicted the trial-by-trial responses of the data by calculating the average RMSE between the response times from the data and the response times predicted by each model. To measure the difference between a subject’s response time distribution and the fitted model’s distribution (<xref ref-type="fig" rid="fig6s1">Figure 6—figure supplement 1</xref>), we used Kullback-Leibler (KL) divergence:<disp-formula id="equ33"><label>(19)</label><mml:math id="m33"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mtext>KL</mml:mtext><mml:mo>=</mml:mo><mml:munderover><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mn>15</mml:mn></mml:mrow></mml:munderover><mml:msub><mml:mtext>RT</mml:mtext><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mi>ln</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mtext>RT</mml:mtext><mml:mrow><mml:mi>D</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mrow><mml:msub><mml:mtext>RT</mml:mtext><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>In <xref ref-type="disp-formula" rid="equ33">Equation 19</xref>, <inline-formula><mml:math id="inf322"><mml:mi>i</mml:mi></mml:math></inline-formula> is a time index representing the number of observed token movements, <inline-formula><mml:math id="inf323"><mml:mrow><mml:msub><mml:mtext>RT</mml:mtext><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of responding after <inline-formula><mml:math id="inf324"><mml:mi>i</mml:mi></mml:math></inline-formula> token movements from the subject data, and <inline-formula><mml:math id="inf325"><mml:mrow><mml:msub><mml:mtext>RT</mml:mtext><mml:mi>M</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the probability of responding after <inline-formula><mml:math id="inf326"><mml:mi>i</mml:mi></mml:math></inline-formula> token movements from the model’s response distribution. Smaller values of KL divergence indicate that the model’s response distribution is more similar to the subject data.</p></sec></sec><sec id="s4-5"><title>Code availability</title><p>See <ext-link ext-link-type="uri" xlink:href="https://github.com/nwbarendregt/AdaptNormThresh">https://github.com/nwbarendregt/AdaptNormThresh</ext-link>; (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bbb89f81d744a0e8835a6f6791e8b653b7d431be;origin=https://github.com/nwbarendregt/AdaptNormThresh;visit=swh:1:snp:a568cc5cff39965992895669038ed8c43a59eedd;anchor=swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2">swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2</ext-link>; <xref ref-type="bibr" rid="bib52">Thresh, 2022</xref>) for the MATLAB code used to generate all results and figures.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>Senior editor, <italic>eLife</italic></p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Software, Formal analysis, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Supervision, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Funding acquisition, Writing - review and editing</p></fn><fn fn-type="con" id="con4"><p>Supervision, Funding acquisition, Writing - review and editing</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-79824-mdarchecklist1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>MATLAB code used to generate all results and figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/nwbarendregt/AdaptNormThresh">https://github.com/nwbarendregt/AdaptNormThresh</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bbb89f81d744a0e8835a6f6791e8b653b7d431be;origin=https://github.com/nwbarendregt/AdaptNormThresh;visit=swh:1:snp:a568cc5cff39965992895669038ed8c43a59eedd;anchor=swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2">swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2</ext-link>).</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Paul Cisek for providing response data from the tokens task used in our analysis.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ashwood</surname><given-names>ZC</given-names></name><name><surname>Roy</surname><given-names>NA</given-names></name><name><surname>Stone</surname><given-names>IR</given-names></name><name><surname>Urai</surname><given-names>AE</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name><name><surname>Pillow</surname><given-names>JW</given-names></name><collab>International Brain Laboratory</collab></person-group><year iso-8601-date="2022">2022</year><article-title>Mice alternate between discrete strategies during perceptual decision-making</article-title><source>Nature Neuroscience</source><volume>25</volume><fpage>201</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-01007-z</pub-id><pub-id pub-id-type="pmid">35132235</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Balci</surname><given-names>F</given-names></name><name><surname>Simen</surname><given-names>P</given-names></name><name><surname>Niyogi</surname><given-names>R</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Hughes</surname><given-names>JA</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Acquisition of decision making criteria: reward rate ultimately beats accuracy</article-title><source>Attention, Perception &amp; Psychophysics</source><volume>73</volume><fpage>640</fpage><lpage>657</lpage><pub-id pub-id-type="doi">10.3758/s13414-010-0049-7</pub-id><pub-id pub-id-type="pmid">21264716</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Barendregt</surname><given-names>NW</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Analyzing dynamic decision-making models using chapman-kolmogorov equations</article-title><source>Journal of Computational Neuroscience</source><volume>47</volume><fpage>205</fpage><lpage>222</lpage><pub-id pub-id-type="doi">10.1007/s10827-019-00733-5</pub-id><pub-id pub-id-type="pmid">31734803</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bellman</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1957">1957</year><source>Dynamic Programming</source><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib5"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Berger</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Rate-Distortion Theory</source><publisher-name>Wiley Encyclopedia of Telecommunications</publisher-name></element-citation></ref><ref id="bib6"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Bertsekas</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2012">2012</year><source>Dynamic Programming and Optimal Control</source><publisher-name>Athena scientific</publisher-name></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boehm</surname><given-names>U</given-names></name><name><surname>van Maanen</surname><given-names>L</given-names></name><name><surname>Evans</surname><given-names>NJ</given-names></name><name><surname>Brown</surname><given-names>SD</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A theoretical analysis of the reward rate optimality of collapsing decision criteria</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>82</volume><fpage>1520</fpage><lpage>1534</lpage><pub-id pub-id-type="doi">10.3758/s13414-019-01806-4</pub-id><pub-id pub-id-type="pmid">31359378</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Brown</surname><given-names>E</given-names></name><name><surname>Moehlis</surname><given-names>J</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>The physics of optimal decision making: a formal analysis of models of performance in two-alternative forced-choice tasks</article-title><source>Psychological Review</source><volume>113</volume><fpage>700</fpage><lpage>765</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.113.4.700</pub-id><pub-id pub-id-type="pmid">17014301</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bogacz</surname><given-names>R</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Forstmann</surname><given-names>BU</given-names></name><name><surname>Nieuwenhuis</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The neural basis of the speed-accuracy tradeoff</article-title><source>Trends in Neurosciences</source><volume>33</volume><fpage>10</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2009.09.002</pub-id><pub-id pub-id-type="pmid">19819033</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Burge</surname><given-names>J</given-names></name><name><surname>Yates</surname><given-names>J</given-names></name><name><surname>Pillow</surname><given-names>J</given-names></name><name><surname>Cormack</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Continuous psychophysics: target-tracking to measure visual sensitivity</article-title><source>Journal of Vision</source><volume>15</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.1167/15.3.14</pub-id><pub-id pub-id-type="pmid">25795437</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Brunham</surname><given-names>K</given-names></name><name><surname>Anderson</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2002">2002</year><source>Model Selection and Multimodel Inference: A Practical Information-Theoretic Approach</source><publisher-loc>New York Inc</publisher-loc><publisher-name>Springer</publisher-name></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busemeyer</surname><given-names>JR</given-names></name><name><surname>Rapoport</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Psychological models of deferred decision making</article-title><source>Journal of Mathematical Psychology</source><volume>32</volume><fpage>91</fpage><lpage>134</lpage><pub-id pub-id-type="doi">10.1016/0022-2496(88)90042-9</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carland</surname><given-names>MA</given-names></name><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>The urgency-gating model can explain the effects of early evidence</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>22</volume><fpage>1830</fpage><lpage>1838</lpage><pub-id pub-id-type="doi">10.3758/s13423-015-0851-2</pub-id><pub-id pub-id-type="pmid">26452377</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cavanaugh</surname><given-names>JE</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Unifying the derivations for the akaike and corrected akaike information criteria</article-title><source>Statistics &amp; Probability Letters</source><volume>33</volume><fpage>201</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1016/S0167-7152(96)00128-9</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chittka</surname><given-names>L</given-names></name><name><surname>Skorupski</surname><given-names>P</given-names></name><name><surname>Raine</surname><given-names>NE</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Speed-Accuracy tradeoffs in animal decision making</article-title><source>Trends in Ecology &amp; Evolution</source><volume>24</volume><fpage>400</fpage><lpage>407</lpage><pub-id pub-id-type="doi">10.1016/j.tree.2009.02.010</pub-id><pub-id pub-id-type="pmid">19409649</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cisek</surname><given-names>P</given-names></name><name><surname>Puskas</surname><given-names>GA</given-names></name><name><surname>El-Murr</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Decisions in changing conditions: the urgency-gating model</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>11560</fpage><lpage>11571</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1844-09.2009</pub-id><pub-id pub-id-type="pmid">19759303</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Combes</surname><given-names>SA</given-names></name><name><surname>Rundle</surname><given-names>DE</given-names></name><name><surname>Iwasaki</surname><given-names>JM</given-names></name><name><surname>Crall</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Linking biomechanics and ecology through predator-prey interactions: flight performance of dragonflies and their prey</article-title><source>The Journal of Experimental Biology</source><volume>215</volume><fpage>903</fpage><lpage>913</lpage><pub-id pub-id-type="doi">10.1242/jeb.059394</pub-id><pub-id pub-id-type="pmid">22357584</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Crapse</surname><given-names>TB</given-names></name><name><surname>Lau</surname><given-names>H</given-names></name><name><surname>Basso</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>A role for the superior colliculus in decision criteria</article-title><source>Neuron</source><volume>97</volume><fpage>181</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.12.006</pub-id><pub-id pub-id-type="pmid">29301100</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ditterich</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Evidence for time-variant decision making</article-title><source>The European Journal of Neuroscience</source><volume>24</volume><fpage>3628</fpage><lpage>3641</lpage><pub-id pub-id-type="doi">10.1111/j.1460-9568.2006.05221.x</pub-id><pub-id pub-id-type="pmid">17229111</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Churchland</surname><given-names>AK</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>The cost of accumulating evidence in perceptual decision making</article-title><source>The Journal of Neuroscience</source><volume>32</volume><fpage>3612</fpage><lpage>3628</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4010-11.2012</pub-id><pub-id pub-id-type="pmid">22423085</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014a</year><article-title>Optimal decision-making with time-varying evidence reliability</article-title><conf-name>In Advances in neural information processing systems</conf-name><fpage>748</fpage><lpage>756</lpage></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Moreno-Bote</surname><given-names>R</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014b</year><article-title>Relation between belief and performance in perceptual decision making</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e96511</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0096511</pub-id><pub-id pub-id-type="pmid">24816801</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="report"><person-group person-group-type="author"><name><surname>Drugowitsch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2015">2015</year><source>Notes on normative solutions to the speed-accuracy trade-off in preceptual decision-making</source><publisher-name>FENS-Hertie Winter School</publisher-name></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Einfalt</surname><given-names>LM</given-names></name><name><surname>Grace</surname><given-names>EJ</given-names></name><name><surname>Wahl</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Effects of simulated light intensity, habitat complexity and forage type on predator-prey interactions in walleye sander vitreus</article-title><source>Ecology of Freshwater Fish</source><volume>21</volume><fpage>560</fpage><lpage>569</lpage><pub-id pub-id-type="doi">10.1111/j.1600-0633.2012.00576.x</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Eissa</surname><given-names>TL</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Suboptimal Human Inference Inverts the Bias-Variance Trade-off for Decisions with Asymmetric Evidence</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.12.06.413591</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>NJ</given-names></name><name><surname>Trueblood</surname><given-names>JS</given-names></name><name><surname>Holmes</surname><given-names>WR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A parameter recovery assessment of time-variant models of decision-making</article-title><source>Behavior Research Methods</source><volume>52</volume><fpage>193</fpage><lpage>206</lpage><pub-id pub-id-type="doi">10.3758/s13428-019-01218-0</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Faisal</surname><given-names>AA</given-names></name><name><surname>Selen</surname><given-names>LPJ</given-names></name><name><surname>Wolpert</surname><given-names>DM</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Noise in the nervous system</article-title><source>Nature Reviews. Neuroscience</source><volume>9</volume><fpage>292</fpage><lpage>303</lpage><pub-id pub-id-type="doi">10.1038/nrn2258</pub-id><pub-id pub-id-type="pmid">18319728</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Frazier</surname><given-names>P</given-names></name><name><surname>Yu</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Sequential Hypothesis Testing under Stochastic Deadlines</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glaze</surname><given-names>CM</given-names></name><name><surname>Kable</surname><given-names>JW</given-names></name><name><surname>Gold</surname><given-names>JI</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Normative evidence accumulation in unpredictable environments</article-title><source>eLife</source><volume>4</volume><elocation-id>e08825</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.08825</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glickman</surname><given-names>M</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Evidence integration and decision confidence are modulated by stimulus consistency</article-title><source>Nature Human Behaviour</source><volume>6</volume><fpage>988</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1038/s41562-022-01318-6</pub-id><pub-id pub-id-type="pmid">35379981</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Banburismus and the brain: decoding the relationship between sensory stimuli, decisions, and reward</article-title><source>Neuron</source><volume>36</volume><fpage>299</fpage><lpage>308</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)00971-6</pub-id><pub-id pub-id-type="pmid">12383783</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gold</surname><given-names>JI</given-names></name><name><surname>Shadlen</surname><given-names>MN</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>The neural basis of decision making</article-title><source>Annual Review of Neuroscience</source><volume>30</volume><fpage>535</fpage><lpage>574</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.113038</pub-id><pub-id pub-id-type="pmid">17600525</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hanks</surname><given-names>TD</given-names></name><name><surname>Kopec</surname><given-names>CD</given-names></name><name><surname>Brunton</surname><given-names>BW</given-names></name><name><surname>Duan</surname><given-names>CA</given-names></name><name><surname>Erlich</surname><given-names>JC</given-names></name><name><surname>Brody</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Distinct relationships of parietal and prefrontal cortices to evidence accumulation</article-title><source>Nature</source><volume>520</volume><fpage>220</fpage><lpage>223</lpage><pub-id pub-id-type="doi">10.1038/nature14066</pub-id><pub-id pub-id-type="pmid">25600270</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jun</surname><given-names>EJ</given-names></name><name><surname>Bautista</surname><given-names>AR</given-names></name><name><surname>Nunez</surname><given-names>MD</given-names></name><name><surname>Allen</surname><given-names>DC</given-names></name><name><surname>Tak</surname><given-names>JH</given-names></name><name><surname>Alvarez</surname><given-names>E</given-names></name><name><surname>Basso</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Causal role for the primate superior colliculus in the computation of evidence for perceptual decisions</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1121</fpage><lpage>1131</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00878-6</pub-id><pub-id pub-id-type="pmid">34183869</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name><name><surname>Holmes</surname><given-names>WR</given-names></name><name><surname>Eissa</surname><given-names>TL</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Optimal models of decision-making in dynamic environments</article-title><source>Current Opinion in Neurobiology</source><volume>58</volume><fpage>54</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1016/j.conb.2019.06.006</pub-id><pub-id pub-id-type="pmid">31326724</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Louie</surname><given-names>K</given-names></name><name><surname>Glimcher</surname><given-names>PW</given-names></name><name><surname>Webb</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Adaptive neural coding: from biological to behavioral decision-making</article-title><source>Current Opinion in Behavioral Sciences</source><volume>5</volume><fpage>91</fpage><lpage>99</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2015.08.008</pub-id><pub-id pub-id-type="pmid">26722666</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ma</surname><given-names>WJ</given-names></name><name><surname>Jazayeri</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Neural coding of uncertainty and probability</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>205</fpage><lpage>220</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-071013-014017</pub-id><pub-id pub-id-type="pmid">25032495</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mahadevan</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Average reward reinforcement learning: foundations, algorithms, and empirical results</article-title><source>Machine Learning</source><volume>22</volume><fpage>159</fpage><lpage>195</lpage><pub-id pub-id-type="doi">10.1007/BF00114727</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>Leslie</surname><given-names>DS</given-names></name><name><surname>Ludwig</surname><given-names>CJH</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Overcoming indecision by changing the decision boundary</article-title><source>Journal of Experimental Psychology. General</source><volume>146</volume><fpage>776</fpage><lpage>805</lpage><pub-id pub-id-type="doi">10.1037/xge0000286</pub-id><pub-id pub-id-type="pmid">28406682</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Malhotra</surname><given-names>G</given-names></name><name><surname>Leslie</surname><given-names>DS</given-names></name><name><surname>Ludwig</surname><given-names>CJH</given-names></name><name><surname>Bogacz</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Time-Varying decision boundaries: insights from optimality analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>971</fpage><lpage>996</lpage><pub-id pub-id-type="doi">10.3758/s13423-017-1340-6</pub-id><pub-id pub-id-type="pmid">28730465</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ossmy</surname><given-names>O</given-names></name><name><surname>Moran</surname><given-names>R</given-names></name><name><surname>Pfeffer</surname><given-names>T</given-names></name><name><surname>Tsetsos</surname><given-names>K</given-names></name><name><surname>Usher</surname><given-names>M</given-names></name><name><surname>Donner</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The timescale of perceptual evidence integration can be adapted to the environment</article-title><source>Current Biology</source><volume>23</volume><fpage>981</fpage><lpage>986</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2013.04.039</pub-id><pub-id pub-id-type="pmid">23684972</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Palestro</surname><given-names>JJ</given-names></name><name><surname>Weichart</surname><given-names>E</given-names></name><name><surname>Sederberg</surname><given-names>PB</given-names></name><name><surname>Turner</surname><given-names>BM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Some task demands induce collapsing bounds: evidence from a behavioral analysis</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>1225</fpage><lpage>1248</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1479-9</pub-id><pub-id pub-id-type="pmid">29845433</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Philiastides</surname><given-names>MG</given-names></name><name><surname>Ratcliff</surname><given-names>R</given-names></name><name><surname>Sajda</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Neural representation of task difficulty and decision making during perceptual categorization: a timing diagram</article-title><source>The Journal of Neuroscience</source><volume>26</volume><fpage>8965</fpage><lpage>8975</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1655-06.2006</pub-id><pub-id pub-id-type="pmid">16943552</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Radillo</surname><given-names>AE</given-names></name><name><surname>Veliz-Cuba</surname><given-names>A</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Performance of Normative and Approximate Evidence Accumulation on the Dynamic Clicks Task</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/541045</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rapoport</surname><given-names>A</given-names></name><name><surname>Burkheimer</surname><given-names>GJ</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>Models for deferred decision making</article-title><source>Journal of Mathematical Psychology</source><volume>8</volume><fpage>508</fpage><lpage>538</lpage><pub-id pub-id-type="doi">10.1016/0022-2496(71)90005-8</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ratcliff</surname><given-names>R</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>A theory of memory retrieval</article-title><source>Psychological Review</source><volume>85</volume><fpage>59</fpage><lpage>108</lpage><pub-id pub-id-type="doi">10.1037/0033-295X.85.2.59</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shinn</surname><given-names>M</given-names></name><name><surname>Lam</surname><given-names>NH</given-names></name><name><surname>Murray</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A flexible framework for simulating and fitting generalized drift-diffusion models</article-title><source>eLife</source><volume>9</volume><elocation-id>e56938</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.56938</pub-id><pub-id pub-id-type="pmid">32749218</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simen</surname><given-names>P</given-names></name><name><surname>Contreras</surname><given-names>D</given-names></name><name><surname>Buck</surname><given-names>C</given-names></name><name><surname>Hu</surname><given-names>P</given-names></name><name><surname>Holmes</surname><given-names>P</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Reward rate optimization in two-alternative decision making: empirical tests of theoretical predictions</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>35</volume><fpage>1865</fpage><lpage>1897</lpage><pub-id pub-id-type="doi">10.1037/a0016926</pub-id><pub-id pub-id-type="pmid">19968441</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Reinforcement learning: an introduction</article-title><source>IEEE Transactions on Neural Networks</source><volume>9</volume><elocation-id>1054</elocation-id><pub-id pub-id-type="doi">10.1109/TNN.1998.712192</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Optimal policy for value-based decision-making</article-title><source>Nature Communications</source><volume>7</volume><elocation-id>12400</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms12400</pub-id><pub-id pub-id-type="pmid">27535638</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tajima</surname><given-names>S</given-names></name><name><surname>Drugowitsch</surname><given-names>J</given-names></name><name><surname>Patel</surname><given-names>N</given-names></name><name><surname>Pouget</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Optimal policy for multi-alternative decisions</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1503</fpage><lpage>1511</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0453-9</pub-id><pub-id pub-id-type="pmid">31384015</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Thresh</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>AdaptNormThresh</data-title><version designator="swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2">swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:bbb89f81d744a0e8835a6f6791e8b653b7d431be;origin=https://github.com/nwbarendregt/AdaptNormThresh;visit=swh:1:snp:a568cc5cff39965992895669038ed8c43a59eedd;anchor=swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2">https://archive.softwareheritage.org/swh:1:dir:bbb89f81d744a0e8835a6f6791e8b653b7d431be;origin=https://github.com/nwbarendregt/AdaptNormThresh;visit=swh:1:snp:a568cc5cff39965992895669038ed8c43a59eedd;anchor=swh:1:rev:2878a3d9f5a3b9b89a0084a897bef3414e9de4a2</ext-link></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Beauregard-Racine</surname><given-names>J</given-names></name><name><surname>Fradet</surname><given-names>CW</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Decision making by urgency gating: theory and experimental support</article-title><source>Journal of Neurophysiology</source><volume>108</volume><fpage>2912</fpage><lpage>2930</lpage><pub-id pub-id-type="doi">10.1152/jn.01071.2011</pub-id><pub-id pub-id-type="pmid">22993260</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Cos</surname><given-names>I</given-names></name><name><surname>Trung</surname><given-names>J</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Context-Dependent urgency influences speed-accuracy trade-offs in decision-making and movement execution</article-title><source>The Journal of Neuroscience</source><volume>34</volume><fpage>16442</fpage><lpage>16454</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0162-14.2014</pub-id><pub-id pub-id-type="pmid">25471582</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Modulation of premotor and primary motor cortical activity during volitional adjustments of speed-accuracy trade-offs</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>938</fpage><lpage>956</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2230-15.2016</pub-id><pub-id pub-id-type="pmid">26791222</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The basal ganglia do not select reach targets but control the urgency of commitment</article-title><source>Neuron</source><volume>95</volume><fpage>1160</fpage><lpage>1170</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.07.039</pub-id><pub-id pub-id-type="pmid">28823728</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thura</surname><given-names>D</given-names></name><name><surname>Cisek</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Microstimulation of dorsal premotor and primary motor cortex delays the volitional commitment to an action choice</article-title><source>Journal of Neurophysiology</source><volume>123</volume><fpage>927</fpage><lpage>935</lpage><pub-id pub-id-type="doi">10.1152/jn.00682.2019</pub-id><pub-id pub-id-type="pmid">31995433</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Trueblood</surname><given-names>JS</given-names></name><name><surname>Heathcote</surname><given-names>A</given-names></name><name><surname>Evans</surname><given-names>NJ</given-names></name><name><surname>Holmes</surname><given-names>WR</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Urgency, leakage, and the relative nature of information processing in decision-making</article-title><source>Psychological Review</source><volume>128</volume><fpage>160</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1037/rev0000255</pub-id><pub-id pub-id-type="pmid">32852976</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Veliz-Cuba</surname><given-names>A</given-names></name><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name><name><surname>Josić</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Stochastic models of evidence accumulation in changing environments</article-title><source>SIAM Review</source><volume>58</volume><fpage>264</fpage><lpage>289</lpage><pub-id pub-id-type="doi">10.1137/15M1028443</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wald</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1945">1945</year><article-title>Sequential tests of statistical hypotheses</article-title><source>The Annals of Mathematical Statistics</source><volume>16</volume><fpage>117</fpage><lpage>186</lpage><pub-id pub-id-type="doi">10.1214/aoms/1177731118</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79824.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Latham</surname><given-names>Peter</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.04.27.489722" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.27.489722"/></front-stub><body><p>This paper makes an important contribution to the study of decision-making under time pressure. The authors provide convincing evidence that decision boundaries can be highly nontrivial – even reaching infinity in realistic regimes. This paper will be of broad interest to both experimentalists and theorists working on decision-making under time pressure.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79824.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Latham</surname><given-names>Peter</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02jx3x895</institution-id><institution>University College London</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Malhotra</surname><given-names>Gaurav</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/0524sp257</institution-id><institution>University of Bristol</institution></institution-wrap><country>United Kingdom</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.04.27.489722">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.04.27.489722v2">the preprint</ext-link> for the benefit of readers; ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Normative Decision Rules in Changing Environments&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen Timothy Behrens as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Gaurav Malhotra (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>All three reviewers very much liked the paper. It was nice to see the formalism used to solve these problems take a central place in the manuscript, and the huge variability in bounds is something we haven't seen before.</p><p>But that didn't stop us from making a huge number of comments – you have either the good luck or the bad luck, depending on your point of view, of being reviewed by experts. Most comments have to do with the presentation: important information was missing (or at least we couldn't find it), and there were even places where we got lost (not a good sign, given that all three of us work in the field). Details follow.</p><p>I. The tokens task can be analyzed using the formalism introduced in Equation (3), but it seems pretty far from the &quot;dynamic context&quot; examples emphasized in the bulk of the paper. That doesn't mean the tokens task shouldn't be included. But it does mean we have no evidence one way or the other whether subjects would adopt the highly idiosyncratic boundaries found in simulations (for instance, the infinite threshold boundaries in Figures 2i, 2ii).</p><p>You need to be clear about this. The way the paper reads, it sounds like you have provided evidence for the dynamic context setup, when in fact that's not the case. It should be crystal clear that dynamic context problems have not been explored, at least not in the lab. Instead, what you showed is that a normative model can beat a particular heuristic model.</p><p>II. The following are technical but important.</p><p>1. Adding noise: you are currently optimizing the model parameters/policy before adding sensory and motor noise. Decision-makers could be aware of sensory noise and so could try to optimize their decision processes with that knowledge. Would you be able to also compare the models' performances if they have been optimized to maximize performance in the presence of all noise? From our understanding, this should be feasible for the Const and UGM model, but might be harder for the normative model. Sensory noise could be included by finding Equation (13) that includes such noise, but finding the optimal thresholds once RT noise is included might be prohibitive. This is just a suggestion, not an essential inclusion. However, it might be worth at least discussing the difference between what you do, and being clear on the scenario you considered.</p><p>2. Fitting token task data: according to Cisek et al. (2009), the same participants performed both the slow and the fast version of the task. However, their fitted reward magnitudes differ by an order of magnitude between the two conditions (your Figure 6C/F). Is it just that the fitting objective didn't well-constrain these parameters? Given that you use MCMC for model fits, you could compare the parameter posteriors across conditions. Furthermore, how much worse would the model fits become if you would fit both conditions simultaneously and share all parameters that can be meaningfully shared across conditions? In any case, an explanation for this difference should be provided in the manuscript.</p><p>3. The dynamic context examples would seem to apply only when subjects take many seconds to make a decision. This would seem to rule out perceptual decision-making tasks. Is this true? If so, you should be upfront about this – so that those who work on perceptual decision-making will know what they're getting into.</p><p>4. A known and predictable change in the middle of a task seems somewhat unrealistic. Given that it plays such a central role, concrete examples where this comes up would be very helpful. Or at least you should make a proposal for laboratory experiments where it could come up. The examples in the introduction (&quot;Some of these factors can change quickly and affect our deliberations in real time; e.g., an unexpected shower will send us hurrying down the faster route (Figure 1A), whereas spotting a new ice cream store can make the longer route more attractive.&quot;) don't quite fall into the &quot;known and predictable change&quot; category.</p><p>III. Better contact with existing literature needs to be made. For instance:</p><p>1. Drugowitsch, Moreno-Bote and Pouget (2014) already computed normative decision policies for time-varying SNR, with the difference that they assumed the SNR to follow a stochastic process rather than a known, deterministic time course. Thus, the work is closely related, but not equivalent.</p><p>2. Some early models to predict dynamic decision boundaries were proposed by Busemeyer and Rapoport (1988) and Rapoport and Burkheimer (1971) in the context of a deferred decision-making task.</p><p>3. One of the earliest models to use dynamic programming to predict non-constant decision boundaries was Frazier and Yu (2007). Indeed some boundaries predicted by the authors (e.g. Figure 2v) are very similar to boundaries predicted by this model. In fact, the switch from high to low reward used to propose boundaries in Figure 2v can be seen as a &quot;softer&quot; version of the deadline task in Frazier and Yu (2007).</p><p>4. Another early observation that time-varying boundaries can account for empirical data was made by Ditterich (2006). Seems highly relevant to the authors' predictions, but is not cited.</p><p>5. The authors seem to imply that their results are the first results showing non-monotonic thresholds. This is not true. See, for example, Malhotra et al. (2018). What is novel here is the specific shape of these non-monotonic boundaries.</p><p>IV. Clarity could be massively improved. If you want to write an unclear paper that is your prerogative. However, if you do, you can't say &quot;Our results can aid experimentalists investigating the nuances of complex decision-making in several ways&quot;. It would be difficult to aid experimentalists if they have to struggle to understand the paper.</p><p>Below are comments collected from the three reviewers, and more or less collated (so it's possible there's some overlap, and the order isn't exactly optimized). You can, in fact, almost ignore them, if you take into account the main message: all information should be easily accessible, in the main text, and the figures should be easy to make sense of.</p><p>As authors, we are aware that the length of replies can sometimes exceed the paper, which is not a good use of anybody's time. Please use your judgment as to which ones you reply to. For instance, if you're going to implement our suggestions, no reason to tell us. Maybe comment only if you have a major objection? Or use some other scheme? What we really care about is that the revised paper is easy to read!</p><p>1. When the UGM was introduced, all you say is &quot;urgency-gating models (UGMs) use thresholds that collapse monotonically over time&quot;. You include some references, but for the casual reader, it looks like you're considering a generic collapsing bound model. In fact, you're considering a particular shape for the collapsing bound and particular filtering of the evidence. This should be clear. It also needs to be justified. For instance, Voskuilen et al. (J. Math. Psych. 73:59-79, 2016) use a different functional form for the collapsing bound, and they don't filter the evidence. Why use one model over another?</p><p>And while we're on the topic of the UGM: Equation (4) low-pass filters the noise-free observer's belief y that reflects all accumulated evidence up to current time t. According to our reading of Cisek et al. (2009), the UGM low-pass filters the momentary internal estimate of sensory information (the Ei(tau) defined below Equation (1); Equations. (17)-(19) for the low-pass filter in Cisek et al.) rather than the accumulated estimate of sensory information. Are we misinterpreting Cisek et al. (2009) or your Equation. (4)? Either way, please clarify.</p><p>In Equation. 4 it would be more clear to put -E + 0.5*tanh(y) on the RHS. What's the justification for tanh? Why not just filter y? Do you use tanh because the original paper did? If so, you should point that out.</p><p>Also, what's y in that equation?</p><p>2. Important inline equations need to be displayed. There's nothing more annoying than having to crawl through text to look for the definition of an important symbol. To take a few (hardly exhaustive) examples: <inline-formula><mml:math id="sa1m1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>f</mml:mi><mml:mo>±</mml:mo></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo><mml:mi>y</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mi>p</mml:mi></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. The actual list is much longer. If any symbol is going to be used again, please make it easy to find! This in itself is a reason for displayed equations: you can refer to equation numbers when introducing variables that you haven't used for several pages.</p><p>3. A lot of the lines don't have line numbers, which is relevant mainly for us, since it's hard to refer to things without line numbers. This is a bug, but there's a way to fix it. I think (but I'm not sure) that in your latex file you need to leave a space between equations and surrounding text. (Or maybe no space? It's been a while). Although I believe there's a more elegant fix.</p><p>4. Not all equations were numbered. We know, in some conventions only equations one refers to are numbered (that's what one of us grew up with), but it turns out to be not so convenient for us as reviewers when we want to refer to an un-numbered equation.</p><p>5. Lines 43-6: &quot;Efforts to model decision-making thresholds under dynamic conditions have focused largely on heuristic strategies. For instance, &quot;urgency-gating models&quot; (UGMs) use thresholds that collapse monotonically over time (equivalent to dilating the belief in time) to explain decisions based on time-varying evidence quality&quot;.</p><p>In fact, a collapsing bound is not necessarily a heuristic; it can be optimal, although the exact shape of the collapsing bound has to be found by dynamic programming. Please reword to reflect this.</p><p>6. Line 76: c(t) is barely motivated at all here. It's better motivated in Methods, but its value is very hard to justify. Why not stick with optimizing average reward, for which c=0? And I don't think you ever told us what c(t) was; only that it was constant (although we could have missed its value).</p><p>7. Figure 2C would be easier to make sense of if it were square.</p><p>8. In general, information is scattered all over the place, and much of it seems to be missing. Each task should be described succinctly in the main text, with enough information to actually figure out what's going on. In addition, there should be a table listing _all_ the parameters; right now the reader has to go to Methods, and even then it seems that many are missing. For instance, we don't think we were ever told the value of tau in Equation. 4.</p><p>9. Lots of questions/comments about Figure 4:</p><p>a. It would be very helpful to include the optimal model. I think NB is the optimal model when σ_y=0, but I also believe that in most panels σ_y \ne 0.</p><p>b. It would be helpful to emphasize, in the figure caption, that NB with σ_y = 0 is the optimal model. Assuming that's true.</p><p>c. Figure 4A: What's the post-reward rate? And please indicate the pre-reward rate at which pre-reward = post-reward. Also, If pre and post-reward rates sum to 11 (as mentioned in Methods, line 411), why are the curves' minima at around 5 rather than 5.5?</p><p>g. Figure 4B: horizontal axis label missing (presumably &quot;pre reward&quot;?). And we assume you used the following color code: Orange: reward(NB)-reward(Const); violet: reward(NB)-reward(UGM). Correct? Either way, this should be stated in the figure caption.</p><p>e. Figure 4C: what are the pre and post-rewards? And presumably noise strength = σ_y? This should be stated clearly. And more explanation, in the main text, of what &quot;noise strength&quot; is would help.</p><p>f. Figure 4F: It is not clear to us why UGM in 0 noise condition have RTs aligned to the time reward increases from <inline-formula><mml:math id="sa1m2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to <inline-formula><mml:math id="sa1m3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Surely, this model does not take RR into account to compute the thresholds, does it? In fact, looking at Figure 4B, Supplement 1, the thresholds are always highest at t=0. Please clarify.</p><p>10. Lines 207-9: &quot;Because the total number of tokens was finite and known to the subject, token movements varied in their informativeness within a trial, yielding a dynamic and history-dependent evidence quality that, in principle, could benefit from adaptive decision processes&quot;.</p><p>To us, &quot;history-dependent&quot; implies non-Markov, whereas the tokens task is Markov. But maybe that's not what history-dependent means here? This should be clarified.</p><p>11. We assume the y-axis in Figure 5i-iv is the difference between the number of tokens on the top and the number on the bottom. This should be stated (if it's true). And please explain how you differentiate between motifs iii and iv. We believe it's the presence of two threshold increases (rather than just one) in motif iv, but we're not sure.</p><p>12. What's the reward/punishment structure for the tokens task? It seemed that this was only half explained.</p><p>13. Lines 229-232: &quot;To determine the relevance of these adaptive decision strategies to human behavior, we fit discrete-time versions of the noisy Bayesian (four free parameters), constant-threshold (three free parameters), and urgency-gating (five free parameters) models to response-time data from the tokens task collected by Cisek et al. (2009).&quot;</p><p>As mentioned above, the parameters should go in a table.</p><p>14. You should tell us what V(T_final) is, and why. We believe it's the same as V(0), but We could be wrong.</p><p>15. After Equation. 11: it says m = 2 mu<sup>2</sup>/σ<sup>2</sup>. Are these mu and σ different than the ones on line 383? If so, that should be clear. (If not, we're lost.)</p><p>16. We looked, but couldn't find, the definition of f_p. We believe it's just a conditional probability,</p><p>f_p(p_{n+1}|p_n) = P(p_{n+1}|p_n).</p><p>If so, why not use that notation? It would be a lot easier to remember. In any case, when this is used, please tell us what it is, or where it was originally defined (which should be in a displayed equation!).</p><p>17. State space is parameterized by p_n, and that needs to be discretized, right? If so, that's worth mentioning. If not, we're lost.</p><p>18. Analysis (in particular Equation. 13) would be a lot easier if you used y_n instead of p_n. y_n is what is generally accumulated in DDMs, and it's what you generally plot on the y-axis. So why use p_n?</p><p>19. Equations. 14 and 15 should really be in the main text. They're simple and important.</p><p>20. We didn't understand the inferred reward change task, in the text starting after line 393. We might have been able to guess, but please put in equations so it's crystal clear.</p><p>21. Somewhere below line 404: &quot;a constant threshold … is predicted to be optimal only in simple, static decision environments.&quot; It's worth pointing out that the decision environments have to be _very_ simple. Even adding one more mean induces a non-constant (and typically collapsing) bound.</p><p>22. Equation above line 405: why repeat that equation, and not repeat Equation. 4? Just curious.</p><p>23. Lines 409-11: Couldn't parse.</p><p>24. After line 411, we find out that R1+R2=11. This is important and simple; you should tell us in the main text.</p><p>25. After line 411: we couldn't parse &quot;allowing us to find the exact tuning of the normative model.&quot;</p><p>26. In fact, we're lost in pretty much everything between line 411 and the tokens task.</p><p>27. Line 429: what's &quot;post-decision token acceleration&quot;?</p><p>28. Line 433: &quot;We used three models to fit the subject response data …&quot;. As far as we could tell, the three models are continuous time models. How were they adapted to this task, which runs in discrete time? Is it just a matter of making the time step larger?</p><p>29. Lines 432-434: please be more clear about parameter counting -- by listing parameters.</p><p>30. Lines 437-8: &quot;For more details as to our specific implementation of MCMC for this data, see the MATLAB code available at https://github.com/nwbarendregt/AdaptNormThresh&quot;.</p><p>We shouldn't have to look at code to get details; all important details should be in the paper.</p><p>31. Figure 2—figure supplement 2 and Figure 3—figure supplement 1: we thought the reward changed only once. But it's changing a lot in panel A. What's going on?</p><p>32. The Abstract / Introduction isn't clear enough about what you refer to as a &quot;changing / dynamic environment&quot;. In particular, there is a rich history of research on environments whose state changes across decisions rather than within individual decisions. Making this distinction explicit, and clarifying that you care about the latter rather than the former should make Abstract / Intro clearer.</p><p>33. In the text around Equation. (2), you should mention that you're assuming independence across time.</p><p>34. Equation. (3): should c(dt) really be c(t)dt? Its dependence on only the time step size seems incompatible with its initial definition in line 77, where it depends on time t since trial onset. Although eventually, it does become a constant.</p><p>35. Below Equation. (3): &quot;We choose generating distributions f_+/- that allow us to explicitly compute the average future value […]&quot; – can you compute the average future value explicitly, or just f_p(p_n+1 | p_n)? Methods only discuss the latter.</p><p>36. Figures 2 and 3: the assumed reward/cost magnitudes should be mentioned in the main text, and also if the results were qualitatively dependent on these magnitudes (we assume not?).</p><p>37. Figure 2B: &quot;belief&quot; in Bayesian statistics usually refers to a posterior probability, whereas you seem to be using it to refer to log-posterior odds (or log-odds). Please clarify in the text what you mean by &quot;belief&quot; (if you haven't done so already and we missed it). This also refers to Figure 3B and clarifies what the thresholds are on in Figures3/4/5.</p><p>38. Figures2C/3C: the letter placements are slightly unclear. In particular, in Figure 2C it is hard to see where exactly 'iv' is placed. Maybe using labeled dots instead would increase placement precision?</p><p>39. Line 130: &quot;[…] in which reward fluctuations are governed by a two-state Markov process […]&quot;. We couldn't figure out from the description in the main text what setup you are referring to and how to interpret Figure 2 – suppl 3. Please provide more detail (not just in Methods) on the reward switching process: what information is provided to the decision-maker to infer its state, etc.</p><p>40. below Line 156: we got lost in the notation for the different noisy / noise-less accumulator models. y_tilde appears to be accumulation with added sensory noise but is in the second point referred to as the &quot;belief y_tilde [of the] normative model&quot;, which, being normative, presumably wouldn't have sensory noise. Furthermore, the UGM model seems to use the &quot;noise-free observer's belief y&quot;. Is that the belief as defined in Equation. (2) which still includes the sample noise, such that calling it &quot;noise-free&quot; might be confusing?</p><p>41. Starting on line 169: the text is unclear on how the models are tuned to cope with the noise, if at all. How the model parameters of the Const and UGM are chosen should also be mentioned in the main text, not just Methods – in particular, that they are tuned to maximize decision performance.</p><p>42. Line 332: &quot;+- theta&quot; – missing &quot;(t)&quot;?</p><p>43. Line 333: &quot;where observations every dt time units&quot; – fragment?</p><p>44. Equation. (10): shouldn't V+ / V- / Vw also be functions of rho?</p><p>45. The equation above Equation. (12): how is the expected future value computed? I assume that this can only be done numerically? Either way, please specify the details of how you do so. Referring to a Github repo isn't sufficient.</p><p>45. The evidence setup that leads to Equation. (13) appears to be equivalent to the one leading to Equation. (16) in Drugowitsch et al. (2012) for M=1. Is this correct? If yes, is the result equivalent? Either way, the relationship would be worth pointing out.</p><p>46. Line 411: &quot;the measured response time T_tilde was drawn from a normal distribution […]&quot; – what happened for predicted response times &lt;0? Did you truncate the normal distribution at 0?</p><p>47. Line 432: what was the objective function for the MCMC fits? The joint likelihood of RTs and choices?</p><p>48. One of the more realistic scenarios is presented in Figure 2—figure supplement 3, where reward doesn't switch at a fixed time, but uses instead a Markov process. But you do not provide enough details of the task or the results. Is m_R = R_H / R_L? Is it the dark line that corresponds to m_R=\inf (as indicated by legend) or the dotted line (as indicated by caption)? For what value of drift are these thresholds derived? These details should be included.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.79824.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>I. The tokens task can be analyzed using the formalism introduced in Equation. (3), but it seems pretty far from the &quot;dynamic context&quot; examples emphasized in the bulk of the paper. That doesn't mean the tokens task shouldn't be included. But it does mean we have no evidence one way or the other whether subjects would adopt the highly idiosyncratic boundaries found in simulations (for instance, the infinite threshold boundaries in Figures 2i, 2ii).</p><p>You need to be clear about this. The way the paper reads, it sounds like you have provided evidence for the dynamic context setup, when in fact that's not the case. It should be crystal clear that dynamic context problems have not been explored, at least not in the lab. Instead, what you showed is that a normative model can beat a particular heuristic model.</p></disp-quote><p>We have revised the text substantially to clarify and expand upon these important points. Specifically, we:</p><p>a. More clearly define the broad set of possible “dynamic context” conditions, including changes in outcome expectations or evidence quality while the evidence is being processed, where the changes can be either: (1) abrupt, as in the reward-change and SNR-change tasks we introduce, which we analyze only theoretically, or (2) gradual, as in the evidence quality changes in the tokens task, which we analyze theoretically and experimentally (e.g., in Results: Even for such simple tasks, there is a broad set of possible dynamic contexts. In the next section, we will analyze a task where context changes gradually (the tokens task)). Here we focus on tasks where the context changes abruptly.</p><p>b. Explain that our theoretical framework is general enough to account for both abrupt and gradual changes clarify that our analysis of data from the tokens task shows that the behavior of subjects is better described by a noisy normative model than by previously considered alternatives applied to that particular form of a dynamic-context task. We also state explicitly that more work is needed to determine if and how people follow normative principles for other dynamic-context tasks, …</p><disp-quote content-type="editor-comment"><p>II. The following are technical but important.</p><p>1. Adding noise: you are currently optimizing the model parameters/policy before adding sensory and motor noise. Decision-makers could be aware of sensory noise and so could try to optimize their decision processes with that knowledge. Would you be able to also compare the models' performances if they have been optimized to maximize performance in the presence of all noise? From our understanding, this should be feasible for the Const and UGM model, but might be harder for the normative model. Sensory noise could be included by finding Equation. (13) that includes such noise, but finding the optimal thresholds once RT noise is included might be prohibitive. This is just a suggestion, not an essential inclusion. However, it might be worth at least discussing the difference between what you do, and being clear on the scenario you considered.</p></disp-quote><p>We appreciate these important points and now consider them in the revised Discussion. However, we have chosen not to extend our analyses, for several reasons: (1) An optimal observer without internal sensory and motor noise gives the best possible responses, and thus provides a useful benchmark; and (2) we fear that adding results that define optimality with respect to internal sensory and motor noise would, because of the assumptions we would have to make about both the nature and knowledge of those noise sources, be distracting as well as much more speculative and thus make the paper harder to follow.</p><p>We have updated the Methods section to highlight these points:</p><p>“We have chosen to add these two sources of noise after optimizing each model to maximize average reward rate, rather than reoptimizing each model after adding these additional noise sources. Although we could have reoptimized each model to maximize performance across noise realizations, we were interested in how the models responded to perturbations that drove their performance to be sub-optimal (but possibly near-optimal).”</p><p>as well as the Discussion:</p><p>“Task-relevant variability can also arise from internal sources, including noise in neural processing of sensory input and motor output (Ma and Jazayeri, 2014; Faisal et al., 2008). We assumed subjects do not have precise knowledge of the strength or nature of these noise sources, and thus they could not optimize their strategy accordingly. However, people may be capable of rapidly estimating performance error that results from such internal noise processes and adjusting on-line (Bonnen et al., 2015). To extend the models we considered, we could therefore assume that subjects can estimate the magnitude of such sensory and motor noise, and use this information to adapt their decision strategies to improve performance.”</p><disp-quote content-type="editor-comment"><p>2. Fitting token task data: according to Cisek et al. (2009), the same participants performed both the slow and the fast version of the task. However, their fitted reward magnitudes differ by an order of magnitude between the two conditions (your Figure 6C/F). Is it just that the fitting objective didn't well-constrain these parameters? Given that you use MCMC for model fits, you could compare the parameter posteriors across conditions. Furthermore, how much worse would the model fits become if you would fit both conditions simultaneously and share all parameters that can be meaningfully shared across conditions? In any case, an explanation for this difference should be provided in the manuscript.</p></disp-quote><p>We now include a supplementary figure (Figure 6—figure supplement 2) comparing the posteriors across conditions as well as reward magnitudes in the slow and fast versions of the tokens task for a representative subject. The maximum likelihood estimate of the reward magnitude tended to be much higher in the slow task than in the fast task. It appears that subjects thus use distinct strategies in the two contexts, which we do not find surprising. We therefore do not expect to obtain fits of the same quality if we assume that subjective reward magnitude is the same across conditions. We speculate that subjects may value reward more in the slow task because it is obtained less frequently. Related effects have been attributed to amplified dopamine responses when rewards are rare (Rothenhoefer et al. 2021 Nat Neurosci). We added text to the Results section to point out this interesting finding:</p><p>“This result also shows that, assuming subjects used a normative model, they used distinct model parameters, and thus different strategies, for both the fast and slow task conditions. This finding is clearer when looking at the posterior parameter distribution for each subject and model parameter (see Figure 6—figure supplement 1 for an example). We speculate that the higher estimated value of reward in the slow task may arise due to subjects valuing frequent rewards more favorably.”</p><disp-quote content-type="editor-comment"><p>3. The dynamic context examples would seem to apply only when subjects take many seconds to make a decision. This would seem to rule out perceptual decision-making tasks. Is this true? If so, you should be upfront about this – so that those who work on perceptual decision-making will know what they're getting into.</p></disp-quote><p>We disagree. The impact of normative decision rules is relevant even on shorter timescales, including those relevant to perceptual decisions (e.g., on the order of 100 ms). Figure 2 —figure supplement 2 and Figure 3 —figure supplement 1 demonstrate that even though normative decision rules may invoke plans across multiple context changepoints, often decisions are made within the 1st or 2nd changepoint, and the corresponding reaction time distributions would have a character distinct from those emerging from strategies with flat decision thresholds. Moreover, there is ample evidence that subjects are capable of adapting perceptual evidence integration to sub-second timescales (Ossmy et al. 2013; Glaze et al. 2015). We thus speculate that perceptual decision rules could adapt on similar timescales as predicted by our normative models.</p><p>We have updated the Discussion to clarify these points:</p><p>“Perceptual decision-making tasks provide a readily accessible route for validating this theory, especially considering the ease with which task difficulty can be parameterized to identify parameter ranges in which strategies can best be differentiated (Philiastides et al. 2006). There is ample evidence already that people can tune the timescale of leaky evidence accumulation processes to the switching rate of an unpredictably changing state governing the statistics of a visual stimulus, to efficiently integrate observations and make a decision about the state (Ossmy et al. 2013; Glaze et al. 2015). We thus speculate that adaptive decision rules could be identified similarly in the strategies people use to make decisions about perceptual stimuli in dynamic contexts.”</p><disp-quote content-type="editor-comment"><p>4. A known and predictable change in the middle of a task seems somewhat unrealistic. Given that it plays such a central role, concrete examples where this comes up would be very helpful. Or at least you should make a proposal for laboratory experiments where it could come up. The examples in the introduction (&quot;Some of these factors can change quickly and affect our deliberations in real time; e.g., an unexpected shower will send us hurrying down the faster route (Figure 1A), whereas spotting a new ice cream store can make the longer route more attractive.&quot;) don't quite fall into the &quot;known and predictable change&quot; category.</p></disp-quote><p>Foraging animals must often deal with unpredictable changes in light and visibility conditions, but they also adjust to predictable changes in light brought about by the variation in sunlight with time of day. Sunrise and sunset represent stereotyped changes in foraging conditions as well as necessary escape conditions for prey animals. On shorter timescales, birds and other animals seeking mates, parents, or offspring must often discriminate between two or more calls with known amplitude modulations over time. Financial traders make decisions in markets with fixed open and closing times that strongly shape trading context. Dutch auctions are structured so that an item’s cost is successively lowered until a bidder agrees to pay that amount, reflecting a predictable stair-stepping procedure for cost changes. In all these examples the quality of evidence changes in a predictable way, while the evidence remains noisy.</p><p>Concerning laboratory experiments, the first half of the paper already proposes a visual decision-making task. The experiment we analyzed could be implemented as a switching context random dot motion discrimination task with either changes in signal-to-noise (coherence) levels, or changes in reward amounts. Such changes could be signaled or consistently implemented at the same time each trial, so as to be known.</p><p>We now have added a sentence in the Introduction:</p><p>“People and other animals thus must cope with unpredictable changes in context, such as breaks in the weather (Grubb, 1975), as well as predictable changes that affect their observations, like the daily sunrise and sunset (McNamara et al., 1994).”</p><p>as well as a note in the Discussion to indicate the relevance of such task structures, and describe how they can be implemented in a laboratory setting:</p><p>“Model-driven experimental design can aid in identification of adaptive decision rules in practice. People commonly encounter unpredictable (e.g., an abrupt thunderstorm) and predictable (e.g., sunset) context changes when making decisions. Natural extensions of common perceptual decision tasks (e.g., random-dot motion discrimination Gold and Shadlen 2002) could include within-trial changes in stimulus signal-to-noise ratio (evidence quality) or anticipated reward payout.”</p><disp-quote content-type="editor-comment"><p>III. Better contact with existing literature needs to be made. For instance:</p><p>1. Drugowitsch, Moreno-Bote and Pouget (2014) already computed normative decision policies for time-varying SNR, with the difference that they assumed the SNR to follow a stochastic process rather than a known, deterministic time course. Thus, the work is closely related, but not equivalent.</p></disp-quote><p>Indeed we had not explained in detail the differences between their work and ours. We have now added the following sentence to the Discussion to make this clear:</p><p>“These strategies include dynamically changing decision thresholds when signal-to-noise ratios of evidence streams vary according to a Cox-Ingersoll-Ross process (Drugowitsch et al., 2014a)”</p><disp-quote content-type="editor-comment"><p>2. Some early models to predict dynamic decision boundaries were proposed by Busemeyer and Rapoport (1988) and Rapoport and Burkheimer (1971) in the context of a deferred decision-making task.</p></disp-quote><p>Thanks very much for pointing out these seminal references, which we now include in the Discussion:</p><p>“Several early normative theories were, like ours, based on dynamic programming (Rapoport and Burkheimer, 1971; Busemeyer and Rapoport, 1988), and in some cases models fit to experimental data (Ditterich, 2006).”</p><disp-quote content-type="editor-comment"><p>3. One of the earliest models to use dynamic programming to predict non-constant decision boundaries was Frazier and Yu (2007). Indeed some boundaries predicted by the authors (e.g. Figure 2v) are very similar to boundaries predicted by this model. In fact, the switch from high to low reward used to propose boundaries in Figure 2v can be seen as a &quot;softer&quot; version of the deadline task in Frazier and Yu (2007).</p></disp-quote><p>Again, we very much appreciate the pointer to the very relevant reference, which we include in the Discussion:</p><p>“For example, dynamic programming was used to show that certain optimal decisions can require non-constant decision boundaries similar to those of our normative models in dynamic reward tasks (Frazier and Yu, 2007) (Figure 2).”</p><disp-quote content-type="editor-comment"><p>4. Another early observation that time-varying boundaries can account for empirical data was made by Ditterich (2006). Seems highly relevant to the authors' predictions, but is not cited.</p></disp-quote><p>We agree and regret the oversight. We now reference that paper.</p><disp-quote content-type="editor-comment"><p>5. The authors seem to imply that their results are the first results showing non-monotonic thresholds. This is not true. See, for example, Malhotra et al. (2018). What is novel here is the specific shape of these non-monotonic boundaries.</p></disp-quote><p>As with the work by Drugowitsch et al. (2014), this work demonstrates the emergence of non-monotonic boundaries, but in tasks and settings distinct from the ones we consider (which specifically employ dynamic context). We have clarified these points in the manuscript.</p><disp-quote content-type="editor-comment"><p>IV. Clarity could be massively improved. If you want to write an unclear paper that is your prerogative. However, if you do, you can't say &quot;Our results can aid experimentalists investigating the nuances of complex decision-making in several ways&quot;. It would be difficult to aid experimentalists if they have to struggle to understand the paper.</p><p>Below are comments collected from the three reviewers, and more or less collated (so it's possible there's some overlap, and the order isn't exactly optimized). You can, in fact, almost ignore them, if you take into account the main message: all information should be easily accessible, in the main text, and the figures should be easy to make sense of.</p><p>As authors, we are aware that the length of replies can sometimes exceed the paper, which is not a good use of anybody's time. Please use your judgment as to which ones you reply to. For instance, if you're going to implement our suggestions, no reason to tell us. Maybe comment only if you have a major objection? Or use some other scheme? What we really care about is that the revised paper is easy to read!</p></disp-quote><p>Thanks for providing us with flexibility in how and to what we respond. Generally, we found all comments helpful, and so we have endeavored to make edits that address everything the reviewers brought to our attention. To simplify this letter, we include below only those points that require additional explanation. Otherwise all changes can be found in red in the revised manuscript.</p><disp-quote content-type="editor-comment"><p>6. Line 76: c(t) is barely motivated at all here. It's better motivated in Methods, but its value is very hard to justify. Why not stick with optimizing average reward, for which c=0? And I don't think you ever told us what c(t) was; only that it was constant (although we could have missed its value).</p></disp-quote><p>We have added the following motivation of the cost function <italic>c(t)</italic> to the main text:</p><p>“The incremental evidence function <italic>c(t)</italic> represents both explicit time costs, such as a price for gathering evidence, and implicit costs, such as the opportunity cost. While there are many forms of this cost function, we will make the simplifying assumption that it is constant, <italic>c(t)=c</italic>. Because more complex cost functions can influence decision threshold dynamics (Drugowitsch et al., 2012), restricting the cost function to a constant ensures that threshold dynamics are governed purely by changes in the (external) task conditions and not the (internal) cost function.”</p><p>We also specified the cost function <italic>c(t)</italic> = 1 that we used in Figure 2-4 in the figure captions. We revised the caption of Figure 5 to make it more clear that we are finding decision threshold motifs as a function of the cost function <italic>c</italic>:</p><p>“… B: Colormap of normative threshold dynamics for the “slow'' version of the tokens task in reward-evidence cost parameter space (i.e., as a function of <italic>R<sub>c</sub></italic> and <italic>c(t) = c</italic> from Equation 3, with punishment <italic>R<sub>i</sub></italic> set to -1). Distinct …”</p><p>We also added in more clarification to the caption of Figure 6C,F to emphasize that we are fitting the cost function <italic>c(t) = c</italic>.</p><disp-quote content-type="editor-comment"><p>10. Lines 207-9: &quot;Because the total number of tokens was finite and known to the subject, token movements varied in their informativeness within a trial, yielding a dynamic and history-dependent evidence quality that, in principle, could benefit from adaptive decision processes&quot;.</p><p>To us, &quot;history-dependent&quot; implies non-Markov, whereas the tokens task is Markov. But maybe that's not what history-dependent means here? This should be clarified.</p></disp-quote><p>Yes, the token count differential is driven by a Markov process, since there is always a 50/50 chance of the token being moved to the top or bottom target. However, the log likelihood ratio associated with either target having more tokens at the end is a non-Markovian, history-dependent process, because the possible LLR increments on each token movement are determined by the token movements so far. This subtlety does make this a dynamic context task, where the evidence quality is the context that changes gradually throughout a trial. We addressed this in our response to the major comments above as we describe the temporal dynamics of the tokens task.</p><p>“In addition, the task included two different post-decision token movement speeds, ``slow'' and ``fast'': once the subject committed to a choice, the tokens finished out their animation, moving either once every 170 ms (slow task) or once every 20 ms (fast task). This post-decision movement acceleration changed the value associated with commitment by making the average inter-trial interval (<italic>t<sub>i</sub></italic> in Equation 1) decrease over time. Because of this modulation, we can interpret the tokens task as a multi-change reward task, where commitment value is controlled through <italic>t<sub>i</sub></italic> rather than through reward <italic>R<sub>c</sub></italic>.<italic>”</italic></p><disp-quote content-type="editor-comment"><p>19. Equations. 14 and 15 should really be in the main text. They're simple and important.</p></disp-quote><p>We added the following text to include these Heaviside functions in the main text and to better motivate our investigation into single-change environments for reward:</p><p>“Environments with multiple fluctuations during a single decision lead to complex threshold dynamics, but are comprised of threshold change ``motifs.'' These motifs occur on shorter intervals and tend to emerge from simple monotonic changes in context parameters (Figure 2—figure supplement 2). To better understand the range of possible threshold motifs, we focused on environments with single changes in task parameters. For the reward-change task, we set punishment to <italic>R<sub>i</sub> = 0</italic>, and assumed reward <italic>R<sub>c</sub></italic> changes abruptly, so that its dynamics are described by a Heaviside function</p><p><inline-formula><mml:math id="sa2m4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> Thus, the reward switches from a pre-change value of <inline-formula><mml:math id="sa2m5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> to a post-change value of <inline-formula><mml:math id="sa2m6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>R</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> at <italic>t=0.5</italic>. For this single-change task, …”</p><p>and quality:</p><p>“In the SNR-change task, optimal strategies for environments with multiple fluctuations are characterized by threshold dynamics adapted to changes in evidence quality in a way similar to changes in reward (Figure 3—figure supplement 1). To study the range of possible threshold motifs, we again considered environments with single changes in evidence quality <inline-formula><mml:math id="sa2m7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>m</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msup><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:msup><mml:mi>σ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula> by taking <italic>μ</italic> to be a Heaviside function: <inline-formula><mml:math id="sa2m8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>−</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> For this single-change task, we again found similar threshold motifs to those in the reward-change task (Figure 3A,B).”</p><disp-quote content-type="editor-comment"><p>23. Lines 409-11: Couldn't parse.</p></disp-quote><p>We have revised this paragraph for clarity and to include more details and motivation:</p><p>“In addition to the inference noise with strength <inline-formula><mml:math id="sa2m9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>y</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, we also filtered each process through a Gaussian response-time filter with zero mean and standard deviation <inline-formula><mml:math id="sa2m10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>mn</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. Under this response-time filter, if the model predicted a response time <italic>T</italic>, the measured response time <inline-formula><mml:math id="sa2m11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was drawn from a normal distribution centered at <italic>T</italic> with standard deviation <inline-formula><mml:math id="sa2m12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mtext>mn</mml:mtext></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. If the response time <inline-formula><mml:math id="sa2m13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> was drawn outside of the simulation's time discretization (i.e., if <inline-formula><mml:math id="sa2m14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> &lt; 0 or <inline-formula><mml:math id="sa2m15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>&gt; <inline-formula><mml:math id="sa2m16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mfrac><mml:msub><mml:mi>T</mml:mi><mml:mrow><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mn>5</mml:mn></mml:mfrac></mml:mrow></mml:mstyle></mml:math></inline-formula>), we redrew <inline-formula><mml:math id="sa2m17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>T</mml:mi><mml:mo stretchy="false">~</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> until it fell within the discretization. This filter was chosen to represent both ``early responses'' caused by attentional lapses, as well as ``late responses'' caused by motor processing delays between the formation of a choice in the brain and the physical response.”</p></body></sub-article></article>