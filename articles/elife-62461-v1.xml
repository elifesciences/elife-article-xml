<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article article-type="research-article" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">62461</article-id><article-id pub-id-type="doi">10.7554/eLife.62461</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Tools and Resources</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Bayesian inference of population prevalence</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-32067"><name><surname>Ince</surname><given-names>Robin AA</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8427-0507</contrib-id><email>robin.ince@glasgow.ac.uk</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-238645"><name><surname>Paton</surname><given-names>Angus T</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-205710"><name><surname>Kay</surname><given-names>Jim W</given-names></name><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-32069"><name><surname>Schyns</surname><given-names>Philippe G</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>School of Psychology and Neuroscience, University of Glasgow</institution><addr-line><named-content content-type="city">Glasgow</named-content></addr-line><country>United Kingdom</country></aff><aff id="aff2"><label>2</label><institution>Department of Statistics, University of Glasgow</institution><addr-line><named-content content-type="city">Glasgow</named-content></addr-line><country>United Kingdom</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, San Diego</institution><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Behrens</surname><given-names>Timothy E</given-names></name><role>Senior Editor</role><aff><institution>University of Oxford</institution><country>United Kingdom</country></aff></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>06</day><month>10</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e62461</elocation-id><history><date date-type="received" iso-8601-date="2020-08-25"><day>25</day><month>08</month><year>2020</year></date><date date-type="accepted" iso-8601-date="2021-07-26"><day>26</day><month>07</month><year>2021</year></date></history><permissions><copyright-statement>© 2021, Ince et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Ince et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-62461-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-62461-figures-v1.pdf"/><abstract><p>Within neuroscience, psychology, and neuroimaging, the most frequently used statistical approach is null hypothesis significance testing (NHST) of the population mean. An alternative approach is to perform NHST within individual participants and then infer, from the proportion of participants showing an effect, the prevalence of that effect in the population. We propose a novel Bayesian method to estimate such population prevalence that offers several advantages over population mean NHST. This method provides a population-level inference that is currently missing from study designs with small participant numbers, such as in traditional psychophysics and in precision imaging. Bayesian prevalence delivers a quantitative population estimate with associated uncertainty instead of reducing an experiment to a binary inference. Bayesian prevalence is widely applicable to a broad range of studies in neuroscience, psychology, and neuroimaging. Its emphasis on detecting effects within individual participants can also help address replicability issues in these fields.</p></abstract><abstract abstract-type="executive-summary"><title>eLife digest</title><p>Scientists use statistical tools to evaluate observations or measurements from carefully designed experiments. In psychology and neuroscience, these experiments involve studying a randomly selected group of people, looking for patterns in their behaviour or brain activity, to infer things about the population at large.</p><p>The usual method for evaluating the results of these experiments is to carry out null hypothesis statistical testing (NHST) on the population mean – that is, the average effect in the population that the study participants were selected from. The test asks whether the observed results in the group studied differ from what might be expected if the average effect in the population was zero. However, in psychology and neuroscience studies, people’s brain activity and performance on cognitive tasks can differ a lot. This means important effects in individuals can be lost in the overall population average.</p><p>Ince et al. propose that this shortcoming of NHST can be overcome by shifting the statistical analysis away from the population mean, and instead focusing on effects in individual participants. This led them to create a new statistical approach named Bayesian prevalence. The method looks at effects within each individual in the study and asks how likely it would be to see the same result if the experiment was repeated with a new person chosen from the wider population at random.</p><p>Using this approach, it is possible to quantify how typical or uncommon an observed effect is in the population, and the uncertainty around this estimate. This differs from NHST which only provides a binary ‘yes or no’ answer to the question, ‘does this experiment provide sufficient evidence that the average effect in the population is not zero?’ Another benefit of Bayesian prevalence is that it can be applied to studies with small numbers of participants which cannot be analysed using other statistical methods.</p><p>Ince et al. show that the Bayesian prevalence can be applied to a range of psychology and neuroimaging experiments, from brain imaging to electrophysiology studies. Using this alternative statistical method could help address issues of replication in these fields where NHST results are sometimes not the same when studies are repeated.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>statistics</kwd><kwd>generalisation</kwd><kwd>inference</kwd><kwd>prevalence</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd><kwd>Other</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>214120/Z/18/Z</award-id><principal-award-recipient><name><surname>Ince</surname><given-names>Robin AA</given-names></name><name><surname>Paton</surname><given-names>Angus T</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100004440</institution-id><institution>Wellcome Trust</institution></institution-wrap></funding-source><award-id>107802</award-id><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000266</institution-id><institution>Engineering and Physical Sciences Research Council</institution></institution-wrap></funding-source><award-id>MURI 1720461</award-id><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100000288</institution-id><institution>Royal Society</institution></institution-wrap></funding-source><award-id>RSWF\R3\183002</award-id><principal-award-recipient><name><surname>Schyns</surname><given-names>Philippe G</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Bayesian estimation of the proportion of the population that would show an effect in an experiment provides a broadly applicable alternative to standard null hypothesis tests on the population mean with several advantages.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Within neuroscience, psychology, and neuroimaging, the common experimental paradigm is to run an experiment on a sample of participants and then infer and quantify any effect of the experimental manipulation in the population from which the sample was drawn. For example, in a psychology experiment, a particular set of stimuli (e.g. visual or auditory stimuli) might be presented to a sample of human participants, who are asked to categorise the stimuli or perform some other task. Each participant repeats the procedure several times with different stimuli (experimental trials), and their responses and reaction times are recorded. In a neuroimaging experiment, the same procedure is employed with neuroimaging signals recorded in addition to behavioural responses. The researcher analyses these responses to infer something about brain function in the population from which the participants were drawn.</p><p>In this standard experimental paradigm, the implicit goal is usually to determine the presence of a causal relationship between the experimental manipulation and the response of interest. For example, between a stimulus property and the neural activity in a particular brain region, or between neural activity and a behavioural measure (e.g. accuracy, reaction time). A properly controlled experimental design in which other extraneous factors are held constant (i.e. a randomised control trial) enables a causal interpretation of a correlational relationship (<xref ref-type="bibr" rid="bib3">Angrist and Pischke, 2014</xref>; <xref ref-type="bibr" rid="bib44">Pearl, 2009</xref>). We use statistical tools to evaluate the measured effect and ensure that we are not being fooled by randomness—that is, it should be unlikely our observed result was produced by random fluctuations with no effect present. This is often formalised as null hypothesis significance testing (NHST). We reject the null hypothesis (often that the population mean is zero) when the probability of observing a given effect size (or larger) is less than some prespecified value (often <italic>p=</italic>0.05) if the null hypothesis was true (i.e. if the population mean really was zero). Simply stated, we would be unlikely to obtain the observed effect size if the null hypothesis was true.</p><p>Researchers performing such studies usually wish to infer something about the population from which the experimental participants are selected (<xref ref-type="bibr" rid="bib26">Holmes and Friston, 1998</xref>), rather than about the specific sample of participants that were examined (as in a case study). Importantly, any statistical inference from a sample to the population requires a model of the population itself. The ubiquitous approach used in psychology and neuroimaging is to model the effect in the population with a normal distribution and perform inference on the mean of this model: the population mean (see Materials and methods). For example, the null hypothesis is often that the population mean is zero, and the probability of the observed sample data is computed under this assumption, taking into account the variance between individuals in the sample of participants.</p><p>However, an alternative and equally valid question is to ask how typical is an effect in the population (<xref ref-type="bibr" rid="bib16">Friston et al., 1999b</xref>). In this approach, we infer an effect in each individual of the sample, and from that infer the <italic>prevalence</italic> of the effect in the population—that is, the proportion of the population that would show the effect, if tested (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Friston et al., 1999a</xref>; <xref ref-type="bibr" rid="bib52">Rosenblatt et al., 2014</xref>). The results obtained using the population mean (vs. population prevalence) can differ, particularly when effects are heterogenous across participants.</p><p>Here, we argue that in many experimental applications in psychology and neuroscience, the individual participant is the natural replication unit of interest (<xref ref-type="bibr" rid="bib37">Little and Smith, 2018</xref>; <xref ref-type="bibr" rid="bib40">Nachev et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Smith and Little, 2018</xref>; <xref ref-type="bibr" rid="bib59">Thiebaut de Schotten and Shallice, 2017</xref>). This is because many aspects of cognition, and the neural mechanisms underlying them, are likely to vary across individuals. Therefore, we should seek to quantify effects and ensure that our results can be reliably distinguished from chance within individual participants. We argue that with such a shift in perspective towards experimental assessment within individual participants, we should also shift our statistical focus at the population level from NHST of the population mean to estimating the population prevalence: the proportion of individuals in the population who would show a true positive above-chance effect in a specific experiment. This can be thought of as the expected within-participant replication probability. Although we focus here on a broad class of experiments in psychology and neuroimaging that feature human participants and non-invasive recording modalities, the arguments we present are general and apply equally well to other experimental model organisms or sampling units.</p><p>To support this shift in perspective, we present a simple Bayesian method to estimate population prevalence based on the results of within-participant NHST, including prevalence differences between groups of participants or between tests on the same participants. This approach can also be applied without explicit within-participant NHST to estimate prevalence of different effect sizes, giving a new view on what can be learned about a population from an experimental sample. We suggest that applying Bayesian population prevalence in studies that are sufficiently powered within individual participants could address many of the recent issues raised about replicability in psychology and neuroimaging research (<xref ref-type="bibr" rid="bib6">Benjamin et al., 2018</xref>; <xref ref-type="bibr" rid="bib31">Ioannidis, 2005</xref>). Bayesian prevalence provides a population prevalence estimate with associated uncertainty and therefore avoids reducing an entire experiment to a binary NHST inference on a population mean (<xref ref-type="bibr" rid="bib39">McShane et al., 2019</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Population vs. within-participant statistical tests</title><p>To illustrate, we simulate data from the standard hierarchical population model that underlies inference of a population mean effect based on the normal distribution (<xref ref-type="bibr" rid="bib17">Friston, 2007</xref>; <xref ref-type="bibr" rid="bib26">Holmes and Friston, 1998</xref>; <xref ref-type="bibr" rid="bib46">Penny and Holmes, 2007</xref>) (see Materials and methods).</p><p>We emphasise that this standard hierarchical population simulation is the simplest possible illustrative example, intended to demonstrate the different perspective of within-participant vs. population mean inference. The model simulated here is used implicitly in almost every random-effects inference employed in psychology and neuroimaging studies, in which the tested null hypothesis is that the population distribution is normal with zero mean and non-zero variance. We also emphasise that the simulated data values within individuals are considered to represent the <italic>effect</italic> of a controlled experimental manipulation—that is, a causal relationship. However, this could be any causal relationship that we can quantify in psychology, neuroscience, or neuroimaging. For example, we could consider a within-participant difference in reaction time between two classes of stimuli, a coefficient or contrast from a linear model (e.g. a General Linear Model of fMRI data), a cross-validated out-of-sample predictive correlation from a high-dimensional stimulus encoding model (e.g. a model predicting auditory cortex MEG responses to continuous speech stimuli), a rank correlation of dissimilarity matrices in a Representational Similarity Analysis, or parameters of computational models of decision making (e.g. the Diffusion Drift Model) or learning. When evaluating the results of the simulations of <xref ref-type="fig" rid="fig1">Figure 1</xref>, it is important to keep in mind that these results are meant to represent any statistical quantification of any experimental manipulation.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Population vs individual inference.</title><p>For each simulation, we sample <inline-formula><mml:math id="inf1"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:math></inline-formula> individual participant mean effects from a normal distribution with population mean <inline-formula><mml:math id="inf2"><mml:mi>μ</mml:mi></mml:math></inline-formula> (<bold>A, B</bold>) <inline-formula><mml:math id="inf3"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>; (<bold>C, D</bold>) <inline-formula><mml:math id="inf4"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> and between-participant standard deviation <inline-formula><mml:math id="inf5"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>2</mml:mn></mml:math></inline-formula>. Within each participant, <inline-formula><mml:math id="inf6"><mml:mi>T</mml:mi></mml:math></inline-formula> trials (<bold>A, C</bold>) <inline-formula><mml:math id="inf7"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:math></inline-formula>; (<bold>B, D</bold>) <inline-formula><mml:math id="inf8"><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>500</mml:mn></mml:math></inline-formula> are drawn from a normal distribution with the participant-specific mean and a common within-participant standard deviation <inline-formula><mml:math id="inf9"><mml:msub><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib4">Baker et al., 2020</xref>). Orange and blue indicate, respectively, exceeding or not exceeding a <italic>p</italic>=0.05 threshold for a t-test at the population level (on the within-participant means, population normal density curves) or at the individual participant level (individual sample means ± s.e.m.). (<bold>E</bold>): Bayesian posterior distributions of population prevalence of true positive results for the four simulated data sets (<bold>A–D</bold>). Circles show Bayesian maximum a posteriori (MAP) estimates. Thick and thin horizontal lines indicate 50% and 96% highest posterior density intervals (HPDIs), respectively. MAP (96% HPDI) values are shown in the legend.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-fig1-v1.tif"/></fig><p><xref ref-type="fig" rid="fig1">Figure 1</xref> illustrates the results of four simulations that differ in the true population mean, µ (A, B: <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>; C, D, <inline-formula><mml:math id="inf11"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>) and in the number of trials performed per participant (A, C: 20 trials; B, D: 500 trials). For each simulation, we performed inference based on a standard two-sided, one-sample t-test against zero at two levels. First, we applied the standard summary statistic approach: we took the mean across trials for each participant and performed the second-level population t-test on these per-participant mean values. This provides an inference on the population mean, taking into account between-participant variation. This is equivalent to inference on the full hierarchical model in a design where participants are separable (<xref ref-type="bibr" rid="bib26">Holmes and Friston, 1998</xref>; <xref ref-type="bibr" rid="bib46">Penny and Holmes, 2007</xref>; <xref ref-type="bibr" rid="bib45">Penny et al., 2003</xref>) (see Materials and methods). The modelled population distribution is plotted as a solid curve, coloured according to the result of the population mean inference (orange for significant population mean, blue for non-significant population mean). Second, we performed inference within each participant, applying the t-test on within-participant samples, separately for each participant. The sample mean ± s.e.m. is plotted for each participant (orange for significant participants, blue for non-significant participants).</p><p>The population mean inference correctly fails to reject the null hypothesis for <xref ref-type="fig" rid="fig1">Figure 1</xref> panels A and B (ground truth <inline-formula><mml:math id="inf12"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>) and correctly rejects the null in panels C and D (ground truth <inline-formula><mml:math id="inf13"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>). But consider carefully <xref ref-type="fig" rid="fig1">Figure 1B,C</xref>. With 500 trials in panel B, 32/50 participants (orange markers) show a significant within-participant result. The probability of this happening by chance, if there was no effect in any members of the population, can be calculated from the cumulative density function of the binomial distribution. In this case, it is tiny—for a test with false positive rate <inline-formula><mml:math id="inf14"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>, and no effect in any individual, <italic>p</italic>&lt;2.2×10<sup>-16</sup> (below 64-bit floating-point precision). Compare that to <italic>p</italic>=0.008 for the population t-test in panel C. Thus, the panel B results provide much stronger statistical evidence of a non-zero effect <italic>at the population level:</italic> the observed results are very unlikely if no individuals in the population have a non-zero effect in this test. This statistical evidence would be ignored in analyses based only on the population mean. Considering inference at the individual level, panel C results (11/50 significant) have <italic>p</italic>=4.9×10<sup>-6</sup> if there was no effect within any individuals (i.e. the proportion of the population showing an effect was zero). Thus, even panel C, which simulates an experiment with only 20 trials per participant, provides stronger evidence for a population effect from the within-participant perspective than from the population mean perspective.</p><p>Obviously, these two different p-values are calculated from two different null hypotheses. The population t-test tests the null hypothesis that the population mean is zero, assuming that individual means follow a normal distribution:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></disp-formula></p><p>while the p-value for the number of significant within-participant tests comes from the null hypothesis that there is no effect in any individual in the population, termed the <italic>global null</italic> (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>; <xref ref-type="bibr" rid="bib43">Nichols et al., 2005</xref>):<disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>:</mml:mo><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn><mml:mtext> </mml:mtext><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mtext> </mml:mtext><mml:mi>a</mml:mi><mml:mi>l</mml:mi><mml:mi>l</mml:mi><mml:mtext> </mml:mtext><mml:mi>i</mml:mi></mml:mrow></mml:math></disp-formula></p><p>where i indexes all members of the population. These are testing different questions, but importantly both are framed at the population level and both provide a population-level inference. Clearly, the global null itself is quite a blunt tool. The goal of our Bayesian prevalence method is to quantify within-participant effects at the population level in a more meaningful and graded way. We agree that it is important to align ‘the meaning of the quantitative inference with the meaning of the qualitative hypothesis we’re interested in evaluating’ (<xref ref-type="bibr" rid="bib60">Yarkoni, 2020</xref>). Often, when the goal of the analysis is to infer the presence of a causal relationship within individuals in the population, the within-participant perspective may be more appropriate. We argue that performing NHST at the individual participant level is preferable for conceptual reasons in psychology and neuroimaging, but also for practical reasons related to the replicability crisis (see Discussion).</p><p>Although we have so far considered only the global null, the simulations show that the within-participant perspective can give a very different impression of the evidence for a population-level effect from a data set. The results in <xref ref-type="fig" rid="fig1">Figure 1B</xref> represent strong evidence of a non-zero effect within many participants, albeit one that is approximately balanced across participants between positive and negative directions. We do not intend to suggest that such two-sided effects are particularly common but would highlight how they are implicitly assumed in traditional population mean inference models. <xref ref-type="fig" rid="fig2">Figure 2</xref> provides further examples of how the within-participant and population mean approaches can diverge in situations with effects in one direction. In fact, if the researcher believes that all effects in an experiment are positive, then the standard null model illustrated here is inappropriate. The traditional population mean t-test is actually providing a fixed-effects analysis based on the global null rather than the random-effects interpretation of generalisation to the population that is normally applied (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>).</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Simulated examples where Bayesian prevalence and second-level t-tests diverge.</title><p>EEG traces are simulated for 100 trials from 20 participants as white noise, <inline-formula><mml:math id="inf15"><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula>, with an additive Gaussian activation (<inline-formula><mml:math id="inf16"><mml:mi>σ</mml:mi></mml:math></inline-formula> = 20 ms) with amplitudes drawn from a uniform distribution on [0 0.6]. For each simulation, mean traces are shown per participant (upper-left panel). A second-level t-test is performed at each time point separately (blue curve, lower-left panel), dashed line shows the <italic>p</italic>=0.05 threshold, Bonferroni corrected over time points. A within-participant t-test is performed at each time point and for each participant separately (right-hand panel); the blue points show the maximum T-statistic over time for each participant, and the dashed line shows the <italic>p</italic>=0.05 Bonferroni corrected threshold. Lower-right panel shows posterior distribution of population prevalence for an effect in the analysis window. Black curves (lower-left panel) show the prevalence posterior at each time point (black line maximum a posteriori [MAP], shaded area 96% highest posterior density interval [HPDI]). (<bold>A</bold>) An effect is simulated in all participants, with a peak time uniformly distributed in the range 100–400 ms. (<bold>B</bold>) An effect is simulated in 10 participants, with a peak time uniformly distributed in the range 200–275 ms.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Estimating population prevalence</title><p>The p-values under the global null are obtained from the cumulative density function of the binomial distribution, based on a within-participant false positive rate <inline-formula><mml:math id="inf17"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>. However, we can also model the number of above-threshold individuals in a sample when the population prevalence of true positive (significant) test results is <inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (see Materials and methods). Consider a within-participant test with a false positive rate <inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. In this case, the distribution of the number of significant test results follows a binomial distribution with success probability <inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>γ</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Here, we present a Bayesian approach to estimate the population prevalence of true positive test results, <inline-formula><mml:math id="inf21"><mml:mi>γ</mml:mi></mml:math></inline-formula>, from this binomial model of within-participant testing. Alternatively, frequentist inference approaches can be used (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Friston et al., 1999a</xref>). Note that we are not estimating the population prevalence of the ground truth binary status of the tested effect. We could only obtain a lower bound on this quantity because there might be individuals with an effect too small to detect with our test. Therefore, here we focus throughout on the prevalence of true positive test results—that is, the proportion of the population we would expect to give a true positive test result given the specific experimental test procedure considered.</p><p>The Bayesian approach provides a full posterior distribution for <inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, from which we can obtain the maximum a posteriori (MAP) estimate, together with measures of uncertainty—for example, highest posterior density intervals (HPDIs) or lower bound posterior quantiles. <xref ref-type="fig" rid="fig1">Figure 1E</xref> shows the Bayesian posteriors, MAPs, and HPDIs for the four simulated data sets in <xref ref-type="fig" rid="fig1">Figure 1A–D</xref>. Even though there is no population mean effect in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, the MAP estimate of the prevalence is 0.62 (96% HPDI: [0.47 0.76]). Given the data, the probability that the population prevalence is greater than 47% is higher than 0.96. Based on this result, we would consider it highly likely that more than 47% of the population would show a true positive effect if tested in the same experiment with 500 trials.</p><p><xref ref-type="fig" rid="fig2">Figure 2</xref> demonstrates two situations where Bayesian prevalence gives a different statistical assessment of the presence of an effect in the population compared to a second-level t-test for a non-zero population mean. We simulated a simple EEG experiment with 100 trials repeated in each of 20 participants. Template Gaussian event-related potentials (ERPs) are added on each trial with fixed width and uniformly distributed amplitude, with a specific peak time per participant. Both within-participant and second-level t-tests are Bonferroni corrected over the 600 time points considered. <xref ref-type="fig" rid="fig2">Figure 2A</xref> shows a simulation where all participants have an effect, with peak times drawn from a uniform distribution over the 100–400 ms range. There is strong evidence for an effect in each participant, and the estimated prevalence is high: 1 [0.85 1] (MAP [96% HPDI]). However, because the effects are not aligned in time across participants, there are no time points at which the null hypothesis of zero population mean can be rejected. In the simulation shown in <xref ref-type="fig" rid="fig2">Figure 2B</xref>, only 10/20 participants demonstrate an effect, with peak times drawn from a uniform distribution over the range 200–275 ms. Here, effects can be reliably detected in those 10 participants, leading to an estimated prevalence of 0.47 [0.25 0.70]. However, because the effects are not present in all participants, there are no time points when the null hypothesis of zero population mean can be rejected. Interestingly, plotting the prevalence posterior distribution as a function of time does reveal evidence for an effect in the population during the time window of the simulated effect.</p></sec><sec id="s2-3"><title>Estimating differences in prevalence</title><p>Often the scientific question of interest might involve comparing an effect between groups of participants or between different experimental conditions in the same set of participants. In the first case, a researcher might, for example, be interested in examining differences in a behavioural or neuroimaging effect between patients and healthy controls, or between participants from two different cultures. In the second case, a researcher might be interested in determining the effect of an intervention on a particular measured effect, testing the same set of participants on two occasions, once with an intervention and once in a control condition. From the population mean perspective, these questions would typically be addressed with a two-sample unpaired t-test for the first case and a paired t-test for the second. From the prevalence perspective, the question would be whether the prevalence of true positive results differs between two sampled populations (in the first case) or between two experimental tests (in the second case). We therefore provide additional functions (see Materials and methods) to directly estimate the difference in prevalence of true positive test results for these two comparisons, which we term between-group (same test applied to two different samples) and within-group (two different tests applied to a single sample).</p><p>To estimate the difference in prevalence of true positive results for a given test between two samples from separate populations (e.g. patients vs. healthy controls), the input data required is the count of positive results and the total number of participants in each group. We illustrate this with a simulated data set. We specify the true prevalence in the two populations as <inline-formula><mml:math id="inf23"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.75</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf24"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:math></inline-formula>, respectively, and draw a random sample based on the respective binomial distributions with parameters <inline-formula><mml:math id="inf25"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see Materials and methods). We simulate <inline-formula><mml:math id="inf26"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:math></inline-formula> participants in the first group and <inline-formula><mml:math id="inf27"><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>40</mml:mn></mml:math></inline-formula> participants in the second group. The results of one such draw give <inline-formula><mml:math id="inf28"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>45</mml:mn></mml:math></inline-formula>, <inline-formula><mml:math id="inf29"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>11</mml:mn></mml:math></inline-formula> positive tests in each group, respectively. In this case, the MAP [96% HPDI] prevalence difference <inline-formula><mml:math id="inf30"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, calculated from these four values (<inline-formula><mml:math id="inf31"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>), is 0.49 [0.29 0.67], which closely matches the ground truth (0.5). <xref ref-type="fig" rid="fig3">Figure 3A and B</xref> shows how the between-group posterior prevalence difference estimates scale with the number of participants for three different simulations.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Bayesian inference of difference of prevalence.</title><p>(<bold>A, B</bold>) We consider two independent groups of participants with population prevalence of true positives <inline-formula><mml:math id="inf32"><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula> of [25%, 25%] (blue), [25%, 50%] (red), and [25%, 75%] (yellow). We show how (<bold>A</bold>) the Bayesian MAP estimate and (<bold>B</bold>) 96% highest posterior density interval (HPDI) width of the estimated between-group prevalence difference <inline-formula><mml:math id="inf33"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> scale with the number of participants. (<bold>C, D</bold>) We consider two tests applied to the same sample of participants. Here, each simulation is parameterised by the population prevalence of true positives for the two tests, <inline-formula><mml:math id="inf34"><mml:mo>[</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>]</mml:mo></mml:math></inline-formula>, as well as <inline-formula><mml:math id="inf35"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, the correlation between the (binary) test results across the population. We show this for [50%, 50%] with <inline-formula><mml:math id="inf36"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula> (blue), [50%, 0%] with <inline-formula><mml:math id="inf37"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula> (red), and [75%, 50%] with <inline-formula><mml:math id="inf38"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula> (yellow). We show how (<bold>C</bold>) the Bayesian maximum a posteriori (MAP) estimate and (<bold>D</bold>) 96% HPDI width of the estimated within-group prevalence difference <inline-formula><mml:math id="inf39"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> scale with the number of participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-fig3-v1.tif"/></fig><p>To estimate the difference in prevalence of true positive results between two different tests applied to the same sample of participants, the input parameters are the number of participants significant in both tests, the number significant only in each of the two tests, and the total number of participants. We simulate two tests applied to a group of <inline-formula><mml:math id="inf40"><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>50</mml:mn></mml:math></inline-formula> participants. Each test detects a certain property with false positive rate <inline-formula><mml:math id="inf41"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>. The ground truth prevalences of true positives for the two tests are <inline-formula><mml:math id="inf42"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math></inline-formula> and <inline-formula><mml:math id="inf43"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:math></inline-formula>, respectively, and the correlation between positive results is <inline-formula><mml:math id="inf44"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.2</mml:mn></mml:math></inline-formula> (i.e. participants who test positive on one test are more likely to also test positive on the other test). The results of one random draw from this model give: (see Materials and methods) <inline-formula><mml:math id="inf45"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>8</mml:mn></mml:math></inline-formula> participants with a significant result in both tests; <inline-formula><mml:math id="inf46"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>19</mml:mn></mml:math></inline-formula> participants with a significant result in the first test but not the second; and <inline-formula><mml:math id="inf47"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math></inline-formula> participants with a significant result in the second but not the first. In this case, the MAP [96% HPDI] prevalence difference <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, calculated from these four values (<inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>N</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>), is 0.28 [0.08 0.46], which again matches the ground truth (0.25). <xref ref-type="fig" rid="fig3">Figure 3C,D</xref> shows how the within-group posterior prevalence difference estimates scale with the number of participants for three different ground truth situations, given as <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p><p>Both these approaches are implemented using Monte Carlo methods, and the functions return posterior samples (<xref ref-type="bibr" rid="bib18">Gelman, 2014</xref>), which can be used to calculate other quantities, such as the posterior probability that one test or group has a higher prevalence than the other. The posterior log odds in favour of this hypothesis can be computed by applying the logit function to the proportion of posterior samples in favour of a hypothesis. In the between-group example above, the proportion of posterior samples in favor of the hypothesis <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is 0.9999987, corresponding to log posterior odds of 13.5. In the above within-group comparison, the proportion of posterior samples in favour of the hypothesis <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> is 0.9979451, corresponding to log posterior odds of 6.2 (each computed from 10 million posterior samples).</p><p>The additional example in <xref ref-type="fig" rid="fig4">Figure 4</xref> demonstrates how between-group prevalence differences can occur between two populations with the same mean. We simulated two groups of participants where group 1 was from a homogenous population with a non-zero mean whereas group 2 comprised participants drawn from two different distributions, one with zero mean and one with a large mean. The two samples (and populations) have similar means, with no significant difference between them (two-sample t-test <italic>p=</italic>0.78). However, considering the prevalence in the two populations clearly reveals their difference (<xref ref-type="fig" rid="fig4">Figure 4C</xref>), which is formalised with the posterior distribution of the prevalence difference between the two populations (<xref ref-type="fig" rid="fig4">Figure 4D</xref>).</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Example where between-group prevalence diverges from two-sample t-test.</title><p>We simulate standard hierarchical Gaussian data for two groups of 20 participants, <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>20</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> per group. (<bold>A</bold>) Group 1 participants are drawn from a single population Gaussian distribution with <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>4</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. Group 2 participants are drawn from two Gaussian distributions. 75% of participants are drawn from <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>0.01</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and 25% of participants are drawn from <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>N</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mn>16</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>. Dashed line shows the p=0.05 within-participant threshold (one-sample t-test). The means of these two groups are not significantly different (<bold>B</bold>), but they have very different prevalence posteriors (<bold>C</bold>). The posterior distribution for the difference in prevalence shows the higher prevalence in group 1: 0.61 [0.36 0.85] (MAP [96% HPDI]) (<bold>D</bold>). MAP: maximum a posteriori; HDPI: highest posterior density interval.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-fig4-v1.tif"/></fig></sec><sec id="s2-4"><title>Prevalence as a function of effect size</title><p>In the above, we focused on performing explicit statistical inference within each participant. A possible criticism of this approach is that the within-participant binarisation of a continuous effect size can lose information. If the null distribution is the same for each participant, then the within-participant inference involves comparing each participant’s effect size, <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, to a common statistical threshold <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The prevalence estimation can therefore be interpreted as estimating the population prevalence of participants for which <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. In the NHST case, <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> is chosen so that <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>E</mml:mi><mml:mo>&gt;</mml:mo><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> (usually <inline-formula><mml:math id="inf64"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula>), but we can generally consider the prevalence of participants with effects exceeding any value <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. We therefore estimate the prevalence of <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> as a function of <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This can reveal if a data set provides evidence for population prevalence of a subthreshold within-participant effect, as well as showing how population prevalence decreases for larger effects. <xref ref-type="fig" rid="fig5">Figure 5</xref> demonstrates this approach for the simulated systems of <xref ref-type="fig" rid="fig1">Figure 1</xref>, showing that prevalence results for both right-sided, <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, and left-sided, <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, effect size thresholds. Note that this approach requires the null distribution to be the same for each participant and requires the false positive rate <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be calculated for each effect size considered. It reveals everything we can learn about the population prevalence of different effect sizes from our data set, exploring beyond the binarisation of the within-participant inference. The asymmetry visible in <xref ref-type="fig" rid="fig5">Figure 5C,D</xref> reflects the positive population mean for those simulations.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>One-sided prevalence as a function of effect size.</title><p>We consider the same simulated systems shown in <xref ref-type="fig" rid="fig1">Figure 1</xref>, showing both right-tailed (<inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) and left-tailed (<inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>) prevalence as a function of effect size. Orange lines show the effect size corresponding to the two-sided <inline-formula><mml:math id="inf73"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:math></inline-formula> within-participant test, as used in <xref ref-type="fig" rid="fig1">Figure 1</xref>. Dashed lines show the effect size corresponding to the ground truth of the simulation. (<bold>A</bold>,<bold>B</bold>) <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, (<bold>C</bold>,<bold>D</bold>) <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. (<bold>A</bold>,<bold>C</bold>) T = 20 trials, (<bold>B</bold>,<bold>D</bold>) T = 500 trials. Black line shows maximum a posteriori (MAP), shaded region shows 96% highest posterior density interval (HPDI).</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-fig5-v1.tif"/></fig><p>In <xref ref-type="fig" rid="fig6">Figure 6</xref>, three simulated EEG data sets (c.f. <xref ref-type="fig" rid="fig2">Figure 2</xref>) show how prevalence as a function of effect size can disambiguate situations where the prevalence of <italic>p=</italic>0.05 NHST rejections does not differ. Each panel (A–C) shows a simulation of a different EEG experiment. All three have a similar population prevalence of <italic>p=</italic>0.05 null hypothesis rejections, as shown in the lower-right posterior distributions. However, the prevalence curves as a function of effect size differ in each simulation. For example, in panel A there is no evidence that the population contains individuals with an effect size greater than <italic>T</italic>(99) = 12, whereas the MAP for prevalence of effects greater than 12 is around 50% in panel B. Similarly, the prevalence curve in panel C reflects the larger population variance of the effect in that simulation compared to panel A. While these differences would also be clear from a standard plot of the participant data (e.g. violin or raincloud plot of per-participant maximal effect sizes), the posterior prevalence curves go beyond descriptive visualisations of the specific data sample by quantifying a formal inference to the population, including its associated uncertainty.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Examples of different effect size prevalence curves with similar p=0.05 prevalence.</title><p>EEG traces are simulated for 100 trials from 20 participants as white noise [<inline-formula><mml:math id="inf76"><mml:mi>N</mml:mi><mml:mfenced separators="|"><mml:mrow><mml:mn>0,1</mml:mn></mml:mrow></mml:mfenced></mml:math></inline-formula> ] with an additive Gaussian activation (<inline-formula><mml:math id="inf77"><mml:mi>σ</mml:mi></mml:math></inline-formula> = 20 ms) with amplitudes drawn from a uniform distribution. For each simulation, mean traces are shown per participant (upper-left panel). A within-participant t-test is performed at each time point and for each participant separately (right-hand panel); the blue points show the maximum T-statistic over time for each participant, and the dashed line shows the <italic>p</italic>=0.05 Bonferroni corrected threshold. Lower-right panel shows posterior distribution of population prevalence for an effect in the analysis window. Black curves (lower-left panel) show the prevalence (maximum a posteriori [MAP], shaded area 96% highest posterior density interval [HPDI]) as a function of effect size threshold. (<bold>A</bold>) A weak early effect is simulated in all participants (peak time uniformly distributed 100–150 ms). (<bold>B</bold>) In addition to the same early effect, a stronger, longer (<inline-formula><mml:math id="inf78"><mml:mi>σ</mml:mi></mml:math></inline-formula> = 40 ms), and more temporally variable later effect is simulated in 10 participants (peak times 250–450 ms). (<bold>C</bold>) Early events are simulated with the same timing as (<bold>A</bold>), but each participant has a different maximum amplitude (participants ordered by effect size). All three simulations have similar prevalence of <italic>p</italic>=0.05 effects, but show differing patterns of prevalence over different effect size thresholds.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-fig6-v1.tif"/></fig></sec><sec id="s2-5"><title>How to apply Bayesian prevalence in practice</title><p>As in the simulation of <xref ref-type="fig" rid="fig1">Figure 1</xref>, a typical population mean inference is often framed as a two-level summary statistic procedure. At the first level, the effect is quantified within each participant (e.g. a difference in mean response between two conditions). At the second level, the population mean is inferred under the assumption that the effect is normally distributed in the population (i.e. based on the mean and standard deviation of the measured effect across participants). Bayesian prevalence is similarly framed as a two-level procedure. At the first level, a statistical test is applied within each participant, the result of which can be binarized via a within-participant NHST (e.g. using a parametric test, as in our simulation, or alternatively using non-parametric permutation methods, independently for each participant), or via an arbitrary effect size threshold <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. At the second level, the binary results from the first level (i.e. the counts of significant participants) are the input to the Bayesian population prevalence computation. To accompany this paper, we provide code in MATLAB, Python, and R to visualise the full posterior distribution of the population prevalence, as well as extract properties, such as the MAP point estimate and HPDIs. We also provide functions to provide Bayesian estimates of the difference in prevalence between two mutually exclusive participant groups to the same test (between-group prevalence difference), as well as the difference in prevalence between two different tests applied to a single sample of participants (within-group prevalence difference). We suggest reporting population prevalence inference results as the MAP estimate together with one or more HPDIs (e.g. with probability 0.5 or 0.96, see Materials and methods).</p><p>It is important to stress that the second-level prevalence inference does not impose any requirements on the first-level within-participant tests, other than that each test should provide strong control of the same false positive rate <inline-formula><mml:math id="inf80"><mml:mi>α</mml:mi></mml:math></inline-formula> (see Materials and methods). It is not required, for example, that each participant have the same number of trials, degrees of freedom, or within-participant variance. The within-participant test can be parametric (e.g. a t-test) or non-parametric (e.g. based on permutation testing). It can be a single statistical test or an inference over multiple tests (e.g. a neuroimaging effect within a certain brain region), provided that the family-wise error rate is controlled at <inline-formula><mml:math id="inf81"><mml:mi>α</mml:mi></mml:math></inline-formula> (e.g. by using permutation tests with the method of maximum statistics).</p></sec><sec id="s2-6"><title>Effect of number of participants and number of trials</title><p><xref ref-type="fig" rid="fig7">Figure 7</xref> illustrates how Bayesian prevalence inference scales with the number of participants and trials. <xref ref-type="fig" rid="fig7">Figure 7A–C</xref> suggests that there are benefits to having larger numbers of participants for the Bayesian prevalence metrics (including decrease in variance of obtained MAP and HPDI width, increase in prevalence lower bound). However, above around 50 participants, these benefits become less pronounced. <xref ref-type="fig" rid="fig7">Figure 7E</xref> shows that, on average, inferred prevalence is mostly sensitive to the number of trials per participant (horizontal contours) and is invariant to the number of participants (although variance decreases with increasing N, as in <xref ref-type="fig" rid="fig7">Figure 7A, C and F</xref>). By comparison, t-test power (<xref ref-type="fig" rid="fig7">Figure 7D</xref>) is mostly sensitive to the number of participants (vertical contours) and is largely invariant to the number of trials above around 100 trials per participant (<xref ref-type="bibr" rid="bib4">Baker et al., 2020</xref>). In sum, compared to the population mean t-test, prevalence exhibits greater sensitivity to the number of trials obtained per participant and less sensitivity to the number of participants.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Characterisation of Bayesian prevalence inference.</title><p>(<bold>A–C</bold>) We consider the binomial model of within-participant testing for three ground truth population proportions: 25%, 50%, and 75% (blue, orange, yellow, respectively). We show how (<bold>A</bold>) the Bayesian maximum a posteriori (MAP) estimate, (<bold>B</bold>) 95% Bayesian lower bound, and (<bold>C</bold>) 96% highest posterior density interval (HPDI) width scale with the number of participants. Lines show theoretical expectation, coloured regions show ±1 s.d. (<bold>D–F</bold>) We consider the population model from <xref ref-type="fig" rid="fig1">Figure 1C and D</xref> (<inline-formula><mml:math id="inf82"><mml:mi>μ</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>). (<bold>D</bold>) Power contours for the population inference using a t-test (<xref ref-type="bibr" rid="bib4">Baker et al., 2020</xref>). Colour scale shows statistical power (probability of rejecting the null hypothesis). (<bold>E</bold>) Contours of average Bayesian MAP estimate for <inline-formula><mml:math id="inf83"><mml:mi>γ</mml:mi></mml:math></inline-formula>. Colour scale shows MAP prevalence proportion. (<bold>F</bold>) Contours of average 95% Bayesian lower bound for <inline-formula><mml:math id="inf84"><mml:mi>γ</mml:mi></mml:math></inline-formula>. Colour scale shows lower bound prevalence. From the prevalence perspective, the number of trials obtained per participant has a larger effect on the resulting population inference than does the number of participants.</p></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-fig7-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The fields of psychology and neuroimaging are currently dominated by a statistical approach in which the primary focus is the population mean. Here, we propose a conceptual shift in perspective, from estimating population means to estimating the prevalence of effects detected within individuals. We provide a simple but novel Bayesian method to estimate the population prevalence of true positive results from any experimental test. We propose that within-participant inference, combined with prevalence estimation, may often match a researcher’s qualitative hypothesis regarding the existence of an effect in the population better than a binary inference that the population mean differs from zero. Particularly in the cognitive sciences, we would argue that effects within individuals are often the ultimate object of study, although these must be assessed at the population level to generalize from a specific experiment. Bayesian prevalence estimation provides this generalisation. It can easily be applied to almost any statistical evaluation of any experiment, provided a NHST can be performed at the individual participant level. The simulations presented here can also be used for simple power analyses when designing studies from this perspective.</p><p>Together, this conceptual shift and novel practical method support an alternative perspective for statistics in which the individual participant becomes the most relevant experimental unit to consider for the purposes of replication (<xref ref-type="bibr" rid="bib40">Nachev et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Smith and Little, 2018</xref>; <xref ref-type="bibr" rid="bib59">Thiebaut de Schotten and Shallice, 2017</xref>), and where power should be considered for effects within individual participants. This perspective gives a very different view of the strength of evidence provided by a data set and of the importance of sample size (for both participants and trials) compared to the more common population mean perspective (<xref ref-type="bibr" rid="bib4">Baker et al., 2020</xref>). For example, the simulation of 50 participants with 20 trials in <xref ref-type="fig" rid="fig1">Figure 1C</xref> has <italic>p</italic>=0.008 for a group mean different from zero, a result that is as surprising, under the null hypothesis, as observing seven heads in a row from tosses of a fair coin (<xref ref-type="bibr" rid="bib50">Rafi and Greenland, 2020</xref>). This is weaker evidence than just two out of five participants showing an effect at <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<italic>p</italic>=0.0012 under the global null, or about as surprising as 10 heads in a row). Three out of five significant participants correspond to <italic>p</italic>=0.00003 under the global null (as surprising as 15 heads in a row); this is substantially stronger evidence for a population-level effect than that provided by the population mean inference in <xref ref-type="fig" rid="fig1">Figure 1D</xref> (from 50 participants, even with 500 trials). However, in the current scientific climate, the weaker result obtained from the larger sample size would commonly be viewed as providing more satisfactory evidence by most readers and reviewers. We would like to highlight this pervasive misunderstanding: that larger participant numbers automatically imply better evidence at the population level. The crux of our argument is that most studies focus on the difference between <xref ref-type="fig" rid="fig1">Figure 1A and C</xref> (a small difference in population mean, ignoring the implications of large between-participant variance in studies that are underpowered for within-participant effects). In contrast, moving towards the situation shown in <xref ref-type="fig" rid="fig1">Figure 1B and D</xref> (increased power within individual participants) would provide both improved replicability and greater insight into the size and prevalence of effects in the population.</p><p>The practice of collecting enough data to perform within-participant inference is not a new idea—much of traditional psychophysics employs this approach (<xref ref-type="bibr" rid="bib57">Smith and Little, 2018</xref>). We have employed this technique with EEG (<xref ref-type="bibr" rid="bib55">Schyns et al., 2011</xref>; <xref ref-type="bibr" rid="bib56">Smith et al., 2006</xref>) and MEG (<xref ref-type="bibr" rid="bib28">Ince et al., 2015</xref>; <xref ref-type="bibr" rid="bib61">Zhan et al., 2019</xref>), and in fMRI it is becoming more common to collect large quantities of data for fitting high-dimensional, cross-validated machine learning models within individual participants (<xref ref-type="bibr" rid="bib27">Huth et al., 2016</xref>; <xref ref-type="bibr" rid="bib41">Naselaris et al., 2021</xref>; <xref ref-type="bibr" rid="bib58">Stansbury et al., 2013</xref>). Recently, this practice has also been adopted in the resting-state fMRI field where it is termed <italic>dense sampling</italic> or <italic>precision imaging</italic> (<xref ref-type="bibr" rid="bib8">Braga and Buckner, 2017</xref>; <xref ref-type="bibr" rid="bib19">Gordon et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Laumann et al., 2015</xref>; <xref ref-type="bibr" rid="bib49">Poldrack, 2017</xref>), or more recently <italic>deep imaging</italic> (<xref ref-type="bibr" rid="bib20">Gratton and Braga, 2021</xref>). Ensuring that experiments are sufficiently powered to obtain reliable effect size estimates within individual participants could benefit studies relating individual differences in experimental effects to other external factors. In fact, the focus on population mean may have emphasised experimental effects which have low between-participant variance and which are therefore less well-suited for studying individual differences (<xref ref-type="bibr" rid="bib11">Elliott et al., 2020</xref>; <xref ref-type="bibr" rid="bib25">Hedge et al., 2018</xref>). With more reliable within-participant estimates, comparisons between groups can look beyond means to consider other differences in the distribution of effect sizes (<xref ref-type="bibr" rid="bib54">Rousselet et al., 2017</xref>), as we show here considering prevalence as a function of effect size (<xref ref-type="fig" rid="fig6">Figure 6</xref>).</p><p>Note that while similar points regarding within-participant inference have been made elsewhere (<xref ref-type="bibr" rid="bib21">Grice et al., 2020</xref>; <xref ref-type="bibr" rid="bib40">Nachev et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Smith and Little, 2018</xref>; <xref ref-type="bibr" rid="bib59">Thiebaut de Schotten and Shallice, 2017</xref>), such results are typically considered to form a case study, without an inferential link that allows generalisation to the population (<xref ref-type="bibr" rid="bib42">Neuroscience, S. for, 2020</xref>). (‘[The within-participant] approach only allows for statements that pertain to the existence and magnitude of effects in those subjects, rather than in the populations those subjects are drawn from.’) The methods presented here address this concern by providing an inferential bridge from within-participant tests to the population level, even when the number of participants is small. Randomisation tests (<xref ref-type="bibr" rid="bib10">Edgington et al., 2007</xref>) provide a completely non-parametric and assumption-free approach to experimental statistics, but their interpretation is limited since they cannot provide statistical statements which generalise to a population. The combination of within-participant randomisation tests with Bayesian prevalence provides a population inference with minimal assumptions (compared to tests of population mean which usually assume a normal distribution).</p><p>Bayesian prevalence produces a quantitative prevalence estimate with associated uncertainty, rather than reducing an entire experiment to a single, binary, population NHST result. This addresses many of the problems noted with the NHST framework (<xref ref-type="bibr" rid="bib2">Amrhein et al., 2019</xref>; <xref ref-type="bibr" rid="bib39">McShane et al., 2019</xref>) and also reduces the risk of questionable research practices, such as p-hacking (<xref ref-type="bibr" rid="bib14">Forstmeier et al., 2017</xref>). Each participant serves as a replication since the same statistical analysis is performed on separate sets of data. This provides a degree of protection from problems such as researcher degrees of freedom. While it takes on average 20 different independent analyses of null data to find a single <italic>p</italic>=0.05 population mean NHST rejection in a multi-participant data set, it would take 400 different analyses to reject a within-participant NHST at <italic>p=</italic>0.05 by chance in two different participants, or 8000 different analyses to find three participants significant by chance. Therefore, within-participant inference provides exponential protection with increasing participants against issues such as p-hacking and researcher degrees of freedom, likely resulting in more robust and replicable findings. We have also shown how to estimate the prevalence of different effect size thresholds, which avoids focusing on a single within-participant dichotomisation. However, estimated prevalence is only one approach to evaluate the strength of evidence provided by a data set, and it should be assessed alongside the quality of the experimental design, within-participant power, and the effect sizes within individuals. Prevalence and population mean estimation are not mutually exclusive; ideally one could report within-participant effect sizes and estimated population prevalence together with estimation and inference of the population mean.</p><p>It is also possible to perform frequentist inference on the prevalence testing model (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>). Various null hypotheses for specific prevalence values can be considered, for example, the global null (<inline-formula><mml:math id="inf86"><mml:mi>γ</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:math></inline-formula>), the majority null (<inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>), or any value of interest. However, this approach results in a binarisation of the experimental outcome to significant or not, and adds a degree of freedom to the researcher in terms of the particular value of <inline-formula><mml:math id="inf88"><mml:mi>γ</mml:mi></mml:math></inline-formula> to test. An alternative approach is to consider the family of null hypotheses <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> and find the largest <inline-formula><mml:math id="inf90"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> value for which the corresponding null hypothesis can be rejected at the chosen <inline-formula><mml:math id="inf91"><mml:mi>α</mml:mi></mml:math></inline-formula>. This can be interpreted as a frequentist lower bound on the true prevalence given the specified false positive rate. However, the Bayesian posterior we present here gives a full quantitative description of the population based on the evidence provided by the data, avoids dichotomisation of the final analysis result, provides direct comparison between different groups or tests, and allows for prevalence as a function of effect size to give a more detailed picture of the population. Prevalence as a function of effect size can give insights into the variation of effects across the population (e.g. subgroups, <xref ref-type="fig" rid="fig6">Figure 6B</xref>) without requiring a parametric model (e.g. a normal distribution).</p><p>We have focused on human participants in typical psychology or neuroimaging experiments, but the proposed methods can be applied to infer the population prevalence of effects in other types of experimental units. For example, our approach could be directly applied to identified single-unit neurons in electrophysiological recordings. Typically, only a subset of the recorded neurons respond to a specific stimulus or experimental manipulation. Bayesian prevalence can be applied to estimate the proportion of neurons that respond in a particular way in the population of neurons in a particular brain region. For example, one could consider well-isolated single units recorded in the ventroposterior medial nucleus (VPm) of rodent thalamus (<xref ref-type="bibr" rid="bib47">Petersen et al., 2008</xref>). Of all the <italic>N</italic> cells recorded (across sessions and perhaps animals), a certain number <italic>k</italic> might be found to be sensitive to the velocity of whisker motion, based on a non-parametric test of an encoding model. Applying Bayesian prevalence to this data would give a posterior distribution for the proportion of neurons in VPm thalamus that are velocity sensitive in a particular experimental preparation. Our between-group comparison method could also be used to formally compare the population prevalence of a certain type of responsive neuron between different brain areas, between different species, or between different experimental conditions (e.g. before or after learning, or with or without a pharmacological intervention). Thus, although it is common in electrophysiology to have individual neurons as the replication unit and perform inference at that level, the inferential bridge to a population of cells that Bayesian prevalence provides offers a new perspective on the results of such studies.</p><p>One potential criticism is that the demonstration of within-participant effects, which the prevalence approach requires, sets a much higher bar of evidence. It might be impractical to reach sufficient within-participant power in some imaging modalities with typical experimental designs. However, many statisticians have explicitly argued that the replicability crisis results from standards of evidence for publication being too weak (<xref ref-type="bibr" rid="bib6">Benjamin et al., 2018</xref>). If so, the within-participant approach should lead to increased replicability. Indeed, it is common in neuroimaging studies to have no consistent pattern discernible in the effect size maps of individual participants, and yet such studies report a significant population mean effect in a focal region. In our view, this is problematic if our ultimate goal is to relate neuroimaging results to cognitive functions within individuals (<xref ref-type="bibr" rid="bib41">Naselaris et al., 2021</xref>; <xref ref-type="bibr" rid="bib57">Smith and Little, 2018</xref>). By contrast, as our simulations show (<xref ref-type="fig" rid="fig1">Figures 1B</xref> and <xref ref-type="fig" rid="fig2">2</xref>), strong evidence for a modulation can occur in the absence of a population mean effect when the effect is heterogeneous across participants.</p><p>It is natural to expect considerable heterogeneity to exist across populations for a wide range of experimental tasks. In fact, the normal distribution that underlies most inferences on the population mean implies such heterogeneity (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). In clinical studies of rare diseases, patient numbers are often limited, and heterogeneity can be higher relative to the healthy population. If an experiment is sufficiently powered within individual participants, then Bayesian prevalence provides a statistical statement about the population of patients with the disease, even from a small sample and without assuming a normal distribution for the population.</p><p>It has recently been suggested that researchers should define a smallest effect size of interest (SESOI) (<xref ref-type="bibr" rid="bib34">Lakens, 2017</xref>) and should consider this when calculating statistical power. We suggest that the SESOI should also be considered at the individual level and explicitly related to the population variance obtained from hierarchical mixed effects models. If the population variance is large, then this implies that individuals exist in the population who have effects that are larger than the individual SESOI, even if the population mean is not larger. This possibility is almost always ignored in applications of such modelling, which usually focus only on the population mean. Equivalence tests (<xref ref-type="bibr" rid="bib34">Lakens, 2017</xref>) could be applied within each participant and used to estimate the population prevalence of effects significantly smaller than the SESOI (i.e. the population prevalence of the absence of the effect).</p><p>Furthermore, the common assumption that effect sizes in the population follow a normal distribution is strong, although seldom justified (<xref ref-type="bibr" rid="bib35">Lakens et al., 2018</xref>). For example, information processing, decision making, risk taking, and other strategies might vary categorically within the healthy population and across clinical and sub-clinical groups (<xref ref-type="bibr" rid="bib40">Nachev et al., 2018</xref>; <xref ref-type="bibr" rid="bib57">Smith and Little, 2018</xref>). In neuroimaging studies, there are related issues of variability in anatomical and functional alignment. To address these issues, results are often spatially smoothed within individuals before performing population mean inference. However, many new experimental developments, such as high-resolution 7T fMRI to image cortical layers and high-density intracranial recordings in humans, pose increasing difficulties in terms of aligning data across participants. A major advantage of the prevalence approach is that we can perform within-participant inference corrected for multiple comparisons, and then estimate the population prevalence of these effects without requiring them to be precisely aligned (in space, time frequency, etc.) across participants (as illustrated in <xref ref-type="fig" rid="fig2">Figure 2</xref>). For example, one might report that 24/30 participants showed an EEG alpha band power effect between 500 ms and 1000 ms post stimulus, which implies a population prevalence MAP of 0.79 (96% HPDI [0.61 0.91]), without requiring these individual effects to occur at precisely the same time points in those 24 participants. Similarly, if within-participant inference is corrected for multiple comparisons, one can infer prevalence of, say, an effect in a certain layer of primary visual cortex, without requiring precise alignment of the anatomical location of the effect between participants (<xref ref-type="bibr" rid="bib13">Fedorenko, 2021</xref>).</p><p>Of course, there are many cases where the population mean is indeed the primary interest; in such cases, estimating and inferring on the population mean using hierarchical models is the most appropriate analysis, when possible. Linear mixed-effect models can also be interrogated in different ways to investigate the question of prevalence, for example, by examining the variance of the by-participant random slopes or explicitly computing prevalence of different effect sizes from the normal distribution fit to the population. It is possible to extend Bayesian hierarchical models to explicitly account for different sub-groups of participants (<xref ref-type="bibr" rid="bib5">Bartlema et al., 2014</xref>; <xref ref-type="bibr" rid="bib22">Haaf and Rouder, 2017</xref>; <xref ref-type="bibr" rid="bib23">Haaf and Rouder, 2019</xref>; <xref ref-type="bibr" rid="bib53">Rouder and Haaf, 2020</xref>). However, these approaches currently are not widely adopted, cannot easily be applied to non-linear or high-dimensional analysis methods common in neuroimaging, and add both mathematical and computational complexity compared to the second-level Bayesian prevalence method we present here, which is straightforward to apply to any first-level, within-participant analysis. Further, for many complex computational techniques—from modelling learning behaviour and decision making, to neuroimaging analysis techniques such as Representational Similarity Analysis, or high-dimensional encoding models (<xref ref-type="bibr" rid="bib24">Haxby et al., 2014</xref>)—it is currently not possible to employ a multi-level linear modelling approach down to the trial level due to the complexity of the non-linear analysis functions and models employed. It is also important to note that if the effect of interest is an unsigned information measure such as mutual information (<xref ref-type="bibr" rid="bib29">Ince et al., 2017</xref>) or classification accuracy (<xref ref-type="bibr" rid="bib24">Haxby et al., 2014</xref>) then the t-test on the population mean does not actually provide a random-effects inference that generalises to the population, but is equivalent to rejection of the global null showing only some sampled participants have an effect, that is, a fixed-effect analysis (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>). In such cases, Bayesian prevalence estimates may be more descriptive and provide stronger generalisation than the population mean NHST.</p><p>In sum, Bayesian prevalence has a broad range of applicability—spanning dense sampling studies with high within-participant power, to more traditional sampling models (more participants, fewer trials, e.g. <xref ref-type="fig" rid="fig1">Figure 1C</xref>). It is applicable to any behavioural study, including detailed computational models of behaviour, provided that model comparison or inference on model parameters can be performed within individuals. In neuroimaging, Bayesian prevalence can be applied to any imaging modality (EEG, MEG, fMRI, fNIRS, intracranial EEG), to individual neurons within a brain region (to infer the proportion of responsive neurons), and with any statistical approach, including non-linear and multivariate analysis methods. The crucial requirement is an adequately powered experiment to detect effects within individual participants (or other units of interest, e.g. neurons). We argue that ensuring experiments are adequately powered to detect effects within individuals would have a wide range of advantages, including strengthening the robustness and replicability of reported results.</p><sec id="s3-1"><title>Conclusions</title><p>While the problems that underlie the replication crisis are being increasingly recognised, there is currently no consensus as to alternative statistical approaches that are needed to address the underlying problems. Here, we propose that shifting our focus to quantifying and inferring effects within individuals addresses many of the pressing concerns recently highlighted in psychology and neuroimaging (<xref ref-type="bibr" rid="bib2">Amrhein et al., 2019</xref>; <xref ref-type="bibr" rid="bib6">Benjamin et al., 2018</xref>; <xref ref-type="bibr" rid="bib14">Forstmeier et al., 2017</xref>; <xref ref-type="bibr" rid="bib31">Ioannidis, 2005</xref>; <xref ref-type="bibr" rid="bib39">McShane et al., 2019</xref>). We present a Bayesian approach to estimating population prevalence which is broadly applicable as it places no assumptions on the within-participant tests nor on the distribution of effects in the population. Further, prevalence does not require a Bayesian treatment; frequentist inference approaches can be used instead. The crucial point is to shift our perspective to first evaluate effects within individual participants, who represent the natural replication unit for studies of brains and behaviour.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Simulations from an hierarchical population model</title><p>The data shown in <xref ref-type="fig" rid="fig1">Figure 1</xref> were simulated from the standard hierarchical model:<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mtext> </mml:mtext><mml:mo>∼</mml:mo><mml:mtext> </mml:mtext><mml:mi>N</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:msubsup><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf92"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the measurement made on the <italic>j</italic>th trial (out of <italic>T</italic>) of the <italic>i</italic>th participant (out of <italic>N</italic>). <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the mean of each individual participant, <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>w</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents a common within-participant standard deviation over trials, <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the standard deviation between participants, and <inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>μ</mml:mi><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> represents the overall population mean. This can be written as<disp-formula id="equ4"><mml:math id="m4"><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf97"><mml:msub><mml:mrow><mml:mi>η</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></inline-formula>, and <inline-formula><mml:math id="inf98"><mml:msub><mml:mrow><mml:mi>ϵ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>~</mml:mo><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></inline-formula>. Note that under this model the distribution of the within-participant means is <inline-formula><mml:math id="inf99"><mml:mi>N</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>μ</mml:mi></mml:mrow><mml:mrow><mml:mi>p</mml:mi><mml:mi>o</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mrow><mml:mi>σ</mml:mi></mml:mrow><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>)</mml:mo></mml:math></inline-formula>.</p></sec><sec id="s4-2"><title>Binomial model of population prevalence of the ground truth state of sampled units</title><p>We consider a population of experimental units (e.g. human participants or individual neurons) which are of two types: those that have a particular binary effect or property, and those that do not. We consider the population prevalence of the ground truth state of each unit <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, which is the proportion of the population from which the sample was drawn that have the effect (<inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). If the true status of each individual unit could be directly observed, then the sample could be modelled with a binomial distribution with probability parameter <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>. However, we cannot directly observe the true status of each unit. Instead, we apply to each unit a statistical test following the NHST framework, which outputs a binary result (which we term here positive vs. negative). This test has a false positive rate <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> and sensitivity <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. Thus, the probability that a randomly selected unit from the population that does not possess the defined effect but will produce a positive test result is <inline-formula><mml:math id="inf105"><mml:mi>α</mml:mi></mml:math></inline-formula>, while the probability that a randomly selected unit that does possess the defined effect will produce a positive test result is <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi><mml:mo>.</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> Under the assumption that the units are independent and <inline-formula><mml:math id="inf107"><mml:mi>α</mml:mi></mml:math></inline-formula> and <inline-formula><mml:math id="inf108"><mml:mi>β</mml:mi></mml:math></inline-formula> are constant across units, the number of positive tests <inline-formula><mml:math id="inf109"><mml:mi>k</mml:mi></mml:math></inline-formula> in a sample of size <inline-formula><mml:math id="inf110"><mml:mi>n</mml:mi></mml:math></inline-formula> can be modelled as a binomial distribution with parameter <inline-formula><mml:math id="inf111"><mml:mi>θ</mml:mi></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>; <xref ref-type="bibr" rid="bib16">Friston et al., 1999b</xref>; <xref ref-type="bibr" rid="bib15">Friston et al., 1999a</xref>; <xref ref-type="bibr" rid="bib51">Rogan and Gladen, 1978</xref>):<disp-formula id="equ5"><mml:math id="m5"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>θ</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>g</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mi>β</mml:mi></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-3"><title>Binomial model of population prevalence of true positives for a given test procedure</title><p>A major issue with the above approach is that it requires the sensitivity <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> to be specified and constant across units. <inline-formula><mml:math id="inf113"><mml:mi>β</mml:mi></mml:math></inline-formula> is the probability of a significant result given that the individual has an effect and differs as a function of the ground truth effect size. In general, we do not assume that all members of the population have the same effect size, so it is not possible to specify a common <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> for all individuals.</p><p>Therefore, rather than modelling the prevalence of ground truth effects, we consider the prevalence of true positive test results for the particular test procedure we employ,<inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ6"><mml:math id="m6"><mml:mrow><mml:mtable columnalign="right left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>X</mml:mi><mml:mo>=</mml:mo><mml:mi>k</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mtable columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:mi>n</mml:mi></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>k</mml:mi></mml:mtd></mml:mtr></mml:mtable><mml:mo>)</mml:mo></mml:mrow><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msup></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mi>θ</mml:mi></mml:mtd><mml:mtd><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>In this case, the only assumption is that test procedure has the same false positive rate <inline-formula><mml:math id="inf116"><mml:mi>α</mml:mi></mml:math></inline-formula> for every individual, which is easily satisfied by most common parametric and non-parametric statistical tests. Note that this is equivalent to estimating ground truth prevalence with a test with <inline-formula><mml:math id="inf117"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>. In general, a test with lower sensitivity allows inference of a higher prevalence for an observed <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> because some of the observed negative results will be missed true positive results. Therefore, prevalence of true positives obtained with <inline-formula><mml:math id="inf119"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> is a conservative lower bound on the prevalence of ground truth state (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>; <xref ref-type="bibr" rid="bib15">Friston et al., 1999a</xref>).</p><p>Throughout this paper we consider the prevalence of true positives for a given test procedure rather than the prevalence of ground truth state and so omit the subscript <italic>tp</italic>. This quantifies the proportion of the population that would be expected to provide a true positive test result (i.e. have a non-null effect that would be detected by the experimental test procedure considered).</p></sec><sec id="s4-4"><title>Frequentist estimation of and inference on population prevalence</title><p>Various frequentist approaches can be used with the above binomial model of statistical testing. First, the maximum likelihood estimate of the population prevalence of true positives can be obtained as<disp-formula id="equ7"><mml:math id="m7"><mml:mrow><mml:mrow><mml:mover><mml:mi>γ</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Standard bootstrap techniques (<xref ref-type="bibr" rid="bib32">Johnson, 2001</xref>) can give percentile bootstrap confidence intervals as an indication of uncertainty in this estimate. We can also explicitly test various null hypotheses at the population level. For example, we can test a compound null hypothesis <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>, termed the majority null (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>). This is chosen with the idea that a prevalence of &gt;50% supports a claim that the effect is <italic>typical</italic> in the population. Other explicit compound nulls of this form can also be tested (e.g. that <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:mn>0.75</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>). Alternatively, it is possible to infer a lower bound on the population prevalence by finding the largest value <inline-formula><mml:math id="inf123"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, such that <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>X</mml:mi><mml:mo>&gt;</mml:mo><mml:mi>k</mml:mi><mml:mtext> </mml:mtext><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mtext> </mml:mtext><mml:mi>γ</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>0.05</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> (<xref ref-type="bibr" rid="bib1">Allefeld et al., 2016</xref>; <xref ref-type="bibr" rid="bib9">Donhauser et al., 2018</xref>). This inferred lower bound provides a more graded output than a binary significance result of testing against a specific compound null (i.e. the continuous value <inline-formula><mml:math id="inf125"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>).</p></sec><sec id="s4-5"><title>Bayesian estimation of population prevalence</title><p>We apply standard Bayesian techniques to estimate the population prevalence parameter of this model (<xref ref-type="bibr" rid="bib18">Gelman, 2014</xref>). Assuming a beta prior distribution for <inline-formula><mml:math id="inf126"><mml:mi>θ</mml:mi></mml:math></inline-formula> with shape parameters <inline-formula><mml:math id="inf127"><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi>s</mml:mi></mml:math></inline-formula>, together with a binomial likelihood function, the posterior distribution for <inline-formula><mml:math id="inf128"><mml:mi>θ</mml:mi></mml:math></inline-formula> is given by a beta distribution with parameters <inline-formula><mml:math id="inf129"><mml:mo>(</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>r</mml:mi><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mi>s</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula>, truncated to the interval <inline-formula><mml:math id="inf130"><mml:mo>[</mml:mo><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>]</mml:mo></mml:math></inline-formula>, where <inline-formula><mml:math id="inf131"><mml:mi>k</mml:mi></mml:math></inline-formula> is the number of participants showing an above-threshold effect out of <inline-formula><mml:math id="inf132"><mml:mi>n</mml:mi></mml:math></inline-formula> tested. In the examples shown here, we use a uniform prior (beta with shape parameters <inline-formula><mml:math id="inf133"><mml:mi>r</mml:mi><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula>), as in the general case there is no prior information regarding <inline-formula><mml:math id="inf134"><mml:mi>θ</mml:mi></mml:math></inline-formula>. This implies a uniform prior also for <inline-formula><mml:math id="inf135"><mml:mi>γ</mml:mi></mml:math></inline-formula>, so, a priori, we consider any value of population prevalence equally likely. While we default to the uniform prior, the code supports any beta distribution as a prior. Alternative priors could be implemented via Markov chain Monte Carlo methods (<xref ref-type="bibr" rid="bib18">Gelman, 2014</xref>) together with the models described here. Note that similar Bayesian approaches have been applied in the field of epidemiology, where sometimes multiple complementary diagnostic tests for a disease are applied with or without a gold standard diagnosis in a subset of the sampled units (<xref ref-type="bibr" rid="bib7">Berkvens et al., 2006</xref>; <xref ref-type="bibr" rid="bib12">Enøe et al., 2000</xref>; <xref ref-type="bibr" rid="bib33">Joseph et al., 1995</xref>).</p><p>Under the uniform prior, the Bayesian MAP estimate for prevalence proportion of true positives is available analytically and is equivalent to the maximum likelihood estimate:<disp-formula id="equ8"><mml:math id="m8"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mi>a</mml:mi><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>k</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mi>n</mml:mi><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>Following <xref ref-type="bibr" rid="bib38">McElreath, 2016</xref>, we present 96% HPDIs here to emphasise the arbitrary nature of this value and reduce the temptation to interpret the interval as a frequentist <italic>p</italic>=0.05 inference.</p></sec><sec id="s4-6"><title>Bayesian estimation of the prevalence difference between two independent groups</title><p>We consider here a situation where the same test is applied to units sampled from two different populations. In addition to the prevalence of true positives within each population, we wish to directly estimate the difference in prevalence between the two populations. We denote the prevalence within each population as <inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula>, respectively. We sample <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>n</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mstyle></mml:math></inline-formula> participants at random from each population and record <inline-formula><mml:math id="inf139"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, the number of significant within-participant tests in each sample. Assuming independent uniform priors on the prevalences and associated <inline-formula><mml:math id="inf140"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> variables as above with:<disp-formula id="equ9"><mml:math id="m9"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>then the posterior distribution for <inline-formula><mml:math id="inf141"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> is given by the product of two truncated beta distributions, with parameters <inline-formula><mml:math id="inf142"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn><mml:mo>)</mml:mo></mml:math></inline-formula>, respectively, both truncated to the interval <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>α</mml:mi><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. The prevalence difference can be obtained as:<disp-formula id="equ10"><mml:math id="m10"><mml:mrow><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>For non-truncated beta distributions, an analytic exact result is available (<xref ref-type="bibr" rid="bib48">Pham-Gia et al., 1993</xref>). This result could be extended to provide an exact distribution for the prevalence difference, but the mathematical expressions involved are quite complex. We find it simpler to employ Monte Carlo methods, which can provide as close an approximation to the exact answer as desired. Here we use Monte Carlo methods to draw samples from the posterior for <inline-formula><mml:math id="inf144"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula>, obtaining samples for the prevalence difference with the above expression. We use these samples to numerically compute the MAP and HPDIs.</p></sec><sec id="s4-7"><title>Bayesian estimation of the prevalence difference of two different tests within the same sample of participants</title><p>In this situation, we consider that two different test procedures are applied to a single sample of <inline-formula><mml:math id="inf145"><mml:mi>n</mml:mi></mml:math></inline-formula> units. We assume both tests have the same value of <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>α</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and define:<disp-formula id="equ11"><mml:math id="m11"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></disp-formula></p><p>Here <inline-formula><mml:math id="inf147"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the probability that a randomly selected unit from the population will show a positive result on the <italic>i</italic>th test, and <inline-formula><mml:math id="inf148"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the population prevalence of true positives associated with the <italic>i</italic>th test. Now, each unit provides one of four mutually exclusive results based on the combination of binary results from the two tests. <inline-formula><mml:math id="inf149"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> represents the number of units that have a positive result in both tests, <inline-formula><mml:math id="inf150"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> represent the number of units that have a positive result only in the first or second test, respectively, and <inline-formula><mml:math id="inf151"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the number of units that do not show a positive result in either test. So <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>k</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>n</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>. We can define analogous variables <inline-formula><mml:math id="inf153"><mml:msub><mml:mrow><mml:mi mathvariant="bold-italic">θ</mml:mi><mml:mo>=</mml:mo><mml:mo>{</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>}</mml:mo></mml:math></inline-formula>, representing the population proportion for each of the four combined test outcomes. Note that <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>,</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>. The marginal success probabilities <inline-formula><mml:math id="inf156"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> can be expressed as:<disp-formula id="equ12"><mml:math id="m12"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:math></disp-formula></p><p>and so<disp-formula id="equ13"><mml:math id="m13"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:math></disp-formula></p><p>The marginal probabilities <inline-formula><mml:math id="inf157"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> are subject to the constraints<disp-formula id="equ14"><mml:math id="m14"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>and so<disp-formula id="equ15"><mml:math id="m15"><mml:mrow><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mtext> </mml:mtext><mml:mi>α</mml:mi><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></disp-formula></p><p>Assuming a uniform prior and a multinomial distribution for the <inline-formula><mml:math id="inf158"><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>, the posterior of <inline-formula><mml:math id="inf159"><mml:mi mathvariant="bold-italic">θ</mml:mi></mml:math></inline-formula> is a truncated Dirichlet distribution with parameters <inline-formula><mml:math id="inf160"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>k</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:math></inline-formula> subject to the constraints above (which are analogous to the truncation of the beta posterior distribution in the case of a single test). We use a Monte Carlo approach to draw samples from this posterior following a modified stick-breaking process.</p><list list-type="bullet"><list-item><p>Draw a sample <inline-formula><mml:math id="inf161"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> from a <inline-formula><mml:math id="inf162"><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> distribution truncated to the interval [0,1].</p></list-item><list-item><p>Draw a sample <inline-formula><mml:math id="inf163"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> from a <inline-formula><mml:math id="inf164"><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> distribution truncated to the interval <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Set <inline-formula><mml:math id="inf166"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p>Draw a sample <inline-formula><mml:math id="inf167"><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> from a <inline-formula><mml:math id="inf168"><mml:mi>B</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:math></inline-formula> distribution truncated to the interval <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mo form="prefix" movablelimits="true">max</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>α</mml:mi><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mtext> </mml:mtext><mml:mo form="prefix" movablelimits="true">min</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfrac><mml:mo>,</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. Set <inline-formula><mml:math id="inf170"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p>Set <inline-formula><mml:math id="inf171"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>.</p></list-item><list-item><p>Then <inline-formula><mml:math id="inf172"><mml:mfenced separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi> </mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math></inline-formula> is a draw from the required truncated Dirichlet distribution, and <inline-formula><mml:math id="inf173"><mml:mo>(</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo><mml:mo>/</mml:mo><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>α</mml:mi><mml:mo>)</mml:mo></mml:math></inline-formula> is a draw from the posterior distribution of the prevalence difference.</p></list-item></list><p>We use these samples to numerically compute properties like the MAP estimate and HPDIs.</p><p>To specify a ground truth to simulate data from two tests applied to the same participants (<xref ref-type="fig" rid="fig3">Figure 3</xref>), we require <inline-formula><mml:math id="inf174"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf175"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, the population prevalences of the two tested effects, together with <inline-formula><mml:math id="inf176"><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, the correlation between the presence of the two effects across the population. From this we can calculate <inline-formula><mml:math id="inf177"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula>, the proportion of true positive results to both tests as<disp-formula id="equ16"><mml:math id="m16"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>ρ</mml:mi></mml:mrow><mml:mrow><mml:mn>12</mml:mn></mml:mrow></mml:msub><mml:msqrt><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mfenced separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>(</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>)</mml:mo></mml:msqrt></mml:math></disp-formula></p><p>Similarly, we can define <inline-formula><mml:math id="inf178"><mml:msub><mml:mrow><mml:mi>γ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> representing the population proportions corresponding to the other test result configurations. Then we can generate multinomial samples using the parameters <inline-formula><mml:math id="inf179"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> computed as<disp-formula id="equ17"><mml:math id="m17"><mml:mrow><mml:mtable columnalign="left left" columnspacing="1em" rowspacing="4pt"><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msup><mml:mi>α</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mi>α</mml:mi><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>α</mml:mi><mml:mo>+</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:msub><mml:mi>γ</mml:mi><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>00</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>11</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>10</mml:mn></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>01</mml:mn></mml:mrow></mml:msub></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec id="s4-8"><title>Prevalence as a function of effect size threshold</title><p>Estimating the prevalence of <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> proceeds as for prevalence inference based on within-participant NHST. One additional step is the need to calculate <inline-formula><mml:math id="inf181"><mml:mi>α</mml:mi></mml:math></inline-formula>, the false positive rate under the null hypothesis of no effect, for each threshold value <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. This is simply the probability of <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> under the null hypothesis. In the examples shown here, we calculate this from the cumulative distribution function of the appropriate t-distribution, but for other tests this could also be estimated non-parametrically. A number of <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> values are selected, linearly spaced over the observed range of the sample. For each of these values the count of the number of participants satisfying the inequality and the <inline-formula><mml:math id="inf185"><mml:mi>α</mml:mi></mml:math></inline-formula> value corresponding to the inequality are used to obtain the Bayesian posterior for prevalence of true positives. Note that this can be applied to either tail <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mrow><mml:mover><mml:mi>E</mml:mi><mml:mo stretchy="false">^</mml:mo></mml:mover></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>.</p></sec><sec id="s4-9"><title>Data availability</title><p>Scripts implementing the simulations and creating the figures from the paper are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/robince/bayesian-prevalence">https://github.com/robince/bayesian-prevalence</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7a28ad70d4094c416d18e3c3c6b1d80861e9b16f;origin=https://github.com/robince/bayesian-prevalence;visit=swh:1:snp:e9b55eb1618ec5862ec83404fa7195053dc1b7d9;anchor=swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec">swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec</ext-link>, <xref ref-type="bibr" rid="bib30">Ince, 2021</xref>).</p></sec><sec id="s4-10"><title>Code availability</title><p>To accompany this paper, we provide functions in MATLAB, Python, and R to calculate the Bayesian prevalence posterior density (e.g. to plot the full posterior distribution), the MAP estimate of the population prevalence, HPDI intervals of the posterior and lower bound quantiles of the posterior, as well as prevalence differences between samples or between tests within a sample. We also provide example scripts that produce posterior plots as in <xref ref-type="fig" rid="fig1">Figure 1E</xref>. See <ext-link ext-link-type="uri" xlink:href="https://github.com/robince/bayesian-prevalence">https://github.com/robince/bayesian-prevalence</ext-link>.</p></sec></sec></body><back><sec id="s5" sec-type="additional-information"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Formal analysis, Funding acquisition, Investigation, Methodology, Project administration, Software, Visualization, Writing – original draft, Writing – review and editing</p></fn><fn fn-type="con" id="con2"><p>Data curation, Formal analysis, Validation, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Formal analysis, Investigation, Methodology, Software, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Funding acquisition, Writing – review and editing</p></fn></fn-group></sec><sec id="s6" sec-type="supplementary-material"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media mime-subtype="docx" mimetype="application" xlink:href="elife-62461-transrepform1-v1.docx"/></supplementary-material></sec><sec id="s7" sec-type="data-availability"><title>Data availability</title><p>Scripts implementing the simulations and creating the figures from the manuscript are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/robince/bayesian-prevalence">https://github.com/robince/bayesian-prevalence</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec">https://archive.softwareheritage.org/swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec</ext-link>). A toolbox of functions to calculate Bayesian prevalence is provided for Matlab, Python and R.</p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Carsten Allefeld for useful discussions. RAAI was supported by the Wellcome Trust [214120/Z/18/Z]. PGS was supported by the EPSRC [MURI 1720461] and the Wellcome Trust [107802]. PGS is a Royal Society Wolfson Fellow [RSWF\R3\183002].</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Allefeld</surname><given-names>C</given-names></name><name><surname>Görgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Valid population inference for information-based imaging: From the second-level t-test to prevalence inference</article-title><source>NeuroImage</source><volume>141</volume><fpage>378</fpage><lpage>392</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.07.040</pub-id><pub-id pub-id-type="pmid">27450073</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amrhein</surname><given-names>V</given-names></name><name><surname>Greenland</surname><given-names>S</given-names></name><name><surname>McShane</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Retire statistical significance</article-title><source>Nature</source><volume>567</volume><fpage>305</fpage><lpage>307</lpage><pub-id pub-id-type="doi">10.1038/d41586-019-00857-9</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Angrist</surname><given-names>JD</given-names></name><name><surname>Pischke</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Mastering ’metrics: The Path from Cause to Effect</source><publisher-loc>Princeton ; Oxford</publisher-loc><publisher-name>Princeton University Press</publisher-name></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>DH</given-names></name><name><surname>Vilidaite</surname><given-names>G</given-names></name><name><surname>Lygo</surname><given-names>FA</given-names></name><name><surname>Smith</surname><given-names>AK</given-names></name><name><surname>Flack</surname><given-names>TR</given-names></name><name><surname>Gouws</surname><given-names>AD</given-names></name><name><surname>Andrews</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Power contours: Optimising sample size and precision in experimental psychology and human neuroscience</article-title><source>Psychological Methods</source><volume>1</volume><elocation-id>met0000337</elocation-id><pub-id pub-id-type="doi">10.1037/met0000337</pub-id><pub-id pub-id-type="pmid">32673043</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bartlema</surname><given-names>A</given-names></name><name><surname>Lee</surname><given-names>M</given-names></name><name><surname>Wetzels</surname><given-names>R</given-names></name><name><surname>Vanpaemel</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A bayesian hierarchical mixture approach to individual differences: Case studies in selective attention and representation in category learning</article-title><source>Journal of Mathematical Psychology</source><volume>59</volume><fpage>132</fpage><lpage>150</lpage><pub-id pub-id-type="doi">10.1016/j.jmp.2013.12.002</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamin</surname><given-names>DJ</given-names></name><name><surname>Berger</surname><given-names>JO</given-names></name><name><surname>Johannesson</surname><given-names>M</given-names></name><name><surname>Nosek</surname><given-names>BA</given-names></name><name><surname>Wagenmakers</surname><given-names>E-J</given-names></name><name><surname>Berk</surname><given-names>R</given-names></name><name><surname>Bollen</surname><given-names>KA</given-names></name><name><surname>Brembs</surname><given-names>B</given-names></name><name><surname>Brown</surname><given-names>L</given-names></name><name><surname>Camerer</surname><given-names>C</given-names></name><name><surname>Cesarini</surname><given-names>D</given-names></name><name><surname>Chambers</surname><given-names>CD</given-names></name><name><surname>Clyde</surname><given-names>M</given-names></name><name><surname>Cook</surname><given-names>TD</given-names></name><name><surname>De Boeck</surname><given-names>P</given-names></name><name><surname>Dienes</surname><given-names>Z</given-names></name><name><surname>Dreber</surname><given-names>A</given-names></name><name><surname>Easwaran</surname><given-names>K</given-names></name><name><surname>Efferson</surname><given-names>C</given-names></name><name><surname>Fehr</surname><given-names>E</given-names></name><name><surname>Fidler</surname><given-names>F</given-names></name><name><surname>Field</surname><given-names>AP</given-names></name><name><surname>Forster</surname><given-names>M</given-names></name><name><surname>George</surname><given-names>EI</given-names></name><name><surname>Gonzalez</surname><given-names>R</given-names></name><name><surname>Goodman</surname><given-names>S</given-names></name><name><surname>Green</surname><given-names>E</given-names></name><name><surname>Green</surname><given-names>DP</given-names></name><name><surname>Greenwald</surname><given-names>AG</given-names></name><name><surname>Hadfield</surname><given-names>JD</given-names></name><name><surname>Hedges</surname><given-names>LV</given-names></name><name><surname>Held</surname><given-names>L</given-names></name><name><surname>Hua Ho</surname><given-names>T</given-names></name><name><surname>Hoijtink</surname><given-names>H</given-names></name><name><surname>Hruschka</surname><given-names>DJ</given-names></name><name><surname>Imai</surname><given-names>K</given-names></name><name><surname>Imbens</surname><given-names>G</given-names></name><name><surname>Ioannidis</surname><given-names>JPA</given-names></name><name><surname>Jeon</surname><given-names>M</given-names></name><name><surname>Jones</surname><given-names>JH</given-names></name><name><surname>Kirchler</surname><given-names>M</given-names></name><name><surname>Laibson</surname><given-names>D</given-names></name><name><surname>List</surname><given-names>J</given-names></name><name><surname>Little</surname><given-names>R</given-names></name><name><surname>Lupia</surname><given-names>A</given-names></name><name><surname>Machery</surname><given-names>E</given-names></name><name><surname>Maxwell</surname><given-names>SE</given-names></name><name><surname>McCarthy</surname><given-names>M</given-names></name><name><surname>Moore</surname><given-names>DA</given-names></name><name><surname>Morgan</surname><given-names>SL</given-names></name><name><surname>Munafó</surname><given-names>M</given-names></name><name><surname>Nakagawa</surname><given-names>S</given-names></name><name><surname>Nyhan</surname><given-names>B</given-names></name><name><surname>Parker</surname><given-names>TH</given-names></name><name><surname>Pericchi</surname><given-names>L</given-names></name><name><surname>Perugini</surname><given-names>M</given-names></name><name><surname>Rouder</surname><given-names>J</given-names></name><name><surname>Rousseau</surname><given-names>J</given-names></name><name><surname>Savalei</surname><given-names>V</given-names></name><name><surname>Schönbrodt</surname><given-names>FD</given-names></name><name><surname>Sellke</surname><given-names>T</given-names></name><name><surname>Sinclair</surname><given-names>B</given-names></name><name><surname>Tingley</surname><given-names>D</given-names></name><name><surname>Van Zandt</surname><given-names>T</given-names></name><name><surname>Vazire</surname><given-names>S</given-names></name><name><surname>Watts</surname><given-names>DJ</given-names></name><name><surname>Winship</surname><given-names>C</given-names></name><name><surname>Wolpert</surname><given-names>RL</given-names></name><name><surname>Xie</surname><given-names>Y</given-names></name><name><surname>Young</surname><given-names>C</given-names></name><name><surname>Zinman</surname><given-names>J</given-names></name><name><surname>Johnson</surname><given-names>VE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Redefine statistical significance</article-title><source>Nature Human Behaviour</source><volume>2</volume><fpage>6</fpage><lpage>10</lpage><pub-id pub-id-type="doi">10.1038/s41562-017-0189-z</pub-id><pub-id pub-id-type="pmid">30980045</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berkvens</surname><given-names>D</given-names></name><name><surname>Speybroeck</surname><given-names>N</given-names></name><name><surname>Praet</surname><given-names>N</given-names></name><name><surname>Adel</surname><given-names>A</given-names></name><name><surname>Lesaffre</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Estimating disease prevalence in a bayesian framework using probabilistic constraints</article-title><source>Epidemiology</source><volume>17</volume><fpage>145</fpage><lpage>153</lpage><pub-id pub-id-type="doi">10.1097/01.ede.0000198422.64801.8d</pub-id><pub-id pub-id-type="pmid">16477254</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braga</surname><given-names>RM</given-names></name><name><surname>Buckner</surname><given-names>RL</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Parallel interdigitated distributed networks within the individual estimated by intrinsic functional connectivity</article-title><source>Neuron</source><volume>95</volume><fpage>457</fpage><lpage>471</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.06.038</pub-id><pub-id pub-id-type="pmid">28728026</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Donhauser</surname><given-names>PW</given-names></name><name><surname>Florin</surname><given-names>E</given-names></name><name><surname>Baillet</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Imaging of neural oscillations with embedded inferential and group prevalence statistics</article-title><source>PLOS Computational Biology</source><volume>14</volume><elocation-id>e1005990</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005990</pub-id><pub-id pub-id-type="pmid">29408902</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Edgington</surname><given-names>E</given-names></name><name><surname>Edgington</surname><given-names>E</given-names></name><name><surname>Onghena</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Randomization Tests</source><publisher-name>CRC Press/Taylor &amp; Francis Group</publisher-name><pub-id pub-id-type="doi">10.1201/9781420011814</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Elliott</surname><given-names>ML</given-names></name><name><surname>Knodt</surname><given-names>AR</given-names></name><name><surname>Ireland</surname><given-names>D</given-names></name><name><surname>Morris</surname><given-names>ML</given-names></name><name><surname>Poulton</surname><given-names>R</given-names></name><name><surname>Ramrakha</surname><given-names>S</given-names></name><name><surname>Sison</surname><given-names>ML</given-names></name><name><surname>Moffitt</surname><given-names>TE</given-names></name><name><surname>Caspi</surname><given-names>A</given-names></name><name><surname>Hariri</surname><given-names>AR</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>What Is the Test-Retest Reliability of Common Task-Functional MRI Measures? New Empirical Evidence and a Meta-Analysis</article-title><source>Psychological Science</source><volume>31</volume><fpage>792</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1177/0956797620916786</pub-id><pub-id pub-id-type="pmid">32489141</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Enøe</surname><given-names>C</given-names></name><name><surname>Georgiadis</surname><given-names>MP</given-names></name><name><surname>Johnson</surname><given-names>WO</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Estimation of sensitivity and specificity of diagnostic tests and disease prevalence when the true disease state is unknown</article-title><source>Preventive Veterinary Medicine</source><volume>45</volume><fpage>61</fpage><lpage>81</lpage><pub-id pub-id-type="doi">10.1016/s0167-5877(00)00117-3</pub-id><pub-id pub-id-type="pmid">10802334</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fedorenko</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The early origins and the growing popularity of the individual-subject analytic approach in human neuroscience</article-title><source>Current Opinion in Behavioral Sciences</source><volume>40</volume><fpage>105</fpage><lpage>112</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.02.023</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Forstmeier</surname><given-names>W</given-names></name><name><surname>Wagenmakers</surname><given-names>EJ</given-names></name><name><surname>Parker</surname><given-names>TH</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Detecting and avoiding likely false-positive findings – a practical guide</article-title><source>Biological Reviews</source><volume>92</volume><fpage>1941</fpage><lpage>1968</lpage><pub-id pub-id-type="doi">10.1111/brv.12315</pub-id><pub-id pub-id-type="pmid">27879038</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Price</surname><given-names>CJ</given-names></name><name><surname>Büchel</surname><given-names>C</given-names></name><name><surname>Worsley</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="1999">1999a</year><article-title>Multisubject fMRI Studies and Conjunction Analyses</article-title><source>NeuroImage</source><volume>10</volume><fpage>385</fpage><lpage>396</lpage><pub-id pub-id-type="doi">10.1006/nimg.1999.0484</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Worsley</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="1999">1999b</year><article-title>How Many Subjects Constitute a Study</article-title><source>NeuroImage</source><volume>10</volume><fpage>1</fpage><lpage>5</lpage><pub-id pub-id-type="doi">10.1006/nimg.1999.0439</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2007">2007</year><source>Statistical Parametric Mapping: The Analysis of Functional Brain Images</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib18"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gelman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Bayesian Data Analysis</source><publisher-loc>Boca Raton</publisher-loc><publisher-name>CRC Press</publisher-name></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gordon</surname><given-names>EM</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Gilmore</surname><given-names>AW</given-names></name><name><surname>Newbold</surname><given-names>DJ</given-names></name><name><surname>Greene</surname><given-names>DJ</given-names></name><name><surname>Berg</surname><given-names>JJ</given-names></name><name><surname>Ortega</surname><given-names>M</given-names></name><name><surname>Hoyt-Drazen</surname><given-names>C</given-names></name><name><surname>Gratton</surname><given-names>C</given-names></name><name><surname>Sun</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Precision Functional Mapping of Individual Human Brains</article-title><source>Neuron</source><volume>95</volume><fpage>791</fpage><lpage>807</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.07.011</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gratton</surname><given-names>C</given-names></name><name><surname>Braga</surname><given-names>RM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Editorial overview: Deep imaging of the individual brain: past, practice, and promise</article-title><source>Current Opinion in Behavioral Sciences</source><volume>40</volume><fpage>iii</fpage><lpage>vi</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2021.06.011</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grice</surname><given-names>JW</given-names></name><name><surname>Medellin</surname><given-names>E</given-names></name><name><surname>Jones</surname><given-names>I</given-names></name><name><surname>Horvath</surname><given-names>S</given-names></name><name><surname>McDaniel</surname><given-names>H</given-names></name><name><surname>O’lansen</surname><given-names>C</given-names></name><name><surname>Baker</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Persons as Effect Sizes</article-title><source>Advances in Methods and Practices in Psychological Science</source><volume>3</volume><fpage>443</fpage><lpage>455</lpage><pub-id pub-id-type="doi">10.1177/2515245920922982</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haaf</surname><given-names>JM</given-names></name><name><surname>Rouder</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Developing constraint in bayesian mixed models</article-title><source>Psychological Methods</source><volume>22</volume><fpage>779</fpage><lpage>798</lpage><pub-id pub-id-type="doi">10.1037/met0000156</pub-id><pub-id pub-id-type="pmid">29265850</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haaf</surname><given-names>JM</given-names></name><name><surname>Rouder</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Some do and some don’t? Accounting for variability of individual difference structures</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>26</volume><fpage>772</fpage><lpage>789</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1522-x</pub-id><pub-id pub-id-type="pmid">30251148</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haxby</surname><given-names>JV</given-names></name><name><surname>Connolly</surname><given-names>AC</given-names></name><name><surname>Guntupalli</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Decoding Neural Representational Spaces Using Multivariate Pattern Analysis</article-title><source>Annual Review of Neuroscience</source><volume>37</volume><fpage>435</fpage><lpage>456</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062012-170325</pub-id><pub-id pub-id-type="pmid">25002277</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hedge</surname><given-names>C</given-names></name><name><surname>Powell</surname><given-names>G</given-names></name><name><surname>Sumner</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The reliability paradox: Why robust cognitive tasks do not produce reliable individual differences</article-title><source>Behav Res</source><volume>50</volume><fpage>1166</fpage><lpage>1186</lpage><pub-id pub-id-type="doi">10.3758/s13428-017-0935-1</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Holmes</surname><given-names>AP</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Generalisability, Random Effects &amp; Population Inference</article-title><source>NeuroImage</source><volume>7</volume><elocation-id>S754</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(18)31587-8</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Huth</surname><given-names>AG</given-names></name><name><surname>de Heer</surname><given-names>WA</given-names></name><name><surname>Griffiths</surname><given-names>TL</given-names></name><name><surname>Theunissen</surname><given-names>FE</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Natural speech reveals the semantic maps that tile human cerebral cortex</article-title><source>Nature</source><volume>532</volume><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="doi">10.1038/nature17637</pub-id><pub-id pub-id-type="pmid">27121839</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>van Rijsbergen</surname><given-names>NJ</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Tracing the Flow of Perceptual Features in an Algorithmic Brain Network</article-title><source>Scientific Reports</source><volume>5</volume><elocation-id>17681</elocation-id><pub-id pub-id-type="doi">10.1038/srep17681</pub-id><pub-id pub-id-type="pmid">26635299</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>Giordano</surname><given-names>BL</given-names></name><name><surname>Kayser</surname><given-names>C</given-names></name><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A statistical framework for neuroimaging data analysis based on mutual information estimated via a gaussian copula</article-title><source>Human Brain Mapping</source><volume>38</volume><fpage>1541</fpage><lpage>1573</lpage><pub-id pub-id-type="doi">10.1002/hbm.23471</pub-id><pub-id pub-id-type="pmid">27860095</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Ince</surname><given-names>RAA</given-names></name></person-group><year iso-8601-date="2021">2021</year><data-title>Bayesian-prevalence</data-title><version designator="swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec">swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:7a28ad70d4094c416d18e3c3c6b1d80861e9b16f;origin=https://github.com/robince/bayesian-prevalence;visit=swh:1:snp:e9b55eb1618ec5862ec83404fa7195053dc1b7d9;anchor=swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec">https://archive.softwareheritage.org/swh:1:dir:7a28ad70d4094c416d18e3c3c6b1d80861e9b16f;origin=https://github.com/robince/bayesian-prevalence;visit=swh:1:snp:e9b55eb1618ec5862ec83404fa7195053dc1b7d9;anchor=swh:1:rev:a10f2760930f7638d1c2a73944719e6283aedcec</ext-link></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ioannidis</surname><given-names>JPA</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Why most published research findings are false</article-title><source>PLOS Medicine</source><volume>2</volume><elocation-id>e124</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pmed.0020124</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Johnson</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="2001">2001</year><chapter-title><italic>Teaching Statistics</italic></chapter-title><person-group person-group-type="editor"><name><surname>Johnson</surname><given-names>RW</given-names></name></person-group><source>An Introduction to the Bootstrap</source><publisher-name>CRC press</publisher-name><fpage>49</fpage><lpage>54</lpage><pub-id pub-id-type="doi">10.1111/1467-9639.00050</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Joseph</surname><given-names>L</given-names></name><name><surname>Gyorkos</surname><given-names>TW</given-names></name><name><surname>Coupal</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Bayesian estimation of disease prevalence and the parameters of diagnostic tests in the absence of a gold standard</article-title><source>American Journal of Epidemiology</source><volume>141</volume><fpage>263</fpage><lpage>272</lpage><pub-id pub-id-type="doi">10.1093/oxfordjournals.aje.a117428</pub-id><pub-id pub-id-type="pmid">7840100</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Equivalence tests: A practical primer for T tests, correlations, and meta-analyses</article-title><source>Social Psychological and Personality Science</source><volume>8</volume><fpage>355</fpage><lpage>362</lpage><pub-id pub-id-type="doi">10.1177/1948550617697177</pub-id><pub-id pub-id-type="pmid">28736600</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lakens</surname><given-names>D</given-names></name><name><surname>Adolfi</surname><given-names>FG</given-names></name><name><surname>Albers</surname><given-names>CJ</given-names></name><name><surname>Anvari</surname><given-names>F</given-names></name><name><surname>Apps</surname><given-names>MAJ</given-names></name><name><surname>Argamon</surname><given-names>SE</given-names></name><name><surname>Baguley</surname><given-names>T</given-names></name><name><surname>Becker</surname><given-names>RB</given-names></name><name><surname>Benning</surname><given-names>SD</given-names></name><name><surname>Bradford</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Justify your alpha</article-title><source>Nature Human Behaviour</source><volume>2</volume><elocation-id>168</elocation-id><pub-id pub-id-type="doi">10.1038/s41562-018-0311-x</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Gordon</surname><given-names>EM</given-names></name><name><surname>Adeyemo</surname><given-names>B</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Joo</surname><given-names>SJ</given-names></name><name><surname>Chen</surname><given-names>MY</given-names></name><name><surname>Gilmore</surname><given-names>AW</given-names></name><name><surname>McDermott</surname><given-names>KB</given-names></name><name><surname>Nelson</surname><given-names>SM</given-names></name><name><surname>Dosenbach</surname><given-names>NUF</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Functional System and Areal Organization of a Highly Sampled Individual Human Brain</article-title><source>Neuron</source><volume>87</volume><fpage>657</fpage><lpage>670</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2015.06.037</pub-id><pub-id pub-id-type="pmid">26212711</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Little</surname><given-names>DR</given-names></name><name><surname>Smith</surname><given-names>PL</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Replication is already mainstream: Lessons from small- n designs</article-title><source>Behavioral and Brain Sciences</source><volume>41</volume><elocation-id>766</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X18000766</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>McElreath</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2016">2016</year><source>Statistical Rethinking: A Bayesian Course with Examples in R and Stan</source><publisher-loc>Boca Raton</publisher-loc><publisher-name>CRC Press/Taylor &amp; Francis Group</publisher-name></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McShane</surname><given-names>BB</given-names></name><name><surname>Gal</surname><given-names>D</given-names></name><name><surname>Gelman</surname><given-names>A</given-names></name><name><surname>Robert</surname><given-names>C</given-names></name><name><surname>Tackett</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Abandon statistical significance</article-title><source>The American Statistician</source><volume>73</volume><fpage>235</fpage><lpage>245</lpage><pub-id pub-id-type="doi">10.1080/00031305.2018.1527253</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nachev</surname><given-names>P</given-names></name><name><surname>Rees</surname><given-names>G</given-names></name><name><surname>Frackowiak</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Lost in Translation</article-title><source>F1000Research</source><volume>7</volume><elocation-id>620</elocation-id><pub-id pub-id-type="doi">10.12688/f1000research.15020.2</pub-id><pub-id pub-id-type="pmid">30210786</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Allen</surname><given-names>E</given-names></name><name><surname>Kay</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Extensive sampling for complete models of individual brains</article-title><source>Current Opinion in Behavioral Sciences</source><volume>40</volume><fpage>45</fpage><lpage>51</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.12.008</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><collab>Neuroscience, S. for</collab></person-group><year iso-8601-date="2020">2020</year><article-title>Consideration of sample size in neuroscience studies</article-title><source>The Journal of Neuroscience</source><volume>40</volume><fpage>4076</fpage><lpage>4077</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0866-20.2020</pub-id><pub-id pub-id-type="pmid">32434857</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nichols</surname><given-names>T</given-names></name><name><surname>Brett</surname><given-names>M</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Wager</surname><given-names>T</given-names></name><name><surname>Poline</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Valid conjunction inference with the minimum statistic</article-title><source>NeuroImage</source><volume>25</volume><fpage>653</fpage><lpage>660</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.12.005</pub-id><pub-id pub-id-type="pmid">15808966</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Pearl</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2009">2009</year><source>Causality</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511803161</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>W</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name><name><surname>Friston</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2003">2003</year><source>Random Effects Analysis</source><publisher-name>Human Brain Function</publisher-name></element-citation></ref><ref id="bib46"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Penny</surname><given-names>W</given-names></name><name><surname>Holmes</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><conf-name>Random effects analysis</conf-name><article-title>Statistical Parametric Mapping: The Analysis of Functional Brain Images</article-title><fpage>156</fpage><lpage>165</lpage></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Petersen</surname><given-names>RS</given-names></name><name><surname>Brambilla</surname><given-names>M</given-names></name><name><surname>Bale</surname><given-names>MR</given-names></name><name><surname>Alenda</surname><given-names>A</given-names></name><name><surname>Panzeri</surname><given-names>S</given-names></name><name><surname>Montemurro</surname><given-names>MA</given-names></name><name><surname>Maravall</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Diverse and temporally precise kinetic feature selectivity in the VPM thalamic nucleus</article-title><source>Neuron</source><volume>60</volume><fpage>890</fpage><lpage>903</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2008.09.041</pub-id><pub-id pub-id-type="pmid">19081382</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Pham-Gia</surname><given-names>T</given-names></name><name><surname>Turkkan</surname><given-names>N</given-names></name><name><surname>Eng</surname><given-names>P</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Bayesian analysis of the difference of two proportions</article-title><source>Communications in Statistics - Theory and Methods</source><volume>22</volume><fpage>1755</fpage><lpage>1771</lpage><pub-id pub-id-type="doi">10.1080/03610929308831114</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Precision neuroscience: Dense sampling of individual brains</article-title><source>Neuron</source><volume>95</volume><fpage>727</fpage><lpage>729</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2017.08.002</pub-id><pub-id pub-id-type="pmid">28817793</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rafi</surname><given-names>Z</given-names></name><name><surname>Greenland</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Semantic and Cognitive Tools to Aid Statistical Science: Replace Confidence and Significance by Compatibility and Surprise</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.08579">https://arxiv.org/abs/1909.08579</ext-link></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rogan</surname><given-names>WJ</given-names></name><name><surname>Gladen</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Estimating prevalence from the results of a screening test</article-title><source>American Journal of Epidemiology</source><volume>107</volume><fpage>71</fpage><lpage>76</lpage><pub-id pub-id-type="doi">10.1093/oxfordjournals.aje.a112510</pub-id><pub-id pub-id-type="pmid">623091</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rosenblatt</surname><given-names>JD</given-names></name><name><surname>Vink</surname><given-names>M</given-names></name><name><surname>Benjamini</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Revisiting multi-subject random effects in FMRI: Advocating prevalence estimation</article-title><source>NeuroImage</source><volume>84</volume><fpage>113</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.025</pub-id><pub-id pub-id-type="pmid">23988271</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Rouder</surname><given-names>JN</given-names></name><name><surname>Haaf</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Are There Reliable Qualitative Individual Difference in Cognition</article-title><source>PsyArXiv</source><ext-link ext-link-type="uri" xlink:href="https://psyarxiv.com/3ezmw/">https://psyarxiv.com/3ezmw/</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rousselet</surname><given-names>GA</given-names></name><name><surname>Pernet</surname><given-names>CR</given-names></name><name><surname>Wilcox</surname><given-names>RR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Beyond differences in means: Robust graphical methods to compare two groups in neuroscience</article-title><source>European Journal of Neuroscience</source><volume>46</volume><fpage>1738</fpage><lpage>1748</lpage><pub-id pub-id-type="doi">10.1111/ejn.13610</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schyns</surname><given-names>PG</given-names></name><name><surname>Thut</surname><given-names>G</given-names></name><name><surname>Gross</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Cracking the code of oscillatory activity</article-title><source>PLOS Biology</source><volume>9</volume><elocation-id>e1001064</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.1001064</pub-id><pub-id pub-id-type="pmid">21610856</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>ML</given-names></name><name><surname>Gosselin</surname><given-names>F</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Perceptual moments of conscious visual experience inferred from oscillatory brain activity</article-title><source>PNAS</source><volume>103</volume><fpage>5626</fpage><lpage>5631</lpage><pub-id pub-id-type="doi">10.1073/pnas.0508972103</pub-id><pub-id pub-id-type="pmid">16567643</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>PL</given-names></name><name><surname>Little</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Small is beautiful: In defense of the small-n design</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>25</volume><fpage>2083</fpage><lpage>2101</lpage><pub-id pub-id-type="doi">10.3758/s13423-018-1451-8</pub-id><pub-id pub-id-type="pmid">29557067</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stansbury</surname><given-names>DE</given-names></name><name><surname>Naselaris</surname><given-names>T</given-names></name><name><surname>Gallant</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Natural scene statistics account for the representation of scene categories in human visual cortex</article-title><source>Neuron</source><volume>79</volume><fpage>1025</fpage><lpage>1034</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.06.034</pub-id><pub-id pub-id-type="pmid">23932491</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thiebaut de Schotten</surname><given-names>M</given-names></name><name><surname>Shallice</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Identical, similar or different? Is a single brain model sufficient?</article-title><source>Cortex; a Journal Devoted to the Study of the Nervous System and Behavior</source><volume>86</volume><fpage>172</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/j.cortex.2016.12.002</pub-id><pub-id pub-id-type="pmid">28041614</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Yarkoni</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><source>The Generalizability Crisis</source><publisher-name>Behavioral and Brain Sciences</publisher-name></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhan</surname><given-names>J</given-names></name><name><surname>Ince</surname><given-names>RAA</given-names></name><name><surname>van Rijsbergen</surname><given-names>N</given-names></name><name><surname>Schyns</surname><given-names>PG</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Dynamic construction of reduced representations in the brain for perceptual decision behavior</article-title><source>Current Biology</source><volume>29</volume><fpage>319</fpage><lpage>326</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.11.049</pub-id><pub-id pub-id-type="pmid">30639108</pub-id></element-citation></ref></ref-list></back><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62461.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Serences</surname><given-names>John T</given-names></name><role>Reviewing Editor</role><aff><institution>University of California, San Diego</institution><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Huth</surname><given-names>Alex</given-names></name><role>Reviewer</role><aff><institution>University of Texas</institution><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="reviewer"><name><surname>Ling</surname><given-names>Sam</given-names></name><role>Reviewer</role></contrib></contrib-group></front-stub><body><boxed-text id="box1"><p>In the interests of transparency, eLife publishes the most substantive revision requests and the accompanying author responses.</p></boxed-text><p><bold>Acceptance summary:</bold></p><p>This is paper describes methods to move away from frequentist null hypothesis testing toward a Bayesian approach of estimate the prevalence of within-participant effects. This methodological advance should be widely applicable in many fields, and should promote a more robust evaluation of effects.</p><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Bayesian inference of population prevalence&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Alex Huth (Reviewer #1); Sam Ling (Reviewer #2).</p><p>The reviewers have discussed the reviews with one another and the Reviewing Editor has drafted this decision to help you prepare a revised submission. We will all re-evaluate the manuscript after the re-submission, and I hope the comments below are useful.</p><p>Summary:</p><p>As you'll see below, the reviewers and I were all enthusiastic about the paper. That said, we all felt that a revision is required to provide concrete examples of when this method is preferred over frequentist approaches, as well as the general types of questions that this method is appropriate for addressing.</p><p>In addition, we all felt that the focus of the paper should be expanded to give more attention to non-fMRI studies. For example, there is passing mention in the Discussion section about how this might apply to single unit studies, but that seems more of an afterthought at the moment. Thus, the paper is missing a chance to speak to, and influence, a much broader audience.</p><p>Essential revisions:</p><p>In addition to revising the text as indicated above (and in the more detailed reviews), it seems likely that some additional simulations may need to be run to address all of the points. This is particularly true of the suggestion to demonstrate a situation where prevalence estimation is more powerful than standard methods.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>In this paper, Ince and colleagues describe a novel set of Bayesian methods for estimating and working with &quot;population prevalence&quot; of an effect, which they pose as an alternative to NHST on the mean effect in the population. In turns this paper was tantalizing, exciting, and frustrating. I very much want the method the authors propose to be useful, but I was left without a clear sense of (1) what specific questions (&quot;qualitative hypotheses&quot;) this method is appropriate for answering, and (2) why (quantitatively) this method should be preferred over others. Specific comments and suggestions are enumerated below.</p><p>1. Please show us how prevalence estimation can be more useful than classical approaches! The authors convincingly argued that estimating prevalence amounts to replicating an experiment in each individual subject, a shift in perspective that may help alleviate issues of non-replicability in psychology and neuroimaging (I am very sympathetic to these arguments!). However, it was somewhat frustrating that none of the computational experiments seemed to directly contrast prevalence estimation vs. population mean estimation. If prevalence is indeed a superior way to conceptualize effects in a population, then there should be some experiment where prevalence and population mean give different results for testing the same qualitative hypothesis. For example, in the discussion the authors bring up the issue of uninteresting heterogeneity across subjects, which can reduce the effectiveness of standard approaches (i.e. where the precise anatomical location or timing of an effect is variable across subjects). This paper would be much more convincing if it could experimentally demonstrate a situation like this (in simulation, of course), where prevalence estimation might truly be more powerful than standard methods.</p><p>2. Tell us more about what inferences can actually be made using prevalence estimation! The experiment in Figure 1 lays out the differences between prevalence and population mean estimation quite clearly, but the conclusions are much less clear. The authors &quot;suggest that often, if the goal of the analysis is to infer the presence of a causal relationship within individuals in the population, the within-participant perspective may be more appropriate&quot; – but more appropriate for what? Exploring-specifically at this point in the paper-which types of qualitative hypotheses (or some specific examples?) can be meaningfully addressed using prevalence analyses would, in my opinion, greatly strengthen the authors' case.</p><p>3. The argument that prevalence estimation is more sensitive to trials per participant than number of participants (Figure 2E) seems circular. The authors define prevalence as the fraction of subjects with &quot;true positive (significant) test results&quot; rather than the fraction where the null hypothesis is actually false, because they can only find a lower bound on the latter measure. Given this definition, the finding that &quot;prevalence greater sensitivity to the number of trials per participant, and less sensitivity to the number of participants&quot; (which is seemingly framed as the main conclusion of the analyses in Figure 2?) is highly unsurprising.</p><p>4. How does estimating group differences in prevalence compare to estimating differences in group means? The experiment shown in Figure 3 is tantalizing, but what I really wanted to know here is whether (and under what conditions) comparing groups using prevalence might be more powerful than comparing groups using traditional models.</p><p>5. The analysis of prevalence as a function of effect size is missing context and conclusions. Ultimately, I was left without any sense of the utility of estimating prevalence for different effect sizes after reading this section (and Figure 4). What is this specific procedure useful for? What kinds of hypotheses can it test? How does it compare to other methods? Answering these questions would undoubtedly strengthen the paper.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>This manuscript advocates for a shift in the statistical (and methodological) approaches taken in neuroscience and psychology, wherein effects are considered from the perspective of prevalence, rather than population-centric NHST. They provide simple yet compelling simulations to make their case, and introduce the bayesian prevalence method, along with code to implement on one's own. This manuscript is incredibly well written, the simulations are tightly constructed, and will, I hope, become widely read by the neuroscience and psychology community. It's a straightforward treatment of a statistic that the field needs to consider shifting towards, and the ability to generalize to any NHST will no doubt make this user friendly for most all researchers. I will readily admit that this was a method that my own lab has been in search of for a while, and we plan to deploy this in our own work. Below, I outline my questions/comments, but they are largely meant to support clarification to a broader audience.</p><p>1. While the authors promote a Bayesian treatment of the estimate of prevalence, they readily admit that a frequentist approach is suitable as well. It would be worth detailing a bit more the advantages and disadvantages of the two, if any. Particularly because the frequentist approach to neuroimaging has been out in the wild for a while now.</p><p>2. The authors simulate individuals within the population as having equal within-subject (or unit) variability. To what degree are the assumptions for prevalence estimation upheld in the case where individual subjects have unequal variance?</p><p>3. I assume this rests on the assumption of independence between units of replication (ie subjects, voxels, ROIs, cells). Is this true? And how robust is this method to a lack of independence? I ask because if one were carrying this method out on a voxel-by-voxel basis, within a participant, there's quite a bit of non-independence between these voxels. And likely same goes for between adjacent ROIs, or even between time points. It would be helpful if the authors dedicated some text to this assumption.</p><p>4. This comment is more to aid in promotion of the work, but I think the introduction and walkthrough of the simulation would be better serviced by using a case example throughout. It could be a psychophysics example, clinical psych example, or neuroscience example, doesn't matter. I think it would simply help the reader better concretize the reasoning this way. This is entirely up to the authors, but I imagined that the broader audience that they (and I) want to reach would be more likely to engage if this was more explicitly related to their work.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for submitting your article &quot;Bayesian inference of population prevalence&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 2 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Timothy Behrens as the Senior Editor. The following individual involved in review of your submission has agreed to reveal their identity: Sam Ling (Reviewer #2).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential Revisions:</p><p>This is a paper that describes methods to move away from frequentist null hypothesis testing toward a Bayesian approach of estimate the prevalence of within-participant effects. This methodological advance should be widely applicable in many fields, and should promote a more robust evaluation of effects.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>In their revised manuscript, Ince and colleagues have added a number of illustrative simulations and clarified several important points, substantially improving the final product. In particular, the simulated EEG analysis (new Figure 2) is an extremely clear and powerful demonstration of the advantages of prevalence estimation over population mean inference for a realistic problem. This is exactly the type of comparison that seemed to be missing in the initial submission, and its inclusion strengthens the paper substantially. The new simulations in Figures 4 and 6 also clarify the related points. Overall, I think the current manuscript is excellent, interesting, and timely.</p><p>Here I will respond to two of the authors' points specifically.</p><p>Authors: We avoid direct quantitative comparisons because the hypotheses tested (non-zero population mean, vs. posterior estimate of prevalence of within-participant effects) and statistical methods (frequentist vs. Bayesian) are different, although we do invite comparison of the different frequentist p-values (for null of mean zero vs global null of no participants with an effect) for the examples in Figure 1. We focused instead on qualitative arguments in favour of the prevalence view (better match to qualitative hypothesis of interest in psychology and neuroimaging, which we believe should better consider effects within individual behaving brains, and greater robustness given that the effect is shown independently in multiple participants).</p><p>This is spot on. The hypotheses tested by these methods are different, but the key question is how these (quantitative) hypotheses map to the implicit, qualitative hypotheses that are actually the subject of inquiry. On the discussion of this point, the current manuscript does not differ substantially from the original. Yet I still found myself much more convinced of its correctness this time around than the first. To this I credit the new simulations that the authors added (Figure 2 and 4). By directly comparing the results of the two methods, these simulations highlight how much better the hypotheses tested by prevalence estimation align with intuitive/qualitative hypotheses than do t-tests.</p><p>Minor aside: oddly, I think Figure 1 actually does the authors' arguments something of a disservice on this central point. Yes, it clearly illustrates how the two methods differ. But it presumes a world in which all the assumptions of the hierarchical population mean test are met. This leads to a situation where the standard method matches the reader's expectations (/qualitative hypotheses) much more closely than does the prevalence method. The authors acknowledge the weirdness of the two-sided effects seen in Figure 1B, and point out that this weirdness is actually implicit in the standard hierarchical model, but I worry that by this point the damage is already done. I'm not sure how best to do this – Figure 1 is a very clear illustration of the methods – but it could be worthwhile to consider reorganization that de-prioritizes this simple-but-weird example in favor of less weird examples like Figure 2.</p><p>Authors (regarding prevalence as a function of effect size): The main point of this section is to answer the potential criticism that we are dichotomising within participant results and to show that this is not a requirement for valid quantification of population prevalence. Prevalence as a function of effect size gives a fuller picture of the distribution of effect sizes observed. One could argue this is also given by a standard density plot, for example in a violin or raincloud plot, but these are descriptive properties of the specific data sample, without any formal inferential link to the population.</p><p>The new simulations shown in Figure 6 certainly clarify this point by showing examples with different underlying population statistics. I'm not specifically asking for anything more – I think this is sufficiently convincing as-is – but I think this would be stronger still with a more concrete example that shows how the prevalence/effect size curve could appear in different real-world situations, with implications for the reader's qualitative hypotheses. For an example like the question posed by Reviewer 2, this curve would look quite different if the individual subject standard deviation was not fixed, but was instead drawn from some distribution. Again, this is not critical, but a more real-world-like example would make this section more convincing.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The authors have done an admirable job addressing my questions. I'm eager for this work, which I hope to be influential in the field, to be out in the wild for others to adopt!</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.62461.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Summary:</p><p>As you'll see below, the reviewers and I were all enthusiastic about the paper. That said, we all felt that a revision is required to provide concrete examples of when this method is preferred over frequentist approaches, as well as the general types of questions that this method is appropriate for addressing.</p></disp-quote><p>We thank the reviewers for their useful comments and apologise for the delay in producing our revisions. We have added additional simulations to compare directly with Null Hypothesis Statistical Testing (NHST) on the population mean and reordered the results presentation to emphasise the broad applicability of our Bayesian prevalence method.</p><disp-quote content-type="editor-comment"><p>In addition, we all felt that the focus of the paper should be expanded to give more attention to non-fMRI studies. For example, there is passing mention in the Discussion section about how this might apply to single unit studies, but that seems more of an afterthought at the moment. Thus, the paper is missing a chance to speak to, and influence, a much broader audience.</p></disp-quote><p>It was not our intention to focus on fMRI, although it’s possible that the citations reflected this area of research more than others. In fact, our approach has been developed primarily from behavioural and M/EEG studies. We revised the manuscript to clarify that the new approach is completely general with a simple abstract example to propose the concept. Also, we have now added simulations of typical event-related EEG experiments to motivate how Bayesian prevalence can be applied and how its results can differ from a second level t-test. The extension to single unit studies is a discussion point because we prefer to focus the examples to develop the critical point that human participants are the replication unit of interest in a broad range of psychology and neuroimaging studies. We have extended this discussion point with a more concrete example of how prevalence could be applied with neurons as the replication unit of interest.</p><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>In addition to revising the text as indicated above (and in the more detailed reviews), it seems likely that some additional simulations may need to be run to address all of the points. This is particularly true of the suggestion to demonstrate a situation where prevalence estimation is more powerful than standard methods.</p></disp-quote><p>We want to emphasise that we don’t necessarily claim that Bayesian prevalence is more powerful than standard methods. In fact, the frequentist notion of power doesn’t directly apply to Bayesian estimation. Rather, we want to explain how Bayesian prevalence quantifies a different property of the sampled population, and thereby supports a conceptual shift of perspective, where the replication unit is the individual participant, and the proportion of participants showing a significant effect becomes the focus of statistical inference (in contrast to Null Hypothesis Significance Testing of the mean across participants). There are situations where this different perspective shows effects or differences that the population mean is not sensitive to, and we have added such example simulations as requested by the reviewers (new Figures 2, 4, 6).</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>In this paper, Ince and colleagues describe a novel set of Bayesian methods for estimating and working with &quot;population prevalence&quot; of an effect, which they pose as an alternative to NHST on the mean effect in the population. In turns this paper was tantalizing, exciting, and frustrating. I very much want the method the authors propose to be useful, but I was left without a clear sense of (1) what specific questions (&quot;qualitative hypotheses&quot;) this method is appropriate for answering, and (2) why (quantitatively) this method should be preferred over others. Specific comments and suggestions are enumerated below.</p><p>1. Please show us how prevalence estimation can be more useful than classical approaches! The authors convincingly argued that estimating prevalence amounts to replicating an experiment in each individual subject, a shift in perspective that may help alleviate issues of non-replicability in psychology and neuroimaging (I am very sympathetic to these arguments!). However, it was somewhat frustrating that none of the computational experiments seemed to directly contrast prevalence estimation vs. population mean estimation. If prevalence is indeed a superior way to conceptualize effects in a population, then there should be some experiment where prevalence and population mean give different results for testing the same qualitative hypothesis. For example, in the discussion the authors bring up the issue of uninteresting heterogeneity across subjects, which can reduce the effectiveness of standard approaches (i.e. where the precise anatomical location or timing of an effect is variable across subjects). This paper would be much more convincing if it could experimentally demonstrate a situation like this (in simulation, of course), where prevalence estimation might truly be more powerful than standard methods.</p></disp-quote><p>In the original submission, we tried to strike a balance between a clear conceptual exposition and lists of practical examples, because the breadth of possible qualitative hypotheses to which Bayesian prevalence can be applied is so broad. In fact, pretty much any experimental question or test that can be considered at the per participant level can be framed as a population prevalence question, as an alternative to the typical population mean inference. So, in the revised manuscript, we keep the examples generic in the hope that readers could export the conceptual shift embedded within the prevalence concept to any questions that might arise from their own work.</p><p>We avoid direct quantitative comparisons because the hypotheses tested (non-zero population mean, vs. posterior estimate of prevalence of within-participant effects) and statistical methods (frequentist vs. Bayesian) are different, although we do invite comparison of the different frequentist p-values (for null of mean zero vs global null of no participants with an effect) for the examples in Figure 1. We focused instead on qualitative arguments in favour of the prevalence view (better match to qualitative hypothesis of interest in psychology and neuroimaging, which we believe should better consider effects within individual behaving brains, and greater robustness given that the effect is shown independently in multiple participants).</p><p>We would argue the simple generic simulation of the standard hierarchical population model shown in Figure 1 does demonstrate a situation where prevalence and population mean give different results for testing the same qualitative hypothesis (presence of a non-zero experimental effect in the population). This simulation directly contrasts inference of non-zero population mean (color of population curves, reported p-values on the left), with the prevalence approach (p-values for rejection of global null on the right, posterior prevalence curves in panel E). For example, the experiment simulated in Figure 1B shows very different results from the two approaches to testing the qualitative hypothesis “do participants in the population show a non-zero effect?”. The population mean provides no evidence for this (p=0.52), whereas the prevalence perspective provides very strong evidence (global null, p=2.2x10<sup>-16</sup>; purple posterior distribution in Panel 1E).</p><p>To give further intuition into situations where the NHST and Bayesian Prevalence can differ, we have added two new simulations of more concrete and realistic situations with similar divergence between the two approaches (Figure 2). These additional simulations don’t depend on symmetrical signed effects as the examples of Figure 1 (although we would like to emphasise that the presence of symmetrical signed effects is directly implied by the null hypothesis employed whenever a second-level t-test is used). The new Figure 2 shows simulated EEG data for two situations where prevalence can quantify robust population effects that are not visible from the population mean perspective. Panel A illustrates the alignment problem, robust effects are obtained in each participant, but at different post-stimulus times. Panel B illustrates the homogeneity problem, here the participants are heterogeneous, with 50% showing a strong effect and 50% showing no effect. In both cases the population mean NHST fails to show an effect when correcting for multiple comparisons over time. In contrast, the prevalence method does show strong evidence for effects at the population level (with the same multiple comparison correction applied within-participants).</p><p>As an aside, it was not our intention to suggest between participant heterogeneity is uninteresting; only that it can present a problem for traditional population-mean based approaches. In fact, we think exploring individual differences in anatomy and function is an important area of research. Our point is that Bayesian prevalence can provide an inferential link to the population which is currently missing in such cases.</p><disp-quote content-type="editor-comment"><p>2. Tell us more about what inferences can actually be made using prevalence estimation! The experiment in Figure 1 lays out the differences between prevalence and population mean estimation quite clearly, but the conclusions are much less clear. The authors &quot;suggest that often, if the goal of the analysis is to infer the presence of a causal relationship within individuals in the population, the within-participant perspective may be more appropriate&quot; – but more appropriate for what? Exploring-specifically at this point in the paper-which types of qualitative hypotheses (or some specific examples?) can be meaningfully addressed using prevalence analyses would, in my opinion, greatly strengthen the authors' case.</p></disp-quote><p>We aimed to present the prevalence approach as a completely general second level approach to drawing population level inferences from data obtained from multiple participants. In this sense, it is completely general and can test any qualitative hypothesis that can be quantified within individual participants (either via an effect size, or a within-participant NHST). Our argument is that, in many cases, the within-participant perspective may be more appropriate for quantifying the presence of a causal relationship (which could be any relationship tested with a randomised experiment and quantified with any test statistic or model). It can be applied to everything from behavioural modelling (including uncertainty, risk, expectation etc.), reaction time measures, multivariate decoding, encoding models for complex naturalistic stimuli, simple neuroimaging contrasts (face vs house), spectral analyses (phase or power response, phase-amplitude coupling etc.).</p><p>We have now moved the paragraph covering the breadth of possible applications to the start of this section to ensure it is in the front of the readers mind when considering the results of the simulations in Figure 1.</p><disp-quote content-type="editor-comment"><p>3. The argument that prevalence estimation is more sensitive to trials per participant than number of participants (Figure 2E) seems circular. The authors define prevalence as the fraction of subjects with &quot;true positive (significant) test results&quot; rather than the fraction where the null hypothesis is actually false, because they can only find a lower bound on the latter measure. Given this definition, the finding that &quot;prevalence greater sensitivity to the number of trials per participant, and less sensitivity to the number of participants&quot; (which is seemingly framed as the main conclusion of the analyses in Figure 2?) is highly unsurprising.</p></disp-quote><p>We agree this result is unsurprising when due consideration is given to the different perspective of prevalence. However, we still like to show it explicitly as most discussion of statistical power and power analysis in the psychology, neuroscience and neuroimaging literature considers only number of participants. So, we think it is important to emphasise this point – that the different perspectives have different properties, and that these give very different views in terms of experimental design. We also think these simulations can be used directly, or the scripts easily repurposed to provide simulation based “power analysis”. While not formal frequentist power analysis, these results could nevertheless provide some motivation for justifying number of participants in a particular study design. We have moved this to the end of the Results section to de-emphasise.</p><disp-quote content-type="editor-comment"><p>4. How does estimating group differences in prevalence compare to estimating differences in group means? The experiment shown in Figure 3 is tantalizing, but what I really wanted to know here is whether (and under what conditions) comparing groups using prevalence might be more powerful than comparing groups using traditional models.</p></disp-quote><p>They are quantifying fundamentally different properties of the populations. In Figure 4, we have added an additional simulation to illustrate the differences between t-test for difference in means and prevalence difference between two groups. We show two groups whose means do not significantly differ, but for which there is evidence for a clear difference in the proportion (of the population, not just the samples) that shows a statistically significant within-participant effect. If the population mean is the primary interest, then the t-test should be used. If the proportion of participants who show the effect is of interest, then Bayesian prevalence provides a valid methodology for making statistical claims at the population level. Crucially, these are valid quantitative statistical statements about the population, and not just a case study of the specific collected samples, which is the criticism traditionally levelled at within-participant statistics, e.g. “It should be acknowledged that this approach only allows for statements that pertain to the existence and magnitude of effects in those subjects, rather than in the populations those subjects are drawn from.” https://doi.org/10.1523/JNEUROSCI.0866-20.2020</p><disp-quote content-type="editor-comment"><p>5. The analysis of prevalence as a function of effect size is missing context and conclusions. Ultimately, I was left without any sense of the utility of estimating prevalence for different effect sizes after reading this section (and Figure 4). What is this specific procedure useful for? What kinds of hypotheses can it test? How does it compare to other methods? Answering these questions would undoubtedly strengthen the paper.</p></disp-quote><p>The main point of this section is to answer the potential criticism that we are dichotomising within participant results and to show that this is not a requirement for valid quantification of population prevalence. Prevalence as a function of effect size gives a fuller picture of the distribution of effect sizes observed. One could argue this is also given by a standard density plot, for example in a violin or raincloud plot, but these are descriptive properties of the specific data sample, without any formal inferential link to the population.</p><p>The prevalence as a function of effect size provides a model-free valid inferential link to the population and can be interpreted as Bayesian evidence for prevalence at the population level, rather than as a property only of the observed sample. Figure 6 shows six new simulations to illustrate situations where prevalence based on within-participant NHST at <italic>p=0.05</italic> is equivalent (panels A,B,C and D,E,F respectively), but the prevalence effect size plot gives further information, at the population level, that clearly distinguishes the simulated populations.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>This manuscript advocates for a shift in the statistical (and methodological) approaches taken in neuroscience and psychology, wherein effects are considered from the perspective of prevalence, rather than population-centric NHST. They provide simple yet compelling simulations to make their case, and introduce the bayesian prevalence method, along with code to implement on one's own. This manuscript is incredibly well written, the simulations are tightly constructed, and will, I hope, become widely read by the neuroscience and psychology community. It's a straightforward treatment of a statistic that the field needs to consider shifting towards, and the ability to generalize to any NHST will no doubt make this user friendly for most all researchers. I will readily admit that this was a method that my own lab has been in search of for a while, and we plan to deploy this in our own work. Below, I outline my questions/comments, but they are largely meant to support clarification to a broader audience.</p></disp-quote><p>We thank the reviewer for their comments.</p><disp-quote content-type="editor-comment"><p>1. While the authors promote a Bayesian treatment of the estimate of prevalence, they readily admit that a frequentist approach is suitable as well. It would be worth detailing a bit more the advantages and disadvantages of the two, if any. Particularly because the frequentist approach to neuroimaging has been out in the wild for a while now.</p></disp-quote><p>We have added some comments on this to the Discussion section.</p><disp-quote content-type="editor-comment"><p>2. The authors simulate individuals within the population as having equal within-subject (or unit) variability. To what degree are the assumptions for prevalence estimation upheld in the case where individual subjects have unequal variance?</p></disp-quote><p>The only assumptions for the Bayesian prevalence are that the participants are independent, and the false positive rate is properly controlled to the nominal α level within each participant. So long as the within-participant test is robust there is no problem with differences in variance between participants. We show here <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref>, a version of Figure 1 generated with between-subject standard deviation randomly sampled from a log-normal distribution with mean log(10) and standard deviation log(3). Prevalence decreases w.r.t. Figure 1 in the paper because some participants have much larger variance and so there are fewer within-participant significant results, but the prevalence estimation is still valid and the interpretations remain the same. We have added text making this point explicit in section “How to apply Bayesian prevalence in practise”.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>Same as Figure 1 from manuscript, but within-participant standard deviation is sampled from a log-normal distribution with mean log(10) and standard deviation log(3).</title></caption><graphic mime-subtype="tiff" mimetype="image" xlink:href="elife-62461-sa2-fig1-v1.tif"/></fig><disp-quote content-type="editor-comment"><p>3. I assume this rests on the assumption of independence between units of replication (ie subjects, voxels, ROIs, cells). Is this true? And how robust is this method to a lack of independence? I ask because if one were carrying this method out on a voxel-by-voxel basis, within a participant, there's quite a bit of non-independence between these voxels. And likely same goes for between adjacent ROIs, or even between time points. It would be helpful if the authors dedicated some text to this assumption.</p></disp-quote><p>The method does assume independence between “replication units” (most commonly thinking of participants) and so doesn’t address any hierarchical structure (e.g. time of day, time of year, testing lab, characteristics of university subject pool vs full population etc.). We would argue this is common to many statistical methods and needs to be evaluated by the particular scientific field as a “non-statistical” consideration (Eddington and Onghena, “Randomization Tests”, CRC Press). For example, the second-level t-test used for NHST of the population mean also assumes the participants are independent samples of the population.</p><p>We agree that treating voxels or ROIs as the replication unit of interest would make this problem particularly pronounced, and we haven’t suggested that as an intended application. We propose to do population prevalence across participants (ie. the proportion of the population that would be expected to show an effect in a particular voxel or region), which is different to treating voxels as the replication unit of interest and attempting to estimate the prevalence of an effect in a population of voxels (within a subject, the entire population of voxels for that individual is available). To simplify, throughout the paper we have participants as the replication unit of interest over which prevalence is calculated. The multiplicity which results from multiple time points, sensors or voxels is dealt with using existing multiple comparison correction techniques, but applied at the individual participant, rather than the population level. We hope the new simulations in Figure 2 make this more explicit (prevalence is calculated over participants, either for a time window, or for each time point, but prevalence is not carried out over time points).</p><p>Another possible replication unit which we discuss is to consider well isolated single unit neuron recordings. We have expanded this point in the discussion with a clearer example application. In this case, we propose combining neurons from multiple animals to determine the proportion of neurons in a particular region (where the population is all neurons across all animals). Again, the issue of whether it is valid to ignore the hierarchical structure is a problem-specific non-statistical consideration that the researcher must address. Perhaps it would be more reasonable to combine cells recorded in multiple sessions from the same animal (separate electrode insertions in the same brain region) to make an inference about the prevalence of an effect within the population of cells in that region in that individual if there is reason to suspect large variation between individual animals.</p><disp-quote content-type="editor-comment"><p>4. This comment is more to aid in promotion of the work, but I think the introduction and walkthrough of the simulation would be better serviced by using a case example throughout. It could be a psychophysics example, clinical psych example, or neuroscience example, doesn't matter. I think it would simply help the reader better concretize the reasoning this way. This is entirely up to the authors, but I imagined that the broader audience that they (and I) want to reach would be more likely to engage if this was more explicitly related to their work.</p></disp-quote><p>We have kept the simulations the same, but we have moved up to the start of this section the paragraph discussing the breadth and generality of possible applications, inviting readers to keep in mind the most relevant application to their work when considering the results of the simulations in Figure 1.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>In their revised manuscript, Ince and colleagues have added a number of illustrative simulations and clarified several important points, substantially improving the final product. In particular, the simulated EEG analysis (new Figure 2) is an extremely clear and powerful demonstration of the advantages of prevalence estimation over population mean inference for a realistic problem. This is exactly the type of comparison that seemed to be missing in the initial submission, and its inclusion strengthens the paper substantially. The new simulations in Figures 4 and 6 also clarify the related points. Overall, I think the current manuscript is excellent, interesting, and timely.</p></disp-quote><p>We thank the reviewer for their comments. The paper evolved originally from a short perspective piece. We were aiming to keep the presentation general without locking in to one particular modality (like EEG) that not all readers might be familiar with. But we agree adding Figure 2 improves the presentation a lot and thank the reviewer for the suggestion.</p><disp-quote content-type="editor-comment"><p>This is spot on. The hypotheses tested by these methods are different, but the key question is how these (quantitative) hypotheses map to the implicit, qualitative hypotheses that are actually the subject of inquiry. On the discussion of this point, the current manuscript does not differ substantially from the original. Yet I still found myself much more convinced of its correctness this time around than the first. To this I credit the new simulations that the authors added (Figure 2 and 4). By directly comparing the results of the two methods, these simulations highlight how much better the hypotheses tested by prevalence estimation align with intuitive/qualitative hypotheses than do t-tests.</p></disp-quote><p>We thank the reviewer for such a frank opinion, which we of course share. However, we don’t want to fall into the trap of statisticians telling users what they want to know http://daniellakens.blogspot.com/2017/11/the-statisticians-fallacy.html We try to make the argument that the population mean is not the only population property we can formally quantify. In many cases prevalence might be of interest and particularly so in cognitive domains, where we would argue that prevalence might often better match the qualitative scientific question of interest. We agree that the new Figures make this point much more clearly and hope our new Figure 6 will similarly add to the weight of the argument. We thank the reviewer for their input that helped us understand how to better communicate the shift of perspective from means to prevalence.</p><disp-quote content-type="editor-comment"><p>Minor aside: oddly, I think Figure 1 actually does the authors' arguments something of a disservice on this central point. Yes, it clearly illustrates how the two methods differ. But it presumes a world in which all the assumptions of the hierarchical population mean test are met. This leads to a situation where the standard method matches the reader's expectations (/qualitative hypotheses) much more closely than does the prevalence method. The authors acknowledge the weirdness of the two-sided effects seen in Figure 1B, and point out that this weirdness is actually implicit in the standard hierarchical model, but I worry that by this point the damage is already done. I'm not sure how best to do this – Figure 1 is a very clear illustration of the methods – but it could be worthwhile to consider reorganization that de-prioritizes this simple-but-weird example in favor of less weird examples like Figure 2.</p></disp-quote><p>We understand what the Reviewer is suggesting, weighted both options, even tried to change the presentation as suggested. However, after careful thinking the logic of presentation of current Figure 2 still requires that the prevalence method is clearly defined. The best way to do this is to introduce it step by step (from the idea of within-participant significance providing evidence against a population effect given by the global null). So, in the end, we decided to leave current Figures 1 and 2 figures in this order, for the purpose of logical clarity of presentation. We have added a sentence to signpost Figure 2 in the last paragraph of the <italic>Population vs within-participant statistical tests</italic> section.</p><disp-quote content-type="editor-comment"><p>Authors (regarding prevalence as a function of effect size): The main point of this section is to answer the potential criticism that we are dichotomising within participant results and to show that this is not a requirement for valid quantification of population prevalence. Prevalence as a function of effect size gives a fuller picture of the distribution of effect sizes observed. One could argue this is also given by a standard density plot, for example in a violin or raincloud plot, but these are descriptive properties of the specific data sample, without any formal inferential link to the population.</p><p>The new simulations shown in Figure 6 certainly clarify this point by showing examples with different underlying population statistics. I'm not specifically asking for anything more – I think this is sufficiently convincing as-is – but I think this would be stronger still with a more concrete example that shows how the prevalence/effect size curve could appear in different real-world situations, with implications for the reader's qualitative hypotheses. For an example like the question posed by Reviewer 2, this curve would look quite different if the individual subject standard deviation was not fixed, but was instead drawn from some distribution. Again, this is not critical, but a more real-world-like example would make this section more convincing.</p></disp-quote><p>We have produced a new Figure 6, which uses the same EEG simulation setting as Figure 2 to motivate three different situations where the overall prevalence of <italic>p=0.05</italic> is similar, but the effect size prevalence curve reveals interpretable differences between the simulated data sets. We hope this makes a more intuitive and clearer link between data and effect size prevalence results. As these three new examples cover the main features of the old Figure 6 (different variances and subgroups) we replaced the old figure with this new one.</p></body></sub-article></article>