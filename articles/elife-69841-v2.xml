<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">69841</article-id><article-id pub-id-type="doi">10.7554/eLife.69841</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Learning accurate path integration in ring attractor models of the head direction system</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-238345"><name><surname>Vafidis</surname><given-names>Pantelis</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9768-0609</contrib-id><email>pvafeidi@caltech.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-96988"><name><surname>Owald</surname><given-names>David</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-7747-7884</contrib-id><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund2"/><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" equal-contrib="yes" id="author-161917"><name><surname>D'Albis</surname><given-names>Tiziano</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1585-1433</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes" id="author-42870"><name><surname>Kempter</surname><given-names>Richard</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-5344-2983</contrib-id><email>r.kempter@biologie.hu-berlin.de</email><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="equal-contrib1">†</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05dxps055</institution-id><institution>Computation and Neural Systems, California Institute of Technology</institution></institution-wrap><addr-line><named-content content-type="city">Pasadena</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05ewdps05</institution-id><institution>Bernstein Center for Computational Neuroscience</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01hcx6992</institution-id><institution>Institute for Theoretical Biology, Department of Biology, Humboldt-Universität zu Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/001w7jn25</institution-id><institution>Institute of Neurophysiology, Charité – Universitätsmedizin Berlin, corporate member of Freie Universität Berlin and Humboldt-Universität zu Berlin, and Berlin Institute of Health</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/001w7jn25</institution-id><institution>NeuroCure, Charité - Universitätsmedizin Berlin</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05s5xvk70</institution-id><institution>Einstein Center for Neurosciences</institution></institution-wrap><addr-line><named-content content-type="city">Berlin</named-content></addr-line><country>Germany</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>Ecole Normale Superieure Paris</institution></institution-wrap><country>France</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Calabrese</surname><given-names>Ronald L</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03czfpz43</institution-id><institution>Emory University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><author-notes><fn fn-type="con" id="equal-contrib1"><label>†</label><p>These authors contributed equally to this work</p></fn></author-notes><pub-date publication-format="electronic" date-type="publication"><day>20</day><month>06</month><year>2022</year></pub-date><pub-date pub-type="collection"><year>2022</year></pub-date><volume>11</volume><elocation-id>e69841</elocation-id><history><date date-type="received" iso-8601-date="2021-04-28"><day>28</day><month>04</month><year>2021</year></date><date date-type="accepted" iso-8601-date="2022-06-17"><day>17</day><month>06</month><year>2022</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2021-03-12"><day>12</day><month>03</month><year>2021</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2021.03.12.435035"/></event></pub-history><permissions><copyright-statement>© 2022, Vafidis et al</copyright-statement><copyright-year>2022</copyright-year><copyright-holder>Vafidis et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-69841-v2.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-69841-figures-v2.pdf"/><abstract><p>Ring attractor models for angular path integration have received strong experimental support. To function as integrators, head direction circuits require precisely tuned connectivity, but it is currently unknown how such tuning could be achieved. Here, we propose a network model in which a local, biologically plausible learning rule adjusts synaptic efficacies during development, guided by supervisory allothetic cues. Applied to the <italic>Drosophila</italic> head direction system, the model learns to path-integrate accurately and develops a connectivity strikingly similar to the one reported in experiments. The mature network is a quasi-continuous attractor and reproduces key experiments in which optogenetic stimulation controls the internal representation of heading in flies, and where the network remaps to integrate with different gains in rodents. Our model predicts that path integration requires self-supervised learning during a developmental phase, and proposes a general framework to learn to path-integrate with gain-1 even in architectures that lack the physical topography of a ring.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>recurrent neural networks</kwd><kwd>synaptic plasticity</kwd><kwd>predictive coding</kwd><kwd>compartmentalized neuron</kwd><kwd>self-supervised learning</kwd><kwd>path integration</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>None</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>German Research Foundation</institution></institution-wrap></funding-source><award-id>SFB 1315 - project-ID 327654276</award-id><principal-award-recipient><name><surname>Owald</surname><given-names>David</given-names></name><name><surname>Kempter</surname><given-names>Richard</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution>Emmy Noether Programme</institution></institution-wrap></funding-source><award-id>282979116</award-id><principal-award-recipient><name><surname>Owald</surname><given-names>David</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002347</institution-id><institution>Federal Ministry of Education and Research</institution></institution-wrap></funding-source><award-id>01GQ1705</award-id><principal-award-recipient><name><surname>Kempter</surname><given-names>Richard</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100005302</institution-id><institution>Onassis Foundation</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Vafidis</surname><given-names>Pantelis</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002839</institution-id><institution>Charité – Universitätsmedizin Berlin</institution></institution-wrap></funding-source><principal-award-recipient><name><surname>Owald</surname><given-names>David</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>A theoretical model combines self-supervised predictive learning with structural inductive biases to reveal how quasi-continuous attractors that perform accurate angular path integration can be learned from experience during development in the <italic>Drosophila</italic> and potentially other animal models.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Spatial navigation is crucial for the survival of animals in the wild and has been studied in many model organisms (<xref ref-type="bibr" rid="bib65">Tolman, 1948</xref>; <xref ref-type="bibr" rid="bib49">O’Keefe and Nadel, 1978</xref>; <xref ref-type="bibr" rid="bib22">Gallistel, 1993</xref>; <xref ref-type="bibr" rid="bib18">Eichenbaum, 2017</xref>). To orient themselves in an environment, animals rely on external sensory cues (e.g. visual, tactile, or auditory), but such allothetic cues are often ambiguous or absent. In these cases, animals have been found to update internal representations of their current location based on idiothetic cues, a process that is termed path integration (PI, <xref ref-type="bibr" rid="bib15">Darwin, 1873</xref>; <xref ref-type="bibr" rid="bib44">Mittelstaedt and Mittelstaedt, 1980</xref>; <xref ref-type="bibr" rid="bib43">McNaughton et al., 1996</xref>; <xref ref-type="bibr" rid="bib19">Etienne et al., 1996</xref>; <xref ref-type="bibr" rid="bib47">Neuser et al., 2008</xref>; <xref ref-type="bibr" rid="bib8">Burak and Fiete, 2009</xref>). The head direction (HD) system partakes in PI by performing one of the computations required: estimating the current HD by integrating angular velocities; namely angular integration. Furthermore, head direction cells in rodents and flies provide an internal representation of orientation that can persist in darkness (<xref ref-type="bibr" rid="bib55">Ranck, 1984</xref>; <xref ref-type="bibr" rid="bib45">Mizumori and Williams, 1993</xref>; <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>).</p><p>In rodents, the internal representation of heading takes the form of a localized &quot;bump&quot; of activity in the high-dimensional neural manifold of HD cells (<xref ref-type="bibr" rid="bib11">Chaudhuri et al., 2019</xref>). It has been proposed that such a localized activity bump could be sustained by a ring attractor network with local excitatory connections (<xref ref-type="bibr" rid="bib61">Skaggs et al., 1995</xref>; <xref ref-type="bibr" rid="bib56">Redish et al., 1996</xref>; <xref ref-type="bibr" rid="bib32">Hahnloser, 2003</xref>; <xref ref-type="bibr" rid="bib57">Samsonovich and McNaughton, 1997</xref>; <xref ref-type="bibr" rid="bib62">Song and Wang, 2005</xref>; <xref ref-type="bibr" rid="bib64">Stringer et al., 2002</xref>; <xref ref-type="bibr" rid="bib78">Xie et al., 2002</xref>), resembling reverberation mechanisms proposed for working memory (<xref ref-type="bibr" rid="bib74">Wang, 2001</xref>). Ring attractor networks used to model HD cells fall in the theoretical framework of continuous attractor networks (<xref ref-type="bibr" rid="bib2">Amari, 1977</xref>; <xref ref-type="bibr" rid="bib5">Ben-Yishai et al., 1995</xref>; <xref ref-type="bibr" rid="bib59">Seung, 1996</xref>). In this setting, HD cells can update the heading representation in darkness by smoothly moving the bump around the ring obeying idiothetic angular-velocity cues.</p><p>Interestingly, a physical ring-like attractor network of HD cells was observed in the <italic>Drosophila</italic> central complex (CX, <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib28">Green et al., 2017</xref>; <xref ref-type="bibr" rid="bib29">Green et al., 2019</xref>; <xref ref-type="bibr" rid="bib21">Franconville et al., 2018</xref>; <xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>; <xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>). Notably, in <italic>Drosophila</italic> (from here on simply referred to as ‘fly’), HD cells (named E-PG neurons, also referred to as ‘compass’ neurons) are physically arranged in a ring, and an activity bump is readily observable from a small number of cells (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>). Moreover, as predicted by some computational models (<xref ref-type="bibr" rid="bib61">Skaggs et al., 1995</xref>; <xref ref-type="bibr" rid="bib57">Samsonovich and McNaughton, 1997</xref>; <xref ref-type="bibr" rid="bib64">Stringer et al., 2002</xref>; <xref ref-type="bibr" rid="bib62">Song and Wang, 2005</xref>), the fly HD system also includes cells (named P-EN1 neurons) that are conjunctively tuned to head direction and head angular velocity. We refer to these neurons as head rotation (HR) cells because of their putative role in shifting the HD bump across the network according to the head’s angular velocity (<xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>).</p><p>A model for PI needs to both sustain a bump of activity and move it with the right speed and direction around the ring. The latter presents a great challenge, since the bump has to be ‘pushed’ for the right amount starting from any location and for all angular velocities. Therefore, ring attractor models that act as path integrators require that synaptic connections are precisely tuned (<xref ref-type="bibr" rid="bib32">Hahnloser, 2003</xref>). If the circuit was completely hardwired, the amount of information that an organism would need to genetically encode connection strenghts would be exceedingly high. Additionally, it would be unclear how these networks could cope with variable sensory experiences. In fact, remarkable experimental studies in rodents have shown that when animals are placed in an augmented reality environment where visual and self-motion information can be manipulated independently, PI capabilities adapt accordingly (<xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref>). These findings suggest that PI networks are able to self-organize and to constantly recalibrate. Notably, in mature flies there is no evidence for such plasticity (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>) — however, the presence of plasticity has not been tested in young animals.</p><p>Here, we propose that a simple local learning rule could support the emergence of a PI circuit during development and its re-calibration once the circuit has formed. Specifically, we suggest that accurate PI is achieved by associating allothetic and idiothetic inputs at the cellular level. When available, the allothetic sensory input (here chosen to be visual) acts as a ‘teacher’ to guide learning. The learning rule is an example of self-supervised multimodal learning, where one sense acts as a teaching signal for the other and the need for an external teacher is obviated. It exploits the relation between the allothetic heading of the animal (given by the visual input) and the idiothetic self-motion cues (which are always available), to learn how to integrate the latter.</p><p>The learning rule is inspired by previous experimental and computational work on mammalian cortical pyramidal neurons, which are believed to associate inputs to different compartments through an in-built cellular mechanism (<xref ref-type="bibr" rid="bib42">Larkum, 2013</xref>; <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>; <xref ref-type="bibr" rid="bib7">Brea et al., 2016</xref>). In fact, it was shown that in layer 5 pyramidal cells internal and external information about the world arrive at distinct anatomical locations, and active dendritic gating controls learning between the two (<xref ref-type="bibr" rid="bib16">Doron et al., 2020</xref>). In a similar fashion, we propose that learning PI in the HD system occurs by associating inputs at opposite poles of compartmentalized HD neurons, which we call ‘associative neurons’ (<xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>; <xref ref-type="bibr" rid="bib7">Brea et al., 2016</xref>). Therefore, to accomplish PI the learning rule relies on structural inductive biases in terms of the morphology and arborization of HD cells.</p><p>In summary, here we show for the first time how a biologically plausible synaptic plasticity rule enables to learn and maintain the complex circuitry required for PI. We apply our framework to the fly HD system because it is well characterized; yet our model setting is general and can be used to learn PI in other animal models once more details about the HD circuit there are known (<xref ref-type="bibr" rid="bib1">Abbott et al., 2020</xref>). We find that the learned network is a ring attractor with a connectivity that is strikingly similar to the one found in the fly CX (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>) and that it can accurately path-integrate in darkness for the entire range of angular velocities that the fly displays. Crucially, the learned network accounts for several key findings in the experimental literature, and it generates predictions, including the presence of plasticity in young animals, that could be tested experimentally.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>To illustrate basic principles of how PI could be achieved, we study a computational model of the HD system and show that synaptic plasticity could shape its circuitry through visual experience. In particular, we simulate the development of a network that, after learning, provides a stable internal representation of head direction and uses only angular-velocity inputs to update the representation in darkness. The internal representation of heading (after learning) takes the form of a localized bump of activity in the ring of HD cells. All neurons in our model are rate-based, i.e., spiking activity is not modeled explicitly.</p><sec id="s2-1"><title>Model setup</title><p>The gross model architecture closely resembles the one found in the fly CX (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). It comprises HD cells organized in a ring, and HR cells organized in two wings. One wing is responsible for leftward and the other for rightward movement of the internal heading representation. HD cells receive visual input from the so-called ‘ring’ neurons; this input takes the form of a disinhibitory bump centered at the current HD (<xref ref-type="fig" rid="fig1">Figure 1B</xref>, <xref ref-type="bibr" rid="bib48">Omoto et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>). The location of this visual bump in the network is controlled by the current head direction. We simulate head movements by sampling head-turning velocities from an Ornstein-Uhlenbeck process (Materials and methods), and we provide the corresponding velocity input to the HR cells (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). HR cells provide direct input to HD cells, and HR cells also receive input from HD cells (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Both HR and HD cells receive global inhibition, which is in line with a putative ‘local’ model of HD network organization (<xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>). The connections from HR to HD cells (<inline-formula><mml:math id="inf1"><mml:msup><mml:mi>W</mml:mi><mml:mi>HR</mml:mi></mml:msup></mml:math></inline-formula>) and the recurrent connections among HD cells (<inline-formula><mml:math id="inf2"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>) are assumed to be plastic. The goal of learning is to tune these plastic connections so that the network can achieve PI in the absence of visual input.</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Network architecture.</title><p>(<bold>A</bold>) The ring of HD cells projects to two wings of HR cells, a leftward (Left HR cells, abbreviated as L-HR) and a rightward (Right HR cells, or R-HR), so that each wing receives selective connections only from a specific HD cell (L: left, R: right) for every head direction. For illustration purposes, the network is scaled-down by a factor of 5 compared to the cell numbers <inline-formula><mml:math id="inf3"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula> in the model. The schema shows the outgoing connections (<inline-formula><mml:math id="inf4"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf5"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>) only from the green HD neurons and the incoming connections (<inline-formula><mml:math id="inf6"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf7"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>) only to the light blue and yellow HD neurons. Furthermore, the visual input to HD cells and the velocity inputs to HR cells are indicated. (<bold>B</bold>) Visual input to the ring of HD cells as a function of radial distance from the current head direction (see <xref ref-type="disp-formula" rid="equ5">Equation 5</xref>). (<bold>C</bold>) Angular-velocity input to the wings of HR cells for three angular velocities: 720 (green), 0 (blue), and -360 (orange) deg/s (see <xref ref-type="disp-formula" rid="equ10">Equation 10</xref>). (<bold>D</bold>) The associative neuron: <inline-formula><mml:math id="inf8"><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf9"><mml:msup><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> denote the voltage in the axon-proximal (i.e. closer to the axon initial segment) and axon-distal (i.e. further away from the axon initial segment) compartment, respectively. Arrows indicate the inputs to the compartments, as in (<bold>A</bold>), and <inline-formula><mml:math id="inf10"><mml:msup><mml:mi>I</mml:mi><mml:mtext>vis</mml:mtext></mml:msup></mml:math></inline-formula> is the visual input current. (<bold>E</bold>) Left: skeleton plot of an example HD (E-PG) neuron (Neuron ID =416642425) created using neuPrint (<xref ref-type="bibr" rid="bib12">Clements et al., 2020</xref>) the ellipsoid body (EB) and protocerebral bridge (PB) are overlayed. Right: zoomed in area in the EB indicated by the box, showing postsynaptic locations in the EB for this E-PG neuron; for details, see Methods. The neuron receives recurrent and HR input (green and orange dots, corresponding to inputs from P-EN1 and P-EN2 cells, respectively) and visual input (purple and blue dots, corresponding to inputs from visually responsive R2 and R4d cells, respectively) in distinct spatial locations.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig1-v2.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Separation of axon-proximal and axon-distal inputs to HD (E-PG) neurons in the <italic>Drosophila</italic> EB.</title><p>(<bold>A</bold>) Synaptic locations in the EB where visual (R2 and R4d) and recurrent and HR-to-HD (P-EN1 and P-EN2) inputs arrive, for a total of 16 HD neurons tested (Neuron ID above each panel). Similarly to the example in <xref ref-type="fig" rid="fig1">Figure 1E</xref> (repeated here in the top left panel, Neuron ID 416642425), these two sets of inputs appear to arrive in separate locations. (<bold>B</bold>) Binary classification between the two classes (R2 and R4d vs. P-EN1 and P-EN2) using SVMs with Gaussian kernel. Nested 5-fold crossvalidation was performed 30 times for every neuron tested, and the test accuracy histograms per neuron are plotted. The two classes can be separated with a test accuracy greater than 0.95 for every neuron.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig1-figsupp1-v2.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-69841-fig1-video1.mp4" id="fig1video1"><label>Figure 1—video 1.</label><caption><title>A three-dimensional rotating video of the synapse locations in <xref ref-type="fig" rid="fig1">Figure 1E</xref>.</title></caption></media></fig-group><p>The unit that controls plasticity in our network is an ‘associative neuron’. It is inspired by pyramidal neurons of the mammalian cortex whose dendrites act, via backpropagating action potentials, as coincidence detectors for signals arriving from different layers of the cortex and targeting different compartments of the neuron (<xref ref-type="bibr" rid="bib41">Larkum et al., 1999</xref>). Paired with synaptic plasticity, coincidence detection can lead to long-lasting associations between these signals (<xref ref-type="bibr" rid="bib42">Larkum, 2013</xref>). To map the morphology of a cortical pyramidal cell to the one of a HD cell in the fly, we first point out that all relevant inputs arrive at the dendrites of HD cells within the ellipsoid body (EB) of the fly (<xref ref-type="bibr" rid="bib79">Xu, 2020</xref>) moreover, the soma itself is externalized in the fly brain, and it is unlikely to contribute considerably to computations (<xref ref-type="bibr" rid="bib27">Gouwens and Wilson, 2009</xref>; <xref ref-type="bibr" rid="bib70">Tuthill, 2009</xref>). We thus link the dendrites of the pyramidal associative neuron to the axon-distal dendritic compartment of the associative HD neuron in the fly, and we link the soma of the pyramidal associative neuron to the axon-proximal dendritic compartment of the associative HD neuron in the fly. Furthermore, we assume that the axon-proximal compartment is electrotonically closer to the axon initial segment, and therefore, similarly to the somatic compartment in pyramidal neurons, inputs there can more readily initiate action potentials. Note that our model does not require <italic>active</italic> backpropagation of action potentials — <italic>passive</italic> spread of voltage to the axon-distal compartment would be sufficient (for details, see Materials and methods and Discussion). We also assume that associative HD cells receive visual input (<inline-formula><mml:math id="inf11"><mml:msup><mml:mi>I</mml:mi><mml:mtext>vis</mml:mtext></mml:msup></mml:math></inline-formula>) in the axon-proximal compartment, and both recurrent input (<inline-formula><mml:math id="inf12"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>) and HR input (<inline-formula><mml:math id="inf13"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>) in the axon-distal compartment; accordingly, we model HD neurons as two-compartment units (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). The associative neuron can learn the synaptic weights of the incoming connections in the axon-distal compartment, therefore, as mentioned, we let <inline-formula><mml:math id="inf14"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf15"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> be plastic.</p><p>We find that the assumption of spatial segregation of postsynapses of HD cells is consistent with our analysis of EM data from the fly (<xref ref-type="bibr" rid="bib79">Xu, 2020</xref>). For an example HD (E-PG) neuron, <xref ref-type="fig" rid="fig1">Figure 1E</xref> depicts that head rotation and recurrent inputs (mediated by P-EN1 and P-EN2 cells, respectively [<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>]) contact the E-PG cell in locations within the EB that are distinct compared to those of visually responsive neurons R2 and R4d (<xref ref-type="bibr" rid="bib48">Omoto et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>), as hypothesized. The same pattern was observed for a total of 16 E-PG neurons (one for each ‘wedge’ of the EB) that we analyzed (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1A</xref>). To further support the assumption that visual inputs are separated from recurrent and HR-to-HD inputs, we perform binary classification between the two classes, using SVMs (for details, see Materials and methods). <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1B</xref> shows that predicting class identity from spatial location alone in held-out test data is excellent (test accuracy &gt;0.95 across neurons and model runs).</p><p>The connections from HD to HR cells (<inline-formula><mml:math id="inf16"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula>) are assumed to be fixed, and HR cells are modeled as single-compartment units. Projections are organized such that each wing neuron receives input from only one specific HD neuron for every HD (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). This simple initial wiring makes HR cells conjunctively tuned to HR and HD, and we assume that it has already been formed, for example, during circuit assembly. We note that the conditions for 1-to-1 wiring and constant amplitude of the HD-to-HR connections can be relaxed, because the learning rule can balance asymmetries in the initial architecture (see Appendix 3). In addition, the connections carrying the visual and angular velocity inputs are also assumed to be fixed. Although plasticity in the visual inputs has been shown to exist (<xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref>), here we focus on how the path-integrating circuit itself originally self-organizes. Therefore, to simplify the setting and without loss of generality, we assume a fixed anchoring to environmental cues as the animal moves in the same environment (for details, see Discussion).</p><p>In our model, the visual input acts as a supervisory signal during learning (as in <xref ref-type="bibr" rid="bib17">D’Albis and Kempter, 2020</xref>), which is used to change weights of synapses onto the axon-distal compartment of HD cells. We utilize the learning rule proposed by <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref> (for details, see Materials and methods), which tunes the incoming synaptic connections in the axon-distal compartment in order to minimize the discrepancy between the firing rate of the neuron <inline-formula><mml:math id="inf17"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf18"><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:math></inline-formula> is the axon-proximal voltage, primarily controlled by the visual input) and the prediction of the firing rate by the axon-distal compartment from axon-distal inputs alone, <inline-formula><mml:math id="inf19"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> (where <inline-formula><mml:math id="inf20"><mml:mi>p</mml:mi></mml:math></inline-formula> is a constant and <inline-formula><mml:math id="inf21"><mml:msup><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> is the axon-distal voltage, which depends on head rotation velocity). From now on, we refer to this discrepancy as ‘learning error’, or simply ‘error’ (<xref ref-type="disp-formula" rid="equ18">Equation 18</xref>; in units of firing rate). The synaptic weight change <inline-formula><mml:math id="inf22"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mtext mathvariant="italic">pre,post</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> from a presynaptic (HD or HR) neuron to a postsynaptic HD neuron is then given by:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mtext mathvariant="italic">pre,post</mml:mtext></mml:msub></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mpadded lspace="1.7pt" width="+1.7pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mtext>post</mml:mtext><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mtext>post</mml:mtext><mml:mi>d</mml:mi></mml:msubsup></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mtext>pre</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf23"><mml:mi>η</mml:mi></mml:math></inline-formula> is the constant learning rate and <inline-formula><mml:math id="inf24"><mml:msub><mml:mtext>P</mml:mtext><mml:mtext>pre</mml:mtext></mml:msub></mml:math></inline-formula> is the postsynaptic potential from the presynaptic neuron. When implementing this learning rule, we low-pass filter the prospective weight change <inline-formula><mml:math id="inf25"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mtext mathvariant="italic">pre,post</mml:mtext></mml:msub></mml:mrow></mml:math></inline-formula> to ensure smoothness of learning.</p><p>Importantly, this learning rule is biologically plausible because the firing rate of an associative neuron <inline-formula><mml:math id="inf26"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is locally available at every synapse in the axon-distal compartment due to the (passive or active) backpropagation of axonal activity to the axon-distal dendrites. The other two signals that enter the learning rule are the voltage of the axon-distal compartment <inline-formula><mml:math id="inf27"><mml:msup><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> and the postsynaptic potential <italic>P</italic>, which are also available locally at the synapse; for details, see Materials and methods. Furthermore, recent behavioral experiments show that conditioning in <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib81">Zhao et al., 2021</xref>) is not well explained by classical correlation-based plasticity, but it can be well accounted for by predictive synaptic plasticity. The latter is in line with the learning rule utilized here.</p></sec><sec id="s2-2"><title>Mature network can path-integrate in darkness</title><p><xref ref-type="fig" rid="fig2">Figure 2A</xref> shows an example of the performance of a trained network, for the light condition (i.e. when visual input is available; yellow overbars) and for PI in darkness (purple overbars); the performance is quantified by the PI error (in units of degrees) over time. PI error refers to the accumulated difference between the internal representation of heading and the true heading, and it is different from the learning error introduced previously.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Path integration (PI) performance of the network.</title><p>(<bold>A</bold>) Example activity profiles of HD, L-HR, and R-HR neurons (firing rates gray-scale coded). Activities are visually guided (yellow overbars) or are the result of PI in the absence of visual input (purple overbar). The ability of the circuit to follow the true heading is slightly degraded during PI in darkness. The PI error, that is, the difference between the PVA and the true heading of the animal as well as the instantaneous head angular velocity are plotted separately. (<bold>B</bold>) Temporal evolution of the distribution of PI errors in darkness, for 1000 simulations. The distribution gets wider with time, akin to a diffusion process. We estimate the diffusion coefficient to be <inline-formula><mml:math id="inf28"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>24.5</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>deg</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> (see ‘Diffusion Coefficient’ in Materials and methods). Note that, unless otherwise stated, for this type of plot we limit the range of angular velocities to those normally exhibited by the fly, i.e. <inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> deg/s. (<bold>C</bold>) Relation between head angular velocity and neural angular velocity, i.e., the speed with which the bump moves in the network. There is almost perfect (gain 1) PI in darkness for head angular velocities within the range of maximum angular velocities that are displayed by the fly (dashed green horizontal lines; see Methods). (<bold>D</bold>) Example of consecutive stimulations in randomly permeated HD locations, simulating optogenetic stimulation experiments in <xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>. Red overbars indicate when the network is stimulated with stronger than normal visual-like input, at the location indicated by the animal’s true heading (light green line), while red dashed vertical lines indicate the onset of the stimulation. The network is then left in the dark. Our simulations show that the bump remains at the stimulated positions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig2-v2.tif"/></fig><p>A unique bump of activity is clearly present at all times in the HD network (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, top), in both light and darkness conditions, and this bump moves smoothly across the network for a variable angular velocity (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, bottom). The position of the bump is defined as the population vector average (PVA) of the neural activity in the HD network. The HD bump also leads to the emergence of bumps in the HR network, separately for L-HR and R-HR cells (<xref ref-type="fig" rid="fig2">Figure 2A</xref>, second and third panel from top). In light conditions (0–20 s in <xref ref-type="fig" rid="fig2">Figure 2A</xref>), the PVA closely tracks the head direction of the animal in HD, L-HR, and R-HR cells alike, which is expected because the visual input guides the network activity. Importantly, however, in darkness (20–50 s in <xref ref-type="fig" rid="fig2">Figure 2A</xref>), the self-motion input alone is enough to track the animal’s heading, leading to a small PI error between the internal representation of heading and the ground truth. This error is corrected after the visual input reappears (at 50 s in <xref ref-type="fig" rid="fig2">Figure 2A</xref>). Such PI errors in darkness are qualitatively consistent with data reported in the experimental literature (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>). The correction of the PI error also reproduces in silico the experimental finding that the visual input (whenever available) exerts stronger control on the bump location than the self-motion input (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>), which suggests that even the mature network does not rely on PI when visual cues are available.</p><p>To quantify the accuracy of PI in our model, we draw 1,000 trials, each 60 s long, for constant synaptic weights and in the absence of visual input. We also limit the angular velocities in these trials to retain only velocities that flies realistically display (see dashed green lines in <xref ref-type="fig" rid="fig2">Figure 2C</xref> and Methods). We then plot the distribution of PI errors every 10 s (<xref ref-type="fig" rid="fig2">Figure 2B</xref>). We find that average absolute PI errors (widths of distributions) increase with time in darkness, but most of the PI errors at 60 s are within 60 deg of the true heading. This vastly exceeds the PI performance of flies (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>). In flies, the correlation between the PVA estimate and the true heading in darkness varied widely across animals in the range [0.3, 0.95] (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>), whereas for the model it is close to 1. However, it should be noted that the model here corresponds to an ideal scenario that serves as a proof of principle. We will later incorporate irregularities owing to biological factors (asymmetry in the weights, biological noise) that bring the network’s performance closer to the fly’s behavior.</p><p>To further assess the network’s ability to integrate different angular velocities, we simulate the system both with and without visual input in 5 s intervals during which the angular velocity is constant. We then compute the average movement velocity of the bump across the network, that is the neural velocity, and compare it to the real velocity provided as input. <xref ref-type="fig" rid="fig2">Figure 2C</xref> shows that the network achieves a PI gain (defined as the ratio between neural and real velocity) close to 1 both with and without supervisory visual input, meaning that the neural velocity matches very well the angular velocity of the animal, for all angular velocities that are observed in experiments (<inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> deg/s for walking and flying) (<xref ref-type="bibr" rid="bib24">Geurten et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Stowers et al., 2017</xref>). Although expected in light conditions, the fact that gain 1 is achieved in darkness shows that the network predicts the missing visual input from the velocity input, that is, the network path integrates accurately. Note that PI is impaired in our model for very small angular velocities (<xref ref-type="fig" rid="fig2">Figure 2C</xref>, flat purple line for <inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>30</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> deg/s), similarly to previous hand-tuned theoretical models (<xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>). This is a direct consequence of the fact that maintaining a stable activity bump and moving it across the network at very small angular velocities are competing goals. Crucially, it has been reported that such an impairment of PI for small angular velocities exists in flies (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>). Note that if we increase the number of HD neurons from 60 (∼50 were reported in the fly by <xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Xu, 2020</xref>) to 120 or 240, this flat region is no longer observed (data not shown).</p></sec><sec id="s2-3"><title>The network is a quasi-continuous attractor</title><p>A continuous attractor network (CAN) should be able to maintain a localised bump of activity in virtually a continuum of locations around the ring of HD cells. To prove that the learned network approximates this property, we seek to reproduce in silico experimental findings in <xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>. There it was shown that local optogenetic stimulation of HD cells in the ring can cause the activity bump to jump to a new position and persist in that location — supported by internal dynamics alone.</p><p>To reproduce the experiments by <xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>, we simulate optogenetic stimulation of HD cells in our network as visual input of increased strength and extent (for details, see Materials and methods). We find that the strength and extent of the stimulation needs to be increased relative to that of the visual input; only in this case, a bump at some other location in the network can be suppressed, and a new bump emerges at the stimulated location. The stimuli are assumed to appear instantaneously at random locations, but we restrict our set of stimulation locations to the discrete angles represented by the finite number of HD neurons. Furthermore, the velocity input is set to zero for the entire simulation, signaling lack of head movement.</p><p><xref ref-type="fig" rid="fig2">Figure 2D</xref> shows network activity in response to several stimuli, when the stimulation location changes abruptly every 5 s. During stimulation (2 s long, red overbars), the bump is larger than normal due to the use of a stronger than usual visual-like input to mimic optogenetic stimulation. The way in which the network responds to a stimulation depends on how far away from the ‘current’ location it is stimulated: for shorter distances, the bump activity shifts to the new location, as evidenced by the transient dynamics at the edges of the bump resembling a decay from an initial to a new location (see <xref ref-type="fig" rid="fig2">Figure 2D</xref> at {5,15,20} s). However, for larger phase shifts <inline-formula><mml:math id="inf32"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula> the bump first emerges in the new location and subsequently disappears at the initial location, a mechanism akin to a ‘jump’ (<xref ref-type="fig" rid="fig2">Figure 2D</xref>, all other transitions). Similar effects have been observed in the experimental literature (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>). The way the network responds to stimulation indicates that it operates in a CAN manner, and not as a winner-takes-all network where changes in bump location would always be instantaneous (<xref ref-type="bibr" rid="bib10">Carpenter and Grossberg, 1987</xref>; <xref ref-type="bibr" rid="bib33">Itti et al., 1998</xref>; <xref ref-type="bibr" rid="bib75">Wang, 2002</xref>). That is to say, the network operates as expected from a quasi-continuous attractor. Furthermore, we find that the transition strategy in our model changes from predominantly smooth transitions to jumps at <inline-formula><mml:math id="inf33"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>90</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, which matches experiments well (<xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>).</p><p>Following a 2 s stimulation, the network activity has converged to the new cued location. After the stimulation has been turned off, the bump remains at the new location (within the angular resolution <inline-formula><mml:math id="inf34"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:math></inline-formula> of the network), supported by internal network dynamics alone (<xref ref-type="fig" rid="fig2">Figure 2D</xref>). We confirmed in additional simulations that the bump does not drift away from the stimulated location for extended periods of time (3 min duration tested, only 3 s shown), and for all discrete locations in the HD network (only six locations shown). Therefore, we conclude that the HD network is a quasi-continuous attractor that can reliably sustain a heading representation over time in all HD locations. Note that for the network size used (<inline-formula><mml:math id="inf35"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula>) we still obtain discrete attractors with separated basins of attraction; however it is expected that with increasing <inline-formula><mml:math id="inf36"><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> adjacent attractors will merge when the intrinsic noise overcomes the barrier separating them. Indeed, we find that for <inline-formula><mml:math id="inf37"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula> it is easier to diffuse to adjacent attractors in the presence of synaptic input noise; for the impact of noise, see <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>. In reality, the bump may drift away due to asymmetries in the connectivity of the biological circuit as well as intrinsic noise (<xref ref-type="bibr" rid="bib9">Burak and Fiete, 2012</xref>) see also Appendix 1. In flies, for instance, the bump can stay put only for several seconds (<xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>).</p></sec><sec id="s2-4"><title>Learning results in synaptic connectivity that matches the one in the fly</title><p>To gain more insight into how the network achieves PI and attains CAN properties, we show how the synaptic weights of the network are tuned during a developmental period (<xref ref-type="fig" rid="fig3">Figure 3</xref>). <xref ref-type="fig" rid="fig3">Figure 3A and B</xref> shows the learned recurrent synaptic weights among the HD cells, <inline-formula><mml:math id="inf38"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>, and the learned synaptic weights from HR to HD cells, <inline-formula><mml:math id="inf39"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>, respectively. Circular symmetry is apparent in both matrices, a crucial property for a symmetric ring attractor. Therefore, we also plot the profiles of the learned weights as a function of receptive field difference in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. Note that pixelized appearance in these plots is due to the fact that two adjacent HD neurons are tuned for the same HD, and develop identical synaptic strengths.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The network connectivity during and after learning.</title><p>(<bold>A</bold>), (<bold>B</bold>) The learned weight matrices (color coded) of recurrent connections in the HD ring, <inline-formula><mml:math id="inf40"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>, and of HR-to-HD connections, <inline-formula><mml:math id="inf41"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>, respectively. Note the circular symmetry in both matrices. (<bold>C</bold>) Profiles of (<bold>A</bold>) and (<bold>B</bold>), averaged across presynaptic neurons. (<bold>D</bold>) Absolute learning error in the network (<xref ref-type="disp-formula" rid="equ19">Equation 19</xref>) for 12 simulations (transparent lines) and average across simulations (opaque line). At time <inline-formula><mml:math id="inf42"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we initialize all the plastic weights at random and train the network for <inline-formula><mml:math id="inf43"><mml:mrow><mml:mn>8</mml:mn><mml:mo>×</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>4</mml:mn></mml:msup></mml:mrow></mml:math></inline-formula> s (∼22 hr). The mean learning error increases in the beginning while a bump in <inline-formula><mml:math id="inf44"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> is emerging, which is necessary to generate a pronounced bump in the network activity. For weak activity bumps, absolute errors are small because the overall network activity is low. After ∼1 hr of training, the mean learning error decreases with increasing training time and converges to a small value. (<bold>E</bold>), (<bold>F</bold>) Time courses of development of the profiles of <inline-formula><mml:math id="inf45"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf46"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>, respectively. Note the logarithmic time scale.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig3-v2.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Removal of long-range excitatory projections impairs PI for high angular velocities.</title><p>(<bold>A</bold>) Profiles of the HR-to-HD weight matrix <inline-formula><mml:math id="inf47"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> from <xref ref-type="fig" rid="fig3">Figure 3C</xref> (dashed lines), and the same profiles after the long-range excitatory projections have been removed (solid lines). (<bold>B</bold>) PI in the resulting network is impaired for high angular velocities, compared to <xref ref-type="fig" rid="fig2">Figure 2C</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig3-figsupp1-v2.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Details of learning.</title><p>(<bold>A</bold>) Learning errors (<xref ref-type="disp-formula" rid="equ18">Equation 18</xref>) in the converged network in light conditions (yellow overbar) or during PI in darkness (purple overbar). Note the difference in scale. In light conditions, the error is zero in all positions apart from the edges of the bump, where the error is substantial. Such errors occur because the velocity pathway, which implements PI, cannot move the bump for very small angular velocities, and tends to move it slightly faster for intermediate velocities, and slower for large ones (see <xref ref-type="fig" rid="fig2">Figure 2C</xref>). The velocity pathway is active and affects network activity even in the presence of visual input; hence, in light conditions, it creates errors at the edges of the bump, and the sign of the errors is consistent with the aforementioned PI velocity biases. Other than that, the angular velocity input predicts the visual input near-perfectly, as evidenced by the near-zero error everywhere else in the network. During PI in darkness, the network operates in a self-consistent manner, merely integrating the angular velocity input, and the learning error is much smaller. (<bold>B</bold>) Snapshot of the bump and the errors at <inline-formula><mml:math id="inf48"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>11.5</mml:mn></mml:mrow></mml:math></inline-formula> s in light conditions from (<bold>A</bold>). Also overlaid is the hypothetical form of the bump if only the visual input was present in the axon-proximal compartment of the HD neurons, termed ‘Visual bump’. Notice that the errors are due to the fact that the visual bump is trailing in relation to the bump in the network. As a result, at the front of the bump the subthreshold visual input is actually inhibiting the bump. Also note that the bump in the network has a square form, in contrast to the smoother form that would be expected from visual input alone. This is because the learning rule in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> only converges when HD neurons reach saturation (see also panel A2). (<bold>C</bold>) Histogram of entrained velocities.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig3-figsupp2-v2.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>PI performance of a perturbed network.</title><p>After learning, the synaptic connections in <xref ref-type="fig" rid="fig3">Figure 3A and B</xref> have been perturbed with Gaussian noise with standard deviation ∼1.5. (<bold>A</bold>), (<bold>B</bold>) Synaptic weight matrices after noise addition. (<bold>C</bold>) Example of PI. The activity of HD, L-HR, and R-HR neurons along with the PI error and instantaneous angular velocity are displayed, as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>. (<bold>D</bold>) Temporal evolution of distribution of PI errors during PI in darkness. Compared to <xref ref-type="fig" rid="fig2">Figure 2B</xref> the distribution widens faster, and also exhibits side bias. (<bold>E</bold>) PI is impaired compared to <xref ref-type="fig" rid="fig2">Figure 2C</xref>, particularly for small angular velocities. Note that the exact form of the PI gain curve at very small angular velocities may vary slightly depending on the noise realization, but the findings mentioned in the Results (middle part of last paragraph of section ‘Learning results in synaptic connectivity that matches the one in the fly’) remain consistent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig3-figsupp3-v2.tif"/></fig></fig-group><p>First, we discuss the properties of the learned weights. Local excitatory connections have developed along the main diagonal of <inline-formula><mml:math id="inf49"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>, similar to what is observed in the CX (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>). This local excitation can be readily seen in the weight profile of <inline-formula><mml:math id="inf50"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, and it is the substrate that allows the network to support stable activity bumps in virtually any location. In addition, we observe inhibition surrounding the local excitatory profile in both directions. This inhibition emerges despite the fact that we provide global inhibition to all HD cells (<inline-formula><mml:math id="inf51"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> parameter, Materials and methods), in line with suggestions from previous work (<xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>). Surrounding inhibition was a feature we observed consistently in learned networks of different sizes and for different global inhibition levels. Finally, the angular offset of the two negative sidelobes in the connectivity depends on the size and shape of the entrained HD bump (for details, see Appendix 5).</p><p>Furthermore, we find a consistent pattern of both L-HR and R-HR populations to excite the direction for which they are selective (<xref ref-type="fig" rid="fig3">Figure 3C</xref>), which is also similar to what is observed in the CX (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>). Excitation in one direction is accompanied by inhibition in the reverse direction in the learned network. As a result of the symmetry in our learning paradigm, the connectivity profiles of L-HR and R-HR cells are mirrored versions of each other, which is also clearly visible in <xref ref-type="fig" rid="fig3">Figure 3C</xref>. The inhibition of the reverse direction has a width comparable to the bump size and acts as a ‘break’ to prevent the bump from moving in this direction. The excitation in the selective direction, on the other hand, has a wider profile, which allows the network to path integrate for a wide range of angular velocities, that is for high angular velocities neurons further downstream can be ‘primed’ and activated in rapid succession. Indeed, when we remove the wide projections from the excitatory connectivity, PI performance is impaired for the higher angular velocities exclusively (<xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref>). The even weight profile in <inline-formula><mml:math id="inf52"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and the mirror symmetry for L-HR vs. R-HR profiles in <inline-formula><mml:math id="inf53"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>, together with the circular symmetry of the weights throughout the ring, guarantee that there is no side bias (i.e. tendency of the bump to favor one direction of movement versus the other) during PI. Indeed, the PI error distribution in <xref ref-type="fig" rid="fig2">Figure 2B</xref> remains symmetric throughout the 60 s simulations.</p><p>Next, we focus our attention on the dynamics of learning. For training times larger than a few hours, the absolute learning error drops and settles to a low value, indicating that learning has converged after ∼20 hr (or 4000 cycles, each cycle lasting <inline-formula><mml:math id="inf54"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula>) of training time (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). The non-zero value of the final error is only due to errors occurring at the edges of the bump (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2A</xref>, top panel). An intuitive explanation of why these errors persist is that the velocity pathway is learning to predict the visual input; as a result, when the visual input is present, the velocity pathway creates errors that are consistent with PI velocity biases in darkness.</p><p><xref ref-type="fig" rid="fig3">Figure 3E and F</xref> shows the weight development history for the entire simulation. The first structure that emerges during learning is the local excitatory recurrent connections in <inline-formula><mml:math id="inf55"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>. For these early stages of learning, the initial connectivity is controlled by the autocorrelation of the visual input, which gets imprinted in the recurrent connections by means of Hebbian co-activation of adjacent HD neurons. As a result, the width of the local excitatory profile mirrors the width of the visual input. Once a clear bump is established in the HD ring, the HR connections are learned to support bump movement, and negative sidelobes in <inline-formula><mml:math id="inf56"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> emerge. To understand the shape of the learned connectivity profiles and the dynamics of their development, we study a reduced version of the full model, which follows learning in bump-centric coordinates (see Appendix 5). The reduced model produces a connectivity strikingly similar to the full model, and highlights the important role of non-linearities in the system.</p><p>So far, we have shown results in which our model far outperforms flies in terms of PI accuracy. To bridge this gap, we add noise to the weight connectivity in <xref ref-type="fig" rid="fig3">Figure 3A and B</xref> and obtain the connectivity matrices in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A,B</xref>, respectively. This perturbation of the weights could account for irregularities in the fly HD system owning to biological factors such as uneven synaptic densities. The resulting neural velocity gain curve in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3E</xref> is impaired mainly for small angular velocities (<xref ref-type="fig" rid="fig2">Figure 2C</xref>). Interestingly, it now bears greater similarity to the one observed in flies, because the previously flat area for small angular velocities is wider (flat for <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> deg/s, cf. extended data fig. 7G,J in <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>). This happens because the noisy connectivity is less effective in initiating bump movement. Finally, the PI errors in the network with noisy connectivity grow much faster and display a strong side bias (<xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3D</xref>, <xref ref-type="fig" rid="fig2">Figure 2B</xref>). The latter can be attributed to the fact that the noise in the connectivity generates local minima that are easier to transverse from one direction vs. the other. Side bias can also emerge if the learning rate <inline-formula><mml:math id="inf58"><mml:mi>η</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ16">Equation 16</xref> is increased, effectively forcing learning to converge faster to a local minimum, which results in slight deviations from circularly symmetric connectivity (data not shown). It is therefore expected that different animals will display different degrees and directions of side bias during PI, owning either to fast learning or asymmetries in the underlying neurobiology. Since the exact behavior of the network with noise in the connectivity depends on the specific realization, we also generate multiple such networks and estimate the diffusion coefficient during path integration, which quantifies how fast the width of the PI error distribution in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3D</xref> increases. We find the grand average to be <inline-formula><mml:math id="inf59"><mml:mrow><mml:mn>82.3</mml:mn><mml:mo>±</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>15.7</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>deg</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, which is considerably larger (Student’s t-test, 95% conf. intervals for a total of 12 networks) than the diffusion coefficient for networks without a perturbation in the weights (<inline-formula><mml:math id="inf60"><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>24.5</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>deg</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Finally, in Appendix 1 we also incorporate random Gaussian noise to all inputs, which can account for noisy percepts or stochasticity of spiking, and show that learning is not disrupted even for high noise levels.</p></sec><sec id="s2-5"><title>Fast adaptation of neural velocity gain</title><p>Having shown how PI and CAN properties are learned in our model, we now turn our attention to the flexibility that our learning setup affords. Motivated by augmented-reality experiments in rodents where the relative gain of visual and self-motion inputs is manipulated (<xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref>), we test whether our network can rewire to learn an arbitrary gain between the two. In other words, we attempt to learn an arbitrary gain <inline-formula><mml:math id="inf61"><mml:mi>g</mml:mi></mml:math></inline-formula> between the idiothetic angular velocity <inline-formula><mml:math id="inf62"><mml:mi>v</mml:mi></mml:math></inline-formula> sensed by the HR cells and the neural velocity <inline-formula><mml:math id="inf63"><mml:mrow><mml:mi>g</mml:mi><mml:mo>⋅</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> dictated by the allothetic visual input. This simulates the conditions in an augmented reality environment, where the speed at which the world around the animal rotates is determined by the experimenter, but the proprioceptive sense of head angular velocity remains the same.</p><p>Starting with the learned network shown in <xref ref-type="fig" rid="fig3">Figure 3</xref>, which displayed gain <inline-formula><mml:math id="inf64"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, we suddenly switch to a different gain, that is we learn weights for <inline-formula><mml:math id="inf65"><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo>,</mml:mo><mml:mn>1.5</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. In all cases, we observe that the network readily rewires to achieve the new gain. The mean learning error after the gain switch is initially high, but reaches a lower, constant level after at most 3 hr of training (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). We note that convergence is much faster compared to the time it takes for the gain-1 network to emerge from scratch (compare to <xref ref-type="fig" rid="fig3">Figure 3D</xref>), especially for the smaller gain changes. Importantly, <xref ref-type="fig" rid="fig4">Figure 4B</xref> shows that PI performance in the resulting networks is excellent for the new gains, with some degradation only for very low and very high angular velocities. There are two reasons why high angular velocities are not learned that well: limited training of these velocities, and saturation of HR cell activity. Both reasons are by design and do not reflect a fundamental limit of the network.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>The network adapts rapidly to new gains.</title><p>Starting from the converged network in <xref ref-type="fig" rid="fig3">Figure 3</xref>, we change the gain <inline-formula><mml:math id="inf66"><mml:mi>g</mml:mi></mml:math></inline-formula> between visual and self-motion inputs, akin to experiments conducted in VR in flies and rodents (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref>). (<bold>A</bold>) The mean learning error averaged across 12 simulations for each gain. After an initial increase due to the change of gain, the errors decrease rapidly and settle to a lower value. The steady-state values depend on the gain due to the by-design impairment of high angular velocities, which affects high gains preferentially. Crucially, adaptation to a new gain is much faster than learning the HD system from scratch (<xref ref-type="fig" rid="fig3">Figure 3D</xref>). (<bold>B</bold>) Velocity gain curves for different gains. The network has remapped to learn accurate PI with different gains for the entire dynamic range of head angular velocity inputs (approx. [-500, 500] deg/s). (<bold>C</bold>), (<bold>D</bold>) Final profiles of <inline-formula><mml:math id="inf67"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf68"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>, respectively, for different gains.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig4-v2.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Limits of PI gain adaptation.</title><p>(<bold>A</bold>) Normalized root mean square error (NRMSE) between neural and head angular velocity, for gain-1 networks that subsequently have been rewired to learn different gains. To compute the NRMSE, we first estimate from PI performance plots (examples in (<bold>B</bold>), (<bold>C</bold>), and (<bold>D</bold>)) the root mean square error (RMSE) between the neural and head angular velocity, but only within the range of head angular velocities that each network can handle. This range is restricted because there is a maximum neural angular velocity <inline-formula><mml:math id="inf69"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (e.g. blue dot-dashed line in C; see also <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>); thus the range of head angular velocity is given by this maximum neural angular velocity divided by the gain <inline-formula><mml:math id="inf70"><mml:mi>g</mml:mi></mml:math></inline-formula>. Then, to obtain the NRMSE we divide the RMSE by that range. For instance, in (<bold>C</bold>), <inline-formula><mml:math id="inf71"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1150</mml:mn></mml:mrow></mml:math></inline-formula> deg/s. Then the head angular velocity range we test is determined by the x-coordinate for which the Gain-10 line (gray, dashed) meets the neural velocity limit line (blue, dot-dash); hence in this extreme example we only test for the range [-115, 115] deg/s. We find that rewiring performance is excellent for gains <inline-formula><mml:math id="inf73"><mml:mi>g</mml:mi></mml:math></inline-formula> between 0.25 and 4.5, for which NRMSE is less than 0.15. Note that the more a new gain differs from original gain 1, the longer it takes for the network to rewire. (<bold>B</bold>), (<bold>C</bold>) PI performance plots for a small (<inline-formula><mml:math id="inf74"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>0.125</mml:mn></mml:mrow></mml:math></inline-formula>) and a large (<inline-formula><mml:math id="inf75"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula>) gain. The NRMSE is 0.31 and 0.46, respectively. Performance is impaired because the flat area for small angular velocities gets enlarged in (<bold>B</bold>), whereas the network struggles to keep up with the desired gain in (<bold>C</bold>). (<bold>D</bold>) PI performance plot for a network that has been instructed to reverse its gain (from +1 to -1), i.e. when the visual and self-motion inputs are signaling movement in opposite directions. Performance is excellent, indicating that there is nothing special about negative gains; albeit learning takes considerably more time. (<bold>E</bold>), (<bold>F</bold>) Weight history for HR-to-HD and for recurrent connections, respectively, for the network trained to reverse its gain. The directionality of the asymmetric HR-to-HD connections in (<bold>E</bold>) reverses only after ∼20 hours, while the recurrent weights in (<bold>F</bold>) remain largely unaltered.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-fig4-figsupp1-v2.tif"/></fig></fig-group><p>In Appendix 2, we show that without the aforementioned limitations the network learns to path-integrate up to an angular velocity limit set by synaptic delays and that the bump width sets a trade-off between location and velocity-integration accuracy in the HD system.</p><p><xref ref-type="fig" rid="fig4">Figure 4C and D</xref> compare the weight profiles of the circularly symmetric matrices <inline-formula><mml:math id="inf76"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf77"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> resulting from the initial gain <inline-formula><mml:math id="inf78"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, with the weight profiles resulting from adaptation to the most extreme gains shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>, that is <inline-formula><mml:math id="inf79"><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. An increase in gain slightly suppresses the recurrent connections and slightly amplifies the HR-to-HD connections, while a decrease in gain substantially amplifies the recurrent connections and slightly suppresses the HR-to-HD connections. The latter explains why the flat region for small angular velocities in <xref ref-type="fig" rid="fig4">Figure 4B</xref> has been extended for <inline-formula><mml:math id="inf80"><mml:mrow><mml:mi>g</mml:mi><mml:mo>∈</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:mn>0.5</mml:mn><mml:mo stretchy="false">}</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>: it is now harder for small angular velocities to overcome the attractor formed by stronger recurrent weights and move the bump.</p><p>Finally, we address the limits of the ability of the network to rewire to new gains (<xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). We find that after rewiring the performance is excellent for gains between 0.25 and 4.5. The network can even reverse its gain to <inline-formula><mml:math id="inf81"><mml:mrow><mml:mi>g</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:mrow></mml:math></inline-formula>, that is, when allothetic and idiothetic inputs are signaling movement in opposite directions. However, for larger gain changes, learning takes longer.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>The ability of animals to navigate in the absence of external cues is crucial for their survival. Head direction, place, and grid cells provide internal representations of space (<xref ref-type="bibr" rid="bib55">Ranck, 1984</xref>; <xref ref-type="bibr" rid="bib46">Moser et al., 2008</xref>) that can persist in darkness and possibly support path integration (PI) (<xref ref-type="bibr" rid="bib45">Mizumori and Williams, 1993</xref>; <xref ref-type="bibr" rid="bib53">Quirk et al., 1990</xref>; <xref ref-type="bibr" rid="bib31">Hafting et al., 2005</xref>). Extensive theoretical work has focused on how the spatial navigation system might rely on continuous attractor networks (CANs) to maintain and update a neural representation of the animal’s current location. Special attention was devoted to models representing orientation, with the ring attractor network being one of the most famous of these models (<xref ref-type="bibr" rid="bib2">Amari, 1977</xref>; <xref ref-type="bibr" rid="bib5">Ben-Yishai et al., 1995</xref>; <xref ref-type="bibr" rid="bib61">Skaggs et al., 1995</xref>; <xref ref-type="bibr" rid="bib59">Seung, 1996</xref>). So far, modelling of the HD system has been relying on hand-tuned synaptic connectivity (<xref ref-type="bibr" rid="bib80">Zhang, 1996</xref>; <xref ref-type="bibr" rid="bib78">Xie et al., 2002</xref>; <xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>; <xref ref-type="bibr" rid="bib50">Page et al., 2019</xref>) without reference to its origin; or has been relying on synaptic plasticity rules that either did not achieve gain-1 PI (<xref ref-type="bibr" rid="bib64">Stringer et al., 2002</xref>) or were not biologically plausible (<xref ref-type="bibr" rid="bib32">Hahnloser, 2003</xref>).</p><sec id="s3-1"><title>Summary of findings</title><p>Inspired by the recent discovery of a ring attractor network for HD in <italic>Drosophila</italic> (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>), we show how a biologically plausible learning rule leads to the emergence of a circuit that achieves gain-1 PI in darkness. The learned network features striking similarities in terms of connectivity to the one experimentally observed in the fly (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>), and reproduces experiments on CAN dynamics (<xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>) and gain changes between external and self-motion cues in rodents (<xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref>). Furthermore, an impairment of PI for small angular velocities is observed in the mature network, which is a feature that has been reported in experiments (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>). Finally, the proposed learning rule can serve to compensate deviations from circular symmetry in the synaptic weight profiles; such deviations are expected in biological systems and — if not compensated — could lead to large PI errors.</p><p>The mature circuit displays two properties characteristic of CANs: (1) it can support and actively maintain a local bump of activity at a virtual continuum of locations, and (2) it can move the bump across the network by integrating self-motion cues. Note that we did not explicitly train the network to achieve these CAN properties, but they rather emerged in a self-organized manner.</p><p>To achieve gain-1 PI performance, our network must attribute learning errors to the appropriate weights. The learning rule we adopt in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> is a ‘delta-like’ rule, with a learning error that gates learning in the network, and a Hebbian component that comes in the form of the postsynaptic potential and assigns credit to synapses that are active when errors are large. The learning rule leads to the emergence of both symmetric local connectivity between HD cells (which is required for bump maintenance and stability), and asymmetric connectivity from HR to HD cells (which is required for bump movement in darkness). The first happens because adjacent neurons are co-active due to correlated visual input; the second because only one HR population is predominantly active during rotation: the population that corresponds to the current rotation direction. Crucial to the understanding of the learning dynamics of the model was the development of a reduced model, which follows learning in bump-centric coordinates and is analytically tractable (see Appendix 5). The reduced model can be extended to higher dimensional manifolds (<xref ref-type="bibr" rid="bib23">Gardner et al., 2022</xref>), and therefore it offers a general framework to study how activity-dependent synaptic plasticity shapes CANs.</p></sec><sec id="s3-2"><title>Relation to experimental literature</title><p>Our work comes at a time at which the fly HD system receives a lot of attention (<xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>; <xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>), and suggests a mechanism of how this circuit could self-organize during development. Synaptic plasticity has been shown to be important in this circuit for anchoring the visual input to the HD neurons when the animal is exposed to a new environment (<xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>). This has also been demonstrated in models of the mammalian HD system (<xref ref-type="bibr" rid="bib61">Skaggs et al., 1995</xref>; <xref ref-type="bibr" rid="bib80">Zhang, 1996</xref>; <xref ref-type="bibr" rid="bib62">Song and Wang, 2005</xref>). Here, we assume that an initial anchoring of the topographic visual input to the HD neurons with arbitrary offset with respect to external landmarks already exists prior to the development of the PI circuit; such an anchoring could even be prewired. In our model, it is sufficient that the visual input tuning is local and topographically arranged. Once the PI circuit has developed, visual connections could be anchored to different environments, as shown by <xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref> and <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>. Alternatively, the HD system itself could come prewired with an initial gross connectivity, sufficient to anchor the visual input; in this case, our learning rule would enable fine tuning of this connectivity for gain-1 PI. In either case, for the sake of simplicity and without loss of generality, we study the development of the path-integrating circuit while the animal moves in the same environment, and keep the visual input tuning fixed. Therefore, the present work addresses the important question of how the PI circuit itself could be formed, and it is complementary to the problem of how allothetic inputs to the PI circuit are wired (<xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>; <xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref>). The interplay of the two forms of plasticity during development would be of particular future interest.</p><p>A requirement for the learning rule we use is that information about the firing rate of HD neurons is available at the axon-distal compartment. There is no evidence for active backpropagation of APs in E-PG neurons in the fly, but passive backpropagation would suffice in this setting. In fact, passive spread of activity has been shown to attenuate weakly in central fly neurons (<xref ref-type="bibr" rid="bib27">Gouwens and Wilson, 2009</xref>). In HD neurons, the axon-proximal and axon-distal compartments belong to the same dendritic tuft (<xref ref-type="fig" rid="fig1">Figure 1E</xref>), and since we assume that the axon initial segment is close to the axon-proximal compartment, the generated AP would need to propagate only a short distance compared to the effective electrotonic length. This means that APs would not be attenuated much on their way from the axon initial segment to the axon-distal compartment, and thus would maintain some of their high-frequency component, which could be used at synapses to differentiate them from slower postsynaptic potentials.</p><p>In <xref ref-type="fig" rid="fig4">Figure 4</xref>, we show that our network can adapt to altered gains much faster than the time required to learn the network from scratch. Our simulations are akin to experiments where rodents are placed in a VR environment and the relative gain between visual and proprioceptive signals is altered by the experimenter (<xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref>). In this scenario, <xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref> found that the PI gain of place cells can be recalibrated rapidly. In contrast, <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref> found that PI gain in darkness is not significantly affected when flies are exposed to different gains in light conditions. We note, however, that <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref> tested mature animals (8–11 days old), whereas plasticity in the main HD network is presumably stronger in younger animals. Also note that the manipulation we use to address adaptation of PI to different gains differs from the one in <xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref> who used optogenetic stimulation of the HD network combined with rotation of the visual scene to trigger a remapping of the visual input to the HD cells in a Hebbian manner. The findings in <xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref> can only be reconciled by plasticity in the PI circuit, and not in the sensory inputs to the circuit.</p><p>In order to address the core mechanisms that underlie the emergence of a path integrating network, we use a model that is a simplified version of the biological circuit. For example, we did not model inhibitory neurons explicitly and omitted some of the recurrent connectivity in the circuit, whose functional role is uncertain (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>). We also choose to separate PI from other complex processes that occur in the CX (<xref ref-type="bibr" rid="bib54">Raccuglia et al., 2019</xref>). Finally, we do not force the network to obey Dale’s law and do not model spiking explicitly.</p><p>Nevertheless, after learning, we obtain a network connectivity that is strikingly similar to the one of the fly HD system. Indeed, the mature model exhibits local excitatory connectivity in the HD neurons (<xref ref-type="fig" rid="fig3">Figure 3A and C</xref>), which in the fly is mediated by the excitatory loop from E-PG to P-EG to P-EN2 and back to E-PG (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>), a feature that hand-tuned models of the fly HD system did not include (<xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>). Furthermore, the HR neurons have excitatory projections towards the directions they are selective for (<xref ref-type="fig" rid="fig3">Figure 3B and C</xref>), similar to P-EN1 neurons in the fly. Interestingly, these key features that we uncover from learning have been utilized in other hand-tuned models of the system (<xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>; <xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib37">Kim et al., 2019</xref>). Future work could endeavor to come closer to the architecture of the fly HD system and benefit from the incorporation of more neuron types and the richness of recurrent connectivity that has been discovered in the fly (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>).</p><p>Compared to the fly, our network achieved better PI performance. As a simple way to match the performances, we added noise to the learned connectivity in the model; however, this is not an explanation why the fly performs worse. Indeed, there could be multiple reasons why PI performance is worse in the biological circuit. For instance, a confounder that would affect performance but not necessarily learning could be the presence of inputs that are unrelated to path integration, for example, inputs related to circadian cycles and sleep (<xref ref-type="bibr" rid="bib54">Raccuglia et al., 2019</xref>). In the presence of such confounders, a precise tuning of the weights might be crucial in order to reach the performance of the fly. In other words, only if the model outperforms the biological circuit in a simplified setting, it has a chance to perform as well in a realistic setting, with all the additional complexities the latter comes with.</p></sec><sec id="s3-3"><title>Relation to theoretical literature</title><p>A common problem with CANs is that they require fine tuning: even a slight deviation from the optimal synaptic weight tuning leads to catastrophic drifting (<xref ref-type="bibr" rid="bib26">Goldman et al., 2009</xref>). A way around this problem is to sacrifice the continuity of the attractor states in favor of a discrete number of stable states that are much more robust to noise or weight perturbations (<xref ref-type="bibr" rid="bib35">Kilpatrick et al., 2013</xref>). In our network, the small number of HD neurons enables a coarse-grained representation of heading; the network is a CAN only in a quasi-continuous manner, and the number of discrete attractors corresponds to the number of HD neurons. This makes it harder to transition to adjacent attractors, since a ‘barrier’ has to be overcome in the quasi-continuous case (<xref ref-type="bibr" rid="bib35">Kilpatrick et al., 2013</xref>). The somewhat counter-intuitive conclusion follows that a CAN with more neurons and, as a result, finer angular resolution, will not be as potent in maintaining activity, and diffusion to nearby attractors will be easier since the barrier will be lower. Indeed, we found that doubling the number of neurons produces a CAN that is less robust to noise. Overall, the quasi-continuous and coarse nature of the attractor shields the internal representation of heading against the ever-present biological noise, which would otherwise lead to diffusion of the bump with time. The fact that the network can still path-integrate accurately with this coarse-grained representation of heading is remarkable.</p><p>Seminal theoretical work on ring attractors has proven that in order to achieve gain-1 PI, the asymmetric component of the network connectivity (corresponding here to <inline-formula><mml:math id="inf82"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>) needs to be proportional to the derivative of the symmetric component (corresponding to <inline-formula><mml:math id="inf83"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula>) (<xref ref-type="bibr" rid="bib80">Zhang, 1996</xref>). However, this result rests on the assumption that asymmetric and symmetric weight profiles are mediated by the same neuronal population, as in the double-ring architecture proposed by <xref ref-type="bibr" rid="bib78">Xie et al., 2002</xref> and <xref ref-type="bibr" rid="bib32">Hahnloser, 2003</xref>, but does not readily apply to the architecture of the fly HD system where HD and HR cells are separate. In our learned network, we find that the HR weight profile is not proportional to the derivative of the recurrent weight profile, therefore this requirement is not necessary for gain-1 PI in our setting. Note that our learning setup can also learn gain-1 PI for a double-ring architecture, which additionally obeys Dale’s law (<xref ref-type="bibr" rid="bib72">Vafidis, 2019</xref>). Finally, we emphasize that circular symmetry is not a necessary condition for a ring attractor (<xref ref-type="bibr" rid="bib14">Darshan and Rivkind, 2021</xref>). Rather, symmetry in our model results from the symmetry in the architecture, the symmetrically prewired weights, and the symmetric stimulus space. If any of those were to be relaxed, the resulting network would not be circular symmetric; then, the reduced model analysis that we perform in Appendix 5 would also not be feasible, because local asymmetries in the setup would result in non-local deviations from circular symmetry of the learned weights, which was our main assumption there. Nevertheless, we demonstrated that the full model can handle such asymmetries in the setup and learn accurate PI (see Appendix 3).</p><p>Our learning setup, inspired by <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>, is similar to the one in <xref ref-type="bibr" rid="bib30">Guerguiev et al., 2017</xref> in the sense that both involve compartmentalized neurons that receive ‘target’ signals in a distinct compartment. It differs, however, in the algorithm and learning rule used. <xref ref-type="bibr" rid="bib30">Guerguiev et al., 2017</xref> use local gradient descent during a ‘target’ phase, which is separate from a forward propagation phase, akin to forward/backward propagation stages in conventional deep learning. In contrast, we use a modified Hebbian rule, and in our model ‘forward’ computation and learning happen at the same time; time multiplexing, whose origin in the brain is unclear, is not required. Our setting would be more akin to the one in <xref ref-type="bibr" rid="bib30">Guerguiev et al., 2017</xref> if an episode of PI in darkness would be required before an episode of learning in light conditions, which does not seem in line with the way animals naturally learn.</p><p>Previous theoretical work showed that head direction cells, head rotation cells, and grid cells emerge in neural networks trained for PI (<xref ref-type="bibr" rid="bib3">Banino et al., 2018</xref>; <xref ref-type="bibr" rid="bib13">Cueva and Wei, 2018</xref>). These networks were trained with backpropagation, therefore achieving gain-1 PI was not their primary focus; rather, this work elegantly demonstrated that the aforementioned cell types are efficient representations for spatial navigation that could be learned from experience.</p></sec><sec id="s3-4"><title>Testable predictions</title><p>We devote this section to discussing predictions of our model, and we suggest future experiments in flies and, potentially, other animal models. An obvious prediction of our model is that synaptic plasticity is critical for the development of the PI network for heading, and the lack of a supervisory allothetic sensory input (e.g. visual) during development should disrupt the formation of the PI system. Previous experimental work showed that head direction cells in rat pups displayed mature properties already in their first exploration of the environment outside their nest (<xref ref-type="bibr" rid="bib40">Langston et al., 2010</xref>), which may seem to contradict our assumption that the PI circuit wires during development; however, directional selectivity of HD cells in the absence of allothetic inputs and PI performance were not tested in this study. In addition, it has been shown that visually impaired flies were not able to learn to accurately estimate the size of their body. This type of learning also requires visual inputs and, upon consolidation, remains stable (<xref ref-type="bibr" rid="bib38">Krause et al., 2019</xref>).</p><p>We also predict that HD neurons have a compartmental structure where idiothetic inputs are separated from allothetic sensory inputs, which initiate action potentials more readily due to being electrotonically closer to the axon initial segment. While we already demonstrate the separation of allothetic and idiothetic inputs to E-PG neurons in the fly EB (<xref ref-type="fig" rid="fig1">Figure 1E</xref>, <xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>), our prediction can only be tested with electrophysiological experiments. Another model prediction that can be tested only with electrophysiology is that APs backpropagate from the axon-proximal compartment (at least passively but with little attenuation) to the axon-distal compartment. Then spikes could be separated from postsynaptic potentials locally at the synapse by cellular mechanisms sensitive to the spectral density of the voltage.</p><p>Finally, similarly to place cell studies in rodents (<xref ref-type="bibr" rid="bib34">Jayakumar et al., 2019</xref>), we predict that during development the PI system can adapt to experimenter-defined gain manipulations, and that it can do so faster than the time required for the system to develop from scratch. Therefore, a suggestion from this study would be to repeat in young flies the adaptation experiments by <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>.</p></sec><sec id="s3-5"><title>Outlook</title><p>The present study adds to the growing literature of potential computational abilities of compartmentalized neurons (<xref ref-type="bibr" rid="bib52">Poirazi et al., 2003</xref>; <xref ref-type="bibr" rid="bib25">Gidon et al., 2020</xref>; <xref ref-type="bibr" rid="bib51">Payeur et al., 2021</xref>). The associative HD neuron used in this study is a coincidence detector, which serves to associate external and internal inputs arriving at different compartments of the cell. Coupled with memory-specific gating of internally generated inputs, coincidence detection has been suggested to be the fundamental mechanism that allows the mammalian cortex to form and update internal knowledge about external contingencies (<xref ref-type="bibr" rid="bib16">Doron et al., 2020</xref>; <xref ref-type="bibr" rid="bib60">Shin et al., 2021</xref>). This structured form of learning does not require engineered ‘hints’ during training, and it might be the reason why neural circuits evolved to be so efficient at reasoning about the world, with the mammalian cortex being the pinnacle of this achievement. Here, we demonstrate that learning at the cellular level can predict external inputs (visual information) by associating firing activity with internally generated signals (velocity inputs) during training. This effect is due to the anti-Hebbian component of the learning rule in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>, where the product of postsynaptic axon-distal and presynaptic activity comes with a negative sign. Specifically, it has previously been demonstrated that anti-Hebbian synaptic plasticity can stabilize persistent activity (<xref ref-type="bibr" rid="bib77">Xie and Seung, 2000</xref>) and perform predictive coding (<xref ref-type="bibr" rid="bib4">Bell et al., 1997</xref>; <xref ref-type="bibr" rid="bib32">Hahnloser, 2003</xref>). At the population level, this provides a powerful mechanism to internally produce activity patterns that are identical to the ones induced from an external stimulus. This mechanism can serve as a way to anticipate external events or, as in our case, as a way of ‘filling in’ missing information in the absence of external inputs.</p><p>Local, Hebb-like learning rules are considered a weak form of learning, due to their inability to utilize error information in a sophisticated manner. Despite that, we show that local associative learning can be particularly successful in learning appropriate fine-tuned synaptic connectivity, when operating within a cell structured for coincidence detection. Therefore, in learning and reasoning about the environment, our study highlights the importance of inductive biases with developmental origin (e.g. allothetic and idiothetic inputs arrive in different compartments of associative neurons) (<xref ref-type="bibr" rid="bib39">Lake et al., 2017</xref>).</p><p>In conclusion, the present work addresses the age-old question of how to develop a CAN that performs accurate, gain-1 PI in the absence of external sensory cues. We show that this feat can be achieved in a network model of the HD system by means of a biologically plausible learning rule at the cellular level. Even though our network architecture is tailored to the one of the fly CX, the learning setup where idiothetic and allothetic cues are associated at the cellular level is general and can be applied to other PI circuits. Of particular interest is the rodent HD system: despite the lack of evidence for a topographically organized recurrent HD network in rodents, a one-dimensional HD manifold has been extracted in an unsupervised way (<xref ref-type="bibr" rid="bib11">Chaudhuri et al., 2019</xref>). Therefore, our work lays the path to study the development of ring-like neural manifolds in mammals. Finally, it has recently been shown that grid cells in mammals form a continuous attractor manifold with toroidal topology (<xref ref-type="bibr" rid="bib23">Gardner et al., 2022</xref>). It would be interesting to see if a similar mechanism underlies the emergence of PI in place and grid cells. Our model can be extended to higher dimensional CAN manifolds and provides a framework to interrogate this assumption.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><p>In what follows, we describe our computational model for learning a ring attractor network that accomplishes accurate angular PI. The model described here focuses on the HD system of the fly; however, the proposed computational setup is general and could be applied to other systems. Unless otherwise stated, the simulation parameter values are the ones summarized in <xref ref-type="table" rid="table1">Table 1</xref>. Simulation results for a given choice of parameters are very consistent across runs, hence most figures are generated from a single simulation run, unless otherwise stated.</p><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Parameter values.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Parameter</th><th valign="bottom">Value</th><th valign="bottom">Unit</th><th valign="bottom">Explanation</th></tr></thead><tbody><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf84"><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom">60</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Number of head direction (HD) neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf85"><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom">60</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Number of head rotation (HR) neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf86"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">12</td><td align="left" valign="bottom">deg</td><td align="left" valign="bottom">Angular resolution of network</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf87"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">65</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Synaptic time constant</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf88"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">-1</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Global inhibition to HD neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf89"><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Leak time constant of axon-distal compartment of HD neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf90"><mml:mi>C</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Capacitance of axon-proximal compartment of HD neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf91"><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">1</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Leak conductance of axon-proximal compartment of HD neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf92"><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">2</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Conductance from axon-distal to axon-proximal compartment</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf93"><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">4</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Excitatory input to axon-proximal compartment in light conditions</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf94"><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Synaptic input noise level</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf95"><mml:mi>M</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">4</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Visual input amplitude</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf96"><mml:msub><mml:mi>M</mml:mi><mml:mtext>stim</mml:mtext></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">16</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Optogenetic stimulation amplitude</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf97"><mml:mi>σ</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">0.15</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Visual receptive field width</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf98"><mml:msub><mml:mi>σ</mml:mi><mml:mtext>stim</mml:mtext></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0.25</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Optogenetic stimulation width</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf99"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">-5</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Visual input baseline</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf100"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">150</td><td align="left" valign="bottom">spikes/s</td><td align="left" valign="bottom">Maximum firing rate</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf101"><mml:mi>β</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">2.5</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Steepness of activation function</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf102"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">1</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Input level for 50% of the maximum firing rate</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf103"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:math></inline-formula></td><td align="left" valign="bottom">-1.5</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Global inhibition to HR neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf104"><mml:mi>k</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">1/360</td><td align="left" valign="bottom">s/deg</td><td align="left" valign="bottom">Constant ratio of velocity input and head angular velocity</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf105"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">2</td><td align="left" valign="bottom"/><td align="left" valign="bottom">Input range for which <inline-formula><mml:math id="inf106"><mml:mi>f</mml:mi></mml:math></inline-formula> has not saturated</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf107"><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula></td><td align="left" valign="bottom"><inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mn>13.</mml:mn><mml:mover><mml:mn>3</mml:mn><mml:mo accent="false">¯</mml:mo></mml:mover></mml:mrow></mml:mstyle></mml:math></inline-formula></td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Constant weight from HD to HR neurons</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf109"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Plasticity time constant</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf110"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">ms</td><td align="left" valign="bottom">Euler integration step size</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf111"><mml:msub><mml:mi>τ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">s</td><td align="left" valign="bottom">Time constant of velocity decay</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf112"><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">450</td><td align="left" valign="bottom">deg/<inline-formula><mml:math id="inf113"><mml:msqrt><mml:mi>s</mml:mi></mml:msqrt></mml:math></inline-formula></td><td align="left" valign="bottom">Standard deviation of angular velocity noise</td></tr><tr><td align="left" valign="bottom"><inline-formula><mml:math id="inf114"><mml:mi>η</mml:mi></mml:math></inline-formula></td><td align="left" valign="bottom">0.05</td><td align="char" char="." valign="bottom">1 /s</td><td align="left" valign="bottom">Learning rate</td></tr></tbody></table><table-wrap-foot><fn><p>Parameter values, in the order they appear in the Methods section. These values apply to all simulations, unless otherwise stated. Note that voltages, currents, and conductances are assumed unitless in the text; therefore capacitances have the same units as time constants.</p></fn></table-wrap-foot></table-wrap><p>.</p><sec id="s4-1"><title>Network architecture</title><p>We model a recurrent neural network comprising <inline-formula><mml:math id="inf115"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula> head-direction (HD) and <inline-formula><mml:math id="inf116"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula> head-rotation (HR) cells, which are close to the number of E-PG and P-EN1 cells in the fly central complex (CX), respectively (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>; <xref ref-type="bibr" rid="bib79">Xu, 2020</xref>). A scaled-down version of the network for <inline-formula><mml:math id="inf117"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>12</mml:mn></mml:mrow></mml:math></inline-formula> is shown in <xref ref-type="fig" rid="fig1">Figure 1A</xref>. The average spiking activity of HD and HR cells is modelled by firing-rate neurons. HD cells are organized in a ring and receive visual input, which encodes the angular position of the animal’s head with respect to external landmarks. We use a discrete representation of angles and we model two HD cells for each head direction, as observed in the biological system (<xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>). Therefore the network can represent head direction with an angular resolution <inline-formula><mml:math id="inf118"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>12</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>.</p><p>Motivated by the anatomy of the fly CX (<xref ref-type="bibr" rid="bib28">Green et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>), HR cells are divided in two populations (<xref ref-type="fig" rid="fig1">Figure 1A</xref>): a ‘leftward’ (L-HR) population (with increased velocity input when the head turns leftwards) and a ’rightward’ (R-HR) population (with increased velocity input when the head turns rightwards). After learning, these two HR populations are responsible to move the HD bump in the anticlockwise and clockwise directions, respectively.</p><p>The recurrent connections among HD cells and the connections from HR to HD cells are assumed to be plastic. On the contrary, connections from HD to HR cells are assumed fixed and determined as follows: for every head direction, one HD neuron projects to a cell in the L-HR population, and the other to a cell in the R-HR population. Because HD cells project to HR cells in a 1-to-1 manner, each HR neuron is simultaneously tuned to a particular head direction and a particular head rotation direction. The synaptic strength of the HD-to-HR projections is the same for all projections (these restrictions on the HD-to-HR connections are relaxed in Appendix 3). Finally, HR cells do not form recurrent connections.</p></sec><sec id="s4-2"><title>Neuronal model</title><p>We assume that each HD neuron is a rate-based associative neuron (<xref ref-type="fig" rid="fig1">Figure 1D</xref>), that is, a two-compartmental neuron comprising an axon-proximal and an axon-distal dendritic compartment (<xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>; <xref ref-type="bibr" rid="bib7">Brea et al., 2016</xref>). The two compartments model the dendrites of that neuron that are closer to or further away from the axon initial segment. Note that here the axon-proximal compartment replaces the somatic compartment in the original model by <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>. This is because the somata of fly neurons are typically electrotonically segregated from the rest of the cell and they are assumed to contribute little to computation (<xref ref-type="bibr" rid="bib27">Gouwens and Wilson, 2009</xref>; <xref ref-type="bibr" rid="bib70">Tuthill, 2009</xref>). We also note that to fully capture the input/output transformations that HD neurons in the fly perform, more compartments than two might be needed (<xref ref-type="bibr" rid="bib79">Xu, 2020</xref>). Finally, only HD cells are associative neurons, whereas HR cells are simple rate-based point neurons.</p><p>HD cells receive an input current <inline-formula><mml:math id="inf119"><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> to the axon-distal dendrites, which obeys<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>⁢</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf120"><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> is a vector of length <inline-formula><mml:math id="inf121"><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> with each entry corresponding to one HD cell. In <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, <inline-formula><mml:math id="inf122"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is the synaptic time constant, <inline-formula><mml:math id="inf123"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> is a <inline-formula><mml:math id="inf124"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> matrix of the recurrent synaptic weights among HD cells, <inline-formula><mml:math id="inf125"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> is a <inline-formula><mml:math id="inf126"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> matrix of the synaptic weights from HR to HD cells, <inline-formula><mml:math id="inf127"><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf128"><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> are vectors of the firing rates of HR and HD cells respectively, <inline-formula><mml:math id="inf129"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> is a constant inhibitory input common to all HD cells, and <inline-formula><mml:math id="inf130"><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> is a random noise input to the axon-distal compartment. <inline-formula><mml:math id="inf131"><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">d</mml:mi></mml:msup></mml:math></inline-formula> is drawn IID from <inline-formula><mml:math id="inf132"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, and its variance is scaled by <inline-formula><mml:math id="inf133"><mml:msubsup><mml:mi>σ</mml:mi><mml:mi>n</mml:mi><mml:mn>2</mml:mn></mml:msubsup></mml:math></inline-formula>. Note that in the main text we set <inline-formula><mml:math id="inf134"><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> to zero, but we explore different values for this parameter in Appendix 1. The constant current <inline-formula><mml:math id="inf135"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> is in line with a global-inhibition model with local recurrent connectivity, as opposed to having long-range inhibitory recurrent connectivity (<xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>). The inhibitory current <inline-formula><mml:math id="inf136"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> suppresses HD bumps in general; however the exact strength of this inhibition is not important in our model.</p><p>Since several electrophysiological parameters of the fly neurons modeled here are unknown, we use dimensionless conductance values. Therefore, in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, which describes the dynamics of the axon-distal input of HD cells, currents (e.g. <inline-formula><mml:math id="inf137"><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula>, <inline-formula><mml:math id="inf138"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula>, and <inline-formula><mml:math id="inf139"><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula>) are dimensionless. Membrane voltages are also chosen to be dimensionless, and because we measure firing rates in units of 1 /s, all synaptic weights (e.g. <inline-formula><mml:math id="inf140"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf141"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>) then have, strictly speaking, the unit ‘seconds’ (s), even though we mostly suppress this unit in the text. Importantly, all time constants (e.g. <inline-formula><mml:math id="inf142"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>), which define the time scale of dynamics, are measured in units of time (in seconds).</p><p>Our model incorporates several time scales, whose interplay is not obvious. To facilitate understanding, we summarize the parameters that define the time scales in <xref ref-type="table" rid="app4table1">Appendix 4—table 1</xref>, and discuss their relation in Appendix 4.</p><p>The axon-distal voltage <inline-formula><mml:math id="inf143"><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> of HD cells is a low-pass filtered version of the input current <inline-formula><mml:math id="inf144"><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula>, that is,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf145"><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> is the leak time constant of the axon-distal compartment. The voltage <inline-formula><mml:math id="inf146"><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> and the current <inline-formula><mml:math id="inf147"><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> have the same unit (both dimensionless), which means that the leak resistance of the axon-distal compartment is also dimensionless, and we assume that it is unity for simplicity. We choose values of <inline-formula><mml:math id="inf148"><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf149"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> (for specific values, see <xref ref-type="table" rid="table1">Table 1</xref>) so that their sum matches the phenomenological time constant of HD neurons (E-PG in the fly), while <inline-formula><mml:math id="inf150"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> equals to the phenomenological time constant of HR neurons (P-EN1 in the fly, <xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>). Note that <inline-formula><mml:math id="inf151"><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> is the low-frequency component of the axon-distal voltage originating from postsynaptic potentials, that is excluding occasional high-frequency contributions from backpropagating action potentials.</p><p>The axon-proximal voltage <inline-formula><mml:math id="inf152"><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msup></mml:math></inline-formula> of HD cells is then given by<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:mrow><mml:mi>C</mml:mi><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo>-</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mi>a</mml:mi></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf153"><mml:mi>C</mml:mi></mml:math></inline-formula> is the capacitance of the membrane of the axon-proximal compartment, <inline-formula><mml:math id="inf154"><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> is the leak conductance, <inline-formula><mml:math id="inf155"><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:math></inline-formula> is the conductance of the coupling from axon-distal to axon-proximal dendrites, <inline-formula><mml:math id="inf156"><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is a vector of visual input currents to the axon-proximal compartment of HD cells, <inline-formula><mml:math id="inf157"><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> is an excitatory input to the axon-proximal compartment, and <inline-formula><mml:math id="inf158"><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msup></mml:math></inline-formula> is a random noise vector injected to the axon-proximal compartment, drawn IID from <inline-formula><mml:math id="inf159"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. The excitatory current <inline-formula><mml:math id="inf160"><mml:msubsup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> is assumed to be present only in light conditions. The values of <inline-formula><mml:math id="inf161"><mml:mi>C</mml:mi></mml:math></inline-formula>, <inline-formula><mml:math id="inf162"><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula>, and <inline-formula><mml:math id="inf163"><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:math></inline-formula> in the fly HD (E-PG) neurons are unknown, thus we keep these parameters unitless, and set their values to the ones in <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>. Note that since conductances are dimensionless here, <inline-formula><mml:math id="inf164"><mml:mi>C</mml:mi></mml:math></inline-formula> is effectively a time constant.</p><p>Following <xref ref-type="bibr" rid="bib32">Hahnloser, 2003</xref>, the visual input to the i-th HD cell is a localized bump of activity at angular location <inline-formula><mml:math id="inf165"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>:<disp-formula id="equ5"><label>(5)</label><mml:math id="m5"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>M</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>σ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msup><mml:mi>sin</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf166"><mml:mi>M</mml:mi></mml:math></inline-formula> scales the bump’s amplitude, <inline-formula><mml:math id="inf167"><mml:mi>σ</mml:mi></mml:math></inline-formula> controls the width of the bump, <inline-formula><mml:math id="inf168"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> is the preferred orientation of the i-th HD neuron , <inline-formula><mml:math id="inf169"><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the position of a visual landmark at time <inline-formula><mml:math id="inf170"><mml:mi>t</mml:mi></mml:math></inline-formula> in head-centered coordinates, and <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>o</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>&lt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is a constant inhibitory current that acts as the baseline for the visual input. We choose <inline-formula><mml:math id="inf172"><mml:mi>M</mml:mi></mml:math></inline-formula> so that the visual input can induce a weak bump in the network at the beginning of learning, and we choose <inline-formula><mml:math id="inf173"><mml:mi>σ</mml:mi></mml:math></inline-formula> so that the resulting bump after learning is ∼60 deg wide. Note that the bump in the mature network has a square shape (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref>); therefore we elect to make it slightly narrower than the average full width at half maximum of the experimentally observed bump (∼80 deg; <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>; <xref ref-type="bibr" rid="bib36">Kim et al., 2017</xref>; <xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>). In addition, the current <inline-formula><mml:math id="inf174"><mml:msubsup><mml:mi>I</mml:mi><mml:mi>o</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> is negative enough to make the visual input purely inhibitory, as reported (<xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>). The visual input is more inhibitory in the surround to suppress activity outside of the HD receptive field. Therefore, the mechanism in which the visual input acts on the HD neurons is disinhibition.</p><p>The firing rate of HD cells, which is set by the voltage in the axon-proximal compartment, is given by<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>-</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>is a sigmoidal activation function applied element-wise to the vector <inline-formula><mml:math id="inf175"><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msup></mml:math></inline-formula>. The variable <inline-formula><mml:math id="inf176"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> sets the maximum firing rate of the neuron, <inline-formula><mml:math id="inf177"><mml:mi>β</mml:mi></mml:math></inline-formula> is the slope of the activation function, and <inline-formula><mml:math id="inf178"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is the input level at which half of the maximum firing rate is attained. The value of <inline-formula><mml:math id="inf179"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is arbitrary, while <inline-formula><mml:math id="inf180"><mml:mi>β</mml:mi></mml:math></inline-formula> is chosen such that the activation function has sufficient dynamic range, and <inline-formula><mml:math id="inf181"><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math></inline-formula> is chosen such that for small negative inputs the activation function is non-zero.</p><p>We note that the saturation of the activation function <inline-formula><mml:math id="inf182"><mml:mi>f</mml:mi></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref> is an essential feature of our model, especially for the convergence of the plasticity rule in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>; see also the section ‘Synaptic Plasticity Rule’. Even though, to the best of our knowledge, it is currently not known whether E-PG neurons actually reach saturation, other <italic>Drosophila</italic> neurons are known to reach saturation with increasing inputs, instead of some sort of depolarization block (<xref ref-type="bibr" rid="bib76">Wilson, 2013</xref>; <xref ref-type="bibr" rid="bib6">Brandão et al., 2021</xref>). Saturation with increasing inputs may be due to, for instance, short-term synaptic depression: beyond a certain frequency of incoming action potentials, the synaptic input current is almost independent of that frequency (<xref ref-type="bibr" rid="bib66">Tsodyks and Markram, 1997</xref>; <xref ref-type="bibr" rid="bib67">Tsodyks et al., 1998</xref>).</p><p>The firing rates of the HR cells are given by<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:msup><mml:mi>r</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mtext>LP</mml:mtext><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>n</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf183"><mml:msup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> is the vector of length <inline-formula><mml:math id="inf184"><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> of firing rates of HR cells, the <inline-formula><mml:math id="inf185"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>×</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> matrix <inline-formula><mml:math id="inf186"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> encodes the fixed connections from the HD to the HR cells, <inline-formula><mml:math id="inf187"><mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mtext>LP</mml:mtext><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> is a low-pass filtered version of the firing rate of the HD cells where the filter accounts for delays due to synaptic transmission in the incoming synapses from HD cells, <inline-formula><mml:math id="inf188"><mml:msup><mml:mi mathvariant="bold-italic">I</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is the angular velocity input, <inline-formula><mml:math id="inf189"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:math></inline-formula> is a constant inhibitory input common to all HR cells, and <inline-formula><mml:math id="inf190"><mml:msup><mml:mi mathvariant="bold-italic">n</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> is a random noise input to the HR cells drawn IID from <inline-formula><mml:math id="inf191"><mml:mrow><mml:mi>N</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. We set <inline-formula><mml:math id="inf192"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:math></inline-formula> to a value that still allows sufficient activity in the HR cell bump, even when the animal does not move. The low-pass filtered firing-rate vector <inline-formula><mml:math id="inf193"><mml:msubsup><mml:mi mathvariant="bold-italic">r</mml:mi><mml:mtext>LP</mml:mtext><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> is given by<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mtext>LP</mml:mtext><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msubsup><mml:mi>r</mml:mi><mml:mtext>LP</mml:mtext><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mi>r</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and the angular-velocity input to the i-th HR neuron is given by<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>q</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>k</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="1em"/><mml:mtext>with</mml:mtext><mml:mspace width="1em"/><mml:mi>q</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="left left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>for </mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>≤</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">HR</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mn>1</mml:mn></mml:mtd><mml:mtd><mml:mrow><mml:mtext>for </mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>&gt;</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mrow><mml:mrow><mml:mtext mathvariant="italic">HR</mml:mtext></mml:mrow></mml:mrow></mml:msup><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:mrow></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf194"><mml:mi>k</mml:mi></mml:math></inline-formula> is the proportionality constant between head angular velocity and velocity input to the network, <inline-formula><mml:math id="inf195"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the head angular velocity at time <inline-formula><mml:math id="inf196"><mml:mi>t</mml:mi></mml:math></inline-formula> in units of deg/s, and the factor <inline-formula><mml:math id="inf197"><mml:mi>q</mml:mi></mml:math></inline-formula> is chosen such that the left (right) half of the HR cells are primarily active during leftward (rightward) head rotation. Note that the same <inline-formula><mml:math id="inf198"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is in both <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and <xref ref-type="disp-formula" rid="equ9">Equation 9</xref>. Finally, as mentioned earlier, the matrix <inline-formula><mml:math id="inf199"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> encodes the hardwired 1-to-1 HD-to-HR connections, i.e., <inline-formula><mml:math id="inf200"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow></mml:math></inline-formula> if the j-th HD neuron projects to the i-th HR neuron, and <inline-formula><mml:math id="inf201"><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise. Specifically, for <inline-formula><mml:math id="inf202"><mml:mi>j</mml:mi></mml:math></inline-formula> odd, HD neuron <inline-formula><mml:math id="inf203"><mml:mi>j</mml:mi></mml:math></inline-formula> projects to L-HR neuron <inline-formula><mml:math id="inf204"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>j</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:math></inline-formula>, whereas for <inline-formula><mml:math id="inf205"><mml:mi>j</mml:mi></mml:math></inline-formula> even, HD neuron <inline-formula><mml:math id="inf206"><mml:mi>j</mml:mi></mml:math></inline-formula> projects to R-HR neuron <inline-formula><mml:math id="inf207"><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mn>30</mml:mn><mml:mo>+</mml:mo><mml:mfrac><mml:mi>j</mml:mi><mml:mn>2</mml:mn></mml:mfrac></mml:mrow></mml:mrow></mml:math></inline-formula>. The synaptic strength <inline-formula><mml:math id="inf208"><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> is chosen such that the range of the firing rates of the HD cells is mapped to the entire range of firing rates of the HR cells. Specifically, we set <inline-formula><mml:math id="inf209"><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:mfrac></mml:mrow></mml:math></inline-formula>, where <inline-formula><mml:math id="inf210"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the range of inputs for which <inline-formula><mml:math id="inf211"><mml:mi>f</mml:mi></mml:math></inline-formula> has not saturated, i.e., the input values for which <inline-formula><mml:math id="inf212"><mml:mi>f</mml:mi></mml:math></inline-formula> remains between about 7% and 93% of its maximum firing rate <inline-formula><mml:math id="inf213"><mml:msub><mml:mi>f</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (see <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>). Finally, the proportionality constant <inline-formula><mml:math id="inf214"><mml:mi>k</mml:mi></mml:math></inline-formula> is set so that the firing rate of HR neurons does not reach saturation for the range of velocities relevant for the fly (approx. [-500, 500] deg/s), given all other inputs they receive.</p></sec><sec id="s4-3"><title>Synaptic plasticity rule</title><p>In our network, the associative HD neurons receive direct visual input in the axon-proximal compartment and indirect angular velocity input in the axon-distal compartment through the HR-to-HD connections (<xref ref-type="fig" rid="fig1">Figure 1D</xref>). We hypothesize that the visual input acts as a supervisory signal that controls the axon-proximal voltage <inline-formula><mml:math id="inf215"><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msup></mml:math></inline-formula> directly, and the latter initiates spikes. Therefore, the goal of learning is for the axon-distal voltage <inline-formula><mml:math id="inf216"><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi>d</mml:mi></mml:msup></mml:math></inline-formula> to predict the axon-proximal voltage by changing the synaptic weights <inline-formula><mml:math id="inf217"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf218"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>. This change is achieved by minimizing the difference between the firing rate <inline-formula><mml:math id="inf219"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mi mathvariant="bold-italic">a</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the presence of visual input and the axon-distal prediction <inline-formula><mml:math id="inf220"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi mathvariant="bold-italic">V</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the firing rate in the absence of visual input. In the latter case and at steady-state, the voltage <inline-formula><mml:math id="inf221"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> for the i-th HD neuron is an attenuated version of the axon-distal voltage,<disp-formula id="equ11"><label>(11)</label><mml:math id="m11"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>with conductance <inline-formula><mml:math id="inf222"><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub></mml:math></inline-formula> of the coupling from the axon-distal to axon-proximal dendrites and leak conductance <inline-formula><mml:math id="inf223"><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:math></inline-formula> of the axon-proximal compartment, as explained in <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, and <inline-formula><mml:math id="inf224"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref>. Therefore, following <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>, we define the plasticity-induction variable <inline-formula><mml:math id="inf225"><mml:msub><mml:mtext>PI</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> for the connection between the j-th presynaptic neuron and i-th postsynaptic neuron as<disp-formula id="equ12"><label>(12)</label><mml:math id="m12"><mml:mrow><mml:msub><mml:mtext>PI</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mpadded lspace="1.7pt" width="+1.7pt"><mml:mi>f</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mtext>P</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf226"><mml:msub><mml:mtext>P</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> is the postsynaptic potential of neuron <inline-formula><mml:math id="inf227"><mml:mi>j</mml:mi></mml:math></inline-formula>, which is a low-pass filtered version of the presynaptic firing rate <italic>r</italic><sub><italic>j</italic></sub>. That is,<disp-formula id="equ13"><label>(13)</label><mml:math id="m13"><mml:mrow><mml:mrow><mml:msub><mml:mtext>P</mml:mtext><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∗</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where * denotes convolution. The transfer function<disp-formula id="equ14"><label>(14)</label><mml:math id="m14"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mfrac><mml:mi>t</mml:mi><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo rspace="4.2pt">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is derived from the filtering dynamics in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> and <xref ref-type="disp-formula" rid="equ3">Equation 3</xref> and accounts for the delays introduced by the synaptic time constant <inline-formula><mml:math id="inf228"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and the leak time constant <inline-formula><mml:math id="inf229"><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula>. In <xref ref-type="disp-formula" rid="equ14">Equation 14</xref>, <inline-formula><mml:math id="inf230"><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the Heaviside step function, that is, <inline-formula><mml:math id="inf231"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf232"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf233"><mml:mrow><mml:mrow><mml:mi>u</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> otherwise. The plasticity-induction variable is then low-pass filtered to account for slow learning dynamics,<disp-formula id="equ15"><label>(15)</label><mml:math id="m15"><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msub><mml:mtext>PI</mml:mtext><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mpadded></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and the final weight change is given by<disp-formula id="equ16"><label>(16)</label><mml:math id="m16"><mml:mrow><mml:mfrac><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mtext>d</mml:mtext><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>=</mml:mo><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf234"><mml:mi>η</mml:mi></mml:math></inline-formula> is the learning rate and <inline-formula><mml:math id="inf235"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is the connection weight from the j-th presynaptic neuron to the i-th postsynaptic neuron. Note that the synaptic weight <inline-formula><mml:math id="inf236"><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> is an element of either the matrix <inline-formula><mml:math id="inf237"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> or the matrix <inline-formula><mml:math id="inf238"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> depending on whether the presynaptic neuron <inline-formula><mml:math id="inf239"><mml:mi>j</mml:mi></mml:math></inline-formula> is an HD or an HR neuron, respectively. The value of the plasticity time constant <inline-formula><mml:math id="inf240"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula> is not known, therefore we adopt the value suggested by <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>.</p><p><xref ref-type="disp-formula" rid="equ12">Equation 12</xref> is a ‘delta-like’ rule that can be interpreted as an extension of the Hebbian rule; compared to a generic Hebbian rule, we have replaced the postsynaptic firing rate <inline-formula><mml:math id="inf241"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> by the difference between <inline-formula><mml:math id="inf242"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> and the predicted firing rate <inline-formula><mml:math id="inf243"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of the axon-distal compartment of the postsynaptic neuron. This difference drives plasticity in the model. We note that <inline-formula><mml:math id="inf244"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is a continuous approximation of the spike train of the postsynaptic neuron, which could be available at the axon-distal compartment via back-propagating action potentials (<xref ref-type="bibr" rid="bib42">Larkum, 2013</xref>). Furthermore, the axon-distal voltage <inline-formula><mml:math id="inf245"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:math></inline-formula> and postsynaptic potentials are by definition available at the synapses arriving at the axon-distal compartment. Note that even though <inline-formula><mml:math id="inf246"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is the firing rate in the absence of visual input, it can still be computed at the axon-distal compartment when the visual input is available; <inline-formula><mml:math id="inf247"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>d</mml:mi></mml:msubsup></mml:math></inline-formula> is the local voltage and therefore only a constant multiplicative factor (<xref ref-type="disp-formula" rid="equ11">Equation 11</xref>) and the static nonlinearity <inline-formula><mml:math id="inf248"><mml:mi>f</mml:mi></mml:math></inline-formula> need to be computed to obtain <inline-formula><mml:math id="inf249"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. Therefore, the learning rule is biologically plausible because all information is locally available at the synapse.</p><p>The learning rule used here differs from the one in the original work of <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref> because we utilize a rate-based version instead of the original spike-based version. Even though spike trains can introduce Poisson noise to <inline-formula><mml:math id="inf250"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref> show that once learning has converged, asymmetries in the weights due the spiking noise are on average canceled out.</p><p>Another difference in our learning setup is that, unlike in <xref ref-type="bibr" rid="bib71">Urbanczik and Senn, 2014</xref>, the input to the axon-proximal compartment does not reach zero in equilibrium (see, e.g. <xref ref-type="fig" rid="fig3">Figure 3D</xref>, and Appendix 5). Therefore, an activation function with a saturating non-linearity, as in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>, is crucial for convergence, which could not be achieved with a less biologically plausible threshold-linear activation function. This lack of strict convergence in our setup is responsible for the square form of the bump (<xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2B</xref> and Appendix 5).</p></sec><sec id="s4-4"><title>Training protocol</title><p>We train the network with synthetically generated angular velocities, simulating head turns of the animal. <inline-formula><mml:math id="inf251"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf252"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> are both initialized with random connectivity drawn from a normal distribution with mean 0 and standard deviation <inline-formula><mml:math id="inf253"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:msqrt><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:msqrt></mml:mrow></mml:math></inline-formula>, as common practise in the modeling literature. In further simulations with various other initial conditions (e.g. in the simulations with gain changes in <xref ref-type="fig" rid="fig4">Figure 4</xref> or in simulations in which we randomly shuffled weights after learning, not shown), we confirmed that the final PI performance is virtually independent of the initial distribution of weights <inline-formula><mml:math id="inf254"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf255"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>.</p><p>The network dynamics are updated in discrete time steps <inline-formula><mml:math id="inf256"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:math></inline-formula> using forward Euler integration. The entrained angular velocities cover the range of angular velocities exhibited by the fly, which are at maximum ∼500 deg/s during walking or flying (<xref ref-type="bibr" rid="bib24">Geurten et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Stowers et al., 2017</xref>). The angular velocity <inline-formula><mml:math id="inf257"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is modeled as an Ornstein-Uhlenbeck process given by<disp-formula id="equ17"><label>(17)</label><mml:math id="m17"><mml:mrow><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>α</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mi>v</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:msqrt><mml:mi mathvariant="normal">Δ</mml:mi><mml:mi>t</mml:mi></mml:msqrt><mml:mspace width="thinmathspace"/><mml:mi>n</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf258"><mml:mrow><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf259"><mml:msub><mml:mi>τ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> is the time constant with which <inline-formula><mml:math id="inf260"><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> decays to zero, <inline-formula><mml:math id="inf261"><mml:mrow><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is noise drawn from a normal distribution with mean 0 and standard deviation 1 at each time step, and <inline-formula><mml:math id="inf262"><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> scales the noise strength.</p><p>We pick <inline-formula><mml:math id="inf263"><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf264"><mml:msub><mml:mi>τ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula> so that the resulting angular velocity distribution in <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2C</xref> and its time course, for example in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, are similar to what has been reported in flies during walking or flying (<xref ref-type="bibr" rid="bib24">Geurten et al., 2014</xref>; <xref ref-type="bibr" rid="bib63">Stowers et al., 2017</xref>). Finally, note that we train the network for angular velocities a little larger than what flies typically display (up to ±720 deg/s).</p></sec><sec id="s4-5"><title>Quantification of the mean learning error</title><p>In <xref ref-type="disp-formula" rid="equ12">Equation 12</xref> we have used the learning error<disp-formula id="equ18"><label>(18)</label><mml:math id="m18"><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>which controls learning in the i-th associative HD neuron. To quantify the mean learning error <inline-formula><mml:math id="inf265"><mml:mrow><mml:mtext>err</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> in the whole network at time <inline-formula><mml:math id="inf266"><mml:mi>t</mml:mi></mml:math></inline-formula>, we average <inline-formula><mml:math id="inf267"><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> across all HD neurons and across a small time interval <inline-formula><mml:math id="inf268"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>t</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula>, that is,<disp-formula id="equ19"><label>(19)</label><mml:math id="m19"><mml:mrow><mml:mrow><mml:mtext>err</mml:mtext><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:munderover><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mi>t</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>w</mml:mi></mml:msub></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo fence="true" stretchy="false">|</mml:mo><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo fence="true" rspace="4.2pt" stretchy="false">|</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mi>τ</mml:mi></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with <inline-formula><mml:math id="inf269"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mi>w</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> s. In <xref ref-type="fig" rid="fig3">Figure 3D</xref>, we plot this mean error at every 1% of the simulation, for 12 simulations, and averaged across the ensemble of the simulations. Note that individual simulations occasionally display ‘spikes’ in the error. Large errors occur if the network happens to be driven by very high velocities that the network does not learn very well because they are rare; larger errors also occur for very small velocities, that is, when the velocity input is not strong enough to overcome the local attractor dynamics, as seen, for example, in <xref ref-type="fig" rid="fig2">Figure 2C</xref>. On average, though, we can clearly see that the mean learning error decreases with increasing time and settles to a small value (e.g. <xref ref-type="fig" rid="fig3">Figure 3D</xref> and <xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p></sec><sec id="s4-6"><title>Population vector average</title><p>To decode from the activity of HD neurons an average HD encoded by the network, we use the population vector average (PVA). We thus first convert the tuning direction <inline-formula><mml:math id="inf270"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> of the i-th HD neuron to the corresponding complex number <inline-formula><mml:math id="inf271"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:math></inline-formula> on the unitary circle, where <inline-formula><mml:math id="inf272"><mml:mi>j</mml:mi></mml:math></inline-formula> is the imaginary unit. This complex number is multiplied by the firing rate <inline-formula><mml:math id="inf273"><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> of the i-th HD neuron, and then averaged across neurons to yield the PVA<disp-formula id="equ20"><label>(20)</label><mml:math id="m20"><mml:mrow><mml:mrow><mml:msub><mml:mi>r</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:munderover><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:munderover><mml:mrow><mml:msubsup><mml:mi>r</mml:mi><mml:mi>i</mml:mi><mml:mtext>HD</mml:mtext></mml:msubsup><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msup><mml:mi>e</mml:mi><mml:mrow><mml:mi>j</mml:mi><mml:mo>⁢</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:msup></mml:mpadded></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The PVA is a vector in the 2-D complex plane and points to the center of mass of activity in the HD network. Finally, we take the angle <inline-formula><mml:math id="inf274"><mml:mi>θ</mml:mi></mml:math></inline-formula> of the PVA as a measure for the current heading direction represented by the network.</p></sec><sec id="s4-7"><title>Diffusion coefficient</title><p>To quantify the variability of heading direction in the trained networks, we define the diffusion coefficient <inline-formula><mml:math id="inf275"><mml:mi>D</mml:mi></mml:math></inline-formula> as:<disp-formula id="equ21"><label>(21)</label><mml:math id="m21"><mml:mrow><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mrow><mml:mo>⟨</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>⟩</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>sim</mml:mtext></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf276"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>θ</mml:mi></mml:mrow></mml:math></inline-formula> is the change in heading direction in a time interval <inline-formula><mml:math id="inf277"><mml:msub><mml:mi>t</mml:mi><mml:mtext>sim</mml:mtext></mml:msub></mml:math></inline-formula>. Therefore, <inline-formula><mml:math id="inf278"><mml:mi>D</mml:mi></mml:math></inline-formula> is given by the variance of the distribution of displacements in a given time interval, divided by the time interval.</p><p>In the main text, we estimate <inline-formula><mml:math id="inf279"><mml:mi>D</mml:mi></mml:math></inline-formula> during PI, i.e. with velocity inputs only. In this setting, <inline-formula><mml:math id="inf280"><mml:mi>D</mml:mi></mml:math></inline-formula> is the rate at which the variance of the PI errors increases (see e.g. <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Deviations from gain-1 PI contribute to this estimate; hence, to single out the effects of noise during training on the stability of the learned attractor in Appendix 1, we also estimate <inline-formula><mml:math id="inf281"><mml:mi>D</mml:mi></mml:math></inline-formula> in the presence of test noise when no inputs are received at all.</p></sec><sec id="s4-8"><title>Fly connectome analysis</title><p>Our model assumes the segregation of visual inputs to HD (E-PG) cells from head rotation and recurrent inputs to the same cells. To test this hypothesis, we leverage on the fly hemibrain connectome (<xref ref-type="bibr" rid="bib79">Xu, 2020</xref>; <xref ref-type="bibr" rid="bib12">Clements et al., 2020</xref>). First, we randomly choose one E-PG neuron per wedge of the EB, for a total of 16 E-PG neurons. We reasoned this sample would be sufficient because the way E-PG neurons in the same wedge are innervated is expected to be similar. We then find all incoming connections to these neurons from visually responsive ring neurons R2 and R4d (<xref ref-type="bibr" rid="bib48">Omoto et al., 2017</xref>; <xref ref-type="bibr" rid="bib20">Fisher et al., 2019</xref>). These are the connections that arrive at the axon-proximal compartment in our model. We then find all incoming connections from P-EN1 cells, which correspond to the HR neurons, and from P-EN2 cells, which are involved in a recurrent excitatory loop from E-PG to P-EG to P-EN2 and back to E-PG (<xref ref-type="bibr" rid="bib69">Turner-Evans et al., 2020</xref>). These are the connections that arrive at the axon-distal compartment in our model.</p><p>To further support the assumption that visual inputs are separated from recurrent and HR-to-HD inputs in the <italic>Drosophila</italic> EB, we perform binary classification between the two classes (R2 and R4d vs. P-EN1 and P-EN2). We use SVMs with Gaussian kernel, and perform nested 5-fold cross validation, for a total of 30 model runs for every neuron tested (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>).</p></sec><sec id="s4-9"><title>Quantification of PI performance</title><p>To quantify PI performance of the network and compare to fly performance, we use the measure defined by <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref> and estimate the correlation coefficient between the unwrapped PVA and true heading in darkness. We estimate the correlation in 140 s long trials and report the point estimate and 95% confidence intervals (Student’s t-test, <inline-formula><mml:math id="inf282"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>100</mml:mn></mml:mrow></mml:math></inline-formula>).</p></sec><sec id="s4-10"><title>Resource availability</title><p>All code used in this work is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/panvaf/LearnPI">https://github.com/panvaf/LearnPI</ext-link> (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:94623fb678bde9a80cec006ae68b36b0da4b13f7;origin=https://github.com/panvaf/LearnPI;visit=swh:1:snp:d2bf616300e0b9b6efa575c1be2040dd9e7ee4dc;anchor=swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315">swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315</ext-link>, <xref ref-type="bibr" rid="bib73">Vafidis, 2022</xref>). The files required to reproduce the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/pavaf/LearnPI">https://gin.g-node.org/pavaf/LearnPI</ext-link>.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceived the study, Performed analyses, Wrote the initial draft of the manuscript, Wrote the manuscript</p></fn><fn fn-type="con" id="con2"><p>Supervised the research, Wrote the manuscript</p></fn><fn fn-type="con" id="con3"><p>Conceived the study, Contributed to analyses, Supervised the research, Wrote the manuscript</p></fn><fn fn-type="con" id="con4"><p>Conceived the study, Supervised the research, Wrote the manuscript</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="transrepform"><label>Transparent reporting form</label><media xlink:href="elife-69841-transrepform1-v2.pdf" mimetype="application" mime-subtype="pdf"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All code used in this work is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/panvaf/LearnPI">https://github.com/panvaf/LearnPI</ext-link>, (copy archived at <ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:94623fb678bde9a80cec006ae68b36b0da4b13f7;origin=https://github.com/panvaf/LearnPI;visit=swh:1:snp:d2bf616300e0b9b6efa575c1be2040dd9e7ee4dc;anchor=swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315">swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315</ext-link>). The files required to reproduce the figures can be found at <ext-link ext-link-type="uri" xlink:href="https://gin.g-node.org/pavaf/LearnPI">https://gin.g-node.org/pavaf/LearnPI</ext-link>.</p><p>The following previously published dataset was used:</p><p><element-citation publication-type="data" specific-use="references" id="dataset1"><person-group person-group-type="author"><name><surname>Cs</surname><given-names>Xu</given-names></name><name><surname>Januszewski</surname><given-names>M</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>ya Takemura</surname><given-names>S</given-names></name><name><surname>Hayworth</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2020">2020</year><data-title>A Connectome of the Adult Drosophila Central Brain</data-title><source>neuPrint</source><pub-id pub-id-type="accession" xlink:href="https://elifesciences.org/articles/57443">57443</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Raquel Suárez-Grimalt and Marcel Heim for helpful discussions and Louis Kang for comments on the manuscript. This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation; SFB 1315 – project-ID 327654276 to RK and DO; and the Emmy Noether Programme 282979116 to DO and Germany´s Excellence Strategy – EXC-2049 – 390688087 to DO), the German Federal Ministry for Education and Research (BMBF; Grant 01GQ1705 to RK), and the Onassis Foundation (PV). The funding sources were not involved in study design, data collection and interpretation, or the decision to submit the work for publication.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Callaway</surname><given-names>EM</given-names></name><name><surname>Denk</surname><given-names>W</given-names></name><name><surname>Dulac</surname><given-names>C</given-names></name><name><surname>Fairhall</surname><given-names>AL</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name><name><surname>Harris</surname><given-names>KM</given-names></name><name><surname>Helmstaedter</surname><given-names>M</given-names></name><name><surname>Jain</surname><given-names>V</given-names></name><name><surname>Kasthuri</surname><given-names>N</given-names></name><name><surname>LeCun</surname><given-names>Y</given-names></name><name><surname>Lichtman</surname><given-names>JW</given-names></name><name><surname>Littlewood</surname><given-names>PB</given-names></name><name><surname>Luo</surname><given-names>L</given-names></name><name><surname>Maunsell</surname><given-names>JHR</given-names></name><name><surname>Reid</surname><given-names>RC</given-names></name><name><surname>Rosen</surname><given-names>BR</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Sejnowski</surname><given-names>TJ</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name><name><surname>Svoboda</surname><given-names>K</given-names></name><name><surname>Tank</surname><given-names>DW</given-names></name><name><surname>Tsao</surname><given-names>D</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The mind of a mouse</article-title><source>Cell</source><volume>182</volume><fpage>1372</fpage><lpage>1376</lpage><pub-id pub-id-type="doi">10.1016/j.cell.2020.08.010</pub-id><pub-id pub-id-type="pmid">32946777</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Amari</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1977">1977</year><article-title>Dynamics of pattern formation in lateral-inhibition type neural fields</article-title><source>Biological Cybernetics</source><volume>27</volume><fpage>77</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1007/BF00337259</pub-id><pub-id pub-id-type="pmid">911931</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Mirowski</surname><given-names>P</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Chadwick</surname><given-names>MJ</given-names></name><name><surname>Degris</surname><given-names>T</given-names></name><name><surname>Modayil</surname><given-names>J</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Soyer</surname><given-names>H</given-names></name><name><surname>Viola</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Goroshin</surname><given-names>R</given-names></name><name><surname>Rabinowitz</surname><given-names>N</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Beattie</surname><given-names>C</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Sadik</surname><given-names>A</given-names></name><name><surname>Gaffney</surname><given-names>S</given-names></name><name><surname>King</surname><given-names>H</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Hadsell</surname><given-names>R</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vector-based navigation using grid-like representations in artificial agents</article-title><source>Nature</source><volume>557</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0102-6</pub-id><pub-id pub-id-type="pmid">29743670</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bell</surname><given-names>CC</given-names></name><name><surname>Han</surname><given-names>VZ</given-names></name><name><surname>Sugawara</surname><given-names>Y</given-names></name><name><surname>Grant</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Synaptic plasticity in a cerebellum-like structure depends on temporal order</article-title><source>Nature</source><volume>387</volume><fpage>278</fpage><lpage>281</lpage><pub-id pub-id-type="doi">10.1038/387278a0</pub-id><pub-id pub-id-type="pmid">9153391</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ben-Yishai</surname><given-names>R</given-names></name><name><surname>Bar-Or</surname><given-names>RL</given-names></name><name><surname>Sompolinsky</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Theory of orientation tuning in visual cortex</article-title><source>PNAS</source><volume>92</volume><fpage>3844</fpage><lpage>3848</lpage><pub-id pub-id-type="doi">10.1073/pnas.92.9.3844</pub-id><pub-id pub-id-type="pmid">7731993</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brandão</surname><given-names>SC</given-names></name><name><surname>Silies</surname><given-names>M</given-names></name><name><surname>Martelli</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Adaptive temporal processing of odor stimuli</article-title><source>Cell and Tissue Research</source><volume>383</volume><fpage>125</fpage><lpage>141</lpage><pub-id pub-id-type="doi">10.1007/s00441-020-03400-9</pub-id><pub-id pub-id-type="pmid">33404843</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brea</surname><given-names>J</given-names></name><name><surname>Gaál</surname><given-names>AT</given-names></name><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Prospective coding by spiking neurons</article-title><source>PLOS Computational Biology</source><volume>12</volume><elocation-id>e1005003</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1005003</pub-id><pub-id pub-id-type="pmid">27341100</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate path integration in continuous attractor network models of grid cells</article-title><source>PLOS Computational Biology</source><volume>5</volume><elocation-id>e1000291</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000291</pub-id><pub-id pub-id-type="pmid">19229307</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Fiete</surname><given-names>IR</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Fundamental limits on persistent activity in networks of noisy neurons</article-title><source>PNAS</source><volume>109</volume><fpage>17645</fpage><lpage>17650</lpage><pub-id pub-id-type="doi">10.1073/pnas.1117386109</pub-id><pub-id pub-id-type="pmid">23047704</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Carpenter</surname><given-names>GA</given-names></name><name><surname>Grossberg</surname><given-names>S</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>A massively parallel architecture for A self-organizing neural pattern recognition machine</article-title><source>Computer Vision, Graphics, and Image Processing</source><volume>37</volume><fpage>54</fpage><lpage>115</lpage><pub-id pub-id-type="doi">10.1016/S0734-189X(87)80014-2</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chaudhuri</surname><given-names>R</given-names></name><name><surname>Gerçek</surname><given-names>B</given-names></name><name><surname>Pandey</surname><given-names>B</given-names></name><name><surname>Peyrache</surname><given-names>A</given-names></name><name><surname>Fiete</surname><given-names>I</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1512</fpage><lpage>1520</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0460-x</pub-id><pub-id pub-id-type="pmid">31406365</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Clements</surname><given-names>J</given-names></name><name><surname>Dolafi</surname><given-names>T</given-names></name><name><surname>Umayam</surname><given-names>L</given-names></name><name><surname>Neubarth</surname><given-names>NL</given-names></name><name><surname>Berg</surname><given-names>S</given-names></name><name><surname>Scheffer</surname><given-names>LK</given-names></name><name><surname>Plaza</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Neu Print: Analysis Tools for EM Connectomics</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.16.909465</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Cueva</surname><given-names>CJ</given-names></name><name><surname>Wei</surname><given-names>XX</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Emergence of Grid-like Representations by Training Recurrent Neural Networks to Perform Spatial Localization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1803.07770">https://arxiv.org/abs/1803.07770</ext-link></element-citation></ref><ref id="bib14"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Darshan</surname><given-names>R</given-names></name><name><surname>Rivkind</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Learning to Represent Continuous Variables in Heterogeneous Neural Networks</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.06.01.446635</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Darwin</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1873">1873</year><article-title>Origin of Certain Instincts</article-title><source>Nature</source><volume>7</volume><fpage>417</fpage><lpage>418</lpage><pub-id pub-id-type="doi">10.1038/007417a0</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doron</surname><given-names>G</given-names></name><name><surname>Shin</surname><given-names>JN</given-names></name><name><surname>Takahashi</surname><given-names>N</given-names></name><name><surname>Drüke</surname><given-names>M</given-names></name><name><surname>Bocklisch</surname><given-names>C</given-names></name><name><surname>Skenderi</surname><given-names>S</given-names></name><name><surname>de Mont</surname><given-names>L</given-names></name><name><surname>Toumazou</surname><given-names>M</given-names></name><name><surname>Ledderose</surname><given-names>J</given-names></name><name><surname>Brecht</surname><given-names>M</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Perirhinal input to neocortical layer 1 controls learning</article-title><source>Science</source><volume>370</volume><elocation-id>eaaz3136</elocation-id><pub-id pub-id-type="doi">10.1126/science.aaz3136</pub-id><pub-id pub-id-type="pmid">33335033</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>D’Albis</surname><given-names>T</given-names></name><name><surname>Kempter</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Recurrent amplification of grid-cell activity</article-title><source>Hippocampus</source><volume>30</volume><fpage>1268</fpage><lpage>1297</lpage><pub-id pub-id-type="doi">10.1002/hipo.23254</pub-id><pub-id pub-id-type="pmid">33022854</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Eichenbaum</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>The role of the hippocampus in navigation is memory</article-title><source>Journal of Neurophysiology</source><volume>117</volume><fpage>1785</fpage><lpage>1796</lpage><pub-id pub-id-type="doi">10.1152/jn.00005.2017</pub-id><pub-id pub-id-type="pmid">28148640</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Etienne</surname><given-names>AS</given-names></name><name><surname>Maurer</surname><given-names>R</given-names></name><name><surname>Séguinot</surname><given-names>V</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Path integration in mammals and its interaction with visual landmarks</article-title><source>The Journal of Experimental Biology</source><volume>199</volume><fpage>201</fpage><lpage>209</lpage><pub-id pub-id-type="doi">10.1242/jeb.199.1.201</pub-id><pub-id pub-id-type="pmid">8576691</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fisher</surname><given-names>YE</given-names></name><name><surname>Lu</surname><given-names>J</given-names></name><name><surname>D’Alessandro</surname><given-names>I</given-names></name><name><surname>Wilson</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Sensorimotor experience remaps visual input to a heading-direction network</article-title><source>Nature</source><volume>576</volume><fpage>121</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1772-4</pub-id><pub-id pub-id-type="pmid">31748749</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Franconville</surname><given-names>R</given-names></name><name><surname>Beron</surname><given-names>C</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Building a functional connectome of the <italic>Drosophila</italic> central complex</article-title><source>eLife</source><volume>7</volume><elocation-id>e37017</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.37017</pub-id><pub-id pub-id-type="pmid">30124430</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Gallistel</surname><given-names>CR</given-names></name></person-group><year iso-8601-date="1993">1993</year><source>The Organization of Learning</source><publisher-name>Bradford Books/MIT Press</publisher-name></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gardner</surname><given-names>RJ</given-names></name><name><surname>Hermansen</surname><given-names>E</given-names></name><name><surname>Pachitariu</surname><given-names>M</given-names></name><name><surname>Burak</surname><given-names>Y</given-names></name><name><surname>Baas</surname><given-names>NA</given-names></name><name><surname>Dunn</surname><given-names>BA</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Toroidal topology of population activity in grid cells</article-title><source>Nature</source><volume>602</volume><fpage>123</fpage><lpage>128</lpage><pub-id pub-id-type="doi">10.1038/s41586-021-04268-7</pub-id><pub-id pub-id-type="pmid">35022611</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Geurten</surname><given-names>BRH</given-names></name><name><surname>Jähde</surname><given-names>P</given-names></name><name><surname>Corthals</surname><given-names>K</given-names></name><name><surname>Göpfert</surname><given-names>MC</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Saccadic body turns in walking <italic>Drosophila</italic></article-title><source>Frontiers in Behavioral Neuroscience</source><volume>8</volume><elocation-id>365</elocation-id><pub-id pub-id-type="doi">10.3389/fnbeh.2014.00365</pub-id><pub-id pub-id-type="pmid">25386124</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gidon</surname><given-names>A</given-names></name><name><surname>Zolnik</surname><given-names>TA</given-names></name><name><surname>Fidzinski</surname><given-names>P</given-names></name><name><surname>Bolduan</surname><given-names>F</given-names></name><name><surname>Papoutsi</surname><given-names>A</given-names></name><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Holtkamp</surname><given-names>M</given-names></name><name><surname>Vida</surname><given-names>I</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Dendritic action potentials and computation in human layer 2/3 cortical neurons</article-title><source>Science</source><volume>367</volume><fpage>83</fpage><lpage>87</lpage><pub-id pub-id-type="doi">10.1126/science.aax6239</pub-id><pub-id pub-id-type="pmid">31896716</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Goldman</surname><given-names>M</given-names></name><name><surname>Compte</surname><given-names>A</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2009">2009</year><chapter-title>Neural integrator models</chapter-title><person-group person-group-type="editor"><name><surname>Goldman</surname><given-names>M</given-names></name></person-group><source>In Encyclopedia of Neuroscience</source><publisher-name>Elsevier</publisher-name><fpage>165</fpage><lpage>178</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gouwens</surname><given-names>NW</given-names></name><name><surname>Wilson</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Signal propagation in <italic>Drosophila</italic> central neurons</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>6239</fpage><lpage>6249</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0764-09.2009</pub-id><pub-id pub-id-type="pmid">19439602</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>J.</given-names></name><name><surname>Adachi</surname><given-names>A</given-names></name><name><surname>Shah</surname><given-names>KK</given-names></name><name><surname>Hirokawa</surname><given-names>JD</given-names></name><name><surname>Magani</surname><given-names>PS</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A neural circuit architecture for angular integration in <italic>Drosophila</italic></article-title><source>Nature</source><volume>546</volume><fpage>101</fpage><lpage>106</lpage><pub-id pub-id-type="doi">10.1038/nature22343</pub-id><pub-id pub-id-type="pmid">28538731</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Green</surname><given-names>J</given-names></name><name><surname>Vijayan</surname><given-names>V</given-names></name><name><surname>Mussells Pires</surname><given-names>P</given-names></name><name><surname>Adachi</surname><given-names>A</given-names></name><name><surname>Maimon</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A neural heading estimate is compared with an internal goal to guide oriented navigation</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1460</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0444-x</pub-id><pub-id pub-id-type="pmid">31332373</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Lillicrap</surname><given-names>TP</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Towards deep learning with segregated dendrites</article-title><source>eLife</source><volume>6</volume><elocation-id>e22901</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.22901</pub-id><pub-id pub-id-type="pmid">29205151</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hafting</surname><given-names>T</given-names></name><name><surname>Fyhn</surname><given-names>M</given-names></name><name><surname>Molden</surname><given-names>S</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Microstructure of a spatial map in the entorhinal cortex</article-title><source>Nature</source><volume>436</volume><fpage>801</fpage><lpage>806</lpage><pub-id pub-id-type="doi">10.1038/nature03721</pub-id><pub-id pub-id-type="pmid">15965463</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hahnloser</surname><given-names>RHR</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Emergence of neural integration in the head-direction system by visual supervision</article-title><source>Neuroscience</source><volume>120</volume><fpage>877</fpage><lpage>891</lpage><pub-id pub-id-type="doi">10.1016/s0306-4522(03)00201-x</pub-id><pub-id pub-id-type="pmid">12895528</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Itti</surname><given-names>L</given-names></name><name><surname>Koch</surname><given-names>C</given-names></name><name><surname>Niebur</surname><given-names>E</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>A model of saliency-based visual attention for rapid scene analysis</article-title><source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source><volume>20</volume><fpage>1254</fpage><lpage>1259</lpage><pub-id pub-id-type="doi">10.1109/34.730558</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jayakumar</surname><given-names>RP</given-names></name><name><surname>Madhav</surname><given-names>MS</given-names></name><name><surname>Savelli</surname><given-names>F</given-names></name><name><surname>Blair</surname><given-names>HT</given-names></name><name><surname>Cowan</surname><given-names>NJ</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Recalibration of path integration in hippocampal place cells</article-title><source>Nature</source><volume>566</volume><fpage>533</fpage><lpage>537</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-0939-3</pub-id><pub-id pub-id-type="pmid">30742074</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kilpatrick</surname><given-names>ZP</given-names></name><name><surname>Ermentrout</surname><given-names>B</given-names></name><name><surname>Doiron</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Optimizing working memory with heterogeneity of recurrent cortical excitation</article-title><source>The Journal of Neuroscience</source><volume>33</volume><fpage>18999</fpage><lpage>19011</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1641-13.2013</pub-id><pub-id pub-id-type="pmid">24285904</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>SS</given-names></name><name><surname>Rouault</surname><given-names>H</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Ring attractor dynamics in the <italic>Drosophila</italic> central brain</article-title><source>SScience</source><volume>356</volume><fpage>849</fpage><lpage>853</lpage><pub-id pub-id-type="doi">10.1126/science.aal4835</pub-id><pub-id pub-id-type="pmid">28473639</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>SS</given-names></name><name><surname>Hermundstad</surname><given-names>AM</given-names></name><name><surname>Romani</surname><given-names>S</given-names></name><name><surname>Abbott</surname><given-names>LF</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Generation of stable heading representations in diverse visual scenes</article-title><source>Nature</source><volume>576</volume><fpage>126</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1038/s41586-019-1767-1</pub-id><pub-id pub-id-type="pmid">31748750</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Krause</surname><given-names>T</given-names></name><name><surname>Spindler</surname><given-names>L</given-names></name><name><surname>Poeck</surname><given-names>B</given-names></name><name><surname>Strauss</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title><italic>Drosophila</italic> acquires a long-lasting body-size memory from visual feedback</article-title><source>Current Biology</source><volume>29</volume><fpage>1833</fpage><lpage>1841</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.04.037</pub-id><pub-id pub-id-type="pmid">31104933</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lake</surname><given-names>BM</given-names></name><name><surname>Ullman</surname><given-names>TD</given-names></name><name><surname>Tenenbaum</surname><given-names>JB</given-names></name><name><surname>Gershman</surname><given-names>SJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Building machines that learn and think like people</article-title><source>The Behavioral and Brain Sciences</source><volume>40</volume><elocation-id>e253</elocation-id><pub-id pub-id-type="doi">10.1017/S0140525X16001837</pub-id><pub-id pub-id-type="pmid">27881212</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Langston</surname><given-names>RF</given-names></name><name><surname>Ainge</surname><given-names>JA</given-names></name><name><surname>Couey</surname><given-names>JJ</given-names></name><name><surname>Canto</surname><given-names>CB</given-names></name><name><surname>Bjerknes</surname><given-names>TL</given-names></name><name><surname>Witter</surname><given-names>MP</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Development of the spatial representation system in the rat</article-title><source>Science</source><volume>328</volume><fpage>1576</fpage><lpage>1580</lpage><pub-id pub-id-type="doi">10.1126/science.1188210</pub-id><pub-id pub-id-type="pmid">20558721</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname><given-names>ME</given-names></name><name><surname>Zhu</surname><given-names>JJ</given-names></name><name><surname>Sakmann</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>A new cellular mechanism for coupling inputs arriving at different cortical layers</article-title><source>Nature</source><volume>398</volume><fpage>338</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1038/18686</pub-id><pub-id pub-id-type="pmid">10192334</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Larkum</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>A cellular mechanism for cortical associations: an organizing principle for the cerebral cortex</article-title><source>Trends in Neurosciences</source><volume>36</volume><fpage>141</fpage><lpage>151</lpage><pub-id pub-id-type="doi">10.1016/j.tins.2012.11.006</pub-id><pub-id pub-id-type="pmid">23273272</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McNaughton</surname><given-names>BL</given-names></name><name><surname>Barnes</surname><given-names>CA</given-names></name><name><surname>Gerrard</surname><given-names>JL</given-names></name><name><surname>Gothard</surname><given-names>K</given-names></name><name><surname>Jung</surname><given-names>MW</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name><name><surname>Kudrimoti</surname><given-names>H</given-names></name><name><surname>Qin</surname><given-names>Y</given-names></name><name><surname>Skaggs</surname><given-names>WE</given-names></name><name><surname>Suster</surname><given-names>M</given-names></name><name><surname>Weaver</surname><given-names>KL</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Deciphering the hippocampal polyglot: the hippocampus as a path integration system</article-title><source>The Journal of Experimental Biology</source><volume>199</volume><fpage>173</fpage><lpage>185</lpage><pub-id pub-id-type="doi">10.1242/jeb.199.1.173</pub-id><pub-id pub-id-type="pmid">8576689</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mittelstaedt</surname><given-names>ML</given-names></name><name><surname>Mittelstaedt</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>Homing by path integration in a mammal</article-title><source>Die Naturwissenschaften</source><volume>67</volume><fpage>566</fpage><lpage>567</lpage><pub-id pub-id-type="doi">10.1007/BF00450672</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mizumori</surname><given-names>SJ</given-names></name><name><surname>Williams</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Directionally selective mnemonic properties of neurons in the lateral dorsal nucleus of the thalamus of rats</article-title><source>The Journal of Neuroscience</source><volume>13</volume><fpage>4015</fpage><lpage>4028</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.13-09-04015.1993</pub-id><pub-id pub-id-type="pmid">8366357</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Kropff</surname><given-names>E</given-names></name><name><surname>Moser</surname><given-names>MB</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Place cells, grid cells, and the brain’s spatial representation system</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>69</fpage><lpage>89</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.31.061307.090723</pub-id><pub-id pub-id-type="pmid">18284371</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Neuser</surname><given-names>K</given-names></name><name><surname>Triphan</surname><given-names>T</given-names></name><name><surname>Mronz</surname><given-names>M</given-names></name><name><surname>Poeck</surname><given-names>B</given-names></name><name><surname>Strauss</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Analysis of a spatial orientation memory in <italic>Drosophila</italic></article-title><source>Nature</source><volume>453</volume><fpage>1244</fpage><lpage>1247</lpage><pub-id pub-id-type="doi">10.1038/nature07003</pub-id><pub-id pub-id-type="pmid">18509336</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Omoto</surname><given-names>JJ</given-names></name><name><surname>Keleş</surname><given-names>MF</given-names></name><name><surname>Nguyen</surname><given-names>BCM</given-names></name><name><surname>Bolanos</surname><given-names>C</given-names></name><name><surname>Lovick</surname><given-names>JK</given-names></name><name><surname>Frye</surname><given-names>MA</given-names></name><name><surname>Hartenstein</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Visual input to the <italic>Drosophila</italic> central complex by developmentally and functionally distinct neuronal populations</article-title><source>Current Biology</source><volume>27</volume><fpage>1098</fpage><lpage>1110</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2017.02.063</pub-id><pub-id pub-id-type="pmid">28366740</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Nadel</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1978">1978</year><source>The Hippocampus as a Cognitive Map</source><publisher-name>Oxford University Press</publisher-name></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Page</surname><given-names>HJI</given-names></name><name><surname>Walters</surname><given-names>D</given-names></name><name><surname>Stringer</surname><given-names>SM</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>A speed-accurate self-sustaining head direction cell path integration model without recurrent excitation</article-title><source>Network</source><volume>29</volume><fpage>37</fpage><lpage>69</lpage><pub-id pub-id-type="doi">10.1080/0954898X.2018.1559960</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Payeur</surname><given-names>A</given-names></name><name><surname>Guerguiev</surname><given-names>J</given-names></name><name><surname>Zenke</surname><given-names>F</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Naud</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Burst-dependent synaptic plasticity can coordinate learning in hierarchical circuits</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1010</fpage><lpage>1019</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00857-x</pub-id><pub-id pub-id-type="pmid">33986551</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Poirazi</surname><given-names>P</given-names></name><name><surname>Brannon</surname><given-names>T</given-names></name><name><surname>Mel</surname><given-names>BW</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Pyramidal neuron as two-layer neural network</article-title><source>Neuron</source><volume>37</volume><fpage>989</fpage><lpage>999</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(03)00149-1</pub-id><pub-id pub-id-type="pmid">12670427</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Quirk</surname><given-names>GJ</given-names></name><name><surname>Muller</surname><given-names>RU</given-names></name><name><surname>Kubie</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>The firing of hippocampal place cells in the dark depends on the rat’s recent experience</article-title><source>The Journal of Neuroscience</source><volume>10</volume><fpage>2008</fpage><lpage>2017</lpage><pub-id pub-id-type="pmid">2355262</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Raccuglia</surname><given-names>D</given-names></name><name><surname>Huang</surname><given-names>S</given-names></name><name><surname>Ender</surname><given-names>A</given-names></name><name><surname>Heim</surname><given-names>M-M</given-names></name><name><surname>Laber</surname><given-names>D</given-names></name><name><surname>Suárez-Grimalt</surname><given-names>R</given-names></name><name><surname>Liotta</surname><given-names>A</given-names></name><name><surname>Sigrist</surname><given-names>SJ</given-names></name><name><surname>Geiger</surname><given-names>JRP</given-names></name><name><surname>Owald</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>network-specific synchronization of electrical slow-wave oscillations regulates sleep drive in <italic>Drosophila</italic></article-title><source>Current Biology</source><volume>29</volume><fpage>3611</fpage><lpage>3621</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2019.08.070</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ranck</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="1984">1984</year><article-title>Head direction cells in the deep layer of dorsal presubiculum in freely moving rats</article-title><source>Society of Neuroscience Abstract</source><volume>10</volume><elocation-id>599</elocation-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Elga</surname><given-names>AN</given-names></name><name><surname>Touretzky</surname><given-names>DS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>A coupled attractor model of the rodent head direction system</article-title><source>Network</source><volume>7</volume><fpage>671</fpage><lpage>685</lpage><pub-id pub-id-type="doi">10.1088/0954-898X_7_4_004</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samsonovich</surname><given-names>A</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Path integration and cognitive mapping in a continuous attractor neural network model</article-title><source>The Journal of Neuroscience</source><volume>17</volume><fpage>5900</fpage><lpage>5920</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.17-15-05900.1997</pub-id><pub-id pub-id-type="pmid">9221787</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seelig</surname><given-names>JD</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Neural dynamics for landmark orientation and angular path integration</article-title><source>Nature</source><volume>521</volume><fpage>186</fpage><lpage>191</lpage><pub-id pub-id-type="doi">10.1038/nature14446</pub-id><pub-id pub-id-type="pmid">25971509</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>How the brain keeps the eyes still</article-title><source>PNAS</source><volume>93</volume><fpage>13339</fpage><lpage>13344</lpage><pub-id pub-id-type="doi">10.1073/pnas.93.23.13339</pub-id><pub-id pub-id-type="pmid">8917592</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shin</surname><given-names>JN</given-names></name><name><surname>Doron</surname><given-names>G</given-names></name><name><surname>Larkum</surname><given-names>ME</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Memories off the top of your head</article-title><source>Science</source><volume>374</volume><fpage>538</fpage><lpage>539</lpage><pub-id pub-id-type="doi">10.1126/science.abk1859</pub-id><pub-id pub-id-type="pmid">34709915</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Skaggs</surname><given-names>WE</given-names></name><name><surname>Knierim</surname><given-names>JJ</given-names></name><name><surname>Kudrimoti</surname><given-names>HS</given-names></name><name><surname>McNaughton</surname><given-names>BL</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>A model of the neural basis of the rat’s sense of direction</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>173</fpage><lpage>180</lpage></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Song</surname><given-names>P</given-names></name><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Angular path integration by moving “hill of activity”: A spiking neuron model without recurrent excitation of the head-direction system</article-title><source>The Journal of Neuroscience</source><volume>25</volume><fpage>1002</fpage><lpage>1014</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.4172-04.2005</pub-id><pub-id pub-id-type="pmid">15673682</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stowers</surname><given-names>JR</given-names></name><name><surname>Hofbauer</surname><given-names>M</given-names></name><name><surname>Bastien</surname><given-names>R</given-names></name><name><surname>Griessner</surname><given-names>J</given-names></name><name><surname>Higgins</surname><given-names>P</given-names></name><name><surname>Farooqui</surname><given-names>S</given-names></name><name><surname>Fischer</surname><given-names>RM</given-names></name><name><surname>Nowikovsky</surname><given-names>K</given-names></name><name><surname>Haubensak</surname><given-names>W</given-names></name><name><surname>Couzin</surname><given-names>ID</given-names></name><name><surname>Tessmar-Raible</surname><given-names>K</given-names></name><name><surname>Straw</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Virtual reality for freely moving animals</article-title><source>Nature Methods</source><volume>14</volume><fpage>995</fpage><lpage>1002</lpage><pub-id pub-id-type="doi">10.1038/nmeth.4399</pub-id><pub-id pub-id-type="pmid">28825703</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stringer</surname><given-names>SM</given-names></name><name><surname>Trappenberg</surname><given-names>TP</given-names></name><name><surname>Rolls</surname><given-names>ET</given-names></name><name><surname>de Araujo</surname><given-names>IET</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Self-organizing continuous attractor networks and path integration: one-dimensional models of head direction cells</article-title><source>Network</source><volume>13</volume><fpage>217</fpage><lpage>242</lpage><pub-id pub-id-type="pmid">12061421</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tolman</surname><given-names>EC</given-names></name></person-group><year iso-8601-date="1948">1948</year><article-title>Cognitive maps in rats and men</article-title><source>Psychological Review</source><volume>55</volume><fpage>189</fpage><lpage>208</lpage><pub-id pub-id-type="doi">10.1037/h0061626</pub-id><pub-id pub-id-type="pmid">18870876</pub-id></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>MV</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>The neural code between neocortical pyramidal neurons depends on neurotransmitter release probability</article-title><source>PNAS</source><volume>94</volume><fpage>719</fpage><lpage>723</lpage><pub-id pub-id-type="doi">10.1073/pnas.94.2.719</pub-id><pub-id pub-id-type="pmid">9012851</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsodyks</surname><given-names>M</given-names></name><name><surname>Pawelzik</surname><given-names>K</given-names></name><name><surname>Markram</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Neural networks with dynamic synapses</article-title><source>Neural Computation</source><volume>10</volume><fpage>821</fpage><lpage>835</lpage><pub-id pub-id-type="doi">10.1162/089976698300017502</pub-id><pub-id pub-id-type="pmid">9573407</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner-Evans</surname><given-names>D</given-names></name><name><surname>Wegener</surname><given-names>S</given-names></name><name><surname>Rouault</surname><given-names>H</given-names></name><name><surname>Franconville</surname><given-names>R</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><name><surname>Seelig</surname><given-names>JD</given-names></name><name><surname>Druckmann</surname><given-names>S</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Angular velocity integration in a fly heading circuit</article-title><source>eLife</source><volume>6</volume><elocation-id>e23496</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.23496</pub-id><pub-id pub-id-type="pmid">28530551</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Turner-Evans</surname><given-names>DB</given-names></name><name><surname>Jensen</surname><given-names>KT</given-names></name><name><surname>Ali</surname><given-names>S</given-names></name><name><surname>Paterson</surname><given-names>T</given-names></name><name><surname>Sheridan</surname><given-names>A</given-names></name><name><surname>Ray</surname><given-names>RP</given-names></name><name><surname>Wolff</surname><given-names>T</given-names></name><name><surname>Lauritzen</surname><given-names>JS</given-names></name><name><surname>Rubin</surname><given-names>GM</given-names></name><name><surname>Bock</surname><given-names>DD</given-names></name><name><surname>Jayaraman</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>The neuroanatomical ultrastructure and function of a biological ring attractor</article-title><source>Neuron</source><volume>108</volume><fpage>145</fpage><lpage>163</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.08.006</pub-id><pub-id pub-id-type="pmid">32916090</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tuthill</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Lessons from a compartmental model of a <italic>Drosophila</italic> neuron</article-title><source>The Journal of Neuroscience</source><volume>29</volume><fpage>12033</fpage><lpage>12034</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3348-09.2009</pub-id><pub-id pub-id-type="pmid">19793961</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Urbanczik</surname><given-names>R</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Learning by the dendritic prediction of somatic spiking</article-title><source>Neuron</source><volume>81</volume><fpage>521</fpage><lpage>528</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.11.030</pub-id><pub-id pub-id-type="pmid">24507189</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="thesis"><person-group person-group-type="author"><name><surname>Vafidis</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning of a path-integrating circuit</article-title><publisher-name>Technical University of Berlin</publisher-name></element-citation></ref><ref id="bib73"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Vafidis</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>LearnPI</data-title><version designator="swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315">swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:94623fb678bde9a80cec006ae68b36b0da4b13f7;origin=https://github.com/panvaf/LearnPI;visit=swh:1:snp:d2bf616300e0b9b6efa575c1be2040dd9e7ee4dc;anchor=swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315">https://archive.softwareheritage.org/swh:1:dir:94623fb678bde9a80cec006ae68b36b0da4b13f7;origin=https://github.com/panvaf/LearnPI;visit=swh:1:snp:d2bf616300e0b9b6efa575c1be2040dd9e7ee4dc;anchor=swh:1:rev:c6e354f80bf435114e577af70892db41c3ce5315</ext-link></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Synaptic reverberation underlying mnemonic persistent activity</article-title><source>Trends in Neurosciences</source><volume>24</volume><fpage>455</fpage><lpage>463</lpage><pub-id pub-id-type="doi">10.1016/s0166-2236(00)01868-3</pub-id><pub-id pub-id-type="pmid">11476885</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>XJ</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Probabilistic decision making by slow reverberation in cortical circuits</article-title><source>Neuron</source><volume>36</volume><fpage>955</fpage><lpage>968</lpage><pub-id pub-id-type="doi">10.1016/s0896-6273(02)01092-9</pub-id><pub-id pub-id-type="pmid">12467598</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>RI</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Early olfactory processing in <italic>Drosophila</italic>: mechanisms and principles</article-title><source>Annual Review of Neuroscience</source><volume>36</volume><fpage>217</fpage><lpage>241</lpage><pub-id pub-id-type="doi">10.1146/annurev-neuro-062111-150533</pub-id><pub-id pub-id-type="pmid">23841839</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>Spike-based learning rules and stabilization of persistent neural activity</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>199</fpage><lpage>208</lpage></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Xie</surname><given-names>X</given-names></name><name><surname>Hahnloser</surname><given-names>RHR</given-names></name><name><surname>Seung</surname><given-names>HS</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Double-ring network model of the head-direction system</article-title><source>Physical Review. E, Statistical, Nonlinear, and Soft Matter Physics</source><volume>66</volume><elocation-id>041902</elocation-id><pub-id pub-id-type="doi">10.1103/PhysRevE.66.041902</pub-id><pub-id pub-id-type="pmid">12443230</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Xu</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Connectome of the adult <italic>Drosophila</italic> central brain</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.01.21.911859</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>2112</fpage><lpage>2126</lpage><pub-id pub-id-type="pmid">8604055</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhao</surname><given-names>C</given-names></name><name><surname>Widmer</surname><given-names>YF</given-names></name><name><surname>Diegelmann</surname><given-names>S</given-names></name><name><surname>Petrovici</surname><given-names>MA</given-names></name><name><surname>Sprecher</surname><given-names>SG</given-names></name><name><surname>Senn</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Predictive olfactory learning in <italic>Drosophila</italic></article-title><source>Scientific Reports</source><volume>11</volume><elocation-id>6795</elocation-id><pub-id pub-id-type="doi">10.1038/s41598-021-85841-y</pub-id><pub-id pub-id-type="pmid">33762640</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhong</surname><given-names>W</given-names></name><name><surname>Lu</surname><given-names>Z</given-names></name><name><surname>Schwab</surname><given-names>DJ</given-names></name><name><surname>Murugan</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Nonequilibrium statistical mechanics of continuous attractors</article-title><source>Neural Computation</source><volume>32</volume><fpage>1033</fpage><lpage>1068</lpage><pub-id pub-id-type="doi">10.1162/neco_a_01280</pub-id><pub-id pub-id-type="pmid">32343645</pub-id></element-citation></ref></ref-list><app-group><app id="appendix-1"><title>Appendix 1</title><sec sec-type="appendix" id="s8"><title>Robustness to noise</title><p>In the main text, the only source of stochasticity in the network came from the angular velocity noise in the Ornstein-Uhlenbeck process (Materials and methods, <xref ref-type="disp-formula" rid="equ17">Equation 17</xref>). Biological HD systems, however, are subject to other forms of biological noise like randomness of ion channels. To address that, we include Gaussian IID synaptic current noise to every location in the network where inputs arrive: the axon-proximal and axon-distal compartments of HD cells and the HR cells (see Materials and methods, parameterized by <inline-formula><mml:math id="inf283"><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ2">Equation 2</xref>, <xref ref-type="disp-formula" rid="equ4">Equation 4</xref>, and <xref ref-type="disp-formula" rid="equ8">Equation 8</xref>). We then ask how robustly can the network learn in the presence of such additional stochasticity.</p><p>To quantify the network’s robustness to noise, we need to define a comparative measure of useful signals vs. noise in the network. By ‘signals’ we refer to the velocity/visual inputs and any network activity resulting from them, whereas ‘noise’ is the aforementioned Gaussian IID variables. We thus define the signal-to-noise ratio (<italic>SNR</italic>) to be the squared ratio of the active range <inline-formula><mml:math id="inf284"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> of the activation function <inline-formula><mml:math id="inf285"><mml:mi>f</mml:mi></mml:math></inline-formula> (defined in <xref ref-type="disp-formula" rid="equ7">Equation 7</xref>) over two times the standard deviation of the Gaussian noise, <inline-formula><mml:math id="inf286"><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula>, i.e.<disp-formula id="equ22"><label>(22)</label><mml:math id="m22"><mml:mrow><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:msup><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>This definition is motivated by the fact that <inline-formula><mml:math id="inf287"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> determines the useful range that signals in the network can have. If any of the signals exceed this range, they cannot impact the network in any meaningful way because the neuronal firing rate has saturated, unless they are counterbalanced by other signals reliably present. The factor 2 in the denominator is due to the fact that the noise can extend to both positive and negative values, whereas <inline-formula><mml:math id="inf288"><mml:msub><mml:mi>A</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> denotes the entire range of useful inputs.</p><p>Here, we vary the <italic>SNR</italic> and observe its impact on learning and network performance. <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A–D</xref> shows the performance of a network that has been trained with <inline-formula><mml:math id="inf289"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>≈</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>. The resulting network connectivity remains circularly symmetric and maintains the required asymmetry in the HR-to-HD connections for L- and R-HR cells (data not shown). Therefore we plot only the profiles in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>, which look very similar to the ones in <xref ref-type="fig" rid="fig3">Figure 3C</xref> trained with <inline-formula><mml:math id="inf290"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>. The peak of the local excitatory connectivity in <inline-formula><mml:math id="inf291"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> is not as pronounced. This happens because the noise corrupts auto-correlations of firing during learning.</p><p>The network activity still displays a clear bump that smoothly follows the ground truth in the absence of visual input (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). There are only minor differences compared to the network without noise (<xref ref-type="fig" rid="fig2">Figure 2A</xref>). The presence of the noise is most obvious in the HR cells, since HD cells that do not participate in the bump are deep into inhibition, and therefore synaptic input noise does not affect as much their activity. We note that the network can no longer sustain a bump in darkness when <inline-formula><mml:math id="inf292"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula>, that is when the standard deviation of the noise covers the full active range of inputs (data not shown).</p><p>Finally, the neural velocity slightly overestimates the head angular velocity (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref> compared to <xref ref-type="fig" rid="fig2">Figure 2C</xref>), and the PI errors diffuse faster in the network with noise (<inline-formula><mml:math id="inf293"><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>88.1</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>deg</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D</xref> compared to <inline-formula><mml:math id="inf294"><mml:mrow><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>24.5</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mtext>deg</mml:mtext><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mtext>s</mml:mtext></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="fig2">Figure 2B</xref>); these values are also indicated in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1E</xref> (triangles). Importantly, we find that the diffusion assumption holds, because the estimation of the diffusion coefficient when varying simulation time (between 10 and 60 s) is consistent.</p><fig id="app1fig1" position="float"><label>Appendix 1—figure 1.</label><caption><title>Robustness to injected noise.</title><p>(<bold>A</bold>) PI example in a network trained with noise (<inline-formula><mml:math id="inf295"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>≈</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>, train noise <inline-formula><mml:math id="inf296"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula>). Panels are organized as in <xref ref-type="fig" rid="fig2">Figure 2A</xref>, which shows the activity in a network trained without noise (<inline-formula><mml:math id="inf297"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf298"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Profiles of learned weights. Both <inline-formula><mml:math id="inf299"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf300"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> are circularly symmetric. Panel is organized as in <xref ref-type="fig" rid="fig3">Figure 3C</xref>, which shows weight profiles in a network trained without noise (<inline-formula><mml:math id="inf301"><mml:mrow><mml:mtext>SNR</mml:mtext><mml:mo>=</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf302"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>C</bold>) The network achieves almost perfect gain-1 PI, despite noisy inputs. Compared to <xref ref-type="fig" rid="fig2">Figure 2C</xref> the performance is only slightly impaired. (<bold>D</bold>) Temporal evolution of distribution of PI errors during PI in darkness. Compared to <xref ref-type="fig" rid="fig2">Figure 2B</xref> the distribution widens faster, however it also does not exhibit side bias. (<bold>E</bold>) Diffusion coefficient for networks as a function of the level of test noise (for details, see section &quot;Diffusion Coefficient&quot; in Materials and methods). We distinguish between networks that experienced noise during training (<inline-formula><mml:math id="inf303"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula>, orange) and networks that were trained without injected noise (<inline-formula><mml:math id="inf304"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, blue), which were studied in the Results of the main text. Diffusion coefficients that include contributions from PI errors, estimated from (<bold>D</bold>) and <xref ref-type="fig" rid="fig2">Figure 2B</xref>, are also plotted (triangles).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app1-fig1-v2.tif"/></fig><p>So far we have addressed the diffusivity of PI in networks that receive velocity input, and we have compared the performance of networks that were trained with and without synaptic input noise. However, in mature networks, that is during testing, it is unclear how large the impact of synaptic input noise is, compared to noise that originates from imperfect PI. To disentangle these two noise contributions during testing, we study diffusivity in networks that do not receive velocity inputs at all (i.e. without PI). In the absence of velocity inputs, we vary the level of synaptic input noise during testing (called ‘test noise’), and estimate diffusion coefficients. Specifically, for each test noise magnitude <inline-formula><mml:math id="inf305"><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub></mml:math></inline-formula> we run 1000 simulations of <inline-formula><mml:math id="inf306"><mml:mrow><mml:msub><mml:mi>t</mml:mi><mml:mtext>sim</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>10</mml:mn></mml:mrow></mml:math></inline-formula> s, each time randomly initializing the network at one of the angular locations <inline-formula><mml:math id="inf307"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> for which HD neurons are tuned for. For <inline-formula><mml:math id="inf308"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula>, we run one simulation per <inline-formula><mml:math id="inf309"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula>, since the simulation is deterministic.</p><p>Our results in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1E</xref> show that the diffusion coefficients obtained in networks without velocity inputs (dots) are always much smaller that those with velocity inputs (triangles). Thus, imperfect integration of velocity inputs is by far the dominating source of noise in trained, mature networks. Omitting the velocity inputs, we detected small differences between networks that were trained without synaptic input noise (blue dots) and with such noise (orange dots, train noise <inline-formula><mml:math id="inf310"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:math></inline-formula>). The network trained with noise is sligthly more diffusive up to the level of test noise at which is was trained (<inline-formula><mml:math id="inf311"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>n</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math></inline-formula> – 0.7), and it is slightly less diffusive beyond that level (<inline-formula><mml:math id="inf312"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mrow><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0.7</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>).</p><p>We conclude that learning PI is robust to synaptic input noise during learning, and that synaptic input noise during testing degrades performance much less than errors due to deviations from perfect gain-1 PI, which are already quite small.</p></sec></app><app id="appendix-2"><title>Appendix 2</title><sec sec-type="appendix" id="s9"><title>Synaptic delays set a neural velocity limit during path integration</title><p>In the main text we trained networks for a set of angular velocities that cover the full range exhibited by the fly (<inline-formula><mml:math id="inf313"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>v</mml:mi><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>&lt;</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> deg/s), and we showed that the mature network can account for several key experimental findings. However, the ability of any continuous attractor network to path-integrate is naturally limited for high angular velocities, due to the synaptic delays inherent in any such network (<xref ref-type="bibr" rid="bib82">Zhong et al., 2020</xref>). To evaluate the ability of our network to integrate angular velocities, we sought to identify a limit of what velocities could be learned.</p><p>The width of the HD bump in our network is here termed <italic>BW</italic>, and it is largely determined by the width <inline-formula><mml:math id="inf314"><mml:mi>σ</mml:mi></mml:math></inline-formula> of the visual receptive field. This is because during training we force the network to produce a bump with a width matching that of the visual input, and this width is then maintained when the latter is not present. The reason for this behavior is that the width of the learned local excitatory connectivity profile in <inline-formula><mml:math id="inf315"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> that guarantees such stable bumps of activity will be similar to the width of the bump, because recurrent connections during learning are only drawn from active neurons (non-zero <inline-formula><mml:math id="inf316"><mml:msub><mml:mtext>P</mml:mtext><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ12">Equation 12</xref>). As mentioned in the main text, this emphasizes the Hebbian component of our learning rule (fire together — wire together). As a result, the width of local excitatory recurrent connections should be approximately <italic>BW</italic>.</p><p>In <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplement 1</xref> we show that the higher angular velocities are served by the long-range excitatory connections in <inline-formula><mml:math id="inf317"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>. However, these connections might not be strong enough to move the bump by themselves; a contribution from HD cells might still be needed to move the bump at such high angular velocities. In that case, the width of the connectivity bump in <inline-formula><mml:math id="inf318"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> might limit how <italic>far away</italic> from the current location the bump can be moved. In addition, there is a limitation in how <italic>quickly</italic> the bump can be moved: the learning rule in <xref ref-type="disp-formula" rid="equ1">Equation 1</xref> tries to predict the next state of the network from the current state; but to activate the next HD neurons in line, current HD and HR cell activity must go through the synaptic delay <inline-formula><mml:math id="inf319"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>. Therefore, the maximum velocity that the network can achieve without external guidance (i.e. without visual input) should be inversely proportional to <inline-formula><mml:math id="inf320"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, i.e.<disp-formula id="equ23"><label>(23)</label><mml:math id="m23"><mml:mrow><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>σ</mml:mi><mml:mo>,</mml:mo><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mfrac></mml:mrow></mml:math></disp-formula></p><p>where we assume that <inline-formula><mml:math id="inf321"><mml:mi>b</mml:mi></mml:math></inline-formula> might reflect an effective HD connectivity bump width, which depends on <inline-formula><mml:math id="inf322"><mml:mi>σ</mml:mi></mml:math></inline-formula> but also on the angular resolution of the HD network <inline-formula><mml:math id="inf323"><mml:mrow><mml:mi mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi>ϕ</mml:mi></mml:mrow></mml:math></inline-formula>, due to discretization effects. In reality, the HR-to-HD connectivity profiles in <inline-formula><mml:math id="inf324"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> likely also have a bearing on <inline-formula><mml:math id="inf325"><mml:mi>b</mml:mi></mml:math></inline-formula>.</p><p>We then systematically vary <inline-formula><mml:math id="inf326"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and test what velocities the network can learn. We indeed find that networks can path-integrate all angular velocities up to a limit, but not higher than that. As predicted, this limit is inversely proportional to <inline-formula><mml:math id="inf327"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>, for a wide range of delays (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref>). Furthermore, <inline-formula><mml:math id="inf328"><mml:mi>b</mml:mi></mml:math></inline-formula> matches <italic>BW</italic> reasonably well. Fitting <xref ref-type="disp-formula" rid="equ23">Equation 23</xref> to the data we obtain <inline-formula><mml:math id="inf329"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.25</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>6</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mtext>BW</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>96</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf330"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>120</mml:mn></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf331"><mml:mrow><mml:mrow><mml:mi>b</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>0.15</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>12</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>75</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>, <inline-formula><mml:math id="inf332"><mml:mrow><mml:mtext>BW</mml:mtext><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>60</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf333"><mml:mrow><mml:msup><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:msup><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msup><mml:mo>=</mml:mo><mml:mn>60</mml:mn></mml:mrow></mml:math></inline-formula>.</p><p>As mentioned in the main text, there are two limitations other than synaptic delays why the network could not learn high angular velocities: limited training of these velocities, and saturation of HR cell activity. These limitations kick in for <inline-formula><mml:math id="inf334"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>150</mml:mn><mml:mtext> </mml:mtext><mml:mrow/><mml:mrow><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>, for which <inline-formula><mml:math id="inf335"><mml:msub><mml:mi>v</mml:mi><mml:mrow><mml:mi>m</mml:mi><mml:mo>⁢</mml:mo><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> matches the maximum velocity the fly displays (500 deg/s). Therefore to create <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1A</xref> for these delays, we increased the standard deviation of the velocity noise in the Ornstein-Uhlenbeck process to <inline-formula><mml:math id="inf336"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>800</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg/s</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> to address the first limitation, and we increased the dynamic range of angular velocity inputs by decreasing the proportionality constant in <xref ref-type="disp-formula" rid="equ10">Equation 10</xref> to <inline-formula><mml:math id="inf337"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mpadded width="+1.7pt"><mml:mn>540</mml:mn></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mtext>s/deg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> to address the second.</p><p>The velocity gain plot for an example network with high synaptic delays (<inline-formula><mml:math id="inf338"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>190</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>) is shown in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1B</xref>. Interestingly, we notice that the performance drop at the velocity limit is not gradual; instead, the neural velocity abruptly drops to a near-zero value once past the velocity limit. Further investigation reveals that for velocities higher than this limit, the network can no longer sustain a bump (<xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1C</xref>). This happens because the HD network cannot activate neurons downstream fast enough to keep the bump propagating, and therefore the bump disappears and the velocity gain plot becomes flat.</p><p><xref ref-type="disp-formula" rid="equ23">Equation 23</xref> is similar to a relationship reported in <xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref> (their page 35, 1st paragraph), where it was demonstrated that the phase shift of the HR population bump compared to the HD bump limits angular velocity. Our result hence generalizes this finding in the case where recurrent connections between HD neurons are also allowed.</p><p>Finally, we note that so far we only tested the limits of network performance when increasing <inline-formula><mml:math id="inf339"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula>. To demonstrate that smaller delays also work, as an extreme example we show PI performance in a network where <inline-formula><mml:math id="inf340"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>1</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D and E</xref>. A potential issue with such small synaptic delays is that the network would not be able to distinguish rightward from leftward rotation for small angular velocities, because the motion direction offset of the HR bumps would be small, and the activity in the two HR populations comparable. In such a setting it is harder to learn the asymmetries in the HR-to-HD connections required to differentiate leftward from rightward movement. Indeed, this effect is visible in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1E</xref> where the amplitude of HR-to-HD connections has been suppressed, and in <xref ref-type="fig" rid="app1fig1">Appendix 1—figure 1D</xref> where the flat region for small angular velocities has been extended compared to <xref ref-type="fig" rid="fig2">Figure 2C</xref>.</p><p>Overall, these results indicate that the network learns to path-integrate angular velocities up to a fundamental limit imposed by the architecture of the HD system in the fly. Furthermore, we conclude that the phenomenological delays observed in the fly HD system in <xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref> are not fundamentally limiting the system’s performance, since they can support PI for angular velocities much higher than the ones normally displayed by the fly.</p><fig id="app2fig1" position="float"><label>Appendix 2—figure 1.</label><caption><title>Limits of network performance when varying synaptic delays.</title><p>(<bold>A</bold>) Maximum neural angular velocity learned is inversely proportional to the synaptic delay <inline-formula><mml:math id="inf341"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> in the network, with constant <inline-formula><mml:math id="inf342"><mml:mrow><mml:mi>b</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>75</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>deg</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula> in <xref ref-type="disp-formula" rid="equ23">Equation 23</xref> (blue dot-dashed line). Green dots: point estimate of maximum neural velocity learned, green bars: 95% confidence intervals (Student’s t-test, <inline-formula><mml:math id="inf343"><mml:mrow><mml:mi>N</mml:mi><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:math></inline-formula>). (<bold>B</bold>) Example neural velocity gain plot (as in <xref ref-type="fig" rid="fig2">Figure 2C</xref>) in a network with increased synaptic delays (<inline-formula><mml:math id="inf344"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> increased from the &quot;standard&quot; value 65 ms to the new value 190 ms). (<bold>C</bold>) Behavior of the activity of HD cells in the network with parameters as in (<bold>B</bold>) near the velocity limit. The example network is driven by a single velocity in every column, in light (top row) and darkness (bottom row) conditions. In darkness, near and below the limit (left and middle column), there is a delay in the appearance of the bump, which then path-integrates with gain 1; above the limit observed in (<bold>B</bold>), however, the bump cannot stabilize, resulting in the dip in neural velocity (right column). (<bold>D</bold>) PI performance for a network with drastically reduced synaptic delays (<inline-formula><mml:math id="inf345"><mml:mrow><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>⁢</mml:mo><mml:mtext>ms</mml:mtext></mml:mrow></mml:mrow></mml:math></inline-formula>). Compared to <xref ref-type="fig" rid="fig2">Figure 2C</xref> performance is worse for small angular velocities. This occurs because for small angular velocities, the offset of the HR bump in leftward vs. rightward movement is not as pronounced. As a result, it is harder to differentiate leftward from rightward movement. (<bold>E</bold>) For the same reason, the asymmetries in the learned HR-to-HD connectivity are not as prevalent as in <xref ref-type="fig" rid="fig3">Figure 3C</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app2-fig1-v2.tif"/></fig></sec></app><app id="appendix-3"><title>Appendix 3</title><sec sec-type="appendix" id="s10"><title>Robustness to architectural asymmetries</title><p>The networks we have trained in the Results had a circular symmetric initial architecture, including the hardwired HD-to-HR connections <inline-formula><mml:math id="inf346"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula>. However, such symmetry is unrealistic for any biological system that is assembled by imperfect processes; deviations from symmetry should be expected. Therefore, in this Appendix we let <inline-formula><mml:math id="inf347"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> vary randomly, and observe how PI performance is affected.</p><p>First, we remind the reader that the magnitude <inline-formula><mml:math id="inf348"><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> of the HD-to-HR connections is chosen so that we take advantage of the full dynamic range of HR neurons; however, the exact magnitude should not be critical for our model. Homeostatic plasticity could adjust the magnitude, but for simplicity we have not incorporated such plasticity rules in our model. Instead, to see whether the exact values of synaptic weights, their circular symmetry, and the 1-to-1 nature of the HD-to-HR connections is crucial for our model, we draw connection strengths randomly.</p><p>In a first approach, we let HD neurons project also to adjacent HR neurons. Specifically, if <inline-formula><mml:math id="inf349"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> denotes the uniform distribution in the interval <inline-formula><mml:math id="inf350"><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>b</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:math></inline-formula>, we sample the magnitude of weights from <inline-formula><mml:math id="inf351"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the main diagonal and <inline-formula><mml:math id="inf352"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mpadded width="+1.7pt"><mml:mn>3</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow><mml:mn>4</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the side diagonals of <inline-formula><mml:math id="inf353"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1A</xref>). We then adjust the network connectivity (<inline-formula><mml:math id="inf354"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf355"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>) in a learning phase, similar to the one illustrated in <xref ref-type="fig" rid="fig3">Figure 3E and F</xref>. After learning, as shown in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1C-E</xref>, PI is still excellent because the learning rule can balance out any deviations from circular symmetry in <inline-formula><mml:math id="inf356"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula>. It does so by introducing deviations from circular symmetry in the learned weights, mainly in <inline-formula><mml:math id="inf357"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1B</xref>). Thus, small deviations from circular symmetry in the learned weights are essential for PI.</p><fig id="app3fig1" position="float"><label>Appendix 3—figure 1.</label><caption><title>Performance of a network where HD-to-HR connection weights are allowed to vary randomly, and HD neurons are projecting to HR neurons also adjacent to the ones they correspond to, respecting the topography of the protocerebral bridge (PB).</title><p>(<bold>A</bold>) The HD-to-HR connectivity matrix, <inline-formula><mml:math id="inf358"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula>. Note that, compared to what is described in the Materials and methods (final paragraph of ‘Neuronal Model’), the order of HD neurons is rearranged: we have grouped HD neurons that project to the same wing of the PB together, so that the diagonal structure of the connections is clearly visible. (<bold>B</bold>) The learned HR-to-HD connections, <inline-formula><mml:math id="inf359"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula>, depart from circular symmetry (as, e.g., in <xref ref-type="fig" rid="fig3">Figure 3B</xref>), so that asymmetries in <inline-formula><mml:math id="inf360"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> could be counteracted. The recurrent connections <inline-formula><mml:math id="inf361"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> (not shown) remain largely unaltered compared to the ones shown in <xref ref-type="fig" rid="fig3">Figure 3A</xref>. (<bold>C</bold>–<bold>E</bold>) Despite the randomization and lack of 1-to-1 nature of HD-to-HR connections, PI in the converged network remains excellent (<xref ref-type="fig" rid="fig2">Figure 2A–C</xref>).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app3-fig1-v2.tif"/></fig><p>We illustrate the necessity to counterbalance small deviations of circular symmetry again in a second example that is based on the symmetric network studied in the Results. Here, we use the connectivity of the network illustrated in <xref ref-type="fig" rid="fig3">Figure 3A–C</xref> and also preserve the 1-to-1 nature of HD-to-HR connections, but now we randomly vary their magnitude, while maintaining the same average connection strength; specifically, we sample the magnitude of weights from <inline-formula><mml:math id="inf362"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mn>2</mml:mn></mml:mfrac><mml:mo>,</mml:mo><mml:mfrac><mml:mrow><mml:mn>3</mml:mn><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow><mml:mn>2</mml:mn></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>. PI performance in this network is considerably impaired compared to the original network (compare <xref ref-type="fig" rid="app3fig2">Appendix 3—figure 2</xref> to <xref ref-type="fig" rid="fig2">Figure 2A and C</xref>). This is a further argument in favor of synaptic plasticity operating to fine-tune connectivity, because as mentioned we expect that such anatomical asymmetries are indeed present in the biological circuit. Therefore, even if the circular symmetric synaptic weights were passed down genetically with great accuracy, PI performance in flies should be considerably degraded for a biological circuit with anatomical asymmetries when no learning is involved.</p><fig id="app3fig2" position="float"><label>Appendix 3—figure 2.</label><caption><title>PI performance in a network with random HD-to-HR connection strengths and learned weights from network in <xref ref-type="fig" rid="fig3">Figure 3</xref>.</title><p>Here we vary the magnitude of the main diagonal HD-to-HR connections but preserve the 1-to-1 nature of the connections. We assume that <inline-formula><mml:math id="inf363"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf364"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> are passed down genetically (i.e. there is no further learning of these connections), and therefore the same, circular symmetric profiles apply to every location in the circuit. We choose these (assumed here to be genetically stored) profiles to be the ones we learned in the network outlined in <xref ref-type="fig" rid="fig3">Figure 3A and B</xref>. (<bold>A</bold>) Example that shows that PI is impaired, because the circular symmetric profiles passed down genetically cannot counteract small asymmetries in the architecture that are likely to be present in any biological system. Notice that it can even take several seconds for the large PI error to be corrected by the visual input. (<bold>B</bold>) PI errors grow fast (compare to e.g. <xref ref-type="fig" rid="fig2">Figure 2B</xref>). Already by <inline-formula><mml:math id="inf365"><mml:mrow><mml:mpadded width="+3.3pt"><mml:mn>20</mml:mn></mml:mpadded><mml:mo>⁢</mml:mo><mml:mtext>sec</mml:mtext></mml:mrow></mml:math></inline-formula> of PI the heading estimate is random.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app3-fig2-v2.tif"/></fig><p>To better quantify the effect of anatomical asymmetries, we incorporate both noise in the learned connectivity as in <xref ref-type="fig" rid="fig3s3">Figure 3—figure supplement 3A</xref>,B and noise in the HD-to-HR connections as in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1A</xref>. We tune the noise independently for each weight matrix: for <inline-formula><mml:math id="inf366"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf367"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> we set the variance of the Gaussian noise to <inline-formula><mml:math id="inf368"><mml:mi>p</mml:mi></mml:math></inline-formula> times the variance of the individual weight matrices, while for <inline-formula><mml:math id="inf369"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> we draw the connections connections from <inline-formula><mml:math id="inf370"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>1</mml:mn><mml:mo>+</mml:mo><mml:mi>p</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the main diagonal and <inline-formula><mml:math id="inf371"><mml:mrow><mml:mi>U</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>p</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for the side diagonals. We find that for <inline-formula><mml:math id="inf372"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> the correlation between the PVA and true heading in darkness drops to <inline-formula><mml:math id="inf373"><mml:mrow><mml:mn>0.27</mml:mn><mml:mo>±</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:math></inline-formula> which is below reported fly PI performance (mean correlation across animals <inline-formula><mml:math id="inf374"><mml:mrow><mml:mi/><mml:mo>∼</mml:mo><mml:mn>0.5</mml:mn></mml:mrow></mml:math></inline-formula> in <xref ref-type="bibr" rid="bib58">Seelig and Jayaraman, 2015</xref>), while the structure of the weights is preserved. We observed a steep decline in the correlation coefficient between <inline-formula><mml:math id="inf375"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.25</mml:mn></mml:mrow></mml:math></inline-formula> (for which the correlation is <inline-formula><mml:math id="inf376"><mml:mrow><mml:mn>0.92</mml:mn><mml:mo>±</mml:mo><mml:mn>0.04</mml:mn></mml:mrow></mml:math></inline-formula>) and <inline-formula><mml:math id="inf377"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula>. Furthermore, we study the effect of perturbing individual weight matrices, and find that perturbing only <inline-formula><mml:math id="inf378"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> with <inline-formula><mml:math id="inf379"><mml:mrow><mml:mi>p</mml:mi><mml:mo>=</mml:mo><mml:mn>0.3</mml:mn></mml:mrow></mml:math></inline-formula> considerably affects performance (correlation <inline-formula><mml:math id="inf380"><mml:mrow><mml:mn>0.39</mml:mn><mml:mo>±</mml:mo><mml:mn>0.09</mml:mn></mml:mrow></mml:math></inline-formula>) while perturbation of <inline-formula><mml:math id="inf381"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf382"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> together has a much smaller effect on performance. This again argues in favor of learning <inline-formula><mml:math id="inf383"><mml:msup><mml:mi>W</mml:mi><mml:mtext>rec</mml:mtext></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf384"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HR</mml:mtext></mml:msup></mml:math></inline-formula> to counterbalance asymmetries in <inline-formula><mml:math id="inf385"><mml:msup><mml:mi>W</mml:mi><mml:mtext>HD</mml:mtext></mml:msup></mml:math></inline-formula> (<xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>). Furthermore, note that confounders other than imperfect weights might be responsible for the degradation of PI performance in the fly, which further argues in favor of learning.</p><p>As a final test for the capability of the learning rule to balance anatomical asymmetries, in <xref ref-type="fig" rid="app3fig3">Appendix 3—figure 3</xref> we use a completely random connectivity for HD-to-HR connections, drawing weights from a folded Gaussian distribution. We find that even then, PI performance of the converged network is great, albeit for a smaller range of velocities. In addition, bumps are not clearly visible in the HR populations anymore; in the main network, HR bumps were inherited from the HD bump due to the sparseness of the HD-to-HR connections. However, when HD-to-HR connections are random, HR cells are no longer mapped to a topographic state space.</p><fig id="app3fig3" position="float"><label>Appendix 3—figure 3.</label><caption><title>PI performance of a network where HD-to-HR connection weights are completely random.</title><p>(<bold>A</bold>) The HD-to-HR weights are drawn from a folded normal distribution, originating from a normal distribution with 0 mean and <inline-formula><mml:math id="inf386"><mml:mrow><mml:mrow><mml:mi>π</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mtext>HD</mml:mtext></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>/</mml:mo><mml:mn>200</mml:mn></mml:mrow></mml:math></inline-formula> variance. (<bold>B</bold>) As a result, the learned HR-to-HD connections have also lost their structure. (<bold>C</bold>) The recurrent connections preserve some structure, since adjacency in the HD network is still important. (<bold>D</bold>) Impressively, the converged network can still PI with a gain close to 1, but for a reduced range of angular velocities compared to, e.g. the network in <xref ref-type="fig" rid="app3fig1">Appendix 3—figure 1</xref>. (<bold>E</bold>) A bump still appears in the HD network and gets integrated in darkness, albeit with larger errors. Note that bumps no longer appear in the HR populations; HR bumps are inherited from the HD bump only when adjacencies in the HD population are carried over to the HR populations by the HD-to-HR connections. Note that we have restricted angular velocities to the interval <inline-formula><mml:math id="inf387"><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mn>360</mml:mn></mml:mrow><mml:mo>,</mml:mo><mml:mn>360</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:math></inline-formula> deg/s for this example, to showcase that PI is still accurate within this interval.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app3-fig3-v2.tif"/></fig></sec></app><app id="appendix-4"><title>Appendix 4</title><sec sec-type="appendix" id="s11"><title>Requirements on time scales</title><p>We devote this Appendix to discuss requirements for the time scales involved in our model (see <xref ref-type="table" rid="app4table1">Appendix 4—table 1</xref>). Several of these time scales are well constrained by biology, and thus we chose to keep them constant. These include the membrane time constants of the axon-proximal and axon-distal compartments, <inline-formula><mml:math id="inf388"><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf389"><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula>, respectively, which should be in the order of milliseconds; and the velocity decay time constant <inline-formula><mml:math id="inf390"><mml:msub><mml:mi>τ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula>, for which we choose a value in the same order of magnitude (0.5 s) as experimentally reported (<xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>).</p><p>In general, the learning time scale given by <inline-formula><mml:math id="inf391"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> should be the slowest one in our network model. The time scale should be large enough so that the network samples the input statistics for a long enough time. Varying <inline-formula><mml:math id="inf392"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> from 2s to 20s to 200s, we find that the final learned weights are virtually identical (not shown). However, <inline-formula><mml:math id="inf393"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> should not be too large to enable fast enough learning.</p><p>The synaptic time constant <inline-formula><mml:math id="inf394"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> is determined from phenomenological delays in the network (<xref ref-type="bibr" rid="bib68">Turner-Evans et al., 2017</xref>). Nevertheless, we also addressed the impact of varying <inline-formula><mml:math id="inf395"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> in Appendix 2 and found that learning PI was robust in a wide range of values.</p><p>In additional simulations, we varied the weight update filtering time constant <inline-formula><mml:math id="inf396"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula> from 0 (effectively removing filtering) to 1000s (which is four orders of magnitude larger than the default value). We observed almost no effect on learning dynamics, and the performance of the final networks was almost identical (not shown). Since the specific value of <inline-formula><mml:math id="inf397"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula> is of little consequence in our network (if <inline-formula><mml:math id="inf398"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula> is large enough), there are hardly any limitations on its value compared to other time scales. Therefore, this justifies ignoring <inline-formula><mml:math id="inf399"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula> in the derivation of a reduced model without noise in Appendix 5.</p><table-wrap id="app4table1" position="float"><label>Appendix 4—table 1.</label><caption><title>Default values for time scales in the model ordered with respect to their magnitude.</title></caption><table frame="hsides" rules="groups"><thead><tr><th valign="bottom">Time scale</th><th valign="bottom">Expression</th><th valign="bottom">Value</th><th valign="bottom">Unit</th></tr></thead><tbody><tr><td align="left" valign="bottom">Membrane time constant of axon-proximal compartment</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf400"><mml:mrow><mml:mi>C</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">1</td><td align="left" valign="bottom">ms</td></tr><tr><td align="left" valign="bottom">Membrane time constant of axon-distal compartment</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf401"><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">10</td><td align="left" valign="bottom">ms</td></tr><tr><td align="left" valign="bottom">Synaptic time constant</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf402"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">65</td><td align="left" valign="bottom">ms</td></tr><tr><td align="left" valign="bottom">Weight update filtering time constant</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf403"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">100</td><td align="left" valign="bottom">ms</td></tr><tr><td align="left" valign="bottom">Velocity decay time constant</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf404"><mml:msub><mml:mi>τ</mml:mi><mml:mi>v</mml:mi></mml:msub></mml:math></inline-formula></td><td align="left" valign="bottom">0.5</td><td align="left" valign="bottom">s</td></tr><tr><td align="left" valign="bottom">Learning time scale</td><td align="left" valign="bottom"><inline-formula><mml:math id="inf405"><mml:mrow><mml:mn>1</mml:mn><mml:mo>/</mml:mo><mml:mi>η</mml:mi></mml:mrow></mml:math></inline-formula></td><td align="left" valign="bottom">20</td><td align="left" valign="bottom">s</td></tr></tbody></table></table-wrap></sec></app><app id="appendix-5"><title>Appendix 5</title><sec sec-type="appendix" id="s12"><title>Reduced model for a circular symmetric learned network</title><p>In this section, we derive a reduced model for the dynamics of the synaptic weights during learning. The goal is to gain an intuitive understanding of the structure obtained in the full model (<xref ref-type="fig" rid="fig3">Figure 3</xref> of the main text). Such a model reduction is obtained by (1) exploiting the circular symmetry in the system; (2) averaging weight changes across different speeds and moving directions; (3) writing dynamical equations in terms of convolutions and cross-correlations. With these methods, we derive a non-linear dynamical system for the weight changes as a function of head direction. Finally, we simulate this dynamical system and inspect how the different variables interact to obtain the final weights. We find that the reduced model results in nearly identical connectivity and learning dynamics to the full network in the main text, and explains how the latter assigns learning errors to the correct weights. Furthermore, it drastically reduces simulation times by two orders of magnitude.</p><p>Note that in this section we use slightly different notation compared to the main text. Notably, we refer to the recurrent head direction weight matrix simply as <inline-formula><mml:math id="inf406"><mml:mi>W</mml:mi></mml:math></inline-formula> (omitting the superscript <italic>rec</italic>), and use capital letters for functions of time and small letters for functions of heading direction.</p><p>We study the learning equation (see <xref ref-type="disp-formula" rid="equ12 equ13 equ14 equ15 equ16">Equation 12–16</xref> in the main text where the low-pass filtering with time constant <inline-formula><mml:math id="inf407"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula> has been ignored, since we find that the value of <inline-formula><mml:math id="inf408"><mml:msub><mml:mi>τ</mml:mi><mml:mi>δ</mml:mi></mml:msub></mml:math></inline-formula> is not important for learning (see Appendix 4))<disp-formula id="equ24"><label>(24)</label><mml:math id="m24"><mml:mrow><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mpadded width="+1.7pt"><mml:mi>η</mml:mi></mml:mpadded><mml:mo>⁢</mml:mo><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ25"><label>(25)</label><mml:math id="m25"><mml:mrow><mml:mrow><mml:msub><mml:mi>E</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is the pre-synaptic error at the i-th cell and<disp-formula id="equ26"><label>(26)</label><mml:math id="m26"><mml:mrow><mml:mrow><mml:msub><mml:mi>P</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mi>a</mml:mi></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is the post-synaptic potential at HD cell <inline-formula><mml:math id="inf409"><mml:mi>j</mml:mi></mml:math></inline-formula>, and <inline-formula><mml:math id="inf410"><mml:mi>H</mml:mi></mml:math></inline-formula> is a temporal filter (with time constants <inline-formula><mml:math id="inf411"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf412"><mml:msub><mml:mi>τ</mml:mi><mml:mi>l</mml:mi></mml:msub></mml:math></inline-formula>, see <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> of the main text).</p><sec sec-type="appendix" id="s12-1"><title>Clockwise movement</title><p>Assuming that the head turns clockwise (which equals to rightward rotation, i.e. rotation towards decreasing angles) and anti-clockwise (leftward, i.e. towards increasing angles) with equal probability, we can approximate the weight dynamics by summing the average weight change <inline-formula><mml:math id="inf413"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> for clockwise movement and the average weight change <inline-formula><mml:math id="inf414"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:msubsup></mml:math></inline-formula> for anti-clockwise movement:<disp-formula id="equ27"><label>(27)</label><mml:math id="m27"><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>-</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We start by assuming head movement at constant speed and we later generalize the results for multiple speeds. We compute the expected weight change <inline-formula><mml:math id="inf415"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> for one lap in the clockwise direction at speed <inline-formula><mml:math id="inf416"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula>:<disp-formula id="equ28"><label>(28)</label><mml:math id="m28"><mml:mrow><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:msubsup><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>τ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ29"><label>(29)</label><mml:math id="m29"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is the post-synaptic potential for clockwise movement, and<disp-formula id="equ30"><label>(30)</label><mml:math id="m30"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is the error for a clockwise movement. Assuming that the axon-proximal voltage is at steady state (<xref ref-type="disp-formula" rid="equ4">Equation 4</xref> of the main text with the l.h.s. set to zero and <inline-formula><mml:math id="inf417"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>x</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:math></inline-formula> absorbed into <inline-formula><mml:math id="inf418"><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula>), the clockwise axon-proximal voltage reads<disp-formula id="equ31"><label>(31)</label><mml:math id="m31"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where (see <xref ref-type="disp-formula" rid="equ11">Equation 11</xref> of the main text)<disp-formula id="equ32"><label>(32)</label><mml:math id="m32"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>From <xref ref-type="disp-formula" rid="equ2 equ3">Equation 2 and 3</xref> of the main text, we can write the axon-distal voltage <inline-formula><mml:math id="inf419"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> as a low-pass filtered version of the total axon-distal current <inline-formula><mml:math id="inf420"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> for clockwise movement (see also <xref ref-type="disp-formula" rid="equ14">Equation 14</xref> of the main text):<disp-formula id="equ33"><label>(33)</label><mml:math id="m33"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>which yields<disp-formula id="equ34"><label>(34)</label><mml:math id="m34"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Importantly, the visual input <inline-formula><mml:math id="inf421"><mml:msup><mml:mi>I</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:math></inline-formula> is translation invariant:<disp-formula id="equ35"><label>(35)</label><mml:math id="m35"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf422"><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub></mml:math></inline-formula> and <inline-formula><mml:math id="inf423"><mml:msub><mml:mi>θ</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math></inline-formula> are the preferred head directions of the j-th and i-th cell, respectively. As a result of this translation invariance, the recurrent weight matrix <inline-formula><mml:math id="inf424"><mml:mi>W</mml:mi></mml:math></inline-formula> develops circular symmetry:<disp-formula id="equ36"><label>(36)</label><mml:math id="m36"><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:msub></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf425"><mml:msub><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msub></mml:math></inline-formula> is the number of HD cells in the system. Consequently, the post-synaptic potential <inline-formula><mml:math id="inf426"><mml:msubsup><mml:mi>P</mml:mi><mml:mi>j</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> is also translation invariant:<disp-formula id="equ37"><label>(37)</label><mml:math id="m37"><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mi>P</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mfrac><mml:mover><mml:mrow><mml:mover><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>⏞</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi><mml:mo>:=</mml:mo></mml:mstyle></mml:mrow></mml:mover><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mfrac><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>In this case, without loss of generality, we can rewrite <xref ref-type="disp-formula" rid="equ28">Equation 28</xref> for a single row of the matrix <inline-formula><mml:math id="inf427"><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup></mml:mrow></mml:math></inline-formula> as a function of the angle difference <inline-formula><mml:math id="inf428"><mml:mrow><mml:mi>θ</mml:mi><mml:mo>:=</mml:mo><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mn>0</mml:mn></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula>:<disp-formula id="equ38"><label>(38)</label><mml:math id="m38"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msub></mml:mrow></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>τ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>j</mml:mi><mml:mo>-</mml:mo><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msub></mml:mrow><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ39"><label>(39)</label><mml:math id="m39"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>η</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>τ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ40"><label>(40)</label><mml:math id="m40"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>φ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>E</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>P</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ41"><label>(41)</label><mml:math id="m41"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>φ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ42"><label>(42)</label><mml:math id="m42"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ43"><label>(43)</label><mml:math id="m43"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mo>=</mml:mo><mml:mo>:</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where we defined <inline-formula><mml:math id="inf429"><mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf430"><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and <inline-formula><mml:math id="inf431"><mml:mo>⋆</mml:mo></mml:math></inline-formula> denotes circular cross-correlation.</p><p>From <xref ref-type="disp-formula" rid="equ29">Equation 29</xref>, we derive<disp-formula id="equ44"><label>(44)</label><mml:math id="m44"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msubsup><mml:mi>P</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ45"><label>(45)</label><mml:math id="m45"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>β</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo movablelimits="false" stretchy="false">|</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup><mml:mo movablelimits="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>H</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo movablelimits="false">/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="142%">h</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">β</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>a</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo movablelimits="false">-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow><mml:mo movablelimits="false">/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="142%">v</mml:mi><mml:mrow><mml:mi mathsize="140%">a</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mrow><mml:mi mathsize="142%">φ</mml:mi><mml:mo mathsize="142%" stretchy="false">-</mml:mo><mml:mi mathsize="142%">β</mml:mi></mml:mrow><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ46"><label>(46)</label><mml:math id="m46"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The approximation in <xref ref-type="disp-formula" rid="equ45">Equation 45</xref> holds when a bump exists in the network and moves with a velocity below the velocity limit, and it is valid if the temporal filter <inline-formula><mml:math id="inf432"><mml:mi>H</mml:mi></mml:math></inline-formula> is shorter than <inline-formula><mml:math id="inf433"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, that is for <inline-formula><mml:math id="inf434"><mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf435"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which holds for the filtering time constants and velocity distribution we assumed (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1</xref>). Therefore, plugging <xref ref-type="disp-formula" rid="equ46">Equation 46</xref> into <xref ref-type="disp-formula" rid="equ43">Equation 43</xref>, we obtain:<disp-formula id="equ47"><label>(47)</label><mml:math id="m47"><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><fig id="app5fig1" position="float"><label>Appendix 5—figure 1.</label><caption><title>Left: assumed distribution of head-turning speeds (black) and discrete approximation used for the simulations.</title><p>The colored vertical lines indicate speeds for which the filter <inline-formula><mml:math id="inf436"><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> is plotted in the right panel. Right: temporal filter <inline-formula><mml:math id="inf437"><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> for several example speeds (see vertical lines in the left panel). Note that even for the largest speeds (blue curve) the filter decays within one turn around the circle.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app5-fig1-v2.tif"/></fig><p>By using the definition of <inline-formula><mml:math id="inf438"><mml:mrow><mml:mi>ϵ</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> we derive<disp-formula id="equ48"><label>(48)</label><mml:math id="m48"><mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msubsup><mml:mi>E</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>a</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo movablelimits="false">/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="142%">v</mml:mi><mml:mrow><mml:mi mathsize="140%">a</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">φ</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo movablelimits="false">/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="142%">v</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="140%">s</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="140%">s</mml:mi></mml:mrow><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">φ</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>with (<xref ref-type="disp-formula" rid="equ31">Equation 31</xref>)<disp-formula id="equ49"><label>(49)</label><mml:math id="m49"><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mfrac><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>v</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>i</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo movablelimits="false">/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo movablelimits="false">+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo>=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo>:</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi mathsize="142%">I</mml:mi><mml:mo mathsize="142%" stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi mathsize="140%">v</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="140%">i</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="140%">s</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">φ</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>and (<xref ref-type="disp-formula" rid="equ34">Equation 34</xref>)<disp-formula id="equ50"><label>(50)</label><mml:math id="m50"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ51"><label>(51)</label><mml:math id="m51"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>β</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo movablelimits="false" stretchy="false">|</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup><mml:mo movablelimits="false" stretchy="false">|</mml:mo></mml:mrow></mml:mfrac></mml:mstyle><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>H</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mrow><mml:mi>β</mml:mi><mml:mo movablelimits="false">/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mrow><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="142%">h</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">β</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo>⁢</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mo movablelimits="false">+</mml:mo></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false">(</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>φ</mml:mi><mml:mo movablelimits="false">-</mml:mo><mml:mi>β</mml:mi></mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mfrac></mml:mstyle><mml:mo movablelimits="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="142%">d</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mrow><mml:mi mathsize="142%">φ</mml:mi><mml:mo mathsize="142%" stretchy="false">-</mml:mo><mml:mi mathsize="142%">β</mml:mi></mml:mrow><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:munder></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ52"><label>(52)</label><mml:math id="m52"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>≈</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The approximation in <xref ref-type="disp-formula" rid="equ52">Equation 52</xref> is valid if the temporal filter <inline-formula><mml:math id="inf439"><mml:mi>H</mml:mi></mml:math></inline-formula> is shorter than <inline-formula><mml:math id="inf440"><mml:mrow><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula>, which again holds true for our parameter choices (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1</xref>).</p><sec sec-type="appendix" id="s12-1-1"><title>Calculation of the axon-distal input</title><p>Let us compute the axon-distal current <inline-formula><mml:math id="inf441"><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup></mml:math></inline-formula> to the i-th neuron for clockwise movement. From <xref ref-type="disp-formula" rid="equ2">Equation 2</xref> of the main text, setting the l.h.s. to zero, and splitting the rotation-cell activities in the two populations (L-HR and R-HR), we derive<disp-formula id="equ53"><label>(53)</label><mml:math id="m53"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mi>i</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>f</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false" stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathsize="142%">D</mml:mi><mml:mi mathsize="140%">i</mml:mi><mml:mrow><mml:mrow><mml:mi mathsize="140%">r</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="140%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="140%">c</mml:mi></mml:mrow><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">t</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>R</mml:mi></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>f</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false" stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathsize="142%">D</mml:mi><mml:mi mathsize="140%">i</mml:mi><mml:mrow><mml:mi mathsize="140%">R</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">t</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:munder><mml:mo largeop="true" movablelimits="false" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:munder><mml:mrow><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mi>L</mml:mi></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow><mml:mo movablelimits="false">⁢</mml:mo><mml:mi>f</mml:mi><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false" stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:=</mml:mo><mml:mrow><mml:msubsup><mml:mi mathsize="142%">D</mml:mi><mml:mi mathsize="140%">i</mml:mi><mml:mrow><mml:mi mathsize="140%">L</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mi mathsize="142%">t</mml:mi><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf442"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mtext>R</mml:mtext></mml:msubsup></mml:math></inline-formula> (<inline-formula><mml:math id="inf443"><mml:msubsup><mml:mi>W</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow><mml:mtext>L</mml:mtext></mml:msubsup></mml:math></inline-formula>) are the weights from the right (left) rotation cells, and <inline-formula><mml:math id="inf444"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> (<inline-formula><mml:math id="inf445"><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>) are the voltages of the right (left) rotation cells (see <xref ref-type="disp-formula" rid="equ8 equ9 equ10">Equation 8–10</xref> of the main text):<disp-formula id="equ54"><label>(54)</label><mml:math id="m54"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ55"><label>(55)</label><mml:math id="m55"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mi/></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mi mathvariant="normal">∞</mml:mi></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mtext>s</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mpadded></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>The function <inline-formula><mml:math id="inf446"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>S</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>t</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mrow><mml:mi>exp</mml:mi><mml:mo>⁡</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>/</mml:mo><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> is a temporal low pass filter with time constant <inline-formula><mml:math id="inf447"><mml:msub><mml:mi>τ</mml:mi><mml:mi>s</mml:mi></mml:msub></mml:math></inline-formula> and the velocity input reads (<xref ref-type="disp-formula" rid="equ10">Equation 10</xref> of the main text)<disp-formula id="equ56"><label>(56)</label><mml:math id="m56"><mml:mrow><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>:=</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p><xref ref-type="disp-formula" rid="equ54">Equation 54</xref> and <xref ref-type="disp-formula" rid="equ55">Equation 55</xref> show that the rotation-cell voltages are re-scaled and filtered versions of the corresponding HD-cell firing rates with a baseline shift <inline-formula><mml:math id="inf448"><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> that is differentially applied to right and left rotation cells.</p><p>From <xref ref-type="disp-formula" rid="equ53">Equation 53</xref>, we derive<disp-formula id="equ57"><label>(57)</label><mml:math id="m57"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>φ</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>φ</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>φ</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mfrac><mml:mi>φ</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfrac><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Assuming a large number <inline-formula><mml:math id="inf449"><mml:msub><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msub></mml:math></inline-formula> of HD cells evenly spaced around the circle, the recurrent axon-distal input reads<disp-formula id="equ58"><label>(58)</label><mml:math id="m58"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mrow><mml:mi>r</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>c</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>φ</mml:mi><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mfrac></mml:mstyle><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:msub><mml:mo largeop="true" symmetric="true">∑</mml:mo><mml:mi>j</mml:mi></mml:msub></mml:mstyle><mml:mrow><mml:msub><mml:mi>W</mml:mi><mml:mrow><mml:mn>0</mml:mn><mml:mo>⁢</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>φ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ59"><label>(59)</label><mml:math id="m59"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HD</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>θ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="160%" minsize="160%">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>a</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msubsup><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false">(</mml:mo><mml:mstyle displaystyle="false"><mml:mfrac><mml:mrow><mml:mi>φ</mml:mi><mml:mo movablelimits="false">+</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup></mml:mfrac></mml:mstyle><mml:mo movablelimits="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:mrow><mml:msup><mml:mi mathsize="142%">v</mml:mi><mml:mrow><mml:mi mathsize="140%">a</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="142%" minsize="142%">(</mml:mo><mml:mrow><mml:mi mathsize="142%">φ</mml:mi><mml:mo mathsize="142%" stretchy="false">+</mml:mo><mml:mi mathsize="142%">θ</mml:mi></mml:mrow><mml:mo maxsize="142%" minsize="142%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:munder><mml:mo maxsize="160%" minsize="160%">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ60"><label>(60)</label><mml:math id="m60"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mpadded width="+1.7pt"><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HD</mml:mtext></mml:msub></mml:mpadded><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf450"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HD</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>HD</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the density of the HD neurons around the circle and we used the fact that the axon-proximal voltage is translation invariant (see also <xref ref-type="disp-formula" rid="equ37">Equation 37</xref>):<disp-formula id="equ61"><label>(61)</label><mml:math id="m61"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mi>j</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Following a similar procedure for <inline-formula><mml:math id="inf451"><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula> and <inline-formula><mml:math id="inf452"><mml:msubsup><mml:mi>D</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup></mml:math></inline-formula>, we obtain:<disp-formula id="equ62"><label>(62)</label><mml:math id="m62"><mml:mrow><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HD</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf453"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>N</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mrow></mml:math></inline-formula> is the density of the HR neurons for one particular turning direction (note that we assumed <inline-formula><mml:math id="inf454"><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HD</mml:mtext></mml:msub></mml:mrow></mml:mrow></mml:math></inline-formula> in the main text). In deriving <xref ref-type="disp-formula" rid="equ62">Equation 62</xref> we defined<disp-formula id="equ63"><label>(63)</label><mml:math id="m63"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ64"><label>(64)</label><mml:math id="m64"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:msubsup><mml:mi>V</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where we defined the filter <inline-formula><mml:math id="inf455"><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, and the approximations are valid if <inline-formula><mml:math id="inf456"><mml:mrow><mml:mrow><mml:msub><mml:mi>H</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≪</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></inline-formula> for <inline-formula><mml:math id="inf457"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>t</mml:mi><mml:mo>&gt;</mml:mo><mml:mn>2</mml:mn><mml:mi>π</mml:mi><mml:mrow><mml:mo>/</mml:mo></mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula>, which holds true for the time constant and velocity distribution assumed in the main text.</p><p>Finally, we compute the rotation-cells’ weights change. For these weights, the learning rule is the same as the one for the recurrent connections, except that the post-synaptic HD input is replaced by the post-synaptic HR input. Therefore, following the same procedure as in <xref ref-type="disp-formula" rid="equ38">Equation 38</xref>–<xref ref-type="disp-formula" rid="equ46">Equation 46</xref>, the rotation weight changes are given by:<disp-formula id="equ65"><label>(65)</label><mml:math id="m65"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ66"><label>(66)</label><mml:math id="m66"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>In summary, for clockwise movement, we obtain the following system of equations:<disp-formula id="equ67"><label>(67)</label><mml:math id="m67"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HD</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msubsup><mml:mi>h</mml:mi><mml:mi>s</mml:mi><mml:mo>+</mml:mo></mml:msubsup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup><mml:mo movablelimits="false">∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msup><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:msup><mml:mi mathsize="142%">p</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup><mml:mo movablelimits="false">∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msup><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:msup><mml:mi mathsize="142%">p</mml:mi><mml:msup><mml:mi mathsize="140%">R</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:msup></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:munder><mml:munder accentunder="true"><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:msup><mml:mo movablelimits="false">∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo movablelimits="false">⁢</mml:mo><mml:mrow><mml:mo movablelimits="false" stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo movablelimits="false">+</mml:mo></mml:mrow></mml:msup><mml:mo movablelimits="false" stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo movablelimits="false">⏟</mml:mo></mml:munder><mml:mrow><mml:mo mathsize="142%" stretchy="false">=</mml:mo><mml:mo>⁣</mml:mo><mml:mrow><mml:mi/><mml:mo mathsize="142%" stretchy="false">:</mml:mo><mml:msup><mml:mi mathsize="142%">p</mml:mi><mml:mrow><mml:mi mathsize="140%">L</mml:mi><mml:mo mathsize="140%" stretchy="false">+</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:mrow></mml:munder><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec></sec><sec sec-type="appendix" id="s12-2"><title>Anti-clockwise movement</title><p>We now consider anticlockwise movements with speed <inline-formula><mml:math id="inf458"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula>. First we note that the temporal filter<disp-formula id="equ68"><label>(68)</label><mml:math id="m68"><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>:=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mi>H</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is a mirrored version about the origin of its clockwise counterpart <inline-formula><mml:math id="inf459"><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula>, whereas the visual input is unchanged because it is symmetric around the origin (see <xref ref-type="disp-formula" rid="equ5">Equation 5</xref> of the main text)<disp-formula id="equ69"><label>(69)</label><mml:math id="m69"><mml:mrow><mml:mrow><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msubsup><mml:mi>I</mml:mi><mml:mn>0</mml:mn><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msubsup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi><mml:mo>/</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Let us first assume that<disp-formula id="equ70"><label>(70)</label><mml:math id="m70"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>we shall verify the validity of this assumption self-consistently at the end of this section. From <xref ref-type="disp-formula" rid="equ68">Equation 68</xref>–<xref ref-type="disp-formula" rid="equ70">Equation 70</xref> it follows that <inline-formula><mml:math id="inf460"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msup><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula> is a mirrored version of <inline-formula><mml:math id="inf461"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>, that is,<disp-formula id="equ71"><label>(71)</label><mml:math id="m71"><mml:mrow><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>and, as a result,<disp-formula id="equ72"><label>(72)</label><mml:math id="m72"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>We now compute the anticlockwise weight change for the recurrent weights<disp-formula id="equ73"><label>(73)</label><mml:math id="m73"><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi>η</mml:mi><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>The r.h.s. of <xref ref-type="disp-formula" rid="equ73">Equation 73</xref>, without the <inline-formula><mml:math id="inf462"><mml:mrow><mml:mi>η</mml:mi><mml:mo>/</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> pre-factor reads:<disp-formula id="equ74"><label>(74)</label><mml:math id="m74"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>τ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mi>τ</mml:mi><mml:mo>+</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ75"><label>(75)</label><mml:math id="m75"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>τ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mi>τ</mml:mi></mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo>+</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ76"><label>(76)</label><mml:math id="m76"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>τ</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>τ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:msubsup><mml:mo largeop="true" symmetric="true">∫</mml:mo><mml:mn>0</mml:mn><mml:mrow><mml:mn>2</mml:mn><mml:mo>⁢</mml:mo><mml:mi>π</mml:mi></mml:mrow></mml:msubsup></mml:mstyle><mml:mrow><mml:mrow><mml:mo rspace="0pt">d</mml:mo><mml:mpadded width="+1.7pt"><mml:mi>s</mml:mi></mml:mpadded></mml:mrow><mml:mo>⁢</mml:mo><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>τ</mml:mi><mml:mo>-</mml:mo><mml:mi>θ</mml:mi><mml:mo>-</mml:mo><mml:mi>s</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ77"><label>(77)</label><mml:math id="m77"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mo stretchy="false">{</mml:mo><mml:mrow><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⋆</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msup><mml:mi>h</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">}</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>where from <xref ref-type="disp-formula" rid="equ75">Equation 75</xref> to <xref ref-type="disp-formula" rid="equ76">Equation 76</xref> we used variable substitution. Therefore, the weight change for clockwise movement is the mirrored version around the origin of the weight change for anticlockwise movement:<disp-formula id="equ78"><label>(78)</label><mml:math id="m78"><mml:mrow><mml:mrow><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>meaning that, with learning, the recurrent weights develop into an even function:<disp-formula id="equ79"><label>(79)</label><mml:math id="m79"><mml:mrow><mml:mrow><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Let us now study the anticlockwise weight change for the rotation weights. The rotation-cell voltages during anticlockwise movement read:<disp-formula id="equ80"><label>(80)</label><mml:math id="m80"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>-</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ81"><label>(81)</label><mml:math id="m81"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:msub><mml:mi>h</mml:mi><mml:mi>s</mml:mi></mml:msub><mml:mo>∗</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>Using <xref ref-type="disp-formula" rid="equ71">Equation 71</xref> in <xref ref-type="disp-formula" rid="equ80">Equation 80</xref> and <xref ref-type="disp-formula" rid="equ81">Equation 81</xref> we find<disp-formula id="equ82"><label>(82)</label><mml:math id="m82"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula><disp-formula id="equ83"><label>(83)</label><mml:math id="m83"><mml:mrow><mml:mtable columnalign="right left" rowspacing="4pt" columnspacing="1em"><mml:mtr><mml:mtd><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p><p>Therefore, applying the same procedure outlined in <xref ref-type="disp-formula" rid="equ73">Equation 73</xref>–<xref ref-type="disp-formula" rid="equ77">Equation 77</xref>, to the anticlockwise change in the rotation weights yields<disp-formula id="equ84"><label>(84)</label><mml:math id="m84"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula><disp-formula id="equ85"><label>(85)</label><mml:math id="m85"><mml:mtable columnspacing="5pt" displaystyle="true"><mml:mtr><mml:mtd columnalign="right"><mml:mrow><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:mtd><mml:mtd/></mml:mtr></mml:mtable></mml:math></disp-formula></p><p>meaning that, during learning, the right and left rotation weights develop mirror symmetry:<disp-formula id="equ86"><label>(86)</label><mml:math id="m86"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>To verify that our original assumption in <xref ref-type="disp-formula" rid="equ70">Equation 70</xref> holds, we compute the axon-distal input for anticlockwise movement:<disp-formula id="equ87"><label>(87)</label><mml:math id="m87"><mml:mrow><mml:mrow><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HD</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:msub><mml:mi>ρ</mml:mi><mml:mtext>HR</mml:mtext></mml:msub><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>-</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo stretchy="false">]</mml:mo></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mpadded width="+1.7pt"><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HD</mml:mtext></mml:msubsup></mml:mpadded></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Using <xref ref-type="disp-formula" rid="equ71 equ79 equ80 equ81 equ86">Equation 71, 79, 80, 81, 86</xref> in <xref ref-type="disp-formula" rid="equ87">Equation 87</xref>, yields<disp-formula id="equ88"><label>(88)</label><mml:math id="m88"><mml:mrow><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>HD</mml:mtext></mml:mrow></mml:msub><mml:mi>w</mml:mi><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msub><mml:mi>ρ</mml:mi><mml:mrow><mml:mtext>HR</mml:mtext></mml:mrow></mml:msub><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msup><mml:mo>⋆</mml:mo><mml:mi>f</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>h</mml:mi><mml:mi>i</mml:mi><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mtext>HD</mml:mtext></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:msup><mml:mi>d</mml:mi><mml:mrow><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mo>−</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mspace width="thinmathspace"/><mml:mo>.</mml:mo></mml:mrow></mml:math></disp-formula></p><p>Finally, using <xref ref-type="disp-formula" rid="equ78 equ84 equ85">Equation 78, 84, and 85</xref>, the total synaptic weight changes for both clockwise and anticlockwise movement read<disp-formula id="equ89"><label>(89)</label><mml:math id="m89"><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnspacing="5pt" displaystyle="true" rowspacing="0pt"><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd columnalign="left"><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mtd><mml:mtd columnalign="left"><mml:mrow><mml:mrow><mml:mi/><mml:mo>=</mml:mo><mml:mrow><mml:mstyle displaystyle="false"><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>t</mml:mi></mml:mrow></mml:mfrac></mml:mstyle><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>θ</mml:mi></mml:mrow><mml:mo rspace="4.2pt" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow><mml:mo>.</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:mrow></mml:math></disp-formula></p></sec><sec sec-type="appendix" id="s12-3"><title>Averaging across speeds</title><p>So far, we have only considered head turnings at a fixed speed <inline-formula><mml:math id="inf463"><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> (clockwise) and <inline-formula><mml:math id="inf464"><mml:mrow><mml:msup><mml:mi>v</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo>=</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:math></inline-formula> (anticlockwise). However, in the full model described in the main text, velocities are sampled stochastically from an OU process. This random process generates a half-normal distribution of speeds with spread <inline-formula><mml:math id="inf465"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula> (<xref ref-type="fig" rid="app5fig1">Appendix 5—figure 1</xref>, left, see also <xref ref-type="table" rid="table1">Table 1</xref> in the main text). We thus compute the expected weight changes with respect to this speed distribution:<disp-formula id="equ90"><label>(90)</label><mml:math id="m90"><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mtable columnalign="right center" rowspacing="0.9em 0.9em 0.4em" columnspacing="1em"><mml:mtr><mml:mtd><mml:mfrac><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:mi>w</mml:mi></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>:=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>:=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>R</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:mo fence="false" stretchy="false">⟨</mml:mo><mml:mrow><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msup></mml:mrow><mml:msub><mml:mo fence="false" stretchy="false">⟩</mml:mo><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd><mml:mtd><mml:mo>:=</mml:mo><mml:msubsup><mml:mo>∫</mml:mo><mml:mrow><mml:mn>0</mml:mn></mml:mrow><mml:mrow><mml:mi mathvariant="normal">∞</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>v</mml:mi><mml:mspace width="thinmathspace"/><mml:mi>p</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mfrac><mml:mi mathvariant="normal">d</mml:mi><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:mfrac><mml:msubsup><mml:mi>w</mml:mi><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi></mml:mrow></mml:msubsup><mml:mo stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mtd></mml:mtr></mml:mtable><mml:mo fence="true" stretchy="true" symmetric="true"/></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>where <italic>w</italic><sub><italic>v</italic></sub> is the weight change for speed <inline-formula><mml:math id="inf466"><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>+</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mo stretchy="false">|</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mo>-</mml:mo></mml:msup><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>v</mml:mi></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf467"><mml:mrow><mml:mi>p</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>v</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is an half-normal distribution with spread <inline-formula><mml:math id="inf468"><mml:mrow><mml:msub><mml:mi>σ</mml:mi><mml:mi>v</mml:mi></mml:msub><mml:mo>/</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:math></inline-formula>.</p></sec><sec sec-type="appendix" id="s12-4"><title>Simulation of the reduced model</title><p>In this section, we show the dynamics of the reduced model numerically simulated according to <xref ref-type="disp-formula" rid="equ67 equ89 equ90">Equation 67, 89, and 90</xref>. Weight changes are computed at discrete time steps and integrated using the forward Euler method. At each time step we compute the weight changes for each speed <inline-formula><mml:math id="inf469"><mml:mi>v</mml:mi></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ67">Equation 67</xref> and <xref ref-type="disp-formula" rid="equ89">Equation 89</xref>) and we estimate the expected weight change according to <xref ref-type="disp-formula" rid="equ90">Equation 90</xref>. We then update the weights and proceed to the next step of the simulation. Note that <xref ref-type="disp-formula" rid="equ67">Equation 67</xref> requires the firing rates of HD and HR cells at the previous time step (recurrent input, first line of <xref ref-type="disp-formula" rid="equ67">Equation 67</xref>). Therefore, at each time step, we save the HD and HR firing rates for every speed value <inline-formula><mml:math id="inf470"><mml:mi>v</mml:mi></mml:math></inline-formula> and provide them as input to the next iteration of the simulation.</p><p><xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2</xref> shows the evolution of the reduced system for 400 time steps, starting from an initial condition where all weights are zero. One can see that from time steps 75–100 the system switches from a linear regime (HD firing rates below saturation, see top panel) to a non-linear regime (saturated HD rates). Such a switch is accompanied by peaks in the average absolute error (third panel from the top). Notably, the rotation weights start developing a structure only after such switch has occurred (see two bottom panels)—a feature that has been observed also in the full model (<xref ref-type="fig" rid="fig3">Figure 3E</xref> of the main text).</p><fig id="app5fig2" position="float"><label>Appendix 5—figure 2.</label><caption><title>Evolution of the reduced model.</title><p>The figure shows from top to bottom: (<bold>A</bold>) the HD-cells’ firing rate <inline-formula><mml:math id="inf471"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>a</mml:mi><mml:mo>+</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula>; (<bold>B</bold>) the error <inline-formula><mml:math id="inf472"><mml:mi>ϵ</mml:mi></mml:math></inline-formula>; (<bold>C</bold>) the average absolute error; (<bold>D</bold>) the recurrent weights <inline-formula><mml:math id="inf473"><mml:mi>w</mml:mi></mml:math></inline-formula>; (<bold>E–F</bold>) the rotation weights <inline-formula><mml:math id="inf474"><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf475"><mml:msup><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:math></inline-formula>. The HD firing rate and the errors (panels A-C) are averaged across speeds and both movement directions. The vertical dashed lines denote the time points shown in and <xref ref-type="fig" rid="app5fig4">Appendix 5—figure 4</xref>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app5-fig2-v2.tif"/></fig><fig id="app5fig3" position="float"><label>Appendix 5—figure 3.</label><caption><title>Development of the recurrent weights.</title><p>The figure provides an intuition for the shape of the recurrent-weights profiles that emerge during learning. Each column refers to a different time step (see also dashed lines in <xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2</xref>). Each row shows a different set of variables of the model (see legends in the first column). The figure is to be read from top to bottom, because variables in the lower rows are computed from variables in the upper rows. Blue (orange) lines always refer to clockwise (anticlockwise) motion. Black lines in C show the total weight changes for both clockwise and anti-clockwise motion, that is, <inline-formula><mml:math id="inf476"><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow><mml:mo>+</mml:mo><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app5-fig3-v2.tif"/></fig><fig id="app5fig4" position="float"><label>Appendix 5—figure 4.</label><caption><title>Development of the rotation weights.</title><p>The figure provides an intuition for the shape of the rotation-weights profiles that emerge during learning. Each column refers to a different time step (see also dashed lines in <xref ref-type="fig" rid="app5fig2">Appendix 5—figure 2</xref>). Each row shows a different set of variables of the model (see legends in the first column). The figure is to be read from top to bottom, because variables in the lower rows are computed from variables in the upper rows. Blue (orange) lines always refer to clockwise (anticlockwise) motion.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-app5-fig4-v2.tif"/></fig><sec sec-type="appendix" id="s12-4-1"><title>Development of the recurrent weights</title><p><xref ref-type="fig" rid="app5fig3">Appendix 5—figure 3</xref> provides an intuition for the shape of the recurrent-weights profiles that emerge during learning. The first column shows the evolution of the recurrent weights in the linear regime (<inline-formula><mml:math id="inf477"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>), that is, before the HD rates reach saturation. In this regime, both recurrent and rotation weights are small, and the steady-state axon-distal rate<disp-formula id="equ91"><label>(91)</label><mml:math id="m91"><mml:mrow><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mrow><mml:mi>s</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>≈</mml:mo><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mfrac><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mrow><mml:msub><mml:mi>g</mml:mi><mml:mi>D</mml:mi></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mi>g</mml:mi><mml:mi>L</mml:mi></mml:msub></mml:mrow></mml:mfrac><mml:mo>⁢</mml:mo><mml:msub><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>is flat and close to zero. Therefore, the HD output rate <inline-formula><mml:math id="inf478"><mml:mrow><mml:mi>f</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>v</mml:mi><mml:mi>a</mml:mi></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> is dominated by the visual input <inline-formula><mml:math id="inf479"><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ67">Equation 67</xref>, third line), which has the shape of a localized bump (panel A1). Thus the error <inline-formula><mml:math id="inf480"><mml:mi>ϵ</mml:mi></mml:math></inline-formula> has also the shape of a bump (B1). Additionally, the post-synaptic inputs <inline-formula><mml:math id="inf481"><mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf482"><mml:msup><mml:mi>p</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> are shifted and filtered versions of this bump (<xref ref-type="disp-formula" rid="equ67">Equation 67</xref>, seventh line). The recurrent weight changes <inline-formula><mml:math id="inf483"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf484"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> for clockwise and anticlockwise movement are given by the cross-correlation of the errors <inline-formula><mml:math id="inf485"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf486"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> with the post-synaptic inputs <inline-formula><mml:math id="inf487"><mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf488"><mml:msup><mml:mi>p</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> (panel C1; see <xref ref-type="disp-formula" rid="equ67">Equation 67</xref> seventh line and <xref ref-type="disp-formula" rid="equ73">Equation 73</xref>). Note that because <inline-formula><mml:math id="inf489"><mml:mrow><mml:mrow><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>⋆</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>=</mml:mo><mml:mrow><mml:mrow><mml:mrow><mml:mi>a</mml:mi><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mo>-</mml:mo><mml:mi>x</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo>∗</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mo>⁢</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>x</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></inline-formula>, the operation of cross-correlation can be understood graphically as a convolution between the mirrored first function <inline-formula><mml:math id="inf490"><mml:mi>a</mml:mi></mml:math></inline-formula> and the second function <inline-formula><mml:math id="inf491"><mml:mi>b</mml:mi></mml:math></inline-formula>. Such a mirroring is irrelevant in C1 (linear regime) because the error is an even function, but becomes important in C2 (non-linear regime). As a result of this cross-correlation, the recurrent recurrent-weight changes <inline-formula><mml:math id="inf492"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf493"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:mrow></mml:math></inline-formula> are shifted bumps (colored lines in C1), which merge into a single central bump after summing clockwise and anticlockwise contributions (black line in C1). Therefore, in the linear regime, the recurrent weights develop a single central peak in the origin (panel D1).</p><p>The second column of shows the development of the recurrent weights in the non-linear regime (time step 350). Panel A2 shows that in this scenario the HD firing-rate bumps are broader and approach saturation due to the strong recurrent input. The coupling between the axon-distal and axon-proximal compartment acts as a self-amplifying signal during learning which results in the activity of all active neurons participating in the bump reaching saturation. Additionally, because the recurrent input is filtered in time (<xref ref-type="disp-formula" rid="equ67">Equation 67</xref>, second line), such bumps are also shifted towards the direction of movement. Importantly, due to the lack of visual input, within the receptive field the steady-state axon-distal rates are always smaller than the firing rates. As a result, the errors <inline-formula><mml:math id="inf494"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf495"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> show small negative bumps in the direction of movement, and small positive bumps in the opposite direction (panel B2). Additionally, the post-synaptic inputs <inline-formula><mml:math id="inf496"><mml:msup><mml:mi>p</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf497"><mml:msup><mml:mi>p</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> shift further apart from the origin. Consequently, the total weight change <inline-formula><mml:math id="inf498"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:math></inline-formula> develops negative peaks around 60 deg (black line in C2, contrast to panel C1), and these peaks get imprinted in the final recurrent weights’ profiles (panel D2).</p></sec><sec sec-type="appendix" id="s12-4-2"><title>Development of the rotation weights</title><p><xref ref-type="fig" rid="app5fig4">Appendix 5—figure 4</xref> provides an intuitive explanation for the shape of the rotation-weights profiles <inline-formula><mml:math id="inf499"><mml:msup><mml:mi>w</mml:mi><mml:mi>R</mml:mi></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf500"><mml:msup><mml:mi>w</mml:mi><mml:mi>L</mml:mi></mml:msup></mml:math></inline-formula> that emerge during learning. The first column shows the evolution of the rotation weights in the linear regime (<inline-formula><mml:math id="inf501"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:math></inline-formula>), i.e., before the HD rates reach saturation. In this regime, the rotation-cell firing rates are filtered versions of the HD bumps but re-scaled by a factor <inline-formula><mml:math id="inf502"><mml:mrow><mml:mrow><mml:msub><mml:mi>A</mml:mi><mml:mtext>active</mml:mtext></mml:msub><mml:mo>/</mml:mo><mml:msub><mml:mi>f</mml:mi><mml:mtext>max</mml:mtext></mml:msub></mml:mrow><mml:mo>≈</mml:mo><mml:mn>0.013</mml:mn></mml:mrow></mml:math></inline-formula> and baseline-shifted by an amount <inline-formula><mml:math id="inf503"><mml:mrow><mml:mrow><mml:mo>±</mml:mo><mml:msub><mml:mover accent="true"><mml:mi>I</mml:mi><mml:mo stretchy="false">¯</mml:mo></mml:mover><mml:mrow><mml:mi>v</mml:mi><mml:mo>⁢</mml:mo><mml:mi>e</mml:mi><mml:mo>⁢</mml:mo><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>+</mml:mo><mml:msubsup><mml:mi>I</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>n</mml:mi><mml:mo>⁢</mml:mo><mml:mi>h</mml:mi><mml:mo>⁢</mml:mo><mml:mi>i</mml:mi><mml:mo>⁢</mml:mo><mml:mi>b</mml:mi></mml:mrow><mml:mtext>HR</mml:mtext></mml:msubsup></mml:mrow></mml:math></inline-formula> (<xref ref-type="disp-formula" rid="equ67">Equation 67</xref> lines 4 and 5; panel A1, compare to panel A1). This baseline shift acts as a switch that determines from which rotation cells population connections will be mainly drawn from, depending on the direction of motion. Panel B1 shows that the errors <inline-formula><mml:math id="inf504"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf505"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> overlap and have the shape of a bump centered at the origin (same curves as in panel B1). Additionally, the post-synaptic potentials <inline-formula><mml:math id="inf506"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>±</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf507"><mml:msup><mml:mi>p</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>±</mml:mo></mml:mrow></mml:msup></mml:math></inline-formula> in B1 are filtered versions of the curves in A1 (<xref ref-type="disp-formula" rid="equ67">Equation 67</xref>, lines 7 and 8). As a result, the weight changes <inline-formula><mml:math id="inf508"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>R</mml:mi><mml:mo>±</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula> and <inline-formula><mml:math id="inf509"><mml:mrow><mml:mi mathvariant="normal">d</mml:mi><mml:mo>⁢</mml:mo><mml:msup><mml:mi>w</mml:mi><mml:mrow><mml:mi>L</mml:mi><mml:mo>±</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:math></inline-formula>, that is, the errors cross-correlated by the post-synaptic potentials, appear similar to the bumps in A1, but they are smoother and further apart from the origin (panel C1). Finally, such weight changes get imprinted in the rotation weights (panel D1).</p><p>The second column shows the evolution of the rotation weights in the non-linear regime (<inline-formula><mml:math id="inf510"><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>350</mml:mn></mml:mrow></mml:math></inline-formula>), that is, after the HD rates reach saturation. In this case, the large recurrent input gives rise to larger rotation rates (A2, compare to A1) and larger post-synaptic potentials (B2, compare to B1). In panel B2, we can see that the errors <inline-formula><mml:math id="inf511"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>+</mml:mo></mml:msup></mml:math></inline-formula> and <inline-formula><mml:math id="inf512"><mml:msup><mml:mi>ϵ</mml:mi><mml:mo>-</mml:mo></mml:msup></mml:math></inline-formula> show positive and negatives peaks shifted from the origin (same curves as in panel B2), which generate weight changes with both positive and negative lobes (panel C2). Such weight changes get finally imprinted in the rotation weights (panel D2).</p></sec></sec></sec></app></app-group></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69841.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>Ecole Normale Superieure Paris</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2021.03.12.435035" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2021.03.12.435035"/></front-stub><body><p>This paper will be of interest to neuroscientists studying the navigation system, and in particular, those who study the ability of animals to path integrate. This study proposes an elegant synaptic plasticity rule that maintains the connectivity required for path integration by integrating visual and self-motion input arriving at different dendritic locations in a neuron. This idea is applied to the central complex of <italic>Drosophila</italic>, a well-characterized experimental system.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69841.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Ostojic</surname><given-names>Srdjan</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05a0dhs15</institution-id><institution>Ecole Normale Superieure Paris</institution></institution-wrap><country>France</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Rouault</surname><given-names>Hervé</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/02vjkv261</institution-id><institution>Inserm</institution></institution-wrap><country>France</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2021.03.12.435035">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2021.03.12.435035v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Learning accurate path integration in ring attractor models of the head direction system&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, and the evaluation has been overseen by a Reviewing Editor and Ronald Calabrese as the Senior Editor. The following individuals involved in review of your submission have agreed to reveal their identity: Hervé Rouault (Reviewer #4).</p><p>The reviewers have discussed their reviews with one another, and believe the manuscript is potentially suitable for publication, provided a number of comments are addressed or discussed. The Reviewing Editor has drafted a merged feedback to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Plasticity:</p><p>– A main question is whether the type of synaptic plasticity proposed by the authors is necessary and operative in the CX. One motivation for the plasticity rule is that, as the authors state, ring attractors &quot;require that synaptic connections are precisely tuned (Hahnloser, 2003). Therefore, if the circuit was completely hardwired, the amount of information that an organism would need to genetically encode connection strenghts would be exceedingly high.&quot; However, in Figure 3 and S3, it is shown that the learnedPI &quot;far outperforms&quot; flies, so the authors add random noise to the synaptic weights in order to make a more realistic model. But this means that the synaptic weights don't actually need to be that precise. Doesn't this mean that the connectivity may in fact be genetically encoded?</p><p>– Another proposed function of the plasticity rule is in adjusting to changes in relative gain between proprioceptive signals. The authors cite Jayakumar et al. (2019) in the introduction as motivation for this, without noting until later that the study used rodents rather than flies. In the discussion, it is mentioned that Seelig and Jayaraman (2015) actually did not find evidence for gain adjustments in flies. This seems to go against the plasticity mechanism being proposed as being relevant for flies. Is there any evidence for this in flies?</p><p>– Biologically plausible learning rule: One of the main assumptions that allows the authors to claim for a biologically plausible learning rule is that the 'firing rate' of the neuron is available for all synapses at the axon-distal compartment due to the assumed back-propagation (line 167). As this seems to be crucial, it will be good to give references showing that there is a back-propagation AP in E-PG neurons, or to explain what the alternatives are.</p><p>– Writing style: The paper would be easier to understand if some of the key relevant equations, for example those that describe the synaptic plasticity rule, were included in the main text.</p><p>– Analysis of connectome data. The connectome data is a nice support for why using a learning rule that relies on a two-compartment model. However, some points could be clarified. What is exactly is X,Y position? Can the authors plot the shape of the neuron on the same plot, and maybe also to give access to a 3D video of this cloud points? Also, why showing only 1 example, instead of analyzing all the EPG neurons and showing all of them in a supplementary figure. Then, the authors should do a statistical analysis to support their claim, such as doing some clustering analysis. You want to at least convince the reader that you can reject the null hypothesis that inputs are not segregated.</p><p>– Predictions: It would be good if the authors could try and give more concrete predictions, or better, suggest testable predictions. While a theory doesn't have to give predictions, as mentioned in the public review, in this specific case it would be good if the authors could spend one or two paragraphs on concrete predictions and maybe even suggest new experiments to the community. In other words, while not necessary for publishing the paper, the authors could go beyond saying that PI requires supervised learning during development, which seems more like a hypothesis of the model than a real prediction.</p><p>– Network initial architecture: It is not clear how the results depend on the specific initialization of the network and how sensitive the main conclusions of the paper are to these choices. More specifically:</p><p>– It is not completely clear what the initial recurrent connectivity in the network is, and how the results depend on this choice. For example, does the symmetry, which the authors rely upon to derive their reduced model, persist if the initial connectivity is random?</p><p>– Would the results and conclusions of the paper change if the authors use a different choice of the HD-&gt;HR connections? Could they be random? Could they be connected to more than one neuron? Could they be plastic as well?</p><p>Related to this issue, in line 266 the authors claim that 'circular symmetry is a crucial property for any ring attractor'. While this might be a common knowledge in the field, please see Darshan and Rivkind 21 that argue against it. Following this, it is unclear if symmetry is needed for the authors to derive the reduced model, or if it is a principle that they think the system must have. The former is rather technical, while the latter is an important principle. It would be good if the authors could address this in their discussion.</p><p>– Saturation: It seems that saturation is crucial in this work. Is it known that E-PG neurons reach saturation levels? For example, in cortex this rarely happens, where neurons are usually active around an expensive non-linearity regime and are far from saturation. It will be beneficial if the authors could show that similar phenomena hold with different types of non-linearities, in which neurons are not saturated. Alternatively, the authors can explain in the manuscript why they think that E-PG neurons in reality are saturated (maybe give references?).</p><p>– Line 221 (and 389) &quot;…our work reproduces this feature of the fly HD as an emergent property of learning&quot;. It is not clear what the authors mean by this. Is the impairment of PI in small angular velocities a result of the finite number of neurons in the network, or an emergent property of the learning? For example, what would happen if the authors would take 2000 neurons, will they still see this 'emergent property'? This also goes against what is written in Line 465 of the discussion.</p><p>– Timescales: there are many timescales in the model. Although it is addressed, to some extent, in the supplementary material, it would be good to add a paragraph stating what are the requirements on these timescales. For example, does the model fail if one of the timescales is smaller\larger than the others? Did the authors assume in the derivation that one timescale is slower than the rest? Why tau<sub>δ</sub> can be ignored in the derivation? Can the authors show the failure modes of the network\derivation when changing the timescale?</p><p>– Randomness in network connectivity- Figure S3. In case of the random connectivity, as the network is so small, the behavior of the network depends on the specific network realization and, therefore, it is hard to assess how representative FigS3D-E are. it will be good to show some statistical analysis of the diffusion across these networks.</p><p>– Noise in the dynamics- Figure S4. It is unclear if the diffusion is larger in this case just because the authors added noise, which stays after training, or it is something deeper than that. Could the authors compare the diffusivity in networks that were trained with and without noise? Also, in FigS4 panel E- it seems that in contrast to the finite-size network effects, in this case there is no problem to integrate at very small velocities. Why is that? Does the noise contribute to smoothing the barriers and helps to go from a discrete to a more continuous representation when doing PI? Does this have to do with the small number of involved neurons in the fly central complex? One of the reviewers disagrees with the remark about a feature build-in by hand in previous models l220-222. This is not something the learning rule per se allows to understand.</p><p>– What is the limitation of the adaptation in the network? For example, the authors claim that 'we test whether our network can rewire to learn an arbitrary gain between the two', but in fact only showed examples for a 50% increase or decrease in gain and only mentioned a negative gain without showing it. It will be useful to have a figure, showing how the performance varies with changing the gain. Is there a maximal\minimal gain that above\below it the network always fail to perform? Which of the network parameters are important for these gain changes?</p><p>– The authors seem to assimilate path integration with angular integration (l31-40). The way insects manage to perform path integration (i.e. integrate distances) is still undetermined to our knowledge. Clearly, the angular integration performed in the central complex is not enough. The authors should try to disambiguate this point.</p><p>– The authors mention the existence of two PEN populations (PEN-1 and PEN-2). Could the proposed model could shed light on the presence of these two populations. Would they account for the inhibitory and excitatory part of the HR curves on Figure 3C?</p><p>– On l 625, the authors state that the constant inhibitory drive contributes to the uniqueness of the bump. One of the reviewers was not convinced of this statement as this constant inhibition does not play a role in the competition between several bumps. Only the inhibitory recurrent connectivity within the HD population or going through the HR populations would play such a role.</p><p>– In Figure S5B, the bump velocity goes to zero around 500º/s. Could this come from a defect in the network that would, for instance, abolish the presence of a bump in left and right HR populations? In a more robust behavior, only one of the left or right bumps would disappear, and the HD bump would go at a max constant velocity.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><p>Thank you for resubmitting your work entitled &quot;Learning accurate path integration in ring attractor models of the head direction system&quot; for further consideration by <italic>eLife</italic>. Your revised article has been evaluated by Ronald Calabrese (Senior Editor) and a Reviewing Editor (Srdjan Ostojic).</p><p>The manuscript has been much improved but there are some remaining issues related to the lack of evidence for plasticity in CX. After consultation, the reviewers agreed that the current lack of evidence for plasticity should be pointed out earlier in the manuscript, and suggested presenting the need for plasticity in younger flies as a prediction of the model. One of the reviewers provided additional suggestions listed below.</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The authors have made a number of improvements to the manuscript. My central concern, which is that there isn't any evidence for this plasticity being present in the CX, still stands. Obviously, a purely modeling study is unable to address this concern. The authors have argued in Appendix 3 that asymmetries in the architecture can severely disrupt the performance of the model and that this supports the need for plasticity. Here are a few suggestions related to this central issue.</p><p>1) The authors refer to the performance of flies from Seelig and Jayaraman (2015), stating at various points whether their models outperform the actual performance or not. This should actually be quantified in the figures and/or text. Specifically, both Figure 2 and Appendix 3 would benefit from a quantitative comparison to real fly performance.</p><p>2) It would be useful to show how much imprecision must be added to the synaptic connections before the model can account for the performance of actual flies. This would answer the question of &quot;how much precision must be genetically encoded to account for fly behavior.&quot;</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>I find the paper to be timely and very interesting. The paper was well presented already in the first version, but I did have a few concerns. The authors did a great job in addressing all of these concerns and comments in the new version of their manuscript.</p><p>I strongly recommend it for publications in <italic>eLife</italic>.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.69841.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Plasticity:</p><p>– A main question is whether the type of synaptic plasticity proposed by the authors is necessary and operative in the CX. One motivation for the plasticity rule is that, as the authors state, ring attractors &quot;require that synaptic connections are precisely tuned (Hahnloser, 2003). Therefore, if the circuit was completely hardwired, the amount of information that an organism would need to genetically encode connection strengths would be exceedingly high.&quot; However, in Figure 3 and S3, it is shown that the learnedPI &quot;far outperforms&quot; flies, so the authors add random noise to the synaptic weights in order to make a more realistic model. But this means that the synaptic weights don't actually need to be that precise. Doesn't this mean that the connectivity may in fact be genetically encoded?</p></disp-quote><p>This is indeed an important point to expand on, and we thank the reviewer for bringing it up. A main finding of the present work is that a local synaptic plasticity rule operating in compartmentalized neurons can learn the precise synaptic connectivity required for accurate PI. Because simulation conditions are idealized,PI performance of the model can exceed that of the fly. However, there are many unknown factors that could limit the performance in the biological system, factors that, for simplicity, we do not model explicitly. Adding noise to the connectivity (as in Figure 3 —figure supplement 3; was ‘S3’ in the first submission) is a simple way to bring the model’s performance closer to the one of the fly; not an explanation of why the fly’s performance is worse. Figure 3 —figure supplement 3 thus shows how a perturbation of the learned network results in more realisticPI performance, deviating from the idealized simulation conditions.</p><p>In reality, there could be multiple reasons whyPI performance is worse in the fly than in the model. For instance, a confounder that would affect performance but not necessarily learning could be the presence of inputs that are unrelated to path integration, e.g., inputs related to circadian cycles (Raccuglia et al., 2019). In the presence of such confounders, a precise tuning of the weights might be crucial in order to reach the performance of the fly. In other words, only if the model outperforms the biological circuit in a simplified setting, it has a chance to perform as well in a realistic setting, with all the additional complexities the latter comes with. We have added these remarks in the final paragraph of “Relation to experimental literature” in the Discussion.</p><p>A further reason why synaptic plasticity might be necessary to achieve a PI performance comparable to the fly is to counteract asymmetries in the initial architecture, such as noisy hardwired connectivity: In the main text we assume that the architecture and the hardwired HD to HR connections are completely circular symmetric. However, as the reviewers have also pointed out in other comments below, this is unrealistic for a biological system. In the new Appendix 3 we show that if we add variability to these connections while maintaining the same profiles for HR to HD and recurrent connections, PI in the model is much worse than in the fly (Appendix 3 – Figure 2). This implies that even if the optimal weight connectivity was to be passed down genetically, it would still be impossible to path integrate as well as the fly, because minimal biological asymmetries in the circuit can have a big impact on PI. Instead, as we show in Appendix 3 – Figure 1, our learning rule can counteract asymmetries in the initial architecture by producing asymmetries in the learned connectivity, andPI remains excellent. Therefore the PI performance of the fly can be matched only if there is plasticity during development.</p><p>Finally, we note that since this is a modeling study, we cannot prove that plasticity is operative in the CX; instead, this is a prediction of our model that can only be tested with experiments.</p><disp-quote content-type="editor-comment"><p>– Another proposed function of the plasticity rule is in adjusting to changes in relative gain between proprioceptive signals. The authors cite Jayakumar et al. (2019) in the introduction as motivation for this, without noting until later that the study used rodents rather than flies. In the discussion, it is mentioned that Seelig and Jayaraman (2015) actually did not find evidence for gain adjustments in flies. This seems to go against the plasticity mechanism being proposed as being relevant for flies. Is there any evidence for this in flies?</p></disp-quote><p>To the best of our knowledge, there is no evidence of plasticity being used for gain adjustments in flies. However, it has not been tested in young animals, and this is why we propose it as a testable prediction. As noted in the Discussion (section “Relation to experimental literature”, third paragraph), in the Seelig and Jayaraman (2015) study, the experimenters used mature flies, whereas we propose that, in flies, this form of plasticity is strong only during development. In rodents, on the other hand, there is evidence that such plasticity ensues past the developmental stage.</p><p>To better distinguish properties ofPI in rodents and flies, we mention now explicitly in the Introduction that Jayakumar et al. (2019) used rodents. Furthermore we suggest (in the last paragraph of the Introduction and last paragraph of section “Testable predictions” in the Discussion) that our prediction could be tested in young flies.</p><disp-quote content-type="editor-comment"><p>– Biologically plausible learning rule: One of the main assumptions that allows the authors to claim for a biologically plausible learning rule is that the 'firing rate' of the neuron is available for all synapses at the axon-distal compartment due to the assumed back-propagation (line 167). As this seems to be crucial, it will be good to give references showing that there is a back-propagation AP in E-PG neurons, or to explain what the alternatives are.</p></disp-quote><p>Backpropagation of APs could occur with either active or passive mechanisms. In our setting, passive backpropagation would suffice, and passive spread of activity has been shown to not attenuate too fast in fly neurons (Gouwens and Wilson, 2009). The axon-proximal and axon-distal compartments belong to the same dendritic tuft, and we assume that the axon initial segment is close to the axon-proximal compartment. Thus, the generated AP would only need to travel a short distance compared to the electrotonic length, hence it would not be attenuated considerably (e.g. see figure 5 in Gouwens and Wilson, 2009). We have added these remarks in the second paragraph of “Relation to experimental literature” in the Discussion. Finally, we note that to the best of our knowledge, there is no evidence for active backpropagation of APs in E-PG neurons.</p><disp-quote content-type="editor-comment"><p>– Writing style: The paper would be easier to understand if some of the key relevant equations, for example those that describe the synaptic plasticity rule, were included in the main text.</p></disp-quote><p>The learning rule is the key equation, as it contains the most important variables of the model. In this revised manuscript, we have added a simplified version (i.e. without low-pass filtering the weight changes) of the learning rule reported in section “Learning rule” in Methods (see second to last paragraph of the section “Model setup” in Results).</p><disp-quote content-type="editor-comment"><p>– Analysis of connectome data. The connectome data is a nice support for why using a learning rule that relies on a two-compartment model. However, some points could be clarified. What is exactly is X,Y position? Can the authors plot the shape of the neuron on the same plot, and maybe also to give access to a 3D video of this cloud points? Also, why showing only 1 example, instead of analyzing all the EPG neurons and showing all of them in a supplementary figure. Then, the authors should do a statistical analysis to support their claim, such as doing some clustering analysis. You want to at least convince the reader that you can reject the null hypothesis that inputs are not segregated.</p></disp-quote><p>To clarify E-PG connectivity at the EB, we provide new figures and analysis in accordance with the reviewers’ remarks. First, we include the skeleton plot of the example EP-G neuron in Figure 1E, using coordinates Y and Z in line with the fly connectome. The orientation of the skeleton plot is the same as in the synapse location plot, whose placement is indicated by the zoom box. To avoid congestion, we decided to not plot the shape of the neuron on top of the synapse location plot; the shape of the neuron can be readily seen in the skeleton plot. In addition, we provide a 3D rotating video of the point cloud for this example neuron (Figure 1 – video 1).</p><p>Following the reviewer's suggestions, we plot the synapse locations for all 16 neurons we analyzed in the new Figure 1 —figure supplement 1A. Furthermore, to support the claim that visual inputs are separated from recurrent and HR to HD inputs, we perform binary classification between the two classes (R2 and R4d vs. P-EN1 and P-EN2), using SVMs and 5-fold cross validation (for details, see the last paragraph in “Fly Connectome Analysis” in Methods). We find (Figure 1 —figure supplement 1B) that, in held-out test data, the model is excellent (test accuracy &gt; 0.95 across neurons and model runs) at predicting class identity from location alone. We report this finding in paragraph 3 of section “Model Setup” of the results.</p><disp-quote content-type="editor-comment"><p>– Predictions: It would be good if the authors could try and give more concrete predictions, or better, suggest testable predictions. While a theory doesn't have to give predictions, as mentioned in the public review, in this specific case it would be good if the authors could spend one or two paragraphs on concrete predictions and maybe even suggest new experiments to the community. In other words, while not necessary for publishing the paper, the authors could go beyond saying thatPI requires supervised learning during development, which seems more like a hypothesis of the model than a real prediction.</p></disp-quote><p>We agree with the reviewers that it is very important to suggest testable predictions. Therefore we have included a new section “Testable predictions” in the Discussion, where we delineate our model predictions and put forth ways to test them. Briefly, these predictions are:</p><list list-type="bullet"><list-item><p>Synaptic plasticity during development is crucial in setting up the HD system</p></list-item><list-item><p>HD neurons have a compartmentalized structure where idiothetic inputs are separated from allothetic inputs</p></list-item><list-item><p>Allothetic inputs more readily control the firing rate of the HD neurons</p></list-item><list-item><p>Passive or active backpropagation of action potentials makes the output of the neuron available to the compartment that receives idiothetic inputs</p></list-item><list-item><p>The HD system can readily adapt to manipulations of gain during development</p></list-item></list><disp-quote content-type="editor-comment"><p>– Network initial architecture: It is not clear how the results depend on the specific initialization of the network and how sensitive the main conclusions of the paper are to these choices. More specifically:</p><p>– It is not completely clear what the initial recurrent connectivity in the network is, and how the results depend on this choice. For example, does the symmetry, which the authors rely upon to derive their reduced model, persist if the initial connectivity is random?</p></disp-quote><p>The reviewers correctly point out that this information was missing from the manuscript. We now describe network initialization in the first paragraph of “Training protocol” in Methods. Briefly, trainable weights for all networks are initialized with random connectivity drawn from a normal distribution with mean 0 and standard deviation 1/ sqrt(N<sup>HD</sup>), where N<sup>HD</sup> is the number of HD neurons in the network, as is common practice in the modeling literature.</p><p>Since this initialization results in weights that are much smaller that the final weights, to better address the question on the dependence of the results on initial conditions, we randomly shuffle the learned weights in Figure 3A,B to completely eradicate any structure and initialize the network with the shuffled weights. <xref ref-type="fig" rid="sa2fig1">Author response image 1</xref> shows that even in this scenario, learning converges to a low learning error, and PI performance is excellent, virtually indistinguishable from the one of the main text network (see Figure 2). We note however that learning is not able to completely eradicate the noise in the initial weights. Instead, it settles to a noisier connectivity and weights span a larger range (Figure 3A,B to <xref ref-type="fig" rid="sa2fig1">Author response image 1A, B</xref>). Despite that, PI remains accurate (see <xref ref-type="fig" rid="sa2fig1">Author response image 1F</xref>), therefore the deviations caused by individual weights should be balanced out when network activity as a whole is considered.</p><fig id="sa2fig1" position="float"><label>Author response image 1.</label><caption><title>PI performance of a network that was initialized with randomly shuffled weights from Fig.</title><p>3A,B. (A), (B) Resulting weight matrices after ~22 hours of training. The weights matrices look very similar to the ones in the main text in Fig. 3A,B, albeit connectivity remains noisy and weights span a larger range. (C) Example of PI shows that the network can still path-integrate accurately. (D) Temporal evolution of distribution of PI errors during PI in darkness. Performance is comparable to Fig. 2B of the main network, with only minimal side bias. (E) PI performance is comparable to the one in Fig. 2C.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-sa2-fig1-v2.tif"/></fig><p>We now mention this also in the first paragraph of “Training protocol” in Methods (together with a hint on simulations in Figure 4 that illustrate gain changes), concluding that the final PI performance is virtually independent of the initial distribution of weights.</p><disp-quote content-type="editor-comment"><p>– Would the results and conclusions of the paper change if the authors use a different choice of the HD-&gt;HR connections? Could they be random? Could they be connected to more than one neuron? Could they be plastic as well?</p></disp-quote><p>To address these very relevant questions, we include the new Appendix 3; furthermore, we added in the section “Model Setup” (fourth paragraph) in the Results a brief statement that our assumption of “1-to-1 wiring and constant amplitude of the HD to HR connections” is uncritical. Briefly, in new simulations we drop the 1-to-1 and constant-magnitude assumptions of the HD to HR connections, and we find that PI performance is not impaired (Appendix 3 – Figure 1). The network can even path integrate when these connections are completely random, albeit for a smaller angular velocity range (Appendix 3 – Figure 3). Finally, we demonstrate that since circular symmetry in the hardwired HD-to-HR connections is unreasonable for a biological system, the learning rule is crucial in balancing out any deviations from circular symmetry in the initial architecture; otherwise, PI performance would be considerably impaired (Appendix 3 – Figure 2, also see our answer to the 1st reviewer comment above about the necessity of synaptic plasticity).</p><p>Regarding the possibility of the HD-to-HR connections being plastic, we note that our model does not incorporate plasticity for connections other than the incoming connections to associative neurons; therefore the HD-to-HR connections cannot be learned with our model. We assume that they could be prewired, e.g. during prenatal circuit assembly. If some other form of plasticity operated in these synapses, the network should still learn accurate PI, provided that HD-to-HR connections settle to final values after some time. This is supported by the fact that, as mentioned in the previous paragraph, the network learns accurate PI even for completely random HD-to-HR connectivity.</p><disp-quote content-type="editor-comment"><p>Related to this issue, in line 266 the authors claim that 'circular symmetry is a crucial property for any ring attractor'. While this might be a common knowledge in the field, please see Darshan and Rivkind 21 that argue against it. Following this, it is unclear if symmetry is needed for the authors to derive the reduced model, or if it is a principle that they think the system must have. The former is rather technical, while the latter is an important principle. It would be good if the authors could address this in their discussion.</p></disp-quote><p>The reviewers are correct to point out that circular symmetry is not required for building a ring attractor in general. Therefore we have corrected the statement (first paragraph of section “Learning results in synaptic connectivity that matches the one in the fly ” in the Results) from “any ring attractor” to “a symmetric ring attractor”, and included a discussion of the suggested paper in the section “Relation to theoretical literature” in the Discussion. It follows that the results in our Mathematical Appendix only hold for the case where the symmetries in the initial learning setup result in a circular symmetric connectivity. Therefore, anatomical symmetry is an assumption of our reduced model, but not a requirement for all ring attractors.</p><disp-quote content-type="editor-comment"><p>– Saturation: It seems that saturation is crucial in this work. Is it known that E-PG neurons reach saturation levels? For example, in cortex this rarely happens, where neurons are usually active around an expensive non-linearity regime and are far from saturation. It will be beneficial if the authors could show that similar phenomena hold with different types of non-linearities, in which neurons are not saturated. Alternatively, the authors can explain in the manuscript why they think that E-PG neurons in reality are saturated (maybe give references?).</p></disp-quote><p>From calcium imaging videos in the original study of Seelig and Jayaraman (2015) and later ones, it seems that E-PG neurons within the bump fire vigorously at similar levels, whereas neurons outside the bump fire very sparsely. Therefore the shape of the bump in the Seelig and Jayaraman (2015) experiments seems to not be very far from square, which might hint at saturation. Even though, to the best of our knowledge, it is currently not known whether E-PG neurons actually reach saturation, other <italic>Drosophila</italic> neurons are known to reach saturation with increasing inputs, instead of some sort of depolarization block (Wilson, 2013; Brandao et al., 2021). Saturation with increasing inputs may come about due to, for instance, short-term synaptic depression: it has been reported that beyond a certain frequency of incoming action potentials, the synaptic input current is almost independent of that frequency (Tsodyks and Markram, 1997; Tsodyks et al., 1998). We now better explain our assumptions underlying saturation in the section “Neuronal model” in the Methods.</p><disp-quote content-type="editor-comment"><p>– Line 221 (and 389) &quot;…our work reproduces this feature of the fly HD as an emergent property of learning&quot;. It is not clear what the authors mean by this. Is the impairment ofPI in small angular velocities a result of the finite number of neurons in the network, or an emergent property of the learning? For example, what would happen if the authors would take 2000 neurons, will they still see this 'emergent property'? This also goes against what is written in Line 465 of the discussion.</p></disp-quote><p>The reviewers are right to point this out. Following their suggestion, in <xref ref-type="fig" rid="sa2fig2">Author response image 2</xref> we increase the number of neurons twofold and fourfold and we provide the velocity-gain plots for these simulations. In both cases, the flat segment at small velocities has completely disappeared. We further confirm this by only looking at small velocities of +- 20 deg/s, and reducing the velocity interval to 0.5 deg/s. Therefore, as suggested by the reviewer, the impairment is indeed caused by the limited number of neurons that the fly has at its disposal, and it is not an emergent property from learning. Hence we have corrected all statements regarding the emergent property. Additionally, we now mention that the flat region disappears if we increase the number of neurons (see final paragraph of the section “Mature network can path-integrate in darkness”).</p><fig id="sa2fig2" position="float"><label>Author response image 2.</label><caption><title>PI performance for networks with more neurons.</title><p>(A) For N<sup>HD</sup> = N<sup>HR</sup> = 120 and training time ~4.5 hours, PI performance is excellent, and the flat area for small angular velocities observed in Figure 2C is no longer present. (B) To confirm that small angular velocities are no longer impaired, we limit the range of tested velocities and reduce the interval between tested velocities to 0.5 deg/s. (C), (D) same as (A), (B), for N<sup>HD</sup> = N<sup>HR</sup> = 240 and training time ~2 hours.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-69841-sa2-fig2-v2.tif"/></fig><disp-quote content-type="editor-comment"><p>– Timescales: there are many timescales in the model. Although it is addressed, to some extent, in the supplementary material, it would be good to add a paragraph stating what are the requirements on these timescales. For example, does the model fail if one of the timescales is smaller\larger than the others? Did the authors assume in the derivation that one timescale is slower than the rest? Why tau<sub>δ</sub> can be ignored in the derivation? Can the authors show the failure modes of the network\derivation when changing the timescale?</p></disp-quote><p>There are indeed many time scales in the model, which are briefly summarized here:</p><list list-type="bullet"><list-item><p>synaptic time constant τ<sub>s</sub> = 65 ms</p></list-item><list-item><p>membrane time constant of axon-distal compartment τ<sub>l</sub> = 10 ms</p></list-item><list-item><p>membrane time constant of axon-proximal compartment, C/g<sub>L</sub> = 1 ms</p></list-item><list-item><p>weight update filtering time constant τ<sub>δ</sub> = 100 ms</p></list-item><list-item><p>time constant of velocity decay τ<sub>v</sub> = 0.5 s</p></list-item><list-item><p>learning time scale (set by 1/η = 20 s)</p></list-item></list><p>These time constants are now summarized in the Table 1 in the new Appendix 4. Several of these parameters are well constrained by biology, and thus we chose to keep them constant, while others have quite liberal requirements. We discuss all these in detail in the new Appendix 4.</p><disp-quote content-type="editor-comment"><p>– Randomness in network connectivity- Figure S3. In case of the random connectivity, as the network is so small, the behavior of the network depends on the specific network realization and, therefore, it is hard to assess how representative FigS3D-E are. it will be good to show some statistical analysis of the diffusion across these networks.</p></disp-quote><p>The reviewers are correct to point out that variability exists between different realizations of networks with added noise in the learned weights (now shown in Figure 3 —figure supplement 3, which is identical to the old Figure S3). A useful quantity to characterize the performance of a network is the diffusion coefficient during path integration, which quantifies how fast the width of the PI error distribution in e.g. Figure 3 —figure supplement 3D increases (see new section “Diffusion Coefficient” in Methods). Therefore, we created multiple networks with perturbed weights, and estimated the diffusion coefficient for all of them. We report the point estimate and 95 % confidence intervals (Students t-Test) of the grand average to be 82.3 +- 15.7 deg^2/s, which is considerably larger than the diffusion coefficient for networks without a perturbation in the weights (24.5 deg^2/s, see Appendix 1 – Figure 1E). We report these results in the section “Learning results in synaptic connectivity that matches the one in the fly” (last paragraph).</p><disp-quote content-type="editor-comment"><p>– Noise in the dynamics- Figure S4. It is unclear if the diffusion is larger in this case just because the authors added noise, which stays after training, or it is something deeper than that. Could the authors compare the diffusivity in networks that were trained with and without noise? Also, in FigS4 panel E- it seems that in contrast to the finite-size network effects, in this case there is no problem to integrate at very small velocities. Why is that? Does the noise contribute to smoothing the barriers and helps to go from a discrete to a more continuous representation when doing PI? Does this have to do with the small number of involved neurons in the fly central complex? One of the reviewers disagrees with the remark about a feature build-in by hand in previous models l220-222. This is not something the learning rule per se allows to understand.</p></disp-quote><p>In the previous comment we addressed the diffusivity of path integration in networks that receive velocity input. To evaluate whether synaptic input noise during training affects learning, we now systematically explore the impact of this “training noise” on diffusivity for various levels of synaptic input noise during testing (called “test noise”). However, we find that the contribution ofPI errors to the diffusion coefficient is always greater than the one of test noise, even though thePI errors themselves are quite small. Hence, to assess robustness to noise, we compute the diffusion coefficient in networks initialized at various locations and left to diffuse without any velocity or visual input (see the new Appendix 1). We then estimate and plot in panel E of a revised Appendix 1-Figure 1 (was labeled Figure S4 in the previous version) the diffusion coefficient for networks that have been trained without input noise (σ<sub>n</sub> = 0 or SNR = 0, blue dots) and with input noise (σ<sub>n</sub> = 0.7 or SNR = 2; orange dots), for various test noise levels. Briefly, we find that the network trained with noise is less diffusive for test noise levels above σ<sub>n</sub> = 0.7 (which is equal to the amount of train noise it has been trained on). We report these findings in the last two paragraphs of Appendix 1.</p><p>The lack of “stickiness” at small angular velocities is indeed an effect that can be attributed to noise applied during testing, as suggested by the reviewer. This noise allows random transitions to adjacent attractor states. Furthermore, these random transitions are biased towards the side of the drift velocity input, even for small velocity inputs. Therefore, the flat region for small angular velocities seen before disappears, and the bump moves with the drift speed on average. Indeed the problem with integration of small velocities is related to the small number of neurons, as previously noted, and should not be considered a property that results from learning.</p><disp-quote content-type="editor-comment"><p>– What is the limitation of the adaptation in the network? For example, the authors claim that 'we test whether our network can rewire to learn an arbitrary gain between the two', but in fact only showed examples for a 50% increase or decrease in gain and only mentioned a negative gain without showing it. It will be useful to have a figure, showing how the performance varies with changing the gain. Is there a maximal\minimal gain that above\below it the network always fail to perform? Which of the network parameters are important for these gain changes?</p></disp-quote><p>To address the reviewer’s comment, we now test the ability of the network to rewire for a larger set of gain values. Specifically, we include 2 more gains in Figure 4, and include the new Figure 4 —figure supplement 1, where we show an even broader range of tested gains from ⅛ to 4.5 (Figure 4 —figure supplement 1A), along with extreme examples (Figure 4 —figure supplement 1B,C) to show the limits of the system. In Figure 4 —figure supplement 1D-F we also include simulation results where the network was instructed to reverse its gain. We discuss all of these new results in the last paragraph of “Fast adaptation of neural velocity gain” in the Results. A metric to assess deviation from an instructed gain, as well as the relevant quantities setting the limits for gain adaptation are described in the caption of Figure 4 —figure supplement 1.</p><disp-quote content-type="editor-comment"><p>– The authors seem to assimilate path integration with angular integration (l31-40). The way insects manage to perform path integration (i.e. integrate distances) is still undetermined to our knowledge. Clearly, the angular integration performed in the central complex is not enough. The authors should try to disambiguate this point.</p></disp-quote><p>We thank you for pointing out this imprecision. We now disambiguate between path integration and angular integration in the first paragraph of the Introduction.</p><disp-quote content-type="editor-comment"><p>– The authors mention the existence of two PEN populations (PEN-1 and PEN-2). Could the proposed model could shed light on the presence of these two populations. Would they account for the inhibitory and excitatory part of the HR curves on Figure 3C?</p></disp-quote><p>As indicated in Figure 1E and also noted in the first paragraph of “Fly Connectome Analysis” in Methods, PEN-1 neurons correspond to HR cells, and PEN-2 cells take part in the excitatory recurrent loop of HD cells. Furthermore, as noted in the next-to last paragraph in section “Relation to experimental literature” in the Discussion, the PEN-1 neurons account for the excitatory part of the learned HR-to-HD connectivity, whereas the negative part could be mediated by other neurons, or interactions between different neurons altogether.</p><disp-quote content-type="editor-comment"><p>– On l 625, the authors state that the constant inhibitory drive contributes to the uniqueness of the bump. One of the reviewers was not convinced of this statement as this constant inhibition does not play a role in the competition between several bumps. Only the inhibitory recurrent connectivity within the HD population or going through the HR populations would play such a role.</p></disp-quote><p>We agree with the reviewer on this pont, and the statement has been corrected (see end of second paragraph of section “Neuronal Model” in Methods). Global inhibition indeed suppresses bumps in general. Only sufficient local activity combined with local excitation can overcome the inhibition. Instead, it is the slightly negative recurrent profile for large offsets that contributes to the competition between bumps.</p><disp-quote content-type="editor-comment"><p>– In Figure S5B, the bump velocity goes to zero around 500º/s. Could this come from a defect in the network that would, for instance, abolish the presence of a bump in left and right HR populations? In a more robust behavior, only one of the left or right bumps would disappear, and the HD bump would go at a max constant velocity.</p></disp-quote><p>The reviewers are correct to point out that the behavior of the network would be more robust if it path-integrated at the maximum velocity when the instructed velocity is higher than that. The observed behavior however is not the outcome of some defect; rather, it reflects the fact that, as we detail in Appendix 2, the velocity of bump movement is limited by how far away from the current location the bump can be propagated and how fast it can do that. In Appendix 2 we argue that contributions from HD neurons might be required to move that bump. For velocities larger than the velocity limit, there are no immediate excitatory connections between the currently active HD neurons and the HD neurons that should be activated the next instance (the phase shift required for a given maximum velocity can be obtained by solving eq. 23 for b). Instead, connections are inhibitory for such large offsets, because bump phase shifts larger than the one corresponding to the velocity limit move the bump to the negative sidelobes of the recurrent connectivity. Therefore, without enough excitation to overcome the global inhibition and keep the bump moving, the bump disappears.</p><p>[Editors' note: further revisions were suggested prior to acceptance, as described below.]</p><disp-quote content-type="editor-comment"><p>The manuscript has been much improved but there are some remaining issues related to the lack of evidence for plasticity in CX. After consultation, the reviewers agreed that the current lack of evidence for plasticity should be pointed out earlier in the manuscript, and suggested presenting the need for plasticity in younger flies as a prediction of the model. One of the reviewers provided additional suggestions listed below.</p><p>Reviewer #2 (Recommendations for the authors):</p><p>The authors have made a number of improvements to the manuscript. My central concern, which is that there isn't any evidence for this plasticity being present in the CX, still stands. Obviously, a purely modeling study is unable to address this concern. The authors have argued in Appendix 3 that asymmetries in the architecture can severely disrupt the performance of the model and that this supports the need for plasticity. Here are a few suggestions related to this central issue.</p></disp-quote><p>We thank the reviewer for their recommendations, and address them subsequently.</p><disp-quote content-type="editor-comment"><p>1) The authors refer to the performance of flies from Seelig and Jayaraman (2015), stating at various points whether their models outperform the actual performance or not. This should actually be quantified in the figures and/or text. Specifically, both Figure 2 and Appendix 3 would benefit from a quantitative comparison to real fly performance.</p></disp-quote><p>We now also quantify PI performance of the network by using the same measure as Seelig and Jayaraman (2015): the correlation coefficient between the PVA and true heading in darkness (see new section “Quantification ofPI performance” in Methods). We report this correlation for the main text network in the Results on Figure 2 (3rd paragraph of “Mature network can path-integrate in darkness”) and extensively in Appendix 3 (see answer to next suggestion). These correlation values allowed us to compare the performance of the network model to that of the fly.</p><disp-quote content-type="editor-comment"><p>2) It would be useful to show how much imprecision must be added to the synaptic connections before the model can account for the performance of actual flies. This would answer the question of &quot;how much precision must be genetically encoded to account for fly behavior.&quot;</p></disp-quote><p>We now add noise to all connections. In the new paragraph 5 of Appendix 3, we specify the amount of noise required to drop below fly performance. Furthermore, we comment on the sharpness of performance drop and which weights are more susceptible to noise.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>I find the paper to be timely and very interesting. The paper was well presented already in the first version, but I did have a few concerns. The authors did a great job in addressing all of these concerns and comments in the new version of their manuscript.</p><p>I strongly recommend it for publications in eLife.</p></disp-quote><p>We thank the reviewer for their remarks.</p></body></sub-article></article>