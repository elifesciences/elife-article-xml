<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"  "JATS-archivearticle1.dtd"><article article-type="correction" dtd-version="1.2" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:xlink="http://www.w3.org/1999/xlink"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn pub-type="epub" publication-format="electronic">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">74328</article-id><article-id pub-id-type="doi">10.7554/eLife.74328</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Correction</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Correction: Bi-channel image registration and deep-learning segmentation (BIRDS) for efficient, versatile 3D mapping of mouse brain</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-210455"><name><surname>Wang</surname><given-names>Xuechun</given-names></name></contrib><contrib contrib-type="author" id="author-209955"><name><surname>Zeng</surname><given-names>Weilin</given-names></name></contrib><contrib contrib-type="author" id="author-209899"><name><surname>Yang</surname><given-names>Xiaodan</given-names></name></contrib><contrib contrib-type="author" id="author-255268"><name><surname>Zhang</surname><given-names>Yongsheng</given-names></name></contrib><contrib contrib-type="author" id="author-209900"><name><surname>Fang</surname><given-names>Chunyu</given-names></name></contrib><contrib contrib-type="author" id="author-124218"><name><surname>Zeng</surname><given-names>Shaoqun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0002-1802-337X</contrib-id></contrib><contrib contrib-type="author" corresp="yes" id="author-209901"><name><surname>Han</surname><given-names>Yunyun</given-names></name><email>yhan@hust.edu.cn</email></contrib><contrib contrib-type="author" corresp="yes" id="author-111080"><name><surname>Fei</surname><given-names>Peng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">http://orcid.org/0000-0003-3764-817X</contrib-id><email>feipeng@hust.edu.cn</email></contrib></contrib-group><pub-date date-type="publication" publication-format="electronic"><day>05</day><month>10</month><year>2021</year></pub-date><pub-date pub-type="collection"><year>2021</year></pub-date><volume>10</volume><elocation-id>e74328</elocation-id><permissions><copyright-statement>© 2021, Wang et al</copyright-statement><copyright-year>2021</copyright-year><copyright-holder>Wang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><related-article ext-link-type="doi" id="ra1" related-article-type="corrected-article" xlink:href="10.7554/eLife.63455"/></article-meta></front><body><boxed-text><p>Wang X, Zeng W, Yang X, Zhang Y, Fang C, Zeng S, Han Y, Fei P. 2021. Bi-channel image registration and deep-learning segmentation (BIRDS) for efficient, versatile 3D mapping of mouse brain. <italic>eLife</italic> <bold>10</bold>:e63455. doi: <ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.7554/eLife.63455">10.7554/eLife.63455</ext-link>.</p><p>Published 18, January 2021</p></boxed-text><p>In our BIRDS software, a few parts in the registration module were contributed by Yongsheng Zhang and Prof. Shaoqun Zeng. When we were writing the eLife paper last year, we misunderstood that this work had been previously published along with their brain research work in their Wang et al 2020a paper (Science Bulletin 65:1203-1216) which we cited in the eLife paper. Due to this misunderstanding their contributions were only acknowledged in the published eLife paper. We are therefore formally correcting the eLife paper to add them both as co-authors. All the authors agree to their addition as co-authors in the paper.</p><p>The details of Yongsheng Zhang and Prof. Shaoqun Zeng’s contribution are explained below:</p><p>&quot;Yongshen Zhang and Shaoqun Zeng first developed the efficient method to improve the registration stability by encoding global axis information using regional grayscale inversion. It allowed the extraction of more features from the raw image data and is indeed helpful in our BIRDS multi-channel registration framework; Yongsheng Zhang also developed the dynamic z down-sampling together with Weilin Zeng (exisiting author), to achieve better registration initialization. Furthermore, Yongsheng Zhang contributed to the development of interactive framework, which allows the operation of the local registration by drawing arrows on the merged coarse registered images instead of marking points, and notable improves the efficiency of semiautomatic registration. &quot;</p><p><bold>New authors list:</bold> Xuechun Wang, Weilin Zeng, Xiaodan Yang, <bold>Yongsheng Zhang</bold>, Chunyu Fang, <bold>Shaoqun Zeng</bold>, Yunyun Han, Peng Fei</p><p><bold>Original authors list:</bold> Xuechun Wang, Weilin Zeng, Xiaodan Yang, Chunyu Fang, Yunyun Han, Peng Fei</p><p>Details for the omitted authors:</p><p><bold>Yongsheng Zhang</bold></p><p><sup>3</sup> Britton Chance Center for Biomedical Photonics, Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China.</p><p><bold>Contribution:</bold> Methodology, Software</p><p><bold>For correspondence:</bold> <ext-link ext-link-type="uri" xlink:href="https://mails.tsinghua.edu.cn/">ys-zhang20@mails.tsinghua.edu.cn</ext-link></p><p><bold>Contributed equally with:</bold> Xuechun Wang, Weiling Zeng, Xiaodan Yang, Chunyu Fang (In the original published article ChunYu Fang was originally not a co-first author)</p><p><bold>Competing interests: </bold>No competing interests exist</p><p><bold>Shaoqun Zeng</bold></p><p><sup>3</sup> Britton Chance Center for Biomedical Photonics, Wuhan National Laboratory for Optoelectronics, Huazhong University of Science and Technology, Wuhan, China.<bold>Contribution:</bold> Methodology</p><p><bold>Competing interests</bold>: No competing interests exist</p><p>Revised author contributions:</p><p>Xuechun Wang: Data curation, Software, Validation, Visualization, Writing - original draft</p><p>Weilin Zeng: Methodology, Software, Validation</p><p>Xiaodan Yang: Resources, Visualization</p><p>Yong Sheng Zhang: Methodology, Software</p><p>Chunyu Fang: Resources</p><p>Shaoqun Zeng: Methodology</p><p>Yunyun Han: Resources, Visualization, Writing - review and editing</p><p>Peng Fei: Software, Funding acquisition, Visualization, Writing - review and editing</p><p>Original author contributions:</p><p>Xuechun Wang: Data curation, Software, Validation, Visualization, Methodology, Writing - original draft</p><p>Weilin Zeng: Software, Validation</p><p>Xiaodan Yang: Resources, Visualization</p><p>Chunyu Fang, Resources, Visualization</p><p>Yunyun Han, Resources, Visualization, Writing - review and editing</p><p>Peng Fei, Software, Funding acquisition, Visualization, Writing - review and editing</p><p>Revised funding statement:</p><p><bold>Funding</bold></p><p>National Natural Science Foundation of China (21874052)</p><p>Peng Fei</p><p>National Natural Science Foundation of China (31871089)</p><p>Yunyun Han</p><p>Innovation Fund of WNLO</p><p>Peng Fei</p><p>Junior Thousand Talents Program of China</p><p>Yunyun Han</p><p>Peng Fei</p><p>The FRFCU (HUST:2172019kfyXKJC077)</p><p>Yunyun Han</p><p>National Key R&amp;D program of China (2017YFA0700501)</p><p>Peng Fei</p><p>973 Project (2015CB755603)</p><p>Shaoqun Zeng</p><p>Yongsheng Zhang</p><p>Director Fund of WNLO</p><p>Shaoqun Zeng</p><p>Yongsheng Zhang</p><p>Original funding statement:</p><p>Funding</p><p>National Natural Science Foundation of China (21874052)</p><p>Peng Fei</p><p>National Natural Science Foundation of China (31871089)</p><p>Yunyun Han</p><p>Innovation Fund of WNLO</p><p>Peng Fei</p><p>Junior Thousand Talents Program of China</p><p>Yunyun Han</p><p>Peng Fei</p><p>The FRFCU (HUST:2172019kfyXKJC077)</p><p>Yunyun Han</p><p>National Key R&amp;D program of China (2017YFA0700501)</p><p>Peng Fei</p><p>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</p><p><bold>Revised Acknowledgements:</bold></p><p>We thank Haohong Li, Luoying Zhang, Man Jiang, Bo Xiong for discussions and comments on the work and Hao Zhang for the help on the code implementation. This work was supported by the National Key R and D program of China (2017YFA0700501 PF), the National Natural Science Foundation of China (21874052 for PF, 31871089 for YH), the Innovation Fund of WNLO (PF) and the Junior Thousand Talents Program of China (PF and YH), the FRFCU (HUST:2172019kfyXKJC077 YH).</p><p><bold>Original Acknowledgements:</bold></p><p>We thank Yongsheng Zhang, Shaoqun Zeng, Haohong Li, Luoying Zhang, Man Jiang, Bo Xiong for discussions and comments on the work. Hao Zhang for the help on the code implementation. This work was supported by the National Key R and D program of China (2017YFA0700501 PF), the National Natural Science Foundation of China (21874052 for PF,31871089 for YH), the Innovation Fund of WNLO (PF) and the Junior Thousand Talents Program of China (PF and YH), the FRFCU (HUST:2172019kfyXKJC077 YH).</p><p>The article has been corrected accordingly.</p></body></article>