<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">85694</article-id><article-id pub-id-type="doi">10.7554/eLife.85694</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Computational and Systems Biology</subject></subj-group><subj-group subj-group-type="heading"><subject>Ecology</subject></subj-group></article-categories><title-group><article-title>Collaborative hunting in artificial agents with deep reinforcement learning</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes" id="author-102505"><name><surname>Tsutsui</surname><given-names>Kazushi</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3443-0749</contrib-id><email>k.tsutsui6@gmail.com</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund4"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-307648"><name><surname>Tanaka</surname><given-names>Ryoya</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6047-6030</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-307649"><name><surname>Takeda</surname><given-names>Kazuya</given-names></name><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-307650"><name><surname>Fujii</surname><given-names>Keisuke</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-5487-4297</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="other" rid="fund1"/><xref ref-type="other" rid="fund3"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04chrp450</institution-id><institution>Graduate School of Informatics, Nagoya University</institution></institution-wrap><addr-line><named-content content-type="city">Nagoya</named-content></addr-line><country>Japan</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04chrp450</institution-id><institution>Institute for Advanced Research, Nagoya University</institution></institution-wrap><addr-line><named-content content-type="city">Nagoya</named-content></addr-line><country>Japan</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04chrp450</institution-id><institution>Graduate School of Science, Nagoya University</institution></institution-wrap><addr-line><named-content content-type="city">Nagoya</named-content></addr-line><country>Japan</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04chrp450</institution-id><institution>Institute of Innovation for Future Society, Nagoya University</institution></institution-wrap><addr-line><named-content content-type="city">Nagoya</named-content></addr-line><country>Japan</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/03ckxwf91</institution-id><institution>RIKEN Center for Advanced Intelligence Project</institution></institution-wrap><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00097mb19</institution-id><institution>PRESTO, Japan Science and Technology Agency</institution></institution-wrap><addr-line><named-content content-type="city">Tokyo</named-content></addr-line><country>Japan</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>MacIver</surname><given-names>Malcolm A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Northwestern University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>07</day><month>05</month><year>2024</year></pub-date><volume>13</volume><elocation-id>e85694</elocation-id><history><date date-type="received" iso-8601-date="2022-12-20"><day>20</day><month>12</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2024-03-19"><day>19</day><month>03</month><year>2024</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-10-11"><day>11</day><month>10</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.10.10.511517"/></event></pub-history><permissions><copyright-statement>© 2024, Tsutsui et al</copyright-statement><copyright-year>2024</copyright-year><copyright-holder>Tsutsui et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-85694-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-85694-figures-v1.pdf"/><related-article related-article-type="article-reference" ext-link-type="doi" xlink:href="10.7554/eLife.84154" id="ra1"/><abstract><p>Collaborative hunting, in which predators play different and complementary roles to capture prey, has been traditionally believed to be an advanced hunting strategy requiring large brains that involve high-level cognition. However, recent findings that collaborative hunting has also been documented in smaller-brained vertebrates have placed this previous belief under strain. Here, using computational multi-agent simulations based on deep reinforcement learning, we demonstrate that decisions underlying collaborative hunts do not necessarily rely on sophisticated cognitive processes. We found that apparently elaborate coordination can be achieved through a relatively simple decision process of mapping between states and actions related to distance-dependent internal representations formed by prior experience. Furthermore, we confirmed that this decision rule of predators is robust against unknown prey controlled by humans. Our computational ecological results emphasize that collaborative hunting can emerge in various intra- and inter-specific interactions in nature, and provide insights into the evolution of sociality.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>From wolves to ants, many animals are known to be able to hunt as a team. This strategy may yield several advantages: going after bigger preys together, for example, can often result in individuals spending less energy and accessing larger food portions than when hunting alone. However, it remains unclear whether this behavior relies on complex cognitive processes, such as the ability for an animal to represent and anticipate the actions of its teammates. It is often thought that ‘collaborative hunting’ may require such skills, as this form of group hunting involves animals taking on distinct, tightly coordinated roles – as opposed to simply engaging in the same actions simultaneously.</p><p>To better understand whether high-level cognitive skills are required for collaborative hunting, Tsutsui et al. used a type of artificial intelligence known as deep reinforcement learning. This allowed them to develop a computational model in which a small number of ‘agents’ had the opportunity to ‘learn’ whether and how to work together to catch a ‘prey’ under various conditions. To do so, the agents were only equipped with the ability to link distinct stimuli together, such as an event and a reward; this is similar to associative learning, a cognitive process which is widespread amongst animal species.</p><p>The model showed that the challenge of capturing the prey when hunting alone, and the reward of sharing food after a successful hunt drove the agents to learn how to work together, with previous experiences shaping decisions made during subsequent hunts. Importantly, the predators started to exhibit the ability to take on distinct, complementary roles reminiscent of those observed during collaborative hunting, such as one agent chasing the prey while another ambushes it.</p><p>Overall, the work by Tsutsui et al. challenges the traditional view that only organisms equipped with high-level cognitive processes can show refined collaborative approaches to hunting, opening the possibility that these behaviors may be more widespread than originally thought – including between animals of different species.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>collaboration</kwd><kwd>multi-agent systems</kwd><kwd>deep reinforcement learning</kwd><kwd>multi-agent reinforcement learning</kwd><kwd>predator-prey interactions</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>20H04075</award-id><principal-award-recipient><name><surname>Fujii</surname><given-names>Keisuke</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>21H04892</award-id><principal-award-recipient><name><surname>Takeda</surname><given-names>Kazuya</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>21H05300</award-id><principal-award-recipient><name><surname>Fujii</surname><given-names>Keisuke</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001691</institution-id><institution>Japan Society for the Promotion of Science</institution></institution-wrap></funding-source><award-id>22K17673</award-id><principal-award-recipient><name><surname>Tsutsui</surname><given-names>Kazushi</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002241</institution-id><institution>Japan Science and Technology Agency</institution></institution-wrap></funding-source><award-id>JPMJPR20CA</award-id><principal-award-recipient><name><surname>Fujii</surname><given-names>Keisuke</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Collaborative hunting, characterized by the division of roles among predators, has emerged within a group of artificial agents through deep reinforcement learning.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Cooperation among animals often provides fitness benefits to individuals in a competitive natural environment (<xref ref-type="bibr" rid="bib58">Smith, 1982</xref>; <xref ref-type="bibr" rid="bib1">Axelrod and Hamilton, 1981</xref>). Cooperative hunting, in which two or more individuals engage in a hunt to successfully capture prey, has been regarded as one of the most widely distributed forms of cooperation in animals (<xref ref-type="bibr" rid="bib49">Packer and Ruttan, 1988</xref>), and has received considerable attention because of the close links between cooperative behavior, its apparent cognitive demand, and even sociality (<xref ref-type="bibr" rid="bib43">Macdonald, 1983</xref>; <xref ref-type="bibr" rid="bib19">Creel and Creel, 1995</xref>; <xref ref-type="bibr" rid="bib14">Brosnan et al., 2010</xref>; <xref ref-type="bibr" rid="bib40">Lang and Farine, 2017</xref>). Cooperative hunts have been documented in a wide variety of species (<xref ref-type="bibr" rid="bib40">Lang and Farine, 2017</xref>; <xref ref-type="bibr" rid="bib2">Bailey et al., 2013</xref>), yet ‘collaboration’ (or ‘collaborative hunting’), in which predators play different and complementary roles, has been reported in only a handful of vertebrate species (<xref ref-type="bibr" rid="bib60">Stander, 1992</xref>; <xref ref-type="bibr" rid="bib7">Boesch and Boesch, 1989</xref>; <xref ref-type="bibr" rid="bib27">Gazda et al., 2005</xref>). For instance, previous studies have shown that mammals such as lions and chimpanzees are capable of dividing roles among individuals, such as when chasing prey or blocking the prey’s escape path, to facilitate capture by the group (<xref ref-type="bibr" rid="bib60">Stander, 1992</xref>; <xref ref-type="bibr" rid="bib7">Boesch and Boesch, 1989</xref>). Collaborative hunts appear to be achieved through elaborate coordination with other hunters, and are often believed to be an advanced hunting strategy requiring large brains that involve high-level cognition such as aspects of theory of mind (<xref ref-type="bibr" rid="bib9">Boesch and Boesch-Achermann, 2000</xref>; <xref ref-type="bibr" rid="bib10">Boesch, 2002</xref>).</p><p>However, recent findings have placed this previous belief under strain. In particular, cases of intra- and inter-specific collaborative hunting have also been demonstrated in smaller-brained vertebrates such as birds (<xref ref-type="bibr" rid="bib5">Bednarz, 1988</xref>), reptiles (<xref ref-type="bibr" rid="bib20">Dinets, 2015</xref>), and fish (<xref ref-type="bibr" rid="bib15">Bshary et al., 2006</xref>; <xref ref-type="bibr" rid="bib62">Steinegger et al., 2018</xref>). It seems possible that apparently elaborate hunting behavior can emerge in a relatively simple decision process in response to ecological needs (<xref ref-type="bibr" rid="bib62">Steinegger et al., 2018</xref>). However, the decision process underlying collaborative hunting remains poorly understood because most previous studies thus far have relied exclusively on behavioral observations. Observational studies are essential for documenting such natural behavior, yet it is often difficult to identify the specific decision process that results in coordinated behavior. This limitation arises because seemingly simple behavior can result from complex processes (<xref ref-type="bibr" rid="bib22">Evans et al., 2019</xref>) and vice versa (<xref ref-type="bibr" rid="bib18">Couzin et al., 2002</xref>).</p><p>We, therefore, sought to further our understanding of the processes underlying collaborative hunting by adopting a different approach, namely, computational multi-agent simulation based on deep reinforcement learning. Deep reinforcement learning mechanisms were originally inspired by animal associative learning (<xref ref-type="bibr" rid="bib64">Sutton and Barto, 1981</xref>), and are thought to be closely related to neural mechanisms for reward-based learning centering on dopamine (<xref ref-type="bibr" rid="bib55">Schultz et al., 1997</xref>; <xref ref-type="bibr" rid="bib51">Samejima et al., 2005</xref>; <xref ref-type="bibr" rid="bib21">Doya, 2008</xref>). Given that associative learning is likely to be the most widely adopted learning mechanism in animals (<xref ref-type="bibr" rid="bib44">Mackintosh, 1974</xref>; <xref ref-type="bibr" rid="bib78">Wynne, 2001</xref>), collaborative hunting could arise through associative learning, where simple decision rules are developed based on behavioral cues [i.e. contingencies of reinforcement (<xref ref-type="bibr" rid="bib57">Skinner, 2014</xref>)].</p><p>Specifically, we first explored whether predator agents based on deep reinforcement learning learn decision rules resulting in collaborative hunting and, if so, under what conditions through predator-prey interactions in a computational ecological environment. We then examined what internal representations are associated with the decision rules. Furthermore, we confirmed the generality of the acquired predators’ decision rules using joint plays between agents (predators) and humans (prey). Notably, our predator agents successfully learned to collaborate in capturing their prey solely through a reinforcement learning algorithm, without employing explicit mechanisms comparable to aspects of theory of mind (<xref ref-type="bibr" rid="bib79">Yoshida et al., 2008</xref>; <xref ref-type="bibr" rid="bib25">Foerster, 2019</xref>; <xref ref-type="bibr" rid="bib32">Hu and Foerster, 2020</xref>). Moreover, our results showed that the acquisition of decision rules resulting in collaborative hunting is facilitated by a combination of two factors: the difficulty of capturing prey during solitary hunting, and food (i.e. reward) sharing following capture. We also found that decisions underlying collaborative hunts were related to distance-dependent internal representations formed by prior experience. Furthermore, the decision rules worked robustly against unknown prey controlled by humans. These provide insight that collaborative hunts do not necessarily require sophisticated cognitive mechanisms, and simple decision rules based on mappings between states and actions can be practically useful in nature. Our results support the recent suggestions that the underlying processes facilitating collaborative hunting can be relatively simple (<xref ref-type="bibr" rid="bib40">Lang and Farine, 2017</xref>).</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>We set out to model the decision process of predators and prey in an interactive environment. In this study, we focused on a chase and escape scenario in a two-dimensional open environment. Chase and escape is a potentially complex phenomenon in which two or more agents interact in environments that change from moment to moment. Nevertheless, many studies have shown that the rules of chase/escape behavior (e.g. which direction to move at each time in a given situation) can be described by relatively simple mathematical models consisting of the current state (e.g. positions and velocities) (<xref ref-type="bibr" rid="bib13">Brighton et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Tsutsui et al., 2020</xref>; <xref ref-type="bibr" rid="bib31">Howland, 1974</xref>). We, therefore, considered modeling the agent’s decision process in a standard reinforcement learning framework for a finite Markov decision process in which each sequence is a distinct state. In this framework, the agent interacts with the environment through a sequence of states, actions, and rewards, and aims to select actions in a manner that maximizes cumulative future reward (<xref ref-type="bibr" rid="bib65">Sutton and Barto, 2018</xref>).</p><fig-group><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Agent architecture and examples of movement trajectories.</title><p>(<bold>a</bold>) An agent’s policy is represented by a deep neural network (see Methods). A state of the environment is given as input to the network. An action is sampled from the network’s output, and the agent receives a reward and a subsequent state. The agent learns to select actions that maximize cumulative future rewards. In this study, each agent learned its policy network independently, that is, each agent treats the other agents as part of the environment. This illustration shows a case with three predators. (<bold>b</bold>) The movement trajectories are examples of interactions between predator(s) (dark blue, blue, and light blue) and prey (red) that overlay 10 episodes in each experimental condition. The experimental conditions were set as the number of predators (one, two, or three), relative mobility (fast, equal, or slow), and reward sharing (individual or shared), based on ecological findings.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig1-v1.tif"/></fig><fig id="fig1s1" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 1.</label><caption><title>Network architecture.</title><p>The neural network is composed of four layers. The input to the neural network was the state and the output was each possible action, namely, a total of 13 action of the ‘acceleration’ in 12 directions every 30 degrees in the relative coordinate system and ‘do nothing.’ After the first two hidden layers of the MLP with 64 units, the network branches off into two streams. Each branch has one MLP layer with 32 hidden units. Rectified linear unit (ReLU) was used as the activation function for each layer. In the visualization of the agents’ internal representations, the 32-dimensional hidden vector (parts filled in gray) was embedded in two dimensions, using t-distributed stochastic neighbor embedding (t-SNE).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig1-figsupp1-v1.tif"/></fig><fig id="fig1s2" position="float" specific-use="child-fig"><label>Figure 1—figure supplement 2.</label><caption><title>Diagram of model input.</title><p>We used position and velocity information as the state (model input) for each agent. Assuming subjective observation, each variable, except absolute position, was converted to a relative coordinate system to the opponent; namely, prey for predators and nearest predator for prey, and inputted to the model. Moreover, for the prey input in the three-predator condition, the predator indices were sorted according to the distance between the prey and each predator. Specifically, we considered predator 1, predator 2, and predator 3 descending order in terms of distance. Similarly, in the predator input in the three-predator condition, we set itself as predator 1, the closer predator to itself as predator 2, and the farther predator as predator 3. In the figure, abs., rel.,bold p, and bold v denote an absolute, relative, position, and velocity, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig1-figsupp2-v1.tif"/></fig></fig-group><sec id="s2-1"><title>Exploring the conditions under which collaborative hunting emerges</title><p>We first performed computational simulations with three experimental conditions to investigate the conditions under which collaborative hunting emerges (<xref ref-type="fig" rid="fig1">Figure 1b</xref>; <xref ref-type="video" rid="video1">Videos 1</xref>–<xref ref-type="video" rid="video3">3</xref>). As experimental conditions, we selected the number of predators, relative mobility, and prey (reward) sharing based on ecological findings (<xref ref-type="bibr" rid="bib2">Bailey et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Lang and Farine, 2017</xref>). For the number of predators, three conditions were set: 1 (one), 2 (two), and 3 (three). In all these conditions, the number of prey was set to 1. For the relative mobility, three conditions were set: 120% (fast), 100% (equal), and 80% (slow), which represented the acceleration of the predator, based on that of the prey. For the prey sharing, two conditions were set: with sharing (shared), in which all predators were rewarded when a predator catches the prey, and without sharing (individual), in which a predator was rewarded only when it catches the prey by itself. In total, there were 15 conditions.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-85694-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Example videos in the one-predator conditions.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-85694-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Example videos in the two-predator conditions.</title></caption></media><media mimetype="video" mime-subtype="mp4" xlink:href="elife-85694-video3.mp4" id="video3"><label>Video 3.</label><caption><title>Example videos in the three-predator conditions.</title></caption></media><p>As the example trajectories show, under the fast and equal conditions, the predators often caught their prey shortly after the episode began, whereas under the slow condition, the predators somewhat struggled to catch their prey (Fig. 1b). To evaluate their behavior, we calculated the proportion of predations that were successful and mean episode duration. For the fast and equal conditions, predations were successful in almost all episodes, regardless of the number of predators and the presence or absence of reward sharing (e.g. 0.99 ± 0.00 for the one × fast and one × equal conditions; <xref ref-type="fig" rid="fig2s1">Figure 2—figure supplement 1</xref>). This indicates that in situations where predators were faster than or equal in speed to their prey, they almost always succeeded in capturing the prey, even when they were the sole predator. Although the mean episode duration decreased with an increasing number of predators in both fast and equal conditions, the difference was small. As a whole, these results indicate that there is little benefit of cooperation among multiple predators in the fast and equal conditions. As it is unlikely that cooperation among predators will emerge under such conditions in nature from an evolutionary perspective (<xref ref-type="bibr" rid="bib58">Smith, 1982</xref>; <xref ref-type="bibr" rid="bib1">Axelrod and Hamilton, 1981</xref>), the analysis below is limited to the slow condition. For the slow condition, a solitary predator was rarely successful, and the proportion of predations that were successful increased with the number of predators (<xref ref-type="fig" rid="fig2">Figure 2a</xref>). Moreover, the mean duration decreased with an increasing number of predators (<xref ref-type="fig" rid="fig2">Figure 2a</xref> bottom). These results indicate that, under the slow condition, the benefits of cooperation among multiple predators are significant. In addition, except for the two × individual condition, the increase in the proportion of success with an increasing number of predators was much greater than the theoretical prediction (<xref ref-type="bibr" rid="bib49">Packer and Ruttan, 1988</xref>), calculated based on the proportion of solitary hunting, assuming that each predator’s performance is independent of the others’ (see Methods). These results indicate that under these conditions, elaborate hunting behavior (e.g. ‘collaboration’) that is qualitatively different from hunting alone may emerge.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Emergence of collaborations among predators.</title><p>(<bold>a</bold>) Proportion of predations that were successful (top) and mean episode duration (bottom). For both panels, quantitative data denote the mean of 100 episodes ± SEM across 10 random seeds. The error bars are barely visible because the variation is negligible. The theoretical prediction values were calculated based on the proportion of solitary hunts (see Methods). The proportion of predations that were successful increased as the number of predators increased (<inline-formula><mml:math id="inf1"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>18</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 1346.67, <inline-formula><mml:math id="inf2"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001; <inline-formula><mml:math id="inf3"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> = 0.87; one vs. two: <inline-formula><mml:math id="inf4"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 20.38, <inline-formula><mml:math id="inf5"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>p</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>&lt;0.001; two vs. three: <inline-formula><mml:math id="inf6"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 38.27, <inline-formula><mml:math id="inf7"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001). The mean duration decreased with increasing number of predators (<inline-formula><mml:math id="inf8"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>18</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 1564.01, <inline-formula><mml:math id="inf9"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001; <inline-formula><mml:math id="inf10"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> = 0.94; one vs. two: <inline-formula><mml:math id="inf11"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 15.98, <inline-formula><mml:math id="inf12"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001; two vs. three: <inline-formula><mml:math id="inf13"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 40.65, <inline-formula><mml:math id="inf14"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001). (<bold>b</bold>) Typical example of different predator routes between the individual (left) and shared (right) conditions, in the two-predator condition. The numbers (1–3) show a series of state transitions (every second) starting from the same initial position. Each panel shows the agent positions and the trajectories leading up to that state. In these instances, the predators ultimately failed to capture the prey within the time limit (30 s) under the individual condition, whereas the predators successfully captured the prey in only 3 s under the shared condition. (<bold>c</bold>) Comparison of heat maps between individual (left) and shared (right) reward conditions. The heat maps of each agent were constructed based on the frequency of stay in each position, which was cumulative for 1000 episodes (100 episodes × 10 random seeds). In the individual condition, there were relatively high correlations between the heat maps of the prey and each predator, regardless of the number of predators (One:<inline-formula><mml:math id="inf15"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.95,<inline-formula><mml:math id="inf16"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001, Two:<inline-formula><mml:math id="inf17"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.83,<inline-formula><mml:math id="inf18"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 1,<inline-formula><mml:math id="inf19"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.78,<inline-formula><mml:math id="inf20"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 2, Three:<inline-formula><mml:math id="inf21"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.41,<inline-formula><mml:math id="inf22"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 1,<inline-formula><mml:math id="inf23"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.56,<inline-formula><mml:math id="inf24"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 2,<inline-formula><mml:math id="inf25"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.45,<inline-formula><mml:math id="inf26"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 3). In contrast, in the shared condition, only one predator had a relatively high correlation, whereas the others had low correlations (Two:<inline-formula><mml:math id="inf27"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.65,<inline-formula><mml:math id="inf28"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 1,<inline-formula><mml:math id="inf29"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.01,<inline-formula><mml:math id="inf30"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>=0.80 in predator 2, Three:<inline-formula><mml:math id="inf31"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.17,<inline-formula><mml:math id="inf32"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 1,<inline-formula><mml:math id="inf33"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.54,<inline-formula><mml:math id="inf34"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001 in predator 2,<inline-formula><mml:math id="inf35"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi></mml:mstyle></mml:math></inline-formula>=0.03,<inline-formula><mml:math id="inf36"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>=0.23 in predator 3).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 1.</label><caption><title>Proportion of predations that were successful, mean episode duration, and heat maps for each condition.</title><p>For both panels, quantitative data denote the mean of 100 episodes ± SEM across 10 random seeds.The theoretical prediction values were calculated based on the proportion of solitary hunts (see Methods). The heatmap of each agent was constructed based on the frequency of stay in each position, which was cumulative for 1,000episodes (100 episodes × 10 random seeds).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 2.</label><caption><title>Circular histogram, concordance rate, and circular correlation.</title><p>To visualize theassociation between each predator in the two- and three-predator conditions and the baseline, which is the predatorin the one-predator condition, we produced overlays of the frequency of selection for each action. We calculated concordance rates and circular correlations to quantitatively evaluate these associations of action selection. The concordance rate has the advantage of being able to compare all 13 actions, whereas the circular correlation has theadvantage of being able to consider the proximity among each action, although it can only evaluate 12 actions,excluding ‘do nothing.’ As shown in this figure, the two indices showed similar trends. The predators whose heat maps were similar to that of their prey tended to have higher values on these indices. For all panels, quantitative datadenote the mean of 100 episodes ± SEM across 10 random seeds.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig2-figsupp2-v1.tif"/></fig><fig id="fig2s3" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 3.</label><caption><title>Scaled distance among predators and proportion of prey capture.</title><p>Scaled distance is ameasure of how far a predator moves to capture its prey compared with the other predators during a hunt. Although simplified, this distribution reflects the role each predator played in a hunt. Specifically, if there is a large difference in the scaled distance among predators, individuals with a larger scaled distance (greater than 1) could play the role of ‘chaser’ (or ‘driver’), while individuals with a smaller scaled distance (less than 1) could play the role of ‘blocker’ (or ‘ambusher’). Moreover, if these distances do not differ among predators (concentrated near 1), it is likely that each predator pursued prey in the same manner, suggesting that there was no role division during the hunt. Furthermore, these distributions can be used to capture the flexibility of role division among predators. That is, if the distributions are separate and do not overlap, there is a division of roles among predators, and these roles are fixed inany hunt. On the other hand, if the distributions are separate but some of them overlap, the roles may have switched across hunts. Our results show that the distribution in the individual condition was concentrated around 1, whereas in the shared condition it was divided among individuals. This means that there was rarely role division among predators in the individual condition, while there was role division in the shared condition. These characteristics were more pronounced in the two-predator condition than in the three-predator condition. Perhaps this depends on the episode duration; the duration tends to be longer in the two-predator condition, and the difference in distance is likely to be clearer. Note that even under the two-predator condition, there was some overlap in the distribution inthe shared condition. This indicates that the basic roles were fixed among individuals, but interchanged according to the situation (or episode) in the condition. Additionally, because these role divisions are often discussed in the context of cooperation and cheating, we calculated the proportion of prey capture for each predator. The results did not indicate which role was more likely to catch the prey. That is, in the shared × two conditions, the chaser tended to catch more prey, but, on the other hand, in the shared × three conditions, the blocker tended to catch more prey. For all panels, quantitative data denote the mean of 100 episodes ± SEM across 10 random seeds.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig2-figsupp3-v1.tif"/></fig><fig id="fig2s4" position="float" specific-use="child-fig"><label>Figure 2—figure supplement 4.</label><caption><title>Typical example of coordinated hunting behavior in the three × individual condition.</title><p>The numbers (1 to 3) show a series of state transitions (every 0.6 s). Each panel shows the agent positions and the trajectories leading up to that state.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig2-figsupp4-v1.tif"/></fig></fig-group><p>Then, we examined agent behavioral patterns and found that there were differences in the movement paths that predators take to catch their prey among the conditions (<xref ref-type="fig" rid="fig2">Figure 2b</xref>). As shown in the typical example, under the individual condition, both predators moved in a similar manner toward their prey (<xref ref-type="fig" rid="fig2">Figure 2b</xref> left) and, in contrast, under the shared condition, one predator moved toward their prey while the other predator moved along a different route (<xref ref-type="fig" rid="fig2">Figure 2b</xref> right). To ascertain their behavioral patterns, we created heat maps showing the frequency of agent presence at each location in the area (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). We found that there was a noticeable difference between the individual and shared reward conditions. In the individual condition, the heat maps of prey and respective predators were quite similar (<xref ref-type="fig" rid="fig2">Figure 2c</xref>), whereas this was not always the case in the shared condition (<xref ref-type="fig" rid="fig2">Figure 2c</xref>). In particular, the heat maps of predator 2 in the two-predator condition and predator 3 in the three-predator condition showed localized concentrations (<xref ref-type="fig" rid="fig2">Figure 2c</xref> far right, respectively). To assess these differences among predators in more detail, we compared the predators’ decisions (i.e. action selections) in these conditions with that in the one-predator condition (i.e. solitary hunts) using two indices, concordance rate, and circular correlation (<xref ref-type="bibr" rid="bib6">Berens, 2009</xref>; <xref ref-type="fig" rid="fig2s2">Figure 2—figure supplement 2</xref>). Following previous studies (<xref ref-type="bibr" rid="bib54">Scheel and Packer, 1991</xref>), we also calculated the ratios of distance moved during hunting among predators (<xref ref-type="fig" rid="fig2s3">Figure 2—figure supplement 3</xref>). Overall, these findings support the idea that predators with heat maps similar to their prey acted as ‘chasers’ (or ‘drivers’), while predators with different heat maps behaved as ‘blockers’ (or ‘ambushers’). That is, our results show that, although most predators acted as chasers, some predators acted as blockers rather than chasers in the shared condition, indicating the emergence of collaborative hunting characterized by role divisions among predators under the condition.</p></sec><sec id="s2-2"><title>Mechanistic interpretability of collaboration</title><p>We next sought the predators’ internal representations to better understand how such collaborative hunting is accomplished. Using a two-dimensional t-distributed stochastic neighbor embedding (t-SNE) (<xref ref-type="bibr" rid="bib73">van der Maaten and Hinton, 2008</xref>), we visualized the last hidden layers of the state and action streams in the policy network as internal representations of agents (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3—figure supplements 1</xref>–<xref ref-type="fig" rid="fig3s3">3</xref>). To understand how each agent represents its environment and what aspects of the state are well represented, we examined the relationship between the scenes of a typical scenario and their corresponding points on the embedding (<xref ref-type="fig" rid="fig3">Figure 3a and b</xref>). As expected, when the predator is likely to catch its prey (e.g. scene 4), the predator estimated a higher state value, whereas, when the predator is not (e.g. scene 5), the predator estimated a lower state value (<xref ref-type="fig" rid="fig3">Figure 3a</xref> top). Related to this, the variance of action values tends to be larger for both predator and prey when they are close (<xref ref-type="fig" rid="fig3">Figure 3a</xref> bottom), indicating that the difference in the value of choosing each action is greater when the choice of action is directly related to the reward (see also <xref ref-type="fig" rid="fig3s4">Figure 3—figure supplement 4</xref>). These results suggest that the agents were able to learn the networks that output the estimations of state and action values consistent with our intuition.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Embedding of internal representations underlying collaborative hunting.</title><p>(<bold>a</bold>) Two-dimensional t-distributed stochastic neighbor embedding (t-SNE) embedding of the representations in the last hidden layers of the state-value stream (top) and action-value stream (bottom) in the shared reward condition. The representation is assigned by the policy network of each agent to states experienced during predator-prey interactions. The points are colored according to the state values and standard deviation of the action values, respectively, predicted by the policy network (ranging from dark red (high) to dark blue (low)). (<bold>b</bold>) Corresponding states for each number in each embedding. The number (1–5) in each embedding corresponds to a selected series of state transitions. The series of agent positions in the state transitions (every second) and, for ease of visibility, the trajectories leading up to that state are shown. (<bold>c</bold>) Embedding colored according to the distances between predators and prey in the individual (left) and shared (right) reward conditions. Distances 1 and 2 denote the distances between predator 1 and prey and predator 2 and prey, respectively. If both distances are short, the point is colored blue; if both are long, it is colored white.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 1.</label><caption><title>Two-dimensional t-distributed stochastic neighbor embedding (t-SNE) embedding of the representations in the last hidden layers of the state-value stream (top) and action-value stream (bottom) in the individual reward condition, in the slow × two conditions.</title><p>The representation is assigned by the policy network of each agent to states experienced during predator-prey interactions. The points are colored according to the state values and standard deviation of the actionvalues, respectively, predicted by the policy network (ranging from dark red (high) to dark blue (low)).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp1-v1.tif"/></fig><fig id="fig3s2" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 2.</label><caption><title>Two-dimensional t-distributed stochastic neighbor embedding (t-SNE) embedding colored according to the absolute coordinates of itself in the individual (left) and shared (right) reward conditions, in the slow × two conditions.</title><p>The absolute coordinates (i.e. x and y positions) are directly associated with the reward, as are the distances between prey and predators, because each agent receives a negative reward (-1) for leaving the play area. We, therefore, colored the internal representation of the agent according to its position. The upper left corner of the play area corresponds to cyan, the lower left to green, the upper right to white, and the lower right to yellow. The embedding of state representations in the prey seems to be roughly clustered according to absolute position, compared to those of the predators. These indicate that prey might estimate the state and action values and make decisions associated with absolute position-dependent representations compared to predators.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp2-v1.tif"/></fig><fig id="fig3s3" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 3.</label><caption><title>Two-dimensional t-distributed stochastic neighbor embedding (t-SNE) embedding of the representations in the last hidden layers ofstate-value stream and action-value stream, in the slow × three conditions.</title><p>The points are colored according to the state values and standard deviation of the action values predicted by the policy network (top), the distances between prey and predators (middle), and absolute coordinates of itself, respectively.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp3-v1.tif"/></fig><fig id="fig3s4" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 4.</label><caption><title>Corresponding state-action values (Q-values) for each state.</title><p>We show the value for eachaction of each agent in a selected series of state transitions (scenes 1 to 5). The panels on the left side of the figure are the same as the plot in <xref ref-type="fig" rid="fig3">Figure 3</xref>. For both predator and prey, each action is defined in terms of a relative coordinate system to the opponent. In other words, action 1 denotes movement toward the opponent (for the prey, the nearest predator) and action 7 denotes movement in the opposite direction of the opponent. Thus, in the estimated action values of the prey, actions 5 to 9 tend to show relatively high values, and in those of predators, actions 1, 2, 3, 11, and 12 tend to show relatively high values. These results suggest that proximity to rewards plays a major role in estimating state-action values.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp4-v1.tif"/></fig><fig id="fig3s5" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 5.</label><caption><title>Rule-based predator agent architectures.</title><p>For consistency with the deep reinforcement learning agents, the input to the rule-based agents used to make decisions is limited to the current information (e.g. position and velocity), and the output is provided in a relative coordinate system to the prey. The predator first determines whether it, or another predator, is closer to the prey, and then, if the other predator is closer, it determines whether the distance 2 is less than the specified distance threshold. The decision rule for each predator is selected by this branching, with predator 1 adopting the three rules ‘chase,’ ‘shortcut,’ and ‘approach,’ and predator 2 adopting the two rules ‘chase’ and ‘ambush’ (see Methods for details). In the chase, the predator first determines whether it is near the outer edge of the play area and, if so, selects actions that will prevent it from leaving the play area. If the predator is not on the outside of the play area, then it determines whether the prey is on the inside of the play area,and, if so, selects actions that will drive them to the outside. In other situations, it selects actions so that the direction of movement is aligned with that of their prey. In the shortcut, the predator determines whether it is near the outeredge of the play area, and if so, selects the actions described above, otherwise selects actions that will produce shorter paths to the prey. In the approach, the predator determines whether it is near the outer edge of the play area and, if so, selects the actions described above, otherwise it selects actions that move it toward the prey. In the ambush, the predator selects actions that move toward the top center or bottom center of the play area and remain there until the situation changes.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp5-v1.tif"/></fig><fig id="fig3s6" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 6.</label><caption><title>Movement trajectories (left) and heat maps (right) of the rule-based predator agents.</title><p>The movement trajectories are examples of predator(s) (dark blue and blue) and prey (red) interactions that overlay 10 episodes. The proportion of successful predation and mean episode duration were 74.4 ± 0.54 and 12.9 ± 0.12 (mean of 100 episodes ± SEM across 10 random seeds), respectively. The heat map of each agent was constructed based on the frequency of stay in each position, which is cumulative for 1,000 episodes (100 episodes × 10 random seeds). One predator had a relatively high correlation between the heat maps, whereas the others had a lowcorrelation (<italic>r</italic> = 0.60, <italic>p</italic>&lt;0.001 in predator 1, <italic>r</italic> = 0.20, <italic>p</italic>&lt;0.001 in predator 2). This trend in results is similar to that in the deep reinforcement learning predator agents (compare <xref ref-type="fig" rid="fig2">Figure 2c</xref>). Note that, in the rule-based agentsimulation, the prey’s decision was made by a policy network in the two × shared condition (i.e. predatorrule-based agent vs. prey deep reinforcement learning agent).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp6-v1.tif"/></fig><fig id="fig3s7" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 7.</label><caption><title>Two-dimensional t-distributed stochastic neighbor embedding (t-SNE) embedding of the representations in the last hidden layers ofthe linear network (top) and the nonlinear network (bottom) in behavioral cloning.</title><p>The embedding is colored according to the distances between predators and prey. Distances 1 and 2 denote the distance between predator 1 and prey and predator 2 and prey, respectively. If both distances are short, the point is colored blue; if both are long, it is colored white. Note that the accuracy in behavioral cloning from the top 1 to the top 5 was, in ascending order, 0.47, 0.61, 0.71, 0.78, 0.80 for predator 1 and 0.44, 0.55, 0.60, 0.66, and 0.70 for predator 2 for the linear network, and 0.65, 0.77, 0.82, 0.88, and 0.95 for predator 1 and 0.68, 0.78, 0.82, 0.85, and 0.90 for predator 2 for the nonlinear network.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp7-v1.tif"/></fig><fig id="fig3s8" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 8.</label><caption><title>Histogram of the state value (V-value) in the individual (left) and shared (right) conditions.</title><p>In coloring the embedding, the lower and upper limits of coloring were set to the fifth percentile and 95<sup>th</sup> percentile (gray lines), respectively, to prevent visibility from being compromised by extreme values that rarely occur.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp8-v1.tif"/></fig><fig id="fig3s9" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 9.</label><caption><title>Histogram of the standard deviation of state-action values (Q-values) in individual (left) and shared (right) conditions.</title><p>In coloring the embedding, the lower and upper limits of coloring were set to the fifth percentile and 95<sup>th</sup> percentile (gray lines), respectively, to prevent visibility from being compromised by extreme values that rarely occur.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp9-v1.tif"/></fig><fig id="fig3s10" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 10.</label><caption><title>Histogram of the distance between the prey and each predator in individual (left) and shared (right) conditions.</title><p>In coloring the embedding, the lower and upper limits of coloring were set to the fifth percentile and 95<sup>th</sup> percentile (gray lines), respectively, to prevent visibility from being compromised by extreme values that rarely occur.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp10-v1.tif"/></fig><fig id="fig3s11" position="float" specific-use="child-fig"><label>Figure 3—figure supplement 11.</label><caption><title>Histogram of the distance between the prey and each predator in the simulations, using rule-based predator agents.</title><p>In coloring the embedding in behavioral cloning, the lower and upper limits of coloring were set to the fifth percentile and 95<sup>th</sup> percentile (gray lines), respectively, to prevent visibility from being compromised by extreme values that rarely occur.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig3-figsupp11-v1.tif"/></fig></fig-group><p>Furthermore, we found a distinct feature in the embedding of predators’ representations. Specifically, in certain state transitions, the position of the points on the embedding changed little, even though the agents were moving (e.g. scenes 1–2 on the embedding of the predator 2). From this, we deduced that the predators’ representations may be more focused on encoding the distance between themselves and others, rather than the specific locations of both parties. To test our reasoning, we colored the representations according to the distance between predators and prey; distance 1 denotes the distance between predator 1 and the prey, and distance 2 denotes that between predator 2 and the prey. As a result, the representations of predators in the shared condition could be clearly separated by the distance-dependent coloration (Fig. 3c right), in contrast to those in the individual condition (Fig. 3c left). These indicate that the predators in the shared condition estimated state and action values and made decisions associated with distance-dependent representations (see <xref ref-type="fig" rid="fig3s2">Figure 3—figure supplement 2</xref> for the prey’s decision).</p></sec><sec id="s2-3"><title>Evaluating the playing strength of predator agents using joint play with humans</title><p>Finally, to verify the generality of predators’ decisions against unknown prey, we conducted an experiment of joint play between agents and humans. In the joint play, human participants controlled prey on a screen using a joystick. The objective, as in the computational simulation described above, was to evade capture until the end of the episode (30 s) while remaining within the area. We found that the outcomes of the joint play showed similar trends to those of the computer simulation (<xref ref-type="fig" rid="fig4">Figure 4a</xref>), showing that the proportion of predations that were successful increased and the mean episode duration decreased as the number of predators increased. These indicate that the predator agents’ decision rules worked well for the prey controlled by humans. To visualize the associations of states experienced by predator agents versus agents and versus humans, we show colored two-dimensional t-SNE embedding of the representations in the last hidden layers of the state and action streams (<xref ref-type="fig" rid="fig4">Figure 4b</xref>, <xref ref-type="fig" rid="fig4s1">Figure 4—figure supplement 1</xref>). These showed that, in contrast to a previous study (<xref ref-type="bibr" rid="bib45">Mnih et al., 2015</xref>), the states were quite distinct, suggesting that predator agents experienced unfamiliar states when playing against the prey controlled by humans. This unfamiliarity may make it difficult for predators to make proper decisions. Indeed, in the one-predator condition, the predator agent occasionally exhibited odd behavior (e.g. staying in one place; see <xref ref-type="fig" rid="fig4s2">Figure 4—figure supplement 2</xref>). On the other hand, in the two- and three-predator conditions, predator agents rarely exhibited such behavior and showed superior performance. This indicates that decision rules of cooperative hunting acquired in certain environments could be applied in other somewhat different environments.</p><fig-group><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Superior performance of predator agents for prey controlled by humans and comparison of internal representations.</title><p>(<bold>a</bold>) Proportion of predations that were successful (top) and mean episode duration (bottom). For both panels, the thin line denotes the performance of each participant, and the thick line denotes the mean. The theoretical prediction values were calculated based on the mean of proportion of solitary hunts. The proportion of predations that were successful increased as the number of predators increased (<inline-formula><mml:math id="inf37"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>1.28</mml:mn><mml:mo>,</mml:mo><mml:mn>11.48</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 276.20, <inline-formula><mml:math id="inf38"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001; <inline-formula><mml:math id="inf39"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> = 0.90; one vs. two: <inline-formula><mml:math id="inf40"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 13.80, <inline-formula><mml:math id="inf41"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001; two vs. three: <inline-formula><mml:math id="inf42"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 5.9402, <inline-formula><mml:math id="inf43"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001). The mean duration decreased with an increasing number of predators (<inline-formula><mml:math id="inf44"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>F</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">b</mml:mi><mml:mi mathvariant="normal">e</mml:mi><mml:mi mathvariant="normal">r</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mn>18</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 23.77, <inline-formula><mml:math id="inf45"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001; <inline-formula><mml:math id="inf46"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> = 0.49; one vs. two: <inline-formula><mml:math id="inf47"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 2.60, <inline-formula><mml:math id="inf48"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>=0.029; two vs. three: <inline-formula><mml:math id="inf49"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">(</mml:mo><mml:mn>9</mml:mn><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> = 5.44, <inline-formula><mml:math id="inf50"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula>&lt;0.001). (<bold>b</bold>) Comparison of two-dimensional t-distributed stochastic neighbor embedding (t-SNE) embedding of the representations in the last hidden layers of state-value stream between self-play (predator agents vs. prey agent) and joint play (predator agents vs. prey human).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig4-v1.tif"/></fig><fig id="fig4s1" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 1.</label><caption><title>Comparison of two-dimensional t-distributed stochastic neighbor embedding (t-SNE) embedding of the internal representations.</title><p>To visualize the associations of states experienced by predator agents versus agents (self-play) and versus humans (jointplay), we show colored two-dimensional t-SNE embedding of the representations in the last hidden layer of the action-value stream. Similar to those of the state stream (<xref ref-type="fig" rid="fig4">Figure 4b</xref>), the experienced states were quite distinct,especially in the one- and two-predator conditions.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig4-figsupp1-v1.tif"/></fig><fig id="fig4s2" position="float" specific-use="child-fig"><label>Figure 4—figure supplement 2.</label><caption><title>Comparison of heat maps between individual (left) and shared (right) reward conditions in joint play.</title><p>The heat map of each agent was made based on the frequency of stay in each position which iscumulative for 500 episodes (50 episodes × 10 participants). Our results showed a similar trend between self-play (predator agents vs. prey agent) and joint play (predator agents vs. prey human) in terms of role division among predators. For example, in the individual condition, the heat maps between/among predators were similar, indicating that there was no clear division of roles. On the other hand, in the shared condition, the heat maps between/among predators differed and it indicates that the roles were divided among them. In addition, one of the differences fromself-play was the instability of the predator agent’s behavior in the one-predator condition. As shown in the figure, under certain conditions, the predator agent stopped moving from its location (the upper right corner of the area).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-85694-fig4-figsupp2-v1.tif"/></fig></fig-group></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Collaborative hunting has been traditionally thought of as an advanced hunting strategy that involves high-level cognition such as aspects of theory of mind (<xref ref-type="bibr" rid="bib9">Boesch and Boesch-Achermann, 2000</xref>; <xref ref-type="bibr" rid="bib10">Boesch, 2002</xref>). Here, we have shown that ‘collaboration’(<xref ref-type="bibr" rid="bib7">Boesch and Boesch, 1989</xref>) can emerge in group hunts of artificial agents based on deep reinforcement learning. Notably, our predator agents successfully learned to collaborate in capturing their prey solely through a reinforcement learning algorithm, without employing explicit mechanisms comparable to aspects of theory of mind (<xref ref-type="bibr" rid="bib79">Yoshida et al., 2008</xref>; <xref ref-type="bibr" rid="bib25">Foerster, 2019</xref>; <xref ref-type="bibr" rid="bib32">Hu and Foerster, 2020</xref>). This means that, in contrast to the traditional view, apparently elaborate coordination can be accomplished by relatively simple decision rules, that is, mappings between states and actions. This result advances our understanding of cooperative hunting behavior and its decision process, and may offer a novel perspective on the evolution of sociality.</p><p>Our results on agent behavior are broadly consistent with previous studies concerning observations of animal behavior in nature. First, as the number of predators increased, success rates increased and hunting duration decreased (<xref ref-type="bibr" rid="bib19">Creel and Creel, 1995</xref>). Second, whether collaborative hunts emerge depended on two factors: the success rate of hunting alone (<xref ref-type="bibr" rid="bib16">Busse, 1978</xref>; <xref ref-type="bibr" rid="bib10">Boesch, 2002</xref>) and the presence or absence of reward sharing following prey capture (<xref ref-type="bibr" rid="bib8">Boesch, 1994</xref>; <xref ref-type="bibr" rid="bib61">Stanford, 1996</xref>). Third, while each predator generally maintained a consistent role during repeated collaborative hunts, there was flexibility for these roles to be swapped as needed (<xref ref-type="bibr" rid="bib60">Stander, 1992</xref>; <xref ref-type="bibr" rid="bib10">Boesch, 2002</xref>). Finally, predator agents in this study acquired different strategies depending on the conditions despite having exactly the same initial values (i.e. network weights), resonating with the findings that lions and chimpanzees living in different regions exhibit different hunting strategies (<xref ref-type="bibr" rid="bib60">Stander, 1992</xref>; <xref ref-type="bibr" rid="bib11">Boesch‐Achermann and Boesch, 1994</xref>). These results suggest the validity of our computational simulations and highlight the close link between predators’ behavioral strategies and their living environments, such as the presence of other predators and sharing of prey.</p><p>The collaborative hunts have shown performance that surpasses the theoretical predictions based on solitary hunting outcomes. This result is in line with the notion that role division among predators in nature could provide fitness benefits (<xref ref-type="bibr" rid="bib40">Lang and Farine, 2017</xref>; <xref ref-type="bibr" rid="bib9">Boesch and Boesch-Achermann, 2000</xref>). Meanwhile, when three predators were involved, performance was comparable whether prey was shared or not. One possible factor that has caused this is spatial constraints. We found that predators occasionally block the prey’s escape path, exploiting the boundaries of the play area and the chasing movements of other predators even in the individual reward condition (<xref ref-type="fig" rid="fig2s4">Figure 2—figure supplement 4</xref>). These results suggest that, under certain scenarios, coordinated hunting behaviors that enhance the success rate of predators may emerge regardless of whether food is shared, potentially relating to the benefits of social predation, including interspecific hunting (<xref ref-type="bibr" rid="bib15">Bshary et al., 2006</xref>; <xref ref-type="bibr" rid="bib68">Thiebault et al., 2016</xref>; <xref ref-type="bibr" rid="bib52">Sampaio et al., 2021</xref>).</p><p>We found that the mappings resulting in collaborative hunting were related to distance-dependent internal representations. Additionally, we showed that the distance-dependent rule-based predators successfully reproduced behaviors similar to those of the deep reinforcement learning predators, supporting the association between decisions and distances (Methods; <xref ref-type="fig" rid="fig3s5">Figure 3—figure supplement 5</xref>, <xref ref-type="fig" rid="fig3s6">Figure 3—figure supplement 6</xref> and <xref ref-type="fig" rid="fig3s7">Figure 3—figure supplement 7</xref>). Deep reinforcement learning has held the promise for providing a comprehensive framework for studying the interplay among learning, representation, and decision making (<xref ref-type="bibr" rid="bib12">Botvinick et al., 2020</xref>; <xref ref-type="bibr" rid="bib46">Mobbs et al., 2021</xref>), but such efforts for natural behavior have been limited (<xref ref-type="bibr" rid="bib4">Banino et al., 2018</xref>; <xref ref-type="bibr" rid="bib36">Jaderberg et al., 2019</xref>). Our result that the distance-dependent representations relate to collaborative hunting is reminiscent of a recent idea about the decision rules obtained by observation in fish (<xref ref-type="bibr" rid="bib62">Steinegger et al., 2018</xref>). Notably, the input variables of predator agents do not include variables corresponding to the distance(s) between the other predator(s) and prey, and this means that the predators in the shared conditions acquired the internal representation relating to distance to prey, which would be a geometrically reasonable indicator, by optimization through interaction with their environment. Our results suggest that deep reinforcement learning methods can extract systems of rules that allow for the emergence of complex behaviors.</p><p>The predator agents’ decision rules (i.e. policy networks) acquired through interactions with other agents (i.e. self-play) were also useful for unknown prey controlled by humans, despite the dissociation of the experienced states. This suggests that decision rules formed by associative learning can successfully address natural problems, such as catching prey with somewhat different movement patterns than one’s usual prey. Note that the learning mechanism of associative learning (or reinforcement learning) is relatively simple, but it allows for flexible behavior in response to situations, in contrast to innate and simple stimulus-response. Indeed, our prey agents achieved a higher rate of successful evasions than those operated by humans. Our view that decisions for successful hunting are made through representations formed by prior experience is a counterpart to the recent idea that computational relevance for successful escape may be cached and ready to use, instead of being computed from scratch on the spot (<xref ref-type="bibr" rid="bib22">Evans et al., 2019</xref>). If animals’ decision processes in predator-prey dynamics are structured in this way, it could be a product of natural selection, enabling rapid, robust, and flexible action in interactions with severe time constraints.</p><p>In conclusion, we demonstrated that the decisions underlying collaborative hunting among artificial agents can be achieved through mappings between states and actions. This means that collaborative hunting can emerge in the absence of explicit mechanisms comparable to aspects of theory of mind, supporting the recent idea that collaborative hunting does not necessarily rely on complex cognitive processes in brains (<xref ref-type="bibr" rid="bib40">Lang and Farine, 2017</xref>). Our computational ecology is an abstraction of a real predator-prey environment. Given that chase and escape often involve various factors, such as energy cost (<xref ref-type="bibr" rid="bib33">Hubel et al., 2016</xref>), partial observability (<xref ref-type="bibr" rid="bib47">Mugan and MacIver, 2020</xref>; <xref ref-type="bibr" rid="bib35">Hunt et al., 2021</xref>), signal communication (<xref ref-type="bibr" rid="bib72">Vail et al., 2013</xref>), and local surroundings (<xref ref-type="bibr" rid="bib22">Evans et al., 2019</xref>), these results are only a first step on the path to understanding real decisions in predator-prey dynamics. Furthermore, exploring how mechanisms comparable to aspects of theory of mind (<xref ref-type="bibr" rid="bib79">Yoshida et al., 2008</xref>; <xref ref-type="bibr" rid="bib25">Foerster, 2019</xref>; <xref ref-type="bibr" rid="bib32">Hu and Foerster, 2020</xref>) or the shared value functions (<xref ref-type="bibr" rid="bib42">Lowe, 2017</xref>; <xref ref-type="bibr" rid="bib24">Foerster et al., 2018</xref>; <xref ref-type="bibr" rid="bib50">Rashid, 2020</xref>), which are increasingly common in multi-agent reinforcement learning, play a role in these interactions could be an intriguing direction for future research. We believe that our results provide a useful advance toward understanding natural value-based decisions and forge a critical link between ecology, ethology, psychology, neuroscience, and computer science.</p></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Environment</title><p>The predator and prey interacted in a two-dimensional world with continuous space and discrete time. This environment was constructed by modifying an environment known as ‘predator-prey’ within a multi-agent particle environment (<xref ref-type="bibr" rid="bib42">Lowe, 2017</xref>). Specifically, the position of each agent was calculated by integrating the acceleration (i.e. selected action) twice with the Euler method, and viscous resistance proportional to velocity was considered. The modifications were that the action space (play area size) was constrained to the range of –1 to 1 on the <italic>x</italic> and <italic>y</italic> axes, all agent (predator/prey) disk diameters were set to 0.1, landmarks (obstacles) were eliminated, and predator-to-predator contact was ignored for simplicity (<xref ref-type="bibr" rid="bib71">Tsutsui et al., 2022</xref>). The predator(s) was rewarded for capturing the prey (+1), namely contacting the disks, and punished for moving out of the area (–1), and the prey was penalized for being captured by the predator or for moving out of the area (–1). The predator and prey were represented as a red and blue disk, respectively, and the play area was represented as a black square enclosing them. The time step was 0.1 s and the time limit in each episode was set to 30 s. The initial positions of the predators and prey in each episode were randomly selected from a range of –0.5 to 0.5 on the <italic>x</italic> and <italic>y</italic> axes.</p></sec><sec id="s4-2"><title>Experimental conditions</title><p>We selected the number of predators, relative mobility, and prey (reward) sharing as experimental conditions, based on ecological findings (<xref ref-type="bibr" rid="bib2">Bailey et al., 2013</xref>; <xref ref-type="bibr" rid="bib40">Lang and Farine, 2017</xref>). For the number of predators, three conditions were set: 1 (one), 2 (two), and 3 (three). In all these conditions, the number of prey was set to 1. For the relative mobility, three conditions were set: 120% (fast), 100% (equal), and 80% (slow) for the acceleration exerted by the predator, based on that exerted by the prey. For the prey sharing, two conditions were set: with sharing (shared), in which all predators were rewarded when a predator catches the prey, and without sharing (individual), in which a predator was rewarded only when it catches prey by itself. In total, there were 15 conditions.</p></sec><sec id="s4-3"><title>Agent architecture</title><p>We considered a sequential decision-making setting in which a single agent interacts with an environment <inline-formula><mml:math id="inf51"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">E</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> in a sequence of observations, actions, and rewards. At each time-step <inline-formula><mml:math id="inf52"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula>, the agent observes a state <inline-formula><mml:math id="inf53"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">S</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> and selects an action <inline-formula><mml:math id="inf54"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> from a discrete set of actions <inline-formula><mml:math id="inf55"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">A</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:mn>1</mml:mn><mml:mo>,</mml:mo><mml:mn>2</mml:mn><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">A</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>. One time step later, in part as a consequence of its action, the agent receives a reward, <inline-formula><mml:math id="inf56"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>∈</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">R</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>, and moves itself to a new state <inline-formula><mml:math id="inf57"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>. In the MDP, the agent thereby gives rise to a sequence that begins as follows: <inline-formula><mml:math id="inf58"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>0</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>3</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo></mml:mstyle></mml:math></inline-formula>, and learns a behavioral rule (policy) that depends upon these sequences.</p><p>The goal of the agent is to maximize the expected discounted return over time through its choice of actions (<xref ref-type="bibr" rid="bib65">Sutton and Barto, 2018</xref>). The discounted return <inline-formula><mml:math id="inf59"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> was defined as <inline-formula><mml:math id="inf60"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munderover><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>T</mml:mi></mml:mrow></mml:munderover><mml:msup><mml:mi>γ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msup><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mi>k</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf61"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi><mml:mo>∈</mml:mo><mml:mo stretchy="false">[</mml:mo><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula> is a parameter called the discount rate that determines the present value of future rewards, and <inline-formula><mml:math id="inf62"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>T</mml:mi></mml:mstyle></mml:math></inline-formula> is the time step at which the task terminates. The state-value function, action-value function, and advantage function are defined as <inline-formula><mml:math id="inf63"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf64"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf65"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>A</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:msup><mml:mi>V</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>, respectively, where <inline-formula><mml:math id="inf66"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>π</mml:mi></mml:mstyle></mml:math></inline-formula> is a policy mapping states to actions. The optimal action-value function <inline-formula><mml:math id="inf67"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is then defined as the maximum expected discounted return achievable by following any strategy, after observing some state <inline-formula><mml:math id="inf68"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> and then taking some action <inline-formula><mml:math id="inf69"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf70"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>π</mml:mi></mml:mrow></mml:munder><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:msub><mml:mi>R</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:mi>π</mml:mi><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>. The optimal action-value function can be computed by finding a fixed point of the Bellman equations:<disp-formula id="equ1"><label>(1)</label><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:munder><mml:msup><mml:mi>Q</mml:mi><mml:mrow><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf71"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf72"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> are the state and action at the next time-step, respectively. This is based on the following intuition: if the optimal value <inline-formula><mml:math id="inf73"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> of the state <inline-formula><mml:math id="inf74"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> was known for all possible actions <inline-formula><mml:math id="inf75"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, the optimal strategy is to select the action <inline-formula><mml:math id="inf76"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> maximizing the expected value of <inline-formula><mml:math id="inf77"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>. The basic idea behind many reinforcement learning algorithms is to estimate the action-value function by using the Bellman equation as an iterative update; <inline-formula><mml:math id="inf78"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mo stretchy="false">[</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">]</mml:mo></mml:mstyle></mml:math></inline-formula>. Such value iteration algorithms converge to the optimal action-value function in situations where all states can be sufficiently sampled, <inline-formula><mml:math id="inf79"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> as <inline-formula><mml:math id="inf80"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mi mathvariant="normal">∞</mml:mi></mml:mstyle></mml:math></inline-formula>. In practice, however, it is often difficult to apply this basic approach, which estimates the action-value function separately for each state, to real-world problems. Instead, it is common to use a function approximator to estimate the action-value function, <inline-formula><mml:math id="inf81"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>≈</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula>.</p><p>There are several possible methods for function approximation, yet we here use a neural network function approximator referred to as deep <inline-formula><mml:math id="inf82"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-network (DQN) (<xref ref-type="bibr" rid="bib45">Mnih et al., 2015</xref>) and some of its extensions to overcome the limitations of the DQN, namely Double DQN (<xref ref-type="bibr" rid="bib74">Van Hasselt et al., 2016</xref>), Prioritized Experience Replay (<xref ref-type="bibr" rid="bib53">Schaul et al., 2015</xref>), and Dueling Networks (<xref ref-type="bibr" rid="bib75">Wang, 2016</xref>). Naively, a <inline-formula><mml:math id="inf83"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-network with weights <inline-formula><mml:math id="inf84"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi></mml:mstyle></mml:math></inline-formula> can be trained by minimizing a loss function <inline-formula><mml:math id="inf85"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo mathvariant="script" stretchy="false">(</mml:mo><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mo mathvariant="script" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula> that changes at each iteration <inline-formula><mml:math id="inf86"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>,<disp-formula id="equ2"><label>(2)</label><mml:math id="m2"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>∼</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mn>2</mml:mn></mml:mfrac><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf87"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-variant" mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is the target value for iteration <inline-formula><mml:math id="inf88"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf89"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is a probability distribution over states <inline-formula><mml:math id="inf90"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>s</mml:mi></mml:mstyle></mml:math></inline-formula> and actions <inline-formula><mml:math id="inf91"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>a</mml:mi></mml:mstyle></mml:math></inline-formula>. The parameters from the previous iteration <inline-formula><mml:math id="inf92"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> are kept constant when optimizing the loss function <inline-formula><mml:math id="inf93"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mo mathvariant="script" stretchy="false">(</mml:mo><mml:mi>θ</mml:mi><mml:mo mathvariant="script" stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mstyle></mml:math></inline-formula>. By differentiating the loss function with respect to the weights we arrive at the following gradient,<disp-formula id="equ3"><label>(3)</label><mml:math id="m3"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>∼</mml:mo><mml:mi>ρ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mi mathvariant="script">E</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">[</mml:mo></mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi><mml:mo>−</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:msub><mml:mi mathvariant="normal">∇</mml:mi><mml:mrow><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msub><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">]</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>We could attempt to use the simplest <inline-formula><mml:math id="inf94"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-learning to learn the weights of the network <inline-formula><mml:math id="inf95"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> online; however, this estimator performs poorly in practice. In this simplest form, they discard incoming data immediately, after a single update. This results in two issues: (<inline-formula><mml:math id="inf96"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) strongly correlated updates that break the i.i.d. assumption of many popular stochastic gradient-based algorithms and (<inline-formula><mml:math id="inf97"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi mathvariant="normal">i</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula>) the rapid forgetting of possibly rare experiences that would be useful later. To address both of these issues, a technique called experience replay is often adopted (<xref ref-type="bibr" rid="bib41">Lin, 1992</xref>), in which the agent’s experiences at each time-step <inline-formula><mml:math id="inf98"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>a</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>r</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>s</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>t</mml:mi><mml:mo>+</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> are stored in a dataset (also referred to as replay memory) <inline-formula><mml:math id="inf99"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mo fence="false" stretchy="false">{</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>…</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mi>e</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo fence="false" stretchy="false">}</mml:mo></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf100"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>N</mml:mi></mml:mstyle></mml:math></inline-formula> is the dataset size, for some time period. When training the <inline-formula><mml:math id="inf101"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-network, instead of only using the current experience as prescribed by standard <inline-formula><mml:math id="inf102"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-learning, mini-batches of experiences are sampled from <inline-formula><mml:math id="inf103"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi></mml:mrow></mml:mstyle></mml:math></inline-formula> uniformly, at random, to train the network. This enables breaking the temporal correlations by mixing more and fewer recent experiences for the updates, and rare experiences will be used for more than just a single update. Another technique, called the target-network, is also often used for updating to stabilize learning. To achieve this, the target value <inline-formula><mml:math id="inf104"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is replaced by <inline-formula><mml:math id="inf105"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf106"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mstyle></mml:math></inline-formula> are the weights, which are frozen for a fixed number of iterations. The full algorithm combining these ingredients, namely experience replay and the target-network, is often called a deep Q-network (DQN), and its loss function takes the form:<disp-formula id="equ4"><label>(4)</label><mml:math id="m4"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">u</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mo stretchy="false">[</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Q</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo stretchy="false">]</mml:mo><mml:mo>,</mml:mo></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ5"> <label>(5)</label><mml:math id="m5"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>Q</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:munder><mml:mo movablelimits="true" form="prefix">max</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mi>i</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf107"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">U</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mo>⋅</mml:mo><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> is a uniform sampling.</p><p>It has become known that <inline-formula><mml:math id="inf108"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-learning algorithms perform poorly in some stochastic environments. This poor performance is caused by large overestimations of action values. These overestimations result from a positive bias that is introduced because <inline-formula><mml:math id="inf109"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-learning uses the maximum action value as an approximation for the maximum expected action value. As a method to alleviate the performance degradation due to the overestimation, Double <inline-formula><mml:math id="inf110"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-learning, which decomposes the maximum operation into action selection and action evaluation by introducing the double estimator, was proposed (<xref ref-type="bibr" rid="bib30">Hasselt, 2010</xref>). Double DQN (DDQN) is an algorithm that applies the Double <inline-formula><mml:math id="inf111"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-learning method to DQN (<xref ref-type="bibr" rid="bib74">Van Hasselt et al., 2016</xref>). For the DDQN, in contrast to the original Double <inline-formula><mml:math id="inf112"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula>-learning and the other proposed method (<xref ref-type="bibr" rid="bib26">Fujimoto et al., 2018</xref>), the target network in the DQN architecture, although not fully decoupled, was used as the second value function, and the target value in the loss function (i.e. Eq. Agent architecture) for iteration <inline-formula><mml:math id="inf113"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> is replaced as follows:<disp-formula id="equ6"><label>(6)</label><mml:math id="m6"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msubsup><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>D</mml:mi><mml:mi>D</mml:mi><mml:mi>Q</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:msup><mml:mi>θ</mml:mi><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>.</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>Prioritized Experience Replay is a method that aims to make the learning more efficient and effective than if all transitions were replayed uniformly (<xref ref-type="bibr" rid="bib53">Schaul et al., 2015</xref>). For the prioritized replay, the probability of sampling from the data-set for transition <inline-formula><mml:math id="inf114"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> is defined as<disp-formula id="equ7"><label>(7)</label><mml:math id="m7"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mfrac><mml:msubsup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>α</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:msubsup><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>α</mml:mi></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf115"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>p</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:mstyle></mml:math></inline-formula> is the priority of transition for iteration <inline-formula><mml:math id="inf116"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>i</mml:mi></mml:mstyle></mml:math></inline-formula> and the exponent <inline-formula><mml:math id="inf117"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> determines how much prioritization is used, with <inline-formula><mml:math id="inf118"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi><mml:mo>=</mml:mo><mml:mn>0</mml:mn></mml:mstyle></mml:math></inline-formula> corresponding to uniform sampling. The priority <inline-formula><mml:math id="inf119"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is determined by <inline-formula><mml:math id="inf120"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>p</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mo>+</mml:mo><mml:mi>ϵ</mml:mi></mml:mstyle></mml:math></inline-formula>, where <inline-formula><mml:math id="inf121"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>δ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> is a temporal-difference (TD) error (e.g. <inline-formula><mml:math id="inf122"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>δ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:msub><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> in DQN) and <inline-formula><mml:math id="inf123"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ϵ</mml:mi></mml:mstyle></mml:math></inline-formula> is a small positive constant that prevents the case of transitions not being revisited once their error is zero. Prioritized replay introduces sampling bias, and therefore changes the solution to which the estimates will converge. This bias can be corrected by importance-sampling (IS) weights <inline-formula><mml:math id="inf124"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:msub><mml:mi>w</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">(</mml:mo></mml:mrow><mml:mfrac><mml:mn>1</mml:mn><mml:mi>N</mml:mi></mml:mfrac><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mfrac><mml:msup><mml:mrow><mml:mo maxsize="1.2em" minsize="1.2em">)</mml:mo></mml:mrow><mml:mrow><mml:mi>β</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:mstyle></mml:math></inline-formula> that fully compensate for the non-uniform probabilities <inline-formula><mml:math id="inf125"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>P</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>i</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> if <inline-formula><mml:math id="inf126"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>β</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mstyle></mml:math></inline-formula>.</p><p>Dueling Network is a neural network architecture designed for value-based algorithms such as DQN (<xref ref-type="bibr" rid="bib75">Wang, 2016</xref>). This features two streams of computation, the value and advantage streams, sharing a common encoder, and is merged by an aggregation module that produces an estimate of the state-action value function. Intuitively, we can expect the dueling network to learn which states are (or are not) valuable, without having to learn the effect of each action for each state. For the reason of stability of the optimization, the last module of the network is implemented as follows:<disp-formula id="equ8"><label>(8)</label><mml:math id="m8"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:mi>V</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>ξ</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>+</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">(</mml:mo></mml:mrow><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mo>−</mml:mo><mml:mfrac><mml:mn>1</mml:mn><mml:mrow><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi mathvariant="script">A</mml:mi><mml:mrow><mml:mo mathvariant="script" stretchy="false">|</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:munder><mml:mo>∑</mml:mo><mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:munder><mml:mi>A</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:mi>θ</mml:mi><mml:mo>,</mml:mo><mml:mi>η</mml:mi><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mo maxsize="2.470em" minsize="2.470em">)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf127"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>θ</mml:mi></mml:mstyle></mml:math></inline-formula> denotes the parameters of the common layers, whereas <inline-formula><mml:math id="inf128"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>η</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf129"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ξ</mml:mi></mml:mstyle></mml:math></inline-formula> are the parameters of the layers of the two streams, respectively.</p><p>We here modeled an agent (predator/prey) with independent learning, one of the simplest approaches to multi-agent reinforcement learning (<xref ref-type="bibr" rid="bib66">Tan, 1993</xref>). In this approach, each agent independently learns its own policy and treats the other agents as part of the environment. In other words, each agent learns policies that are conditioned only on their local observation history, and do not account for the non-stationarity of the multi-agent environment. That is, in contrast to previous studies on multi-agent reinforcement learning (<xref ref-type="bibr" rid="bib67">Tesauro, 2003</xref>; <xref ref-type="bibr" rid="bib23">Foerster et al., 2016</xref>; <xref ref-type="bibr" rid="bib56">Silver et al., 2017</xref>; <xref ref-type="bibr" rid="bib42">Lowe, 2017</xref>; <xref ref-type="bibr" rid="bib24">Foerster et al., 2018</xref>; <xref ref-type="bibr" rid="bib63">Sunehag, 2017</xref>; <xref ref-type="bibr" rid="bib50">Rashid, 2020</xref>; <xref ref-type="bibr" rid="bib59">Son et al., 2019</xref>; <xref ref-type="bibr" rid="bib3">Baker, 2019</xref>; <xref ref-type="bibr" rid="bib17">Christianos et al., 2020</xref>; <xref ref-type="bibr" rid="bib47">Mugan and MacIver, 2020</xref>; <xref ref-type="bibr" rid="bib29">Hamrick, 2021</xref>; <xref ref-type="bibr" rid="bib80">Yu, 2022</xref>), our agents did not share network parameters and value functions, and did not access models of the environment for planning. For each agent <inline-formula><mml:math id="inf130"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula>, the policy <inline-formula><mml:math id="inf131"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>π</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> is represented by a neural network and optimized, with the framework of DQN including DDQN, Prioritized Experience Replay, and Dueling architecture. The loss function of each agent takes the form:<disp-formula id="equ9"><label>(9)</label><mml:math id="m9"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mrow><mml:mi mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">E</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>,</mml:mo><mml:msup><mml:mi>r</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>∼</mml:mo><mml:mrow><mml:mi mathvariant="script">P</mml:mi></mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi mathvariant="script">D</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:msub><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">(</mml:mo></mml:mrow><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>−</mml:mo><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:msup><mml:mrow><mml:mo maxsize="1.623em" minsize="1.623em">)</mml:mo></mml:mrow><mml:mn>2</mml:mn></mml:msup></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where<disp-formula id="equ10"><label>(10)</label><mml:math id="m10"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>y</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mo>+</mml:mo><mml:mi>γ</mml:mi><mml:mi>Q</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:munder><mml:mrow><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup></mml:munder><mml:mi>Q</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msup><mml:mi>s</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo><mml:msup><mml:mi>a</mml:mi><mml:mrow><mml:mi mathvariant="normal">′</mml:mi></mml:mrow></mml:msup><mml:mo>;</mml:mo><mml:msub><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>;</mml:mo><mml:msubsup><mml:mi>θ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>η</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup><mml:mo>,</mml:mo><mml:msubsup><mml:mi>ξ</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mo>−</mml:mo></mml:mrow></mml:msubsup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>and <inline-formula><mml:math id="inf132"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">P</mml:mi></mml:mstyle></mml:math></inline-formula> (·) is a prioritized sampling. For simplicity, we omitted the agent index <inline-formula><mml:math id="inf133"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> in these equations.</p></sec><sec id="s4-4"><title>Training details</title><p>The neural network was composed of four layers (<xref ref-type="fig" rid="fig1s1">Figure 1—figure supplement 1</xref>). There was a separate output unit for each possible action, and only the state representation was an input to the neural network. The inputs to the neural network were the positions of a specific agent in the absolute coordinate system (<inline-formula><mml:math id="inf134"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>- and <inline-formula><mml:math id="inf135"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>-positions) and the positions and velocities of a specific agent and others in the relative coordinate system (<inline-formula><mml:math id="inf136"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi></mml:mstyle></mml:math></inline-formula>- and <inline-formula><mml:math id="inf137"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi></mml:mstyle></mml:math></inline-formula>-positions and <inline-formula><mml:math id="inf138"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>u</mml:mi></mml:mstyle></mml:math></inline-formula>- and <inline-formula><mml:math id="inf139"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>v</mml:mi></mml:mstyle></mml:math></inline-formula>-velocities) (<xref ref-type="fig" rid="fig1s2">Figure 1—figure supplement 2</xref>), which were determined based on findings in neuroscience (<xref ref-type="bibr" rid="bib48">O’Keefe and Dostrovsky, 1971</xref>) and ethology (<xref ref-type="bibr" rid="bib13">Brighton et al., 2017</xref>; <xref ref-type="bibr" rid="bib69">Tsutsui et al., 2020</xref>), respectively. We assumed that delays in sensory processing were compensated for by estimation of motion of self (<xref ref-type="bibr" rid="bib77">Wolpert et al., 1998</xref>; <xref ref-type="bibr" rid="bib37">Kawato, 1999</xref>) and others (<xref ref-type="bibr" rid="bib70">Tsutsui et al., 2021</xref>), and the current information at each time was used as input as is. The outputs were the acceleration in 12 directions every <inline-formula><mml:math id="inf140"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mn>30</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>∘</mml:mo></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> in the relative coordinate system, which were determined with reference to an ecological study (<xref ref-type="bibr" rid="bib76">Wilson et al., 2018</xref>). After the first two hidden layers of the MLP with 64 units, the network branched off into two streams. Each branch had one MLP layer with 32 hidden units. ReLU was used as the activation function for each layer (<xref ref-type="bibr" rid="bib28">Glorot et al., 2011</xref>). The network parameters <inline-formula><mml:math id="inf141"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>θ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf142"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>η</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf143"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>ξ</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula> were iteratively optimized via stochastic gradient descent with the Adam optimizer (<xref ref-type="bibr" rid="bib39">Kingma and Ba, 2014</xref>). In the computation of the loss, we used Huber loss to prevent extreme gradient updates (<xref ref-type="bibr" rid="bib34">Huber, 1992</xref>). The model was trained for 10<sup>6</sup> episodes, and the network parameters were copied to the target-network every 2000 episodes. The replay memory size was 10<sup>4</sup>, the minibatch size during training was 32, and the learning rate was <inline-formula><mml:math id="inf144"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mn>10</mml:mn><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>−</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>. The discount factor <inline-formula><mml:math id="inf145"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>γ</mml:mi></mml:mstyle></mml:math></inline-formula> was set to 0.9, and <inline-formula><mml:math id="inf146"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>α</mml:mi></mml:mstyle></mml:math></inline-formula> was set to 0.6. We used an <inline-formula><mml:math id="inf147"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>-greedy policy as the behavior policy <inline-formula><mml:math id="inf148"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msup><mml:mi>π</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:mstyle></mml:math></inline-formula>, which chooses a random action with probability <inline-formula><mml:math id="inf149"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula> or an action according to the optimal <inline-formula><mml:math id="inf150"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula> function <inline-formula><mml:math id="inf151"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:munder><mml:mrow class="MJX-TeXAtom-OP"><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">r</mml:mi><mml:mi mathvariant="normal">g</mml:mi><mml:mspace width="thinmathspace"/><mml:mi mathvariant="normal">m</mml:mi><mml:mi mathvariant="normal">a</mml:mi><mml:mi mathvariant="normal">x</mml:mi></mml:mrow><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>a</mml:mi><mml:mo>∈</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi class="MJX-tex-caligraphic" mathvariant="script">A</mml:mi></mml:mrow></mml:mrow></mml:munder><mml:mo>⁡</mml:mo><mml:msup><mml:mi>Q</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo>⋆</mml:mo></mml:mrow></mml:msup><mml:mo stretchy="false">(</mml:mo><mml:mi>s</mml:mi><mml:mo>,</mml:mo><mml:mi>a</mml:mi><mml:mo stretchy="false">)</mml:mo></mml:mstyle></mml:math></inline-formula> with probability <inline-formula><mml:math id="inf152"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula>. In this study, <inline-formula><mml:math id="inf153"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula> was annealed linearly from 1 to 0.1 over the first 10<sup>4</sup> episodes and fixed at 0.1 thereafter.</p></sec><sec id="s4-5"><title>Evaluation</title><p>The model performance was evaluated using the trained model. The initial position of each agent and termination criteria in each episode were the same as in training. During the evaluation, <inline-formula><mml:math id="inf154"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ε</mml:mi></mml:mstyle></mml:math></inline-formula> was set to 0, and each agent took greedy actions. If the predator captured the prey within the time limit, the predator was deemed successful; otherwise, the prey was considered successful. Additionally, if one side (predators/prey) moved out of the area, the other side (prey/predators) was deemed successful. We first conducted a computational experiment (self-play: predator agent vs. prey agent). and then conducted a human behavioral experiment (joint play: predator agent vs. prey human). In the computational experiment, we simulated 100 episodes for each of the 10 random seeds (i.e. different initial positions), for a total of 1000 episodes in each condition. In the joint play, human participants controlled prey on a screen using a joystick and interacted with the predator agents for 50 episodes in each condition.</p></sec><sec id="s4-6"><title>Participants</title><p>Ten males participated in the experiment (aged 22–25, mean = 23.5, s.d.=1.2). All participants were right-handed but one, had normal or corrected-to-normal vision, and were naïve to the purpose of the study. This study was approved by the Ethics Committee of Nagoya University Graduate School of Informatics (No. 2021–27). Informed consent was obtained from each participant before the experiment. Participants received 1000 yen per hour as a reward.</p></sec><sec id="s4-7"><title>Apparatus</title><p>Participants were seated in a chair, and they operated the joystick of an Xbox One controller that could tilt freely in any direction to control a disk on the screen. The stimuli were presented on a 26.5-inch monitor (EIZO EV2730Q) at a refresh rate of 60 Hz. A gray square surrounding the disks was defined as the play area. The diameter of each disk on the screen was 2.0 cm, and the width and height of the area were 40.0 cm. The acceleration of each disk on the screen was determined by the inclination of the joystick. Specifically, acceleration was added when the degree of joystick tilt exceeded half of the maximum tilt, and the direction of the acceleration was selected from 12 directions, discretized every 30 degrees in an absolute coordinate system corresponding to the direction of joystick tilt. The reason for setting the direction of acceleration with respect to the absolute coordinate system, rather than the relative coordinate system, in the human behavioral experiment was to allow participants to control more intuitively. The position and velocity of each disk on the screen were updated at 10 Hz (corresponding to the computational simulation) and the position during the episodes was recorded at 10 Hz on a computer (MacBook Pro) with Psychopy version 3.0. The viewing distance of the participants was about 60 cm.</p></sec><sec id="s4-8"><title>Design</title><p>Participants controlled a red disk representing the prey on the screen. They were asked to evade the predator for 30 s without leaving the play area. The agent’s initial position and the outcome of the episode were determined as described above. The experimental block consisted of five sets of 10 episodes, with a warm-up of 10 episodes so that participants could become accustomed to the task. In this experiment, we focused on the slow condition and there were thus five experimental conditions (one, two × individual, two × shared, three × individual, and three × shared). Each participant played one block (i.e. 50 episodes) of each experimental condition. The order of the experimental conditions was pseudo-randomized across participants.</p></sec><sec id="s4-9"><title>Rule-based agent</title><p>We constructed rule-based predator agents to test whether they could reproduce similar behavior to predator agents based on deep reinforcement learning in the two × shared condition. For consistency with the deep reinforcement learning agents, the input to the rule-based agent used to make decisions was limited to the current information (e.g. position and velocity) and the output was provided in a relative coordinate system to the prey; that is, action 1 denotes movement toward the prey and action 7 denotes movement in the opposite direction of the prey. The predator agent first determines whether it, or another predator, is closer to the prey, and then, if the other predator is closer, it determines whether the distance 2 is less than a certain distance threshold (set to 0.4 in our simulation). The decision rule for each predator is selected by this branching, with predator 1 adopting the three rules ‘chase,’ ‘shortcut,’ and ‘approach,’ and predator 2 adopting the two rules ‘chase’ and ‘ambush.’ For the chase, the predator first determines whether it is near the outer edge of the play area and, if so, selects actions that will prevent it from leaving the play area. Specifically, if the predator’s position is such that  <inline-formula><mml:math id="inf155"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> &gt; 0.9 and <inline-formula><mml:math id="inf156"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> &gt; 0.9, action 3 for clockwise (CW) and action 11 for counterclockwise (CCW) was selected, respectively, and if 0.8 &lt; <inline-formula><mml:math id="inf157"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf158"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 0.9 and 0.8 &lt; <inline-formula><mml:math id="inf159"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf160"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 0.9, action 2 for CW and action 12 for CCW was selected. The CW and CCW were determined by the absolute position of the prey and the relative position vector between the closer predator and prey; the play area was divided into four parts based on the signs of the <inline-formula><mml:math id="inf161"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf162"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> coordinates, and CW and CCW were determined by the correspondence between each area and the sign of the larger component of absolute value (<inline-formula><mml:math id="inf163"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> or <inline-formula><mml:math id="inf164"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula>) of the relative position vector. For instance, if the closer predator is at (0.2, 0.3) and the prey is at (0.5, 0.2), it is determined to be CW. If the predator is not outside the play area, then it determines whether the prey is inside the play area, and, if so, selects actions that will drive them outside; if the prey’s position is such that <inline-formula><mml:math id="inf165"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf166"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 0.5 and <inline-formula><mml:math id="inf167"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf168"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 0.5, action 11 for CW and action 3 for CCW was selected, and if 0.5 &lt; <inline-formula><mml:math id="inf169"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>x</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf170"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 0.6 and 0.5 &lt; <inline-formula><mml:math id="inf171"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf172"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 0.6, action 12 for CW and action 2 for CCW was selected. In other situations, the predator selects actions so that the direction of movement is aligned with that of the prey; if the angle of the velocity vectors between the predator and prey <inline-formula><mml:math id="inf173"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ψ</mml:mi></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf174"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> –50 action 3, and if –50 &lt; <inline-formula><mml:math id="inf175"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ψ</mml:mi></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf176"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> –15 action 2, if –15 &lt; <inline-formula><mml:math id="inf177"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ψ</mml:mi></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf178"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 15 action 1, if 15 &lt; <inline-formula><mml:math id="inf179"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ψ</mml:mi></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf180"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 50 action 2, if 50 &lt; <inline-formula><mml:math id="inf181"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>ψ</mml:mi></mml:mstyle></mml:math></inline-formula> action 3 was selected. For the shortcut, the predator determines whether it is near the outer edge of the play area, and if so, selected the action described above, otherwise, it selected actions that producing shorter paths to the prey; action 2 for CW and action 12 for CCW was selected. For the approach, the predator determines whether it is near the outer edge of the play area, and if so, selected the action described above, otherwise, it selected actions that move it toward the prey; action 1 was selected. For the ambush, the predator selected actions that move it toward the top center or bottom center of the play area and to remain that location until the situation changes. If the predator’s position is such that <inline-formula><mml:math id="inf182"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mo>≦</mml:mo></mml:mstyle></mml:math></inline-formula> <inline-formula><mml:math id="inf183"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mo>≦</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> 0, the predator moved with respect to the bottom center point (–0.1, 0.5), and if <inline-formula><mml:math id="inf184"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mo stretchy="false">|</mml:mo></mml:mrow></mml:mstyle></mml:math></inline-formula> &gt; 0, it moved toward the top center point (0, 0.6). The coordinates of the top center and bottom center points were based on the result of deep reinforcement learning agents. Specifically, we first divided the play area into four parts based on the signs of the <inline-formula><mml:math id="inf185"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf186"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> coordinates with respect to the reference (i.e. bottom center or top center) point, and in each area, the predator selected actions 3, 8, or 12 (every 120 degrees) that will move it toward the reference point, depending on the direction of the prey from the predator’s perspective. For instance, if the predator is at (–0.2, 0.8) and the prey is at (−0.2, –0.8), action 12 is selected.</p></sec><sec id="s4-10"><title>Behavioral cloning</title><p>We constructed neural networks to clone the predatory behavior of rule-based agents. The neural network is composed of two weight layers; that is, it takes the state of environments as inputs as in the deep reinforcement learning agents, processes them through a hidden layer, and then outputs probabilities for each of the 13 potential actions using the softmax function. To ensure a fair comparison with the embedding of deep reinforcement learning agents, we set the number of units in the hidden layer to 32. In the networks, all layers were composed of the fully connected layer. In this study, for each agent (i.e. predator 1 and predator 2), we implemented two types of networks: a linear network without any nonlinear transformation, and a nonlinear network with ReLU activations. Specifically, in the linear network, the hidden layer is composed of the fully connected layer without nonlinearity,<disp-formula id="equ11"><mml:math id="m11"><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf187"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf188"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>h</mml:mi></mml:mstyle></mml:math></inline-formula>, <inline-formula><mml:math id="inf189"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula>, and <inline-formula><mml:math id="inf190"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>h</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denote the input to the hidden layer (state), the output of the hidden layer, the input-to-hidden weight, and the bias, respectively. In the nonlinear network, the hidden layer is composed of the fully connected layers with nonlinearity,<disp-formula id="equ12"><mml:math id="m12"><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>φ</mml:mi><mml:mo stretchy="false">(</mml:mo><mml:msub><mml:mi>W</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>x</mml:mi><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mi>x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mi>b</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>h</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">)</mml:mo><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf191"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>φ</mml:mi></mml:mstyle></mml:math></inline-formula>(<inline-formula><mml:math id="inf192"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>)=max(0, <inline-formula><mml:math id="inf193"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>x</mml:mi></mml:mstyle></mml:math></inline-formula>) is the rectified linear unit (ReLU) for nonlinearity. The neural network models were trained to minimize the cross entropy error,<disp-formula id="equ13"><mml:math id="m13"><mml:mi>E</mml:mi><mml:mo>=</mml:mo><mml:mo>−</mml:mo><mml:munder><mml:mo>∑</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:munder><mml:mrow class="MJX-TeXAtom-ORD"><mml:msub><mml:mi>t</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>log</mml:mi><mml:mo>⁡</mml:mo><mml:msub><mml:mi>y</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>,</mml:mo></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf194"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>t</mml:mi></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf195"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>y</mml:mi></mml:mstyle></mml:math></inline-formula> denote the actual actions taken by the rule-based agents and the predicted actions in each class <inline-formula><mml:math id="inf196"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>k</mml:mi></mml:mstyle></mml:math></inline-formula>, respectively. Network parameters were optimized iteratively using stochastic gradient descent with the Adam optimizer. The learning rate, batch size, and epoch were set as 0.0001, 32, and 2000, respectively, for all agents and networks. The networks were trained, validated, and tested using simulation data for 1000 episodes (123,597 time steps), 100 episodes (16,805 time steps), and 100 episode (12076 time steps), respectively. The network weights were saved according to the best performance observed during the validation phase.</p></sec><sec id="s4-11"><title>Data analysis</title><p>All data analysis was performed in Python 3.7. Successful predation was defined as the sum of the number of predators catching prey and the number of prey leaving the play area. The theoretical prediction assumes that each predator’s performance is independent of the others’ performance, and was defined as follows:<disp-formula id="equ14"><label>(11)</label><mml:math id="m14"><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:mo stretchy="false">(</mml:mo><mml:mn>1</mml:mn><mml:mo>−</mml:mo><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msup></mml:math></disp-formula></p><p>where <inline-formula><mml:math id="inf197"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mi>n</mml:mi></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> and <inline-formula><mml:math id="inf198"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:msub><mml:mi>H</mml:mi><mml:mrow class="MJX-TeXAtom-ORD"><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mstyle></mml:math></inline-formula> denote the proportion of successful predation when the number of predators is <inline-formula><mml:math id="inf199"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>n</mml:mi></mml:mstyle></mml:math></inline-formula> and 1, respectively. The duration was defined as the time from the beginning to the end of the episode, with a maximum duration of 30 s. The heat maps were constructed based on the frequency of stay in each position, with the play area divided into 1600 (40×40). The concordance rate was calculated by comparing the actual selected action by each agent in the two or three conditions with the action that would be chosen by the agent in the one condition if it were placed in a same situation. The circular correlation coefficient was calculated by converting the selected actions (1–12) into angles (0–330 degrees) (<xref ref-type="bibr" rid="bib6">Berens, 2009</xref>), and in this analysis, action 13 (do nothing) was excluded from the analysis. The two-dimensional embedding was made by transforming the vectors in the last hidden layers of state-value stream and action-value stream in the policy network using t-distributed stochastic neighbor embedding (t-SNE) (<xref ref-type="bibr" rid="bib73">van der Maaten and Hinton, 2008</xref>). To reduce the influence of extremely large or small values, the color ranges of the <inline-formula><mml:math id="inf200"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>V</mml:mi></mml:mstyle></mml:math></inline-formula> value, SD <inline-formula><mml:math id="inf201"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>Q</mml:mi></mml:mstyle></mml:math></inline-formula> value, and distance were limited from the 5th percentile to the 95<sup>th</sup> percentile of whole values experienced by each agent (see <xref ref-type="fig" rid="fig3s8">Figure 3—figure supplements 8</xref>–<xref ref-type="fig" rid="fig3s11">11</xref>).</p></sec><sec id="s4-12"><title>Statistics</title><p>All quantitative data are reported as mean ± SEM across random seeds in the computational experiment and across participants in the human experiment. In the human experiment, sample sizes were not predetermined statistically, but rather were chosen based on field standards. The data were analyzed using one- or two-way repeated-measures analysis of variance (ANOVA) as appropriate. For these tests, Mauchly’s test was used to test sphericity; if the sphericity assumption was violated, degrees of freedom were adjusted by the Greenhouse–Geisser correction. To adjust the <inline-formula><mml:math id="inf202"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> values for multiple comparisons, the Holm-Bonferonni method was used. The data distribution was assumed to be normal for multiple comparisons, but this was not formally tested. Two-tailed statistical tests were used for all applicable analyses. The significance level was set at an alpha value of 0.05. The theoretical prediction was excluded from statistical analyses (<xref ref-type="fig" rid="fig2">Figures 2a</xref> and <xref ref-type="fig" rid="fig4">4a</xref>) because, from the equation, it is obvious that the proportion of successful predation increases as the number of predators increases. Specific test statistics, <inline-formula><mml:math id="inf203"><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mi>p</mml:mi></mml:mstyle></mml:math></inline-formula> values, and effect sizes for the analyses are detailed in the corresponding figure captions. All statistical analyses were performed using R version 4.0.2 (The R Foundation for Statistical Computing).</p></sec><sec id="s4-13"><title>Code availability</title><p>The code for computational simulation and figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/TsutsuiKazushi/collaborative-hunting">https://github.com/TsutsuiKazushi/collaborative-hunting</ext-link>; (copy archived at <xref ref-type="bibr" rid="bib38">Kazushi, 2023</xref>).</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Resources, Software, Formal analysis, Funding acquisition, Investigation, Visualization, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con2"><p>Validation, Investigation, Methodology, Writing - original draft, Writing - review and editing</p></fn><fn fn-type="con" id="con3"><p>Supervision, Funding acquisition, Project administration</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Validation, Methodology, Writing - original draft, Project administration, Writing - review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: This study was approved by the Ethics Committee of Nagoya University Graduate School of Informatics. Informed consent was obtained from each participant before the experiment.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-85694-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>The data and models used in this study are available at <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.6084/m9.figshare.21184069.v3">https://doi.org/10.6084/m9.figshare.21184069.v3</ext-link>.The code for computational simulation and figures is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/TsutsuiKazushi/collaborative-hunting">https://github.com/TsutsuiKazushi/collaborative-hunting</ext-link> (copy archived at <xref ref-type="bibr" rid="bib38">Kazushi, 2023</xref>).</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Tsutsui</surname><given-names>K</given-names></name><name><surname>Tanaka</surname><given-names>R</given-names></name><name><surname>Takeda</surname><given-names>K</given-names></name><name><surname>Fujii</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Dataset</data-title><source>figshare</source><pub-id pub-id-type="doi">10.6084/m9.figshare.21184069</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by JSPS KAKENHI (Grant Numbers 20H04075, 21H04892, 21H05300, and 22K17673), JST PRESTO (JPMJPR20CA), and the Program for Promoting the Enhancement of Research Universities.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Axelrod</surname><given-names>R</given-names></name><name><surname>Hamilton</surname><given-names>WD</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>The evolution of cooperation</article-title><source>Science</source><volume>211</volume><fpage>1390</fpage><lpage>1396</lpage><pub-id pub-id-type="doi">10.1126/science.7466396</pub-id><pub-id pub-id-type="pmid">7466396</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bailey</surname><given-names>I</given-names></name><name><surname>Myatt</surname><given-names>JP</given-names></name><name><surname>Wilson</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Group hunting within the Carnivora: physiological, cognitive and environmental influences on strategy and cooperation</article-title><source>Behavioral Ecology and Sociobiology</source><volume>67</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1007/s00265-012-1423-3</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Baker</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Emergent Tool Use from Multi-Agent Autocurricula</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1909.07528">https://arxiv.org/abs/1909.07528</ext-link></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Banino</surname><given-names>A</given-names></name><name><surname>Barry</surname><given-names>C</given-names></name><name><surname>Uria</surname><given-names>B</given-names></name><name><surname>Blundell</surname><given-names>C</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Mirowski</surname><given-names>P</given-names></name><name><surname>Pritzel</surname><given-names>A</given-names></name><name><surname>Chadwick</surname><given-names>MJ</given-names></name><name><surname>Degris</surname><given-names>T</given-names></name><name><surname>Modayil</surname><given-names>J</given-names></name><name><surname>Wayne</surname><given-names>G</given-names></name><name><surname>Soyer</surname><given-names>H</given-names></name><name><surname>Viola</surname><given-names>F</given-names></name><name><surname>Zhang</surname><given-names>B</given-names></name><name><surname>Goroshin</surname><given-names>R</given-names></name><name><surname>Rabinowitz</surname><given-names>N</given-names></name><name><surname>Pascanu</surname><given-names>R</given-names></name><name><surname>Beattie</surname><given-names>C</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Sadik</surname><given-names>A</given-names></name><name><surname>Gaffney</surname><given-names>S</given-names></name><name><surname>King</surname><given-names>H</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Hadsell</surname><given-names>R</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Vector-based navigation using grid-like representations in artificial agents</article-title><source>Nature</source><volume>557</volume><fpage>429</fpage><lpage>433</lpage><pub-id pub-id-type="doi">10.1038/s41586-018-0102-6</pub-id><pub-id pub-id-type="pmid">29743670</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bednarz</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>Cooperative hunting Harris’ hawks (Parabuteo unicinctus)</article-title><source>Science</source><volume>239</volume><fpage>1525</fpage><lpage>1527</lpage><pub-id pub-id-type="doi">10.1126/science.239.4847.1525</pub-id><pub-id pub-id-type="pmid">17772751</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Berens</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Circstat: a matlab toolbox for circular statistics</article-title><source>Journal of Statistical Software</source><volume>31</volume><fpage>1</fpage><lpage>21</lpage><pub-id pub-id-type="doi">10.18637/jss.v031.i10</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boesch</surname><given-names>C</given-names></name><name><surname>Boesch</surname><given-names>H</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Hunting behavior of wild chimpanzees in the Taï National Park</article-title><source>American Journal of Physical Anthropology</source><volume>78</volume><fpage>547</fpage><lpage>573</lpage><pub-id pub-id-type="doi">10.1002/ajpa.1330780410</pub-id><pub-id pub-id-type="pmid">2540662</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boesch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Cooperative hunting in wild chimpanzees</article-title><source>Animal Behaviour</source><volume>48</volume><fpage>653</fpage><lpage>667</lpage><pub-id pub-id-type="doi">10.1006/anbe.1994.1285</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Boesch</surname><given-names>C</given-names></name><name><surname>Boesch-Achermann</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2000">2000</year><source>The Chimpanzees of the Taï Forest: Behavioural Ecology and Evolution</source><publisher-name>Oxford University Press</publisher-name><pub-id pub-id-type="doi">10.1093/oso/9780198505082.001.0001</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boesch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Cooperative hunting roles among taï chimpanzees</article-title><source>Human Nature</source><volume>13</volume><fpage>27</fpage><lpage>46</lpage><pub-id pub-id-type="doi">10.1007/s12110-002-1013-6</pub-id><pub-id pub-id-type="pmid">26192594</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boesch‐Achermann</surname><given-names>H</given-names></name><name><surname>Boesch</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Hominization in the rainforest: The chimpanzee’s piece of the puzzle</article-title><source>Evolutionary Anthropology</source><volume>3</volume><fpage>9</fpage><lpage>16</lpage><pub-id pub-id-type="doi">10.1002/evan.1360030106</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>M</given-names></name><name><surname>Wang</surname><given-names>JX</given-names></name><name><surname>Dabney</surname><given-names>W</given-names></name><name><surname>Miller</surname><given-names>KJ</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Deep reinforcement learning and its neuroscientific implications</article-title><source>Neuron</source><volume>107</volume><fpage>603</fpage><lpage>616</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.06.014</pub-id><pub-id pub-id-type="pmid">32663439</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brighton</surname><given-names>CH</given-names></name><name><surname>Thomas</surname><given-names>ALR</given-names></name><name><surname>Taylor</surname><given-names>GK</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Terminal attack trajectories of peregrine falcons are described by the proportional navigation guidance law of missiles</article-title><source>PNAS</source><volume>114</volume><fpage>13495</fpage><lpage>13500</lpage><pub-id pub-id-type="doi">10.1073/pnas.1714532114</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Brosnan</surname><given-names>SF</given-names></name><name><surname>Salwiczek</surname><given-names>L</given-names></name><name><surname>Bshary</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The interplay of cognition and cooperation</article-title><source>Philosophical Transactions of the Royal Society B</source><volume>365</volume><fpage>2699</fpage><lpage>2710</lpage><pub-id pub-id-type="doi">10.1098/rstb.2010.0154</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bshary</surname><given-names>R</given-names></name><name><surname>Hohner</surname><given-names>A</given-names></name><name><surname>Ait-el-Djoudi</surname><given-names>K</given-names></name><name><surname>Fricke</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2006">2006</year><article-title>Interspecific communicative and coordinated hunting between groupers and giant moray eels in the Red Sea</article-title><source>PLOS Biology</source><volume>4</volume><elocation-id>e431</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pbio.0040431</pub-id><pub-id pub-id-type="pmid">17147471</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Busse</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="1978">1978</year><article-title>Do chimpanzees hunt cooperatively?</article-title><source>The American Naturalist</source><volume>112</volume><fpage>767</fpage><lpage>770</lpage><pub-id pub-id-type="doi">10.1086/283318</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Christianos</surname><given-names>F</given-names></name><name><surname>Schäfer</surname><given-names>L</given-names></name><name><surname>Albrecht</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Shared experience actor-critic for multi-agent reinforcement learning</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>10707</fpage><lpage>10717</lpage></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Couzin</surname><given-names>ID</given-names></name><name><surname>Krause</surname><given-names>J</given-names></name><name><surname>James</surname><given-names>R</given-names></name><name><surname>Ruxton</surname><given-names>GD</given-names></name><name><surname>Franks</surname><given-names>NR</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Collective memory and spatial sorting in animal groups</article-title><source>Journal of Theoretical Biology</source><volume>218</volume><fpage>1</fpage><lpage>11</lpage><pub-id pub-id-type="doi">10.1006/jtbi.2002.3065</pub-id><pub-id pub-id-type="pmid">12297066</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Creel</surname><given-names>S</given-names></name><name><surname>Creel</surname><given-names>NM</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Communal hunting and pack size in African wild dogs, Lycaon pictus</article-title><source>Animal Behaviour</source><volume>50</volume><fpage>1325</fpage><lpage>1339</lpage><pub-id pub-id-type="doi">10.1016/0003-3472(95)80048-4</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dinets</surname><given-names>V</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Apparent coordination and collaboration in cooperatively hunting crocodilians</article-title><source>Ethology Ecology &amp; Evolution</source><volume>27</volume><fpage>244</fpage><lpage>250</lpage><pub-id pub-id-type="doi">10.1080/03949370.2014.915432</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Doya</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Modulators of decision making</article-title><source>Nature Neuroscience</source><volume>11</volume><fpage>410</fpage><lpage>416</lpage><pub-id pub-id-type="doi">10.1038/nn2077</pub-id><pub-id pub-id-type="pmid">18368048</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Evans</surname><given-names>DA</given-names></name><name><surname>Stempel</surname><given-names>AV</given-names></name><name><surname>Vale</surname><given-names>R</given-names></name><name><surname>Branco</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Cognitive control of escape behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>334</fpage><lpage>348</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.01.012</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Foerster</surname><given-names>J</given-names></name><name><surname>Assael</surname><given-names>IA</given-names></name><name><surname>De</surname><given-names>N</given-names></name><name><surname>Whiteson</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Learning to communicate with deep multi-agent reinforcement learning</article-title><conf-name>Proceedings of the 30th International Conference on Neural Information Processing Systems</conf-name><conf-loc>Barcelona, Spain</conf-loc><fpage>2145</fpage><lpage>2153</lpage></element-citation></ref><ref id="bib24"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Foerster</surname><given-names>J</given-names></name><name><surname>Farquhar</surname><given-names>G</given-names></name><name><surname>Afouras</surname><given-names>T</given-names></name><name><surname>Nardelli</surname><given-names>N</given-names></name><name><surname>Whiteson</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Counterfactual Multi-Agent Policy Gradients</article-title><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><pub-id pub-id-type="doi">10.1609/aaai.v32i1.11794</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Foerster</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Bayesian action decoder for deep multi-agent reinforcement learning</article-title><conf-name>International Conference on Machine Learning (PMLR</conf-name><fpage>1942</fpage><lpage>1951</lpage></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Fujimoto</surname><given-names>S</given-names></name><name><surname>Hoof</surname><given-names>H</given-names></name><name><surname>Meger</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Addressing function approximation error in actor-critic methods</article-title><conf-name>In International conference on machine learning (PMLR)</conf-name><fpage>1587</fpage><lpage>1596</lpage></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gazda</surname><given-names>SK</given-names></name><name><surname>Connor</surname><given-names>RC</given-names></name><name><surname>Edgar</surname><given-names>RK</given-names></name><name><surname>Cox</surname><given-names>F</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>A division of labour with role specialization in group–hunting bottlenose dolphins (<italic>Tursiops truncatus</italic>) off Cedar Key, Florida</article-title><source>Proceedings of the Royal Society B</source><volume>272</volume><fpage>135</fpage><lpage>140</lpage><pub-id pub-id-type="doi">10.1098/rspb.2004.2937</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Glorot</surname><given-names>X</given-names></name><name><surname>Bordes</surname><given-names>A</given-names></name><name><surname>Bengio</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Deep sparse rectifier neural networks</article-title><conf-name>In Proceedings of the fourteenth international conference on artificial intelligence and statistics (JMLR Workshop and Conference Proceedings)</conf-name><fpage>315</fpage><lpage>323</lpage></element-citation></ref><ref id="bib29"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hamrick</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>On the role of planning in model-based deep reinforcement learning</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib30"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hasselt</surname><given-names>H</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Double q-learning</article-title><conf-name>Advances in neural information processing systems</conf-name><fpage>2613</fpage><lpage>2621</lpage></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Howland</surname><given-names>HC</given-names></name></person-group><year iso-8601-date="1974">1974</year><article-title>Optimal strategies for predator avoidance: the relative importance of speed and manoeuvrability</article-title><source>Journal of Theoretical Biology</source><volume>47</volume><fpage>333</fpage><lpage>350</lpage><pub-id pub-id-type="doi">10.1016/0022-5193(74)90202-1</pub-id><pub-id pub-id-type="pmid">4437191</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Hu</surname><given-names>H</given-names></name><name><surname>Foerster</surname><given-names>JN</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Simplified action decoder for deep multi-agent reinforcement learning</article-title><conf-name>International Conference on Learning Representations</conf-name></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hubel</surname><given-names>TY</given-names></name><name><surname>Myatt</surname><given-names>JP</given-names></name><name><surname>Jordan</surname><given-names>NR</given-names></name><name><surname>Dewhirst</surname><given-names>OP</given-names></name><name><surname>McNutt</surname><given-names>JW</given-names></name><name><surname>Wilson</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Energy cost and return for hunting in African wild dogs and cheetahs</article-title><source>Nature Communications</source><volume>7</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="doi">10.1038/ncomms11034</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Huber</surname><given-names>PJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><chapter-title>Robust estimation of a location parameter</chapter-title><source>In Breakthroughs in Statistics</source><publisher-name>Springer</publisher-name><fpage>492</fpage><lpage>518</lpage></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hunt</surname><given-names>LT</given-names></name><name><surname>Daw</surname><given-names>ND</given-names></name><name><surname>Kaanders</surname><given-names>P</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name><name><surname>Mugan</surname><given-names>U</given-names></name><name><surname>Procyk</surname><given-names>E</given-names></name><name><surname>Redish</surname><given-names>AD</given-names></name><name><surname>Russo</surname><given-names>E</given-names></name><name><surname>Scholl</surname><given-names>J</given-names></name><name><surname>Stachenfeld</surname><given-names>K</given-names></name><name><surname>Wilson</surname><given-names>CRE</given-names></name><name><surname>Kolling</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Formalizing planning and information search in naturalistic decision-making</article-title><source>Nature Neuroscience</source><volume>24</volume><fpage>1051</fpage><lpage>1064</lpage><pub-id pub-id-type="doi">10.1038/s41593-021-00866-w</pub-id><pub-id pub-id-type="pmid">34155400</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jaderberg</surname><given-names>M</given-names></name><name><surname>Czarnecki</surname><given-names>WM</given-names></name><name><surname>Dunning</surname><given-names>I</given-names></name><name><surname>Marris</surname><given-names>L</given-names></name><name><surname>Lever</surname><given-names>G</given-names></name><name><surname>Castañeda</surname><given-names>AG</given-names></name><name><surname>Beattie</surname><given-names>C</given-names></name><name><surname>Rabinowitz</surname><given-names>NC</given-names></name><name><surname>Morcos</surname><given-names>AS</given-names></name><name><surname>Ruderman</surname><given-names>A</given-names></name><name><surname>Sonnerat</surname><given-names>N</given-names></name><name><surname>Green</surname><given-names>T</given-names></name><name><surname>Deason</surname><given-names>L</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Graepel</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Human-level performance in 3D multiplayer games with population-based reinforcement learning</article-title><source>Science</source><volume>364</volume><fpage>859</fpage><lpage>865</lpage><pub-id pub-id-type="doi">10.1126/science.aau6249</pub-id><pub-id pub-id-type="pmid">31147514</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kawato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Internal models for motor control and trajectory planning</article-title><source>Current Opinion in Neurobiology</source><volume>9</volume><fpage>718</fpage><lpage>727</lpage><pub-id pub-id-type="doi">10.1016/s0959-4388(99)00028-8</pub-id><pub-id pub-id-type="pmid">10607637</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="software"><person-group person-group-type="author"><name><surname>Kazushi</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Collaborative-hunting</data-title><version designator="swh:1:rev:b22af27999a97c564cae2ff8142d54a413e29199">swh:1:rev:b22af27999a97c564cae2ff8142d54a413e29199</version><source>Software Heritage</source><ext-link ext-link-type="uri" xlink:href="https://archive.softwareheritage.org/swh:1:dir:fd9557fca4f245d5ee9aeb8282d5ca516b40ca81;origin=https://github.com/TsutsuiKazushi/collaborative-hunting;visit=swh:1:snp:45fd535119fd5409b499c82dff20d0dc869f9423;anchor=swh:1:rev:b22af27999a97c564cae2ff8142d54a413e29199">https://archive.softwareheritage.org/swh:1:dir:fd9557fca4f245d5ee9aeb8282d5ca516b40ca81;origin=https://github.com/TsutsuiKazushi/collaborative-hunting;visit=swh:1:snp:45fd535119fd5409b499c82dff20d0dc869f9423;anchor=swh:1:rev:b22af27999a97c564cae2ff8142d54a413e29199</ext-link></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Kingma</surname><given-names>DP</given-names></name><name><surname>Ba</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Adam: A Method for Stochastic Optimization</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1412.6980">https://arxiv.org/abs/1412.6980</ext-link></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lang</surname><given-names>SDJ</given-names></name><name><surname>Farine</surname><given-names>DR</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>A multidimensional framework for studying social predation strategies</article-title><source>Nature Ecology &amp; Evolution</source><volume>1</volume><fpage>1230</fpage><lpage>1239</lpage><pub-id pub-id-type="doi">10.1038/s41559-017-0245-0</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lin</surname><given-names>LJ</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Self-improving reactive agents based on reinforcement learning, planning and teaching</article-title><source>Machine Learning</source><volume>8</volume><fpage>293</fpage><lpage>321</lpage><pub-id pub-id-type="doi">10.1007/BF00992699</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Lowe</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Multi-agent actor-critic for mixed cooperative-competitive environments</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>6382</fpage><lpage>6393</lpage></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Macdonald</surname><given-names>DW</given-names></name></person-group><year iso-8601-date="1983">1983</year><article-title>The ecology of carnivore social behaviour</article-title><source>Nature</source><volume>301</volume><fpage>379</fpage><lpage>384</lpage><pub-id pub-id-type="doi">10.1038/301379a0</pub-id><pub-id pub-id-type="pmid">6601775</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Mackintosh</surname><given-names>NJ</given-names></name></person-group><year iso-8601-date="1974">1974</year><source>The Psychology of Animal Learning</source><publisher-name>Academic Press</publisher-name></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mnih</surname><given-names>V</given-names></name><name><surname>Kavukcuoglu</surname><given-names>K</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Rusu</surname><given-names>AA</given-names></name><name><surname>Veness</surname><given-names>J</given-names></name><name><surname>Bellemare</surname><given-names>MG</given-names></name><name><surname>Graves</surname><given-names>A</given-names></name><name><surname>Riedmiller</surname><given-names>M</given-names></name><name><surname>Fidjeland</surname><given-names>AK</given-names></name><name><surname>Ostrovski</surname><given-names>G</given-names></name><name><surname>Petersen</surname><given-names>S</given-names></name><name><surname>Beattie</surname><given-names>C</given-names></name><name><surname>Sadik</surname><given-names>A</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>King</surname><given-names>H</given-names></name><name><surname>Kumaran</surname><given-names>D</given-names></name><name><surname>Wierstra</surname><given-names>D</given-names></name><name><surname>Legg</surname><given-names>S</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Human-level control through deep reinforcement learning</article-title><source>Nature</source><volume>518</volume><fpage>529</fpage><lpage>533</lpage><pub-id pub-id-type="doi">10.1038/nature14236</pub-id><pub-id pub-id-type="pmid">25719670</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mobbs</surname><given-names>D</given-names></name><name><surname>Wise</surname><given-names>T</given-names></name><name><surname>Suthana</surname><given-names>N</given-names></name><name><surname>Guzmán</surname><given-names>N</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Leibo</surname><given-names>JZ</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Promises and challenges of human computational ethology</article-title><source>Neuron</source><volume>109</volume><fpage>2224</fpage><lpage>2238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2021.05.021</pub-id><pub-id pub-id-type="pmid">34143951</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mugan</surname><given-names>U</given-names></name><name><surname>MacIver</surname><given-names>MA</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>3057</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-16102-1</pub-id><pub-id pub-id-type="pmid">32546681</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>O’Keefe</surname><given-names>J</given-names></name><name><surname>Dostrovsky</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1971">1971</year><article-title>The hippocampus as a spatial map Preliminary evidence from unit activity in the freely-moving rat</article-title><source>Brain Research</source><volume>34</volume><fpage>171</fpage><lpage>175</lpage><pub-id pub-id-type="doi">10.1016/0006-8993(71)90358-1</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Packer</surname><given-names>C</given-names></name><name><surname>Ruttan</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1988">1988</year><article-title>The evolution of cooperative hunting</article-title><source>The American Naturalist</source><volume>132</volume><fpage>159</fpage><lpage>198</lpage><pub-id pub-id-type="doi">10.1086/284844</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rashid</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Monotonic value function factorisation for deep multi-agent reinforcement learning</article-title><source>The Journal of Machine Learning Research</source><volume>21</volume><fpage>7234</fpage><lpage>7284</lpage></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Samejima</surname><given-names>K</given-names></name><name><surname>Ueda</surname><given-names>Y</given-names></name><name><surname>Doya</surname><given-names>K</given-names></name><name><surname>Kimura</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Representation of action-specific reward values in the striatum</article-title><source>Science</source><volume>310</volume><fpage>1337</fpage><lpage>1340</lpage><pub-id pub-id-type="doi">10.1126/science.1115270</pub-id><pub-id pub-id-type="pmid">16311337</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sampaio</surname><given-names>E</given-names></name><name><surname>Seco</surname><given-names>MC</given-names></name><name><surname>Rosa</surname><given-names>R</given-names></name><name><surname>Gingins</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Octopuses punch fishes during collaborative interspecific hunting events</article-title><source>Ecology</source><volume>102</volume><elocation-id>e03266</elocation-id><pub-id pub-id-type="doi">10.1002/ecy.3266</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Schaul</surname><given-names>T</given-names></name><name><surname>Quan</surname><given-names>J</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Prioritized Experience Replay</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1511.05952">https://arxiv.org/abs/1511.05952</ext-link></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Scheel</surname><given-names>D</given-names></name><name><surname>Packer</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Group hunting behaviour of lions: a search for cooperation</article-title><source>Animal Behaviour</source><volume>41</volume><fpage>697</fpage><lpage>709</lpage><pub-id pub-id-type="doi">10.1016/S0003-3472(05)80907-8</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schultz</surname><given-names>W</given-names></name><name><surname>Dayan</surname><given-names>P</given-names></name><name><surname>Montague</surname><given-names>PR</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>A neural substrate of prediction and reward</article-title><source>Science</source><volume>275</volume><fpage>1593</fpage><lpage>1599</lpage><pub-id pub-id-type="doi">10.1126/science.275.5306.1593</pub-id><pub-id pub-id-type="pmid">9054347</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Silver</surname><given-names>D</given-names></name><name><surname>Schrittwieser</surname><given-names>J</given-names></name><name><surname>Simonyan</surname><given-names>K</given-names></name><name><surname>Antonoglou</surname><given-names>I</given-names></name><name><surname>Huang</surname><given-names>A</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Hubert</surname><given-names>T</given-names></name><name><surname>Baker</surname><given-names>L</given-names></name><name><surname>Lai</surname><given-names>M</given-names></name><name><surname>Bolton</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>Y</given-names></name><name><surname>Lillicrap</surname><given-names>T</given-names></name><name><surname>Hui</surname><given-names>F</given-names></name><name><surname>Sifre</surname><given-names>L</given-names></name><name><surname>van den Driessche</surname><given-names>G</given-names></name><name><surname>Graepel</surname><given-names>T</given-names></name><name><surname>Hassabis</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Mastering the game of Go without human knowledge</article-title><source>Nature</source><volume>550</volume><fpage>354</fpage><lpage>359</lpage><pub-id pub-id-type="doi">10.1038/nature24270</pub-id><pub-id pub-id-type="pmid">29052630</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Skinner</surname><given-names>BF</given-names></name></person-group><year iso-8601-date="2014">2014</year><source>Contingencies of Reinforcement: A Theoretical Analysis</source><publisher-name>BF Skinner Foundation</publisher-name></element-citation></ref><ref id="bib58"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Smith</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Evolution and the Theory of Games</source><publisher-name>Cambridge university press</publisher-name><pub-id pub-id-type="doi">10.1017/CBO9780511806292</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Son</surname><given-names>K</given-names></name><name><surname>Kim</surname><given-names>D</given-names></name><name><surname>Kang</surname><given-names>WJ</given-names></name><name><surname>Hostallero</surname><given-names>DE</given-names></name><name><surname>Yi</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Qtran: Learning to factorize with transformation for cooperative multi-agent reinforcement learning</article-title><conf-name>International conference on machine learning</conf-name><fpage>5887</fpage><lpage>5896</lpage></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stander</surname><given-names>PE</given-names></name></person-group><year iso-8601-date="1992">1992</year><article-title>Cooperative hunting in lions: the role of the individual</article-title><source>Behavioral Ecology and Sociobiology</source><volume>29</volume><fpage>445</fpage><lpage>454</lpage><pub-id pub-id-type="doi">10.1007/BF00170175</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Stanford</surname><given-names>CB</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>The hunting ecology of wild chimpanzees: Implications for the evolutionary ecology of pliocene hominids</article-title><source>American Anthropologist</source><volume>98</volume><fpage>96</fpage><lpage>113</lpage><pub-id pub-id-type="doi">10.1525/aa.1996.98.1.02a00090</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Steinegger</surname><given-names>M</given-names></name><name><surname>Roche</surname><given-names>DG</given-names></name><name><surname>Bshary</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Simple decision rules underlie collaborative hunting in yellow saddle goatfish</article-title><source>Proceedings of the Royal Society B</source><volume>285</volume><elocation-id>20172488</elocation-id><pub-id pub-id-type="doi">10.1098/rspb.2017.2488</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Sunehag</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Value-Decomposition Networks for Cooperative Multi-Agent Learning</article-title><source>arXiv</source><ext-link ext-link-type="uri" xlink:href="https://arxiv.org/abs/1706.05296">https://arxiv.org/abs/1706.05296</ext-link></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="1981">1981</year><article-title>Toward a modern theory of adaptive networks: expectation and prediction</article-title><source>Psychological Review</source><volume>88</volume><fpage>135</fpage><lpage>170</lpage><pub-id pub-id-type="pmid">7291377</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Sutton</surname><given-names>RS</given-names></name><name><surname>Barto</surname><given-names>AG</given-names></name></person-group><year iso-8601-date="2018">2018</year><source>Reinforcement Learning: An Introduction</source><publisher-name>MIT press</publisher-name></element-citation></ref><ref id="bib66"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tan</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1993">1993</year><article-title>Multi-agent reinforcement learning: Independent vs. cooperative agents</article-title><conf-name>Proceedings of the tenth international conference on machine learning</conf-name><fpage>330</fpage><lpage>337</lpage></element-citation></ref><ref id="bib67"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tesauro</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Extending q-learning to general adaptive multi-agent systems</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Thiebault</surname><given-names>A</given-names></name><name><surname>Semeria</surname><given-names>M</given-names></name><name><surname>Lett</surname><given-names>C</given-names></name><name><surname>Tremblay</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>How to capture fish in a school? Effect of successive predator attacks on seabird feeding success</article-title><source>The Journal of Animal Ecology</source><volume>85</volume><fpage>157</fpage><lpage>167</lpage><pub-id pub-id-type="doi">10.1111/1365-2656.12455</pub-id><pub-id pub-id-type="pmid">26768335</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsutsui</surname><given-names>K</given-names></name><name><surname>Shinya</surname><given-names>M</given-names></name><name><surname>Kudo</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Human navigational strategy for intercepting an erratically moving target in chase and escape interactions</article-title><source>Journal of Motor Behavior</source><volume>52</volume><fpage>750</fpage><lpage>760</lpage><pub-id pub-id-type="doi">10.1080/00222895.2019.1692331</pub-id><pub-id pub-id-type="pmid">31790635</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tsutsui</surname><given-names>K</given-names></name><name><surname>Fujii</surname><given-names>K</given-names></name><name><surname>Kudo</surname><given-names>K</given-names></name><name><surname>Takeda</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Flexible prediction of opponent motion with internal representation in interception behavior</article-title><source>Biological Cybernetics</source><volume>115</volume><fpage>473</fpage><lpage>485</lpage><pub-id pub-id-type="doi">10.1007/s00422-021-00891-9</pub-id><pub-id pub-id-type="pmid">34379183</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Tsutsui</surname><given-names>K</given-names></name><name><surname>Takeda</surname><given-names>K</given-names></name><name><surname>Fujii</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Emergence of collaborative hunting via multi-agent deep reinforcement learning</article-title><conf-name>International Conference on Pattern Recognition</conf-name><fpage>210</fpage><lpage>224</lpage></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vail</surname><given-names>AL</given-names></name><name><surname>Manica</surname><given-names>A</given-names></name><name><surname>Bshary</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Referential gestures in fish collaborative hunting</article-title><source>Nature Communications</source><volume>4</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1038/ncomms2781</pub-id><pub-id pub-id-type="pmid">23612306</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>van der Maaten</surname><given-names>L</given-names></name><name><surname>Hinton</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Visualizing data using t-sne</article-title><source>Journal of Machine Learning Research</source><volume>9</volume><fpage>2579</fpage><lpage>2605</lpage></element-citation></ref><ref id="bib74"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Van Hasselt</surname><given-names>H</given-names></name><name><surname>Guez</surname><given-names>A</given-names></name><name><surname>Silver</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Deep Reinforcement Learning with Double Q-Learning</article-title><conf-name>Proceedings of the AAAI Conference on Artificial Intelligence</conf-name><fpage>2094</fpage><lpage>2100</lpage><pub-id pub-id-type="doi">10.1609/aaai.v30i1.10295</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Dueling network architectures for deep reinforcement learning</article-title><conf-name>International conference on machine learning</conf-name></element-citation></ref><ref id="bib76"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wilson</surname><given-names>AM</given-names></name><name><surname>Hubel</surname><given-names>TY</given-names></name><name><surname>Wilshin</surname><given-names>SD</given-names></name><name><surname>Lowe</surname><given-names>JC</given-names></name><name><surname>Lorenc</surname><given-names>M</given-names></name><name><surname>Dewhirst</surname><given-names>OP</given-names></name><name><surname>Bartlam-Brooks</surname><given-names>HLA</given-names></name><name><surname>Diack</surname><given-names>R</given-names></name><name><surname>Bennitt</surname><given-names>E</given-names></name><name><surname>Golabek</surname><given-names>KA</given-names></name><name><surname>Woledge</surname><given-names>RC</given-names></name><name><surname>McNutt</surname><given-names>JW</given-names></name><name><surname>Curtin</surname><given-names>NA</given-names></name><name><surname>West</surname><given-names>TG</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Biomechanics of predator–prey arms race in lion, zebra, cheetah and impala</article-title><source>Nature</source><volume>554</volume><fpage>183</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1038/nature25479</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wolpert</surname><given-names>DM</given-names></name><name><surname>Miall</surname><given-names>RC</given-names></name><name><surname>Kawato</surname><given-names>M</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Internal models in the cerebellum</article-title><source>Trends in Cognitive Sciences</source><volume>2</volume><fpage>338</fpage><lpage>347</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(98)01221-2</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Wynne</surname><given-names>CD</given-names></name></person-group><year iso-8601-date="2001">2001</year><source>Animal Cognition: The Mental Lives of Animals</source><publisher-name>Palgrave MacMillan</publisher-name></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yoshida</surname><given-names>W</given-names></name><name><surname>Dolan</surname><given-names>RJ</given-names></name><name><surname>Friston</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Game theory of mind</article-title><source>PLOS Computational Biology</source><volume>4</volume><elocation-id>e1000254</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1000254</pub-id><pub-id pub-id-type="pmid">19112488</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Yu</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The surprising effectiveness of ppo in cooperative multi-agent games</article-title><conf-name>Advances in Neural Information Processing Systems</conf-name><fpage>24611</fpage><lpage>24624</lpage></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85694.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>MacIver</surname><given-names>Malcolm A</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Northwestern University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.10.10.511517" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.10.10.511517"/></front-stub><body><p>Cooperative hunting is typically attributed to certain mammals (and select birds) which express highly complex behaviors. This paper makes the valuable finding that in a highly idealized open environment, cooperative hunting can emerge through simple rules. This has implications for a reassessment, and perhaps a widening, of what groups of animals are believed to manifest cooperative hunting.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85694.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>MacIver</surname><given-names>Malcolm A</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Northwestern University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>MacIver</surname><given-names>Malcolm A</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/000e0be47</institution-id><institution>Northwestern University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.10.10.511517">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.10.10.511517v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Collaborative hunting in artificial agents with deep reinforcement learning&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Michael Frank as the Senior Editor.</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions (for the authors):</p><p>1) There are extensive remarks from the reviewers on the validity of some of the more theoretical claims. These need to be carefully addressed. Examples include whether it is correct to infer that more complex cognition is not needed for cooperative hunting because DQN can solve the problem; The absence of an explicit model does not mean that there isn't an implicit model of the other agents behaviors that is encoded in the neural network weights.</p><p>2) The lack of causal analysis either needs to be addressed or the &quot;mediated by&quot;-type claims need to be tempered.</p><p>3) There has been a translation of the DQN into simple rules, but at present the discussion is too incomplete for readers to understand why this was done and what can be concluded.</p><p>4) A discussion of the limitations of this work in terms of the absence of things like shared value functions increasingly common in multi-agent RL and absence of partially observable environments common in predator-prey dynamics, would be helpful.</p><p>5) Please address R3's comment that the paper's analysis is possible without RL, such as via behavioral cloning.</p><p>6) Some of the reviews suggest better coverage of the relevant literature. Given the size of this literature, this has to be selective, but it appears some effort could be expended to further improve the scholarship of the work.</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>Figure 2 – The gap between individual and shared only occurs at two predators, for both % successful predation and duration. It would be helpful to have a discussion of this point. Wild dog packs, for example, are typically larger: perhaps this because the space they work over is much larger (relative to disk size), the environment complexity, or something else, but in any case it would be interesting to know whether shared vs individual is the ruling condition.</p><p>It also suggests that sharing is not needed in the 3 predator situation to obtain the same results. Does that mean that the work is suggesting cooperation even occurs without sharing? This seems to be a significant problem, since it's hard to imagine how the term &quot;collaboration&quot; or &quot;cooperation&quot; can be applied in the absence of shared reward. If it is strictly a matter of reduction of duration and increase in rate of success, it may equate to a more limited form of cooperation? Are their biological analogs of group hunts without sharing?</p><p>186 – We found that the mappings resulting in collaborative hunting were mediated by distance-dependent internal representations.</p><p>'Mediated' here seems to play the role of a &quot;filler term&quot; as used in neuroscience (see Krakauer et al. 2017 Neuroscience needs behavior).</p><p>Only correlations have been shown, but this is a causal claim. To support the causal claim, it would be necessary to intervene in the network and show that the interventions in the internal representations have the predicted causal role.</p><p>194 – The organization of this paragraph might be better reversed. One could argue that Figure S8 (which could be referenced here) providing similar results and DQN helps support the hypothesis that the representation of distance in the network plays a causal role in the outcome.</p><p>234 – Initial position of each episode is unclear as previously noted.</p><p>236 – The text above says -1 for moving out of arena to the prey, so if the prey moves out, is it just that, or does the predator also get +1 since the predator is now deemed &quot;successful&quot;?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>– I am not clear that there is sufficient evidence for lines 172-174. The absence of an explicit model does not mean that there isn't an implicit model of the other agents behaviors that is encoded in the neural network weights. I'm not clear what sort of experiment would allow you to distinguish this though there might be a way to run a linear probe to confirm that this information is not in the network weights?</p><p>– The notation in lines 246-251 is confusing because it alternates between POMDP notation (i.e. that the agent gets an observation that is a transformation of the true state) and MDP notation. Is the setup an MDP or a POMDP?</p><p>– Perhaps line 259 should be &quot;by finding a fixed point of the Bellman equations?&quot;</p><p>– Line 271 should be &quot;Dueling Networks&quot; not &quot;Dueling Network&quot; – The sentence starting on line 271 and ending on 273 could or should be cut entirely as it doesn't provide much value and I think it's debatable whether DQN was the first algorithm to solve a high dimensional input problem; it very much depends on how you define high dimensiona</p><p>– To get equation 3 from equation 2, there needs to be a factor of 1/2 somewhere.</p><p>– In line 321 I don't know what identifiability means in the context of Q-learning? Is this a technical term used in some subfield that works on Q-learning? Why does subtracting the mean help with &quot;identifiability?&quot;&quot;</p><p>– A discount factor of 0.9 is a wildly low discount factor, basically leading agents to only care about the next 10 steps. I don't think this necessarily affects the outcome of your project or necessarily requires any changes as I don't think agents need to do long horizon reasoning here, but it's worth keeping in mind!</p><p>– I don't fully understand the claim that this expands the range of things that are understood to be possible to learn via associative learning. There's no theory precluding a model-free algorithm from learning this type of behavior so the claim in the discussion strikes me as odd. In practice, this type of result where model-free RL agents successfully hunt together have been around since the release of the multi-particle envs (see https://proceedings.neurips.cc/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html)</p><p>– I think the rule-based model is neat but I don't understand what what question it answers. Did I perhaps miss something?</p><p>– I don't find the evidence for the distance-dependent features compelling; is all of the evidence for it the t-SNE embeddings?</p><p>– Lines 194-196 are confusing to me. Why does there being a rule-based model employ your DQN agent is also learning a similar rule-based model?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic>My largest suggestion is to fit a linear model to rule-based behaviors and compare the t-SNE embeddings of the behavioral cloning policy with the embeddings of the RL policy? Is the use of RL truly important for this paper?Around line 362, the idea of Rule based agents and human controlled agents are also introduced. I would like to see linear models that take as observations the rule-based agents observations and output the rule based agents actions. Would the t-SNE embeddings look similar for these linear models and for the RL-trained models? If the embeddings look similar, what does that say about the emergence of these capabilities as a result of RL? Does training via RL even matter? Do we care if it doesn't matter?</p><p>There is a large amount of work on multi-agent learning that this paper seemingly ignores, or fails to evaluate against. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments has thousands of citations. However, I am willing to accept that there are limitations to what a single paper can cover.</p><p>More specific comments:</p><p>Line 46-47. I do not know what &quot;simple descriptions with the distances can reproduce similar behavior&quot; is trying to convey.</p><p>Lines 50-51: &quot;Our approach of computational ecology bridges the gap between ecology, ethology, and neuroscience and may provide a comprehensive account of them.&quot; This is probably too strong of a claim.</p><p>Figure 1: The architecture diagram is a little difficult to understand. The key has &quot;layer with ReLU&quot; but then I do not see any clear white boxes? I also do not see any clear units? I think that maybe this is happening inside of the &quot;prey,&quot; &quot;predator 1,&quot; etc boxes. However, this is all much too small. I think you should decide if you want this figure to be about the neural network architecture, or about the fact the environment is broken into 1 prey and N predators that share an observation.</p><p>I think the actions are also not clear. There are probably too many lines in the figure.</p><p>For Figure 1 (b), why not just plot the actual density? Actually, I see this is included in Figure 2. I think this is the more helpful Figure!</p><p>In Figure 2, what form of Hypothesis testing was used? Was this a KS test? You can't assume the distributions are Gaussian? The presence of a chi-squared statistic seems to indicate Gaussian assumptions. But the distribution is strongly non-Gaussian in this case. A little more clarity would be helpful.</p><p>Line 132 mentions that the variance is higher over the action distribution when the predator is about to catch the prey? This is actually the exact opposite of my intuition. I think that the actions hardly matter when the prey is far away, so there is no obvious optimal action, and the choice would be closer to uniform. I'm not sure that this matters very much, but it's interesting.</p><p>Line 325 – Usually IL is reserved for Imitation Learning. I have never seen it used for Independent Learning.</p><p>Line 324 – I think biological organisms usually model the behavior of other organisms and account for it while planning.</p><p>Line 212 – Q-values do implicitly model the competencies of other agents.</p><p>Line 196 – What does it mean to switch the decision rules with the distances?</p><p>Overall, I think the problems considered by this paper are interesting. And I am happy you took the time to write it. This work made me think a lot about my own research. I appreciate your efforts here. Thank you.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.85694.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions (for the authors):</p><p>1) There are extensive remarks from the reviewers on the validity of some of the more theoretical claims. These need to be carefully addressed. Examples include whether it is correct to infer that more complex cognition is not needed for cooperative hunting because DQN can solve the problem; The absence of an explicit model does not mean that there isn't an implicit model of the other agents behaviors that is encoded in the neural network weights.</p></disp-quote><p>Thank you for your constructive criticism regarding the validity of our theoretical claims. We have carefully considered the extensive remarks from both you and the reviewers, and have engaged in in-depth discussions with field experts, including a leading scientist on chimpanzee cognition, to address them. As you and the reviewers have pointed out, it is challenging to verify whether an implicit model of other agents’ behaviors is encoded within the neural network weights. Therefore, we have revised our manuscript to refine our claims to specifically address the absence of aspects of “theory of mind”, as it is certain that the agents in our study do not model or infer the “mental states” of others. Specifically, we have revised the description “high-level cognition such as sharing intentions among predators or modeling competencies of other agents” pointed out by the reviewers to “high-level cognition such as aspects of theory of mind” throughout the manuscript. Although this revision may narrow or moderate our argument, we believe it significantly enhances the precision and accuracy of our discussion. Furthermore, by focusing on aspects of theory of mind, we can create a clear distinction from previous studies that incorporated comparable explicit mechanisms, thus aiding the reader's comprehension of our claims and the future directions of our study. We believe these revisions more accurately address the concerns you and the reviewers have raised and ensure our theoretical claims. We are grateful for the guidance that has helped us in improving the manuscript.</p><disp-quote content-type="editor-comment"><p>2) The lack of causal analysis either needs to be addressed or the &quot;mediated by&quot;-type claims need to be tempered.</p></disp-quote><p>Thank you for your insightful comment regarding the lack of causal analysis. We have carefully considered your critique and agree that a more cautious approach is warranted in the absence of a direct causal analysis. In response, we have tempered our claims throughout the manuscript to reflect this. We have modified any statements that may have implied a stronger causal relationship than is currently supported by the data, ensuring that our descriptions accurately represent the correlative nature of our findings, such as being “related to” or “associated with” specific outcomes.</p><disp-quote content-type="editor-comment"><p>3) There has been a translation of the DQN into simple rules, but at present the discussion is too incomplete for readers to understand why this was done and what can be concluded.</p></disp-quote><p>We appreciate your feedback on the translation of the DQN into simple rules and the need for a more comprehensive discussion to aid reader comprehension. As mentioned above, we have revised our manuscript to focus our claims on the absence of aspects of theory of mind. Additionally, in line with the reviewer’s comment, we have revised the sentence in the Introduction section as follows (lines 36 to 39 in the revised manuscript):</p><p>“Given that associative learning is likely to be the most widely adopted learning mechanism in animals, collaborative hunting could arise through associative learning, where simple decision rules are developed based on behavioral cues (i.e., contingencies of reinforcement).”</p><p>Furthermore, we have added the following sentence to the Introduction section (lines 44 to 46 in the revised manuscript):</p><p>“Notably, our predator agents successfully learned to collaborate in capturing their prey solely through a reinforcement learning algorithm, without employing explicit mechanisms comparable to aspects of theory of mind.”</p><p>These revisions and additions aim to provide a clearer exposition of why the translation was undertaken and to discuss more explicitly what conclusions can be drawn from it. We hope that these changes will make the ideas more accessible and the underlying reasoning more transparent to our readers.</p><disp-quote content-type="editor-comment"><p>4) A discussion of the limitations of this work in terms of the absence of things like shared value functions increasingly common in multi-agent RL and absence of partially observable environments common in predator-prey dynamics, would be helpful.</p></disp-quote><p>We are grateful for your suggestion to discuss the limitations of our work. In response to your feedback, we have incorporated a discussion of these limitations within the existing conclusion paragraph of our manuscript. In this revision, we have included the “partial observability” alongside other elements of predator-prey dynamics. We have also added the sentence regarding the “shared value functions” as potential directions for future research to suggest areas that could benefit from further exploration. The revised conclusion paragraph is as follows (lines 223 to 234 in the revised manuscript):</p><p>“In conclusion, we demonstrated that the decisions underlying collaborative hunting among artificial agents can be achieved through mappings between states and actions. This means that collaborative hunting can emerge in the absence of explicit mechanisms comparable to aspects of theory of mind, supporting the recent idea that collaborative hunting does not necessarily rely on complex cognitive processes in brains. Our computational ecology is an abstraction of a real predator-prey environment. Given that chase and escape often involve various factors, such as energy cost, partial observability, signal communication, and local surroundings, these results are only a first step on the path to understanding real decisions in predator-prey dynamics. Furthermore, exploring how mechanisms comparable to aspects of theory of mind or the shared value functions, which are increasingly common in multi-agent reinforcement learning, play a role in these interactions could be an intriguing direction for future research. We believe that our results provide a useful advance toward understanding natural value-based decisions and forge a critical link between ecology, ethology, psychology, neuroscience, and computer science.”</p><p>We believe that these revisions will greatly assist our readers in understanding the scope and implications of our work.</p><disp-quote content-type="editor-comment"><p>5) Please address R3's comment that the paper's analysis is possible without RL, such as via behavioral cloning.</p></disp-quote><p>Thank you for your comment about the possibility of conducting our analysis without the use of RL. We have considered this perspective and have realized that indeed certain analyses could be substituted with behavioral cloning in some cases. Nevertheless, we would like to emphasize that the use of RL brings clarity to several aspects of our study. We describe the reasons for this below from the perspectives of both data and analysis.</p><p>Data: Collaborative hunting data is generally scarce, and to our knowledge, no extensive dataset exists with complete locational data on all individuals during hunts. Furthermore, in many cases, obtaining completely controlled data from field observations is challenging. For example, data collected in the wild tend to exhibit some biases (Lang and Farine, 2017). Consequently, we believe that the controlled comparisons presented in our Figure 2 would be difficult to achieve without RL, which makes them notable results.</p><p>Analysis: Additionally, we believe that even with a large and controlled dataset, limitations exist in referring the decision-making process from behavioral cloning results and the visualization of internal representations. One such limitation is the prediction accuracy of behavioral cloning, which would not be 100%. Our additional analysis, conducted in response to Reviewer 3's suggestion, demonstrated that prediction accuracy was, at best, 60-70% (Figure 3 supplement 7). In such case, it would be difficult to rule out the possibility that complex cognitive processes are involved in the remaining percentage. Therefore, although we need to be careful in the interpretation of what is being learned on the deep Q-network as you and the reviewers pointed out again and again, the explicit architecture of RL agents strengthens our argument. Thus, even if sufficient data are available, RL would still be meaningful for our analysis.</p><disp-quote content-type="editor-comment"><p>6) Some of the reviews suggest better coverage of the relevant literature. Given the size of this literature, this has to be selective, but it appears some effort could be expended to further improve the scholarship of the work.</p></disp-quote><p>Thank you for pointing that out. Upon reflection, we agree with you and the reviewers that the original manuscript could benefit from more comprehensive coverage of the relevant literature. We have reviewed additional papers focusing mainly on multi-agent reinforcement learning and predator-prey dynamics and have incorporated these into the references in our manuscript.</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>Figure 2 – The gap between individual and shared only occurs at two predators, for both % successful predation and duration. It would be helpful to have a discussion of this point. Wild dog packs, for example, are typically larger: perhaps this because the space they work over is much larger (relative to disk size), the environment complexity, or something else, but in any case it would be interesting to know whether shared vs individual is the ruling condition.</p></disp-quote><p>We appreciate your comment on the gap between individual and shared condition concerning the proportion of successful predation and duration. We have added a discussion in our manuscript that explores the reasons behind the comparable performance in scenarios involving three predators, whether the prey was shared or not (lines 192 to 199 in the revised manuscript). As you suggested, we considered spatial constraints as a contributing factor to this outcome. Our analysis revealed that predators occasionally exploit the play area's boundaries and the movement of other predators to block the prey's escape path. To illustrate these dynamics, we have added a supplementary figure (Figure 2 supplement 4). We believe this addition will provide a clearer understanding of our result and hope you find this enhancement informative.</p><disp-quote content-type="editor-comment"><p>It also suggests that sharing is not needed in the 3 predator situation to obtain the same results. Does that mean that the work is suggesting cooperation even occurs without sharing? This seems to be a significant problem, since it's hard to imagine how the term &quot;collaboration&quot; or &quot;cooperation&quot; can be applied in the absence of shared reward. If it is strictly a matter of reduction of duration and increase in rate of success, it may equate to a more limited form of cooperation? Are their biological analogs of group hunts without sharing?</p></disp-quote><p>Thank you for your point regarding “cooperation” in the absence of reward sharing. We agree with your comment that the significant improvement in success rates and reduction in hunting duration, compared with the theoretical predictions based on solitary hunting results, suggests a more limited form of cooperation. This concept finds analogous to interspecific group hunting. For example, giant moray eels and groupers have been reported to hunt together though they do not share the prey (Bshary et al., 2006); their repeated interactions may eventually lead to a distribution of prey between both predators. This could be a mutually beneficial relationship that emerges over time without direct reward sharing. Our results also showed a consequent distribution of prey (see Figure 2 supplement 3), suggesting the potential emergence of this form of cooperation.</p><disp-quote content-type="editor-comment"><p>186 – We found that the mappings resulting in collaborative hunting were mediated by distance-dependent internal representations.</p><p>'Mediated' here seems to play the role of a &quot;filler term&quot; as used in neuroscience (see Krakauer et al. 2017 Neuroscience needs behavior).</p><p>Only correlations have been shown, but this is a causal claim. To support the causal claim, it would be necessary to intervene in the network and show that the interventions in the internal representations have the predicted causal role.</p></disp-quote><p>Thank you for your constructive comment concerning the use of the term “mediated”. After reviewing the paper by Krakauer et al. 2017 that you referenced, we have understood that our use of “mediated” inappropriately suggested a causal claim in the absence of direct causal analysis. We have therefore revised our manuscript to more accurately reflect the correlational nature of our findings. Specifically, as the editor suggested, we have tempered our statements throughout the manuscript to ensure that it does not imply causality.</p><disp-quote content-type="editor-comment"><p>194 – The organization of this paragraph might be better reversed. One could argue that Figure S8 (which could be referenced here) providing similar results and DQN helps support the hypothesis that the representation of distance in the network plays a causal role in the outcome.</p></disp-quote><p>Thank you for your suggestion regarding the organization of the paragraph. We have revised the text and its sequence to better illustrate the role of the additional analysis with the rule-based model. The revised paragraph reads as follows (lines 200 to 211 in the revised manuscript):</p><p>“We found that the mappings resulting in collaborative hunting were related to distance-dependent internal representations. Additionally, we showed that the distance-dependent rule-based predators successfully reproduced behaviors similar to those of the deep reinforcement learning predators, supporting the association between decisions and distances (Methods; Figure 3 supplements 5, 6, and 7). Deep reinforcement learning has held the promise for providing a comprehensive framework for studying the interplay among learning, representation, and decision making, but such efforts for natural behavior have been limited. Our result that the distance-dependent representations relate to collaborative hunting is reminiscent of a recent idea about the decision rules obtained by observation in fish. Notably, the input variables of predator agents do not include variables corresponding to the distance(s) between the other predator(s) and prey, and this means that the predators in the shared conditions acquired the internal representation relating to distance to prey, which would be a geometrically reasonable indicator, by optimization through interaction with their environment. Our results suggest that deep reinforcement learning methods can extract systems of rules that allow for the emergence of complex behaviors.”</p><disp-quote content-type="editor-comment"><p>234 – Initial position of each episode is unclear as previously noted.</p></disp-quote><p>We thank you for your comment on the initial positions of the agents in each episode. We have revised the manuscript to provide a more precise description of the agents’ initial positioning. These details are described in the second paragraph of the Results section and in the Environment subsection of the Methods section (lines 77 to 79, and 247 to 248, in the revised manuscript, respectively).</p><disp-quote content-type="editor-comment"><p>236 – The text above says -1 for moving out of arena to the prey, so if the prey moves out, is it just that, or does the predator also get +1 since the predator is now deemed &quot;successful&quot;?</p></disp-quote><p>We apologize for any confusion caused by the description of the reward and successful predation. During training phase, if the prey moves out of the arena, the predator does not receive a positive reward. We determined that it was not appropriate for the predator to be rewarded in such instances, especially during the early stages of learning, as the movement of the prey outside the arena is often not directly related to the predator's actions. On the other hand, for the evaluation phase, we consider such instances as “successful predation”. This is because, even with trained prey, there are instances where they may exit the arena in an attempt to evade predators, particularly when multiple predators are involved. In such scenarios, it seems reasonable to regard the prey's moving out as indicative of successful predation. To facilitate the reader's understanding, we have added the following clarification to the second paragraph of the Results section (lines 82 to 85 in the revised manuscript):</p><p>“During the evaluation phase, if the predator captured the prey within the time limit, the predator was deemed successful; otherwise, the prey was considered successful. Additionally, if one side (predators/prey) moved out of the area, the other side (prey/predators) was deemed successful.”</p><p>Furthermore, to avoid any confusion among our readers, we have moved the original description you pointed out from the Environment subsection in the Methods section to the Evaluation subsection (lines 360 to 369 in the revised manuscript). We believe these changes will make the paper more clear and reader-friendly and hope this explanation clarifies your doubts. Thank you for bringing this to our attention.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>– I am not clear that there is sufficient evidence for lines 172-174. The absence of an explicit model does not mean that there isn't an implicit model of the other agents behaviors that is encoded in the neural network weights. I'm not clear what sort of experiment would allow you to distinguish this though there might be a way to run a linear probe to confirm that this information is not in the network weights?</p></disp-quote><p>Thank you for your constructive criticism regarding the validity of our theoretical claims. We have carefully considered the feedback from you and the other reviewers, and have engaged in in-depth discussions with field experts, including a leading scientist on chimpanzee cognition, to address them. As you suggested, it is challenging to verify whether an implicit model of other agents’ behaviors is encoded within the neural network weights. Therefore, we have revised our manuscript to refine our claims to specifically address the absence of aspects of “theory of mind”, as it is certain that the agents in our study do not model or infer the “mental states” of others. Although this revision may narrow or moderate our argument, we believe it significantly enhances the precision and accuracy of our discussion. We believe these revisions more accurately address the concerns you and the reviewers have raised and ensure our theoretical claims. We are grateful for the guidance that has helped us to improve the manuscript.</p><disp-quote content-type="editor-comment"><p>– The notation in lines 246-251 is confusing because it alternates between POMDP notation (i.e. that the agent gets an observation that is a transformation of the true state) and MDP notation. Is the setup an MDP or a POMDP?</p></disp-quote><p>We apologize for any confusion caused by the inconsistent notation. The setup we used is an MDP, not a POMDP. We have corrected the relevant descriptions in the Methods section and the notation in Figure 1a to consistently reflect an MDP framework (lines 257 to 263 in the revised manuscript). Thank you for bringing this to our attention, and we appreciate your patience as we rectify this error.</p><disp-quote content-type="editor-comment"><p>– Perhaps line 259 should be &quot;by finding a fixed point of the Bellman equations?&quot;</p></disp-quote><p>Thank you for your suggestion concerning the phrasing. We have amended the manuscript accordingly (line 271 in the revised manuscript). We appreciate your attention to detail and your assistance in enhancing the technical accuracy of our paper.</p><disp-quote content-type="editor-comment"><p>– Line 271 should be &quot;Dueling Networks&quot; not &quot;Dueling Network&quot;</p></disp-quote><p>Thank you for pointing out the correct terminology. We have made the correction to “Dueling Networks” as you suggested (line 283 in the revised manuscript). Additionally, we have capitalized the initial letters of RL methods throughout the manuscript.</p><disp-quote content-type="editor-comment"><p>– The sentence starting on line 271 and ending on 273 could or should be cut entirely as it doesn't provide much value and I think it's debatable whether DQN was the first algorithm to solve a high dimensional input problem; it very much depends on how you define high dimensiona</p></disp-quote><p>We appreciate your critical feedback on the sentence. Upon reflection, we agree with your suggestion and have therefore removed it from the manuscript. Thank you for guiding us towards a more concise and accurate presentation of our work.</p><disp-quote content-type="editor-comment"><p>– To get equation 3 from equation 2, there needs to be a factor of 1/2 somewhere.</p></disp-quote><p>Thank you for pointing out the discrepancy between equations 2 and 3. We have included the factor of 1/2 to ensure the correctness of the equations. Again, we appreciate your attention to detail and your assistance with our work.</p><disp-quote content-type="editor-comment"><p>– In line 321 I don't know what identifiability means in the context of Q-learning? Is this a technical term used in some subfield that works on Q-learning? Why does subtracting the mean help with &quot;identifiability?&quot;&quot;</p></disp-quote><p>Thank you for your careful review and for bringing to our attention the term “identifiability”. We referred to the term as it was introduced in the paper by Wang et al. (2016) on Dueling Networks. However, after re-evaluating its usage based on your suggestion, we agree that subtracting the mean does not necessarily aid identifiability. Consequently, we have removed the related sentences from the Methods section of our manuscript and appreciate your guidance on this matter.</p><disp-quote content-type="editor-comment"><p>– A discount factor of 0.9 is a wildly low discount factor, basically leading agents to only care about the next 10 steps. I don't think this necessarily affects the outcome of your project or necessarily requires any changes as I don't think agents need to do long horizon reasoning here, but it's worth keeping in mind!</p></disp-quote><p>Thank you for your advice on the choice of the discount factor. We will certainly take this into consideration and pay close attention to the impact of different discount factors on agent behavior in future research!</p><disp-quote content-type="editor-comment"><p>– I don't fully understand the claim that this expands the range of things that are understood to be possible to learn via associative learning. There's no theory precluding a model-free algorithm from learning this type of behavior so the claim in the discussion strikes me as odd. In practice, this type of result where model-free RL agents successfully hunt together have been around since the release of the multi-particle envs (see https://proceedings.neurips.cc/paper/2017/hash/68a9750337a418a86fe06c1991a1d64c-Abstract.html)</p></disp-quote><p>Thank you for your input on our discussion about associative learning. After considering your perspective, we agree with your comment and have removed the related statements from the discussion.</p><disp-quote content-type="editor-comment"><p>– I think the rule-based model is neat but I don't understand what what question it answers. Did I perhaps miss something?</p></disp-quote><p>The rule-based model was developed to support our claim that predator agents' decisions are related to distance-dependent internal representations. We examined the state vectors of last hidden layers in each agent's network, which lead to action values after a single linear transformation and aggregation. With this in mind, we posited that if neural networks encode the distances between predators and prey, a concise rule-based model based on these distances should replicate similar behaviors. This additional analysis, prompted by a reviewer's comments, sought to substantiate our claim. While successfully replicating predator behavior using a distance-dependent rule-based model does not completely prove that the RL agent's decision are associated with the distances, it would provide support for our assertion. Additionally, in response to Reviewer 1's suggestion, we have relocated the description of these results to a dedicated paragraph in the Discussion section that explores the relationship between agent decisions and distance-dependent representations (lines 201 to 203 in the revised manuscript), thereby clarifying the aim of this additional analysis for the reader.</p><disp-quote content-type="editor-comment"><p>– I don't find the evidence for the distance-dependent features compelling; is all of the evidence for it the t-SNE embeddings?</p></disp-quote><p>As mentioned in our previous response, our assertion that predator agents’ decisions are related to the distances is supported by analyses of rule-based modeling as well as t-SNE embeddings. These approaches aim to provide a comprehensive understanding of the role of the distances in the agents' decision processes. Additionally, as Reviewer 1 highlighted, the distances between predators and prey agents are directly related to their rewards, making it plausible that these distances factor into the computation of action values during decision-making. We believe that this evidence addresses your concerns.</p><disp-quote content-type="editor-comment"><p>– Lines 194-196 are confusing to me. Why does there being a rule-based model employ your DQN agent is also learning a similar rule-based model?</p></disp-quote><p>Thank you for your continued engagement with our work. While partially reiterating what was mentioned in our previous response, we would like to clarify the rationale behind employing a rule-based model. It is to demonstrate that if predator agents encode the distances between predators and prey within their neural networks, these distances could potentially be used to construct a simple rule-based model that replicates the agents’ behavior. We have recognized that the initial presentation of the rule-based model's description could have been abrupt and confusing. Consequently, we have moved this discussion to the fourth paragraph of the Discussion section (lines 200 to 211 in the revised manuscript) and have provided a detailed explanation of its purpose and implementation in the Methods section (lines 396 to 432 in the revised manuscript). This rearrangement aims to make the intent and methodology of the additional analysis clearer to the reader.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p><p>My largest suggestion is to fit a linear model to rule-based behaviors and compare the t-SNE embeddings of the behavioral cloning policy with the embeddings of the RL policy? Is the use of RL truly important for this paper?</p></disp-quote><p>Thank you for your substantial suggestion to compare the t-SNE embeddings of both the behavioral cloning policy and the RL policy. As advised, we implemented two types of networks: a linear network without any nonlinear transformation and a nonlinear network with ReLU activations, and have conducted the comparison as shown in Figure 3 —figure supplement 7 (top: linear network, bottom: nonlinear network).</p><p>We found the results to be intriguing because these are somehow similar to the RL embeddings, which we consider promising for potential application to other biological data we possess. We are grateful for this insightful recommendation, and details of these procedures and results have been added to Methods section and Supplementary Figures (lines 433 to 452, and Figure 3 supplement 7, in the revised manuscript, respectively). However, as mentioned in our response to the editor, we firmly believe that the use of RL was essential for the outcomes presented in this paper. While some analyses could indeed be substituted with behavioral cloning in some cases, as this additional analysis has shown, we believe that the use of RL is still important for this paper. The reasons for this are described below from the respective perspectives of data and analysis.</p><p>Data: Collaborative hunting data is generally scarce, and to our knowledge, no extensive dataset exists with complete locational data on all individuals during hunts. Furthermore, in many cases, obtaining completely controlled data from field observations is challenging. For example, data collected in the wild tend to exhibit some biases (Lang and Farine, 2017). Consequently, we believe that the controlled comparisons presented in our Figure 2 would be difficult to achieve without RL, which makes them notable results.</p><p>Analysis: Additionally, we believe that even with a large and controlled dataset, limitations exist in referring the decision-making process from behavioral cloning results and the visualization of internal representations. One such limitation is the prediction accuracy of behavioral cloning, which would not be 100%. Our additional analysis, conducted in response to your suggestion, demonstrated that prediction accuracy was, at best, 60-70% (Figure 3 supplement 7). In such case, it would be difficult to rule out the possibility that complex cognitive processes are involved in the remaining percentage. Therefore, although we need to be careful in the interpretation of what is being learned on the deep Q-network as you and the other reviewers pointed out again and again, the explicit architecture of RL agents strengthens our argument. Thus, even if sufficient data are available, RL would still be meaningful for our analysis.</p><disp-quote content-type="editor-comment"><p>Around line 362, the idea of Rule based agents and human controlled agents are also introduced. I would like to see linear models that take as observations the rule-based agents observations and output the rule based agents actions. Would the t-SNE embeddings look similar for these linear models and for the RL-trained models? If the embeddings look similar, what does that say about the emergence of these capabilities as a result of RL? Does training via RL even matter? Do we care if it doesn't matter?</p></disp-quote><p>As mentioned in our response to your previous comment, we tested both a linear network you suggested and a nonlinear network which is more similar to the RL network. For both networks, we aligned the inputs with those given to the RL network. As demonstrated above, we found that the embeddings do separate to some extent based on the distances. Interestingly, despite the differences in prediction accuracy between the networks, the embeddings were quite similar. These findings suggest the usefulness of analyzing decision-making processes through behavioral cloning, as you have suggested, and show potential for application to the biological data, as already noted. However, as previously mentioned, models with an explicit structure like RL agents bring clarity to our study and are essential in substantiating our arguments. The explicitness of the RL model architecture helps us to dissect and articulate the mechanisms more precisely.</p><disp-quote content-type="editor-comment"><p>There is a large amount of work on multi-agent learning that this paper seemingly ignores, or fails to evaluate against. Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments has thousands of citations. However, I am willing to accept that there are limitations to what a single paper can cover.</p></disp-quote><p>We apologize for the oversight and appreciate your pointing out the omission of significant multi-agent learning literature. We understand the importance of situating our work within the broader research context and have now included relevant citations in our manuscript. We regret any impression of neglecting existing contributions. If there are any specific references you believe we should include, we would appreciate being informed.</p><disp-quote content-type="editor-comment"><p>More specific comments:</p><p>Line 46-47. I do not know what &quot;simple descriptions with the distances can reproduce similar behavior&quot; is trying to convey.</p></disp-quote><p>Thank you for your comment. We have removed the phrase you pointed out “simple descriptions with the distances can reproduce similar behavior” because we deemed it unnecessary. As mentioned in our response to Reviewer 2, the additional analysis was intended to support the insights gained from the t-SNE embedding analyses. To clarify the purpose of this additional analysis, we have relocated the relevant sentences to the paragraph in the Discussion section that deals with the relationship between agent decisions and internal representations (lines 200 to 211 in the revised manuscript). In addition, to improve clarity for the reader, further details have been consolidated in the Methods section and Supplementary Figures (lines 396 to 432, and Figure 3 supplements 5 to 7, in the revised manuscript, respectively).</p><disp-quote content-type="editor-comment"><p>Lines 50-51: &quot;Our approach of computational ecology bridges the gap between ecology, ethology, and neuroscience and may provide a comprehensive account of them.&quot; This is probably too strong of a claim.</p></disp-quote><p>Thank you for your critique regarding the claim made in our manuscript. In accordance with your feedback, we have tempered the statement. Instead of suggesting a bridging of gaps across disciplines, we just assert that (lines 52 to 53 in the revised manuscript):</p><p>“Our results support the recent suggestions that the underlying processes facilitating collaborative hunting can be relatively simple.”</p><p>This revised statement focuses on the contribution of our work to the existing literature by providing evidence that supports current hypotheses about the simplicity of mechanisms underlying collaborative behavior.</p><disp-quote content-type="editor-comment"><p>Figure 1: The architecture diagram is a little difficult to understand. The key has &quot;layer with ReLU&quot; but then I do not see any clear white boxes? I also do not see any clear units? I think that maybe this is happening inside of the &quot;prey,&quot; &quot;predator 1,&quot; etc boxes. However, this is all much too small. I think you should decide if you want this figure to be about the neural network architecture, or about the fact the environment is broken into 1 prey and N predators that share an observation.</p></disp-quote><p>Thank you for your feedback on Figure 1. In line with your suggestions, we have revised the figure to better highlight the environmental setup involving one prey and N predators, thereby aiming to enhance readability and comprehension for readers. Specifically, we have removed the legends “unit”, “layer with ReLU”, “forward connection”, and “aggregating module”. Furthermore, for readers interested in a visualization of the network, we have referenced the Supplementary Figure (Figure 1 supplement 1) that illustrates the network architecture in the “Training details” subsection of the Methods section (line 342 in the revised manuscript).</p><disp-quote content-type="editor-comment"><p>I think the actions are also not clear. There are probably too many lines in the figure.</p></disp-quote><p>Thank you for your comment regarding the indication of actions in Figure 1a. The agents in our study can perform a total of 13 actions: acceleration in 12 directions plus an option to do nothing. The illustration in the “Action” part of Figure 1a accurately depicts these with 12 arrows. Your observation about the excess of lines might also relate to the four lines each for “State”, “Reward”, and “Action”, reflecting the independent learning framework employed in our study. Consolidating these lines into a single one could potentially obscure the individual learning processes of the agents. Therefore, while acknowledging that the figure may appear somewhat cluttered, we have opted to keep the distinct lines as they are to maintain clarity and avoid misunderstanding. We hope this clarification addresses your concerns.</p><disp-quote content-type="editor-comment"><p>For Figure 1 (b), why not just plot the actual density? Actually, I see this is included in Figure 2. I think this is the more helpful Figure!</p></disp-quote><p>Thank you for your suggestion regarding Figure 1b. Indeed, we initially created heat maps to represent the data. However, we found that for conditions where episodes ended quickly, that is the fast and equal conditions, the heat maps were predominantly influenced by the initial positions, resulting in a concentration of distribution in the center of the play area. Therefore, we decided to first present trajectories for each condition to capture the general behavior of the agents and then focused on providing a heat map for the slow condition, where the episode duration was longer and less influenced by the initial positions. Following your valuable feedback, we have added heat maps for the fast and equal conditions as Supplementary Figures to accommodate readers interested in visualizing the density across all conditions (Figure 2 supplement 1 in the revised manuscript). We hope this addition will be helpful.</p><disp-quote content-type="editor-comment"><p>In Figure 2, what form of Hypothesis testing was used? Was this a KS test? You can't assume the distributions are Gaussian? The presence of a chi-squared statistic seems to indicate Gaussian assumptions. But the distribution is strongly non-Gaussian in this case. A little more clarity would be helpful.</p></disp-quote><p>We appreciate your detailed attention to the statistical analysis in our manuscript. Based on the context of variability you have described, we believe that your comments refer to Figure 4 rather than Figure 2. Regarding the sample size of 10 per condition, we acknowledge that the central limit theorem may not provide a strong justification for the assumption of normality. However, we would like to emphasize the robustness of ANOVA when dealing with small sample sizes and its ability to yield reliable results even when data distributions deviate from normality. The lack of formal testing for normality is indeed a limitation, as noted in Statistics subsection in the Methods section of our manuscript (lines 469 to 481 in the revised manuscript). Yet, the ANOVA test has been widely recognized for its robustness, especially in the context of balanced designs, which is the case in our experimental setup. Moreover, the Holm-Bonferroni method has been applied to adjust for multiple comparisons, reducing the risk of Type I errors. We believe that these considerations, along with the conservative nature of our statistical correction methods, provide a reasonable basis to uphold the validity of our findings. Our approach aligns with common practices in the field, where the practical constraints of sample collection and experimental design often necessitate a balance between statistical ideals and real-world applicability.</p><disp-quote content-type="editor-comment"><p>Line 132 mentions that the variance is higher over the action distribution when the predator is about to catch the prey? This is actually the exact opposite of my intuition. I think that the actions hardly matter when the prey is far away, so there is no obvious optimal action, and the choice would be closer to uniform. I'm not sure that this matters very much, but it's interesting.</p></disp-quote><p>Thank you for your comment regarding the variance in action values. We appreciate this opportunity to clarify the interpretation of the variance of action values in our study. A larger variance in action values indicates a situation where there is a significant distinction in the value of possible actions, with some actions being highly valued and others much less so. Conversely, a smaller variance suggests that there is little difference in the value of actions, with the distribution of action values being closer to uniform. This can be observed in Figure 3 supplement 4 in the revised manuscript, where the action values of predator 2 indeed approach a uniform distribution when the prey is distant. Therefore, it seems our findings are consistent with your intuition that when the prey is far away, the actions matter less. If there is any misunderstanding, we would be grateful for the opportunity to ensure our interpretations align with the observed behavior.</p><disp-quote content-type="editor-comment"><p>Line 325 – Usually IL is reserved for Imitation Learning. I have never seen it used for Independent Learning.</p></disp-quote><p>Thank you for bringing this to our attention. We have removed the abbreviation “IL” for Independent Learning to avoid any confusion.</p><disp-quote content-type="editor-comment"><p>Line 324 – I think biological organisms usually model the behavior of other organisms and account for it while planning.</p></disp-quote><p>Thank you for your insightful comment regarding the modeling of biological behaviors. Our initial intention was to illustrate that each policy network in our computational model operates independently, similar to individual neural processes in biological brains, without the shared network weights that are often used in multi-agent reinforcement learning environments. Nonetheless, we agree with your observation that the lack of explicit mechanisms for modeling and planning may not entirely reflect the intricacies of biological organisms. Consequently, we have revised the manuscript to remove the term “biologically plausible” from the Results and Methods sections to prevent any overstatement of our computational model's capabilities. The revised description is as follows (lines 331 to 332 in the revised manuscript):</p><p>“We here modeled an agent (predator/prey) with independent learning, one of the simplest approaches to multi-agent reinforcement learning.”</p><p>We believe this modification more accurately conveys our methodology and the scope of our study.</p><disp-quote content-type="editor-comment"><p>Line 212 – Q-values do implicitly model the competencies of other agents.</p></disp-quote><p>Thank you for your point out. As you and the other reviewers noted, we have recognized that deep Q-networks implicitly model the competencies of other agents. Therefore, we have revised our manuscript to refine our claims to specifically address the absence of aspects of “theory of mind”, as it is certain that the agents in our study do not model or infer the “mental states” of others. Although this revision may narrow or moderate our argument, we believe it significantly enhances the precision and accuracy of our discussion. We are grateful for the guidance that has helped us to improve the manuscript.</p><disp-quote content-type="editor-comment"><p>Line 196 – What does it mean to switch the decision rules with the distances?</p></disp-quote><p>Thank you for your comment. We have removed the phrase you pointed out “switch the decision rules with the distances” because we deemed it ambiguous as you suggested. As mentioned in our response to your previous comment, we have rearranged the relevant paragraph in the Discussion section to clarify the purpose of this additional analysis (lines 200 to 211 in the revised manuscript). We are grateful for the guidance that has helped us to improve the manuscript.</p><disp-quote content-type="editor-comment"><p>Overall, I think the problems considered by this paper are interesting. And I am happy you took the time to write it. This work made me think a lot about my own research. I appreciate your efforts here. Thank you.</p></disp-quote><p>Thank you for your positive feedback. We are delighted to hear that our paper has sparked further thought about your research. It is encouraging to know that the problems we have addressed are considered interesting within the research community. Your kind words are greatly appreciated.</p></body></sub-article></article>