<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.3 20210610//EN"  "JATS-archivearticle1-3-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.3"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">87126</article-id><article-id pub-id-type="doi">10.7554/eLife.87126</article-id><article-id pub-id-type="doi" specific-use="version">10.7554/eLife.87126.5</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Dorsolateral prefrontal activity supports a cognitive space organization of cognitive control</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-306862"><name><surname>Yang</surname><given-names>Guochun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-0516-8772</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-298028"><name><surname>Wu</surname><given-names>Haiyan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0001-8869-6636</contrib-id><xref ref-type="aff" rid="aff5">5</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-306921"><name><surname>Li</surname><given-names>Qi</given-names></name><xref ref-type="aff" rid="aff6">6</xref><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-306322"><name><surname>Liu</surname><given-names>Xun</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1366-8926</contrib-id><email>liux@psych.ac.cn</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-306922"><name><surname>Fu</surname><given-names>Zhongzheng</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-2572-6284</contrib-id><xref ref-type="aff" rid="aff7">7</xref><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-187498"><name><surname>Jiang</surname><given-names>Jiefeng</given-names></name><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="aff" rid="aff4">4</xref><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution>CAS Key Laboratory of Behavioral Science, Institute of Psychology</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05qbk4x57</institution-id><institution>Department of Psychology, University of Chinese Academy of Sciences</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/036jqmy94</institution-id><institution>Department of Psychological and Brain Sciences, University of Iowa</institution></institution-wrap><addr-line><named-content content-type="city">Iowa City</named-content></addr-line><country>United States</country></aff><aff id="aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/036jqmy94</institution-id><institution>Cognitive Control Collaborative, University of Iowa</institution></institution-wrap><addr-line><named-content content-type="city">Iowa City</named-content></addr-line><country>United States</country></aff><aff id="aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01r4q9n85</institution-id><institution>Centre for Cognitive and Brain Sciences and Department of Psychology, University of Macau</institution></institution-wrap><addr-line><named-content content-type="city">Macau</named-content></addr-line><country>China</country></aff><aff id="aff6"><label>6</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/005edt527</institution-id><institution>Beijing Key Laboratory of Learning and Cognition, School of Psychology, Capital Normal University</institution></institution-wrap><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff><aff id="aff7"><label>7</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05byvp690</institution-id><institution>Department of Neurological Surgery, Unversity of Texas Southwestern Medical Center</institution></institution-wrap><addr-line><named-content content-type="city">Dallas</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/05gq02987</institution-id><institution>Brown University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>06</day><month>03</month><year>2024</year></pub-date><volume>12</volume><elocation-id>RP87126</elocation-id><history><date date-type="sent-for-review" iso-8601-date="2023-02-21"><day>21</day><month>02</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint.</event-desc><date date-type="preprint" iso-8601-date="2023-02-14"><day>14</day><month>02</month><year>2023</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2023.02.13.528292"/></event><event><event-desc>This manuscript was published as a reviewed preprint.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-05-09"><day>09</day><month>05</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87126.1"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2023-09-12"><day>12</day><month>09</month><year>2023</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87126.2"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-10"><day>10</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87126.3"/></event><event><event-desc>The reviewed preprint was revised.</event-desc><date date-type="reviewed-preprint" iso-8601-date="2024-01-23"><day>23</day><month>01</month><year>2024</year></date><self-uri content-type="reviewed-preprint" xlink:href="https://doi.org/10.7554/eLife.87126.4"/></event></pub-history><permissions><copyright-statement>Â© 2023, Yang et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Yang et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-87126-v1.pdf"/><self-uri content-type="figures-pdf" xlink:href="elife-87126-figures-v1.pdf"/><abstract><p>Cognitive control resolves conflicts between task-relevant and -irrelevant information to enable goal-directed behavior. As conflicts can arise from different sources (e.g., sensory input, internal representations), how a limited set of cognitive control processes can effectively address diverse conflicts remains a major challenge. Based on the cognitive space theory, different conflicts can be parameterized and represented as distinct points in a (low-dimensional) cognitive space, which can then be resolved by a limited set of cognitive control processes working along the dimensions. It leads to a hypothesis that conflicts similar in their sources are also represented similarly in the cognitive space. We designed a task with five types of conflicts that could be conceptually parameterized. Both human performance and fMRI activity patterns in the right dorsolateral prefrontal cortex support that different types of conflicts are organized based on their similarity, thus suggesting cognitive space as a principle for representing conflicts.</p></abstract><abstract abstract-type="plain-language-summary"><title>eLife digest</title><p>You are reading a book in your local coffeeshop, when your focus gets broken by the couple at the next table, passionately discussing mortgage rates. To minimise this interruption your brain engages in âcognitive controlâ, resolving conflicts between competing stimuli to prioritise one over another.</p><p>Having finally regained your focus, another distraction emerges, this time of a different nature. Does your brain use the same mental mechanisms as before, and therefore a common brain circuit? Or does each kind of stimulus require a specific process?</p><p>Tasks that involve successively presenting different distractors can help explore these questions by testing for a process known as generalization: if the same mental mechanism underpins the resolution of all conflicts, distractors should become easier to ignore after the first trial.</p><p>Based on this paradigm, Yang et al. recorded brain activity during a modified version of a spatial Stroop-Simon task. Participants were asked to press a left or right button based on whether an arrow was pointing up or down, with both the vertical and horizontal position of the symbol potentially causing interference. For instance, accurate decision-making may be impaired when an arrow âdownâ the bottom of the screen is pointing up (Stroop effect); or when participants must press the left button for an arrow shown on their right (Simon effect). Overall, the arrows could appear in 10 possible locations, giving rise to five types of conflicts with a unique blend of Stroop and Simon effects, with different levels of similarity.</p><p>The results showed that the degree to which conflicts could generalize to each other depended on their similarity: the more similar the conflicts, the easier it was to resolve one after having faced another. This is contrary to previous views suggesting that different conflict types either entirely generalized or could not generalize at all.</p><p>In addition, the analyses revealed that the neural networks involved in resolving each conflict type were organised in a continuous manner within a region called the prefrontal cortex. This pattern resembles how spatial information is arranged in the brain, prompting Yang et al. to suggest that cognitive control also falls under a set of principles known as cognitive space representations.</p><p>Overall, the methodology employed in this work could prove useful to researchers from other fields who also investigate whether various stimuli are processed via the same or different neural networks.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>cognitive control</kwd><kwd>cognitive space</kwd><kwd>domain-general</kwd><kwd>domain-specific</kwd><kwd>conflict</kwd><kwd>representation</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>NSFC 62061136001</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Xun</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100001659</institution-id><institution>Deutsche Forschungsgemeinschaft</institution></institution-wrap></funding-source><award-id>DFG TRR-169</award-id><principal-award-recipient><name><surname>Liu</surname><given-names>Xun</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/501100002858</institution-id><institution>China Postdoctoral Science Foundation</institution></institution-wrap></funding-source><award-id>2019M650884</award-id><principal-award-recipient><name><surname>Yang</surname><given-names>Guochun</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>The dorsolateral prefrontal cortex represents different conflict types within an organized cognitive space of cognitive control, positioning more similar conflict types closer together.</meta-value></custom-meta><custom-meta specific-use="meta-only"><meta-name>publishing-route</meta-name><meta-value>prc</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>Cognitive control enables humans to behave purposefully by modulating neural processing to resolve conflicts between task-relevant and task-irrelevant information. For example, when naming the color of the word âBLUEâ printed in red ink, we are likely to be distracted by the word meaning, because reading a word is highly automatic in daily life. To keep our attention on the color, we need to mobilize the cognitive control processes to resolve the conflict between the color and word by boosting/suppressing the processing of color/word meaning. As task-relevant and task-irrelevant information can come from different sources, the sources of conflicts and how they should be resolved can vary greatly (<xref ref-type="bibr" rid="bib53">Kornblum et al., 1990</xref>). For example, the conflict may occur between items of sensory information, such as between a red light and a police officer signaling cars to pass. Alternatively, conflict may occur between sensory and motor information, such as when a voice on the left asks you to turn right. A key unsolved question in cognitive control is how our brain efficiently resolves these different types of conflicts.</p><p>A first step to addressing this question is to examine the commonalities and/or dissociations across different types of conflicts that can be categorized into different <italic>domains</italic>. Examples of the domains of conflicts include experimental paradigm (<xref ref-type="bibr" rid="bib31">Freitas et al., 2007</xref>; <xref ref-type="bibr" rid="bib63">Magen and Cohen, 2007</xref>), sensory modality (<xref ref-type="bibr" rid="bib42">Hazeltine et al., 2011</xref>; <xref ref-type="bibr" rid="bib94">Yang et al., 2017</xref>), or conflict type regarding the dimensional overlap of conflict processes (<xref ref-type="bibr" rid="bib47">Jiang and Egner, 2014</xref>; <xref ref-type="bibr" rid="bib58">Liu et al., 2004</xref>).</p><p>Two solutions to resolving different conflict types are proposed. They differ based on whether the same cognitive control mechanisms are applied across domains. On the one hand, the <italic>domain-general</italic> cognitive control theories posit that the frontoparietal cortex adaptively encodes task information and can thus flexibly implement control strategies for different types of conflicts. This is supported by the generalizable control adjustment (i.e., encountering a conflict trial from one type can facilitate conflict resolution of another type) (<xref ref-type="bibr" rid="bib31">Freitas et al., 2007</xref>; <xref ref-type="bibr" rid="bib50">Kan et al., 2013</xref>) and similar neural patterns (<xref ref-type="bibr" rid="bib72">Peterson et al., 2002</xref>; <xref ref-type="bibr" rid="bib93">Wu et al., 2020</xref>) across distinct conflict tasks. A broader domain-general view holds that the frontoparietal brain regions/networks are widely involved in multiple control demands well beyond the conflict domain (<xref ref-type="bibr" rid="bib4">Assem et al., 2020</xref>; <xref ref-type="bibr" rid="bib17">Cole et al., 2013</xref>), which explains the remarkable flexibility in human behaviors. However, since domain-general processes are by definition likely shared by different tasks, when we need to handle multiple task demands at the same time, the efficiency of both tasks would be impaired due to resource competition or interference (<xref ref-type="bibr" rid="bib68">Musslick and Cohen, 2021</xref>). Therefore, the domain-general processes are evolutionarily less advantageous for humans to deal with the diverse situations requiring high efficiency (<xref ref-type="bibr" rid="bib19">Cosmides and Tooby, 1994</xref>). On the other hand, the <italic>domain-specific</italic> theories argue that different types of conflicts are handled by distinct cognitive control processes (e.g., where and how information processing should be modulated) (<xref ref-type="bibr" rid="bib27">Egner, 2008</xref>; <xref ref-type="bibr" rid="bib52">Kim et al., 2012</xref>). However, according to the domain-specific view, the diverse conflict situations require a multitude of preexisting control processes, which is biologically implausible (<xref ref-type="bibr" rid="bib2">Abrahamse et al., 2016</xref>).</p><p>To reconcile the two theories, researchers recently proposed that cognitive control might be a mixture of domain-general and domain-specific processes. For instance, <xref ref-type="bibr" rid="bib32">Freitas and Clark, 2015</xref> found that trial-by-trial adjustment of control can generalize across two conflict domains to different degrees, leading to domain-general (strong generalization) or domain-specific (weak or no generalization) conclusions depending on the task settings of the consecutive conflicts. Similarly, different brain networks may show domain-generality (i.e., representing multiple conflicts) or domain-specificity (i.e., representing individual conflicts separately) (<xref ref-type="bibr" rid="bib47">Jiang and Egner, 2014</xref>; <xref ref-type="bibr" rid="bib57">Li et al., 2017</xref>). Even within the same brain area (e.g., medial frontal cortex), <xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref> found that the neural population activity can be factorized into orthogonal dimensions encoding both domain-general and domain-specific conflict information, which can be selectively read out by downstream brain regions. While the mixture view provides an explanation for the contradictory findings (<xref ref-type="bibr" rid="bib13">Braem et al., 2014</xref>), it suffers the same criticism as domain-specific cognitive control theories as it still requires many cognitive control processes to fully cover all possible conflicts.</p><p>A key to reconciling domain-general and domain-specific cognitive control is to organize the large number of conflict types using a system with limited, dissociable dimensions. A construct with a similar function is the <italic>cognitive space</italic> (<xref ref-type="bibr" rid="bib9">Bellmund et al., 2018</xref>), which extends the idea of cognitive map (<xref ref-type="bibr" rid="bib7">Behrens et al., 2018</xref>) to the representation of abstract information. Critically, the cognitive space view holds that the representations of different abstract information are organized continuously and the representational geometry in the cognitive space is determined by the similarity among the represented information (<xref ref-type="bibr" rid="bib9">Bellmund et al., 2018</xref>).</p><p>In the human brain, it has been shown that abstract (<xref ref-type="bibr" rid="bib7">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib81">Schuck et al., 2016</xref>) and social (<xref ref-type="bibr" rid="bib71">Park et al., 2020</xref>) information can be represented in a cognitive space. For example, social hierarchies with two independent scores (e.g., popularity and competence) can be represented in a 2D cognitive space (one dimension for each score), such that each social item can be located by its score in the two dimensions (<xref ref-type="bibr" rid="bib71">Park et al., 2020</xref>). In the field of cognitive control, recent studies have begun to conceptualize different control states within a cognitive space (<xref ref-type="bibr" rid="bib6">Badre et al., 2021</xref>). For example, <xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref> mapped different conflict conditions to locations in a low-/high-dimensional cognitive space to demonstrate the domain-general/domain-specific problems; <xref ref-type="bibr" rid="bib40">Grahek et al., 2022</xref> used a cognitive space model of cognitive control settings to explain behavioral changes in the speed-accuracy tradeoff. However, the cognitive spaces proposed in these studies were only applicable to a limited number of control states involved in their designs. Therefore, it remains unclear whether there is a cognitive space that can explain the large number of control states, similar to that of the spatial location (<xref ref-type="bibr" rid="bib9">Bellmund et al., 2018</xref>) and non-spatial knowledge (<xref ref-type="bibr" rid="bib7">Behrens et al., 2018</xref>). A challenge to answering this question lies in how to construct control states with continuous levels of similarity. Our recent work (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>) showed that it is possible to manipulate continuous conflict similarity by using a mixture of two independent conflict types with varying ratios, which can be used to further examine the behavioral and neural evidence for the cognitive space view. It is also unclear how the cognitive space of cognitive control is encoded in the brain, although that of spatial locations and non-spatial abstract knowledge has been relatively well investigated in the medial temporal lobe, medial prefrontal, and orbitofrontal system (<xref ref-type="bibr" rid="bib7">Behrens et al., 2018</xref>; <xref ref-type="bibr" rid="bib9">Bellmund et al., 2018</xref>). Recent research has suggested that the abstract task structure could be encoded and implemented by the frontoparietal network (<xref ref-type="bibr" rid="bib88">Vaidya and Badre, 2022</xref>; <xref ref-type="bibr" rid="bib87">Vaidya et al., 2021</xref>), but whether a similar neural system encodes the cognitive space of cognitive control remains untested.</p><p>The present study aimed to test the geometry of cognitive space in conflict representation. Specifically, we hypothesize that different types of conflicts are represented as points in a cognitive space. Importantly, the distance between the points, which reflects the geometry of the cognitive space, scales with the difference in the sources of the conflicts being represented by the points. The dimensions in the cognitive space of conflicts can be the aforementioned <italic>domains</italic>, in which domain-specific cognitive control processes are defined. For a specific type of conflict, its location in the cognitive space can be parameterized using a limited number of coordinates, which reflect how much control is needed for each of the domain-specific cognitive control processes. The cognitive space can also represent different types of conflicts with low dimensionality (<xref ref-type="bibr" rid="bib6">Badre et al., 2021</xref>; <xref ref-type="bibr" rid="bib61">MacDowell et al., 2022</xref>). Different domains can be represented conjunctively in a single cognitive space to achieve domain-general cognitive control as conflicts from different sources can be resolved using the same set of cognitive control processes. We further hypothesize that the cognitive space representing different types of conflicts may be located in the frontoparietal network due to its essential roles in conflict resolution (<xref ref-type="bibr" rid="bib33">Freund et al., 2021a</xref>; <xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref>) and abstract task representation (<xref ref-type="bibr" rid="bib88">Vaidya and Badre, 2022</xref>).</p><p>In this study, we adjusted the paradigm from our previous study (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>) by including transitions of trials from five different conflict types, which enabled us to test whether these conflict types are organized in a cognitive space (<xref ref-type="fig" rid="fig1">Figure 1A</xref>). Specifically, on each trial, an arrow, pointing either upward or downward, was presented on one of the 10 possible locations on the screen. Participants were required to respond to the pointing direction of the arrow (up or down) by pressing either the left or right key. Importantly, conflicts from two sources can occur in this task. On one hand, the vertical location of the arrow can be incongruent with the direction (e.g., an up-pointing arrow on the lower half of the screen), resulting spatial Stroop conflict (<xref ref-type="bibr" rid="bib58">Liu et al., 2004</xref>; <xref ref-type="bibr" rid="bib60">Lu and Proctor, 1995</xref>). On the other hand, the horizontal location of the arrow can be incongruent with the response key (e.g., an arrow requiring left response presented on the right side of the screen), thus causing Simon conflict (<xref ref-type="bibr" rid="bib60">Lu and Proctor, 1995</xref>; <xref ref-type="bibr" rid="bib83">Simon and Small, 1969</xref>). As the arrow location rotates from the horizontal axis to the vertical axis, spatial Stroop conflict increases, and Simon conflict decreases. Therefore, the 10 possible locations of the arrow give rise to five conflict types with unique blend of spatial Stroop and Simon conflicts (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>). As the increase in spatial Stroop conflict is highly correlated with the decrease in Simon conflict, we can use a 1D cognitive space to represent all five conflict types.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Experimental design.</title><p>(<bold>A</bold>) The left panel shows the orthogonal stimulusâresponse mappings of the two participant groups. In each group, the stimuli were only displayed at two quadrants of the circular locations. One group was asked to respond with the left button to the upward arrow and with the right button to the downward arrow presented in the top-left and bottom-right quadrants, and the other group vice versa. The right panel shows the time course of one example trial. The stimuli were displayed for 600 ms, preceded and followed by fixation crosses that lasted for 1400 ms in total. (<bold>B</bold>) Examples of the five types of conflicts, each containing congruent and incongruent conditions. The arrows were presented at locations along five orientations with isometric polar angles, in which the vertical location introduces the spatial Stroop conflict, and the horizontal location introduces the Simon conflict. Dashed lines are shown only to indicate the location of arrows and were not shown in the experiments. (<bold>C</bold>) The definition of the angular difference between two conflict types and the conflict similarity. The angle Î¸ is determined by the acute angle between two lines that cross the stimuli and the central fixation. Therefore, stimuli of the same conflict type form the smallest angle of 0, and stimuli between Stroop and Simon form the largest angle of 90Â°, and others are in between. Conflict similarity is defined by the cosine value of Î¸. H = high; L = low; M = medium.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig1-v1.tif"/></fig><p>One way to parameterize (i.e., defining a coordinate system) the cognitive space is to encode each conflict type by the angle of the axis connecting its two possible stimulus locations (<xref ref-type="fig" rid="fig1">Figure 1B</xref>). Within this cognitive space, the similarity between two conflict types can be quantified as the cosine value of their angular difference (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). The rationale behind defining conflict similarity based on combinations of different conflict sources, such as spatial-Stroop and Simon, stems from the evidence that these sources undergo independent processing (<xref ref-type="bibr" rid="bib27">Egner, 2008</xref>; <xref ref-type="bibr" rid="bib56">Li et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Liu et al., 2010</xref>; <xref ref-type="bibr" rid="bib92">Wang et al., 2014</xref>). Identifying these distinct sources is critical in efficiently resolving diverse conflicts. If the conflict types are organized as a cognitive space in the brain, the similarity between conflict types in the cognitive space should be reflected in both the behavior and similarity in the neural representations of conflict types. Our data from two experiments using this experimental design support both predictions: using behavioral data, we found that the influence of congruency (i.e., whether the task-relevant and task-irrelevant information indicate the same response) from the previous trial to the next trial increases with the conflict similarity between the two trials. Using fMRI data, we found that more similar conflicts showed higher multivariate pattern similarity in the right dorsolateral prefrontal cortex (dlPFC).</p></sec><sec id="s2" sec-type="results"><title>Results</title><sec id="s2-1"><title>Behavioral congruency effects</title><p>We first tested the congruency effects for the five conflict types by conducting 5 (conflict type) Ã 2 (congruency) repeated-measure ANOVAs with reaction time (RT) and error rate (ER) from both experiments. The results are displayed in <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>.</p><sec id="s2-1-1"><title>Experiment 1</title><p>For the RT, we observed a significant main effect of congruency, <italic>F</italic>(1, 32) = 407.70, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.93, a significant main effect of conflict type, <italic>F</italic>(4, 128) = 6.32, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.16, and an interaction between conflict type and congruency, <italic>F</italic>(4, 128) = 27.86, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.47. Simple effect analyses showed that participants responded more slowly in incongruent conditions than in congruent conditions for all conflict types, p<sub>FDR</sub>s&lt;0.001. Additionally, the congruency effects the St<sub>H</sub>Sm<sub>L</sub>, St<sub>M</sub>Sm<sub>M</sub>, and St<sub>L</sub>Sm<sub>H</sub> were significantly larger than that of spatial Stroop, and the congruency effects of St<sub>H</sub>Sm<sub>L</sub> and St<sub>M</sub>Sm<sub>M</sub> were significantly larger than that of Simon, p<sub>FDR</sub>s&lt;0.05.</p><p>Similar results were found with the ER. We observed a significant main effect of congruency, <italic>F</italic>(1, 32) = 56.83, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.64, a significant main effect of conflict type, <italic>F</italic>(4, 128) = 6.29, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.16, and an interaction between conflict type and congruency, <italic>F</italic>(4, 128) = 13.23, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.29. Simple effect analyses showed that participants were more error-prone in incongruent conditions than in congruent conditions for all conflict types, p<sub>FDR</sub><italic>s</italic>&lt;0.001. The congruency effects of St<sub>H</sub>Sm<sub>L</sub>, St<sub>M</sub>Sm<sub>M</sub>, and St<sub>L</sub>Sm<sub>H</sub> were significantly larger than that of spatial Stroop, and the congruency effects of St<sub>M</sub>Sm<sub>M</sub> and St<sub>L</sub>Sm<sub>H</sub> were significantly larger than that of Simon, p<sub>FDR</sub>s&lt;0.05.</p></sec><sec id="s2-1-2"><title>Experiment 2</title><p>For the RT, we observed a significant main effect of congruency, <italic>F</italic>(1, 34) = 149.71, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.81, a significant main effect of conflict type, <italic>F</italic>(4, 136) = 10.11, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.23, and an interaction between conflict type and congruency, <italic>F</italic>(4, 136) = 7.63, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.18. Simple effect analyses showed that participants responded more slowly in incongruent conditions than in congruent conditions for all conflict types, p<sub>FDR</sub>s&lt;0.001. The congruency effect of the St<sub>L</sub>Sm<sub>H</sub> condition was larger than that of spatial Stroop, and St<sub>M</sub>Sm<sub>M</sub> and St<sub>L</sub>Sm<sub>H</sub> were significantly larger than that of Simon, p<sub>FDR</sub>s&lt;0.05.</p><p>For the ER, we only observed a significant main effect of congruency, <italic>F</italic>(1, 34) = 29.80, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.47. All the types showed a larger ER in incongruent than congruent conditions (p<sub>FDR</sub>s&lt;0.001), except that type 1 only showed a marginal significance (p<sub>FDR</sub>=0.062).</p><p>In sum, we observed strong behavioral congruency effects in both experiments. The findings indicate that these conflict conditions indeed engaged cognitive control (<xref ref-type="bibr" rid="bib34">Freund et al., 2021b</xref>).</p></sec></sec><sec id="s2-2"><title>Conflict-type similarity modulates behavioral congruency sequence effect (CSE)</title><sec id="s2-2-1"><title>Experiment 1</title><p>To test the influence of similarity between conflict types on behavior, we examined the CSE in consecutive trials. Specifically, the CSE was quantified as the interaction between previous and current trial congruency and can reflect how (in)congruency on the previous trial influences cognitive control on the current trial (<xref ref-type="bibr" rid="bib25">Egner, 2007</xref>; <xref ref-type="bibr" rid="bib80">Schmidt and Weissman, 2014</xref>). It has been shown that the CSE diminishes if the two consecutive trials have different conflict types (<xref ref-type="bibr" rid="bib3">AkÃ§ay and Hazeltine, 2011</xref>; <xref ref-type="bibr" rid="bib26">Egner et al., 2007</xref>; <xref ref-type="bibr" rid="bib84">Torres-Quesada et al., 2013</xref>). Similarly, we tested whether the size of CSE increases as a function of conflict similarity between consecutive trials. To this end, we organized trials based on a 5 (previous trial conflict type) Ã 5 (current trial conflict type) Ã 2 (previous trial congruency) Ã 2 (current trial congruency) factorial design, with the first two and the last two factors capturing between-trial conflict similarity and the CSE, respectively. The cells in the 5 Ã 5 matrix were mapped to different similarity levels according to the angular difference between the two conflict types (<xref ref-type="fig" rid="fig1">Figure 1C</xref>). As shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>, the CSE, measured in both RT and ER, scaled with conflict similarity.</p><fig-group><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>The conflict similarity modulation on the behavioral congruency sequence effect (CSE) in experiment 1.</title><p>(<bold>A</bold>) RT and (<bold>B</bold>) ER are plotted as a function of congruency types on trial nâ1 and trial n. Each column shows one similarity level, as indicated by the defined angular difference between two conflict types. Error bars are standard errors. C = congruent; I = incongruent; RT = reaction time; ER = error rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig2-v1.tif"/></fig><fig id="fig2s1" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 1.</label><caption><title>The congruency effects of experiments 1 (<bold>A, B</bold>) and 2 (<bold>C, D</bold>).</title><p>Error bars denote the standard errors of mean. Small insets on top of panel (<bold>A</bold>) denote an example of stimuli positions for each conflict type. RT = reaction time; ER = error rate.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig2-figsupp1-v1.tif"/></fig><fig id="fig2s2" position="float" specific-use="child-fig"><label>Figure 2âfigure supplement 2.</label><caption><title>The conflict similarity modulation on performance of experiments 1 (<bold>A</bold>, <bold>B</bold>, <bold>D, E</bold>) and 2 (<bold>C, F</bold>), respectively.</title><p>(<bold>A</bold>) and (<bold>D</bold>) are scatter plots of CSE [i.e., (CIâCC) â (IIâIC)] for RT and ER as a function of the cosine similarity, respectively. In (<bold>B, C, E</bold>, <bold>F</bold>), the cosine similarity and RT/ER are normalized across conflict similarity levels within each of the four CSE conditions (i.e., CC, II, CI, and IC). Conflict similarity for CC and II conditions is reversed (multiplied by â1), such that for all the four CSE conditions, higher conflict similarity is expected to be associated with worse performance (see âBehavioral analysisâ). Each dot represents a subject. The thin colored lines in (<bold>B, C, E</bold>, <bold>F</bold>) are the fitted lines for each of the four CSE conditions, and the thick black lines are the fitted lines collapsing across all CSE conditions. For panels (<bold>C</bold>) and (<bold>F</bold>), some similarity levels are missing because of the limited trial numbers in the experimental design in experiment 2. CSE = congruency sequence effect; RT = reaction time; ER = error rate; CI = congruent (trial nâ1)-incongruent (trial n); IC = incongruentâcongruent; CC = congruentâcongruent; II = incongruentâincongruent.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig2-figsupp2-v1.tif"/></fig></fig-group><p>To test the modulation of conflict similarity on the CSE, we constructed a linear mixed-effect model to predict RT/ER in each cell of the factorial design using a predictor encoding the interaction between the CSE and conflict similarity (see âMaterials and methodsâ). The results showed a significant effect of conflict similarity [RT: <italic>Î² =</italic> 0.10 Â± 0.01, <italic>t</italic>(1719.7) = 15.82, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.120; ER: <italic>Î² =</italic> 0.15 Â± 0.02, <italic>t</italic>(204.5) = 7.84, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.085, <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2B/E</xref>]. In other words, the CSE increased with the conflict similarity between two consecutive trials. The conflict similarity modulation effect remained significant after regressing out the influence of physical proximity between the stimuli of consecutive trials [RT: <italic>Î²</italic> = 0.09 Â± 0.01, <italic>t</italic>(1902.4) = 13.74, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.025; ER: <italic>Î²</italic> = 0.09 Â± 0.01, <italic>t</italic>(249.3) = 7.66, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.018]. As a control analysis, we also compared this approach to a two-stage analysis that first calculated the CSE for each previous Ã current trial conflict type condition and then tested the modulation of conflict similarity on the CSEs (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>). The two-stage analysis also showed a strong effect of conflict similarity [RT: <italic>Î² =</italic> 0.58 Â± 0.04, <italic>t</italic>(67.5) = 14.74, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.388; ER: <italic>Î² =</italic> 0.36 Â± 0.05, <italic>t</italic>(40.3) = 7.01, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.320, <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2A/D</xref>]. Importantly, individual modulation effects of conflict similarity were positively correlated between the two approaches (RT: <italic>r =</italic> 0.48; ER: <italic>r =</italic> 0.86, both ps&lt;0.003, one-tailed), indicating the consistency of the estimated conflict similarity effects across the two approaches. In the following texts, we will use the terms <italic>conflict similarity effect</italic> and <italic>conflict-type effect</italic> interchangeably.</p><p>Moreover, to test the continuity and generalizability of the similarity modulation, we conducted a leave-one-out prediction analysis. We used the behavioral data from experiment 1 for this test due to its larger amount of data than experiment 2. Specifically, we removed data from one of the five similarity levels (as illustrated by the <italic>Î¸</italic>s in <xref ref-type="fig" rid="fig1">Figure 1C</xref>) and used the remaining data to perform the same mixed-effect model (i.e., the two-stage analysis). This yielded one pair of Î²-coefficients including the similarity regressor and the intercept for each subject, with which we predicted the CSE for the removed similarity level for each subject. We repeated this process for each similarity level once. The predicted results were highly correlated with the original data, with <italic>r</italic> = 0.87 for the RT and <italic>r</italic> = 0.84 for the ER, ps&lt;0.001.</p><p>In addition, we tested whether the conflict similarity modulation on the CSE is susceptible to training. We collected the data of experiment 1 across three sessions, thus it is possible to examine whether the conflict similarity modulation effect changes across time. To this end, we added conflict similarity, session, and their interaction into a mixed-effect linear model, in which the session was set as a categorical variable. With a post hoc ANOVA, we calculated the statistical significance of the interaction term. This approach was applied to both the RT and ER. Results showed no interaction effect in either RT, <italic>F</italic>(2,76.4) = 1.025, p=0.364, or ER, <italic>F</italic>(2,49.4) = 0.789, p=0.460. This result suggests that the modulation effect does not change across time.</p></sec><sec id="s2-2-2"><title>Experiment 2</title><sec id="s2-2-2-1"><title>Behavioral results</title><p>We next conducted an fMRI experiment using a shorter version of the same task with a different sample (n = 35, 17 females) to seek neural evidence of how different conflict types are organized. Using behavioral data, we tested the modulation of conflict similarity on the behavioral CSE using the linear mixed-effect model as in experiment 1 (except the two-stage method). Results showed a significant effect of conflict similarity modulation (RT: <italic>Î² =</italic> 0.24 Â± 0.04, <italic>t</italic>(71.7) = 6.36, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = .096; ER: <italic>Î² =</italic> 0.33 Â± 0.06, <italic>t</italic>(175.4) = 5.81, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.124, <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2C/F</xref>), thus replicating the results of experimental 1 and setting the stage for fMRI analysis. As in experiment 1, the conflict similarity modulation effect remained significant after regressing out the influence of physical proximity between the stimuli of consecutive trials (RT: <italic>Î²</italic> = 0.21 Â± 0.02, <italic>t</italic>(61.0) = 4.71, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.056; ER: <italic>Î²</italic> = 0.20 Â± 0.03, <italic>t</italic>(65.0) = 4.16, p&lt;0.001, <italic>Î·<sub>p</sub></italic><sup>2</sup> = 0.236).</p></sec></sec></sec><sec id="s2-3"><title>Univariate brain activations scale with conflict strength</title><p>In the fMRI analysis, we first replicated the classic congruency effect by searching for brain regions showing higher univariate activation in incongruent than congruent conditions (GLM1, see âMaterials and methodsâ). Consistent with the literature (<xref ref-type="bibr" rid="bib12">Botvinick et al., 2004</xref>; <xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref>), this effect was observed in the pre-supplementary motor area (pre-SMA) (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). We then tested the encoding of conflict type as a cognitive space by identifying brain regions with activation levels parametrically covarying with the coordinates (i.e., axial angle relative to the horizontal/vertical axes) in the hypothesized cognitive space. As shown in <xref ref-type="fig" rid="fig1">Figure 1B</xref>, change in the angle corresponds to change in spatial Stroop and Simon conflicts in opposite directions, so we used opposite contrasts to examine the encoding of spatial Stroop and Simon strength, respectively (see âMaterials and methodsâ). Accordingly, we found the right inferior parietal sulcus (IPS) and the right dorsomedial prefrontal cortex (dmPFC) displayed positive correlation between fMRI activation and the Simon conflict (<xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>, <xref ref-type="table" rid="table1">Table 1</xref>). We did not observe regions showing significant correlation with the spatial Stroop conflict.</p><fig-group><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>The congruency effect and parametric modulation effect detected by univoxel analyses.</title><p>Results displayed are probabilistic threshold-free cluster enhancement (TFCE) enhanced and thresholded with voxel-wise p&lt;0.001 and cluster-wise p&lt;0.05, both one-tailed. The congruency effect denotes the higher activation in incongruent than congruent condition (left panel). The positive parametric modulation effect (I_pm â C_pm) denotes the higher activation when the conflict type contained a higher ratio of Simon conflict component (right panel). I = incongruent; C = congruent; pm = parametric modulator.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig3-v1.tif"/></fig><fig id="fig3s1" position="float" specific-use="child-fig"><label>Figure 3âfigure supplement 1.</label><caption><title>Neural congruency effect (IâC) by GLM2 (see the âEstimation of fMRI activity with univariate general linear modelâ), plotted as a function of conflict type in different cortical regions of interest (ROIs).</title><p>The ROIs were selected because they show a statistically significant congruency effects or parametric modulation effects when analyzed using the univariate GLM1.The pre-SMA showed overall congruency effects regardless of the conflict type (upper panel); the right IPS and right dmPFC were positively modulated by the conflict type (lower panel).</p><p>Pre-SMA = pre-supplementary motor area; IPS = inferior parietal sulcus; dmPFC = dorsomedial prefrontal cortex.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig3-figsupp1-v1.tif"/></fig></fig-group><table-wrap id="table1" position="float"><label>Table 1.</label><caption><title>Brain activations for the univoxel parametric analysis in GLM1 (family-wise error [FWE] corrected after probabilistic threshold-free cluster enhancement [TFCE] enhancement, with voxel-wise p&lt;0.001 and cluster-wise p&lt;0.05, both one-tailed).</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Region</th><th align="left" valign="bottom" rowspan="2">L/R</th><th align="left" valign="bottom" colspan="3">MNI coordinate (mm)</th><th align="left" valign="bottom" rowspan="2">Volume (no. of voxels)</th><th align="left" valign="bottom" rowspan="2">MaxZ (TFCE enhanced)</th><th align="left" valign="bottom" rowspan="2">BA</th></tr><tr><th align="left" valign="bottom">x</th><th align="left" valign="bottom">y</th><th align="left" valign="bottom">z</th></tr></thead><tbody><tr><td align="left" valign="bottom" colspan="8"><italic>Incongruent &gt; congruent</italic></td></tr><tr><td align="left" valign="bottom">Pre-supplementary motor area</td><td align="left" valign="bottom">R</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">12</td><td align="char" char="." valign="bottom">73</td><td align="char" char="." valign="bottom">71</td><td align="char" char="." valign="bottom">4.18</td><td align="char" char="." valign="bottom">6</td></tr><tr><td align="left" valign="bottom" colspan="8"><italic>Positive parametric modulator (linear Simon effect)</italic></td></tr><tr><td align="left" valign="bottom">Inferior parietal sulcus</td><td align="left" valign="bottom">R</td><td align="char" char="." valign="bottom">52</td><td align="char" char="." valign="bottom">â64</td><td align="char" char="." valign="bottom">33</td><td align="char" char="." valign="bottom">81</td><td align="char" char="." valign="bottom">4.53</td><td align="char" char="." valign="bottom">39</td></tr><tr><td align="left" valign="bottom">Dorsomedial prefrontal cortex</td><td align="left" valign="bottom">R</td><td align="char" char="." valign="bottom">15</td><td align="char" char="." valign="bottom">57</td><td align="char" char="." valign="bottom">42</td><td align="char" char="." valign="bottom">43</td><td align="char" char="." valign="bottom">3.92</td><td align="char" char="." valign="bottom">9</td></tr></tbody></table><table-wrap-foot><fn><p>L = left; R = right; BA = Brodmann area.</p></fn></table-wrap-foot></table-wrap><p>To further test whether the univariate results explain the conflict similarity modulation of the behavioral CSE (slope in <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2C</xref>), we conducted brainâbehavioral correlation analyses for regions identified above. Regions with higher spatial Stroop/Simon modulation effects were expected to trigger higher behavioral conflict similarity modulation effect on the CSE. However, none of the two regions (i.e., right IPS and right dmPFC, <xref ref-type="fig" rid="fig3">Figure 3</xref>) were positively correlated with the behavioral performance, both uncorrected ps&gt;0.28, one-tailed. In addition, since the conflict-type difference covaries with the orientation of the arrow location at the individual level (e.g., the stimulus in a higher level of Simon conflict is always closer to the horizontal axis, see <xref ref-type="fig" rid="fig4">Figure 4A</xref>), the univariate modulation effects may not reflect purely conflict-type difference. To further tease these factors apart, we used multivariate analyses.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Rationale of the cross-subject representational similarity analysis (RSA) model and the schematic of key representational similarity matrix (RSM).</title><p>(<bold>A</bold>) The RSM is calculated as the Pearsonâs correlation between each pair of conditions across the 35 subjects. For 17 subjects, the stimuli were displayed on the top-left and bottom-right quadrants, and they were asked to respond with left hand to the upward arrow and right hand to the downward arrow. For the other 18 subjects, the stimuli were displayed on the top-right and bottom-left quadrants, and they were asked to respond with left hand to the downward arrow and right hand to the upward arrow. Within each subject, the conflict type and orientation regressors were perfectly covaried. For instance, the same conflict type will always be on the same orientation. To de-correlate conflict type and orientation effects, we conducted the RSA across subjects from different groups. For example, the bottom-right panel highlights the example conditions that are orthogonal to each other on the orientation, response, and Simon distractor, whereas their conflict type, target, and spatial Stroop distractor are the same. The dashed boxes show the possible target locations for different conditions. (<bold>B</bold>) and (<bold>C</bold>) show the orthogonality between conflict similarity and orientation RSMs. The within-subject RSMs (e.g., group 1âgroup 1) for conflict similarity and orientation are all the same, but the cross-group correlations (e.g., group 2âgroup 1) are different. Therefore, we can separate the contribution of these two effects when including them as different regressors in the same linear regression model. (<bold>D</bold>) and (<bold>E</bold>) show the two alternative models. Like the cosine model (<bold>B</bold>), within-group trial pairs resemble between-group trial pairs in these two models. The domain-specific model is an identity matrix. The domain-general model is estimated from the absolute difference of behavioral congruency effect, but scaled to 0 (lowest similarity) â 1 (highest similarity) to aid comparison. The plotted matrices in (<bold>BâE</bold>) include only one subject each from groups 1 and 2. Numbers 1â5 indicate the conflict-type conditions of spatial Stroop, St<sub>H</sub>Sm<sub>L</sub>, St<sub>M</sub>Sm<sub>M</sub>, St<sub>L</sub>Sm<sub>H</sub>, and Simon, respectively. The thin lines separate four different subconditions, that is, target arrow (up, down) Ã congruency (incongruent, congruent), within each conflict type.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig4-v1.tif"/></fig></sec><sec id="s2-4"><title>Cross-subject representational similarity analysis (RSA) and validation</title><p>The hypothesis that the brain encodes conflict types in a cognitive space predicts that similar conflict types will have similar neural representations. To test this prediction, we computed the representational similarity matrix (RSM) that encoded correlations of blood-oxygen-level dependent (BOLD) signal patterns between each pair of conflict type (Stroop, St<sub>H</sub>Sm<sub>L</sub>, St<sub>M</sub>Sm<sub>M</sub>, St<sub>L</sub>Sm<sub>H</sub>, and Simon, with H, M, and L indicating high, medium, and low, respectively, see also <xref ref-type="fig" rid="fig1">Figure 1B</xref>) Ã congruency (congruent, incongruent) Ã arrow direction (up, down) Ã run Ã subject combinations for each of the 360 cortical regions from the multi-modal parcellation (MMP) cortical atlas (<xref ref-type="bibr" rid="bib38">Glasser et al., 2016</xref>; <xref ref-type="bibr" rid="bib49">Jiang et al., 2020</xref>). The RSM was then submitted to a linear mixed-effect model as the dependent variable to test whether the representational similarity in each region was modulated by various experimental variables (e.g., conflict type, spatial orientation, stimulus, response, etc., see âMaterials and methodsâ). The linear mixed-effect model was used to de-correlate conflict type and spatial orientation leveraging the between-subject manipulation of stimulus locations (<xref ref-type="fig" rid="fig4">Figure 4A</xref>).</p><p>To validate this method, we applied this analysis to test the effects of response/stimulus features and found that representational similarity of the BOLD signal patterns significantly covaried with whether two response/spatial location/arrow directions were the same most strongly in bilateral motor/visual/somatosensory areas, respectively (<xref ref-type="fig" rid="fig5">Figure 5</xref>).</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>The cortical regions showing different effects in the main representational similarity analysis (RSA).</title><p>(<bold>A</bold>) The target effect reflects the encoding of upward and downward arrow directions and is mainly encoded in the visual and sensorimotor regions. (<bold>B</bold>) The response effect reflects the encoding of left and right responses and is mainly encoded in motor regions. (<bold>C</bold>) The spatial Stroop distractor effect reflects the encoding of vertical location of the stimulus and is encoded in bilateral visual regions. (<bold>D</bold>) The Simon distractor effect reflects the encoding of horizontal locations of the stimulus and is mainly encoded at the bilateral visual regions. Regions in (<bold>BâD</bold>) are thresholded with Bonferroni-corrected p&lt;0.05 across the 360 cortical ROIs, whereas regions in (<bold>A</bold>) are thresholded with uncorrected p&lt;0.005. (<bold>E</bold>) The representational strength of target, Stroop distractor, and Simon distractor in left V4 (left panel) for incongruent and congruent conditions. Compared to the congruent conditions, the incongruent condition shows a stronger representation of target, but lower representation of Stroop and Simon distractors. Results are Bonferroni corrected. *p&lt;0.05, ***p&lt;0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig5-v1.tif"/></fig><p>We further validated the cross-subject RSA by testing the hypothesis that exerting cognitive control can enhance target representation and suppress distractor representation. Our underlying assumption was that stimuli are represented in visual areas, so we chose a visual region from the main RSA results showing joint representation of target, spatial Stroop distractor, and Simon distractor (p&lt;0.005, one-tail, uncorrected). Only the left V4 met this criterion. We then tested representations with models for incongruent-only trials, congruent-only trials, and the incongruentâcongruent contrast (see âMaterials and methodsâ). The contrast model additionally used interaction between the congruency and target, Stroop distractor and Simon distractor terms. Results showed that in the incongruent condition, when we employ more cognitive control, the target representation was enhanced [<italic>t</italic>(72.2) = 2.28, p=0.039, Bonferroni corrected] and both spatial Stroop [<italic>t</italic>(85.4) = â3.94, p&lt;0.001, Bonferroni corrected] and Simon [<italic>t</italic>(38.9) = â2.80, p=0.012, Bonferroni corrected] distractor representations were suppressed (<xref ref-type="fig" rid="fig5">Figure 5E</xref>). These are consistent with the idea that the top-down control modulates the stimuli in both directions (<xref ref-type="bibr" rid="bib73">Polk et al., 2008</xref>; <xref ref-type="bibr" rid="bib76">Ritz and Shenhav, 2022</xref>), underscoring the utility of cross-subject RSA in our study.</p></sec><sec id="s2-5"><title>Multivariate patterns of the right dlPFC encode the conflict similarity</title><p>We then identified the cortical regions encoding conflict type as a cognitive space by testing whether their RSMs can be explained by the similarity between conflict types. Specifically, we applied three independent criteria: (1) the cortical regions should exhibit a statistically significant positive conflict similarity effect on the RSM; (2) the conflict similarity effect should be stronger in incongruent than congruent trials to reflect flexible adjustment of cognitive control demand when the conflict is present; and (3) the conflict similarity effect should be positively correlated with the behavioral conflict similarity modulation effect on the CSE (see âBehavioral results<italic>â</italic> of experiment 2). The first criterion revealed several cortical regions encoding the conflict similarity, including the frontal eye field (FEF), region 1, Brodmann 8C area (a subregion of dlPFC) (<xref ref-type="bibr" rid="bib38">Glasser et al., 2016</xref>), a47r, posterior inferior frontal junction (IFJp), anterior intraparietal area (AIP), temporoparietooccipital junction 3 (TPOJ3), PGi, and V3CD in the right hemisphere, and the superior frontal language (SFL) area, 23c, 24dd, 7Am, p32pr, 6r, FOP1, PF, ventromedial visual area (VMV1/2) areas, area 25, MBelt in the left hemisphere (Bonferroni corrected ps&lt;0.05, one-tailed, <xref ref-type="fig" rid="fig6">Figure 6A</xref>). We next tested whether these regions were related to cognitive control by comparing the strength of conflict similarity effect between incongruent and congruent conditions (criterion 2) and correlating the strength to behavioral similarity modulation effect (criterion 3). Given these two criteria pertain to second-order analyses (interaction or individual analyses) and thus might have lower statistical power (<xref ref-type="bibr" rid="bib11">Blake and Gangestad, 2020</xref>), we applied a more lenient threshold using false discovery rate (FDR) correction (<xref ref-type="bibr" rid="bib10">Benjamini and Hochberg, 1995</xref>) on the abovementioned regions. Results revealed that the left SFL, left VMV1, area left 25 and right 8C met this criterion, FDR-corrected ps&lt;0.05, one-tailed, suggesting that the representation of conflict type was strengthened when the conflict was present (e.g., <xref ref-type="fig" rid="fig6">Figure 6D</xref>, <xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref>). The inter-subject brainâbehavioral correlation analysis (criterion 3) showed that the strength of conflict similarity effect on RSM scaled with the modulation of conflict similarity on the CSE (slope in <xref ref-type="fig" rid="fig2s2">Figure 2âfigure supplement 2C</xref>) in right 8C (<italic>r</italic> = 0.52, FDR-corrected p=0.015, one-tailed, <xref ref-type="fig" rid="fig6">Figure 6C</xref>) only. These results are listed in <xref ref-type="table" rid="table2">Table 2</xref>.</p><fig-group><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>The conflict-type effect.</title><p>(<bold>A</bold>) Brain regions surviving the Bonferroni correction (p&lt;0.05) across the 360 regions (criterion 1). Labeled is the region meeting the three criteria. (<bold>B</bold>) Different encoding of conflict-type effect in the incongruent with congruent conditions (criterion 2). *False discovery rate (FDR)-corrected p&lt;0.05. (<bold>C</bold>) The brainâbehavior correlation of the right 8C (criterion 3). The x-axis shows the Î²-coefficient of the conflict-type effect from the representational similarity analysis (RSA), and the y-axis shows the Î²-coefficient obtained from the behavioral linear model using the conflict similarity to predict the congruency sequence effect (CSE) in experiment 2. (<bold>D</bold>) Illustration of the different encoding strength of conflict-type similarity in incongruent versus congruent conditions of right 8C. The y-axis is derived from the z-scored Pearson correlation coefficient after regressing out other factors. See <xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1B</xref>- for a plot of similarity matrix across different conflict conditions in both incongruent and congruent conditions. l = left; r = right.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig6-v1.tif"/></fig><fig id="fig6s1" position="float" specific-use="child-fig"><label>Figure 6âfigure supplement 1.</label><caption><title>The stronger conflict-type similarity effect in incongruent versus congruent conditions.</title><p>Shown are the summary representational similarity matrices for the right 8C region in incongruent (left) and congruent (right) conditions, respectively. Each cell represents the averaged Pearson correlation (after regressing out all factors except the conflict similarity) of cells with the same conflict type and congruency in the 1400 Ã 1400 matrix. Note that the seemingly disparities in the values of within-conflict cells (i.e., the diagonal) did not reach significance for either incongruent or congruent trials, <italic>F</italic>s &lt;1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig6-figsupp1-v1.tif"/></fig></fig-group><table-wrap id="table2" position="float"><label>Table 2.</label><caption><title>Summary statistics of the cross-subject representational similarity analysis (RSA) for regions showing conflict type and orientation effects identified by the three criteria.</title></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="top" rowspan="2">Region name</th><th align="left" valign="top" colspan="3">Criterion 1</th><th align="left" valign="top" colspan="2">Criterion 2</th><th align="left" valign="top" colspan="2">Criterion 3</th></tr><tr><th align="left" valign="top">t</th><th align="left" valign="top"><italic>Î² Â±</italic> SD</th><th align="left" valign="top">p</th><th align="left" valign="top">t</th><th align="left" valign="top">p</th><th align="left" valign="top">r</th><th align="left" valign="top">p</th></tr></thead><tbody><tr><td align="center" valign="top" colspan="8"><italic><bold>Conflict type effect</bold></italic></td></tr><tr><td align="left" valign="top">Left SFL</td><td align="left" valign="top">4.77</td><td align="left" valign="top">0.0061 Â± 0.0013</td><td align="left" valign="top">1.9 Ã 10<sup>â6</sup></td><td align="left" valign="top">2.45</td><td align="left" valign="top">0.007</td><td align="left" valign="top">0.35</td><td align="left" valign="top">0.021</td></tr><tr><td align="left" valign="top">Left 23 c</td><td align="left" valign="top">4.42</td><td align="left" valign="top">0.0049 Â± 0.0011</td><td align="left" valign="top">9.8 Ã 10<sup>â6</sup></td><td align="left" valign="top">1.18</td><td align="left" valign="top">0.118</td><td align="left" valign="top">â0.15</td><td align="left" valign="top">0.800</td></tr><tr><td align="left" valign="top">Left 24dd</td><td align="left" valign="top">6.13</td><td align="left" valign="top">0.0079 Â± 0.0013</td><td align="left" valign="top">8.9 Ã 10<sup>â10</sup></td><td align="left" valign="top">1.61</td><td align="left" valign="top">0.053</td><td align="left" valign="top">â0.04</td><td align="left" valign="top">0.580</td></tr><tr><td align="left" valign="top">Left 7Am</td><td align="left" valign="top">6.76</td><td align="left" valign="top">0.0090 Â± 0.0013</td><td align="left" valign="top">1.4 Ã 10<sup>â11</sup></td><td align="left" valign="top">â0.75</td><td align="left" valign="top">0.772</td><td align="left" valign="top">0.04</td><td align="left" valign="top">0.418</td></tr><tr><td align="left" valign="top">Left p32pr</td><td align="left" valign="top">4.00</td><td align="left" valign="top">0.0044 Â± 0.0011</td><td align="left" valign="top">6.4 Ã 10<sup>â5</sup></td><td align="left" valign="top">â3.95</td><td align="left" valign="top">1.000</td><td align="left" valign="top">â0.01</td><td align="left" valign="top">0.533</td></tr><tr><td align="left" valign="top">Left 6 r</td><td align="left" valign="top">5.41</td><td align="left" valign="top">0.0071 Â± 0.0013</td><td align="left" valign="top">6.3 Ã 10<sup>â8</sup></td><td align="left" valign="top">â1.94</td><td align="left" valign="top">0.973</td><td align="left" valign="top">â0.11</td><td align="left" valign="top">0.737</td></tr><tr><td align="left" valign="top">Left FOP1</td><td align="left" valign="top">4.37</td><td align="left" valign="top">0.0050 Â± 0.0011</td><td align="left" valign="top">1.3 Ã 10<sup>â5</sup></td><td align="left" valign="top">1.83</td><td align="left" valign="top">0.034</td><td align="left" valign="top">â0.25</td><td align="left" valign="top">0.926</td></tr><tr><td align="left" valign="top">Left PF</td><td align="left" valign="top">4.04</td><td align="left" valign="top">0.0058 Â± 0.0014</td><td align="left" valign="top">5.3 Ã 10<sup>â5</sup></td><td align="left" valign="top">â1.90</td><td align="left" valign="top">0.969</td><td align="left" valign="top">â0.24</td><td align="left" valign="top">0.921</td></tr><tr><td align="left" valign="top">Left VMV1</td><td align="left" valign="top">7.45</td><td align="left" valign="top">0.0091Â± 0.0012</td><td align="left" valign="top">9.6 Ã 10<sup>â14</sup></td><td align="left" valign="top">2.83</td><td align="left" valign="top">0.002</td><td align="left" valign="top">â0.27</td><td align="left" valign="top">0.940</td></tr><tr><td align="left" valign="top">Left VMV2</td><td align="left" valign="top">4.75</td><td align="left" valign="top">0.0053 Â± 0.0011</td><td align="left" valign="top">2.0 Ã 10<sup>â6</sup></td><td align="left" valign="top">â0.32</td><td align="left" valign="top">0.624</td><td align="left" valign="top">â0.06</td><td align="left" valign="top">0.630</td></tr><tr><td align="left" valign="top">Left 25</td><td align="left" valign="top">3.70</td><td align="left" valign="top">0.0041 Â± 0.0011</td><td align="left" valign="top">2.1 Ã 10<sup>â4</sup></td><td align="left" valign="top">3.26</td><td align="left" valign="top">0.001</td><td align="left" valign="top">0.05</td><td align="left" valign="top">0.386</td></tr><tr><td align="left" valign="top">Left Mbelt</td><td align="left" valign="top">4.25</td><td align="left" valign="top">0.0064 Â± 0.0015</td><td align="left" valign="top">2.1 Ã 10<sup>â5</sup></td><td align="left" valign="top">1.77</td><td align="left" valign="top">0.039</td><td align="left" valign="top">0.10</td><td align="left" valign="top">0.275</td></tr><tr><td align="left" valign="top">Right FEF</td><td align="left" valign="top">3.98</td><td align="left" valign="top">0.0054 Â± 0.0014</td><td align="left" valign="top">6.8 Ã 10<sup>â5</sup></td><td align="left" valign="top">â5.49</td><td align="left" valign="top">1.000</td><td align="left" valign="top">0.08</td><td align="left" valign="top">0.327</td></tr><tr><td align="left" valign="top">Right 1</td><td align="left" valign="top">3.90</td><td align="left" valign="top">0.0045 Â± 0.0012</td><td align="left" valign="top">9.7 Ã 10<sup>â5</sup></td><td align="left" valign="top">â2.34</td><td align="left" valign="top">0.990</td><td align="left" valign="top">â0.07</td><td align="left" valign="top">0.665</td></tr><tr><td align="left" valign="top">Right 8 C<xref ref-type="table-fn" rid="table2fn2">*</xref></td><td align="left" valign="top">5.41</td><td align="left" valign="top">0.0064 Â± 0.0012</td><td align="left" valign="top">6.1 Ã 10<sup>â8</sup></td><td align="left" valign="top">2.46</td><td align="left" valign="top">0.007</td><td align="left" valign="top">0.52</td><td align="left" valign="top">0.001</td></tr><tr><td align="left" valign="top">Right a47r</td><td align="left" valign="top">5.04</td><td align="left" valign="top">0.0056 Â± 0.0011</td><td align="left" valign="top">4.7 Ã 10<sup>â7</sup></td><td align="left" valign="top">â0.68</td><td align="left" valign="top">0.753</td><td align="left" valign="top">0.05</td><td align="left" valign="top">0.393</td></tr><tr><td align="left" valign="top">Right IFJp</td><td align="left" valign="top">3.78</td><td align="left" valign="top">0.0042 Â± 0.0011</td><td align="left" valign="top">1.6 Ã 10<sup>â4</sup></td><td align="left" valign="top">0.77</td><td align="left" valign="top">0.221</td><td align="left" valign="top">0.27</td><td align="left" valign="top">0.056</td></tr><tr><td align="left" valign="top">Right AIP</td><td align="left" valign="top">4.64</td><td align="left" valign="top">0.0054 Â± 0.0012</td><td align="left" valign="top">3.5 Ã 10<sup>â6</sup></td><td align="left" valign="top">1.86</td><td align="left" valign="top">0.032</td><td align="left" valign="top">â0.02</td><td align="left" valign="top">0.540</td></tr><tr><td align="left" valign="top">Right TPOJ3</td><td align="left" valign="top">4.48</td><td align="left" valign="top">0.0056 Â± 0.0012</td><td align="left" valign="top">7.6 Ã 10<sup>â6</sup></td><td align="left" valign="top">â0.25</td><td align="left" valign="top">0.600</td><td align="left" valign="top">0.21</td><td align="left" valign="top">0.118</td></tr><tr><td align="left" valign="top">Right PGi</td><td align="left" valign="top">4.07</td><td align="left" valign="top">0.0045 Â± 0.0011</td><td align="left" valign="top">4.8 Ã 10<sup>â5</sup></td><td align="left" valign="top">â1.59</td><td align="left" valign="top">0.944</td><td align="left" valign="top">â0.01</td><td align="left" valign="top">0.523</td></tr><tr><td align="left" valign="top">Right V3CD</td><td align="left" valign="top">3.86</td><td align="left" valign="top">0.0043 Â± 0.0011</td><td align="left" valign="top">1.2 Ã 10<sup>â4</sup></td><td align="left" valign="top">1.94</td><td align="left" valign="top">0.026</td><td align="left" valign="top">0.02</td><td align="left" valign="top">0.451</td></tr><tr><td align="center" valign="top" colspan="8"><italic><bold>Orientation effect</bold></italic></td></tr><tr><td align="left" valign="top">Left FEF<xref ref-type="table-fn" rid="table2fn2">*</xref></td><td align="left" valign="top">5.17</td><td align="left" valign="top">0.0060 Â± 0.0012</td><td align="left" valign="top">2.4 Ã 10<sup>â7</sup></td><td align="left" valign="top">2.73</td><td align="left" valign="top">0.003</td><td align="left" valign="top">â0.01</td><td align="left" valign="top">0.518</td></tr><tr><td align="left" valign="top">Left POS1</td><td align="left" valign="top">4.35</td><td align="left" valign="top">0.0051 Â± 0.0012</td><td align="left" valign="top">1.4 Ã 10<sup>â5</sup></td><td align="left" valign="top">â1.52</td><td align="left" valign="top">0.936</td><td align="left" valign="top">0.00</td><td align="left" valign="top">0.500</td></tr><tr><td align="left" valign="top">Left 31pv</td><td align="left" valign="top">5.60</td><td align="left" valign="top">0.0103 Â± 0.0018</td><td align="left" valign="top">2.1 Ã 10<sup>â8</sup></td><td align="left" valign="top">â0.48</td><td align="left" valign="top">0.686</td><td align="left" valign="top">0.05</td><td align="left" valign="top">0.397</td></tr><tr><td align="left" valign="top">Left 6ma</td><td align="left" valign="top">4.75</td><td align="left" valign="top">0.0055 Â± 0.0012</td><td align="left" valign="top">2.0 Ã 10<sup>â6</sup></td><td align="left" valign="top">â1.87</td><td align="left" valign="top">0.970</td><td align="left" valign="top">â0.00</td><td align="left" valign="top">0.500</td></tr><tr><td align="left" valign="top">Left 7PC</td><td align="left" valign="top">3.66</td><td align="left" valign="top">0.0042 Â± 0.0012</td><td align="left" valign="top">2.5 Ã 10<sup>â4</sup></td><td align="left" valign="top">0.76</td><td align="left" valign="top">0.223</td><td align="left" valign="top">â0.20</td><td align="left" valign="top">0.876</td></tr><tr><td align="left" valign="top">Left 8BL</td><td align="left" valign="top">4.55</td><td align="left" valign="top">0.0053 Â± 0.0012</td><td align="left" valign="top">5.3 Ã 10<sup>â6</sup></td><td align="left" valign="top">0.10</td><td align="left" valign="top">0.460</td><td align="left" valign="top">0.14</td><td align="left" valign="top">0.207</td></tr><tr><td align="left" valign="top">Left AIP</td><td align="left" valign="top">3.83</td><td align="left" valign="top">0.0044 Â± 0.0012</td><td align="left" valign="top">1.3 Ã 10<sup>â4</sup></td><td align="left" valign="top">0.62</td><td align="left" valign="top">0.266</td><td align="left" valign="top">â0.23</td><td align="left" valign="top">0.910</td></tr><tr><td align="left" valign="top">Left TE1p</td><td align="left" valign="top">3.95</td><td align="left" valign="top">0.0046 Â± 0.0012</td><td align="left" valign="top">7.8 Ã 10<sup>â5</sup></td><td align="left" valign="top">0.14</td><td align="left" valign="top">0.443</td><td align="left" valign="top">0.01</td><td align="left" valign="top">0.475</td></tr><tr><td align="left" valign="top">Left IP2<xref ref-type="table-fn" rid="table2fn2">*</xref></td><td align="left" valign="top">4.49</td><td align="left" valign="top">0.0052 Â± 0.0012</td><td align="left" valign="top">7.2 Ã 10<sup>â6</sup></td><td align="left" valign="top">4.02</td><td align="left" valign="top">0.000</td><td align="left" valign="top">0.19</td><td align="left" valign="top">0.139</td></tr><tr><td align="left" valign="top">Right V1<xref ref-type="table-fn" rid="table2fn2">*</xref></td><td align="left" valign="top">4.71</td><td align="left" valign="top">0.0083 Â± 0.0018</td><td align="left" valign="top">2.5 Ã 10<sup>â6</sup></td><td align="left" valign="top">2.91</td><td align="left" valign="top">0.002</td><td align="left" valign="top">â0.08</td><td align="left" valign="top">0.672</td></tr><tr><td align="left" valign="top">Right V2<xref ref-type="table-fn" rid="table2fn2">*</xref></td><td align="left" valign="top">3.88</td><td align="left" valign="top">0.0172 Â± 0.0044</td><td align="left" valign="top">1.1 Ã 10<sup>â4</sup></td><td align="left" valign="top">3.19</td><td align="left" valign="top">0.001</td><td align="left" valign="top">0.02</td><td align="left" valign="top">0.462</td></tr><tr><td align="left" valign="top">Right V3</td><td align="left" valign="top">3.94</td><td align="left" valign="top">0.0175 Â± 0.0045</td><td align="left" valign="top">8.2 Ã 10<sup>â5</sup></td><td align="left" valign="top">â1.45</td><td align="left" valign="top">0.927</td><td align="left" valign="top">0.39</td><td align="left" valign="top">0.010</td></tr><tr><td align="left" valign="top">Right LO2</td><td align="left" valign="top">5.95</td><td align="left" valign="top">0.0069 Â± 0.0012</td><td align="left" valign="top">2.8 Ã 10<sup>â9</sup></td><td align="left" valign="top">â1.81</td><td align="left" valign="top">0.965</td><td align="left" valign="top">0.26</td><td align="left" valign="top">0.068</td></tr><tr><td align="left" valign="top">Right POS1<xref ref-type="table-fn" rid="table2fn2">*</xref></td><td align="left" valign="top">3.70</td><td align="left" valign="top">0.0043 Â± 0.0012</td><td align="left" valign="top">2.1 Ã 10<sup>â4</sup></td><td align="left" valign="top">2.98</td><td align="left" valign="top">0.001</td><td align="left" valign="top">0.33</td><td align="left" valign="top">0.028</td></tr><tr><td align="left" valign="top">Right 5 m</td><td align="left" valign="top">5.24</td><td align="left" valign="top">0.0061 Â± 0.0012</td><td align="left" valign="top">1.6 Ã 10<sup>â7</sup></td><td align="left" valign="top">â2.12</td><td align="left" valign="top">0.983</td><td align="left" valign="top">0.00</td><td align="left" valign="top">0.500</td></tr><tr><td align="left" valign="top">Right TF</td><td align="left" valign="top">4.97</td><td align="left" valign="top">0.0058 Â± 0.0012</td><td align="left" valign="top">6.7 Ã 10<sup>â7</sup></td><td align="left" valign="top">â1.08</td><td align="left" valign="top">0.860</td><td align="left" valign="top">â0.07</td><td align="left" valign="top">0.657</td></tr><tr><td align="left" valign="top">Right PHT</td><td align="left" valign="top">4.54</td><td align="left" valign="top">0.0053 Â± 0.0012</td><td align="left" valign="top">5.5 Ã 10<sup>â6</sup></td><td align="left" valign="top">0.03</td><td align="left" valign="top">0.486</td><td align="left" valign="top">â0.04</td><td align="left" valign="top">0.589</td></tr><tr><td align="left" valign="top">Right PF<xref ref-type="table-fn" rid="table2fn2">*</xref></td><td align="left" valign="top">5.57</td><td align="left" valign="top">0.0064 Â± 0.0012</td><td align="left" valign="top">2.6 Ã 10<sup>â8</sup></td><td align="left" valign="top">3.08</td><td align="left" valign="top">0.001</td><td align="left" valign="top">â0.03</td><td align="left" valign="top">0.558</td></tr><tr><td align="left" valign="top">Right A4</td><td align="left" valign="top">4.11</td><td align="left" valign="top">0.0048 Â± 0.0012</td><td align="left" valign="top">3.9 Ã 10<sup>â5</sup></td><td align="left" valign="top">â2.68</td><td align="left" valign="top">0.996</td><td align="left" valign="top">â0.09</td><td align="left" valign="top">0.700</td></tr></tbody></table><table-wrap-foot><fn><p>All p-values listed are one-tailed and uncorrected.</p></fn><fn id="table2fn2"><label>*</label><p>Denotes the regions meeting all three criteria for each effect.</p></fn></table-wrap-foot></table-wrap><p>We observed the right 8C but not the left 8C represented the conflict-type similarity. A further test is to show whether there is a lateralization. We tested several regions of the left dlPFC, including the i6-8, 8Av, 8C, p9-46v, 46, 9-46d, and a9-46v (<xref ref-type="bibr" rid="bib33">Freund et al., 2021a</xref>). We found that none of these regions show the representation of conflict type, all uncorrected ps&gt;0.35. These results indicate that the conflict type is specifically represented in the right dlPFC.</p><p>In addition, we repeated the analysis by using data smoothed with a 6 mm full width at half maximum (FWHM) Gaussian kernel. Results showed a significant conflict similarity effect in right 8C, <italic>t</italic>(1902599.9) = 5.55, p&lt;0.0001, replicating the results on unsmoothed data [<italic>t</italic>(86.8) = 5.41, p&lt;0.0001]. This observation implies that the possibility of variations in brain region locations across subjects, which may have been mitigated using a larger smoothing kernel, does not appear to have influenced the results.</p><p>The model described above employs the cosine similarity measure to define conflict similarity and will be referred to as the cognitive-space model. To examine whether the right 8C specifically encodes the cognitive space rather than the domain-general or domain-specific organizations, we tested two additional models (see âMaterials and methodsâ). Model comparison showed a lower Bayesian information criterion (BIC) in the cognitive-space model (BIC = 5,377,093) than the Domain-General (BIC = 5,377,126) or Domain-Specific (BIC = 5,377,127) models. Further analysis showed the dimensionality of the representation in the right 8C was 1.19, suggesting the cognitive space was close to 1D. Moreover, we replicated the results with only incongruent trials, considering that the pattern of conflict representations is more manifested when the conflict is present (i.e., on incongruent trials) than not (i.e., on congruent trials). We found a poorer fitting in domain-general (BIC = 1,344,127) and domain-specific (BIC = 1,344,129) models than the cognitive-space model (BIC = 1,344,104). These results indicate that the right 8C encodes an integrated cognitive space for resolving Stroop and Simon conflicts. More detailed model comparison results are listed in <xref ref-type="table" rid="table3">Table 3</xref>.</p><table-wrap id="table3" position="float"><label>Table 3.</label><caption><title>Model comparison results of the right 8C.</title><p>RSM_I shows results using incongruent trials only.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" valign="bottom" rowspan="2">Model name</th><th align="left" valign="bottom" colspan="3">Full RSM</th><th align="left" valign="bottom"/><th align="left" valign="bottom" colspan="3">RSM_I</th></tr><tr><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th><th align="left" valign="bottom">BIC</th><th align="left" valign="bottom"/><th align="left" valign="bottom"><italic>t</italic></th><th align="left" valign="bottom">p</th><th align="left" valign="bottom">BIC</th></tr></thead><tbody><tr><td align="left" valign="bottom">Cognitive-space</td><td align="left" valign="bottom">5.41</td><td align="left" valign="bottom">6.1 Ã 10<sup>â8</sup></td><td align="left" valign="bottom">5,377,093</td><td align="left" valign="bottom"/><td align="left" valign="bottom">4.97</td><td align="left" valign="bottom">3.35 Ã 10<sup>â7</sup></td><td align="left" valign="bottom">1,345,201</td></tr><tr><td align="left" valign="bottom">Domain-general</td><td align="left" valign="bottom">0.92</td><td align="left" valign="bottom">.179</td><td align="left" valign="bottom">5,377,126</td><td align="left" valign="bottom"/><td align="left" valign="bottom">1.43</td><td align="left" valign="bottom">0.076</td><td align="left" valign="bottom">1,344,127</td></tr><tr><td align="left" valign="bottom">Domain-specific</td><td align="left" valign="bottom">0.84</td><td align="left" valign="bottom">.200</td><td align="left" valign="bottom">5,377,127</td><td align="left" valign="bottom"/><td align="left" valign="bottom">0.28</td><td align="left" valign="bottom">0.390</td><td align="left" valign="bottom">1,344,129</td></tr></tbody></table></table-wrap><p>In sum, we found converging evidence supporting that the right dlPFC (8C area) encoded conflict similarity parametrically, which further supports the hypothesis that conflict types are represented in a cognitive space.</p></sec><sec id="s2-6"><title>Multivariate patterns of visual and oculomotor areas encode stimulus orientation</title><p>To tease apart the representation of conflict type from that of perceptual information, we tested the modulation of the spatial orientations of stimulus locations on RSM using the aforementioned RSA. We also applied three independent criteria: (1) the cortical regions should exhibit a statistically significant orientation effect on the RSM; (2) the conflict similarity effect should be stronger in incongruent than congruent trials; and (3) the orientation effect should not interact with the CSE since the orientation effect was dissociated from the conflict similarity effect and was not expected to influence cognitive control. We observed increasing fMRI representational similarity between trials with more similar orientations of stimulus location in the occipital cortex, such as right V1, right V2, right V3, bilateral POS1, and right lateral occipital 2 (LO2) areas (Bonferroni corrected ps&lt;0.05). We also found the same effect in the oculomotor related region, that is, the left FEF, and other regions including the left 31pv, 6ma, 7PC, 8BL, AIP, TE1p, IP2, right 5m, TF, PHT, A4, and parietal area F (PF) (<xref ref-type="fig" rid="fig7">Figure 7A</xref>). Then we tested whether any of these brain regions were related to the conflict representation by comparing their encoding strength between incongruent and congruent conditions. Results showed that the right V1, right V2, right POS1, left IP2, left FEF, and right PF encoded stronger orientation effect in the incongruent than the congruent condition, FDR-corrected ps&lt;0.05, one-tailed (<xref ref-type="table" rid="table2">Table 2</xref>, <xref ref-type="fig" rid="fig7">Figure 7B</xref>). We then tested whether any of these regions were related to the behavioral performance, and results showed that none of them positively correlated with the behavioral conflict similarity modulation effect, all uncorrected p<italic>s</italic>&gt;0.18, one-tailed. Thus all identified regions are consistent with criterion 3. Taken together, we found that the visual and oculomotor regions encoded orientations of stimulus location in a continuous manner and that the encoding strength was stronger when the conflict was present.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>The orientation effect.</title><p>(<bold>A</bold>) Brain regions surviving the Bonferroni correction (p&lt;0.05) across the 360 regions (criterion 1). Labeled regions are those meeting the three criteria. (<bold>B</bold>) Different encoding of orientation in the incongruent with congruent conditions. *False discovery rate (FDR)-corrected p&lt;0.05; **FDR-corrected p&lt;0.01; ***FDR-corrected p&lt;0.001.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig7-v1.tif"/></fig><p>We hypothesize that the overlapping spatial information of orientation may have facilitated the encoding of conflict types. To explore the relation between conflict type and orientation representations, we conducted representational connectivity (i.e., the similarity between two within-subject RSMs of two regions) (<xref ref-type="bibr" rid="bib54">Kriegeskorte et al., 2008</xref>) analyses and found that among the orientation effect regions, the right V1 and right V2 showed significant representational connectivity with the right 8C compared to the controlled regions (including those encoding orientation effect but not showing larger encoding strength in incongruent than congruent conditions, as well as eight other regions encoding none of our defined effects in the main RSA, see âMaterials and methodsâ). Compared with the largest connectivity strength in the controlled regions (i.e., the left 6ma, <italic>Î² =</italic> 0.7991 Â± 0.0299), we found higher connectivity in the right V1, <italic>Î² =</italic> 0.8633 Â± 0.0325, and right V2, <italic>Î² =</italic> 0.8772 Â± 0.0335 (<xref ref-type="fig" rid="fig8">Figure 8</xref>).</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>The representational connectivity between the right 8C area and the cortical regions showing significant encoding of orientation.</title><p>The black bars represent regions showing both the overall orientation effect and higher encoding strength of orientation in incongruent than congruent conditions; the gray bars are regions showing only the overall orientation effect but not higher encoding strength of orientation in incongruent than congruent conditions; and the white bars are regions not showing any of the effects of interest (i.e., uncorrected p&gt;0.3 for all the conflict type, orientation, congruency, target, response, spatial Stroop distractor, and Simon distractor effects). Regions plotted in gray and white bars serve as controlled baseline. Error bars are the standard error of the mean. The dashed line indicates the upper bound of the 95% confidence interval of the highest connectivity of controlled regions (i.e., left 6ma). l = left, r = right.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig8-v1.tif"/></fig></sec><sec id="s2-7"><title>The multivariate representations of conflict type and orientation are different from the congruency effect</title><p>An explanation to the stronger encoding of conflict type in incongruent than congruent condition (<xref ref-type="fig" rid="fig6">Figure 6B, D</xref>) in right 8C area may be the encoding of congruency. To test this possibility, we first tested the univariate congruency effect (incongruent minus congruent) using the parametric modulating GLM1 that was used to estimate fMRI activation levels of conflict type Ã congruency conditions. We observed no univariate congruency effect in the right 8C region, <italic>t</italic>(34) = â0.03, p=0.513, one-tailed. Neither did we observe a multivariate congruency effect (i.e., the pattern difference between incongruent and congruent conditions compared to that within each condition) in the right 8C or any other regions. Note the definition of congruency here differed from traditional definitions (i.e., contrast between activity strength of incongruent and congruent conditions), with which we found stronger univariate activities in pre-SMA for incongruent versus congruent conditions. We further tested the possibility that the congruency effect may be manifested in behavioral relevance. To this end, we extracted the contrast of incongruent minus congruent on encoding strength of conflict similarity for each subject from the mixed-effect model based on the cross-subject RSA (see âRepresentational similarity analysisâ) and correlated it with the behavioral congruency effect, averaged across the five conflict types (i.e., the main effect reported in <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). No significant correlation was observed (<italic>r</italic> = 0.14, p=0.380, one-tailed). Taken together, these results suggested that the neural encoding strength of conflict type does not reflect the level of cognitive control engagement, but the dynamic adjustment of cognitive control instead.</p><p>Similarly, we tested whether those regions with stronger encoding of orientation in incongruent than congruent condition (i.e., right V1, V2, PF, POS1, and left FEF and IP2) reflect the congruency effect. We observed no univoxel congruency effect in any of these regions, all uncorrected ps&gt;0.89, one-tailed. In addition, the orientation effect was not correlated to the behavioral congruency in any of the regions, all uncorrected ps&gt;0.074, one-tailed. Together with our finding that there was no correlation between the strength of orientation encoding and the conflict similarity modulation on behavioral CSEs in any of these regions (see âMultivariate patterns of visual and oculomotor areas encode stimulus orientationâ), these results indicate that the encoding of orientation effect did not reflect the encoding of congruency or conflict type. Instead, we speculate that the encoding of orientations provides perceptual information to determine the conflict type.</p></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>Understanding how different types of conflicts are resolved is essential to answer how cognitive control achieves adaptive behavior. However, the dichotomy between domain-general and/or domain-specific processes presents a dilemma (<xref ref-type="bibr" rid="bib13">Braem et al., 2014</xref>; <xref ref-type="bibr" rid="bib27">Egner, 2008</xref>). Reconciliation of the two views also suffers from the inability to fully address how different conflicts can be resolved by a limited set of cognitive control processes. In this study, we hypothesized that this issue can be addressed if conflicts are organized as a cognitive space. Leveraging the well-known dissociation between the spatial Stroop and Simon conflicts (<xref ref-type="bibr" rid="bib56">Li et al., 2014</xref>; <xref ref-type="bibr" rid="bib59">Liu et al., 2010</xref>; <xref ref-type="bibr" rid="bib92">Wang et al., 2014</xref>), we designed five conflict types that are systematically different from each other. The cognitive space hypothesis predicted that the representational proximity/distance between two conflict types scales with their similarities/dissimilarities, which was tested at both behavioral and neural levels. Behaviorally, we found that the CSEs were linearly modulated by conflict similarity between consecutive trials, replicating and extending our previous study (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>). BOLD activity patterns in the right dlPFC further showed that the representational similarity between conflict types was modulated by their conflict similarity, and that strength of the modulation was positively associated with the modulation of conflict similarity on the behavioral CSE. We also observed that activity in two brain regions (right IPS and right dlPFC) was parametrically modulated by the conflict type difference, though they did not directly explain the behavioral results. Additionally, we found that the visual regions encoded the spatial orientation of the stimulus location, which might provide the essential concrete information to determine the conflict type. Together, these results suggest that conflicts may be organized in a cognitive space that enables a limited set of cognitive control processes to resolve a wide variety of distinct types of conflicts.</p><p>Conventionally, the domain-general view of control suggests a common representation for different types of conflicts (<xref ref-type="fig" rid="fig9">Figure 9</xref>, left), while the domain-specific view suggests dissociated representations for different types (<xref ref-type="fig" rid="fig9">Figure 9</xref>, right). Previous research on this topic often adopts a binary manipulation of conflicts (<xref ref-type="bibr" rid="bib13">Braem et al., 2014</xref>) (i.e., each domain only has one conflict type) and gathered evidence for the domain-general/-specific view with the presence/absence of CSE, respectively. Here, we parametrically manipulated the similarity of conflict types and found the CSE systematically vary with conflict similarity level, demonstrating that cognitive control is neither purely domain-general nor purely domain-specific, but can be reconciled as a cognitive space (<xref ref-type="bibr" rid="bib9">Bellmund et al., 2018</xref>; <xref ref-type="fig" rid="fig9">Figure 9</xref>, middle). The model comparison analysis also showed that the cognitive-space model explained the representation in right DLPFC better than the domain-general or domain-specific models. Specifically, the cognitive space provides a solution to use a single cognitive space organization to encode different types of conflicts that are close (domain-general) or distant (domain-specific) to each other. It also shows the potential for how various conflict types can be coded using limited resources (i.e., as different points in a low-dimensional cognitive space), as suggested by its out-of-sample predictability. Moreover, geometry can also emerge in the cognitive space (<xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref>), which will allow for decomposition of a conflict type (e.g., how much conflict in each of the dimensions in the cognitive space) so that it can be mapped into the limited set of cognitive control processes. Such geometry enables fast learning of cognitive control settings from similar conflict types by providing a measure of similarity (e.g., as distance in space).</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Illustration of the hypothesized dimensionalities of different representations.</title><p>The shade of the red color indicates the degree of dimensionality (i.e., how many dimensions are needed to represent different states). The dimensionality of domain-general representation is extremely low, with all representations compressed to one dot. The dimensionality of domain-specific representation is extremely high, with each control state encoded in a unique and orthogonal dimension. The dimensionality of the organized representation is modest, enabling distant states to be separated but also allowing close states to share representations. The solid arrows show the axes of different dimensions. The dashed arrows indicate how the representational dimensionality can be reduced by projecting the independent dimensions to a common dimension.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-fig9-v1.tif"/></fig><p>If the dimensionality of the cognitive space of conflict is extremely high, the cognitive space solution would suffer the same criticism as the domain-specificity theory. We argue that the dimensionality is manageable for the human brain as task information unrelated to differentiating conflicts can be removed. For example, the Simon conflict can be represented in a space consisting of spatial location, stimulus information, and responses. Thus, the dimensionality of the cognitive space of conflicts should not exceed the number of represented features. The dimensionality can be further reduced as humans selectively represent a small number of features when learning task representations (e.g., spatial information is reduced to the horizontal dimension from the 3D space we live in) (<xref ref-type="bibr" rid="bib70">Niv, 2019</xref>). Consistently, we observed a low dimensional (1.19D) space representing the five conflict types. This is expected since the only manipulated variable is the angular distance between conflict types. The reduced dimensionality does not only require less effort to represent the conflict, but also facilitates generalization of cognitive control settings among different conflict types (<xref ref-type="bibr" rid="bib6">Badre et al., 2021</xref>).</p><p>Although our finding of cognitive space in the right dlPFC differs from other cognitive space studies (<xref ref-type="bibr" rid="bib18">Constantinescu et al., 2016</xref>; <xref ref-type="bibr" rid="bib71">Park et al., 2020</xref>; <xref ref-type="bibr" rid="bib81">Schuck et al., 2016</xref>) that highlighted the orbitofrontal and hippocampus regions, it is consistent with the cognitive control literature. The prefrontal cortex has long been believed to be a key region of cognitive control representation (<xref ref-type="bibr" rid="bib64">Mansouri et al., 2007</xref>; <xref ref-type="bibr" rid="bib66">Miller and Cohen, 2001</xref>; <xref ref-type="bibr" rid="bib67">Milner, 1963</xref>) and is widely engaged in multiple task demands (<xref ref-type="bibr" rid="bib17">Cole et al., 2013</xref>; <xref ref-type="bibr" rid="bib22">Duncan, 2010</xref>). However, it is not until recently that the multivariate representation in this region has been examined. For instance, <xref ref-type="bibr" rid="bib87">Vaidya et al., 2021</xref> reported that frontal regions represented latent states that are organized hierarchically. <xref ref-type="bibr" rid="bib33">Freund et al., 2021a</xref> showed that dlPFC encoded the target and congruency in a typical color-word Stroop task. Taken together, we suggest that the right dlPFC might flexibly encode a variety of cognitive spaces to meet the dynamic task demands. In addition, we found no such representation in the left dlPFC, indicating a possible lateralization. Previous studies showed that the left dlPFC was related to the expectancy-related attentional set upregulation, while the right dlPFC was related to the online adjustment of control (<xref ref-type="bibr" rid="bib35">Friehs et al., 2020</xref>; <xref ref-type="bibr" rid="bib89">Vanderhasselt et al., 2009</xref>), which is consistent with our findings. Moreover, the right PFC also represents a composition of single rules (<xref ref-type="bibr" rid="bib75">Reverberi et al., 2012</xref>), which may explain how the spatial Stroop and Simon types can be jointly encoded in a single space.</p><p>Previous researchers have proposed an âexpected value of control (EVC)â theory, which posits that the brain can evaluate the cost and benefit associated with executing control for a demanding task, such as the conflict task, and specify the optimal control strength (<xref ref-type="bibr" rid="bib82">Shenhav et al., 2013</xref>). For instance, <xref ref-type="bibr" rid="bib40">Grahek et al., 2022</xref> found that more frequently switching goals when doing a Stroop task were achieved by adjusting smaller control intensity. Our work complements the EVC theory by further investigating the neural representation of different conflict conditions and how these representations can be evaluated to facilitate conflict resolution. We found that different conflict conditions could be efficiently represented in a cognitive space encoded by the right dlPFC, and participants with stronger cognitive space representation have also adjusted their conflict control to a greater extent based on the conflict similarity (<xref ref-type="fig" rid="fig6">Figure 6C</xref>). The finding suggests that the cognitive space organization of conflicts guides cognitive control to adjust behavior. Previous studies have shown that participants may adopt different strategies to represent a task, with the model-based strategies benefitting goal-related behaviors more than the model-free strategies (<xref ref-type="bibr" rid="bib78">Rmus et al., 2022</xref>). Similarly, we propose that cognitive space could serve as a mental model to assist fast learning and efficient organization of cognitive control settings. Specifically, the cognitive space representation may provide a principle for how our brain evaluates the expected cost of switching and the benefit of generalization between states and selects the path with the best costâbenefit tradeoff (<xref ref-type="bibr" rid="bib2">Abrahamse et al., 2016</xref>; <xref ref-type="bibr" rid="bib82">Shenhav et al., 2013</xref>). The proximity between two states in cognitive space could reflect both the expected cognitive demand required to transition and the useful mechanisms to adapt from. The closer the two conditions are in cognitive space, the lower the expected switching cost and the higher the generalizability when transitioning between them. With the organization of a cognitive space, a new conflict can be quickly assigned a location in the cognitive space, which will facilitate the development of cognitive control settings for this conflict by interpolating nearby conflicts and/or projecting the location to axes representing different cognitive control processes, thus leading to a stronger CSE when following a more similar conflict condition. On the other hand, without a cognitive space, there would be no measure of similarity between conflicts on different trials, hence limiting the ability of fast learning of cognitive control setting from similar trials.</p><p>The cognitive space in the right dlPFC appears to be an abstraction of concrete information from the visual regions. We found that the right V1 and V2 encoded the spatial orientation of the target location (<xref ref-type="fig" rid="fig7">Figure 7</xref>) and showed strong representational connectivity with the right dlPFC (<xref ref-type="fig" rid="fig8">Figure 8</xref>), suggesting that there might be information exchange between these regions. We speculate that the representation of spatial orientation may have provided the essential perceptual information to determine the conflict type (<xref ref-type="fig" rid="fig1">Figure 1</xref>) and thus served as the critical input for the cognitive space. The conflict-type representation further incorporates the stimulusâresponse mapping rules to the spatial orientation representation, so that vertically symmetric orientations can be recognized as the same conflict type (<xref ref-type="fig" rid="fig4">Figure 4</xref>). In other words, the representation of conflict type involves the compression of perceptual information (<xref ref-type="bibr" rid="bib29">Flesch et al., 2022</xref>), which is consistent with the idea of a low-dimensional representation of cognitive control (<xref ref-type="bibr" rid="bib6">Badre et al., 2021</xref>; <xref ref-type="bibr" rid="bib61">MacDowell et al., 2022</xref>). The compression and abstraction processes might be why the frontoparietal regions are the top of hierarchy of information processing (<xref ref-type="bibr" rid="bib37">Gilbert and Li, 2013</xref>) and why the frontoparietal regions are widely engaged in multiple task demands (<xref ref-type="bibr" rid="bib23">Duncan, 2013</xref>).</p><p>Although the spatial orientation information in our design could be helpful to the construction of cognitive space, the cognitive space itself was independent of the stimulus-level representation of the task. We found the conflict similarity modulation on CSE did not change with more training, indicating that the cognitive space did not depend on strategies that could be learned through training. Instead, the cognitive space should be determined by the intrinsic similarity structure of the task design. For example, a previous study (<xref ref-type="bibr" rid="bib32">Freitas and Clark, 2015</xref>) has found that the CSE across different versions of spatial Stroop and flanker tasks was stronger than that across either of the two conflicts and Simon. In their designs, the stimulus similarity was controlled at the same level, so the difference in CSE was only attributable to the similar dimensional overlap between Stroop and flanker tasks, in contrast to the Simon task. Furthermore, recent studies showed that the cognitive space generally exists to represent structured latent states (e.g., <xref ref-type="bibr" rid="bib88">Vaidya and Badre, 2022</xref>), mental strategy cost (<xref ref-type="bibr" rid="bib40">Grahek et al., 2022</xref>), and social hierarchies (<xref ref-type="bibr" rid="bib71">Park et al., 2020</xref>). Therefore, cognitive space is likely a universal strategy that can be applied to different scenarios.</p><p>With conventional univariate analyses, we observed that the overall congruency effect was located at the medial frontal region (i.e., pre-SMA), which is consistent with previous studies (<xref ref-type="bibr" rid="bib12">Botvinick et al., 2004</xref>; <xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref>). Beyond that, we also found regions that can be parametrically modulated by conflict-type difference, including right IPS and right dlPFC (modulated by Simon difference). The right lateralization of these regions is consistent with a previous finding (<xref ref-type="bibr" rid="bib57">Li et al., 2017</xref>). The parametric encoding of conflict also mirrors prior research showing the parametric encoding of task demands (<xref ref-type="bibr" rid="bib21">Dagher et al., 1999</xref>; <xref ref-type="bibr" rid="bib77">Ritz and Shenhav, 2023</xref>). The scaling of brain activities based on conflict difference is potentially important to the representational organization of different types of conflicts. However, we did not observe their brainâbehavioral relevance. One possible reason is that the conflict (dis)similarity is a combination of (dis)similarity of spatial Stroop and Simon conflicts, but each univariate region only reflects difference along a single conflict domain. Also likely, the representational geometry is more of a multivariate problem than what univariate activities can capture (<xref ref-type="bibr" rid="bib34">Freund et al., 2021b</xref>). Future studies may adopt approaches such as repetition suppression-induced fMRI adaptation (<xref ref-type="bibr" rid="bib6">Badre et al., 2021</xref>) to test the role of univariate activities in task representations.</p><p>Recently an interesting debate has arisen concerning whether cognitive control should be considered as a process or a representation (<xref ref-type="bibr" rid="bib34">Freund et al., 2021b</xref>). Traditionally, cognitive control has been predominantly viewed as a process. However, the study of its representation has gained more and more attention. While it may not be as straightforward as the visual representation (e.g., creating a mental image from a real image in the visual area), cognitive control can have its own form of representation. An influential theory, <xref ref-type="bibr" rid="bib65">Marr, 1982</xref> three-level model proposed that representation serves as the algorithm of the process to achieve a goal based on the input. In other words, representation can encompass a dynamic process rather than being limited to static stimuli. Building on this perspective, we posit that the representation of cognitive control consists of an array of dynamic representations embedded within the overall process. A similar idea has been proposed that the representation of task profiles can be progressively constructed with time in the brain (<xref ref-type="bibr" rid="bib51">Kikumoto and Mayr, 2020</xref>). Moreover, we anticipate that the representation of cognitive space is most prominently involved at two critical stages to guide the transference of behavioral CSE. The first stage involves the evaluation of control demands, where the representational distance/similarity between previous and current trials influences the adjustment of cognitive control. The second stage pertains to control execution, where the switch from one control state to another follows a path within the cognitive space. However, we were unable to fully distinguish between these two stages due to the low temporal resolution of fMRI signals in our study. Future research seeking to delve deeper into this question may benefit from methodologies with higher temporal resolutions, such as EEG and MEG.</p><p>Several interesting questions remains to be answered. For example, is the dimension of the unified space across conflict-inducing tasks solely determined by the number of conflict sources? Is this unified space adaptively adjusted within the same brain region? Can we effectively map any sources of conflict with completely different stimuli into a single space? Does the cognitive space vary from population to population, such as between the normal people and patients?</p><sec id="s3-1"><title>Methodological implications</title><p>Previous studies with mixed conflicts have applied mainly categorical manipulations of conflict types, such as the multi-source interference task (<xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref>) and color StroopâSimon task (<xref ref-type="bibr" rid="bib59">Liu et al., 2010</xref>). The categorical manipulations make it difficult to quantify conceptual similarity between conflict types and hence limit the ability to test whether neural representations of conflict capture conceptual similarity. To the best of our knowledge, no previous studies have manipulated the conflict types parametrically. This gap highlights a broader challenge within cognitive science: effectively manipulating and measuring similarity levels for conflicts, as well as other high-level cognitive processes, which are inherently abstract. The use of an experimental paradigm that permits parametric manipulation of conflict similarity provides a way to systematically investigate the organization of cognitive control, as well as its influence on adaptive behaviors. Moreover, the cross-subject RSA provides high sensitivity to the variables of interest and the ability to separate confounding factors. For instance, in addition to dissociating conflict type from orientation, we dissociated target from response, and spatial Stroop distractor from Simon distractor. We further showed cognitive control can both enhance the target representation and suppress the distractor representation (<xref ref-type="fig" rid="fig5">Figure 5E</xref>), which is in line with previous studies (<xref ref-type="bibr" rid="bib73">Polk et al., 2008</xref>; <xref ref-type="bibr" rid="bib76">Ritz and Shenhav, 2022</xref>).</p></sec><sec id="s3-2"><title>Limitations</title><p>A few limitations of this study need to be noted. To parametrically manipulate the conflict similarity levels, we adopted the spatial StroopâSimon paradigm that enables parametrical combinations of spatial Stroop and Simon conflicts. However, since this paradigm is a two-alternative forced choice design, the behavioral CSE is not a pure measure of adjusted control but could be partly confounded by bottom-up factors such as feature integration (<xref ref-type="bibr" rid="bib43">Hommel et al., 2004</xref>). Future studies may replicate our findings with a multiple-choice design (including more varied stimulus sets, locations, and responses) with confound-free trial sequences (<xref ref-type="bibr" rid="bib14">Braem et al., 2019</xref>). Another limitation is that in our design, the spatial Stroop and Simon effects are highly anticorrelated. This constraint may make the five conflict types represented in a unidimensional space (e.g., a circle) embedded in a 2D space. This limitation also means we cannot conclusively rule out the possibility of a real unidimensional space driven solely by spatial Stroop or Simon conflicts. However, this appears unlikely as it would imply that our manipulation of conflict types merely represented varying levels of a single conflict, akin to manipulating task difficulty when everything else being equal. If task difficulty were the primary variable, we would expect to see greater representational similarity between task conditions of similar difficulty, such as the Stroop and Simon conditions, which demonstrates comparable congruency effects (see <xref ref-type="fig" rid="fig2s1">Figure 2âfigure supplement 1</xref>). Contrary to this, our findings reveal that the Stroop-only and Simon-only conditions exhibit the lowest representational similarity (<xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref>). Furthermore, <xref ref-type="bibr" rid="bib36">Fu et al., 2022</xref> have shown that the representation of mixtures of Simon and Flanker conflicts was compositional, rather than reflecting single dimension, which also applies to our cases. Future studies may test the 2D cognitive space with fully independent conditions. A possible improvement to our current design would be to include left, right, up, and down arrows represented in a grid formation across four spatially separate quadrants, with each arrow mapped to its own response button. Additionally, our study is not a comprehensive test of the cognitive space hypothesis but aimed primarily to provide original evidence for the geometry of cognitive space in representing conflict information in cognitive control. Future research should examine other aspects of the cognitive space such as its dimensionality, its applicability to other conflict tasks such as Eriksen Flanker task, and its relevance to other cognitive abilities, such as cognitive flexibility and learning.</p><p>In sum, we showed that the cognitive control can be parametrically encoded in the right dlPFC and guides cognitive control to adjust goal-directed behavior. This finding suggests that different cognitive control states may be encoded in an abstract cognitive space, which reconciles the long-standing debate between the domain-general and domain-specific views of cognitive control and provides a parsimonious and more broadly applicable framework for understanding how our brains efficiently and flexibly represents multiple task settings.</p></sec></sec><sec id="s4" sec-type="materials|methods"><title>Materials and methods</title><sec id="s4-1"><title>Subjects</title><p>In experiment 1, we enrolled 33 college students (ages 19â28 y, average 21.5 Â± 2.3 y; 19 males). In experiment 2, 36 college students were recruited, one of which was excluded due to not following task instructions. The final sample of experiment 2 consisted of 35 participants (ages 19â29 y, average 22.3 Â± 2.5 y; 17 males). The sample sizes were determined based on our previous study (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>). All participants reported no history of psychiatric or neurological disorders and were right-handed, with normal or corrected-to-normal vision. The experiments were approved by the Institutional Review Board of the Institute of Psychology, Chinese Academy of Science (approval #H19036). Informed consent was obtained from all subjects.</p></sec><sec id="s4-2"><title>Method details</title><sec id="s4-2-1"><title>Experiment 1</title><sec id="s4-2-1-1"><title>Experimental design</title><p>We adopted a modified spatial StroopâSimon task (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>; <xref ref-type="fig" rid="fig1">Figure 1</xref>). The task was programmed with E-prime 2.0 (Psychological Software Tools, Inc). The stimulus was an upward or downward black arrow (visual angle of ~1Â°), displayed on a 17-inch LCD monitor with a viewing distance of ~60 cm. The arrow appeared inside a gray square at 1 of 10 locations with the same distance from the center of the screen, including two horizontal (left and right), two vertical (top and bottom), and six corner (orientations of 22.5Â°, 45Â°, and 67.5Â°) locations. The distance from the arrow to the screen center was approximately 3Â°. To dissociate orientation of stimulus locations and conflict types (see below), participants were randomly assigned to two sets of stimulus locations (one included top-right and bottom-left quadrants, and the other included top-left and bottom-right quadrants).</p><p>Each trial started with a fixation cross displayed in the center for 100â300 ms, followed by the arrow for 600 ms and another fixation cross for 1100â1300 ms (the total trial length was fixed at 2000 ms). Participants were instructed to respond to the pointing direction of the arrow by pressing a left or right button and to ignore its location. The mapping between the arrow orientations and the response buttons was counterbalanced across participants. The task design introduced two possible sources of conflicts: on one hand, the direction of the arrow is either congruent or incongruent with the vertical location of the arrow, thus introducing a spatial Stroop conflict (<xref ref-type="bibr" rid="bib60">Lu and Proctor, 1995</xref>; <xref ref-type="bibr" rid="bib62">MacLeod, 1991</xref>), which contains the dimensional overlap between task-relevant stimulus and task-irrelevant stimulus (<xref ref-type="bibr" rid="bib53">Kornblum et al., 1990</xref>) on the other hand, the response (left or right button) is either congruent or incongruent with the horizontal location of the arrow, thus introducing a Simon conflict (<xref ref-type="bibr" rid="bib60">Lu and Proctor, 1995</xref>; <xref ref-type="bibr" rid="bib83">Simon and Small, 1969</xref>), which contains the dimensional overlap between task-irrelevant stimulus and response (<xref ref-type="bibr" rid="bib53">Kornblum et al., 1990</xref>). Therefore, the five polar orientations of the stimulus location (from 0 to 90Â°) defined five unique combinations of spatial Stroop and Simon conflicts, with more similar orientations having more similar composition of conflicts. More generally, the spatial orientation of the arrow location relative to the center of the screen forms a cognitive space of different blending of spatial Stroop and Simon conflicts.</p><p>The formal task consisted of 30 runs of 101 trials each, divided into three sessions of 10 runs each. The participants completed one session each time and all three sessions within 1 wk. Before each session, the participants performed training blocks of 20 trials repeatedly until the accuracy reached 90% in the most recent block. The trial sequences of the formal task were pseudo-randomly generated to ensure that each of the task conditions and their transitions occurred with equal number of trials.</p></sec></sec></sec><sec id="s4-3"><title>Experiment 2</title><sec id="s4-3-1"><title>Experimental design</title><p>The apparatus, stimuli, and procedure were identical to experiment 1 except for the changes below. The stimuli were back projected onto a screen (with viewing angle being ~3.9Â° between the arrow and the center of the screen) behind the subject and viewed via a surface mirror mounted onto the head coil. Due to the time constraints of fMRI scanning, the trial numbers decreased to a total of 340, divided into two runs with 170 trials each. To obtain a better hemodynamic model fitting, we generated two pseudo-random sequences optimized with a genetic algorithm (<xref ref-type="bibr" rid="bib90">Wager and Nichols, 2003</xref>) conducted by the NeuroDesign package (<xref ref-type="bibr" rid="bib24">Durnez et al., 2018</xref>). In detail, two sequences of 170 trials each were generated independently with the NeuroDesign package (<xref ref-type="bibr" rid="bib24">Durnez et al., 2018</xref>). Each sequence was initialized as 10 consecutive sub-blocks of each condition (incongruent and congruent) for each conflict type (Stroop, St<sub>H</sub>Sm<sub>L</sub>, St<sub>M</sub>Sm<sub>M</sub>, St<sub>L</sub>Sm<sub>H</sub>, and Simon). The contrasts of interest were the main effect of congruency (i.e., [1 â1 1 â1 1 â1 1 â1 1 â1]) and the parametric effect (i.e., [â2 â2 â1 â1 0 0 1 1 2 2]). The order was optimized after 5000 cycles of crossover, mutation, immigration, fitness, and natural selection. The final number of trials for different conflict types varied from 64 to 73. In addition, we added 6 s of fixation before each run to allow the stabilization of the hemodynamic signal and 20 s after each run to allow the signal to drop to the baseline.</p><p>Before scanning, participants performed two practice sessions. The first one contained 10 trials of center-displayed arrow and the second one contained 32 trials using the same design as the main task. They repeated both sessions until their performance accuracy for each session reached 90%, after which the scanning began.</p></sec></sec><sec id="s4-4"><title>fMRI Image acquisition and preprocessing</title><p>Functional imaging was performed on a 3T GE scanner (Discovery MR750) using echo-planar imaging (EPI) sensitive to BOLD contrast [in-plane resolution of 3.5 Ã 3.5 mm<sup>2</sup>, 64 Ã 64 matrix, 37 slices with a thickness of 3.5 mm and no interslice skip, repetition time (TR) of 2000 ms, echo-time (TE) of 30 ms, and a flip angle of 90Â°]. In addition, a sagittal T1-weighted anatomical image was acquired as a structural reference scan, with a total of 256 slices at a thickness of 1.0 mm with no gap and an in-plane resolution of 1.0 Ã 1.0 mm<sup>2</sup>.</p><p>Results included in this manuscript come from preprocessing performed using fMRIPrep 20.2.0 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_016216">SCR_016216</ext-link>; <xref ref-type="bibr" rid="bib28">Esteban et al., 2019</xref> ), which is based on Nipype 1.5.1 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002502">SCR_002502</ext-link>; <xref ref-type="bibr" rid="bib39">Gorgolewski et al., 2011</xref>).</p><sec id="s4-4-1"><title>Anatomical data preprocessing</title><p>The T1-weighted (T1w) image was corrected for intensity non-uniformity (INU) with N4BiasFieldCorrection (<xref ref-type="bibr" rid="bib86">Tustison et al., 2010</xref>), distributed with ANTs 2.3.3 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_004757">SCR_004757</ext-link>; <xref ref-type="bibr" rid="bib5">Avants et al., 2008</xref>), and used as T1w-reference throughout the workflow. The T1w-reference was then skull-stripped with a Nipype implementation of the antsBrainExtraction.sh workflow (from ANTs), using OASIS30ANTs as target template. Brain tissue segmentation of cerebrospinal fluid (CSF), white matter (WM), and gray matter (GM) was performed on the brain-extracted T1w using fast (FSL 5.0.9, RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_002823">SCR_002823</ext-link>; <xref ref-type="bibr" rid="bib96">Zhang et al., 2001</xref>). Volume-based spatial normalization to one standard space (MNI152NLin2009cAsym) was performed through nonlinear registration with antsRegistration (ANTs 2.3.3) using brain-extracted versions of both T1w reference and the T1w template. The following template was selected for spatial normalization: ICBM 152 Nonlinear Asymmetrical template version 2009c (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_008796">SCR_008796</ext-link>; TemplateFlow ID: MNI152NLin2009cAsym; <xref ref-type="bibr" rid="bib30">Fonov et al., 2009</xref>).</p></sec><sec id="s4-4-2"><title>Functional data preprocessing</title><p>Before preprocessing, the first three volumes of the functional images were removed due to the instability of the signal at the beginning of the scan. For each of the five BOLD runs found per subject (across all tasks and sessions), the following preprocessing was performed. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Susceptibility distortion correction (SDC) was omitted. The BOLD reference was then co-registered to the T1w reference using flirt (FSL 5.0.9; <xref ref-type="bibr" rid="bib45">Jenkinson and Smith, 2001</xref>) with the boundary-based registration (<xref ref-type="bibr" rid="bib41">Greve and Fischl, 2009</xref>) cost function. Co-registration was configured with nine degrees of freedom to account for distortions remaining in the BOLD reference. Head-motion parameters with respect to the BOLD reference (transformation matrices, and six corresponding rotation and translation parameters) are estimated before any spatiotemporal filtering using mcflirt (FSL 5.0.9; <xref ref-type="bibr" rid="bib46">Jenkinson et al., 2002</xref>). BOLD runs were slice-time corrected using 3dTshift from AFNI 20160207 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_005927">SCR_005927</ext-link>; <xref ref-type="bibr" rid="bib46">Jenkinson et al., 2002</xref>). The BOLD time series (including slice-timing correction when applied) were resampled onto their original, native space by applying the transforms to correct for head motion. These resampled BOLD time series will be referred to as preprocessed BOLD in original space or just preprocessed BOLD. The BOLD time series were resampled into standard space, generating a preprocessed BOLD run in MNI152NLin2009cAsym space. First, a reference volume and its skull-stripped version were generated using a custom methodology of fMRIPrep. Several confounding time series were calculated based on the preprocessed BOLD: framewise displacement (FD), DVARS, and three region-wise global signals. FD was computed using two formulations following Power (absolute sum of relative motions) (<xref ref-type="bibr" rid="bib46">Jenkinson et al., 2002</xref>) and Jenkinson (relative root mean square displacement between affines, <xref ref-type="bibr" rid="bib45">Jenkinson and Smith, 2001</xref>). FD and DVARS are calculated for each functional run, both using their implementations in Nipype (following the definitions by <xref ref-type="bibr" rid="bib74">Power et al., 2014</xref> ). The three global signals are extracted within the CSF, the WM, and the whole-brain masks. Additionally, a set of physiological regressors were extracted to allow for component-based noise correction (CompCor) (<xref ref-type="bibr" rid="bib8">Behzadi et al., 2007</xref>). Principal components are estimated after high-pass filtering the preprocessed BOLD time series (using a discrete cosine filter with 128 s cutoff) for the two CompCor variants: temporal (tCompCor) and anatomical (aCompCor). tCompCor components are then calculated from the top 2% variable voxels within the brain mask. For aCompCor, three probabilistic masks (CSF, WM, and combined CSF + WM) are generated in anatomical space. The implementation differs from that of Behzadi et al. in that instead of eroding the masks by 2 pixels on BOLD space, the aCompCor masks are subtracted a mask of pixels that likely contain a volume fraction of GM. This mask is obtained by thresholding the corresponding partial volume map at 0.05, and it ensures components are not extracted from voxels containing a minimal fraction of GM. Finally, these masks are resampled into BOLD space and binarized by thresholding at 0.99 (as in the original implementation). Components are also calculated separately within the WM and CSF masks. For each CompCor decomposition, the k components with the largest singular values are retained, such that the retained components time series are sufficient to explain 50% of variance across the nuisance mask (CSF, WM, combined, or temporal). The remaining components are dropped from consideration. The head-motion estimates calculated in the correction step were also placed within the corresponding confounds file. The confound time series derived from head-motion estimates and global signals were expanded with the inclusion of temporal derivatives and quadratic terms for each (<xref ref-type="bibr" rid="bib8">Behzadi et al., 2007</xref>). Frames that exceeded a threshold of 0.5 mm FD or 1.5 standardized DVARS were annotated as motion outliers. All resamplings can be performed with a single interpolation step by composing all the pertinent transformations (i.e., head-motion transform matrices, susceptibility distortion correction when available, and co-registrations to anatomical and output spaces). Gridded (volumetric) resamplings were performed using antsApplyTransforms (ANTs), configured with Lanczos interpolation to minimize the smoothing effects of other kernels (<xref ref-type="bibr" rid="bib55">Lanczos, 1964</xref>). Non-gridded (surface) resamplings were performed using mri_vol2surf (FreeSurfer).</p><p>Many internal operations of fMRIPrep use Nilearn 0.6.2 (RRID:<ext-link ext-link-type="uri" xlink:href="https://identifiers.org/RRID/RRID:SCR_001362">SCR_001362</ext-link>; <xref ref-type="bibr" rid="bib1">Abraham et al., 2014</xref>), mostly within the functional processing workflow. For more details of the pipeline, see the section corresponding to workflows in fMRIPrepâs documentation.</p><p>After preprocessing, we resampled the functional data to a spatial resolution of 3 Ã 3 Ã 3 mm<sup>3</sup>. All analyses were conducted in volumetric space, and surface maps are produced with <ext-link ext-link-type="uri" xlink:href="https://www.humanconnectome.org/software/connectome-workbench">Connectome Workbench</ext-link> for display purpose only.</p></sec></sec><sec id="s4-5"><title>Quantification and statistical analysis</title><sec id="s4-5-1"><title>Behavioral analysis</title><sec id="s4-5-1-1"><title>Experiment 1</title><p>RT and ER were the two dependent variables analyzed. As for RTs, we excluded the first trial of each block (0.9%, for CSE analysis only), error trials (3.8%), trials with RTs beyond 3 SDs or shorter than 200 ms (1.3%), and post-error trials (3.4%). For the ER analysis, the first trial of each block and trials after an error were excluded. To exclude the possible influence of response repetition, we centered the RT and ER data within the response repetition and response alternation conditions separately by replacing condition-specific mean with the global mean for each subject.</p><p>To examine the modulation of conflict similarity on the CSE, we organized trials based on a 5 (previous trial conflict type) Ã 5 (current trial conflict type) Ã 2 (previous trial congruency) Ã 2 (current trial congruency) factorial design. As conflict similarity is commutive between conflict types, we expected the previous by current trial conflict-type factorial design to be a symmetrical (e.g., a conflict 1âconflict 2 sequence in theory has the same conflict similarity modulation effect as a conflict 2âconflict 1 sequence), resulting in a total of 15 conditions left for the first two factors of the design (i.e., previous Ã current trial conflict type). For each previous Ã current trial conflict-type condition, the conflict similarity between the two trials can be quantified as the cosine of their angular difference. In the current design, there were five possible angular difference levels (0Â°, 22.5Â°, 42.5Â°, 67.5Â°, and 90Â°, see <xref ref-type="fig" rid="fig1">Figure 1C</xref>). We further coded the previous by current trial congruency conditions (hereafter abbreviated as CSE conditions) as CC, CI, IC, and II, with the first and second letters encoding the congruency (C) or incongruency (I) on the previous and current trials, respectively. As the CSE is operationalized as the interaction between previous and current trial congruency, it can be rewritten as a contrast of (CI â CC) â (II â IC). In other words, the load of CSE on CI, CC, II, and IC conditions is 1, â1, â1, and 1, respectively. To estimate the modulation of conflict similarity on the CSE, we built a regressor by calculating the Kronecker product of the conflict similarity scores of the 15 previous Ã current trial conflict similarity conditions and the CSE loadings of previous Ã current trial congruency conditions. This regressor was regressed against RT and ER data separately, which were normalized across participants and CSE conditions. The regression was performed using a linear mixed-effect model, with the intercept and the slope of the regressor for the modulation of conflict similarity on the CSE as random effects (across both participants and the four CSE conditions). As a control analysis, we built a similar two-stage model (<xref ref-type="bibr" rid="bib95">Yang et al., 2021</xref>). In the first stage, the CSE [i.e., (CI â CC) â (II â IC)] for each of the previous Ã current trial conflict similarity condition was computed. In the second stage, CSE was used as the dependent variable and was predicted using conflict similarity across the 15 previous Ã current trial conflict-type conditions. The regression was also performed using a linear mixed-effect model with the intercept and the slope of the regressor for the modulation of conflict similarity on the CSE as random effects (across participants).</p></sec><sec id="s4-5-1-2"><title>Experiment 2</title><p>Behavioral data was analyzed using the same linear mixed-effect model as experiment 1, with all the CC, CI, IC, and II trials as the dependent variable. In addition, to test whether fMRI activity patterns may explain the behavioral representations differently in congruent and incongruent conditions, we conducted the same analysis to measure behavioral modulation of conflict similarity on the CSE using congruent (CC and IC) and incongruent (CI and II) trials separately.</p></sec></sec></sec><sec id="s4-6"><title>Estimation of fMRI activity with univariate general linear model (GLM)</title><p>To estimate voxel-wise fMRI activity for each of the experimental conditions, the preprocessed fMRI data of each run were analyzed with the GLM. We conducted three GLMs for different purposes. GLM1 aimed to validate the design of our study by replicating the engagement of frontoparietal activities in conflict processing documented in previous studies (<xref ref-type="bibr" rid="bib47">Jiang and Egner, 2014</xref>; <xref ref-type="bibr" rid="bib57">Li et al., 2017</xref>) and explore the cognitive space-related regions that were parametrically modulated by the conflict type. Preprocessed functional images were smoothed using a 6 mm FWHM Gaussian kernel. We included incongruent and congruent conditions as main regressors and appended a parametric modulator for each condition. The modulation parameters for Stroop, St<sub>H</sub>Sm<sub>L</sub>, St<sub>M</sub>Sm<sub>M</sub>, St<sub>L</sub>Sm<sub>H</sub>, and Simon trials were â2, â1, 0, 1, and 2, respectively. In addition, we also added event-related nuisance regressors, including error/missed trials, outlier trials (slower than 3 SDs of the mean or faster than 200 ms) and trials within two TRs of significant head motion (i.e., outlier TRs, defined as standard DVARS &gt; 1.5 or FD &gt; 0.9 mm from previous TR) (<xref ref-type="bibr" rid="bib49">Jiang et al., 2020</xref>). On average, there were 1.2 outlier TRs for each run. These regressors were convolved with a canonical hemodynamic response function (HRF) in SPM 12 (<ext-link ext-link-type="uri" xlink:href="http://www.fil.ion.ucl.ac.uk/spm">http://www.fil.ion.ucl.ac.uk/spm</ext-link>). We further added volume-level nuisance regressors, including the six head-motion parameters, the global signal, the white matter signal, the CSF signal, and outlier TRs. Low-frequency signal drifts were filtered using a cutoff period of 128 s. The two runs were regarded as different sessions and incorporated into a single GLM to get more power. This yielded two beta maps (i.e., a main effect map and a parametric modulation map) for the incongruent and congruent conditions, respectively, and for each subject. At the group level, paired <italic>t</italic>-tests were conducted between incongruent and congruent conditions, one for the main effect and the other for the parametric modulation effect. Since the spatial Stroop and Simon conflicts change in the opposite direction to each other, a positive modulation effect would reflect a higher brain activation when there is more Simon conflict, and a negative modulation effect would reflect a higher brain activation for more spatial Stroop conflict. Results were corrected with the probabilistic threshold-free cluster enhancement (pTFCE) and then thresholded by 3dClustSim function in AFNI (<xref ref-type="bibr" rid="bib20">Cox and Hyde, 1997</xref>) with voxel-wise p&lt;0.001 and cluster-wise p&lt;0.05, both one-tailed. To visualize the parametric modulation effects, we conducted a similar GLM (GLM2), except we used incongruent and congruent conditions from each conflict type as separate regressors with no parametric modulation. Then we extracted Î²-coefficients for each regressor and each participant with regions observed in GLM1 as regions of interest, and finally got the incongruentâcongruent contrasts for each conflict type at the individual level. We reported the results in <xref ref-type="fig" rid="fig3">Figure 3</xref>, <xref ref-type="table" rid="table1">Table 1</xref>, and <xref ref-type="fig" rid="fig3s1">Figure 3âfigure supplement 1</xref>. Visualization of the univoxel results was made by the MRIcron (<ext-link ext-link-type="uri" xlink:href="https://www.mccauslandcenter.sc.edu/mricro/mricron/">https://www.mccauslandcenter.sc.edu/mricro/mricron/</ext-link>).</p><p>The GLM3 aimed to prepare for the RSA (see below). There were several differences compared to GLM1. The unsmoothed functional images after preprocessing were used. This model included 20 event-related regressors, one for each of the 5 (conflict type) Ã 2 (congruency condition) Ã 2 (arrow direction) conditions. The event-related nuisance regressors were similar to GLM1, but with additional regressors of response repetition and post-error trials to account for the nuisance inter-trial effects. To fully expand the variance, we conducted one GLM analysis for each run. After this procedure, a voxel-wise fMRI activation map was obtained per condition, run and subject.</p></sec><sec id="s4-7"><title>Representational similarity analysis</title><p>To measure the neural representation of conflict similarity, we adopted the RSA. RSAs were conducted on each of the 360 cortical regions of a volumetric version of the MMP cortical atlas (<xref ref-type="bibr" rid="bib38">Glasser et al., 2016</xref>). To de-correlate the factors of conflict type and orientation of stimulus location, we leveraged the between-subject manipulation of stimulus locations and conducted RSA in a cross-subject fashion (<xref ref-type="fig" rid="fig4">Figure 4</xref>). Previous studies (e.g., <xref ref-type="bibr" rid="bib15">Chen et al., 2017a</xref>) have demonstrated that consistent multivoxel activation patterns exist across individuals, and successful applications of cross-subject RSA (see review by <xref ref-type="bibr" rid="bib34">Freund et al., 2021b</xref>) and cross-subject decoding approaches <xref ref-type="bibr" rid="bib48">Jiang et al., 2016</xref>; <xref ref-type="bibr" rid="bib85">Tusche et al., 2016</xref> have also been reported. The Î² estimates from GLM3 were noise-normalized by dividing the original Î²-coefficients by the square root of the covariance matrix of the error terms (<xref ref-type="bibr" rid="bib69">Nili et al., 2014</xref>). For each cortical region, we calculated the Pearsonâs correlations between fMRI activity patterns for each run and each subject, yielding a 1400 (20 conditions Ã 2 runs Ã 35 participants) Ã 1400 RSM. The correlations were calculated in a cross-voxel manner using the fMRI activation maps obtained from GLM3 described in the previous section. We excluded within-subject cells from the RSM (thus also excluding the within-run similarity as suggested by <xref ref-type="bibr" rid="bib91">Walther et al., 2016</xref>), and the remaining cells were converted into a vector, which was then z-transformed and submitted to a linear mixed-effect model as the dependent variable. The linear mixed-effect model also included regressors of conflict similarity and orientation similarity. Importantly, conflict similarity was based on how Simon and spatial Stroop conflicts are combined and hence was calculated by first rotating all subjectâs stimulus location to the top-right and bottom-left quadrants, whereas orientation was calculated using original stimulus locations. As a result, the regressors representing conflict similarity and orientation similarity were de-correlated (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Similarity between two conditions was measured as the cosine value of the angular difference. Other regressors included a target similarity regressor (i.e., whether the arrow directions were identical), a response similarity regressor (i.e., whether the correct responses were identical); a spatial Stroop distractor regressor (i.e., vertical distance between two stimulus locations); and a Simon distractor regressor (i.e., horizontal distance between two stimulus locations). Additionally, we also included a regressor denoting the similarity of group (i.e., whether two conditions are within the same subject group, according to the stimulusâresponse mapping). We also added two regressors including ROI-mean fMRI activations for each condition of the pair to remove the possible univoxel influence on the RSM. A last term was the intercept. To control the artifact due to dependence of the correlation pairs sharing the same subject, we included crossed random effects (i.e., row-wise and column-wise random effects) for the intercept, conflict similarity, orientation, and the group factors (<xref ref-type="bibr" rid="bib16">Chen et al., 2017b</xref>). Individual effects for each regressor were also extracted from the model for brainâbehavioral correlation analyses. In brainâbehavioral analyses, only the RT was used as behavioral measure to be consistent with the fMRI results, where the error trials were regressed out.</p><p>The statistical significance of these Î² estimates was based on the outputs of the mixed-effect model estimated with the âfitlmeâ function in MATLAB 2022a. We adjusted the <italic>t</italic> and p values with the degrees of freedom calculated through the Satterthwaite approximation method (<xref ref-type="bibr" rid="bib79">Satterthwaite, 1946</xref>). Of note, this approach was applied to all the mixed-effect model analyses in this study. Multiple comparison correction was applied with the Bonferroni approach across all cortical regions at the p&lt;0.05 level. To test whether the representation strengths are different between congruent and incongruent conditions, we also conducted the RSA using only congruent (RDM_C) and incongruent (RDM_I) trials separately. The contrast analysis was achieved by an additional model with both RDM_C and RDM_I included, adding the congruency and the interaction between conflict type (and orientation) and congruency as both fixed and random factors. The difference between incongruent and congruent representations was indicated by a significant interaction effect. To visualize the difference, we plotted the effect-related patterns (the predictor multiplied by the slope, plus the residual) as a function of the similarity levels (<xref ref-type="fig" rid="fig6">Figure 6D</xref>), and a summary RSM for incongruent and congruent conditions, respectively (<xref ref-type="fig" rid="fig6s1">Figure 6âfigure supplement 1</xref>).</p></sec><sec id="s4-8"><title>Model comparison and representational dimensionality</title><p>To estimate whether the right 8C specifically encodes the cognitive space, rather than the domain-general or domain-specific structures, we conducted two more RSAs. We replaced the cognitive space-based conflict similarity matrix in the RSA we reported above (hereafter referred to as the cognitive-space model) with one of the alternative model matrices, with all other regressors equal. The domain-general model treats each conflict type as equivalent, so each two conflict types only differ in the magnitude of their conflict. Therefore, we defined the domain-general matrix as the absolute difference in their congruency effects indexed by the group-averaged RT in experiment 2. Then the z-scored model vector was sign-flipped to reflect similarity instead of distance. The domain-specific model treats each conflict type differently, so we used a diagonal matrix, with within-conflict-type similarities being 1 and all cross-conflict-type similarities being 0.</p><p>To better capture the dimensionality of the representational space, we estimated its dimensionality using the participation ratio (<xref ref-type="bibr" rid="bib44">Ito and Murray, 2023</xref>). Since we excluded the within-subject cells from the whole RSM, the whole RSM is an incomplete matrix and could not be used. To resolve this issue, we averaged the cells corresponding to each pair of conflict types to obtain an averaged 5 Ã 5 RSM matrix, similar to the matrix shown in <xref ref-type="fig" rid="fig1">Figure 1C</xref>. We then estimated the participation ratio using the formula:<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mstyle displaystyle="true" scriptlevel="0"><mml:mrow><mml:mi>dim</mml:mi></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:msup><mml:mo stretchy="false">)</mml:mo><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow><mml:mrow><mml:munderover><mml:mo>â</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi></mml:mrow></mml:munderover><mml:msubsup><mml:mi>Î»</mml:mi><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mstyle></mml:mrow></mml:math></disp-formula></p><p>where Î»<sub>i</sub> is the eigenvalue of the RSM and <italic>m</italic> is the number of eigenvalues.</p></sec><sec id="s4-9"><title>Representational connectivity analysis</title><p>To explore the possible relevance between the conflict type and the orientation effects, we conducted representational connectivity (<xref ref-type="bibr" rid="bib54">Kriegeskorte et al., 2008</xref>) between regions showing evidence encoding conflict similarity and orientation similarity. We hypothesized that this relationship should exist at the within-subject level, so we conducted this analysis using within-subject RSMs excluding the diagonal. Specifically, the z-transformed RSM vector of each region was extracted and submitted to a mixed linear model, with the RSM of the conflict-type region (i.e., the right 8C) as the dependent variable, and the RSM of one of the orientation regions (e.g., right V2) as the predictor. Intercept and the slope of the regressor were set as random effects at the subject level. The mixed-effect model was conducted for each pair of regions, respectively. Considering there might be strong intrinsic correlations across the RSMs induced by the nuisance factors, such as the within-subject similarity, we added two sets of regions as control. First, we selected regions without showing any effects of interest (i.e., uncorrected ps&gt;0.3 for all the conflict type, orientation, congruency, target, response, spatial Stroop distractor, and Simon distractor effects). Second, we selected regions of orientation effect meeting the first but not the second criterion, to account for the potential correlation between regions of the two partly orthogonal regressors (<xref ref-type="fig" rid="fig4">Figure 4A</xref>). Regions adjacent to the orientation regions were excluded to avoid the inherent strong similarity they may share. Existence of representational connectivity was defined by a connectivity slope higher than 95% of the standard error above the mean of any control region.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Formal analysis, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Project administration, Writing â review and editing</p></fn><fn fn-type="con" id="con2"><p>Writing â review and editing</p></fn><fn fn-type="con" id="con3"><p>Writing â review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Supervision, Funding acquisition, Project administration, Writing â review and editing</p></fn><fn fn-type="con" id="con5"><p>Writing â review and editing</p></fn><fn fn-type="con" id="con6"><p>Supervision, Validation, Methodology, Writing â review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: The experiments were approved by the Institutional Review Board of the Institute of Psychology, Chinese Academy of Science (Approval #H19036). Informed consent was obtained from all subjects.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-87126-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>All data needed to evaluate the conclusions are present in the paper. Raw MRI data (in BIDS format), behavioral data and analysis codes can be accessed at <ext-link ext-link-type="uri" xlink:href="https://osf.io/4b3wd/">https://osf.io/4b3wd/</ext-link>.</p><p>The following dataset was generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Fu</surname><given-names>Z</given-names></name><name><surname>Jiang</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2022">2022</year><data-title>Conflicts are represented in a cognitive space to reconcile domain-general and domain-specific cognitive control</data-title><source>Open Science Framework</source><pub-id pub-id-type="doi">10.17605/OSF.IO/4B3WD</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>We thank Eliot Hazeltine for valuable comments on a previous version of this article. The work was supported by the National Natural Science Foundation of China and the German Research Foundation (NSFC 62061136001/DFG TRR-169) to XL and China Postdoctoral Science Foundation (2019M650884) to GY.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abraham</surname><given-names>A</given-names></name><name><surname>Pedregosa</surname><given-names>F</given-names></name><name><surname>Eickenberg</surname><given-names>M</given-names></name><name><surname>Gervais</surname><given-names>P</given-names></name><name><surname>Mueller</surname><given-names>A</given-names></name><name><surname>Kossaifi</surname><given-names>J</given-names></name><name><surname>Gramfort</surname><given-names>A</given-names></name><name><surname>Thirion</surname><given-names>B</given-names></name><name><surname>Varoquaux</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Machine learning for neuroimaging with scikit-learn</article-title><source>Frontiers in Neuroinformatics</source><volume>8</volume><elocation-id>14</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2014.00014</pub-id><pub-id pub-id-type="pmid">24600388</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Abrahamse</surname><given-names>E</given-names></name><name><surname>Braem</surname><given-names>S</given-names></name><name><surname>Notebaert</surname><given-names>W</given-names></name><name><surname>Verguts</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Grounding cognitive control in associative learning</article-title><source>Psychological Bulletin</source><volume>142</volume><fpage>693</fpage><lpage>728</lpage><pub-id pub-id-type="doi">10.1037/bul0000047</pub-id><pub-id pub-id-type="pmid">27148628</pub-id></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>AkÃ§ay</surname><given-names>Ã</given-names></name><name><surname>Hazeltine</surname><given-names>E</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Domain-specific conflict adaptation without feature repetitions</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>18</volume><fpage>505</fpage><lpage>511</lpage><pub-id pub-id-type="doi">10.3758/s13423-011-0084-y</pub-id><pub-id pub-id-type="pmid">21404129</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Assem</surname><given-names>M</given-names></name><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>A Domain-general cognitive core defined in multimodally parcellated human cortex</article-title><source>Cerebral Cortex</source><volume>30</volume><fpage>4361</fpage><lpage>4380</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhaa023</pub-id><pub-id pub-id-type="pmid">32244253</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Epstein</surname><given-names>CL</given-names></name><name><surname>Grossman</surname><given-names>M</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neurodegenerative brain</article-title><source>Medical Image Analysis</source><volume>12</volume><fpage>26</fpage><lpage>41</lpage><pub-id pub-id-type="doi">10.1016/j.media.2007.06.004</pub-id><pub-id pub-id-type="pmid">17659998</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Badre</surname><given-names>D</given-names></name><name><surname>Bhandari</surname><given-names>A</given-names></name><name><surname>Keglovits</surname><given-names>H</given-names></name><name><surname>Kikumoto</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The dimensionality of neural representations for control</article-title><source>Current Opinion in Behavioral Sciences</source><volume>38</volume><fpage>20</fpage><lpage>28</lpage><pub-id pub-id-type="doi">10.1016/j.cobeha.2020.07.002</pub-id><pub-id pub-id-type="pmid">32864401</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behrens</surname><given-names>TEJ</given-names></name><name><surname>Muller</surname><given-names>TH</given-names></name><name><surname>Whittington</surname><given-names>JCR</given-names></name><name><surname>Mark</surname><given-names>S</given-names></name><name><surname>Baram</surname><given-names>AB</given-names></name><name><surname>Stachenfeld</surname><given-names>KL</given-names></name><name><surname>Kurth-Nelson</surname><given-names>Z</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>What is a cognitive map</article-title><source>Organizing Knowledge for Flexible Behavior. Neuron</source><volume>100</volume><fpage>490</fpage><lpage>509</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2018.10.002</pub-id><pub-id pub-id-type="pmid">30359611</pub-id></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Behzadi</surname><given-names>Y</given-names></name><name><surname>Restom</surname><given-names>K</given-names></name><name><surname>Liau</surname><given-names>J</given-names></name><name><surname>Liu</surname><given-names>TT</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>A component based noise correction method (CompCor) for BOLD and perfusion based fMRI</article-title><source>NeuroImage</source><volume>37</volume><fpage>90</fpage><lpage>101</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2007.04.042</pub-id><pub-id pub-id-type="pmid">17560126</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bellmund</surname><given-names>JLS</given-names></name><name><surname>GÃ¤rdenfors</surname><given-names>P</given-names></name><name><surname>Moser</surname><given-names>EI</given-names></name><name><surname>Doeller</surname><given-names>CF</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Navigating cognition: spatial codes for human thinking</article-title><source>Science</source><volume>362</volume><elocation-id>eaat6766</elocation-id><pub-id pub-id-type="doi">10.1126/science.aat6766</pub-id><pub-id pub-id-type="pmid">30409861</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Benjamini</surname><given-names>Y</given-names></name><name><surname>Hochberg</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Controlling the false discovery rate: a practical and powerful approach to multiple testing</article-title><source>Journal of the Royal Statistical Society</source><volume>57</volume><fpage>289</fpage><lpage>300</lpage><pub-id pub-id-type="doi">10.1111/j.2517-6161.1995.tb02031.x</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Blake</surname><given-names>KR</given-names></name><name><surname>Gangestad</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>On Attenuated interactions, measurement error, and statistical power: guidelines for social and personality psychologists</article-title><source>Personality &amp; Social Psychology Bulletin</source><volume>46</volume><fpage>1702</fpage><lpage>1711</lpage><pub-id pub-id-type="doi">10.1177/0146167220913363</pub-id><pub-id pub-id-type="pmid">32208875</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name><name><surname>Carter</surname><given-names>CS</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Conflict monitoring and anterior cingulate cortex: an update</article-title><source>Trends in Cognitive Sciences</source><volume>8</volume><fpage>539</fpage><lpage>546</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2004.10.003</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braem</surname><given-names>S</given-names></name><name><surname>Abrahamse</surname><given-names>EL</given-names></name><name><surname>Duthoo</surname><given-names>W</given-names></name><name><surname>Notebaert</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>What determines the specificity of conflict adaptation? A review, critical analysis, and proposed synthesis</article-title><source>Frontiers in Psychology</source><volume>5</volume><elocation-id>1134</elocation-id><pub-id pub-id-type="doi">10.3389/fpsyg.2014.01134</pub-id><pub-id pub-id-type="pmid">25339930</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Braem</surname><given-names>S</given-names></name><name><surname>Bugg</surname><given-names>JM</given-names></name><name><surname>Schmidt</surname><given-names>JR</given-names></name><name><surname>Crump</surname><given-names>MJC</given-names></name><name><surname>Weissman</surname><given-names>DH</given-names></name><name><surname>Notebaert</surname><given-names>W</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Measuring adaptive control in conflict tasks</article-title><source>Trends in Cognitive Sciences</source><volume>23</volume><fpage>769</fpage><lpage>783</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2019.07.002</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>J</given-names></name><name><surname>Leong</surname><given-names>YC</given-names></name><name><surname>Honey</surname><given-names>CJ</given-names></name><name><surname>Yong</surname><given-names>CH</given-names></name><name><surname>Norman</surname><given-names>KA</given-names></name><name><surname>Hasson</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2017">2017a</year><article-title>Shared memories reveal shared structure in neural activity across individuals</article-title><source>Nature Neuroscience</source><volume>20</volume><fpage>115</fpage><lpage>125</lpage><pub-id pub-id-type="doi">10.1038/nn.4450</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Chen</surname><given-names>G</given-names></name><name><surname>Taylor</surname><given-names>PA</given-names></name><name><surname>Shin</surname><given-names>YW</given-names></name><name><surname>Reynolds</surname><given-names>RC</given-names></name><name><surname>Cox</surname><given-names>RWJN</given-names></name></person-group><year iso-8601-date="2017">2017b</year><article-title>Untangling the relatedness among correlations, Part II: Inter-subject correlation group analysis through linear mixed-effects modeling</article-title><source>NeuroImage</source><volume>147</volume><fpage>825</fpage><lpage>840</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2016.08.029</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Reynolds</surname><given-names>JR</given-names></name><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Repovs</surname><given-names>G</given-names></name><name><surname>Anticevic</surname><given-names>A</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Multi-task connectivity reveals flexible hubs for adaptive task control</article-title><source>Nature Neuroscience</source><volume>16</volume><fpage>1348</fpage><lpage>1355</lpage><pub-id pub-id-type="doi">10.1038/nn.3470</pub-id><pub-id pub-id-type="pmid">23892552</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Constantinescu</surname><given-names>AO</given-names></name><name><surname>OâReilly</surname><given-names>JX</given-names></name><name><surname>Behrens</surname><given-names>TEJ</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Organizing conceptual knowledge in humans with a gridlike code</article-title><source>Science</source><volume>352</volume><fpage>1464</fpage><lpage>1468</lpage><pub-id pub-id-type="doi">10.1126/science.aaf0941</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Cosmides</surname><given-names>L</given-names></name><name><surname>Tooby</surname><given-names>J</given-names></name></person-group><year iso-8601-date="1994">1994</year><chapter-title>Origins of domain specificity: the evolution of functional organization</chapter-title><person-group person-group-type="editor"><name><surname>Hirschfeld</surname><given-names>LA</given-names></name><name><surname>Gelman</surname><given-names>SA</given-names></name></person-group><source>Mapping the Mind: Domain Specificity in Cognition and Culture</source><publisher-loc>Cambridge</publisher-loc><publisher-name>Cambridge University Press</publisher-name><fpage>85</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1017/CBO9780511752902</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cox</surname><given-names>RW</given-names></name><name><surname>Hyde</surname><given-names>JS</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Software tools for analysis and visualization of fMRI data</article-title><source>NMR in Biomedicine</source><volume>10</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1002/(SICI)1099-1492(199706/08)10:4/5&lt;171::AID-NBM453&gt;3.0.CO;2-L</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dagher</surname><given-names>A</given-names></name><name><surname>Owen</surname><given-names>AM</given-names></name><name><surname>Boecker</surname><given-names>H</given-names></name><name><surname>Brooks</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Mapping the network for planning: a correlational PET activation study with the Tower of London task</article-title><source>Brain</source><volume>122 (Pt 10)</volume><fpage>1973</fpage><lpage>1987</lpage><pub-id pub-id-type="doi">10.1093/brain/122.10.1973</pub-id><pub-id pub-id-type="pmid">10506098</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>The multiple-demand (MD) system of the primate brain: mental programs for intelligent behaviour</article-title><source>Trends in Cognitive Sciences</source><volume>14</volume><fpage>172</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2010.01.004</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duncan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The structure of cognition: attentional episodes in mind and brain</article-title><source>Neuron</source><volume>80</volume><fpage>35</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.09.015</pub-id><pub-id pub-id-type="pmid">24094101</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Blair</surname><given-names>R</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Neurodesign: optimal experimental designs for task fMRI</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/119594</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Congruency sequence effects and cognitive control</article-title><source>Cognitive, Affective, &amp; Behavioral Neuroscience</source><volume>7</volume><fpage>380</fpage><lpage>390</lpage><pub-id pub-id-type="doi">10.3758/CABN.7.4.380</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname><given-names>T</given-names></name><name><surname>Delano</surname><given-names>M</given-names></name><name><surname>Hirsch</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Separate conflict-specific cognitive control mechanisms in the human brain</article-title><source>NeuroImage</source><volume>35</volume><fpage>940</fpage><lpage>948</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2006.11.061</pub-id><pub-id pub-id-type="pmid">17276088</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Multiple conflict-driven control mechanisms in the human brain</article-title><source>Trends in Cognitive Sciences</source><volume>12</volume><fpage>374</fpage><lpage>380</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2008.07.001</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Esteban</surname><given-names>O</given-names></name><name><surname>Markiewicz</surname><given-names>CJ</given-names></name><name><surname>Blair</surname><given-names>RW</given-names></name><name><surname>Moodie</surname><given-names>CA</given-names></name><name><surname>Isik</surname><given-names>AI</given-names></name><name><surname>Erramuzpe</surname><given-names>A</given-names></name><name><surname>Kent</surname><given-names>JD</given-names></name><name><surname>Goncalves</surname><given-names>M</given-names></name><name><surname>DuPre</surname><given-names>E</given-names></name><name><surname>Snyder</surname><given-names>M</given-names></name><name><surname>Oya</surname><given-names>H</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name><name><surname>Wright</surname><given-names>J</given-names></name><name><surname>Durnez</surname><given-names>J</given-names></name><name><surname>Poldrack</surname><given-names>RA</given-names></name><name><surname>Gorgolewski</surname><given-names>KJ</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>fMRIPrep: a robust preprocessing pipeline for functional MRI</article-title><source>Nature Methods</source><volume>16</volume><fpage>111</fpage><lpage>116</lpage><pub-id pub-id-type="doi">10.1038/s41592-018-0235-4</pub-id><pub-id pub-id-type="pmid">30532080</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Flesch</surname><given-names>T</given-names></name><name><surname>Juechems</surname><given-names>K</given-names></name><name><surname>Dumbalska</surname><given-names>T</given-names></name><name><surname>Saxe</surname><given-names>A</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Orthogonal representations for robust context-dependent task performance in brains and neural networks</article-title><source>Neuron</source><volume>110</volume><fpage>1258</fpage><lpage>1270</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2022.01.005</pub-id><pub-id pub-id-type="pmid">35085492</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fonov</surname><given-names>VS</given-names></name><name><surname>Evans</surname><given-names>AC</given-names></name><name><surname>McKinstry</surname><given-names>RC</given-names></name><name><surname>Almli</surname><given-names>CR</given-names></name><name><surname>Collins</surname><given-names>DL</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Unbiased nonlinear average age-appropriate brain templates from birth to adulthood</article-title><source>NeuroImage</source><volume>47</volume><elocation-id>S102</elocation-id><pub-id pub-id-type="doi">10.1016/S1053-8119(09)70884-5</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freitas</surname><given-names>AL</given-names></name><name><surname>Bahar</surname><given-names>M</given-names></name><name><surname>Yang</surname><given-names>S</given-names></name><name><surname>Banai</surname><given-names>R</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Contextual adjustments in cognitive control across tasks</article-title><source>Psychological Science</source><volume>18</volume><fpage>1040</fpage><lpage>1043</lpage><pub-id pub-id-type="doi">10.1111/j.1467-9280.2007.02022.x</pub-id><pub-id pub-id-type="pmid">18031409</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freitas</surname><given-names>AL</given-names></name><name><surname>Clark</surname><given-names>SL</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Generality and specificity in cognitive control: conflict adaptation within and across selective-attention tasks but not across selective-attention and Simon tasks</article-title><source>Psychological Research</source><volume>79</volume><fpage>143</fpage><lpage>162</lpage><pub-id pub-id-type="doi">10.1007/s00426-014-0540-1</pub-id><pub-id pub-id-type="pmid">24487727</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>MC</given-names></name><name><surname>Bugg</surname><given-names>JM</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2021">2021a</year><article-title>A representational similarity analysis of cognitive control during color-word stroop</article-title><source>The Journal of Neuroscience</source><volume>41</volume><fpage>7388</fpage><lpage>7402</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2956-20.2021</pub-id><pub-id pub-id-type="pmid">34162756</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Freund</surname><given-names>MC</given-names></name><name><surname>Etzel</surname><given-names>JA</given-names></name><name><surname>Braver</surname><given-names>TS</given-names></name></person-group><year iso-8601-date="2021">2021b</year><article-title>Neural coding of cognitive control: the representational similarity analysis approach</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>622</fpage><lpage>638</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.03.011</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Friehs</surname><given-names>MA</given-names></name><name><surname>Klaus</surname><given-names>J</given-names></name><name><surname>Singh</surname><given-names>T</given-names></name><name><surname>Frings</surname><given-names>C</given-names></name><name><surname>Hartwigsen</surname><given-names>G</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Perturbation of the right prefrontal cortex disrupts interference control</article-title><source>NeuroImage</source><volume>222</volume><elocation-id>117279</elocation-id><pub-id pub-id-type="doi">10.1016/j.neuroimage.2020.117279</pub-id><pub-id pub-id-type="pmid">32828926</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Fu</surname><given-names>Z</given-names></name><name><surname>Beam</surname><given-names>D</given-names></name><name><surname>Chung</surname><given-names>JM</given-names></name><name><surname>Reed</surname><given-names>CM</given-names></name><name><surname>Mamelak</surname><given-names>AN</given-names></name><name><surname>Adolphs</surname><given-names>R</given-names></name><name><surname>Rutishauser</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>The geometry of domain-general performance monitoring in the human medial frontal cortex</article-title><source>Science</source><volume>376</volume><elocation-id>9922</elocation-id><pub-id pub-id-type="doi">10.1126/science.abm9922</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gilbert</surname><given-names>CD</given-names></name><name><surname>Li</surname><given-names>W</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Top-down influences on visual processing</article-title><source>Nature Reviews. Neuroscience</source><volume>14</volume><fpage>350</fpage><lpage>363</lpage><pub-id pub-id-type="doi">10.1038/nrn3476</pub-id><pub-id pub-id-type="pmid">23595013</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Glasser</surname><given-names>MF</given-names></name><name><surname>Coalson</surname><given-names>TS</given-names></name><name><surname>Robinson</surname><given-names>EC</given-names></name><name><surname>Hacker</surname><given-names>CD</given-names></name><name><surname>Harwell</surname><given-names>J</given-names></name><name><surname>Yacoub</surname><given-names>E</given-names></name><name><surname>Ugurbil</surname><given-names>K</given-names></name><name><surname>Andersson</surname><given-names>J</given-names></name><name><surname>Beckmann</surname><given-names>CF</given-names></name><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>SM</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>A multi-modal parcellation of human cerebral cortex</article-title><source>Nature</source><volume>536</volume><fpage>171</fpage><lpage>178</lpage><pub-id pub-id-type="doi">10.1038/nature18933</pub-id><pub-id pub-id-type="pmid">27437579</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gorgolewski</surname><given-names>K</given-names></name><name><surname>Burns</surname><given-names>CD</given-names></name><name><surname>Madison</surname><given-names>C</given-names></name><name><surname>Clark</surname><given-names>D</given-names></name><name><surname>Halchenko</surname><given-names>YO</given-names></name><name><surname>Waskom</surname><given-names>ML</given-names></name><name><surname>Ghosh</surname><given-names>SS</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>Nipype: a flexible, lightweight and extensible neuroimaging data processing framework in python</article-title><source>Frontiers in Neuroinformatics</source><volume>5</volume><elocation-id>13</elocation-id><pub-id pub-id-type="doi">10.3389/fninf.2011.00013</pub-id><pub-id pub-id-type="pmid">21897815</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Grahek</surname><given-names>I</given-names></name><name><surname>Leng</surname><given-names>X</given-names></name><name><surname>Fahey</surname><given-names>MP</given-names></name><name><surname>Yee</surname><given-names>D</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><chapter-title>Empirical and computational evidence for Reconfiguration costs during within-task adjustments in cognitive control</chapter-title><person-group person-group-type="editor"><name><surname>Grahek</surname><given-names>I</given-names></name><name><surname>Leng</surname><given-names>X</given-names></name><name><surname>Musslick</surname><given-names>S</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><source>Proceedings of the Annual Meeting of the Cognitive Science Society</source><publisher-loc>Merced</publisher-loc><publisher-name>University of California Press</publisher-name><fpage>1</fpage><lpage>9</lpage></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Greve</surname><given-names>DN</given-names></name><name><surname>Fischl</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Accurate and robust brain image alignment using boundary-based registration</article-title><source>NeuroImage</source><volume>48</volume><fpage>63</fpage><lpage>72</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2009.06.060</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hazeltine</surname><given-names>E</given-names></name><name><surname>Lightman</surname><given-names>E</given-names></name><name><surname>Schwarb</surname><given-names>H</given-names></name><name><surname>Schumacher</surname><given-names>EH</given-names></name></person-group><year iso-8601-date="2011">2011</year><article-title>The boundaries of sequential modulations: evidence for set-level control</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>37</volume><fpage>1898</fpage><lpage>1914</lpage><pub-id pub-id-type="doi">10.1037/a0024662</pub-id><pub-id pub-id-type="pmid">21767054</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Hommel</surname><given-names>B</given-names></name><name><surname>Proctor</surname><given-names>RW</given-names></name><name><surname>Vu</surname><given-names>K-PL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>A feature-integration account of sequential effects in the Simon task</article-title><source>Psychological Research</source><volume>68</volume><fpage>1</fpage><lpage>17</lpage><pub-id pub-id-type="doi">10.1007/s00426-003-0132-y</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Ito</surname><given-names>T</given-names></name><name><surname>Murray</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Multitask representations in the human cortex transform along a sensory-to-motor hierarchy</article-title><source>Nature Neuroscience</source><volume>26</volume><fpage>306</fpage><lpage>315</lpage><pub-id pub-id-type="doi">10.1038/s41593-022-01224-0</pub-id><pub-id pub-id-type="pmid">36536240</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>A global optimisation method for robust affine registration of brain images</article-title><source>Medical Image Analysis</source><volume>5</volume><fpage>143</fpage><lpage>156</lpage><pub-id pub-id-type="doi">10.1016/s1361-8415(01)00036-6</pub-id><pub-id pub-id-type="pmid">11516708</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jenkinson</surname><given-names>M</given-names></name><name><surname>Bannister</surname><given-names>P</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Improved optimization for the robust and accurate linear registration and motion correction of brain images</article-title><source>NeuroImage</source><volume>17</volume><fpage>825</fpage><lpage>841</lpage><pub-id pub-id-type="doi">10.1006/nimg.2002.1132</pub-id></element-citation></ref><ref id="bib47"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Using neural pattern classifiers to quantify the modularity of conflict-control mechanisms in the human brain</article-title><source>Cerebral Cortex</source><volume>24</volume><fpage>1793</fpage><lpage>1805</lpage><pub-id pub-id-type="doi">10.1093/cercor/bht029</pub-id><pub-id pub-id-type="pmid">23402762</pub-id></element-citation></ref><ref id="bib48"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Summerfield</surname><given-names>C</given-names></name><name><surname>Egner</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Visual prediction error spreads across object features in human visual cortex</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>12746</fpage><lpage>12763</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1546-16.2016</pub-id><pub-id pub-id-type="pmid">27810936</pub-id></element-citation></ref><ref id="bib49"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jiang</surname><given-names>J</given-names></name><name><surname>Wang</surname><given-names>SF</given-names></name><name><surname>Guo</surname><given-names>W</given-names></name><name><surname>Fernandez</surname><given-names>C</given-names></name><name><surname>Wagner</surname><given-names>AD</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Prefrontal reinstatement of contextual task demand is predicted by separable hippocampal patterns</article-title><source>Nature Communications</source><volume>11</volume><elocation-id>2053</elocation-id><pub-id pub-id-type="doi">10.1038/s41467-020-15928-z</pub-id><pub-id pub-id-type="pmid">32345979</pub-id></element-citation></ref><ref id="bib50"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kan</surname><given-names>IP</given-names></name><name><surname>Teubner-Rhodes</surname><given-names>S</given-names></name><name><surname>Drummey</surname><given-names>AB</given-names></name><name><surname>Nutile</surname><given-names>L</given-names></name><name><surname>Krupa</surname><given-names>L</given-names></name><name><surname>Novick</surname><given-names>JM</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>To adapt or not to adapt: the question of domain-general cognitive control</article-title><source>Cognition</source><volume>129</volume><fpage>637</fpage><lpage>651</lpage><pub-id pub-id-type="doi">10.1016/j.cognition.2013.09.001</pub-id><pub-id pub-id-type="pmid">24103774</pub-id></element-citation></ref><ref id="bib51"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kikumoto</surname><given-names>A</given-names></name><name><surname>Mayr</surname><given-names>U</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Conjunctive representations that integrate stimuli, responses, and rules are critical for action selection</article-title><source>PNAS</source><volume>117</volume><fpage>10603</fpage><lpage>10608</lpage><pub-id pub-id-type="doi">10.1073/pnas.1922166117</pub-id><pub-id pub-id-type="pmid">32341161</pub-id></element-citation></ref><ref id="bib52"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kim</surname><given-names>C</given-names></name><name><surname>Chung</surname><given-names>C</given-names></name><name><surname>Kim</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Conflict adjustment through domain-specific multiple cognitive control mechanisms</article-title><source>Brain Research</source><volume>1444</volume><fpage>55</fpage><lpage>64</lpage><pub-id pub-id-type="doi">10.1016/j.brainres.2012.01.023</pub-id></element-citation></ref><ref id="bib53"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kornblum</surname><given-names>S</given-names></name><name><surname>Hasbroucq</surname><given-names>T</given-names></name><name><surname>Osman</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Dimensional overlap: cognitive basis for stimulus-response compatibility--a model and taxonomy</article-title><source>Psychological Review</source><volume>97</volume><fpage>253</fpage><lpage>270</lpage><pub-id pub-id-type="doi">10.1037/0033-295x.97.2.253</pub-id><pub-id pub-id-type="pmid">2186425</pub-id></element-citation></ref><ref id="bib54"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Mur</surname><given-names>M</given-names></name><name><surname>Bandettini</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Representational similarity analysis - connecting the branches of systems neuroscience</article-title><source>Frontiers in Systems Neuroscience</source><volume>2</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/neuro.06.004.2008</pub-id><pub-id pub-id-type="pmid">19104670</pub-id></element-citation></ref><ref id="bib55"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lanczos</surname><given-names>C</given-names></name></person-group><year iso-8601-date="1964">1964</year><article-title>Evaluation of noisy data</article-title><source>Journal of the Society for Industrial and Applied Mathematics Series B Numerical Analysis</source><volume>1</volume><fpage>76</fpage><lpage>85</lpage><pub-id pub-id-type="doi">10.1137/0701007</pub-id></element-citation></ref><ref id="bib56"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Nan</surname><given-names>W</given-names></name><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Independent processing of stimulus-stimulus and stimulus-response conflicts</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e89249</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0089249</pub-id></element-citation></ref><ref id="bib57"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Qi</surname><given-names>Y</given-names></name><name><surname>Cole</surname><given-names>MW</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Conflict detection and resolution rely on a combination of common and distinct cognitive control networks</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>83</volume><fpage>123</fpage><lpage>131</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2017.09.032</pub-id><pub-id pub-id-type="pmid">29017916</pub-id></element-citation></ref><ref id="bib58"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Banich</surname><given-names>MT</given-names></name><name><surname>Jacobson</surname><given-names>BL</given-names></name><name><surname>Tanabe</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2004">2004</year><article-title>Common and distinct neural substrates of attentional control in an integrated Simon and spatial Stroop task as assessed by event-related fMRI</article-title><source>NeuroImage</source><volume>22</volume><fpage>1097</fpage><lpage>1106</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2004.02.033</pub-id><pub-id pub-id-type="pmid">15219581</pub-id></element-citation></ref><ref id="bib59"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Liu</surname><given-names>X</given-names></name><name><surname>Park</surname><given-names>Y</given-names></name><name><surname>Gu</surname><given-names>X</given-names></name><name><surname>Fan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Dimensional overlap accounts for independence and integration of stimulusâresponse compatibility effects</article-title><source>Attention, Perception, &amp; Psychophysics</source><volume>72</volume><fpage>1710</fpage><lpage>1720</lpage><pub-id pub-id-type="doi">10.3758/APP.72.6.1710</pub-id></element-citation></ref><ref id="bib60"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lu</surname><given-names>C</given-names></name><name><surname>Proctor</surname><given-names>RW</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>The influence of irrelevant location information on performance: a review of the Simon and spatial Stroop effects</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>2</volume><fpage>174</fpage><lpage>207</lpage><pub-id pub-id-type="doi">10.3758/BF03210959</pub-id></element-citation></ref><ref id="bib61"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacDowell</surname><given-names>CJ</given-names></name><name><surname>Tafazoli</surname><given-names>S</given-names></name><name><surname>Buschman</surname><given-names>TJ</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>A Goldilocks theory of cognitive control: Balancing precision and efficiency with low-dimensional control states</article-title><source>Current Opinion in Neurobiology</source><volume>76</volume><elocation-id>102606</elocation-id><pub-id pub-id-type="doi">10.1016/j.conb.2022.102606</pub-id><pub-id pub-id-type="pmid">35870301</pub-id></element-citation></ref><ref id="bib62"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacLeod</surname><given-names>CM</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Half a century of research on the Stroop effect: an integrative review</article-title><source>Psychological Bulletin</source><volume>109</volume><fpage>163</fpage><lpage>203</lpage><pub-id pub-id-type="doi">10.1037/0033-2909.109.2.163</pub-id><pub-id pub-id-type="pmid">2034749</pub-id></element-citation></ref><ref id="bib63"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Magen</surname><given-names>H</given-names></name><name><surname>Cohen</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Modularity beyond perception: evidence from single task interference paradigms</article-title><source>Cognitive Psychology</source><volume>55</volume><fpage>1</fpage><lpage>36</lpage><pub-id pub-id-type="doi">10.1016/j.cogpsych.2006.09.003</pub-id></element-citation></ref><ref id="bib64"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Mansouri</surname><given-names>FA</given-names></name><name><surname>Buckley</surname><given-names>MJ</given-names></name><name><surname>Tanaka</surname><given-names>K</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Mnemonic function of the dorsolateral prefrontal cortex in conflict-induced behavioral adjustment</article-title><source>Science</source><volume>318</volume><fpage>987</fpage><lpage>990</lpage><pub-id pub-id-type="doi">10.1126/science.1146384</pub-id><pub-id pub-id-type="pmid">17962523</pub-id></element-citation></ref><ref id="bib65"><element-citation publication-type="book"><person-group person-group-type="author"><name><surname>Marr</surname><given-names>D</given-names></name></person-group><year iso-8601-date="1982">1982</year><source>Vision: A Computational Investigation into the Human Representation and Processing of Visual Information</source><publisher-name>Henry Holt and Co., Inc</publisher-name></element-citation></ref><ref id="bib66"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Miller</surname><given-names>EK</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>An integrative theory of prefrontal cortex function</article-title><source>Annual Review of Neuroscience</source><volume>24</volume><fpage>167</fpage><lpage>202</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.24.1.167</pub-id><pub-id pub-id-type="pmid">11283309</pub-id></element-citation></ref><ref id="bib67"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Milner</surname><given-names>B</given-names></name></person-group><year iso-8601-date="1963">1963</year><article-title>Effects of different brain lesions on card sorting - role of frontal lobes</article-title><source>Archives of Neurology</source><volume>9</volume><fpage>90</fpage><lpage>100</lpage><pub-id pub-id-type="doi">10.1001/archneur.1963.00460070100010</pub-id></element-citation></ref><ref id="bib68"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Musslick</surname><given-names>S</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Rationalizing constraints on the capacity for cognitive control</article-title><source>Trends in Cognitive Sciences</source><volume>25</volume><fpage>757</fpage><lpage>775</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2021.06.001</pub-id><pub-id pub-id-type="pmid">34332856</pub-id></element-citation></ref><ref id="bib69"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Wingfield</surname><given-names>C</given-names></name><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Su</surname><given-names>L</given-names></name><name><surname>Marslen-Wilson</surname><given-names>W</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>A toolbox for representational similarity analysis</article-title><source>PLOS Computational Biology</source><volume>10</volume><elocation-id>e1003553</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pcbi.1003553</pub-id><pub-id pub-id-type="pmid">24743308</pub-id></element-citation></ref><ref id="bib70"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Learning task-state representations</article-title><source>Nature Neuroscience</source><volume>22</volume><fpage>1544</fpage><lpage>1553</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0470-8</pub-id><pub-id pub-id-type="pmid">31551597</pub-id></element-citation></ref><ref id="bib71"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Park</surname><given-names>SA</given-names></name><name><surname>Miller</surname><given-names>DS</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ranganath</surname><given-names>C</given-names></name><name><surname>Boorman</surname><given-names>ED</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Map making: constructing, combining, and inferring on abstract cognitive maps</article-title><source>Neuron</source><volume>107</volume><fpage>1226</fpage><lpage>1238</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2020.06.030</pub-id></element-citation></ref><ref id="bib72"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Peterson</surname><given-names>BS</given-names></name><name><surname>Kane</surname><given-names>MJ</given-names></name><name><surname>Alexander</surname><given-names>GM</given-names></name><name><surname>Lacadie</surname><given-names>C</given-names></name><name><surname>Skudlarski</surname><given-names>P</given-names></name><name><surname>Leung</surname><given-names>H-C</given-names></name><name><surname>May</surname><given-names>J</given-names></name><name><surname>Gore</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>An event-related functional MRI study comparing interference effects in the Simon and Stroop tasks</article-title><source>Cognitive Brain Research</source><volume>13</volume><fpage>427</fpage><lpage>440</lpage><pub-id pub-id-type="doi">10.1016/S0926-6410(02)00054-X</pub-id></element-citation></ref><ref id="bib73"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Polk</surname><given-names>TA</given-names></name><name><surname>Drake</surname><given-names>RM</given-names></name><name><surname>Jonides</surname><given-names>JJ</given-names></name><name><surname>Smith</surname><given-names>MR</given-names></name><name><surname>Smith</surname><given-names>EE</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Attention enhances the neural processing of relevant features and suppresses the processing of irrelevant features in humans: a functional magnetic resonance imaging study of the Stroop task</article-title><source>The Journal of Neuroscience</source><volume>28</volume><fpage>13786</fpage><lpage>13792</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.1026-08.2008</pub-id><pub-id pub-id-type="pmid">19091969</pub-id></element-citation></ref><ref id="bib74"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Power</surname><given-names>JD</given-names></name><name><surname>Mitra</surname><given-names>A</given-names></name><name><surname>Laumann</surname><given-names>TO</given-names></name><name><surname>Snyder</surname><given-names>AZ</given-names></name><name><surname>Schlaggar</surname><given-names>BL</given-names></name><name><surname>Petersen</surname><given-names>SE</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Methods to detect, characterize, and remove motion artifact in resting state fMRI</article-title><source>NeuroImage</source><volume>84</volume><fpage>320</fpage><lpage>341</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.08.048</pub-id><pub-id pub-id-type="pmid">23994314</pub-id></element-citation></ref><ref id="bib75"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Reverberi</surname><given-names>C</given-names></name><name><surname>GÃ¶rgen</surname><given-names>K</given-names></name><name><surname>Haynes</surname><given-names>J-D</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Compositionality of rule representations in human prefrontal cortex</article-title><source>Cerebral Cortex</source><volume>22</volume><fpage>1237</fpage><lpage>1246</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhr200</pub-id></element-citation></ref><ref id="bib76"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ritz</surname><given-names>H</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Humans reconfigure target and distractor processing to address distinct task demands</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.09.08.459546</pub-id></element-citation></ref><ref id="bib77"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Ritz</surname><given-names>H</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2023">2023</year><article-title>Orthogonal neural encoding of targets and distractors supports multivariate cognitive control</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2022.12.01.518771</pub-id></element-citation></ref><ref id="bib78"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Rmus</surname><given-names>M</given-names></name><name><surname>Ritz</surname><given-names>H</given-names></name><name><surname>Hunter</surname><given-names>LE</given-names></name><name><surname>Bornstein</surname><given-names>AM</given-names></name><name><surname>Shenhav</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Humans can navigate complex graph structures acquired during latent learning</article-title><source>Cognition</source><volume>225</volume><elocation-id>105103</elocation-id><pub-id pub-id-type="doi">10.1016/j.cognition.2022.105103</pub-id></element-citation></ref><ref id="bib79"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Satterthwaite</surname><given-names>FE</given-names></name></person-group><year iso-8601-date="1946">1946</year><article-title>An approximate distribution of estimates of variance components</article-title><source>Biometrics</source><volume>2</volume><fpage>110</fpage><lpage>114</lpage><pub-id pub-id-type="pmid">20287815</pub-id></element-citation></ref><ref id="bib80"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schmidt</surname><given-names>JR</given-names></name><name><surname>Weissman</surname><given-names>DH</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Congruency sequence effects without feature integration or contingency learning confounds</article-title><source>PLOS ONE</source><volume>9</volume><elocation-id>e102337</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0102337</pub-id></element-citation></ref><ref id="bib81"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Schuck</surname><given-names>NW</given-names></name><name><surname>Cai</surname><given-names>MB</given-names></name><name><surname>Wilson</surname><given-names>RC</given-names></name><name><surname>Niv</surname><given-names>Y</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Human orbitofrontal cortex represents a cognitive map of state space</article-title><source>Neuron</source><volume>91</volume><fpage>1402</fpage><lpage>1412</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.08.019</pub-id></element-citation></ref><ref id="bib82"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Shenhav</surname><given-names>A</given-names></name><name><surname>Botvinick</surname><given-names>MM</given-names></name><name><surname>Cohen</surname><given-names>JD</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>The expected value of control: an integrative theory of anterior cingulate cortex function</article-title><source>Neuron</source><volume>79</volume><fpage>217</fpage><lpage>240</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2013.07.007</pub-id><pub-id pub-id-type="pmid">23889930</pub-id></element-citation></ref><ref id="bib83"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Simon</surname><given-names>JR</given-names></name><name><surname>Small</surname><given-names>AM</given-names></name></person-group><year iso-8601-date="1969">1969</year><article-title>Processing auditory information: interference from an irrelevant cue</article-title><source>The Journal of Applied Psychology</source><volume>53</volume><fpage>433</fpage><lpage>435</lpage><pub-id pub-id-type="doi">10.1037/h0028034</pub-id><pub-id pub-id-type="pmid">5366316</pub-id></element-citation></ref><ref id="bib84"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Torres-Quesada</surname><given-names>M</given-names></name><name><surname>Funes</surname><given-names>MJ</given-names></name><name><surname>LupiÃ¡Ã±ez</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Dissociating proportion congruent and conflict adaptation effects in a Simon-Stroop procedure</article-title><source>Acta Psychologica</source><volume>142</volume><fpage>203</fpage><lpage>210</lpage><pub-id pub-id-type="doi">10.1016/j.actpsy.2012.11.015</pub-id><pub-id pub-id-type="pmid">23337083</pub-id></element-citation></ref><ref id="bib85"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tusche</surname><given-names>A</given-names></name><name><surname>BÃ¶ckler</surname><given-names>A</given-names></name><name><surname>Kanske</surname><given-names>P</given-names></name><name><surname>Trautwein</surname><given-names>F-M</given-names></name><name><surname>Singer</surname><given-names>T</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Decoding the charitable brain: empathy, perspective taking, and attention shifts differentially predict altruistic giving</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>4719</fpage><lpage>4732</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.3392-15.2016</pub-id><pub-id pub-id-type="pmid">27122031</pub-id></element-citation></ref><ref id="bib86"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Tustison</surname><given-names>NJ</given-names></name><name><surname>Avants</surname><given-names>BB</given-names></name><name><surname>Cook</surname><given-names>PA</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Egan</surname><given-names>A</given-names></name><name><surname>Yushkevich</surname><given-names>PA</given-names></name><name><surname>Gee</surname><given-names>JC</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>N4ITK: improved N3 bias correction</article-title><source>IEEE Transactions on Medical Imaging</source><volume>29</volume><fpage>1310</fpage><lpage>1320</lpage><pub-id pub-id-type="doi">10.1109/TMI.2010.2046908</pub-id><pub-id pub-id-type="pmid">20378467</pub-id></element-citation></ref><ref id="bib87"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaidya</surname><given-names>AR</given-names></name><name><surname>Jones</surname><given-names>HM</given-names></name><name><surname>Castillo</surname><given-names>J</given-names></name><name><surname>Badre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Neural representation of abstract task structure during generalization</article-title><source>eLife</source><volume>10</volume><elocation-id>e63226</elocation-id><pub-id pub-id-type="doi">10.7554/eLife.63226</pub-id><pub-id pub-id-type="pmid">33729156</pub-id></element-citation></ref><ref id="bib88"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vaidya</surname><given-names>AR</given-names></name><name><surname>Badre</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2022">2022</year><article-title>Abstract task representations for inference and control</article-title><source>Trends in Cognitive Sciences</source><volume>26</volume><fpage>484</fpage><lpage>498</lpage><pub-id pub-id-type="doi">10.1016/j.tics.2022.03.009</pub-id><pub-id pub-id-type="pmid">35469725</pub-id></element-citation></ref><ref id="bib89"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Vanderhasselt</surname><given-names>M-A</given-names></name><name><surname>De Raedt</surname><given-names>R</given-names></name><name><surname>Baeken</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2009">2009</year><article-title>Dorsolateral prefrontal cortex and stroop performance: tackling the lateralization</article-title><source>Psychonomic Bulletin &amp; Review</source><volume>16</volume><fpage>609</fpage><lpage>612</lpage><pub-id pub-id-type="doi">10.3758/PBR.16.3.609</pub-id></element-citation></ref><ref id="bib90"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wager</surname><given-names>TD</given-names></name><name><surname>Nichols</surname><given-names>TE</given-names></name></person-group><year iso-8601-date="2003">2003</year><article-title>Optimization of experimental design in fMRI: a general framework using a genetic algorithm</article-title><source>NeuroImage</source><volume>18</volume><fpage>293</fpage><lpage>309</lpage><pub-id pub-id-type="doi">10.1016/s1053-8119(02)00046-0</pub-id><pub-id pub-id-type="pmid">12595184</pub-id></element-citation></ref><ref id="bib91"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Walther</surname><given-names>A</given-names></name><name><surname>Nili</surname><given-names>H</given-names></name><name><surname>Ejaz</surname><given-names>N</given-names></name><name><surname>Alink</surname><given-names>A</given-names></name><name><surname>Kriegeskorte</surname><given-names>N</given-names></name><name><surname>Diedrichsen</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Reliability of dissimilarity measures for multi-voxel pattern analysis</article-title><source>NeuroImage</source><volume>137</volume><fpage>188</fpage><lpage>200</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2015.12.012</pub-id><pub-id pub-id-type="pmid">26707889</pub-id></element-citation></ref><ref id="bib92"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wang</surname><given-names>K</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Wang</surname><given-names>H</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Temporal and spectral profiles of stimulus-stimulus and stimulus-response conflict processing</article-title><source>NeuroImage</source><volume>89</volume><fpage>280</fpage><lpage>288</lpage><pub-id pub-id-type="doi">10.1016/j.neuroimage.2013.11.045</pub-id><pub-id pub-id-type="pmid">24315839</pub-id></element-citation></ref><ref id="bib93"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wu</surname><given-names>T</given-names></name><name><surname>Spagna</surname><given-names>A</given-names></name><name><surname>Chen</surname><given-names>C</given-names></name><name><surname>Schulz</surname><given-names>KP</given-names></name><name><surname>Hof</surname><given-names>PR</given-names></name><name><surname>Fan</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Supramodal Mechanisms of the Cognitive control network in uncertainty processing</article-title><source>Cerebral Cortex</source><volume>30</volume><fpage>6336</fpage><lpage>6349</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhaa189</pub-id><pub-id pub-id-type="pmid">32734281</pub-id></element-citation></ref><ref id="bib94"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Nan</surname><given-names>W</given-names></name><name><surname>Zheng</surname><given-names>Y</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Distinct cognitive control mechanisms as revealed by modality-specific conflict adaptation effects</article-title><source>Journal of Experimental Psychology. Human Perception and Performance</source><volume>43</volume><fpage>807</fpage><lpage>818</lpage><pub-id pub-id-type="doi">10.1037/xhp0000351</pub-id><pub-id pub-id-type="pmid">28345947</pub-id></element-citation></ref><ref id="bib95"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Yang</surname><given-names>G</given-names></name><name><surname>Xu</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Z</given-names></name><name><surname>Nan</surname><given-names>W</given-names></name><name><surname>Wu</surname><given-names>H</given-names></name><name><surname>Li</surname><given-names>Q</given-names></name><name><surname>Liu</surname><given-names>X</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>The congruency sequence effect is modulated by the similarity of conflicts</article-title><source>Journal of Experimental Psychology. Learning, Memory, and Cognition</source><volume>47</volume><fpage>1705</fpage><lpage>1719</lpage><pub-id pub-id-type="doi">10.1037/xlm0001054</pub-id><pub-id pub-id-type="pmid">34672662</pub-id></element-citation></ref><ref id="bib96"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zhang</surname><given-names>Y</given-names></name><name><surname>Brady</surname><given-names>M</given-names></name><name><surname>Smith</surname><given-names>S</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Segmentation of brain MR images through a hidden Markov random field model and the expectation-maximization algorithm</article-title><source>IEEE Transactions on Medical Imaging</source><volume>20</volume><fpage>45</fpage><lpage>57</lpage><pub-id pub-id-type="doi">10.1109/42.906424</pub-id><pub-id pub-id-type="pmid">11293691</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87126.5.sa0</article-id><title-group><article-title>eLife assessment</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Frank</surname><given-names>Michael J</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution>Brown University</institution><country>United States</country></aff></contrib></contrib-group><kwd-group kwd-group-type="evidence-strength"><kwd>Solid</kwd></kwd-group><kwd-group kwd-group-type="claim-importance"><kwd>Valuable</kwd></kwd-group></front-stub><body><p>Yang et al. investigate whether distinct sources of conflict are represented in a common cognitive space. The study uses an interesting task that mixes different sources of difficulty and reports that the brain appears to represent these sources as a mixture on a continuum in prefrontal areas. While the findings could be <bold>valuable</bold> to theory in this area, there are some concerns with the design and results that raise uncertainty regarding the main conclusion of a shared cognitive space. The authors appropriately acknowledge these limitations while also highlighting the valid contributions that the study makes. Thus, while <bold>solid</bold> evidence is reported here, consistent with the central hypothesis, further experiments are required to support the strictest interpretation.</p></body></sub-article><sub-article article-type="referee-report" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87126.5.sa1</article-id><title-group><article-title>Reviewer #1 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>People can perform a wide variety of different tasks, and a long-standing question in cognitive neuroscience is how the properties of different tasks are represented in the brain. The authors develop an interesting task that mixes two different sources of difficulty, and find that the brain appears to represent this mixture on a continuum, in the prefrontal areas involved in resolving task difficulty. While these results are interesting and in several ways compelling, they overlap with previous findings and rely on novel statistical analyses that may require further validation.</p><p>Strengths</p><p>1. The authors present an interesting and novel task for combining the contributions of stimulus-stimulus and stimulus-response conflict. While this mixture has been measured in the multi-source interference task (MSIT), this task provides a more graded mixture between these two sources of difficulty.</p><p>2. The authors do a good job triangulating regions that encoding conflict similarity, looking for the conjunction across several different measures of conflict encoding. These conflict measures use several best-practice approaches towards estimating representational similarity.</p><p>3. The authors quantify several salient alternative hypothesis, and systematically distinguish their core results from these alternatives.</p><p>4. The question that the authors tackle is important to cognitive control, and they make a solid contribution.</p><p>Concerns</p><p>1. The framing of 'infinite possible types of conflict' feels like a strawman. While they might be true across stimuli (which may motivate a feature-based account of control), the authors explore the interpolation between two stimuli. Instead, this work provides confirmatory evidence that task difficulty is represented parametrically (e.g., consistent with literatures like n-back, multiple object tracking, and random dot motion). This parametric encoding is standard in feature-based attention, and it's not clear what the cognitive map framing is contributing.</p><p>2. The representations within DLPFC appear to treat 100% Stoop and (to a lesser extent) 100% Simon differently than mixed trials. Within mixed trials, the RDM within this region don't strongly match the predictions of the conflict similarity model. It appears that there may be a more complex relationship encoded in this region.</p><p>3. To orthogonalized their variables, the authors need to employ a complex linear mixed effects analysis, with a potential influence of implementation details (e.g., high-level interactions and inflated degrees of freedom).</p></body></sub-article><sub-article article-type="referee-report" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87126.5.sa2</article-id><title-group><article-title>Reviewer #2 (Public Review):</article-title></title-group><contrib-group><contrib contrib-type="author"><anonymous/><role specific-use="referee">Reviewer</role></contrib></contrib-group></front-stub><body><p>Summary</p><p>This study examines the construct of &quot;cognitive spaces&quot; as they relate to neural coding schemes present in response conflict tasks. The authors use a novel experimental design in which different types of response conflict (spatial Stroop, Simon) are parametrically manipulated. These conflict types are hypothesized to be encoded jointly, within an abstract &quot;cognitive space&quot;, in which distances between task conditions depend only on the similarity of conflict types (i.e., where conditions with similar relative proportions of spatial-Stroop versus Simon conflicts are represented with similar activity patterns). Authors contrast such a representational scheme for conflict with several other conceptually distinct schemes, including a domain-general, domain-specific, and two task-specific schemes. The authors conduct a behavioral and fMRI study to test whether prefrontal cortex activity is correlated to one of these coding schemes. Replicating the authors' prior work, this study demonstrates that sequential behavioral adjustments (the congruency sequence effect) are modulated as a function of the similarity between conflict types. In fMRI data, univariate analyses identified activation in left prefrontal and dorsomedial frontal cortex that was modulated by the amount of Stroop or Simon conflict present, and representational similarity analyses that identified coding of conflict similarity, as predicted under the cognitive space model, in right lateral prefrontal cortex.</p><p>Strengths</p><p>This study addresses an important question regarding how conflict or difficulty might be encoded in the brain within a computationally efficient representational format. Relative to the other models reported in the paper, the evidence in support of the cognitive space model is solid. The ideas postulated by the authors are interesting and valuable ones, worthy of follow-up work that provides additional necessary scrutiny of the cognitive-space account.</p><p>Weaknesses</p><p>Future, within-subject experiments will be necessary to disentangle the cognitive space model from confounded task variables. A between-subjects manipulation of stimulus orientation/location renders the results difficult to interpret, as the source and spatial scale of the conflict encoding on cortex may differ from more rigorous (and more typical) within-subject manipulations.</p><p>Results are also difficult to interpret because Stroop and Simon conflict are confounded with each other. For interpretability, these two sources of conflict need to be manipulated orthogonally, so that each source of conflict (as well as their interaction) could be separately estimated and compared in terms of neural encoding. For example, it is therefore not clear whether the RSA results are due to encoding of only one type of conflict (Stroop or Simon), to a combination of both, and/or to interactive effects.</p><p>Finally, the motivation for the use of the term &quot;cognitive space&quot; to describe results is unclear. Evidence for the mere presence of a graded/parametric neural encoding (i.e., the reported conflict RSA effects) would not seem to be sufficient. Indeed, it is discussed in the manuscript that cognitive spaces/maps allow for flexibility through inference and generalization. Future work should therefore focus on linking neural conflict encoding to inference and generalization more directly.</p></body></sub-article><sub-article article-type="author-comment" id="sa3"><front-stub><article-id pub-id-type="doi">10.7554/eLife.87126.5.sa3</article-id><title-group><article-title>Author Response</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Yang</surname><given-names>Guochun</given-names></name><role specific-use="author">Author</role><aff><institution>University of Iowa</institution><addr-line><named-content content-type="city">Iowa</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Wu</surname><given-names>Haiyan</given-names></name><role specific-use="author">Author</role><aff><institution>University of Macau</institution><addr-line><named-content content-type="city">macau</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Li</surname><given-names>Qi</given-names></name><role specific-use="author">Author</role><aff><institution>Capital Normal University</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Liu</surname><given-names>Xun</given-names></name><role specific-use="author">Author</role><aff><institution>Institute of Psychology</institution><addr-line><named-content content-type="city">Beijing</named-content></addr-line><country>China</country></aff></contrib><contrib contrib-type="author"><name><surname>Fu</surname><given-names>Zhongzheng</given-names></name><role specific-use="author">Author</role><aff><institution>Cedars-Sinai Medical Center</institution><addr-line><named-content content-type="city">Los Angeles</named-content></addr-line><country>United States</country></aff></contrib><contrib contrib-type="author"><name><surname>Jiang</surname><given-names>Jiefeng</given-names></name><role specific-use="author">Author</role><aff><institution>University of Iowa</institution><addr-line><named-content content-type="city">Iowa City, IA</named-content></addr-line><country>United States</country></aff></contrib></contrib-group></front-stub><body><p>The following is the authorsâ response to the previous reviews.</p><disp-quote content-type="editor-comment"><p><bold>Reviewer #1:</bold></p><p>Concerns Public Review:</p><p>1)The framing of 'infinite possible types of conflict' feels like a strawman. While they might be true across stimuli (which may motivate a feature-based account of control), the authors explore the interpolation between two stimuli. Instead, this work provides confirmatory evidence that task difficulty is represented parametrically (e.g., consistent with literatures like n-back, multiple object tracking, and random dot motion). This parametric encoding is standard in feature-based attention, and it's not clear what the cognitive map framing is contributing.</p><p>Suggestion:</p><p>1. 'infinite combinations'. I'm frankly confused by the authors response. I don't feel like the framing has changed very much, besides a few minor replacements. Previous work in MSIT (e.g., by the author Zhongzheng Fu) has looked at whether conflict levels are represented similarly across conflict types using multivariate analyses. In the paper mentioned by Ritz &amp; Shenhav (2023), the authors looked at whether conflict levels are represented similarly across conflict types using multivariate analyses. It's not clear what this paper contributes theoretically beyond the connections to cognitive maps, which feel like an interpretative framework rather than a testable hypothesis (i.e., these previous paper could have framed their work as cognitive maps).</p></disp-quote><p>Response: We acknowledge the limitations inherent in our experimental design, which prevents us from conducting a strict test of the cognitive space view. In our previous revision, we took steps to soften our conclusions and emphasize these limitations. However, we still believe that our study offers valuable and novel insights into the cognitive space, and the tests we conducted are not merely strawman arguments.</p><p>Specifically, our study aimed to investigate the fundamental principles of the cognitive space view, as we stated in our manuscript that âthe representations of different abstract information are organized continuously and the representational geometry in the cognitive space is determined by the similarity among the represented information (Bellmund et al., 2018)â. While previous research has applied multivariate analyses to understand cognitive control representation, no prior studies had directedly tested the two key hypotheses associated with cognitive space: (1) that cognitive control representation across conflict types is continuous, and (2) that the similarity among representations of different conflict types is determined by their external similarity.</p><p>Our study makes a unique contribute by directly testing these properties through a parametric manipulation of different conflict types. This approach differs significantly from previous studies in two ways. First, our parametric manipulation involves more than two levels of conflict similarity, enabling us to directly test the two critical hypotheses mentioned above. Unlike studies such as Fu et al. (2022) and other that have treated different conflict types categorically, we introduced a gradient change in conflict similarity. This differentiation allowed us to employ representational similarity analysis (RSA) over the conflict similarity, which goes beyond mere decoding as utilized in prior work (see more explanation below for the difference between Fu et al., 2022 and our study [1]).</p><p>Second, our parametric manipulation of conflict types differs from previous studies that have manipulated task difficulty, and the modulation of multivariate pattern similarity observed in our study could not be attributed by task difficulty. Previous research, including the Ritz &amp; Shenhav (2023) (see below explanation[2]), has primarily shown that task difficulty modulates univoxel brain activation. A recent work by Wen &amp; Egner (2023) reported a gradual change in the multivariate pattern of brain activations across a wide range of frontoparietal areas, supporting the reviewerâs idea that âtask difficulty is represented parametricallyâ. However, we do not believe that our results reflect the task difficulty representation. For instance, in our study, the spatial Stroop-only and Simon-only conditions exhibited similar levels of difficulty, as indicated by their relatively comparable congruency effects (Fig. S1). Despite this similarity in difficulty, we found that the representational similarity between these two conditions was the lowest (see revised Fig. S4, the most off-diagonal value). This observation aligns more closely with our hypothesis that these two conditions are most dissimilar in terms of their conflict types.</p><p>[1] Fu et al. (2022) offers important insights into the geometry of cognitive space for conflict processing. They demonstrated that Simon and flanker conflicts could be distinguished by a decoder that leverages the representational geometry within a multidimensional space. However, their model of cognitive space primarily relies on categorical definitions of conflict types (i.e., Simon versus flanker), rather than exploring a parametric manipulation of these conflict types. The categorical manipulations make it difficult to quantify conceptual similarity between conflict types and hence limit the ability to test whether neural representations of conflict capture conceptual similarity. To the best of our knowledge, no previous studies have manipulated the conflict types parametrically. This gap highlights a broader challenge within cognitive science: effectively manipulating and measuring similarity levels for conflicts, as well as other high-level cognitive processes, which are inherently abstract. We therefore believe our parametric manipulation of conflict types, despite its inevitable limitations, is an important contribution to the literature.</p><p>We have incorporated the above statements into our revised manuscript: Methodological implications. Previous studies with mixed conflicts have applied mainly categorical manipulations of conflict types, such as the multi-source interference task (Fu et al., 2022) and color Stroop-Simon task (Liu et al., 2010). The categorical manipulations make it difficult to quantify conceptual similarity between conflict types and hence limit the ability to test whether neural representations of conflict capture conceptual similarity. To the best of our knowledge, no previous studies have manipulated the conflict types parametrically. This gap highlights a broader challenge within cognitive science: effectively manipulating and measuring similarity levels for conflicts, as well as other high-level cognitive processes, which are inherently abstract. The use of an experimental paradigm that permits parametric manipulation of conflict similarity provides a way to systematically investigate the organization of cognitive control, as well as its influence on adaptive behaviors.</p><p>[2] The work by Ritz &amp; Shenhav (2023) indeed applied multivariate analyses, but they did not test the representational similarity across different levels of task difficulty in a similar way as our investigation into different levels of conflict types, neither did they manipulated conflict types as our study. They first estimated univariate brain activations that were parametrically scaled by task difficulty (e.g., target coherence), yielding one map of parameter estimates (i.e., encoding subspace) for each of the target coherence and distractor congruence. The multivoxel patterns from the above maps were correlated to test whether the target coherence and distractor congruence share the similar neural encoding. It is noteworthy that the encoding of task difficulty in their study is estimated at the univariate level, like the univariate parametric modulation analysis in our study. The representational similarity across target coherence and distractor congruence was the second-order test and did not reflect the similarity across different difficulty levels. Though, we have found another study (Wen &amp; Egner, 2023) that has directly tested the representational similarity across different levels of task difficulty, and they observed a higher representational similarity between conditions with similar difficulty levels within a wide range of brain regions.</p><p>Reference:</p><p>Wen, T., &amp; Egner, T. (2023). Context-independent scaling of neural responses to task difficulty in the multiple-demand network. Cerebral Cortex, 33(10), 6013-6027. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1093/cercor/bhac479">https://doi.org/10.1093/cercor/bhac479</ext-link></p><p>Fu, Z., Beam, D., Chung, J. M., Reed, C. M., Mamelak, A. N., Adolphs, R., &amp; Rutishauser, U. (2022). The geometry of domain-general performance monitoring in the human medial frontal cortex. Science (New York, N.Y.), 376(6593), eabm9922. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1126/science.abm9922">https://doi.org/10.1126/science.abm9922</ext-link></p><disp-quote content-type="editor-comment"><p>Ritz, H., &amp; Shenhav, A. (2023). Orthogonal neural encoding of targets and distractors supports multivariate cognitive control. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1101/2022.12.01.518771">https://doi.org/10.1101/2022.12.01.518771</ext-link> Another issue is suggesting mixtures between two types of conflict may be many independent sources of conflict. Again, this feels like the strawman. There's a difference between infinite combinations of stimuli on the one hand, and levels of feature on the other hand. The issue of infinite stimuli is why people have proposed feature-based accounts, which are often parametric, eg color, size, orientation, spatial frequency. Mixing two forms of conflict is interesting, but the task limitations (i.e., highly correlated features) prevent an analysis of whether these are truly mixed (or eg reflect variations on just one of the conflict types). Without being able to compare a mixture between types vs levels of only one type, it's not clear what you can draw from these results re: how these are combined (and not clear how it reconciles the debate between general and specific).</p></disp-quote><p>Response: As the reviewer pointed out, a feature (or a parameterization) is an efficient way to encode potentially infinite stimuli. This is the same idea as our hypothesis: different conflict types are represented in a cognitive space akin to concrete features such as a color spectrum. This concept can be illustrated in the figure below.</p><fig id="sa3fig1" position="float"><label>Author response image 1.</label><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-sa3-fig1-v1.tif"/></fig><p>We would like to clarify that in our study we have manipulated five levels of conflict types, but they all originated from two fundamental sources: vertically spatial Stroop and horizontally Simon conflicts. We agree that the mixture of these two sources does not inherently generate additional conflict sources. However, this mixture does influence the similarity among different conflict conditions, which provides essential variability that is crucial for testing the core hypotheses (i.e., continuity and similarity modulation, see the response above) of the cognitive space view. This clarification is crucial as the reviewerâs impression might have been influenced by our introduction, where we repeatedly emphasized multiple sources of conflicts. Our aim in the introduction was to outline a broader conceptual framework, which might not directly reflect the specific design of our current study. Recognizing the possibility of misinterpretation, we have adjusted our introduction and discussion to place less emphasis on the variety of possible conflict sources. For example, we have removed the expression âThe large variety of conflict sources implies that there may be innumerable number of conflict conditionsâ from the introduction. As we have addressed in the previous response, the observed conflict similarity effect could not be attributed to merely task difficulty. Similarly, the mixture of spatial Stroop and Simon conflicts should not be attributed to one conflict source only; doing so would oversimplify it to an issue of task difficulty, as it would imply that our manipulation of conflict types merely represented varying levels of a single conflict, akin to manipulating task difficulty when everything else being equal. Importantly, the mixed conditions differ from variations along a single conflict source in that they also incorporate components of the other conflict source, thereby introducing difference beyond that would be found within variances of a single conflict source. There are a few additional evidence challenging the single dimension assumption. In our previous revisions, we compared model fittings between the Cognitive-Space model and the Stroop-/Simon-only models, and results showed that the CognitiveSpace model (BIC = 5377093) outperformed the Stroop-Only (BIC = 5377122) and Simon-Only (BIC = 5377096) models. This suggests that mixed conflicts might not be solely reflective of either Stroop or Simon sources, although we did not include these results due to concerns raised by reviewers about the validity of such comparisons, given the high anticorrelation between the two dimensions. Furthermore, Fu et al. (2022) demonstrated that the mixture of Simon and Flanker conflicts (the sf condition) is represented as the vector sum of the Flanker and Simon dimensions within their space model, indicating a compositional nature. Similarly, our mixed conditions are combinations of Stroop and Simon conflicts, and it is plausible that these mixtures represent a fusion of both Stroop and Simon components, rather than just one. Thus, we disagree that the mixture of conflicts is a strawman. In response to this concern, we have included a statement in our limitation section: âAnother limitation is that in our design, the spatial Stroop and Simon effects are highly anticorrelated. This constraint may make the five conflict types represented in a unidimensional space (e.g., a circle) embedded in a 2D space. This limitation also means we cannot conclusively rule out the possibility of a real unidimensional space driven solely by spatial Stroop or Simon conflicts. However, this appears unlikely, as it would imply that our manipulation of conflict types merely represented varying levels of a single conflict, akin to manipulating task difficulty when everything else being equal. If task difficulty were the primary variable, we would expect to see greater representational similarity between task conditions of similar difficulty, such as the Stroop and Simon conditions, which demonstrates comparable congruency effects (see Fig. S1). Contrary to this, our findings reveal that the Stroop-only and Simon-only conditions exhibit the lowest representational similarity (Fig. S4). Furthermore, Fu et al. (2022) has shown that the representation of mixtures of Simon and Flanker conflicts was compositional, rather than reflecting single dimension, which also applies to our cases.â</p><disp-quote content-type="editor-comment"><p>My recommendation would be to dramatically rewrite to reduce the framing of this providing critical evidence in favor of cognitive maps, and being more overt about the limitations of this task. However, the authors are not required to make further revisions in eLife's new model, and it's not clear how my scores would change if they made those revisions (ie the conceptual limitations would remain, the claims would just now match the more limited scope).</p></disp-quote><p>Response: With the above rationales and the adjustments we have made in the manuscripts, we believe that we have thoroughly acknowledged and articulated the limitations of our study. Therefore, we have decided against a complete rewrite of the manuscript.</p><disp-quote content-type="editor-comment"><p><bold>Public Review:</bold></p><p>1. The representations within DLPFC appear to treat 100% Stoop and (to a lesser extent) 100% Simon differently than mixed trials. Within mixed trials, the RDM within this region don't strongly match the predictions of the conflict similarity model. It appears that there may be a more complex relationship encoded in this region.</p><p>Suggestion:</p><p>1. RSMs in the key region of interest. I don't really understand the authors response here either. e.g,. 'It is essential to clarify that our conclusions were based on the significant similarity modulation effect identified in our statistical analysis using the cosine similarity model, where we did not distinguish between the within-Stroop condition and the other four within-conflict conditions (Fig. 7A, now Fig. 8A). This means that the representation of conflict type was not biased by the seemingly disparities in the values shown here'. In Figure 1C, it does look like they are testing this model.</p><p>It seems like a stronger validation would test just the mixture trials (i.e., ignoring Simon-only and stroop-only). However, simon/stroop-only conditions being qualitatively different does beg the question of whether these are being represented parametrically vs categorically.</p></disp-quote><p>Response: We apologize for the confusion caused by our previous response. To clarify, our conclusions have been drawn based on the robust conflict similarity effect.</p><p>The conflict similarity regressor is defined by higher values in the diagonal cells (representing within-conflict similarity) than the off-diagonal cells (indicating between-conflict similarity), as illustrated in Fig. 1C and Fig. 8A (now Fig. 4B). It is important to note that this regressor may not be particularly sensitive to the variations within the diagonal cells. Our previous response aimed to emphasize that the inconsistencies observed along the diagonal do not contradict our core hypothesis regarding the conflict similarity effect.</p><p>We recognized that since the visualization in Fig. S4, based on the raw RSM (i.e., Pearson correlation), may have been influenced by other regressors in our model than the conflict similarity effect. To reflect pattern similarity with confounding factors controlled for, we have visualized the RSM by including only the fixed effect of the conflict similarity and the residual while excluding all other factors. As shown in the revised Figure S4, the difference between the within-Stroop and other diagonal cells was greatly reduced. Instead, it revealed a clear pattern where that the diagonal values were higher than the off-diagonal values in the incongruent condition, aligning with our hypothesis regarding the conflict similarity modulator. Although some visual distinctions persist within the five diagonal cells (e.g., in the incongruent condition, the Stroop, Simon, and StMSmM conditions appear slightly lower than StHSmL and StLSmM conditions), follow-up one-way ANOVAs among these five diagonal conditions showed no significant differences. This held true for both incongruent and congruent conditions, with Fs &lt; 1. Thus, we conclude that there is no strong evidence supporting the notion that Simon- and spatial Stroop-only conditions are systematically different from other conflict types. As a result, we decided not to exclude these two conflict types from analysis.</p><fig id="sa3fig2" position="float"><label>Author response image 2.</label><caption><title>The stronger conflict type similarity effect in incongruent versus congruent conditions.</title><p>Shown are the summary representational similarity matrices for the right 8C region in incongruent (left) and congruent (right) conditions, respectively. Each cell represents the averaged Pearson correlation (after regressing out all factors except the conflict similarity) of cells with the same conflict type and congruency in the 1400 Ã 1400 matrix. Note that the seemingly disparities in the values of withinconflict cells (i.e., the diagonal) did not reach significance for either incongruent or congruent trials, Fs &lt; 1.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-sa3-fig2-v1.tif"/></fig><disp-quote content-type="editor-comment"><p><bold>Public Review:</bold></p><p>1. To orthogonalized their variables, the authors need to employ a complex linear mixed effects analysis, with a potential influence of implementation details (e.g., high-level interactions and inflated degrees of freedom).</p><p>Suggestion:</p><p>1. The DF for a mixed model should not be the number of observations minus the number of fixed effects. The gold standard is to use satterthwaite correction (e.g. in Matlab, fixedEffects(lme,'DFMethod','satterthwaite')), or number of subjects - number of fixed effects (i.e. you want to generalize to new subjects, not just new samples from the same subjects). Honestly, running a 4-way interaction probably is probably using more degrees of freedom than are appropriate given the number of subjects.</p></disp-quote><p>Response: We concur with the reviewerâs comment that our previous estimation of degrees of freedom (DFs) was inaccurate. Following your suggestion, we have now applied the âSatterthwaiteâ approach to approximate the DFs for all our linear mixed effect model analyses. This adjustment has led to the correction of both DFs and p values. In the Methods section, we have mentioned this revision.</p><p>âWe adjusted the t and p values with the degrees of freedom calculated through the Satterthwaite approximation method (Satterthwaite, 1946). Of note, this approach was applied to all the mixed-effect model analyses in this study.â</p><p>The application of this method has indeed resulted in a reduction of our statistical significance. However, our overall conclusions remained robust. Instead of the highly stringent threshold used in our previous version (Bonferonni corrected p &lt; .0001), we have now adopted a relatively more lenient threshold of Bonferonni correction at p &lt; 0.05, which is commonly employed in the literature. Furthermore, it is worth noting that the follow-up criteria 2 and 3 are inherently second-order analyses. Criterion 2 involves examining the interaction effect (conflict similarity effect difference between incongruent and congruent conditions), and criterion 3 involves individual correlation analyses. Due to their second-order nature, these criteria inherently have lower statistical power compared to criterion 1 (Blake &amp; Gangestad, 2020). We thus have applied a more lenient but still typically acceptable false discovery rate (FDR) correction to criteria 2 and 3. This adjustment helps maintain the rigor of our analysis while considering the inherent differences in statistical power across the various criteria. We have mentioned this revision in our manuscript:</p><p>âWe next tested whether these regions were related to cognitive control by comparing the strength of conflict similarity effect between incongruent and congruent conditions (criterion 2) and correlating the strength to behavioral similarity modulation effect (criterion 3). Given these two criteria pertain to second-order analyses (interaction or individual analyses) and thus might have lower statistical power (Blake &amp; Gangestad, 2020), we applied a more lenient threshold using false discovery rate (FDR) correction (Benjamini &amp; Hochberg, 1995) on the above-mentioned regions.â</p><p>With these adjustments, we consistently identified similar brain regions as observed in our previous version. Specifically, we found that only the right 8C region met the three criteria in the conflict similarity analysis. In addition, the regions meeting the criteria for the orientation effect included the FEF and IP2 in left hemisphere, and V1, V2, POS1, and PF in the right hemisphere. We have thoroughly revised the description of our results, updated the figures and tables in both the revised manuscript and supplementary material to accurately reflect these outcomes.</p><p>Reference:</p><p>Blake, K. R., &amp; Gangestad, S. (2020). On Attenuated Interactions, Measurement Error, and Statistical Power: Guidelines for Social and Personality Psychologists. Pers Soc Psychol Bull, 46(12), 1702-1711. <ext-link ext-link-type="uri" xlink:href="https://doi.org/10.1177/0146167220913363">https://doi.org/10.1177/0146167220913363</ext-link></p><disp-quote content-type="editor-comment"><p>Minor:</p><p>1. Figure 8 should come much earlier (e.g, incorporated into Figure 1), and there should be consistent terms for 'cognitive map' and 'conflict similarity'.</p></disp-quote><p>Response: We appreciate this suggestion. Considering that Figure 7 (âThe crosssubject RSA model and the rationaleâ) also describes the models, we have merged Figure 7 and 8 and moved the new figure ahead, before we report the RSA results. Now you could find it in the new Figure 4, see below. We did not incorporate them into Figure 1 since Figure 1 is already too crowded.</p><fig id="sa3fig3" position="float"><label>Author response image 3.</label><caption><title>Fig 4.</title><p>Rationale of the cross-subject RSA model and the schematic of key RSMs. (A) The RSM is calculated as the Pearsonâs correlation between each pair of conditions across the 35 subjects. For 17 subjects, the stimuli were displayed on the top-left and bottom-right quadrants, and they were asked to respond with left hand to the upward arrow and right hand to the downward arrow. For the other 18 subjects, the stimuli were displayed on the top-right and bottom-left quadrants, and they were asked to respond with left hand to the downward arrow and right hand to the upward arrow. Within each subject, the conflict type and orientation regressors were perfectly covaried. For instance, the same conflict type will always be on the same orientation. To de-correlate conflict type and orientation effects, we conducted the RSA across subjects from different groups. For example, the bottom-right panel highlights the example conditions that are orthogonal to each other on the orientation, response, and Simon distractor, whereas their conflict type, target and spatial Stroop distractor are the same. The dashed boxes show the possible target locations for different conditions. (B) and (C) show the orthogonality between conflict similarity and orientation RSMs. The within-subject RSMs (e.g., Group1-Group1) for conflict similarity and orientation are all the same, but the cross-group correlations (e.g., Group2-Group1) are different. Therefore, we can separate the contribution of these two effects when including them as different regressors in the same linear regression model. (D) and (E) show the two alternative models. Like the cosine model (B), within-group trial pairs resemble betweengroup trial pairs in these two models. The domain-specific model is an identity matrix. The domaingeneral model is estimated from the absolute difference of behavioral congruency effect, but scaled to 0 (lowest similarity) â 1 (highest similarity) to aid comparison. The plotted matrices in B-E include only one subject each from Group 1 and Group 2. Numbers 1-5 indicate the conflict type conditions, for spatial Stroop, StHSmL, StMSmM, StLSmH, and Simon, respectively. The thin lines separate four different sub-conditions, i.e., target arrow (up, down) Ã congruency (incongruent, congruent), within each conflict type.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-87126-sa3-fig3-v1.tif"/></fig><p>In our manuscript, the term âcognitive map/spaceâ was used when explaining the results in a theoretical perspective, whereas the âconflict similarityâ was used to describe the regressor within the RSA. These terms serve distinct purposes in our study and cannot be interchangeably substituted. Therefore, we have retained them in their current format. However, we recognize that the initial introduction of the âCognitive-Space modelâ may have appeared somewhat abrupt. To address this, we have included a brief explanatory note: âThe model described above employs the cosine similarity measure to define conflict similarity and will be referred to as the Cognitive-Space model.â</p></body></sub-article></article>