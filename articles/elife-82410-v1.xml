<?xml version="1.0" encoding="UTF-8"?><!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD with MathML3 v1.2 20190208//EN"  "JATS-archivearticle1-mathml3.dtd"><article xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" article-type="research-article" dtd-version="1.2"><front><journal-meta><journal-id journal-id-type="nlm-ta">elife</journal-id><journal-id journal-id-type="publisher-id">eLife</journal-id><journal-title-group><journal-title>eLife</journal-title></journal-title-group><issn publication-format="electronic" pub-type="epub">2050-084X</issn><publisher><publisher-name>eLife Sciences Publications, Ltd</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="publisher-id">82410</article-id><article-id pub-id-type="doi">10.7554/eLife.82410</article-id><article-categories><subj-group subj-group-type="display-channel"><subject>Research Article</subject></subj-group><subj-group subj-group-type="heading"><subject>Neuroscience</subject></subj-group></article-categories><title-group><article-title>Retinal motion statistics during natural locomotion</article-title></title-group><contrib-group><contrib contrib-type="author" id="author-203044"><name><surname>Muller</surname><given-names>Karl S</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1319-9293</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund3"/><xref ref-type="fn" rid="con1"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-202103"><name><surname>Matthis</surname><given-names>Jonathan</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-3683-646X</contrib-id><xref ref-type="aff" rid="aff2">2</xref><xref ref-type="fn" rid="con2"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-266951"><name><surname>Bonnen</surname><given-names>Kathryn</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-9210-8275</contrib-id><xref ref-type="aff" rid="aff3">3</xref><xref ref-type="other" rid="fund4"/><xref ref-type="other" rid="fund5"/><xref ref-type="fn" rid="con3"/><xref ref-type="fn" rid="conf2"/></contrib><contrib contrib-type="author" id="author-207584"><name><surname>Cormack</surname><given-names>Lawrence K</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-3958-781X</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="fn" rid="con4"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" id="author-21382"><name><surname>Huk</surname><given-names>Alex C</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0003-1430-6935</contrib-id><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund2"/><xref ref-type="fn" rid="con5"/><xref ref-type="fn" rid="conf1"/></contrib><contrib contrib-type="author" corresp="yes" id="author-38156"><name><surname>Hayhoe</surname><given-names>Mary</given-names></name><contrib-id authenticated="true" contrib-id-type="orcid">https://orcid.org/0000-0002-6671-5207</contrib-id><email>hayhoe@utexas.edu</email><xref ref-type="aff" rid="aff1">1</xref><xref ref-type="other" rid="fund1"/><xref ref-type="fn" rid="con6"/><xref ref-type="fn" rid="conf1"/></contrib><aff id="aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/00hj54h04</institution-id><institution>Center for Perceptual Systems, The University of Texas at Austin</institution></institution-wrap><addr-line><named-content content-type="city">Austin</named-content></addr-line><country>United States</country></aff><aff id="aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/04t5xt781</institution-id><institution>Department of Biology, Northeastern University</institution></institution-wrap><addr-line><named-content content-type="city">Boston</named-content></addr-line><country>United States</country></aff><aff id="aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ror">https://ror.org/01kg8sb98</institution-id><institution>School of Optometry, Indiana University</institution></institution-wrap><addr-line><named-content content-type="city">Bloomington</named-content></addr-line><country>United States</country></aff></contrib-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Huxlin</surname><given-names>Krystel R</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022kthw22</institution-id><institution>University of Rochester</institution></institution-wrap><country>United States</country></aff></contrib><contrib contrib-type="senior_editor"><name><surname>Moore</surname><given-names>Tirin</given-names></name><role>Senior Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/006w34k90</institution-id><institution>Howard Hughes Medical Institute, Stanford University</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><pub-date publication-format="electronic" date-type="publication"><day>03</day><month>05</month><year>2023</year></pub-date><pub-date pub-type="collection"><year>2023</year></pub-date><volume>12</volume><elocation-id>e82410</elocation-id><history><date date-type="received" iso-8601-date="2022-08-03"><day>03</day><month>08</month><year>2022</year></date><date date-type="accepted" iso-8601-date="2023-04-09"><day>09</day><month>04</month><year>2023</year></date></history><pub-history><event><event-desc>This manuscript was published as a preprint at bioRxiv.</event-desc><date date-type="preprint" iso-8601-date="2022-09-08"><day>08</day><month>09</month><year>2022</year></date><self-uri content-type="preprint" xlink:href="https://doi.org/10.1101/2022.09.06.506797"/></event></pub-history><permissions><copyright-statement>© 2023, Muller et al</copyright-statement><copyright-year>2023</copyright-year><copyright-holder>Muller et al</copyright-holder><ali:free_to_read/><license xlink:href="http://creativecommons.org/licenses/by/4.0/"><ali:license_ref>http://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This article is distributed under the terms of the <ext-link ext-link-type="uri" xlink:href="http://creativecommons.org/licenses/by/4.0/">Creative Commons Attribution License</ext-link>, which permits unrestricted use and redistribution provided that the original author and source are credited.</license-p></license></permissions><self-uri content-type="pdf" xlink:href="elife-82410-v1.pdf"/><abstract><p>Walking through an environment generates retinal motion, which humans rely on to perform a variety of visual tasks. Retinal motion patterns are determined by an interconnected set of factors, including gaze location, gaze stabilization, the structure of the environment, and the walker’s goals. The characteristics of these motion signals have important consequences for neural organization and behavior. However, to date, there are no empirical in situ measurements of how combined eye and body movements interact with real 3D environments to shape the statistics of retinal motion signals. Here, we collect measurements of the eyes, the body, and the 3D environment during locomotion. We describe properties of the resulting retinal motion patterns. We explain how these patterns are shaped by gaze location in the world, as well as by behavior, and how they may provide a template for the way motion sensitivity and receptive field properties vary across the visual field.</p></abstract><kwd-group kwd-group-type="author-keywords"><kwd>retinal motion statistics</kwd><kwd>natural environments</kwd><kwd>eye movements</kwd><kwd>locomotion</kwd><kwd>cortical motion sensitivity</kwd></kwd-group><kwd-group kwd-group-type="research-organism"><title>Research organism</title><kwd>Human</kwd></kwd-group><funding-group><award-group id="fund1"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000053</institution-id><institution>National Eye Institute</institution></institution-wrap></funding-source><award-id>R01 EY05729</award-id><principal-award-recipient><name><surname>Hayhoe</surname><given-names>Mary</given-names></name></principal-award-recipient></award-group><award-group id="fund2"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>U01 NS116377</award-id><principal-award-recipient><name><surname>Huk</surname><given-names>Alex C</given-names></name></principal-award-recipient></award-group><award-group id="fund3"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000002</institution-id><institution>National Institutes of Health</institution></institution-wrap></funding-source><award-id>T32 LM012414</award-id><principal-award-recipient><name><surname>Muller</surname><given-names>Karl S</given-names></name></principal-award-recipient></award-group><award-group id="fund4"><funding-source><institution-wrap><institution-id institution-id-type="FundRef">http://dx.doi.org/10.13039/100000893</institution-id><institution>Simons Foundation</institution></institution-wrap></funding-source><award-id>Society of Fellows Junior Fellow</award-id><principal-award-recipient><name><surname>Bonnen</surname><given-names>Kathryn</given-names></name></principal-award-recipient></award-group><award-group id="fund5"><funding-source><institution-wrap><institution>ARVO / VSS</institution></institution-wrap></funding-source><award-id>Fellowship</award-id><principal-award-recipient><name><surname>Bonnen</surname><given-names>Kathryn</given-names></name></principal-award-recipient></award-group><funding-statement>The funders had no role in study design, data collection and interpretation, or the decision to submit the work for publication.</funding-statement></funding-group><custom-meta-group><custom-meta specific-use="meta-only"><meta-name>Author impact statement</meta-name><meta-value>Retinal motion patterns during locomotion are shaped by gait, gaze location, and the terrain, and these motion patterns may influence the way motion sensitivity and receptive field properties vary across the visual field.</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="s1" sec-type="intro"><title>Introduction</title><p>A moving observer traveling through a stationary environment generates a pattern of motion that is commonly referred to as optic flow (<xref ref-type="bibr" rid="bib19">Gibson, 1950</xref>; <xref ref-type="bibr" rid="bib28">Koenderink, 1986</xref>). While optic flow is often thought of as a simple pattern of expansive motion centered on the direction of heading, this will be true for the retinal motion pattern only in the case of linear motion with gaze centered on heading direction, a condition only rarely met in natural behavior. The actual retinal motion pattern is much more complex and depends on both the three-dimensional structure of the environment and the motion of the eye through space, which in turn depends on the location of the point of gaze in the scene and the gait-induced oscillations of the body. The pervasive presence of self-motion makes it likely that the structure of motion processing systems is shaped by these patterns at both evolutionary and developmental timescales. This makes it important to understand the statistics of the actual motion patterns generated in the context of natural behavior. While much is known about motion sensitivity in the visual pathways, it is not known how those properties are linked to behavior and how they might be shaped by experience. To do this, it is necessary to measure the actual retinal motion input in the context of natural behavior. A similar point was made by <xref ref-type="bibr" rid="bib8">Bonnen et al., 2020</xref>, who demonstrated that an understanding of the retinal images resulting from binocular viewing geometry allowed a better understanding of the way that cortical neurons might encode the 3D environment.</p><p>Despite many elegant theoretical analyses of the way that observer motion generates retinal flow patterns, a detailed understanding has been limited by the difficulties in recording the visual input during locomotion in natural environments. In this article, we measure eye and body movements during locomotion in a variety of natural terrains and explore how they shape the properties of the retinal input. A number of studies have examined motion patterns generated by cameras moving through natural environments (<xref ref-type="bibr" rid="bib5">Betsch et al., 2005</xref>; <xref ref-type="bibr" rid="bib45">Zanker and Zeil, 2005</xref>), but these data do not accurately reflect the patterns incident on the human retinae because the movement of the cameras does not mimic the movements of the head, nor does it take into account the location of gaze. In natural locomotion, walkers gaze at different locations depending on the complexity of the terrain and the consequent need to find stable footholds (<xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref>). Thus, task goals indirectly affect the motion input. In addition, natural locomotion is not linear. Instead, the head moves through a complex trajectory in space during the gait cycle, while the point of gaze remains stable in the environment, and this imparts a complex pattern of rotation and expansion on the retinal flow as recently described by <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>. Retinal motion is generated by the compensatory rotations of the eye in space while the body moves forward during a step, and gaze is held at a fixed location in space. To characterize the properties of this motion and how it depends on gaze behavior, we simultaneously recorded gaze and image data while subjects walked in a variety of different natural terrains. In addition, to fully characterize the retinal motion we reconstructed a 3D representation of the terrain. This links the eye and body movements to the particular terrain and consequently allows calculation of the motion patterns on the retinae.</p><p>Previous work on the statistics of retinal motion by Calow and Lappe simulated retinal flow patterns using estimates of gaze location and gait oscillations, together with a database of depth images (<xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref>; <xref ref-type="bibr" rid="bib14">Calow and Lappe, 2008</xref>). However, since terrain is a profound influence on gaze deployment, the in situ data collection strategy we use here allows measurement of how gaze location varies with terrain, consequently allowing a more precise and realistic evaluation of the natural statistics than in previous studies. In this article, we focus on the interactions between gaze, body, and the resulting motion patterns. We find a stereotyped pattern of gaze behavior that emerges due to the constraints of the task, and this pattern of gaze, together with gait-induced head movements, drives much of the variation in the resulting visual motion patterns. Most importantly, because walkers stabilize gaze location in the world, the motion statistics result from the motion of the eye in space as it is carried forward by the body while counter-rotating to maintain stability. In this article, we calculate the statistics of the retinal image motion across the visual field. In addition, we describe the effects of changes in both vertical and lateral gaze angle and also the effects of natural terrain structure, independent of gaze location. Thus, a quantitative description of retinal image statistics requires an understanding of the way the body interacts with the world.</p></sec><sec id="s2" sec-type="results"><title>Results</title><p>Eye movements, first-person scene video, and body movements were recorded using a Pupil Labs mobile eye tracker and a Motion Shadow full-body IMU-based capture system. Eye movements were recorded at 120 Hz. The scene camera recorded at 30 Hz with 1920 × 1080 pixel resolution and 100 deg diagonal field of view. The Shadow motion capture system recorded at 100 Hz and was used to estimate joint positions and orientations of a full 3D skeleton. Participants walked over a range of terrains two times in each direction. Examples of the terrains are shown in Figure 2a. In addition, a representation of the 3D terrain structure was reconstructed from the sequence of video images using photogrammetry, as described below in the section on optic flow estimation. Details of the procedure for calibrating and extracting integrated gaze, body, and terrain data are described in ‘Methods’, as well as in <xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref> and <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>.</p><sec id="s2-1"><title>Oculomotor patterns during locomotion</title><p>Because it is important for understanding how the retinal motion patterns are generated, we first describe the basic pattern of eye movements during locomotion as has been described previously (<xref ref-type="bibr" rid="bib24">Imai et al., 2001</xref>; <xref ref-type="bibr" rid="bib20">Grasso et al., 1998</xref>; <xref ref-type="bibr" rid="bib4">Authié et al., 2015</xref>). <xref ref-type="fig" rid="fig1">Figure 1a</xref> shows a schematic of the typical eye movement pattern. When the terrain is complex, subjects mostly direct gaze toward the ground a few steps ahead (<xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref>). This provides visual information to guide upcoming foot placement. As the body moves forward, the subject makes a sequence of saccades to locations further along the direction of travel. Following each saccade, gaze location is held approximately stable in the scene for periods of 200–300 ms so that visual information about upcoming foothold locations can be acquired while the subject moves forward during a step. A video example of the gaze patterns during locomotion, together with the corresponding retina-centered images and traces of eye-in-head angle, is given in <xref ref-type="video" rid="video1">Video 1</xref>. This video is taken from <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>, who collected a subset of the data used in this article. While the walker is fixating and holding gaze stable at a particular location on the ground, the eye rotates slowly to offset the forward motion of the body. <xref ref-type="fig" rid="fig1">Figure 1b</xref> shows an excerpt of the vertical component of gaze during this characteristic gaze pattern. Stabilization is most likely accomplished by the vestibular-ocular reflex, although other eye movement systems might also be involved. This is discussed further below and in <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>.</p><fig id="fig1" position="float"><label>Figure 1.</label><caption><title>Characteristic oculomotor behavior during locomotion.</title><p>(<bold>a</bold>) Schematic of a saccade and subsequent gaze stabilization during locomotion when looking at the nearby ground. In the top left, the walker makes a saccade to an object further along the path. In the middle panel, the walker fixates (holds gaze) at this location for a time. The right panel shows the gaze angle becoming more normal to the ground plane during stabilization. (<bold>b</bold>) Excerpt of vertical gaze angle relative to gravity during a period of saccades and subsequent stabilization. As participants move forward while looking at the nearby ground, they make sequences of saccades (indicated by the gaps in the trace) to new locations, followed by fixations where gaze is held stable at a location in the world while the body moves forward along the direction of travel (indicated by the lower velocity green traces). The higher velocity saccades were detected as described in the text based on both horizontal and vertical velocity and acceleration. These are followed by slower counter-rotations of the eye in the orbit in order to maintain gaze at a fixed location in the scene (the gray time slices).</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig1-v1.tif"/></fig><media mimetype="video" mime-subtype="mp4" xlink:href="elife-82410-video1.mp4" id="video1"><label>Video 1.</label><caption><title>Gaze behavior during locomotion.</title><p>Visualization of visual input and eye and body movements during natural locomotion.</p></caption></media><p>During the periods when gaze location is approximately stable in the scene, the retinal image expands and rotates, depending on the direction of the eye in space, carried by the body. It is these motion patterns that we examine here. An illustration of the retinal motion patterns resulting from forward movement accompanied by gaze stabilization is shown in <xref ref-type="video" rid="video2">Video 2</xref>, which also shows the stark difference between retinal motion patterns and motion relative to the head. This movie is also taken from <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>. We segmented the image into saccades and fixations using an eye-in-orbit velocity threshold of 65 deg/s and an acceleration threshold of 5 deg/s<sup>2</sup>. We use the term ‘fixation’ here to refer to the periods of stable gaze in the world separated by saccades, ‘gaze’ is the direction of the eye in the scene. and gaze location is where that vector intersects the ground plane. Note that historically the term ‘fixation’ has been used to refer to the situation where the head is fixed and the eye is stable in the orbit. However, whenever the head is moving and gaze is fixed on a stable location in the world, the eye will rotate in the orbit. Since head movements are ubiquitous in normal vision, we use the term ‘fixation’ here to refer to the periods of stable gaze in the world separated by saccades, even when the eye rotates in the orbit as a consequence of stabilization mechanisms (for a review, see <xref ref-type="bibr" rid="bib32">Lappi, 2016</xref> for a discussion of the issues in defining fixations in natural behavior). The velocity threshold is quite high in order to accommodate the smooth counter-rotations during stabilization. Saccadic eye movements induce considerably higher velocities, but saccadic suppression and image blur render this information less useful for locomotor guidance, and the neural mechanisms underlying motion analysis during saccades are not well understood (<xref ref-type="bibr" rid="bib38">McFarland et al., 2015</xref>). We consider the retinal motion generated by saccades separately, as described in ‘Methods.’.</p><media mimetype="video" mime-subtype="mp4" xlink:href="elife-82410-video2.mp4" id="video2"><label>Video 2.</label><caption><title>Visual motion during locomotion.</title><p>Visualization of eye and head centered visual motion during natural locomotion.</p></caption></media><p>Incomplete gaze stabilization during a fixation will add image motion. Analysis of image slippage during the fixations revealed that stabilization of gaze location in the world was very good (see Figure 10 in ‘Methods’). Retinal image slippage during fixations had a mode of 0.26 deg and a median of 0.83 deg. This image slippage reflects not only incomplete stabilization but also eye-tracker noise and some small saccades misclassified as fixations, so it is most likely an overestimate. In order to simplify the analysis, we first ignore image slip during a fixation and do the analysis as if gaze were fixed at the initial location for the duration of the fixation. In ‘Methods,’ we evaluate the impact of this idealization and show that it is modest.</p><p>There is variation in how far ahead subjects direct gaze between terrain types, as has been observed previously (<xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref>), although the pattern of saccades followed by stabilizing eye movements is conserved. We summarize this behavior by measuring the angle of gaze relative to gravity and plot gaze angle distributions for the different terrain types in <xref ref-type="fig" rid="fig2">Figure 2</xref>. Consistent with previous observations, gaze location is moved closer to the body in the more complex terrains, with the median gaze angle in rocky terrain being approximately 45 deg, about 2–3 steps ahead, and that on pavement being to far distances, a little below the horizontal. Note that the distributions are all quite broad and sensitive to changes in the terrain, such as that between a paved road and a flat dirt path. Subtle changes like this presumably affect variation in the nature of the visual information needed for foot placement. Individual subject histograms are shown in ‘Methods.’ There is most variability between subjects in the bark and flat terrains, as might be expected from individual trade-offs between energetic costs, stability, and other factors. The bimodality of most of the distributions reflects the observation that subjects alternate between near and far viewing, presumably for different purposes (e.g. path planning versus foothold finding). These changes in gaze angle, in conjunction with the movements of the head, have an important effect on retinal motion speeds, as will be shown below. Thus, motion input indirectly stems from behavioral goals.</p><fig id="fig2" position="float"><label>Figure 2.</label><caption><title>Gaze behavior depends on terrain.</title><p>(<bold>a</bold>) Example images of the five terrain types. Sections of the hiking path were assigned to one of the five terrain types. The <italic>Pavement</italic> terrain included the paved parts of the hiking path, while the <italic>Flat</italic> terrain included the parts of the trail which were composed of flat packed earth. The <italic>Medium</italic> terrain had small irregularities in the path as well as loose rocks and pebbles. The <italic>Bark</italic> terrain (though similar to the Medium terrain) was given a separate designation as it was generally flatter than the Medium terrain but large pieces of bark and occasional tree roots were strewn across the path. Finally, the <italic>Rocks</italic> terrain had significant path irregularities which required attention to locate stable footholds. (<bold>b</bold>) Histograms of vertical gaze angle (angle relative to the direction of gravity) across different terrain types. In very flat, regular terrain (e.g. pavement, flat) participant gaze accumulates at the horizon (90°). With increasing terrain complexity participants shift gaze downward (30°–60°). Data are averaged over 10 subjects for rocky terrain and 8 subjects for the other terrains. Shaded error bars are ±1 SEM. Individual subject data are shown in ‘Methods’.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig2-v1.tif"/></fig></sec><sec id="s2-2"><title>Speed and direction distributions during gaze stabilization</title><p>The way the eye moves in space during the fixations, together with gaze location in the scene, jointly determines the retinal motion patterns. Therefore, we summarize the direction and speed of the stabilizing eye movements in <xref ref-type="fig" rid="fig3">Figure 3a and b</xref>. <xref ref-type="fig" rid="fig3">Figure 3a</xref> shows the distribution of movement speeds, and <xref ref-type="fig" rid="fig3">Figure 3b</xref> shows the distribution of gaze directions (rotations of the eye in the orbit). Rotations are primarily downward as the body moves forward, with rightward and leftward components resulting from both body sway and fixations to the left or right of the future path, occasioned by the need to change direction or navigate around an obstacle. There are a small number of upward eye movements resulting from vertical gait-related motion of the head, and possibly some small saccades that were misclassified as fixations. These movements, together with head trajectory and the depth structure of the terrain, determine the retinal motion. Note that individual differences in walking speed and gaze location relative to the body will affect these measurements, which are pooled over all terrains and subjects. Our goal here is simply to illustrate the general properties of the movements as the context for the generation of the retinal motion patterns.</p><fig id="fig3" position="float"><label>Figure 3.</label><caption><title>Eye rotations during stabilization.</title><p>(<bold>a</bold>) The distribution of speeds during periods of stabilization (i.e. eye movements that keep point of gaze approximately stable in the scene). (<bold>b</bold>) A polar histogram of eye movement directions during these stabilizing movements. 270 deg corresponds to straight down in eye centered coordinates, while 90 deg corresponds to straight up. Stabilizing eye movements are largely in the downward direction, reflecting the forward movement of the body. Some upward eye movements occur and may be due to misclassification of small saccades or variation in head movements relative to the body. Shaded region shows ±1 SEM across 10 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig3-v1.tif"/></fig></sec><sec id="s2-3"><title>Optic flow estimation</title><p>In order to approximate retinal motion input to the visual system, we first use a photogrammetry package called Meshroom to estimate a 3D triangle mesh representation of the terrain structure, as well as a 3D trajectory through the terrain using the head camera video images. Using Blender (<xref ref-type="bibr" rid="bib7">Blender Online Community, 2021</xref>), the 3D triangle mesh representations of the terrain are combined with the spatially aligned eye position and direction data. A virtual camera is then placed at the eye location and oriented in the same direction as the eye, and a depth image is acquired using Blender’s built in z-buffer method. Thus, the depth image input at each frame of the recording is computed. These depth values per location on the virtual imaging surface are mapped to retinal coordinates based on their positions relative to the principal point of the camera. Thus, approximate depth at each location in visual space is known. Visual motion in eye coordinates can then be computed by tracking the movement of projections of 3D locations in the environment onto an image plane orthogonal to gaze, resulting from translation and rotation of the eye (see <xref ref-type="bibr" rid="bib33">Longuet-Higgins and Prazdny, 1980</xref> for generalized approach).</p><p>The retinal motion signal is represented as a 2D grid where grid points (<inline-formula><mml:math id="inf1"><mml:mrow><mml:mi mathsize="120%">x</mml:mi><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mi mathsize="120%">y</mml:mi></mml:mrow></mml:math></inline-formula>), correspond to polar retinal coordinates (<inline-formula><mml:math id="inf2"><mml:mrow><mml:mi mathsize="120%">θ</mml:mi><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mi mathsize="120%">ϕ</mml:mi></mml:mrow></mml:math></inline-formula>) by the relationship<disp-formula id="equ1"><mml:math id="m1"><mml:mrow><mml:mi mathsize="120%">θ</mml:mi><mml:mo mathsize="120%" stretchy="false">=</mml:mo><mml:mrow><mml:mi mathsize="120%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mn mathsize="120%">2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mi mathsize="120%">y</mml:mi><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mi mathsize="120%">x</mml:mi><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow></mml:mrow></mml:math></disp-formula><disp-formula id="equ2"><mml:math id="m2"><mml:mrow><mml:mi mathsize="120%">ϕ</mml:mi><mml:mo mathsize="120%" stretchy="false">=</mml:mo><mml:msqrt><mml:mrow><mml:msup><mml:mi mathsize="120%">x</mml:mi><mml:mn mathsize="120%">2</mml:mn></mml:msup><mml:mo mathsize="120%" stretchy="false">+</mml:mo><mml:msup><mml:mi mathsize="120%">y</mml:mi><mml:mn mathsize="120%">2</mml:mn></mml:msup></mml:mrow></mml:msqrt></mml:mrow></mml:math></disp-formula></p><p>Thus, eccentricity in visual angle is mapped linearly to the image plane as a distance from the point of gaze. At each <inline-formula><mml:math id="inf3"><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mi mathsize="120%">x</mml:mi><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mi mathsize="120%">y</mml:mi><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:math></inline-formula> coordinate, there is a corresponding speed in <inline-formula><mml:math id="inf4"><mml:mfrac><mml:mrow><mml:mi mathsize="120%">d</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">e</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">g</mml:mi></mml:mrow><mml:mi mathsize="120%">s</mml:mi></mml:mfrac></mml:math></inline-formula> and direction <inline-formula><mml:math id="inf5"><mml:mrow><mml:mi mathsize="120%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mn mathsize="120%">2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mrow><mml:mi mathsize="120%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">x</mml:mi></mml:mrow><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mrow><mml:mi mathsize="120%" mathvariant="normal">Δ</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">y</mml:mi></mml:mrow><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow></mml:math></inline-formula> of movement.</p></sec><sec id="s2-4"><title>Average motion speed and direction statistics</title><p>Subjects’ gaze angle modulates the pattern of retinal motion because of the planar structure of the environment (<xref ref-type="bibr" rid="bib27">Koenderink and van Doorn, 1976</xref>). However, we first consider the average motion signal across all the different terrain types and gaze angles. We will then explore the effects of gaze angle and terrain more directly. The mean flow fields for speed and direction, averaged across subjects, for all terrains, are shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. While there will be inevitable differences between subjects caused by the different geometry as a result of different subject heights and idiosyncratic gait patterns, we have chosen to first average the data across subjects since the current goal is to describe the general properties of the flow patterns resulting from natural locomotion across a ground plane. Individual subject data are shown in ‘Methods.’.</p><fig id="fig4" position="float"><label>Figure 4.</label><caption><title>Speed and direction of retinal motion signals as a function of retinal position.</title><p>(<bold>a</bold>) Average speed of retinal motion signals as a function of retinal position. Speed is color mapped (blue = slow, red = fast). The average is computed across all subjects and terrain types. Speed is computed in degrees of visual angle per second. (<bold>b</bold>) Speed distributions at five points in the visual field at the fovea and four cardinal locations. The modal speed increases in all four cardinal locations, though more prominently in the upper/lower visual fields. Speed variability also increases in the periphery in comparable ways. (<bold>c</bold>) Average retinal flow pattern as a function of retinal position. The panel shows the integral curves of the flow field (black) and retinal flow vectors (green). Direction is indicated by the angle of the streamline drawn at particular location. Vector direction corresponds to the direction in a 2D projection of visual space, where eccentricity from the direction of gaze in degrees is mapped linearly to distance in polar coordinates in the 2D projection plane. (<bold>d</bold>) Histogram of the average retinal motion directions (in <bold>c</bold>) as a function of polar angle. Error bars in (<bold>b</bold>) and (<bold>d</bold>) are ±1 SEM over 9 subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig4-v1.tif"/></fig><p><xref ref-type="fig" rid="fig4">Figure 4a</xref> shows a map of the average speed at each visual field location (speed is color mapped with blue being the lowest velocity and yellow being the highest, and the contour lines indicate equal speed). This visualization demonstrates the low speeds near the fovea with increasing speed as a function of eccentricity, a consequence of gaze stabilization. Both the mean and variance of the distributions increase with eccentricity as shown by the speed distributions in <xref ref-type="fig" rid="fig4">Figure 4b</xref>. The increase is not radially symmetric. The lower visual field has steeper increase as a function of eccentricity compared to the upper visual field. This is a consequence of the increasing visual angle of the ground plane close to the walker. The left and right visual field speeds are even lower than the upper visual field since the ground plane rotates in depth around a horizontal axis defined by the fixation point (see <xref ref-type="fig" rid="fig2">Figure 2</xref>). Average speeds in the lower visual field peak at approximately 28.8 deg/s (at 45 deg eccentricity), whereas the upper peaks at 13.6 deg/s.</p><p>Retinal motion directions in <xref ref-type="fig" rid="fig4">Figure 4c</xref> are represented by unit vectors. The average directions of flow exhibit a radially expansive pattern as expected from the viewing geometry. However, the expansive motion (directly away from center) is not radially symmetric. Directions are biased toward the vertical, with only a narrow band in the left and right visual field exhibiting leftward or rightward motion. This can be seen in the histogram in <xref ref-type="fig" rid="fig4">Figure 4d</xref>, which peaks at 90 deg and 270 deg. Again, this pattern results from a combination of the forward motion, the rotation in depth of the ground plane around the horizontal axis defined by the fixation point, and the increasing visual angle of the ground plane.</p></sec><sec id="s2-5"><title>Effects of horizontal and vertical gaze angle on motion patterns</title><p>Averaging the data across the different terrains does not accurately reflect the average motion signals a walker might be exposed to in general as it is weighted by the amount of time the walker spends in different terrains. It also obscures the effect of gaze angle in the different terrains. Similarly, averaging over the gait cycle obscures the effect of the changing angle between the eye and the head direction in space as the body moves laterally during a normal step. We therefore divided the data by gaze angle to reveal the effects of varying horizontal and vertical gaze angle. Vertical gaze angle, the angle of gaze in world coordinates relative to gravity, is driven by different terrain demands that cause the subject to direct gaze closer or further from the body. Vertical gaze angles were binned between 60 and 90 deg, and between 17 and 45 deg. This reflects the top and bottom third of the distribution of vertical gaze angles. We did not calculate separate plots for individual subjects in this figure as the goal is to show the kind and approximate magnitude of the transformation imposed by horizontal and vertical eye rotations.</p><p>The effect of the vertical component of gaze angle can be seen in <xref ref-type="fig" rid="fig5">Figure 5</xref>. As gaze is directed more toward the horizon, the pattern of increasing speed as a function of eccentricity becomes more radially asymmetric, with the peak velocity ranging from less than 5 deg/s in the upper visual field to speeds in the range of 20–40 deg/s in the lower visual fields. (Compare top and bottom panels of <xref ref-type="fig" rid="fig4">Figure 4a and b</xref>.) This pattern may be the most frequent one experienced by walkers to the extent that smooth terrains are most common (see the distributions of gaze angles for flat and pavement terrain in <xref ref-type="fig" rid="fig2">Figure 2</xref>). As gaze is lowered to the ground, these peaks move closer together, the variance of the distributions increase in the upper/left/right fields, and the distribution of motion speeds becomes more radially symmetric. There is some effect on the spatial pattern of motion direction as well, with the density of downward motion vectors increasing at gaze angles closer to the vertical.</p><fig id="fig5" position="float"><label>Figure 5.</label><caption><title>Effect of vertical gaze angle on retinal motion speed and direction.</title><p>This analysis compares the retinal motion statistics for upper (60°–90°) vs. lower vertical gaze angles (17°–45°). The upper vertical gaze angles correspond to far fixations while the lower vertical gaze angles correspond to fixations closer to the body. (<bold>a</bold>) Average motion speeds across the visual field. (<bold>b</bold>) Five example distributions are shown as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. Looking at the ground near the body (i.e. lower vertical gaze angles) reduces the asymmetry between upper and lower visual fields. Peak speeds in the lower visual field are reduced, while speeds are increased in the upper visual field. (<bold>c</bold>) Average retinal flow patterns for upper and lower vertical gaze angles. (<bold>d</bold>) Histograms of the average directions plotted in (<bold>c</bold>). While still peaking for vertical directions, the distribution of directions becomes more uniform as walkers look to more distant locations. Data are pooled across subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig5-v1.tif"/></fig><p>Horizontal gaze angle is defined relative to the direction of travel. For a particular frame of the recording, the head velocity vector projected into a horizontal plane normal to gravity is treated as 0 deg, and the angle relative to this vector of the gaze angle projected into the same plane is the horizontal angle (clockwise being positive when viewed from above). Horizontal gaze angle changes stem both from looks off the path, and from the lateral movement of the body during a step. Body sway accounts for about ±12 deg of rotation of the eye in the orbit. Fixations to the right and left of the travel path deviate by about ±30 deg of visual angle. Data for all subjects were binned for horizontal gaze angles between –180 to –28 deg and from +28 to +180 deg. These bins represent the top and bottom eighths of the distribution of horizontal gaze angles. The effect of these changes can be seen in <xref ref-type="fig" rid="fig6">Figure 6</xref>. Changes in speed distributions are shown on the left (<xref ref-type="fig" rid="fig6">Figure 6a and b</xref>). The main effect is the tilt of the equal speed contour lines in opposite directions, although speed distributions at the five example locations are not affected very much. Changes in horizontal angle primarily influence the spatial pattern of motion direction. This can be seen in the right side of <xref ref-type="fig" rid="fig6">Figure 6</xref> (in c and d), where rightward or leftward gaze introduces clockwise or counterclockwise rotation in addition to expansion. This makes motion directions more perpendicular to the radial direction of the retinal location of the motion as gaze becomes more eccentric relative to the translation direction. This corresponds to the curl signal introduced by the lateral sway of the body during locomotion or by fixations off the path (<xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>). An example of this pattern can be seen in <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>.</p><fig id="fig6" position="float"><label>Figure 6.</label><caption><title>Effect of horizontal gaze angle on motion speed and direction.</title><p>Horizontal gaze angle is measured relative to the head translation direction. (<bold>a</bold>) Average retinal motion speeds across the visual field. (<bold>b</bold>) Five distributions sampled at different points in the visual field, as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The effect of horizontal gaze angle on retinal motion directions is unremarkable, except for a slight tilt in to contour lines. (<bold>c</bold>) Average retinal flow patterns for leftward and rightward gaze angles. (<bold>d</bold>) Histograms of the average directions plotted in (<bold>c</bold>). These histograms demonstrate the shift of the rotational component of the flow field. Data are pooled across subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig6-v1.tif"/></fig></sec><sec id="s2-6"><title>Terrain effects on motion (independent of vertical gaze angle)</title><p>Since subjects generally look close to the body in rough terrain, and to more distant locations in smooth terrain, the data presented thus far confound the effects of 3D terrain structure with the effects of gaze angle. To evaluate the way the terrain itself influenced speed distributions, while controlling for gaze angle, we sampled from the rough and flat terrain datasets so that the distribution of samples across vertical gaze angle were matched. Thus, the comparison of flat and rocky terrain reflects only the contribution of the terrain structure to the motion patterns, This is shown in <xref ref-type="fig" rid="fig7">Figure 7</xref>. The color maps reveal a somewhat smaller difference between upper and lower visual fields in the rocky terrain than in the flat terrain. This can be seen in the contour plots and also in the distributions shown on the right. Thus, the added motion from the rocky terrain structure attenuated the speed difference between upper and lower visual fields, but otherwise did not affect the distributions very much. Note that the speed difference between upper and lower visual fields is greater than in <xref ref-type="fig" rid="fig4">Figure 4</xref> because gaze angle is controlled for, so the difference reflects the reduced visual angle of the distant ground plane. There is little effect on the motion direction distributions. This might be expected as the direction and speed of the motion vectors resulting from local depth variations are likely to average out over the course of the path.</p><fig id="fig7" position="float"><label>Figure 7.</label><caption><title>Effect of terrain (controlling for vertical gaze) on retinal motion direction and speed.</title><p>While the vertical field asymmetry is slightly greater for the flat terrain, the effects of terrain on retinal motion direction and speed are modest. (<bold>a</bold>) Average retinal motion speeds across the visual field. (<bold>b</bold>) Five distributions sampled at different points in the visual field, as in <xref ref-type="fig" rid="fig4">Figure 4</xref>. (<bold>c</bold>) Average retinal flow patterns for leftward and rightward gaze angles. (<bold>d</bold>) Histograms of the average directions plotted in (<bold>c</bold>). Data are pooled across subjects.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig7-v1.tif"/></fig></sec></sec><sec id="s3" sec-type="discussion"><title>Discussion</title><p>We have summarized the retinal motion statistics experienced during locomotion in a variety of natural terrains. In these retinal motion statistics, we observe asymmetry between upper and lower visual fields, low velocities at the fovea, speeds less than 20–30 deg/s in the central 90 deg (see <xref ref-type="fig" rid="fig4">Figure 4b</xref>), the compression of directions near the horizontal meridian (see <xref ref-type="fig" rid="fig4">Figure 4c</xref>), the preponderance of vertical directions (see <xref ref-type="fig" rid="fig4">Figure 4d</xref>), modulated by lateral gaze positions (see <xref ref-type="fig" rid="fig6">Figure 6d</xref>), resulting in global rotational patterns (see <xref ref-type="fig" rid="fig6">Figure 6c</xref>). In our discussion, we will examine the relationship between these findings and existing work in retinal motion statistics, visual psychophysics, and neurophysiological measurements.</p><p>Note that this work does not (and does not intend to) sample the broad domain of motion input and pertains only to the case of locomotion in the presence of a ground plane. For example, we have not included data from self-motion in carpentered environments, where environmental structure may be close to the eye leading to higher retinal velocities, or environments containing moving objects such as pedestrians and vehicles. Specification of retinal motion in these contexts require similar databases of eye, body, and environment. Nor do the results pertain to eye movement patterns involving smooth pursuit of a moving target or optokinetic nystagmus. However, many of the features of the retinal motion distributions stem from a combination of the ground plane, the saccade and fixate strategy, the gait-induced movements of the head, and the task-linked fixation locations, so one might expect the present observations would cover a substantial domain of experienced motion. While the geometry will depend on factors such as eye height, the velocities scale with the ratio of eye height to walking speed. The results are also general across a range of normal walking speeds. The effect of gaze angle in <xref ref-type="fig" rid="fig5">Figures 5</xref> and <xref ref-type="fig" rid="fig6">6</xref>, and the effect of terrain in <xref ref-type="fig" rid="fig7">Figure 7</xref>, should allow some generalization of the data across terrains and tasks that influence the distribution of gaze locations.</p><p>While many aspects of the global pattern of motion can be understood from the geometry, the quantitative details stem from the way the body moves during the gait cycle and the way gaze behavior varies with terrain. It is therefore important to describe how these factors determine the statistics. Perhaps the most notable feature of the retinal motion is the ubiquitous pattern of very low velocities at the fovea, as expected from the pervasive stabilization of gaze. While the flow field is reminiscent of Gibson’s formulation of optic flow (<xref ref-type="bibr" rid="bib19">Gibson, 1950</xref>), with the focus of expansion centered at the point of gaze, the context is very different from the one Gibson first described, resulting from active stabilization rather than coincidence of gaze with the direction of travel (<xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>). Flow patterns from self-motion are generated only indirectly from body motion. The retinal motion pattern (with low velocities at the fovea) all stems from counter-rotation of the eye in the orbit to keep gaze location stable in the world, in response to the variation in momentary translation and rotation of the head during the gait cycle. This variation in momentary head direction makes head-relative flow a poor signal to guide locomotor direction, as discussed by <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>, who suggest that instead retinal flow patterns are used for control of balance while stepping. This is likely to be important during the instabilities introduced by locomotion.</p><p>In this article, we were not concerned, however, with the how flow signals are used, or in the details of the time-varying aspect of retinal flow signals, but in the average statistics over time, which are generated by gait-induced motion. These statistics might provide a possible template for the way that properties of cortical cells or psychophysical sensitivity might vary across the visual field.</p><sec id="s3-1"><title>The role of stabilizing eye movements in low foveal velocities</title><p>In a previous attempt to capture these statistics, <xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref>; <xref ref-type="bibr" rid="bib14">Calow and Lappe, 2008</xref> also found generally low velocities at the fovea, but they estimated that the gain of the stabilizing rotations was Gaussian with a mean and standard deviation of 0.5 based on pursuit and optokinetic movements. It seems likely that the Vestibular Ocular Reflex (VOR) is the primary source of the stabilizing movements given its low latency and the increase in VOR gain during locomotion associated with spinal modulation (<xref ref-type="bibr" rid="bib16">Dietrich and Wuehr, 2019</xref>; <xref ref-type="bibr" rid="bib23">Haggerty and King, 2018</xref>; <xref ref-type="bibr" rid="bib34">MacNeilage et al., 2012</xref>). It is possible that Optokinetic Nystagmus (OKN) and pursuit are involved as well, but the lower latency of these movements suggest the need for a predictive component. Calow and Lappe’s assumption of low stabilization gain introduces added motion at the fovea and throughout the visual field. Our measurements, and previous ones (<xref ref-type="bibr" rid="bib24">Imai et al., 2001</xref>; <xref ref-type="bibr" rid="bib4">Authié et al., 2015</xref>), indicate that gain in real-world conditions is much closer to unity (1.0), with only modest retinal slip during fixations (10). Given the ubiquitous nature of this kind of stabilizing behavior and the pervasive presence of head movements in natural behavior, the low velocities in the central visual field should be the norm (<xref ref-type="bibr" rid="bib29">Land, 2019</xref>). Consequently, gaze patterns always consisted of a series of brief fixations and small saccades to keep gaze a few steps ahead of the walker. This result supports the assumption of a motion prior centered on zero, proposed by <xref ref-type="bibr" rid="bib44">Weiss et al., 2002</xref>, in order to account for particular motion illusions. Other retinal motion patterns would be experienced during smooth pursuit of a moving target, where there would be motion of the background at the fovea. The geometry will also be different for situations that evoke an optokinetic response, or situations where stabilization at the fovea may be less effective, and our results do not pertain to these situations.</p></sec><sec id="s3-2"><title>Visual field asymmetry and the effects of lateral gaze</title><p>We observed a strong asymmetry in retinal motion speed between the upper and lower visual fields (see <xref ref-type="fig" rid="fig4">Figure 4c</xref>). Further analysis of the modulation due to gaze angle reveals that this asymmetry is stronger for optic flow during gaze ahead (gaze angle of 60–90 deg, see top panel in <xref ref-type="fig" rid="fig5">Figure 5b</xref>) compared to when participants direct their gaze lower in their visual field (gaze angle of 17–45 deg, see lower panel in <xref ref-type="fig" rid="fig5">Figure 5b</xref>).</p><p>The asymmetry between upper and lower visual fields was also observed by <xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref> and <xref ref-type="bibr" rid="bib14">Calow and Lappe, 2008</xref> (in particular, see Figure 2 from <xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref>). They concluded that these asymmetries in flow are driven primarily by the asymmetries in depth statistics. Our findings agree with this conclusion. When the gaze is lower (17–45 deg; see lower panel of <xref ref-type="fig" rid="fig5">Figure 5b</xref>), the depth statistics become more uniform between the upper and lower visual field, resulting in a much smaller difference between the retinal motion speeds in the upper vs. lower visual field. In <xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref>, they relied on a gaze distribution that was not linked to the visual demands of locomotion, so the entirety of their optic flow statistics were examined for gaze vectors between elevations of 70–110 deg. This difference is critical. During visually guided locomotion, participants often lower their gaze to the ground. That lowering of gaze dramatically changes the relative depth statistics of the upper/lower visual field, diminishing the field asymmetries. We also note that this difference in gaze distributions (as well as the difference in VOR gain discussed above) is a big driver of why our retinal motion statistics are somewhat larger in magnitude than those reported in <xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref>.</p><p>Additionally, our data revealed a relationship between retinal speed and direction. Specifically, we found a band of low speeds in the horizontal direction centered on the fovea. <xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref> report no relationships between speed and direction. This difference likely stems from the task-linked fixation locations in this study. <xref ref-type="bibr" rid="bib13">Calow and Lappe, 2007</xref> and <xref ref-type="bibr" rid="bib14">Calow and Lappe, 2008</xref> estimated gaze locations from data where subjects viewed images of scenes or walked in different environments. In both cases, the fixation distributions were not linked to the depth images in a way that depended on immediate locomotor demands. In particular, the rugged terrains we used demanded fixations lower in the visual field to determine footholds, and this affects the angular rotation of the ground plane in pitch around the axis defined by the fixation position (see <xref ref-type="fig" rid="fig1">Figure 1</xref>). The ground fixations also underlie the preponderance of vertical directions across the visual field (see <xref ref-type="fig" rid="fig4">Figure 4</xref>). These fixations on the ground close to the body reduce the speed asymmetry between upper and lower visual fields. Added to this, we found that the motion parallax resulting from the rugged terrain structure itself reduces the asymmetry, independently of gaze angle. Thus, the behavioral goals and their interaction with body movements and terrain need to be taken into account for a complete description of retinal motion statistics.</p></sec><sec id="s3-3"><title>Retinal motion statistics and neural processing</title><p>The variations of motion speed and direction across the visual field have implications for the interpretation of the behavior of neurons in motion sensitive areas. Cells in primary visual cortex and even the retina are sensitive to motion, although it is not clear how sensitive these neural substrates might be to environmental statistics. <xref ref-type="bibr" rid="bib6">Beyeler et al., 2016</xref> have argued that selectivity in the motion pathway, in particular, heading direction tuning in MSTd neurons, could simply be an emergent property of efficient coding of the statistics of the input stimulus, without the specific optimization for decoding heading direction, a function most commonly associated with MSTd. In more recent work, Mineault et al. trained an artificial neural network to relate head direction to flow patterns and found units with properties consistent with those of cells in the dorsal visual pathways. However, neither of <xref ref-type="bibr" rid="bib6">Beyeler et al., 2016</xref> or <xref ref-type="bibr" rid="bib39">Mineault et al., 2021</xref> directly addressed factors such as gait or gaze location relative to the ground plane during walking. These have a significant impact on the both the global patterns of retinal motion across the visual field (which we have discussed here) as well as the time-varying properties of retinal motion. Indeed, simple statistical summaries of input signals and the associated behaviors, such as gaze angle, might prove useful in understanding the responses of motion sensitive neurons if response properties arise as a consequence of efficient coding.</p><p>While these theoretical attempts to predict properties of motion sensitive pathways are encouraging, it is unclear to what extent known properties of cells in motion sensitive areas are consistent with the observed statistics. In an early paper, <xref ref-type="bibr" rid="bib1">Albright, 1989</xref> observed a centrifugal directionality bias at greater eccentricities in MT, as might be expected from optic flow generated by forward movement, and speculated that this might reflect cortical sensitivity to the statistics of visual motion. While this is in principle consistent with our results, the neurophysiological data do not provide enough points of comparison to tell whether the motion sensitivity across the visual field is constrained by the current estimates of the statistics either because the stimuli to not resemble natural motion patterns or because responses are not mapped across the visual field.</p><p>Given the ubiquitous presence of a ground plane, one might expect that the preferred speed and direction of MT neurons resemble that pattern across the visual field as shown <xref ref-type="fig" rid="fig4">Figure 4</xref> with low speed preferences near the fovea, and increasing speed preferences with eccentricity. The asymmetry between speed preferences in upper and lower visual fields and the preponderance of vertical motion direction preferences (as seen in <xref ref-type="fig" rid="fig4">Figure 4</xref>) might also be observed in motion-sensitive areas as well as other features of the statistics we observe here such as the flattening of the equal speed contours. In MT, <xref ref-type="bibr" rid="bib37">Maunsell and Van Essen, 1987</xref> found asymmetry in the distribution of receptive fields in upper and lower visual fields with a retinotopic over-representation in the lower visual field that might be related to the relative prominence of high velocities in the lower visual field, or more generally, might be helpful in processing ground plane motions used for navigation. Other work, however, is inconsistent with the variations observed here. For example, Maunsell and van Essen mapped speed preferences of MT neurons as a function of eccentricity out to 24 deg. While there was a weak tendency for preferred speed to increase with eccentricity, the relationship was more variable than expected from the statistics in <xref ref-type="fig" rid="fig4">Figure 4</xref>. <xref ref-type="bibr" rid="bib14">Calow and Lappe, 2008</xref> modeled cortical motion processing based on retinal motion signals by maximizing mutual information between the retinal input and neural representations. They found general similarities between the tuning functions of cells in MT and their predictions. However, as in our work, it was difficult to make direct comparisons with the observed properties of cells in primate motion sensitive areas.</p><p>The role of MSTd in processing optic flow is well studied in both human and monkey cortex (<xref ref-type="bibr" rid="bib17">Duffy and Wurtz, 1991</xref>; <xref ref-type="bibr" rid="bib31">Lappe et al., 1999</xref>; <xref ref-type="bibr" rid="bib12">Britten, 2008</xref>; <xref ref-type="bibr" rid="bib22">Gu et al., 2010</xref>; <xref ref-type="bibr" rid="bib40">Morrone et al., 2000</xref>; <xref ref-type="bibr" rid="bib42">Wall and Smith, 2008</xref>), and activity in these areas appears to be linked to heading judgments. From our data, we might expect that the optimal stimuli for the large MSTd receptive fields would be those that reflected the variation with eccentricity seen in <xref ref-type="fig" rid="fig4">Figure 4</xref>. However, there is no clear consensus on the role of these regions in perception of self-motion. Many of the stimuli used in neurophysiological experiments have been inconsistent with the effects of self-motion since the region of low or zero velocity was displaced from the fovea. However, neurons respond vigorously to such stimuli, which would not be expected if the statistics of experience shape the properties of these cells. In humans, MST responds to motion patterns inconsistent with self-motion in a manner comparable to responses to consistent patterns (<xref ref-type="bibr" rid="bib42">Wall and Smith, 2008</xref>). Similarly in monkeys, MST neurons respond to both kinds of stimulus patterns (<xref ref-type="bibr" rid="bib18">Duffy and Wurtz, 1995</xref>; <xref ref-type="bibr" rid="bib15">Cottereau et al., 2017</xref>; <xref ref-type="bibr" rid="bib41">Strong et al., 2017</xref>).</p><p>A different way that the present results might have implications for neural processing is in possible effects of gaze angle on cell responses. Analysis of the effect of different horizontal and vertical gaze angles revealed a marked dependence of motion direction on horizontal gaze angle and of motion speed on vertical gaze angle. Motion directions were biased in the counterclockwise direction when subjects looked to the left of their translation direction, and clockwise when they looked to the right. The distribution of speed became more radially symmetric as subjects looked closer to their feet. These effects have implications for the role of eye position in processing optic flow, given the relationship between eye orientation and flow patterns. This relationship could be learned and exploited by the visual system. Effects of eye position on response properties of visual neurons have been extensively observed in a variety of different regions, including MT and MST (<xref ref-type="bibr" rid="bib10">Bremmer et al., 1997</xref>; <xref ref-type="bibr" rid="bib3">Andersen et al., 1990</xref>; <xref ref-type="bibr" rid="bib9">Boussaoud et al., 1998</xref>). It is possible that such eye direction tuning could be used to compute the expected flow patterns and allow the walker to detect deviations from this pattern. <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref> suggested that the variation of the retinal flow patterns through the gait cycle could be learnt and used to monitor whether the variation in body movement was consistent with stable posture. Neurons that compute the consistency between expected and actual retinal motion have been observed in the primary visual cortex of mice, where firing rate is related to the degree of mismatch between the experimentally controlled and the anticipated motion signal given a particular movement (<xref ref-type="bibr" rid="bib46">Zmarz and Keller, 2016</xref>). It is possible that a similar strategy might be employed to detect deviations from expected flow patterns resulting from postural instability.</p><p>The most closely related work on neural processing in MSTd has carefully mapped the sensitivity of neurons to translation, rotation, expansion, and their combinations, beginning with <xref ref-type="bibr" rid="bib18">Duffy and Wurtz, 1995</xref>; <xref ref-type="bibr" rid="bib21">Graziano et al., 1994</xref>. This work explicitly investigates stimuli that simulate a ground plane (<xref ref-type="bibr" rid="bib30">Lappe et al., 1996</xref>) and encompasses the relationship of that tuning with eye movements (<xref ref-type="bibr" rid="bib10">Bremmer et al., 1997</xref>; <xref ref-type="bibr" rid="bib3">Andersen et al., 1990</xref>; <xref ref-type="bibr" rid="bib9">Boussaoud et al., 1998</xref>; <xref ref-type="bibr" rid="bib11">Bremmer et al., 2010</xref>) and tasks like steering (<xref ref-type="bibr" rid="bib25">Jacob and Duffy, 2015</xref>). One of the limitations of these findings is that they rely on simulated visual cues for movement through an environment and thus result in inconsistent sensory cues. However, recent advances allow for neurophysiological measurements and eye tracking during experiments with head-fixed running, head-free, and freely moving animals (from mice to marmosets and even macaque monkeys). These emerging paradigms will allow the study of retinal optic flow processing in contexts that do not require simulated locomotion. We caution that these experiments will need to take a decidedly comparative approach as each of these species has a distinct set of body movement patterns, eye movement strategies, a different relationship to the ground-plane, and different tasks it needs perform. Our findings in this article strongly suggest that all of these factors will impact the retinal optic flow patterns that emerge, and perhaps the details of the tuning that we should expect to find in neural populations sensitive to motion. While the exact relation between the retinal motion statistics we have measured and the response properties of motion-sensitive cells remains unresolved, the emerging tools in neurophysiology and computation make similar approaches with different species more feasible.</p></sec><sec id="s3-4"><title>Conclusions</title><p>In summary, we have measured the retinal motion patterns generated when humans walked through a variety of natural terrains. The features of these motion patterns stem from the motion of the eye while walkers hold gaze stable at a location in the scene, and the retinal image expands and rotates as the body moves through the gait cycle. The ground plane imparts an increasing speed gradient from upper to lower visual field that is most likely a ubiquitous feature of natural self-generated motion. Gaze location varies with terrain structure as walkers look closer to their bodies to control foot placement in more rugged terrain, and this reduces the asymmetry between upper and lower visual fields, as does the motion resulting from the more complex 3D terrain structure. Fixation on the ground plane produces a preponderance of vertical directions, and lateral rotation of the eye relative to body direction, both during the gait cycle and while searching for suitable paths, changes the distribution of motion directions across the visual field. Thus, an understanding of the complex interplay between behavior and environment is necessary for understanding the consequences of self-motion on retinal input.</p></sec></sec><sec id="s4" sec-type="methods"><title>Methods</title><sec id="s4-1"><title>Experimental task and data acquisition</title><p>Processed data used to generate figures is shared via Dryad. Code to generate figures is shared via Zenodo. Raw data as well as analysis and visualization is available via Dryad.</p><sec id="s4-1-1"><title>Participants</title><p>Two datasets were used in this study. Both were collected using the same apparatus, but from two separately conducted studies with similar experimental conditions. One group of participants (n = 3, two males, one female) was recruited with informed consent in accordance with the Institutional Review Board at the University of Texas at Austin (approval number 2006-06-0085) and collected in an Austin area rocky creek bed. The second participant group (n = 8, four males, four females) was recruited with informed consent in accordance with the Institutional Review Board at The University of California Berkeley (2011-07-3429) and collected in a nearby state park. Subjects were either normal or corrected to normal acuity, and ranged in age from 24 to 54 y.</p></sec><sec id="s4-1-2"><title>Equipment</title><p>Infrared eye recordings, first-person scene video, and body movements of all participants were recorded using a Pupil Labs mobile eye tracker (<xref ref-type="bibr" rid="bib26">Kassner et al., 2014</xref>) combined with a Motion Shadow full-body IMU-based motion capture system (Motion Shadow, Seattle, WA). The eye tracker has two infrared eye cameras and a single outward-facing scene camera. Each eye camera records at 120 Hz at 640 × 480 resolution, while the outward-facing scene camera records at 30 Hz with 1920 × 1080 pixel resolution, with a 100 deg diagonal field of view. The scene camera is situated approximately 3 cm above the right eye. The Shadow motion capture system is comprised of 17 three-axis accelerometer, gyroscope, and magnetometer sensors. The readings from the suit are processed by software to estimate joint positions and orientations of a full 3D skeleton. The sensors record at 100 Hz, and the data is later processed using custom MATLAB code (MathWorks, Natick, MA). See <xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref> and <xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref> for more details.</p></sec><sec id="s4-1-3"><title>Experimental task</title><p>For both groups of participants (Austin dataset and Berkeley dataset), the experimental task was similar, with variation in the terrain types between the two locations. For the Berkeley participants, the task involved walking back and forth along a loosely defined hiking trail that varied in terrain features and difficulty. This trail was traversed two times in each direction by each participant. Different portions of the trail were pre-examined and designated as distinct terrain types, being labeled as one of pavement, flat (packed earth), medium (some irregularities such as rocks and roots), and bark (large pieces of bark and roots but otherwise not rocky), and rocks (large rocks that constrained the possible step locations). Examples of the terrain are shown in <xref ref-type="fig" rid="fig2">Figure 2</xref>. For the Austin participants, the task involved walking back and forth along a rocky dried out creek bed. Participants walked three times in each direction. This is the same terrain used in <xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref>. This terrain difficulty is most comparable to the rocks condition in the Berkeley dataset. For each of the rocky terrains, the ground was sufficiently complex that subjects needed to use visual information in order to guide foot placement (see <xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref> for more details).</p></sec><sec id="s4-1-4"><title>Calibration and postprocessing</title><p>At the start of each recording session, subjects were instructed to stand on a calibration mat that was used for all subjects. The calibration mat had marked foot locations that were at a fixed distance from a calibration point 1.5 m away from the foot locations. Subjects were then instructed to maintain fixation on this calibration point throughout a calibration process. The calibration process involved rotating the head while maintaining fixation, to eight different predetermined head orientations (the cardinal and oblique directions). This segment of each subjects recording was later used to align and calibrate the data. This was done by finding an optimal rotation that aligned the mobile eye tracker’s coordinate system to that of the motion capture system, such that the distance between the projected gaze vector and the calibration point on the mat was minimized. This rotation was then applied to the position data in the recording, aligning the rest of the recording in space. Each system’s timestamps were then used to align the recording temporally as timestamps were recorded to a single laptop computer on a backpack worn by subjects throughout the recording. (See <xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref> for more details.) The 100 Hz motion capture data was upsampled using linear interpolation to match the 120 Hz eye tracker data.</p></sec><sec id="s4-1-5"><title>Fixation detection</title><p>During locomotion, walkers typically maintain gaze on a stable location in the world as the body moves forward (<xref ref-type="bibr" rid="bib35">Matthis et al., 2018</xref>). During these periods, the eye counter-rotates in the orbit, driven largely by the vestibuloocular reflex, although other eye movement systems might also be involved (<xref ref-type="bibr" rid="bib36">Matthis et al., 2021</xref>). We refer to these periods of stable gaze in the world in the presence of a smooth compensatory eye rotation as ‘fixations,’ although in more common usage of the term the eye is stable in the orbit and the head is fixed. In order to analyze the retinal motion input, we needed to differentiate between saccades and periods of stable gaze since vision is suppressed during saccades, and the visual information used for guidance of locomotion is acquired during the fixation periods. We therefore needed to segment the eye position data into fixations and saccades. The presence of smooth eye movements during stable gaze makes detection of fixations more difficult. We used a velocity and acceleration threshold method with thresholds set such that detected fixations best match hand-coded fixations. The velocity threshold was 65 deg/s velocity and the acceleration threshold was 5 deg/s<sup>2</sup>. Frames of the recording that fall below these thresholds are labeled as fixation frames, and sequential fixation frames are grouped accordingly into fixation instances. The velocity threshold is quite high in order to accommodate the smooth counter-rotations of the eye in the orbit during stabilization. Fixation identified by this algorithm were then checked against manual coding using the trace of velocity versus time.</p></sec><sec id="s4-1-6"><title>Fixation idealization via initial location pinning</title><p>Incomplete stabilization of gaze location in the world during a fixation will add image motion. Analysis of these small eye movements revealed that stabilization of gaze location in the world was very effective. The effectiveness of stabilization of gaze was determined by measuring the deviation of gaze from the initially fixated location over the duration of the fixation. This slippage had a median deviation of 0.83 deg and a mode of 0.26 deg (see below for more details). This image slippage reflects a combination of imperfect stabilization, and artifacts such as small saccades that were misclassified as fixations, and noise in eye tracker signal. In order to simplify the analysis, we first ignore image slip during a fixation and do the analysis as if gaze were fixed at the initial location for the duration of the fixation. We evaluate the impact of this idealization below. Note that in Calow and Lappe’s previous studies they assumed that the stabilization gain varied randomly between 0 and 0.5, which would correspond to much less effective stabilization. This estimate was based on experiments where the subject was stationary, so mostly likely pursuit or optokinetic nystagmus were evoked by the image motion. These eye movement systems have very different drivers and properties than the vestibular-ocular reflex that is most likely responsible for the stabilizing behavior observed during locomotion. Other demonstrations of the precision of stabilization during locomotion have shown good stabilization of the foveal image on the retina (refs). Since spinal mechanisms increase the gain of the vestibular-ocular reflex during locomotion it seems most likely that VOR is the primary contributor, given its low latency, although it is possible that OKN and potentially predictive pursuit make some contribution (<xref ref-type="bibr" rid="bib16">Dietrich and Wuehr, 2019</xref>; <xref ref-type="bibr" rid="bib23">Haggerty and King, 2018</xref>; <xref ref-type="bibr" rid="bib34">MacNeilage et al., 2012</xref>).</p></sec><sec id="s4-1-7"><title>Retinocentric coordinate system and eye movement directions</title><p>The first goal of the study was to compute the retinal motion statistics caused by movement of the body during the periods of (idealized) stable gaze, which we refer to here as fixations. During the periods of stable gaze, retinal motion is caused by expansion of the image as the walker takes a step forward, together with the counter-rotations of the eye in the orbit induced by gait, around the yaw axis.</p><p>Eye movement directions are computed over sequential frame pairs in the spatial reference frame of the first frame in the pair. This is done by considering sequential frame pairs within a fixation or a saccade. Then eye coordinate basis vectors are calculated for the first frame in a pair. We first define an eye relative coordinate system X, Y, Z. Gaze direction is the third dimension (orthogonal to the plane within which the eye movement direction will be calculated), the first dimension is the normalized cross-product between the eye direction and the negative gravity in world coordinates. The second dimension is the cross-product between the first and third dimensions. Thus, the basis X, Y, Z changes on each frame of the recording. This convention assumes that there is no torsion about the viewing axis of the eye, and so one dimension (the first) stays within a plane perpendicular to gravity. Eye movements and retinal motion are both represented as rotations within this coordinate system. These rotations last for one frame of the recording and are described by a start direction and end direction in X, Y, Z. Using these coordinates, the next eye direction (corresponding to the eye movement occurring over frame pairs) is computed in the reference frame of the first eye direction’s coordinates (<inline-formula><mml:math id="inf6"><mml:mrow><mml:mi mathsize="120%">x</mml:mi><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mi mathsize="120%">y</mml:mi><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mi mathsize="120%">z</mml:mi></mml:mrow></mml:math></inline-formula>). The direction is then computed as<disp-formula id="equ3"><mml:math id="m3"><mml:mrow><mml:mi mathsize="120%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">t</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">a</mml:mi><mml:mo>⁢</mml:mo><mml:mi mathsize="120%">n</mml:mi><mml:mo>⁢</mml:mo><mml:mn mathsize="120%">2</mml:mn><mml:mo>⁢</mml:mo><mml:mrow><mml:mo maxsize="120%" minsize="120%">(</mml:mo><mml:mi mathsize="120%">y</mml:mi><mml:mo mathsize="120%" stretchy="false">,</mml:mo><mml:mi mathsize="120%">x</mml:mi><mml:mo maxsize="120%" minsize="120%">)</mml:mo></mml:mrow></mml:mrow></mml:math></disp-formula></p><p>A schematic depicting this coordinate system can be seen in <xref ref-type="fig" rid="fig8">Figure 8</xref>. The eye tracker combined the estimates of gaze point from the two eyes that was used as the point of fixation. We estimate retinal motion on a single cyclopean retina fixated on that point.</p><fig id="fig8" position="float"><label>Figure 8.</label><caption><title>Schematic depicting eye relative coordinate system.</title><p>Left and right show basis vectors used from different viewpoints. Z corresponds to the gaze direction in world coordinates. Then X is the vector perpendicular to both Z and the gravity vector. Finally, the Y coordinate is the vector perpendicular to both X and Z. The X vector thus resides within the plane perpendicular to gravity.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig8-v1.tif"/></fig></sec></sec><sec id="s4-2"><title>3D terrain structure measurement using photogrammetry</title><p>Terrain reconstructions used a computer vision method called photogrammetry, which allows simultaneous estimation of camera poses and environmental structure from sequences of camera images. This was handled using an open-source photogrammetry tool, Meshroom (<xref ref-type="bibr" rid="bib2">AliceVision, 2018</xref>), which bundles various well-established algorithms for different steps in a mesh reconstruction process. The first step is to extract image features from each frame of the input sequence using the SIFT (Ref 93Karl) algorithm and then pairs of image matches are found across the image sequences, allowing calculation of the displacement across the matched images. This contains information about how the camera moved through the environment, as well as information about the structure of the environment. With image pairs selected, as well as corresponding feature locations computed, scene geometry and camera position and orientation for each image pair can then be estimated using structure from motion methods. Structure from motion yields a sparse reconstruction of the environment, represented by 3D points in space, that is used to estimate a depth for each pixel of the input images using the estimated camera pose for each image and the 3D points in view of that camera pose. After each camera view has an estimated depth map, all depth maps are fused into a global octree that is used to create a 3D triangle mesh. For the terrain reconstruction, we leveraged the 3D triangle mesh output, which contained information about the structure of the environment. Blender’s z-buffer method was used in order to calculate depth maps, and these depth maps were then used to compute retinal motion inputs using geometry.</p><p>Optic flow estimation relied on 3D terrain reconstruction provided by Meshroom. Default parameters were used in Meshroom, with the exception of specifying camera intrinsics (focal length in pixels, and viewing angle in degrees). RGB image-based scene reconstruction is subject to noise that will be influenced by the movement of the camera and its distance from the terrain. In order to evaluate the accuracy of the 3D reconstruction, we took advantage of the terrain meshes calculated from different traversals of the same terrain by an individual subject and also by the different subjects. Thus, for the Austin dataset we had 12 traversals (out and back three times by two subjects). Easily identifiable features in the environment (e.g. permanent marks on rocks) were used in order to align coordinate systems from each traversal. A set of corresponding points between two sets of points can be used in order to compute a single similarity transform between those points. Then the iterative closest point method is used to align the corresponding point clouds at a finer scale by iteratively rotating and translating the point cloud such that each point moves closer to its nearest neighbor in the target point cloud. The process is repeated for each recording, and the resulting coordinate transformation is then applied to all recordings respectively such that they are all in the same coordinate frame. Foothold locations were estimated with the different aligned meshes using a method that found the closest location on a mesh to the motion capture system’s foothold location measurement. The distances between corresponding foothold locations between meshes were then measured. There was high agreement between terrain reconstructions, with a median distance of approximately 3 cm, corresponding to 0.5 deg of visual angle. Thus, the 3D structure obtained is quite consistent between different reconstructions of the same terrain. An example of the terrain reconstruction can be seen in <xref ref-type="fig" rid="fig9">Figure 9</xref>. Photogrammetry can be sensitive to lighting, reflections, and other optical artifacts, but most of our recordings were done during brighter times of the day. Reconstruction in the vicinity of where subjects were walking was almost always good and intact. For one subject, certain portions of the recording had to be excluded due to poor reconstruction of the terrain resulting from bad lighting.</p><fig id="fig9" position="float"><label>Figure 9.</label><caption><title>Sample of the terrain reconstruction is rendered as RGB image showing the level of detail provided by the photogrammetry.</title></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig9-v1.tif"/></fig><sec id="s4-2-1"><title>Retinal motion</title><p>For retinal motion, first a set of 62,500 (produced with a method that uses a 250 × 250) vectors in eye relative coordinates are computed. These vectors tile visual space going from eccentricity 0–45 deg and cover 360 deg of polar angle. These vectors encode the position in visual space of a pixel at a particular position in the original 250 × 250 grid. The virtual camera described in ‘Optic flow estimation’ is modeled as a perspective camera with 250 × 250 pixels, with a field of view of 45 deg. Therefore, the depth values at each pixel correspond to depth at the encoded location in visual space according to the 62,500 vectors. These depth values are used to calculate where a given location in 3D space (one for each pixel) moves in the current eye relative coordinate system from the current frame of the recording to the next. This results in two 3D vectors in eye relative coordinates. This allows computation of a speed (angle between these two vectors), as well as a direction (which is computed from the vector between the two vectors in the 250 × 250 grid space). Thus, each of the 250 × 250 positions in visual space has an associated speed and direction of visual motion for each frame. So for each subject, for each frame of data, we have 62,500 different speeds and directions of movement (each at a different location in visual space). When building the histograms, we put each of these 62,500 values per frame into a histogram bin according to the bin edges described above. Once that is done, the counts are normalized such that they sum to 1.</p></sec><sec id="s4-2-2"><title>Combining data across subjects</title><p>Once the gaze points were identified in the 3D reconstruction, gaze angle distributions as a function of terrain were calculated for a given location and then pooled across fixations and trials for a given subject. Individual subjects plots are shown in Figure 14. Similarly, once the gaze points were identified, retinal motion vectors were calculated for a given location and then pooled across fixations and trials. This gave density distributions for speed and direction across the visual field like those shown in <xref ref-type="fig" rid="fig4">Figure 4</xref> for individual subjects. Individual subject data are shown in Figures 15 and 16. Individual subject data was averaged to create <xref ref-type="fig" rid="fig4">Figure 4</xref>. Data in <xref ref-type="fig" rid="fig5">Figures 5</xref>—<xref ref-type="fig" rid="fig7">7</xref> were pooled across subjects, rather than averaged, as the data were binned over large angles and the point of the figures was to reveal the way the plots change with gaze angle and terrain. <xref ref-type="fig" rid="fig10">Figures 10</xref> and <xref ref-type="fig" rid="fig11">11</xref> are also pooled over subjects (<xref ref-type="fig" rid="fig12">Figures 12</xref> and <xref ref-type="fig" rid="fig13">13</xref>).</p><fig id="fig10" position="float"><label>Figure 10.</label><caption><title>Retinal image slippage.</title><p>Normalized relative frequency histogram of the deviation from the location of the initial fixation over the course of a fixation. Initial fixation location is computed and tracked over the course of each fixation, and compared to current fixation for duration of each fixation. Median deviation value is calculated for each fixation. The histogram captures the extent of variability of initially fixated locations relative to the measured location of the fovea over the course of fixations, with most initially fixated locations never deviating more than 2 deg of visual angle during a fixation.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig10-v1.tif"/></fig><fig id="fig11" position="float"><label>Figure 11.</label><caption><title>Effects of 3.2 deg/s slip on velocity.</title><p>The median slip of approximately 0.8 deg during a 250 ms fixation would lead to a retinal velocity of 3.2 deg/s at the fovea. The figure shows how retinal slip of this magnitude would affect speed distributions across the retina. The flow fields in the two cases (perfect stabilization and 0.8 deg of slip) are shown on the left, and the speed distributions are shown on the right. The bottom plot shows a heat map of the difference.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig11-v1.tif"/></fig><fig id="fig12" position="float"><label>Figure 12.</label><caption><title>Speed and direction distributions for saccades.</title><p>The distribution of angular velocities of the eye during saccadic eye movements is shown on the left. On the right is a 2D histogram of vertical and horizontal components of the saccades.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig12-v1.tif"/></fig><fig id="fig13" position="float"><label>Figure 13.</label><caption><title>Effect of saccades on speed distributions.</title><p>Speed distributions at the five locations in the visual field shown in <xref ref-type="fig" rid="fig4">Figure 4</xref>. The top panel is replotted from <xref ref-type="fig" rid="fig4">Figure 4</xref>, the middle panel shows these distributions with the saccades added, and the bottom panel shows the contribution from the saccades alone.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig13-v1.tif"/></fig><fig id="fig14" position="float"><label>Figure 14.</label><caption><title>Individual participant data – compare to <xref ref-type="fig" rid="fig2">Figure 2</xref>.</title><p>Histograms of vertical gaze angle (angle relative to the direction of gravity) across different terrain types. In very flat, regular terrain (e.g. pavement, flat) participant gaze accumulates at the horizon (90°). With increasing terrain complexity, participants shift gaze downward (30°–60°). A total of 10 participants are shown. Participants A1–2 correspond to two participants whose data was collected in Austin, TX. The data from these participants includes only the ‘rocks’ terrain. B3–B10 correspond to eight participants whose data was collected in Berkeley, CA.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig14-v1.tif"/></fig><fig id="fig15" position="float"><label>Figure 15.</label><caption><title>Individual participant data – compare to <xref ref-type="fig" rid="fig4">Figure 4</xref> (five participants: A1–A2, B3–B5).</title><p>Column 1: average speed of retinal motion signals as a function of retinal position. Speed is color mapped (blue = slow, red = fast). The average is computed across all subjects and terrain types. Speed is computed in degrees of visual angle per second. Column 2: speed distributions at five points in the visual field (the fovea and four cardinal locations). Column 3: average retinal flow pattern. Column 4: Histogram of the average retinal motion directions (Column 3) as a function of polar angle.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig15-v1.tif"/></fig><fig id="fig16" position="float"><label>Figure 16.</label><caption><title>Individual participant data – compare to <xref ref-type="fig" rid="fig4">Figure 4</xref> (five participants: B6–B10).</title><p>Column 1: average speed of retinal motion signals as a function of retinal position. Speed is color mapped (blue = slow, red = fast). The average is computed across all subjects and terrain types. Speed is computed in degrees of visual angle per second. Column 2: speed distributions at five points in the visual field (the fovea and four cardinal locations). Column 3: average retinal flow pattern. Column 4 Histogram of the average retinal motion directions (in Column 3) as a function of polar angle. Note: B6’s data is absent because the angle of head camera during data collection precluded proper estimates of retinal flow across the visual field and resulted in poor terrain reconstruction.</p></caption><graphic mimetype="image" mime-subtype="tiff" xlink:href="elife-82410-fig16-v1.tif"/></fig></sec></sec><sec id="s4-3"><title>Retinal slip during fixations</title><p>To measure the extent of the retinal slip during a fixation, we took the gaze location in the camera image at the first fixation frame and then used optic flow vectors computed by Deepflow (<xref ref-type="bibr" rid="bib43">Weinzaepfel et al., 2013</xref>) to track this initial fixation location for the duration of the fixation. This is done by indexing the optic flow vector field at the initial fixation location, measuring its displacement across the first frame pair, computing its new location in the next frame, and then measuring the flow vector at this new location. This is repeated for each frame in the fixation. The resulting trajectory is that of the initial fixation location in camera image space over the duration of the fixation. For each frame of the fixation, the actual gaze location is then compared to the current location of the initial fixation location (with the first frame excluded since this is how the initial fixation is defined). The locations are converted into 3D vectors as described in ‘Optic flow estimation,’ and the median angular distance between gaze location and initial fixation location is computed for each fixation.</p><p>Data from this analysis can be seen in <xref ref-type="fig" rid="fig10">Figure 10</xref>, which plots the normalized relative frequency of the retinal slip during fixations. The mode of the distribution is 0.26 deg and median was 0.83 deg. This is quite good, especially given that the width of a foothold a few steps ahead will subtend about 2 deg and it is unlikely that foot placement requires more precise specification. It is likely that the long tail of the distribution results from errors in specifying the fixations rather than failure of stabilization. In particular, some small saccades are most likely included in the fixations. Other sources of error come from the eye tracker itself. Another source of noise comes from the fact that the image-based slip calculations were done at 30 Hz. Treadmill studies have found long periods (about 1.5 s) of image stability (defined as less than 4 deg/s of slip) during slow walking, with this being decreased to 213 ms for faster walking (<xref ref-type="bibr" rid="bib16">Dietrich and Wuehr, 2019</xref>). <xref ref-type="bibr" rid="bib4">Authié et al., 2015</xref> also found excellent stabilization in subjects walking short paths in a laboratory setting.</p><p>Manual reintroduction of retinal slip (which our measurements suggest arise from a gain in the VOR of &lt;1) simply results in added downward motion to the entire visual field, whose magnitude is equivalent to the slip. This can be seen in <xref ref-type="fig" rid="fig11">Figure 11</xref>. Taking a median value for the slip during a 250 ms fixation of approximately 0.8 deg, the added retinal velocity would be 3.2 deg/s at the fovea.</p><p>We computed how retinal slip of this magnitude would affect speed distributions across the retina. The flow fields in the two cases (perfect stabilization and 0.8 deg of slip) are shown on the left of the figure, and the speed distributions are shown on the right. The bottom plot shows a heat map of the difference. The added slip also has the effect of slightly shifting structure in the motion pattern upward, by however far from the fovea the eccentric location with the same speed as the slip is. When considering the average signal, this shifts the zero point upward to 4 deg of eccentricity for 4 deg/s of retinal slip. The change in speed is quite small, and the other structural features of the signal are conserved (including the radially asymmetric eccentricity versus speed gradient and the variation with gaze angle).</p></sec><sec id="s4-4"><title>Motion generated by saccades</title><p>We have thus far considered only the motion patterns generated during the periods of stable gaze since this is the period when useful visual information is extracted from the image. For completeness, we also examine the retinal motion patterns generated by saccades since this motion is incident on the retina, and it is not entirely clear how these signals are dealt with in the cortical hierarchy.</p><p>First, we show the distribution of saccade velocities and directions in <xref ref-type="fig" rid="fig12">Figure 12</xref>. Saccade velocities are generally less than 150 deg/s as might be expected from the high frequency of small movements to upcoming foothold locations. The higher velocities are likely generated as the walkers saccade from near to far, and vice versa. The direction distribution shows the over-representation of upward and downward saccades. The upward saccades most likely reflect gaze changes toward new foothold further along the path. Some of the downward saccades are from nearby locations to closer ones when more information is needed for stepping.</p><p>The effect of saccades on retinal image velocities is shown in <xref ref-type="fig" rid="fig13">Figure 13</xref>. The speed of the saccade adds to an instantaneous motion input and shifts motion directions in the opposite direction of the saccade. The figure shows retinal speed distributions at the five locations across the visual field shown in the previous figures. The top panel shows the distributions previously described for the periods of stabilization. The middle panel shows retinal speed distributions for the combined data. This shows that the speeds added by the saccades add a high-speed lobe to the data without affecting the pattern during periods of stabilization to any great extent. The lower peak resulting from the saccades results from the fact that saccades are mostly of short duration and so account for a lesser fraction of the total data. As expected, the saccades contribute high speeds to the retinal image motion that are largely outside the domain of the retinal speeds resulting from self-motion in the presence of stabilization.</p></sec></sec></body><back><sec sec-type="additional-information" id="s5"><title>Additional information</title><fn-group content-type="competing-interest"><title>Competing interests</title><fn fn-type="COI-statement" id="conf1"><p>No competing interests declared</p></fn><fn fn-type="COI-statement" id="conf2"><p>No competing interests declared</p></fn></fn-group><fn-group content-type="author-contribution"><title>Author contributions</title><fn fn-type="con" id="con1"><p>Conceptualization, Data curation, Software, Formal analysis, Supervision, Validation, Investigation, Visualization, Methodology, Writing - original draft</p></fn><fn fn-type="con" id="con2"><p>Conceptualization, Data curation, Software, Supervision, Visualization, Methodology, Writing – review and editing</p></fn><fn fn-type="con" id="con3"><p>Conceptualization, Data curation, Methodology, Software, Visualization, Writing – review and editing</p></fn><fn fn-type="con" id="con4"><p>Conceptualization, Formal analysis, Writing – review and editing</p></fn><fn fn-type="con" id="con5"><p>Conceptualization, Resources, Formal analysis, Supervision, Funding acquisition, Writing – review and editing</p></fn><fn fn-type="con" id="con6"><p>Conceptualization, Resources, Data curation, Software, Formal analysis, Supervision, Funding acquisition, Validation, Investigation, Visualization, Methodology, Writing - original draft, Writing – review and editing</p></fn></fn-group><fn-group content-type="ethics-information"><title>Ethics</title><fn fn-type="other"><p>Human subjects: Informed consent and consent to publish was obtained and protocols were approved by the institutional IRBs at University of Texas Austin approval number 2006-06-0085 and UC Berkeley approval number 2011-07-3429.</p></fn></fn-group></sec><sec sec-type="supplementary-material" id="s6"><title>Additional files</title><supplementary-material id="mdar"><label>MDAR checklist</label><media xlink:href="elife-82410-mdarchecklist1-v1.docx" mimetype="application" mime-subtype="docx"/></supplementary-material></sec><sec sec-type="data-availability" id="s7"><title>Data availability</title><p>Processed data used to generate figures is shared via Dryad. Code to generate figures is shared via Zenodo. Raw data as well as analysis and visualization is available via Dryad.</p><p>The following datasets were generated:</p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset1"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Behavior shapes retinal motion statistics during natural locomotion</data-title><source>Dryad Digital Repository</source><pub-id pub-id-type="doi">10.5061/dryad.zcrjdfngp</pub-id></element-citation></p><p><element-citation publication-type="data" specific-use="isSupplementedBy" id="dataset2"><person-group person-group-type="author"><name><surname>Hayhoe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2023">2023</year><data-title>Behavior shapes retinal motion statistics during natural locomotion</data-title><source>Zenodo</source><pub-id pub-id-type="doi">10.5281/zenodo.7055548</pub-id></element-citation></p></sec><ack id="ack"><title>Acknowledgements</title><p>This work was supported by NIH grants R01 EY05729, T32 LM012414, and U01 NS116377.</p></ack><ref-list><title>References</title><ref id="bib1"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Albright</surname><given-names>TD</given-names></name></person-group><year iso-8601-date="1989">1989</year><article-title>Centrifugal directional bias in the middle temporal visual area (mt) of the macaque</article-title><source>Visual Neuroscience</source><volume>2</volume><fpage>177</fpage><lpage>188</lpage><pub-id pub-id-type="doi">10.1017/s0952523800012037</pub-id><pub-id pub-id-type="pmid">2487646</pub-id></element-citation></ref><ref id="bib2"><element-citation publication-type="software"><person-group person-group-type="author"><collab>AliceVision</collab></person-group><year iso-8601-date="2018">2018</year><data-title>Meshroom: A 3D reconstruction software</data-title><version designator="2f36b2a">2f36b2a</version><source>Github</source><ext-link ext-link-type="uri" xlink:href="https://github.com/alicevision/Meshroom">https://github.com/alicevision/Meshroom</ext-link></element-citation></ref><ref id="bib3"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Bracewell</surname><given-names>RM</given-names></name><name><surname>Barash</surname><given-names>S</given-names></name><name><surname>Gnadt</surname><given-names>JW</given-names></name><name><surname>Fogassi</surname><given-names>L</given-names></name></person-group><year iso-8601-date="1990">1990</year><article-title>Eye position effects on visual, memory, and saccade-related activity in areas lip and 7A of macaque</article-title><source>The Journal of Neuroscience</source><volume>10</volume><fpage>1176</fpage><lpage>1196</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.10-04-01176.1990</pub-id><pub-id pub-id-type="pmid">2329374</pub-id></element-citation></ref><ref id="bib4"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Authié</surname><given-names>CN</given-names></name><name><surname>Hilt</surname><given-names>PM</given-names></name><name><surname>N’Guyen</surname><given-names>S</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name><name><surname>Bennequin</surname><given-names>D</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Differences in gaze anticipation for locomotion with and without vision</article-title><source>Frontiers in Human Neuroscience</source><volume>9</volume><elocation-id>312</elocation-id><pub-id pub-id-type="doi">10.3389/fnhum.2015.00312</pub-id><pub-id pub-id-type="pmid">26106313</pub-id></element-citation></ref><ref id="bib5"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Betsch</surname><given-names>B</given-names></name><name><surname>Koerding</surname><given-names>K</given-names></name><name><surname>Einhäuser</surname><given-names>W</given-names></name><name><surname>König</surname><given-names>P</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Movement–induced motion signal distribution in outdoor scenes</article-title><source>Biological Cybernetics</source><volume>90</volume><fpage>41</fpage><lpage>50</lpage><pub-id pub-id-type="doi">10.1007/s00422-003-0434-6</pub-id><pub-id pub-id-type="pmid">14762723</pub-id></element-citation></ref><ref id="bib6"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Beyeler</surname><given-names>M</given-names></name><name><surname>Dutt</surname><given-names>N</given-names></name><name><surname>Krichmar</surname><given-names>JL</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>3D visual response properties of mstd emerge from an efficient, sparse population code</article-title><source>The Journal of Neuroscience</source><volume>36</volume><fpage>8399</fpage><lpage>8415</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.0396-16.2016</pub-id><pub-id pub-id-type="pmid">27511012</pub-id></element-citation></ref><ref id="bib7"><element-citation publication-type="book"><person-group person-group-type="author"><collab>Blender Online Community</collab></person-group><year iso-8601-date="2021">2021</year><source>Blender - a 3D Modelling and Rendering Package</source><publisher-loc>Amsterdam</publisher-loc><publisher-name>Blender Foundation, Blender Institute</publisher-name></element-citation></ref><ref id="bib8"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Czuba</surname><given-names>TB</given-names></name><name><surname>Whritner</surname><given-names>JA</given-names></name><name><surname>Kohn</surname><given-names>A</given-names></name><name><surname>Huk</surname><given-names>AC</given-names></name><name><surname>Cormack</surname><given-names>LK</given-names></name></person-group><year iso-8601-date="2020">2020</year><article-title>Binocular viewing geometry shapes the neural representation of the dynamic three-dimensional environment</article-title><source>Nature Neuroscience</source><volume>23</volume><fpage>113</fpage><lpage>121</lpage><pub-id pub-id-type="doi">10.1038/s41593-019-0544-7</pub-id><pub-id pub-id-type="pmid">31792466</pub-id></element-citation></ref><ref id="bib9"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Boussaoud</surname><given-names>D</given-names></name><name><surname>Jouffrais</surname><given-names>C</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Eye position effects on the neuronal activity of dorsal premotor cortex in the macaque monkey</article-title><source>Journal of Neurophysiology</source><volume>80</volume><fpage>1132</fpage><lpage>1150</lpage><pub-id pub-id-type="doi">10.1152/jn.1998.80.3.1132</pub-id></element-citation></ref><ref id="bib10"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Ilg</surname><given-names>UJ</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Distler</surname><given-names>C</given-names></name><name><surname>Hoffmann</surname><given-names>KP</given-names></name></person-group><year iso-8601-date="1997">1997</year><article-title>Eye position effects in monkey cortex. I. visual and pursuit-related activity in extrastriate areas MT and MST</article-title><source>Journal of Neurophysiology</source><volume>77</volume><fpage>944</fpage><lpage>961</lpage><pub-id pub-id-type="doi">10.1152/jn.1997.77.2.944</pub-id><pub-id pub-id-type="pmid">9065860</pub-id></element-citation></ref><ref id="bib11"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Kubischik</surname><given-names>M</given-names></name><name><surname>Pekel</surname><given-names>M</given-names></name><name><surname>Hoffmann</surname><given-names>K-P</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Visual selectivity for heading in monkey area MST</article-title><source>Experimental Brain Research</source><volume>200</volume><fpage>51</fpage><lpage>60</lpage><pub-id pub-id-type="doi">10.1007/s00221-009-1990-3</pub-id><pub-id pub-id-type="pmid">19727690</pub-id></element-citation></ref><ref id="bib12"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Britten</surname><given-names>KH</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Mechanisms of self-motion perception</article-title><source>Annual Review of Neuroscience</source><volume>31</volume><fpage>389</fpage><lpage>410</lpage><pub-id pub-id-type="doi">10.1146/annurev.neuro.29.051605.112953</pub-id><pub-id pub-id-type="pmid">18558861</pub-id></element-citation></ref><ref id="bib13"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calow</surname><given-names>D</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2007">2007</year><article-title>Local statistics of retinal optic flow for self-motion through natural sceneries</article-title><source>Network</source><volume>18</volume><fpage>343</fpage><lpage>374</lpage><pub-id pub-id-type="doi">10.1080/09548980701642277</pub-id><pub-id pub-id-type="pmid">18360939</pub-id></element-citation></ref><ref id="bib14"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Calow</surname><given-names>D</given-names></name><name><surname>Lappe</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>Efficient encoding of natural optic flow</article-title><source>Network</source><volume>19</volume><fpage>183</fpage><lpage>212</lpage><pub-id pub-id-type="doi">10.1080/09548980802368764</pub-id></element-citation></ref><ref id="bib15"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Cottereau</surname><given-names>BR</given-names></name><name><surname>Smith</surname><given-names>AT</given-names></name><name><surname>Rima</surname><given-names>S</given-names></name><name><surname>Fize</surname><given-names>D</given-names></name><name><surname>Héjja-Brichard</surname><given-names>Y</given-names></name><name><surname>Renaud</surname><given-names>L</given-names></name><name><surname>Lejards</surname><given-names>C</given-names></name><name><surname>Vayssière</surname><given-names>N</given-names></name><name><surname>Trotter</surname><given-names>Y</given-names></name><name><surname>Durand</surname><given-names>JB</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Processing of egomotion-consistent optic flow in the rhesus macaque cortex</article-title><source>Cerebral Cortex</source><volume>27</volume><fpage>330</fpage><lpage>343</lpage><pub-id pub-id-type="doi">10.1093/cercor/bhw412</pub-id><pub-id pub-id-type="pmid">28108489</pub-id></element-citation></ref><ref id="bib16"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Dietrich</surname><given-names>H</given-names></name><name><surname>Wuehr</surname><given-names>M</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Strategies for gaze stabilization critically depend on locomotor speed</article-title><source>Neuroscience</source><volume>408</volume><fpage>418</fpage><lpage>429</lpage><pub-id pub-id-type="doi">10.1016/j.neuroscience.2019.01.025</pub-id><pub-id pub-id-type="pmid">30703510</pub-id></element-citation></ref><ref id="bib17"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duffy</surname><given-names>CJ</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="1991">1991</year><article-title>Sensitivity of mst neurons to optic flow stimuli. i. a continuum of response selectivity to large-field stimuli</article-title><source>Journal of Neurophysiology</source><volume>65</volume><fpage>1329</fpage><lpage>1345</lpage><pub-id pub-id-type="doi">10.1152/jn.1991.65.6.1329</pub-id><pub-id pub-id-type="pmid">1875243</pub-id></element-citation></ref><ref id="bib18"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Duffy</surname><given-names>CJ</given-names></name><name><surname>Wurtz</surname><given-names>RH</given-names></name></person-group><year iso-8601-date="1995">1995</year><article-title>Response of monkey MST neurons to optic flow stimuli with shifted centers of motion</article-title><source>The Journal of Neuroscience</source><volume>15</volume><fpage>5192</fpage><lpage>5208</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.15-07-05192.1995</pub-id><pub-id pub-id-type="pmid">7623145</pub-id></element-citation></ref><ref id="bib19"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gibson</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1950">1950</year><article-title>The perception of the visual world</article-title><source>The American Journal of Psychology</source><volume>63</volume><elocation-id>1418003</elocation-id><pub-id pub-id-type="doi">10.2307/1418003</pub-id><pub-id pub-id-type="pmid">15432778</pub-id></element-citation></ref><ref id="bib20"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Grasso</surname><given-names>R</given-names></name><name><surname>Prévost</surname><given-names>P</given-names></name><name><surname>Ivanenko</surname><given-names>YP</given-names></name><name><surname>Berthoz</surname><given-names>A</given-names></name></person-group><year iso-8601-date="1998">1998</year><article-title>Eye-Head coordination for the steering of locomotion in humans: an anticipatory synergy</article-title><source>Neuroscience Letters</source><volume>253</volume><fpage>115</fpage><lpage>118</lpage><pub-id pub-id-type="doi">10.1016/s0304-3940(98)00625-9</pub-id><pub-id pub-id-type="pmid">9774163</pub-id></element-citation></ref><ref id="bib21"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Graziano</surname><given-names>MS</given-names></name><name><surname>Andersen</surname><given-names>RA</given-names></name><name><surname>Snowden</surname><given-names>RJ</given-names></name></person-group><year iso-8601-date="1994">1994</year><article-title>Tuning of MST neurons to spiral motions</article-title><source>The Journal of Neuroscience</source><volume>14</volume><fpage>54</fpage><lpage>67</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.14-01-00054.1994</pub-id><pub-id pub-id-type="pmid">8283251</pub-id></element-citation></ref><ref id="bib22"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Gu</surname><given-names>Y</given-names></name><name><surname>Fetsch</surname><given-names>CR</given-names></name><name><surname>Adeyemo</surname><given-names>B</given-names></name><name><surname>Deangelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2010">2010</year><article-title>Decoding of mstd population activity accounts for variations in the precision of heading perception</article-title><source>Neuron</source><volume>66</volume><fpage>596</fpage><lpage>609</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2010.04.026</pub-id><pub-id pub-id-type="pmid">20510863</pub-id></element-citation></ref><ref id="bib23"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Haggerty</surname><given-names>SE</given-names></name><name><surname>King</surname><given-names>WM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>The interaction of pre-programmed eye movements with the vestibulo-ocular reflex</article-title><source>Frontiers in Systems Neuroscience</source><volume>12</volume><elocation-id>4</elocation-id><pub-id pub-id-type="doi">10.3389/fnsys.2018.00004</pub-id><pub-id pub-id-type="pmid">29593506</pub-id></element-citation></ref><ref id="bib24"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Imai</surname><given-names>T</given-names></name><name><surname>Moore</surname><given-names>ST</given-names></name><name><surname>Raphan</surname><given-names>T</given-names></name><name><surname>Cohen</surname><given-names>B</given-names></name></person-group><year iso-8601-date="2001">2001</year><article-title>Interaction of the body, head, and eyes during walking and turning</article-title><source>Experimental Brain Research</source><volume>136</volume><fpage>1</fpage><lpage>18</lpage><pub-id pub-id-type="doi">10.1007/s002210000533</pub-id><pub-id pub-id-type="pmid">11204402</pub-id></element-citation></ref><ref id="bib25"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Jacob</surname><given-names>MS</given-names></name><name><surname>Duffy</surname><given-names>CJ</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Steering transforms the cortical representation of self-movement from direction to destination</article-title><source>The Journal of Neuroscience</source><volume>35</volume><fpage>16055</fpage><lpage>16063</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.2368-15.2015</pub-id><pub-id pub-id-type="pmid">26658859</pub-id></element-citation></ref><ref id="bib26"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Kassner</surname><given-names>M</given-names></name><name><surname>Patera</surname><given-names>W</given-names></name><name><surname>Bulling</surname><given-names>A</given-names></name></person-group><year iso-8601-date="2014">2014</year><article-title>Pupil: an open source platform for pervasive eye tracking and mobile gaze-based interaction</article-title><conf-name>Proceedings of the 2014 ACM international joint conference on pervasive and ubiquitous computing: Adjunct publication</conf-name><pub-id pub-id-type="doi">10.1145/2638728.2641695</pub-id></element-citation></ref><ref id="bib27"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname><given-names>JJ</given-names></name><name><surname>van Doorn</surname><given-names>AJ</given-names></name></person-group><year iso-8601-date="1976">1976</year><article-title>Local structure of movement parallax of the plane</article-title><source>Journal of the Optical Society of America</source><volume>66</volume><elocation-id>717</elocation-id><pub-id pub-id-type="doi">10.1364/JOSA.66.000717</pub-id></element-citation></ref><ref id="bib28"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Koenderink</surname><given-names>JJ</given-names></name></person-group><year iso-8601-date="1986">1986</year><article-title>Optic flow</article-title><source>Vision Research</source><volume>26</volume><fpage>161</fpage><lpage>179</lpage><pub-id pub-id-type="doi">10.1016/0042-6989(86)90078-7</pub-id></element-citation></ref><ref id="bib29"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Land</surname><given-names>MF</given-names></name></person-group><year iso-8601-date="2019">2019</year><article-title>Eye movements in man and other animals</article-title><source>Vision Research</source><volume>162</volume><fpage>1</fpage><lpage>7</lpage><pub-id pub-id-type="doi">10.1016/j.visres.2019.06.004</pub-id><pub-id pub-id-type="pmid">31254533</pub-id></element-citation></ref><ref id="bib30"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lappe</surname><given-names>M</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>Pekel</surname><given-names>M</given-names></name><name><surname>Thiele</surname><given-names>A</given-names></name><name><surname>Hoffmann</surname><given-names>K-P</given-names></name></person-group><year iso-8601-date="1996">1996</year><article-title>Optic flow processing in monkey STS: a theoretical and experimental approach</article-title><source>The Journal of Neuroscience</source><volume>16</volume><fpage>6265</fpage><lpage>6285</lpage><pub-id pub-id-type="doi">10.1523/JNEUROSCI.16-19-06265.1996</pub-id><pub-id pub-id-type="pmid">8815907</pub-id></element-citation></ref><ref id="bib31"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lappe</surname><given-names>M</given-names></name><name><surname>Bremmer</surname><given-names>F</given-names></name><name><surname>van den Berg</surname><given-names>AV</given-names></name></person-group><year iso-8601-date="1999">1999</year><article-title>Perception of self-motion from visual flow</article-title><source>Trends in Cognitive Sciences</source><volume>3</volume><fpage>329</fpage><lpage>336</lpage><pub-id pub-id-type="doi">10.1016/S1364-6613(99)01364-9</pub-id></element-citation></ref><ref id="bib32"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Lappi</surname><given-names>O</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Eye movements in the wild: oculomotor control, gaze behavior &amp; frames of reference</article-title><source>Neuroscience and Biobehavioral Reviews</source><volume>69</volume><fpage>49</fpage><lpage>68</lpage><pub-id pub-id-type="doi">10.1016/j.neubiorev.2016.06.006</pub-id><pub-id pub-id-type="pmid">27461913</pub-id></element-citation></ref><ref id="bib33"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Longuet-Higgins</surname><given-names>HC</given-names></name><name><surname>Prazdny</surname><given-names>K</given-names></name></person-group><year iso-8601-date="1980">1980</year><article-title>The interpretation of a moving retinal image</article-title><source>Proceedings of the Royal Society of London. Series B. Biological Sciences</source><volume>208</volume><fpage>385</fpage><lpage>397</lpage><pub-id pub-id-type="doi">10.1098/rspb.1980.0057</pub-id></element-citation></ref><ref id="bib34"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>MacNeilage</surname><given-names>PR</given-names></name><name><surname>Zhang</surname><given-names>Z</given-names></name><name><surname>DeAngelis</surname><given-names>GC</given-names></name><name><surname>Angelaki</surname><given-names>DE</given-names></name></person-group><year iso-8601-date="2012">2012</year><article-title>Vestibular facilitation of optic flow parsing</article-title><source>PLOS ONE</source><volume>7</volume><elocation-id>e40264</elocation-id><pub-id pub-id-type="doi">10.1371/journal.pone.0040264</pub-id><pub-id pub-id-type="pmid">22768345</pub-id></element-citation></ref><ref id="bib35"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Yates</surname><given-names>JL</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2018">2018</year><article-title>Gaze and the control of foot placement when walking in natural terrain</article-title><source>Current Biology</source><volume>28</volume><fpage>1224</fpage><lpage>1233</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2018.03.008</pub-id><pub-id pub-id-type="pmid">29657116</pub-id></element-citation></ref><ref id="bib36"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Matthis</surname><given-names>JS</given-names></name><name><surname>Muller</surname><given-names>KS</given-names></name><name><surname>Bonnen</surname><given-names>K</given-names></name><name><surname>Hayhoe</surname><given-names>MM</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Retinal Optic Flow during Natural Locomotion</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2020.07.23.217893</pub-id></element-citation></ref><ref id="bib37"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Maunsell</surname><given-names>JH</given-names></name><name><surname>Van Essen</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="1987">1987</year><article-title>Topographic organization of the middle temporal visual area in the macaque monkey: representational biases and the relationship to callosal connections and myeloarchitectonic boundaries</article-title><source>The Journal of Comparative Neurology</source><volume>266</volume><fpage>535</fpage><lpage>555</lpage><pub-id pub-id-type="doi">10.1002/cne.902660407</pub-id><pub-id pub-id-type="pmid">2449473</pub-id></element-citation></ref><ref id="bib38"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>McFarland</surname><given-names>JM</given-names></name><name><surname>Bondy</surname><given-names>AG</given-names></name><name><surname>Saunders</surname><given-names>RC</given-names></name><name><surname>Cumming</surname><given-names>BG</given-names></name><name><surname>Butts</surname><given-names>DA</given-names></name></person-group><year iso-8601-date="2015">2015</year><article-title>Saccadic modulation of stimulus processing in primary visual cortex</article-title><source>Nature Communications</source><volume>6</volume><elocation-id>8110</elocation-id><pub-id pub-id-type="doi">10.1038/ncomms9110</pub-id><pub-id pub-id-type="pmid">26370359</pub-id></element-citation></ref><ref id="bib39"><element-citation publication-type="preprint"><person-group person-group-type="author"><name><surname>Mineault</surname><given-names>PJ</given-names></name><name><surname>Bakhtiari</surname><given-names>S</given-names></name><name><surname>Richards</surname><given-names>BA</given-names></name><name><surname>Pack</surname><given-names>CC</given-names></name></person-group><year iso-8601-date="2021">2021</year><article-title>Your Head Is There to Move You around: Goal-Driven Models of the Primate Dorsal Pathway</article-title><source>bioRxiv</source><pub-id pub-id-type="doi">10.1101/2021.07.09.451701</pub-id></element-citation></ref><ref id="bib40"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Morrone</surname><given-names>MC</given-names></name><name><surname>Tosetti</surname><given-names>M</given-names></name><name><surname>Montanaro</surname><given-names>D</given-names></name><name><surname>Fiorentini</surname><given-names>A</given-names></name><name><surname>Cioni</surname><given-names>G</given-names></name><name><surname>Burr</surname><given-names>DC</given-names></name></person-group><year iso-8601-date="2000">2000</year><article-title>A cortical area that responds specifically to optic flow, revealed by fMRI</article-title><source>Nature Neuroscience</source><volume>3</volume><fpage>1322</fpage><lpage>1328</lpage><pub-id pub-id-type="doi">10.1038/81860</pub-id><pub-id pub-id-type="pmid">11100154</pub-id></element-citation></ref><ref id="bib41"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Strong</surname><given-names>SL</given-names></name><name><surname>Silson</surname><given-names>EH</given-names></name><name><surname>Gouws</surname><given-names>AD</given-names></name><name><surname>Morland</surname><given-names>AB</given-names></name><name><surname>McKeefry</surname><given-names>DJ</given-names></name></person-group><year iso-8601-date="2017">2017</year><article-title>Differential processing of the direction and focus of expansion of optic flow stimuli in areas MST and V3A of the human visual cortex</article-title><source>Journal of Neurophysiology</source><volume>117</volume><fpage>2209</fpage><lpage>2217</lpage><pub-id pub-id-type="doi">10.1152/jn.00031.2017</pub-id><pub-id pub-id-type="pmid">28298300</pub-id></element-citation></ref><ref id="bib42"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Wall</surname><given-names>MB</given-names></name><name><surname>Smith</surname><given-names>AT</given-names></name></person-group><year iso-8601-date="2008">2008</year><article-title>The representation of egomotion in the human brain</article-title><source>Current Biology</source><volume>18</volume><fpage>191</fpage><lpage>194</lpage><pub-id pub-id-type="doi">10.1016/j.cub.2007.12.053</pub-id><pub-id pub-id-type="pmid">18221876</pub-id></element-citation></ref><ref id="bib43"><element-citation publication-type="confproc"><person-group person-group-type="author"><name><surname>Weinzaepfel</surname><given-names>P</given-names></name><name><surname>Revaud</surname><given-names>J</given-names></name><name><surname>Harchaoui</surname><given-names>Z</given-names></name><name><surname>Schmid</surname><given-names>C</given-names></name></person-group><year iso-8601-date="2013">2013</year><article-title>Deepflow: Large displacement optical flow with deep matching</article-title><conf-name>Proceedings of the IEEE international conference on computer vision</conf-name><fpage>1385</fpage><lpage>1392</lpage><pub-id pub-id-type="doi">10.1109/ICCV.2013.175</pub-id></element-citation></ref><ref id="bib44"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Weiss</surname><given-names>Y</given-names></name><name><surname>Simoncelli</surname><given-names>EP</given-names></name><name><surname>Adelson</surname><given-names>EH</given-names></name></person-group><year iso-8601-date="2002">2002</year><article-title>Motion illusions as optimal percepts</article-title><source>Nature Neuroscience</source><volume>5</volume><fpage>598</fpage><lpage>604</lpage><pub-id pub-id-type="doi">10.1038/nn0602-858</pub-id><pub-id pub-id-type="pmid">12021763</pub-id></element-citation></ref><ref id="bib45"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zanker</surname><given-names>JM</given-names></name><name><surname>Zeil</surname><given-names>J</given-names></name></person-group><year iso-8601-date="2005">2005</year><article-title>Movement-induced motion signal distributions in outdoor scenes</article-title><source>Network</source><volume>16</volume><fpage>357</fpage><lpage>376</lpage><pub-id pub-id-type="doi">10.1080/09548980500497758</pub-id><pub-id pub-id-type="pmid">16611590</pub-id></element-citation></ref><ref id="bib46"><element-citation publication-type="journal"><person-group person-group-type="author"><name><surname>Zmarz</surname><given-names>P</given-names></name><name><surname>Keller</surname><given-names>GB</given-names></name></person-group><year iso-8601-date="2016">2016</year><article-title>Mismatch receptive fields in mouse visual cortex</article-title><source>Neuron</source><volume>92</volume><fpage>766</fpage><lpage>772</lpage><pub-id pub-id-type="doi">10.1016/j.neuron.2016.09.057</pub-id><pub-id pub-id-type="pmid">27974161</pub-id></element-citation></ref></ref-list></back><sub-article article-type="editor-report" id="sa0"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82410.sa0</article-id><title-group><article-title>Editor's evaluation</article-title></title-group><contrib-group><contrib contrib-type="author"><name><surname>Huxlin</surname><given-names>Krystel R</given-names></name><role specific-use="editor">Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022kthw22</institution-id><institution>University of Rochester</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><related-object id="sa0ro1" object-id-type="id" object-id="10.1101/2022.09.06.506797" link-type="continued-by" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.06.506797"/></front-stub><body><p>This important study provides new information about the statistics of &quot;retinal&quot; motion patterns generated by human participants physically walking a straight path in real terrains that differ in ruggedness. State-of-the-art eye, head and body tracking allowed simultaneous assessment of eye movements, head movements and gait. Compelling evidence was provided for an asymmetrical gradient of flow speeds during the gait cycle of walking, tied predominantly to vertical gaze angle, together with a radial motion direction distribution tied mostly to horizontal gaze angle. This work, by describing fundamental properties of human visual motion statistics during natural behavior, should be of great interest to scientists who seek to understand the neural computations performed by walking humans, given certain behavioral goals.</p></body></sub-article><sub-article article-type="decision-letter" id="sa1"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82410.sa1</article-id><title-group><article-title>Decision letter</article-title></title-group><contrib-group content-type="section"><contrib contrib-type="editor"><name><surname>Huxlin</surname><given-names>Krystel R</given-names></name><role>Reviewing Editor</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/022kthw22</institution-id><institution>University of Rochester</institution></institution-wrap><country>United States</country></aff></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name><surname>Lappi</surname><given-names>Otto</given-names></name><role>Reviewer</role><aff><institution-wrap><institution-id institution-id-type="ror">https://ror.org/040af2s02</institution-id><institution>University of Helsinki</institution></institution-wrap><country>Finland</country></aff></contrib></contrib-group></front-stub><body><boxed-text id="sa2-box1"><p>Our editorial process produces two outputs: (i) <ext-link ext-link-type="uri" xlink:href="https://sciety.org/articles/activity/10.1101/2022.09.06.506797">public reviews</ext-link> designed to be posted alongside <ext-link ext-link-type="uri" xlink:href="https://www.biorxiv.org/content/10.1101/2022.09.06.506797v1">the preprint</ext-link> for the benefit of readers; (ii) feedback on the manuscript for the authors, including requests for revisions, shown below. We also include an acceptance summary that explains what the editors found interesting or important about the work.</p></boxed-text><p><bold>Decision letter after peer review:</bold></p><p>Thank you for submitting your article &quot;Behavior shapes retinal motion statistics during natural locomotion&quot; for consideration by <italic>eLife</italic>. Your article has been reviewed by 3 peer reviewers, one of whom is a member of our Board of Reviewing Editors, and the evaluation has been overseen by Tirin Moore as the Senior Editor. The following individual involved in the review of your submission has agreed to reveal their identity: Otto Lappi (Reviewer #3).</p><p>The reviewers have discussed their reviews with one another, and the Reviewing Editor has drafted this to help you prepare a revised submission.</p><p>Essential revisions:</p><p>Included here is a brief evaluation summary and list of revisions the reviewers and review editor deem essential for the authors to address. The public summaries and full, individual reviewers' recommendations for the authors are also appended below. The authors are advised to address the public summaries briefly, and the individual recommendations in a detailed, point-by-point manner.</p><p>As you will be able to read below, reviewers appreciated the importance of the study and its potentially broad interest. It uses state-of-the-art technology and presents strong behavioral evidence that should prove useful to those who seek to understand active vision and model it. The writing was relatively clear, the figures appropriate and importantly, the study design and analyses were deemed rigorous and generally appropriate. However, reviewers raised concerns with regard to some of the claims made, particularly as pertains to the novelty of the retinal flow patterns described, their implications for neural processing, and the need for additional methodological details. The key points that need to be addressed can be summarized as follows:</p><p>1. The concept that walkers stabilize their point of gaze (i.e., they &quot;fixate&quot; stable points on the ground ahead) is not entirely new (see references below). The fact that this generates low foveal, and higher peripheral flow velocities, especially in the lower retinal hemifield, follows simply from optical geometry. Please incorporate and discuss the references below, explaining in more quantitative and/or empirical terms, what specific gap the new methodology described in the paper actually fills:</p><p>Imai, T., Moore, S. T., Raphan, T. and Cohen, B. Interaction of the body, head, and eyes during walking and turning. Experimental Brain Research 136, 1-18 (2001).</p><p>Grasso, R., Glasauer, S., Takei, Y. and Berthoz, A. The predictive brain: anticipatory control of head direction for the steering of locomotion. NeuroReport 7, 1170-1174 (1996).</p><p>Grasso, R., Prévost, P., Ivanenko, Y. P. and Berthoz, A. Eye-head coordination for the steering of locomotion in humans: an anticipatory synergy. Neuroscience Letters 253, 115-118 (1998).</p><p>Authié, C. N., Hilt, P. M., Nguyen, S., Berthoz, A. and Bennequin, D. Differences in gaze anticipation for locomotion with and without vision. Frontiers in Human Neuroscience 9, https://doi.org/10.3389/fnhum.2015.00312 (2015).</p><p>2. Please add a discussion of the neural processing implications of your findings, preferably in the Discussion (rather than the Introduction). This could include speculations on direction tuning in cortical representations for foveal versus peripheral, upper versus the lower visual field, processing in VOR versus other gaze-stabilization circuits, etc… Feel free to invoke efficient coding or other neuro-mechanistic hypotheses as relevant.</p><p>3. All 3 reviewers commented on the need for additional methodological details – please address this fully and carefully (see details in individual reviews below).</p><p><italic>Reviewer #1 (Recommendations for the authors):</italic></p><p>While the global claims of this paper are potentially very interesting and useful, they would be strengthened by additional methodological details and consideration:</p><p>1. In some cases, it is unclear if the data shown come from a single observer (e.g., Figure 2 and Figure 3) or multiple observers? If from multiple observers, was the data based on all 11 observers or just a subset – and how was it combined across observers?</p><p>2. About half the participants were female. Were there any systematic differences in gait and related retinal flow distributions between male and female participants in the different terrains shown? I believe this study has an excellent opportunity to examine sex as a biological variable in this phenomenon – they should take advantage of it.</p><p>Additionally, the paper would be enhanced by a discussion of the following points:</p><p>1. Many figures in this paper include representations of average speed and average direction on &quot;the retina&quot;. In all cases, a single representation is shown, as though for a cyclopean eye. However, it is unclear how this information is actually represented on the two retinas, and how information from the two eyes is then combined to a different effect (i.e. behavior) by different portions of the visual circuitry. Can the authors please discuss?</p><p><italic>Reviewer #2 (Recommendations for the authors):</italic></p><p>The overall setup is impressive, but I am curious about the computer vision component. As I understand, the optic flow statistics were calculated on a reprojected virtual scene. While the authors report an impressively small error between terrain reconstructions, I would have appreciated a Methods figure that shows example reconstructions. I am especially curious about some of the more rocky terrains, which I expect to be challenging to reconstruct with all its intricate 3D information (e.g., all the twigs and rocks sticking out of the background). Would mispredictions of the depth of these objects affect the optic flow estimations much? Also, how did lighting affect the reconstructions?</p><p>I think the implications of the findings for the neural code of vision could be presented/discussed more rigorously. For instance, the authors report several statistical peculiarities (e.g., the asymmetry between upper and lower visual fields, compression of directions, etc.). Are any of these consistent with the primate neuroscience literature? The few examples mentioned seem to (at least somewhat) contradict existing findings, except for the motion prior centered on 0. For instance, Guo et al. would suggest an overrepresentation of lateral headings, which I believe is counter to Figure 6.</p><p>Related, the comparison of the current findings to the Gibsonian view should be expanded upon. I understand that the underlying reason is different (stabilization vs. coincidence of gaze with the direction of travel), but the end result seems to be the same – that on average you get an expanding flow field. How does this finding relate to earlier work [13], which argued that the Gibsonian view rarely appears in the wild (unless you are a self-driving car)?</p><p><italic>Reviewer #3 (Recommendations for the authors):</italic></p><p>The title of the manuscript is &quot;Behavior shapes retinal motion statistics during natural locomotion&quot;. The statement that behavior shapes retinal motion is quite obvious and known. From geometric optics and the fact that the eyes move, this statement follows. The abstract promises &quot;the properties of the resulting retinal motion patterns&quot;.</p><p>So, any originality hinges on the word &quot;statistics&quot;. Meaning that you expect the Results to emphasize statistics. Descriptive statistics (parameters and their distributions) you present, although some of it is perhaps more visualization than statistics, but that's okay.</p><p>Technical comments:</p><p>4.2. p3 2nd para from bottom: &quot;Stabilization is most likely accomplished by the vestibular ocular reflex, although other eye movement systems might also be involved&quot;; I can see no compelling reason why the compensatory eye movements would be – physiologically speaking – predominantly VOR based, as opposed to optokinetic response or smooth pursuit. I.e. why you would consider VOR &quot;most likely&quot;. The reference you give, as far as I can tell, gives no data that directly speaks on this. You may have some argument in mind but it is not 100% clear to me what that is.</p></body></sub-article><sub-article article-type="reply" id="sa2"><front-stub><article-id pub-id-type="doi">10.7554/eLife.82410.sa2</article-id><title-group><article-title>Author response</article-title></title-group></front-stub><body><disp-quote content-type="editor-comment"><p>Essential revisions:</p><p>Included here is a brief evaluation summary and list of revisions the reviewers and review editor deem essential for the authors to address. The public summaries and full, individual reviewers' recommendations for the authors are also appended below. The authors are advised to address the public summaries briefly, and the individual recommendations in a detailed, point-by-point manner.</p><p>As you will be able to read below, reviewers appreciated the importance of the study and its potentially broad interest. It uses state-of-the-art technology and presents strong behavioral evidence that should prove useful to those who seek to understand active vision and model it. The writing was relatively clear, the figures appropriate and importantly, the study design and analyses were deemed rigorous and generally appropriate. However, reviewers raised concerns with regard to some of the claims made, particularly as pertains to the novelty of the retinal flow patterns described, their implications for neural processing, and the need for additional methodological details. The key points that need to be addressed can be summarized as follows:</p><p>1. The concept that walkers stabilize their point of gaze (i.e., they &quot;fixate&quot; stable points on the ground ahead) is not entirely new (see references below). The fact that this generates low foveal, and higher peripheral flow velocities, especially in the lower retinal hemifield, follows simply from optical geometry. Please incorporate and discuss the references below, explaining in more quantitative and/or empirical terms, what specific gap the new methodology described in the paper actually fills:</p><p>Imai, T., Moore, S. T., Raphan, T. and Cohen, B. Interaction of the body, head, and eyes during walking and turning. Experimental Brain Research 136, 1-18 (2001).</p><p>Grasso, R., Glasauer, S., Takei, Y. and Berthoz, A. The predictive brain: anticipatory control of head direction for the steering of locomotion. NeuroReport 7, 1170-1174 (1996).</p><p>Grasso, R., Prévost, P., Ivanenko, Y. P. and Berthoz, A. Eye-head coordination for the steering of locomotion in humans: an anticipatory synergy. Neuroscience Letters 253, 115-118 (1998).</p><p>Authié, C. N., Hilt, P. M., Nguyen, S., Berthoz, A. and Bennequin, D. Differences in gaze anticipation for locomotion with and without vision. Frontiers in Human Neuroscience 9, https://doi.org/10.3389/fnhum.2015.00312 (2015).</p></disp-quote><p>We certainly do not mean to claim that stabilizing gaze is novel, and agree that the general patterns follow directly from the geometry as worked out very elegantly by Koenderink and others. We spend time describing the gaze behavior because it is essential for understanding the paper. We do not claim that the basic saccade/stabilize/saccade behavior is novel and now make this clearer. What is novel here is that we calculated the time-varying retinal motion patterns generated during the gait cycle using a 3D reconstruction of the terrain. Reference to the work above has been added. We have clarified what is novel in the text and changed the previous video (Video 2) to make the retinal motion patterns clearer. We have also added a video (Video 1) the shows how it relates to the gaze pattern.</p><p>Some Background for the reviewers below:</p><p>The novelty is in the collection of gaze statistics in natural environments, the 3D terrain reconstructions, and perhaps most importantly, the measurement of retinal motion as a function of time during the gait cycle. Previous work on the geometry has not taken the gait -induced movements into account. The other novel aspect is that the motion patterns vary with gaze location which in turn varies with terrain in a way that depends on behavioral goals. So while some aspects of the general patterns are not unexpected, the quantitative values depend on the statistics of the behavior. The actual statistics require these in situ measurements, and this has not previously been done, as stated in the abstract. Another less novel aspect is the data showing that gaze stabilization in these circumstances (ie outdoor walking in rugged environments) is very good. Other investigators (including two of those listed) have looked at stabilization gain, but not outside the laboratory. All the calculations depend on this, which is why we emphasize it.</p><p>Note that while some investigators have been well aware of gait modulations, the consequences for both retinal motion and how flow is used, have not previously been measured and have been consistently underestimated. This is the primary driver of the motion statistics (lateral, vertical, and forward) a fact that has been neglected. Other details of the motion statistics such as the variation in direction densities, and the horizontal band of low velocities through the fovea have not previously been demonstrated, and simply the actual distributions of direction and speed across the retina. These details affect fundamental questions, like what an optimal stimulus for an MST receptive field might be or how self-motion and world motion are parsed by the visual system.</p><p>One of the difficulties in describing our work is that there are different groups of researchers who are sensitive to different aspects of the generation and properties of retinal motion, and how it might be used. For example, the neurophysiology has of necessity all been done with a stable head. This means that there is no gait cycle and no stabilizing eye movements. Investigators use pursuit eye movements typically against a background whose motion is not time-varying in a way linked to gait, and therefore has a different geometry. In addition, many of the motion stimuli chosen in experiments are not clearly related to patterns that might be generated during natural body movements (for example, non-zero motion at the fovea). This has also made it hard to make tight links between the measured statistics and properties of cells in the visual pathways, and we cited those we felt most directly comparable. The psychophysics community also almost universally uses seated observers and pursuit eye movements, simulating a constant body heading direction. Because the retinal motion is directly dependent on behavior and gaze stabilization during the gait cycle, we felt it necessary to make this clear, even though some might find it unremarkable.</p><p>Other groups of researchers (like those cited) especially those who look at natural behavior are sensitive to eye and body movements and the need for image stabilization, and the papers listed address important and interesting questions about coordinated whole body movements. We have cited those references to acknowledge this work on basic gaze behavior and the high gain of the stabilizing mechanisms. However, the goal of the current paper was to specify how this affects retinal motion, and we are not aware of previous work that calculates the retinal motion statistics in the natural world generated by locomotion.</p><p>The manuscript has been edited to make the novelty clearer.</p><disp-quote content-type="editor-comment"><p>2. Please add a discussion of the neural processing implications of your findings, preferably in the Discussion (rather than the Introduction). This could include speculations on direction tuning in cortical representations for foveal versus peripheral, upper versus the lower visual field, processing in VOR versus other gaze-stabilization circuits, etc… Feel free to invoke efficient coding or other neuro-mechanistic hypotheses as relevant.</p></disp-quote><p>The measured statistics provide a well-defined set of hypotheses about the pattern of direction and speed tuning across the visual field. We have added text (see p 14, 15) to be more explicit about what we would expect to find in MT and MSTd if those cell properties are indeed shaped by these statistics. The points of comparison in the existing literature are hard to find because the stimuli have not been closely matched to actual retinal flow patterns. This is discussed on pp 14-16. We have done what we could, but this might fall short of expectations because of difficulty finding comparable stimulus conditions.</p><disp-quote content-type="editor-comment"><p>3. All 3 reviewers commented on the need for additional methodological details – please address this fully and carefully (see details in individual reviews below).</p></disp-quote><p>A more detailed description of the methods including the photogrammetry and the reference frames for the measurements has been added primarily to the Methods section.</p><p>Figure 1 is an example segment of a record for a single subject. All other Figures are data averaged across all subjects as described in the Methods.</p><p>Standard errors between Subjects have now been added to Figure 2, 3 and 4 b and d and Figure 12. The effects of horizontal and vertical gaze angle and terrain are for demonstration so we have not calculated the standard errors for those Figures (see above).</p><disp-quote content-type="editor-comment"><p>Reviewer #1 (Recommendations for the authors):</p><p>While the global claims of this paper are potentially very interesting and useful, they would be strengthened by additional methodological details and consideration:</p><p>1. In some cases, it is unclear if the data shown come from a single observer (e.g., Figure 2 and Figure 3) or multiple observers? If from multiple observers, was the data based on all 11 observers or just a subset – and how was it combined across observers?</p></disp-quote><p>This has been clarified in Figure legends and Methods. Figure 1 is an example segment of a record for a single subject. All other Figures are data averaged across all subjects as described in the Methods. Standard errors between Subjects have now been added to Figure 2, 3 and 4 b and d and Figure 12. The effects of horizontal and vertical gaze angle and terrain are for demonstration so we have not calculated the standard errors for those Figures (see above).</p><disp-quote content-type="editor-comment"><p>Were there any systematic differences in gait and related retinal flow distributions between male and female participants in the different terrains shown? Likewise, age and optical correction may have an impact on gait, head, and eye movements. Neither is indicated in the paper. This should be remedied and the authors should include a discussion of their likely impact on the behaviors examined.</p></disp-quote><p>As described above and in the text, many factors influence step length, step speed, and linked to this, gaze location. Individual subjects also differ in tradeoffs between energetic costs and stability. Given this, the most likely effect of sex is leg length. Since we were most concerned to describe properties that are generally true over a range of natural circumstances we felt that specific investigation of sex was outside the scope, and was only one of many factors that will modulate retinal motion. We all spend a lot of our time walking over ground planes and our goal was to capture a valid basis for how that might relate to the structure of motion processing in the brain. In other work we are examining individual sensitivity to their own motion patterns.We have now indicated the nature of the variability between subjects and show individual subject data in the Supplementary Materials. We agree these are important questions and worth investigating. However, the goal of the present study was to specify the common features for humans walking over ground planes, so we chose not to focus on the individual variability but rather on the commonalities. The properties of the statistics that we report are true for all subjects and should allow one to evaluate the kinds of changes occasioned by looking closer to the body, for example. By showing how the retinal motion derives from gait modulations, gaze angle, and terrain and how these factors influence the statistics we hope that in principle it should be possible to estimate the consequences of increased blur for example, which would lead to fixations closer to the body.</p><disp-quote content-type="editor-comment"><p>Additionally, the paper would be enhanced by a discussion of the following points:</p><p>1. Many figures in this paper include representations of average speed and average direction on &quot;the retina&quot;. In all cases, a single representation is shown, as though for a cyclopean eye. However, it is unclear how this information is actually represented on the two retinas, and how information from the two eyes is then combined to a different effect (i.e. behavior) by different portions of the visual circuitry. Can the authors please discuss?</p></disp-quote><p>The eye tracker combines the estimates of gaze point from the two eyes (mean location) and that is what we use as the point of fixation. We estimate retinal motion on a single cyclopean retina fixated on that point. It would be nice to have eye specific calculations but the intrinsic error in the measurements make this of limited value given the current accuracy of the trackers. This is now specified in the Methods Section.</p><disp-quote content-type="editor-comment"><p>Reviewer #2 (Recommendations for the authors):</p><p>The overall setup is impressive, but I am curious about the computer vision component. As I understand, the optic flow statistics were calculated on a reprojected virtual scene. While the authors report an impressively small error between terrain reconstructions, I would have appreciated a Methods figure that shows example reconstructions. I am especially curious about some of the more rocky terrains, which I expect to be challenging to reconstruct with all its intricate 3D information (e.g., all the twigs and rocks sticking out of the background). Would mispredictions of the depth of these objects affect the optic flow estimations much? Also, how did lighting affect the reconstructions?</p></disp-quote><p>An example is now shown in Figure 9. For one subject, certain portions of the recording had to be excluded due to poor reconstruction of the terrain resulting from bad lighting. The reconstructions can be sensitive to lighting but most of our recordings were done during brighter times of the day. Reconstruction in the vicinity of where subjects were walking was almost always good and intact.</p><disp-quote content-type="editor-comment"><p>I think the implications of the findings for the neural code of vision could be presented/discussed more rigorously.</p></disp-quote><p>We have pointed out a number of contradictions in the Discussion. This is one of the reasons we feel that we are limited in what we can say about the neurophysiology.</p><disp-quote content-type="editor-comment"><p>Related, the comparison of the current findings to the Gibsonian view should be expanded upon. I understand that the underlying reason is different (stabilization vs. coincidence of gaze with the direction of travel), but the end result seems to be the same – that on average you get an expanding flow field. How does this finding relate to earlier work [13], which argued that the Gibsonian view rarely appears in the wild (unless you are a self-driving car)?</p></disp-quote><p>Our previous paper (Matthis et al. 2022) goes into this issue in depth, and we have added discussion of this in the present paper to make it clear that retinal motion is generated not directly by body motion, but indirectly by stabilizing rotations and translations of the eye to counteract body motion during a step. See page 12 and the videos. The implications are for the way the motion is used, and this is related but a bit orthogonal to the current ms. The Matthis et al. paper measures the time-varying nature of the retinal flow field through the gait cycle. What this means is that the direction of the head in space varies wildly during a step, so unless there is a mechanism to integrate heading direction over two steps there is no way that heading can be used to steer towards a goal as is commonly thought. This theme has been a common one in both psychophysics and neurophysiology in the past. Since the implication is that we have as a field been posing the wrong question, as you can imagine it has met with some resistance so we did not seek to highlight it here, since in the current ms, statistics are integrated over time, terrains, subjects etc and the time-varying nature of the flow was not the focus.</p><disp-quote content-type="editor-comment"><p>Reviewer #3 (Recommendations for the authors):</p></disp-quote><disp-quote content-type="editor-comment"><p>Technical comments:</p><p>4.2. p3 2nd para from bottom: &quot;Stabilization is most likely accomplished by the vestibular ocular reflex, although other eye movement systems might also be involved&quot;; I can see no compelling reason why the compensatory eye movements would be – physiologically speaking – predominantly VOR based, as opposed to optokinetic response or smooth pursuit. I.e. why you would consider VOR &quot;most likely&quot;. The reference you give, as far as I can tell, gives no data that directly speaks on this. You may have some argument in mind but it is not 100% clear to me what that is.</p></disp-quote><p>In the literature on visual responses to optic flow, nearly all the work is with a seated observer, so there is no vestibular signal, and much of the focus has been on the role of optic flow in heading towards a goal. In this literature investigators typically use a pursuit eye movement target. We use this language because in natural behavior the head is always moving and the role of the VOR is to stabilize the image on the retina. The head oscillations during locomotion generate the acceleration signals that drive the VOR. The only downside to the VOR here is the low temporal frequency loss of sensitivity which might need to be compensated for perhaps by some predictive mechanism when moving at a constant velociity (because OKN and pursuit are longer latency than the VOR – approximately 100 msec versus 10 msec). Since how it is done is tangential to the goals of the paper and we can’t resolve it, we used what we hoped was diplomatic language. See Land (2019) for a discussion of the ubiquity of the VOR in the animal kingdom. Note that damage to the vestibular system is highly debilitating. We have added to the discussion of stabilization mechanisms on p13 and 18.</p></body></sub-article></article>